<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>No documentation of response from a bulk index request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14257</link><project id="" key="" /><description>The docs simply say that the response is JSON and don't include any examples or schema. 
</description><key id="112923834">14257</key><summary>No documentation of response from a bulk index request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">evanphx</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-10-23T01:33:50Z</created><updated>2016-09-27T15:57:21Z</updated><resolved>2016-09-27T15:57:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-23T02:05:33Z" id="150411080">Did you see this? https://www.elastic.co/guide/en/elasticsearch/guide/current/bulk.html?q=Bulk
</comment><comment author="polyfractal" created="2015-10-23T03:35:57Z" id="150451664">This was my doing (twitter convo with @evanphx, I prompted him to submit a ticket). :) 

I agree that we should add an example response to the reference docs similar to what we have in the guide, just so it's clear. 

(Amusingly, I forgot we had an example in the Guide.  We could probably even steal the same example to put in the ref docs)
</comment><comment author="dadoonet" created="2015-10-23T06:05:00Z" id="150485333">Yes or cross-link?
</comment><comment author="evanphx" created="2015-10-23T15:53:22Z" id="150614670">Nope, I did not see the guide page since they're not linked to from the docs at all.
</comment><comment author="clintongormley" created="2015-11-09T09:47:37Z" id="155013221">I'm going to close both PRs #14444 and #14449 which both just link to the guide.  Instead, let's just copy the relevant text into the ref docs.
</comment><comment author="dakrone" created="2016-09-27T15:57:21Z" id="249909961">We have this now in the 5.x+ documentation, see: https://www.elastic.co/guide/en/elasticsearch/reference/5.x/docs-bulk.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_default_`-mapping needs to allow for negation of `_parent` </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14256</link><project id="" key="" /><description>I might be missing something here (hopefully), but I am at a loss, how to achieve the generic definition of an index, where **all other types except the type "parent" are children of the type "parent"**?

I thought, I could achieve this with index templates + the `_default_` mapping.

The docs for [default mapping](https://www.elastic.co/guide/en/elasticsearch/guide/current/default-mapping.html#default-mapping) state, that

&gt; All types created _after_ the _default_ mapping will include all of these default settings, unless explicitly overridden in the type mapping itself.

The example goes on to display, how `"_all": {"enabled": true}}` can be explicitly overwritten, which seems straightforward. However, I fail to achieve the same thing, after defining a `_parent` mapping in `_default_`:

```
curl -XPOST localhost:9200/_template/familytrouble -d '{
  "template": "trouble*",
  "mappings": {
    "parent": {
  },
    "_default_": {
      "_parent": {"type": "parent"}
    }
  }
}'
curl -XPUT localhost:9200/troubled/parent/1
&gt;&gt; {"error":"RoutingMissingException[routing is required for [troubled]/[parent]/[1]]","status":400}
```

So I guess, if I could use a mapping-definition akin to

```
"_parent": {}
```

in my parent type, this should work. But I could not find the necessary override for the `_parent`-setting.
</description><key id="112918284">14256</key><summary>`_default_`-mapping needs to allow for negation of `_parent` </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels><label>:Index Templates</label><label>:Mapping</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-10-23T00:28:57Z</created><updated>2015-10-29T19:01:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-26T18:42:51Z" id="151244060">@konradkonrad Can you try using 2 templates, where the first has a higher priority, and creates the parent mapping? I have seen issues with precedence before due to serialization/deserialization. The mappings are added one by one, even when multiple are passed at creation time, and unfortunately it isn't always in the same order they are originally parsed in.
</comment><comment author="clintongormley" created="2015-10-29T19:01:38Z" id="152287907">@rjernst that won't work either... I believe the templates are merged before trying to create the mappings, so you end up with the same mappings as if you use just one template.  The problem is that you can't specify a `parent` setting that says "I have no parent settings"
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parsers should throw exception on unknown objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14255</link><project id="" key="" /><description>This PR adds a randomized test to the query test base class that mutates an otherwise correct query by adding an additional object into the query hierarchy. Doing so makes the query illegal and should trigger some kind of exception. The new test revelead that some query parsers quietly return queries when called with such an illegal query. Those are also fixed here.

Relates to #10974 
</description><key id="112830093">14255</key><summary>Parsers should throw exception on unknown objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-22T15:33:58Z</created><updated>2015-11-02T10:48:17Z</updated><resolved>2015-11-02T10:48:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-10-23T07:58:39Z" id="150501137">I like the general idea, especially as it surfaced some issues already. Left a few minor comments.
</comment><comment author="cbuescher" created="2015-10-26T10:14:06Z" id="151090070">@MaineC thanks, changed to using the query builder name in log messages where possible, also modfied the test setup a bit so now insertion of bad field does not happen at random but at every possible start of new object.
</comment><comment author="cbuescher" created="2015-10-29T14:06:44Z" id="152191417">Rebased and ready to go, wdyt @MaineC?
</comment><comment author="MaineC" created="2015-11-02T09:47:56Z" id="152972701">LGTM (apart from another rebase)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recommended Heap - Documentation Error?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14254</link><project id="" key="" /><description>Hi,

Just a quick one, I was reviewing the notes on recommended maximum heap sizes, but noticed one document states 32GB where another states 30.5GB. I assume it's 30.5GB since that is the latest recommendation from Oracle?

https://www.elastic.co/guide/en/elasticsearch/guide/current/heap-sizing.html
https://www.elastic.co/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html

Thanks!
</description><key id="112825387">14254</key><summary>Recommended Heap - Documentation Error?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rylon</reporter><labels><label>docs</label><label>low hanging fruit</label></labels><created>2015-10-22T15:13:47Z</created><updated>2015-10-24T11:47:45Z</updated><resolved>2015-10-23T18:14:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-23T18:14:04Z" id="150652142">This issue was moved to elastic/elasticsearch-definitive-guide#426
</comment><comment author="clintongormley" created="2015-10-23T18:14:56Z" id="150652339">It depends on the JVM - some cut over to uncompressed pointers at different heap sizes.  30.5GB is likely to be safe
</comment><comment author="Rylon" created="2015-10-23T22:53:08Z" id="150710763">Yeah, that makes sense. I just thought it was worth raising so that the docs are consistent.
</comment><comment author="clintongormley" created="2015-10-24T08:43:00Z" id="150777113">@Rylon Agreed - I just moved the issue to the right repository
</comment><comment author="Rylon" created="2015-10-24T11:47:45Z" id="150800255">Thanks! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mutate Processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14253</link><project id="" key="" /><description>meant to have feature parity with [logstash-filter-mutate](https://github.com/logstash-plugins/logstash-filter-mutate)
</description><key id="112819821">14253</key><summary>Mutate Processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-10-22T14:48:28Z</created><updated>2015-11-10T02:52:23Z</updated><resolved>2015-11-10T02:52:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-11-10T02:39:01Z" id="155265182">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait on shard failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14252</link><project id="" key="" /><description>Currently when executing an action (e.g., bulk, delete, or indexing operations) on all shards, if an exception occurs while executing the action on a replica shard we send a shard failure message to the master. However, we do not wait for the master to acknowledge this message and do not handle failures in sending this message to the master. This is problematic because it means that we will acknowledge the action and this can result in losing writes. For example, in a situation where a primary is isolated from the master and its replicas, the following sequence of events can occur:
1. we write to the local primary
2. we fail to write to the replicas
3. we fail in notifying the master to fail the replicas
4. the primary acknowledges the write to the client
5. the master notices the primary is gone and promotes one of the replicas to be primary

In this case, the replica will not have the write that was acknowledged to the client and this amounts to data loss.

Instead, if we waited on the master to acknowledge the shard failures we would never have acknowledged the write to the client in this case.
- [x] Create listener mechanism for executing callbacks when exceptions occur sending a shard failure message to the master #14295
- [x] Add unit tests that show we wait until failure or success (do not have to handle the failures yet) #14707
- [x] Add general support for cluster state batch updates #14899 
- [x] Apply cluster state batch updates to shard failures #15016
- [x] Handle when the node we thought was the master is no longer the master (e.g., master might have stepped down) -&gt; find the actual master (e.g., wait for a new master to be elected) and retry the failed shard notice #15748
- [x] Fail shard failure requests from illegal sources #16275 
- [x] Master tells us we are no longer the primary -&gt; fail the local shard, retry request on new primary #16415 
- [x] Handle failed shard has already been removed from the routing table -&gt; okay #16089 
- [x] Handle master side of shard failures (do not respond to the node until the new cluster state is published, otherwise report failure or allow the node to timeout) #15468
</description><key id="112815998">14252</key><summary>Wait on shard failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Cluster</label><label>enhancement</label><label>Meta</label><label>release highlight</label><label>resiliency</label><label>v5.0.0-alpha1</label></labels><created>2015-10-22T14:32:28Z</created><updated>2016-04-05T06:54:40Z</updated><resolved>2016-02-10T16:40:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-23T14:32:05Z" id="150591103">+1
</comment><comment author="s1monw" created="2015-10-23T20:03:47Z" id="150676476">sounds good to me too 
</comment><comment author="makeyang" created="2016-04-05T05:50:09Z" id="205656721">will this one resovled issue:7572?
</comment><comment author="bleskes" created="2016-04-05T06:54:40Z" id="205682466">@makeyang yes. you are correct. We are waiting with closing that issue until #17038 is in. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace IndexSettings annotation with a full-fledged class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14251</link><project id="" key="" /><description>The @IndexSettings annoationat has been used to differentiate between node-level
and index level settings. It was also decoupled from realtime-updates such that
the settings object that a class got injected when it was created was static and
not subject to change when an update was applied. This change removes the annoation
and replaces it with a full-fledged class that adds type-safety and encapsulates additional
functionality as well as checks on the settings.
</description><key id="112802088">14251</key><summary>Replace IndexSettings annotation with a full-fledged class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Settings</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-22T13:21:01Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-24T13:38:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-22T13:39:00Z" id="150226119">Thank you so much for this! I really disliked using the annotation instead of an actual class!
</comment><comment author="dakrone" created="2015-10-22T20:56:30Z" id="150353943">Left a few (minor) comments, other than that LGTM
</comment><comment author="s1monw" created="2015-10-23T10:07:08Z" id="150533997">@dakrone I pushed some more stuff to make the new IndexSettings more strict. Can you take another look please?
</comment><comment author="jaymode" created="2015-10-23T13:34:34Z" id="150574147">left a few minor comments. Other than that LGTM! This is a nice change and makes it easier to follow IMO
</comment><comment author="dakrone" created="2015-10-23T16:41:08Z" id="150628707">LGTM also, thanks Simon!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add ability to add nested field values to Data document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14250</link><project id="" key="" /><description>Small commit to add ability to add nested paths.

This will be used by processors such as `mutate` for building custom document structure.
</description><key id="112792366">14250</key><summary>add ability to add nested field values to Data document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-10-22T12:28:17Z</created><updated>2015-10-28T17:20:17Z</updated><resolved>2015-10-28T17:20:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-27T04:00:10Z" id="151367381">Left some minor comments OTT LGTM.
</comment><comment author="talevy" created="2015-10-28T15:53:25Z" id="151890334">tests added. @martijnvg, mind taking another look at this?

I will add more functionality in another PR to respond to this: https://github.com/elastic/elasticsearch/issues/14324.
</comment><comment author="martijnvg" created="2015-10-28T16:49:38Z" id="151906815">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch query parsers to use ParseField </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14249</link><project id="" key="" /><description>Posting as work in progress, roughly one third of all parsers touched, more to come.

Relates to #8964
</description><key id="112769157">14249</key><summary>Switch query parsers to use ParseField </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-22T09:58:11Z</created><updated>2015-11-29T21:22:06Z</updated><resolved>2015-11-23T13:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-11-02T21:12:27Z" id="153157968">Had to add the following to turn tests to green again which went red for some of the parsers when switching to ParseFields:

```
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/Abs
index 212dd45..10adf1a 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -316,9 +316,9 @@ public abstract class AbstractQueryTestCase&lt;QB extends AbstractQueryBuilder&lt;QB&gt;&gt;
     public void testFromXContent() throws IOException {
         for (int runs = 0; runs &lt; NUMBER_OF_TESTQUERIES; runs++) {
             QB testQuery = createTestQueryBuilder();
-            assertParsedQuery(testQuery.toString(), testQuery);
+            assertParsedQuery(testQuery.toString(), testQuery, ParseFieldMatcher.STRICT);
             for (Map.Entry&lt;String, QB&gt; alternateVersion : getAlternateVersions().entrySet()) {
-                assertParsedQuery(alternateVersion.getKey(), alternateVersion.getValue());
+                assertParsedQuery(alternateVersion.getKey(), alternateVersion.getValue(), ParseFieldMatcher.EMPTY);
             }
         }
     }
```

@cbuescher does that make sense?

Reasoning behind it:

For IdsQuery here's how we document it in the reference guide:

https://www.elastic.co/guide/en/elasticsearch/reference/1.4/query-dsl-ids-query.html ... only documented parameter name is type.

Here's how we parse it: 
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java#L73

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/query/IdsQueryParser.java#L85

When switching to ParseFields I guessed that the documented parameter is the preferred one. This made the toXContent test fail because here we generate all three versions: 

https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/index/query/IdsQueryBuilderTests.java#L99

which fails when parsing with ParseField and enabling STRICT parsing like we do in the snippet above by default.
</comment><comment author="cbuescher" created="2015-11-03T10:16:49Z" id="153306867">@MaineC makes sense, I think it would be great to get rid of the alternative parameter naming here, it doesn't feel consisten especially because its not documented. I'd say "type" is fine, also for the array version, but I'd wait to see what @clintongormley says about this and if we can safely remove this in 3.0. 
As for the STRICT/EMPTY ParseField flag you added, makes sense to me.
</comment><comment author="MaineC" created="2015-11-03T12:42:57Z" id="153340796">@cbuescher thanks for confirming.

Updated the preview - two tests still failing.

Made _Builder.do_ protected everywhere (was public in some builders) while going through the code.
</comment><comment author="cbuescher" created="2015-11-03T15:22:10Z" id="153386848">@MaineC did a first check on all the changes, this is great. Since it is a huge PR I left a couple of comments, mostly minor. One thing that came to mind when seeing a change where apparently by accident you changes a field name in NestedQueryParser to "path_field": using the parse fields names both while rendering the queries in doXContent as well as in the parser will make accidental renamings like this hard to spot by the current parser tests, unless we have some case where we send the whole query via json string. Maybe we need additional tests for this, but maybe also outside this PR. Thoughts?
</comment><comment author="MaineC" created="2015-11-03T20:22:27Z" id="153475812">+1 on having "real" json tests in addition to the roundtripping thing.

I think one could see this as related to the stuff I did for the java api docs: What we really want is to have codified checks of the examples that are mentioned in the Query DSL docs so we don't accidentally rename parameters. Should relate to what @dadoonet is looking into at the moment.
</comment><comment author="MaineC" created="2015-11-03T20:23:05Z" id="153475945">Oh - forgot: Thanks to you both, @cbuescher and @javanna for the super speedy review and helpful comments!
</comment><comment author="MaineC" created="2015-11-04T12:57:13Z" id="153712996">Just a heads up: I ran an experiment extracting the code snippets we have in the docs and tossing them into a unit test for parsing. Though the current state isn't pretty at all it surfaced some inconsistencies compared to the actual code in our docs already. Working on isolating them, getting the "half-way-generated-half-way-cobbled-together" test into a better state right now.

Maybe one more data point supporting that the examples we include in our docs should be put under test (though fixing docs usually is the lowest entry bar for submitting a PR I can think of ;) )
</comment><comment author="clintongormley" created="2015-11-08T18:40:40Z" id="154856338">&gt;  I think it would be great to get rid of the alternative parameter naming here, it doesn't feel consisten especially because its not documented. I'd say "type" is fine, also for the array version,

++
</comment><comment author="MaineC" created="2015-11-10T12:39:43Z" id="155409844">Update: I believe I addressed all comments.

As mentioned above I found the parameter renaming issues @cbuescher found scary enough to go through the json query dsl reference guide, parse the examples therein into a separate test class to see if everything remains parseable:

https://github.com/elastic/elasticsearch/pull/14249/files#diff-2528bf3a5b01f96751f6fed1ee8f8189R26

No surprise: There were a few typos in the docs as well as a few things that are documented but do not work. However I also found another parameter name that I incidentally renamed. 

To me that's another argument to add more tests to the suite that start with the json representation of the query. However the class linked above looks ugly enough to me that I'd like to throw it away rather sooner than later. For illustration I moved one of the dsl reference guide example queries from above over to the corresponding unit test here:

https://github.com/elastic/elasticsearch/pull/14249/files#diff-262a49e710e2ff18f41b578be2e0fdf7R250

(essentially copying the approach taken here: https://github.com/elastic/elasticsearch/blob/09d2b4130003191631857d5b8a59c22c84ea8fbb/core/src/test/java/org/elasticsearch/index/query/HasChildQueryBuilderTests.java#L200 )

Caveat: Though the change is simple enough for each query, doing it for all will take more than five minutes. So a second look confirming that this is really where we want to go is highly welcome.
</comment><comment author="MaineC" created="2015-11-12T14:38:22Z" id="156118683">Pulled json parsing tests into AbstractQueryTestCase (including the logging method that was part of HasChildQueryBuilderTests), added json parsing tests based on the examples in the reference guide to each query builder, found (and fixed) yet another subtle change in json generation I introduced in the commit introducing ParseFields that is part of this PR.

Ran gradle check successfully.

@martijnvg a second pair of eyes as discussed checking nothing broke would be greatly appreciated.

@cbuescher I added two methods to the AbstractQueryTestCase that I believe are in need for a better name - if you have any ideas, let me know :)

Todo: rebase to current master, squash etc. I left the two commit separate intentionally to separate the refactoring from adding tests. 
</comment><comment author="cbuescher" created="2015-11-16T14:55:50Z" id="157057714">@MaineC Isabel must have switched @martijnvg and @Mpdreamz, I think the earlier comment was directed at @Mpdreamz in response to your recent conversation.
</comment><comment author="cbuescher" created="2015-11-16T15:52:04Z" id="157076177">@MaineC Great +1 for adding at the "reference" json query to the test cases. I left some minor comments. 
Besides that I'm on the fence about the additional test assertions added with the "assertQueryParsedFromJson()" method to each test case. The selection which options are tested there are not entirely clear to me and could seem a bit random for someone casually reading the test. I think most of the parsing should be tested by the existing randomized `testFromXContent()` or am I missing something? On the other hand I understand that it's probably good to have some "controlled" test cases for the parsers. 
</comment><comment author="MaineC" created="2015-11-17T08:29:34Z" id="157307766">&gt; @MaineC Isabel must have switched @martijnvg and @Mpdreamz, I think the earlier comment was 
&gt; directed at @Mpdreamz in response to your recent conversation.

@cbuescher yeah - correct - we have too many Martijns in the autocomplete ;)
</comment><comment author="MaineC" created="2015-11-18T08:55:47Z" id="157648073">Cleaned and squashed. @cbuescher I think this is ready for another look.
</comment><comment author="cbuescher" created="2015-11-18T10:34:33Z" id="157673598">@MaineC I went through the PR again, found one potential issue to look into but other than that seems good to me.
</comment><comment author="MaineC" created="2015-11-18T11:59:21Z" id="157690169">Thought a bit about how one could support checking more than one json, here's one proposal:

```
diff --git a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
index 35d883d..dbc9baa 100644
--- a/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
+++ b/core/src/test/java/org/elasticsearch/index/query/AbstractQueryTestCase.java
@@ -869,19 +869,9 @@ public abstract class AbstractQueryTestCase&lt;QB extends AbstractQueryBuilder&lt;QB&gt;&gt;
         throw new UnsupportedOperationException("this test can't handle MultiTermVector requests");
     }

-    public void testParseFromJSON() throws IOException {
-        String json = createParseableQueryJson();
-        QB query = (QB) parseQuery(json);
-        assertQueryParsedFromJson(json, query);
-
-        // now assert that we actually generate the same JSON
-        XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
-        query.toXContent(builder, ToXContent.EMPTY_PARAMS);
-        assertEquals(msg(json, builder.string()), json.replaceAll("\\s+",""), builder.string().replaceAll("\\s+",""));
-    }    
-
     /**
-     * Override this method to provide a valid json string representing the query under test.
+     * Call this method to check a valid json string representing the query under test against
+     * it's generated json.
      *
      * Note: By the time of this writing (Nov 2015) all queries are taken from the query dsl
      * reference docs mirroring examples there. Here's how the queries were generated:
@@ -895,10 +885,15 @@ public abstract class AbstractQueryTestCase&lt;QB extends AbstractQueryBuilder&lt;QB&gt;&gt;
      * &lt;li&gt; By now the roundtrip check for the json should be happy.
      * &lt;/ul&gt;
      **/
-    protected abstract String createParseableQueryJson();
-    
-    /** Add any checks on the query builder parsed from the json above here. */
-    protected abstract void assertQueryParsedFromJson(String json, QB parsed); // I need a better name.
+    public void checkGeneratedJson(String expected, QB source) throws IOException {
+        // now assert that we actually generate the same JSON
+        XContentBuilder builder = XContentFactory.jsonBuilder().prettyPrint();
+        source.toXContent(builder, ToXContent.EMPTY_PARAMS);
+        assertEquals(
+                msg(expected, builder.string()),
+                expected.replaceAll("\\s+",""),
+                builder.string().replaceAll("\\s+",""));
+    }    

     private String msg(String left, String right) {
         int size = Math.min(left.length(), right.length());
diff --git a/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java b/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
index d8aeb43..3f8ade2 100644
--- a/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
+++ b/core/src/test/java/org/elasticsearch/index/query/MatchQueryBuilderTests.java
@@ -236,9 +236,8 @@ public class MatchQueryBuilderTests extends AbstractQueryTestCase&lt;MatchQueryBuil
         }
     }

-    @Override
-    protected String createParseableQueryJson() {
-        return "{\n" + 
+    public void testSimpleMatchQuery() throws IOException {
+        String json = "{\n" + 
                 "  \"match\" : {\n" + 
                 "    \"message\" : {\n" + 
                 "      \"query\" : \"to be or not to be\",\n" + 
@@ -254,11 +253,10 @@ public class MatchQueryBuilderTests extends AbstractQueryTestCase&lt;MatchQueryBuil
                 "    }\n" + 
                 "  }\n" + 
                 "}";
-    }
+        MatchQueryBuilder qb = (MatchQueryBuilder) parseQuery(json);
+        checkGeneratedJson(json, qb);

-    @Override
-    protected void assertQueryParsedFromJson(String json, MatchQueryBuilder parsed) {
-        assertEquals(json, "to be or not to be", parsed.value());
-        assertEquals(json, Operator.AND, parsed.operator());
+        assertEquals(json, "to be or not to be", qb.value());
+        assertEquals(json, Operator.AND, qb.operator());
     }
 }
```
</comment><comment author="cbuescher" created="2015-11-18T12:13:04Z" id="157692539">@MaineC the diff is a bit hard to read, but if I understand correctly you are moving most of the test (json string setup and assertions) into the test class and use `checkGeneratedJson` only as a common helper (might even be static then?). I think that is a good idea since it leaves it up to each individual query builder test how and how often to make use of it and it gets rid of the `assertQueryParsedFromJson` which felt a bit awkward anyway.
</comment><comment author="MaineC" created="2015-11-19T10:19:59Z" id="158015097">You did understand correctly. Will follow along this path then.
</comment><comment author="MaineC" created="2015-11-19T13:25:28Z" id="158055903">@cbuescher Thanks for the feedback. Here's what I did:
- fixed the Match Query parser parameter typo
- moved all json parsing test logic to the respective query test class leaving only the json comparison methods as static helper methods in AbstractQueryTestCase
- add a test for the Match Query parameter name.

I left the commits separate to make reviewing at least a little bit easier. Tests are still running on my local box while I'm typing this. Edit a few minutes later: At least on my machine the test run was successful.
</comment><comment author="MaineC" created="2015-11-19T13:50:58Z" id="158061568">Renamed has-child-query:child_type query parameter back to "type" as per https://github.com/elastic/elasticsearch/pull/14800#discussion_r45340994
</comment><comment author="cbuescher" created="2015-11-19T17:58:39Z" id="158137451">Thanks, LGTM
</comment><comment author="clintongormley" created="2015-11-28T15:21:27Z" id="160310123">Nice, @MaineC! Can https://github.com/elastic/elasticsearch/issues/8964 be closed?
</comment><comment author="MaineC" created="2015-11-29T21:22:06Z" id="160472889">@clintongormley #8964 talks about replacing all parameter parsing, this PR only switched over the stuff related to queries, @colings86 is making similar changes while he's touching aggregations - but this still leaves us with IIRC roughly 100 classes to move over last time I counted.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Acknowledged: false for successful index settings operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14248</link><project id="" key="" /><description>In a cluster of ~25 nodes running ES 1.7.1, ~900 indices, ~20k shards and ~1TB of data, when I run some "expensive" API call involving all indexes (example: set index allocation delay) I get `{ "acknowledged" : false }` even though the setting is applied properly. 

This API inconsistency block me to orchestrate automated operations without implementing some hack like splitting the calls to subset of indexes, retries and so on.

This issue seems to be related to #10607
</description><key id="112762487">14248</key><summary>Acknowledged: false for successful index settings operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">giaquinti</reporter><labels /><created>2015-10-22T09:14:58Z</created><updated>2015-10-22T12:50:07Z</updated><resolved>2015-10-22T09:22:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-22T09:22:34Z" id="150159108">if your operation is expensive and takes longer than the default ack timeout of 30s, you should increase it using the `timeout` parameter to whatever makes sense. I'm closing this for now, please reopen if you have any more issues.
</comment><comment author="giaquinti" created="2015-10-22T09:37:45Z" id="150162369">Hi @bleskes thanks for the suggestion but unfortunately I still see it as a workaround.  

I think that ES should provide API consistency.
</comment><comment author="bleskes" created="2015-10-22T09:39:17Z" id="150162644">I'm afraid I don't follow you. What do you mean exactly with consistency? can you give a concrete example?
</comment><comment author="giaquinti" created="2015-10-22T09:45:18Z" id="150163650">For consistency I mean: 
- if an operation fails API should return '{ "acknowledged" : false }'
- if an operation doesn't fail API should return '{ "acknowledged" : true }'

Right now the operation doesn't fail and the API returns fail. 

Do you suggest something like use a timeout parameters to an infinite number? In that case you'll probably wait forever but the response should be consistent, right?
</comment><comment author="bleskes" created="2015-10-22T09:49:00Z" id="150164417">I see. We communicate failures using http status codes and an error indication. The acknowledgement indicates a time out - it means the master have done the requested operation (updated index settings) but waiting on the nodes timed out (because one of the nodes took too long to respond). 
</comment><comment author="giaquinti" created="2015-10-22T10:55:43Z" id="150179870">What about having a 'wait_for' flag similar than what we have for the cluster api? 

Example:

```
curl -XPUT 'http://localhost:9200/_all/settings?wait_for_ack=X&amp;timeout=50s' -d '{something}'

wait for completion
Will wait (until the timeout provided) until the operation is acknowledge successfully by X nodes. Use 'all' for all the nodes.

timeout
A time based parameter controlling how long to wait if one of the wait_for_XXX are provided. Defaults to 30s.
```

This could be a useful improvement. What do you think about it @bleskes ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add cat API for repositories and snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14247</link><project id="" key="" /><description>Adds the following two endpoints:
- /_cat/repositories
- /_cat/snapshots/{repository}

Relates to #13919
</description><key id="112761251">14247</key><summary>Add cat API for repositories and snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-22T09:08:15Z</created><updated>2016-01-06T22:32:44Z</updated><resolved>2015-10-26T17:40:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-23T01:01:57Z" id="150398207">This looks good, but can you add some rest tests for the two new _cat APIs?
</comment><comment author="ywelsch" created="2015-10-26T09:22:50Z" id="151073795">Thanks for the review @dakrone. Can you have another look? I added tests and documentation.
</comment><comment author="dakrone" created="2015-10-26T14:21:16Z" id="151152829">Left one really minor comment, other than that LGTM
</comment><comment author="gmile" created="2016-01-06T22:32:28Z" id="169483607">@ywelsch out of curiosity, how does this file gets generated? Is there a strict protocol / process to follow when adding new API endpoints?

https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/src/main/resources/rest-api-spec/api/cat.repositories.json
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API: Index templates not getting applied correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14246</link><project id="" key="" /><description>Occurs in ES 1.7.2, 1.7.3.

I have created two perfectly equivalent mappings, once through the Java API and once through curl/Marvel Sense. Only the mapping created via curl is applied to new indexes.
- Start with no indexes and no defined index templates.
- Create a simple index template with one field type not analyzed through curl/Sense.

``` javascript
PUT /_template/template_curl
{
  "template": "curl-*",
  "mappings": {
    "visit": {
      "properties": {
        "visitType": {
          "index": "not_analyzed",
          "type": "string"
        }
      }
    }
  }
}
```
- Create another mapping through the Java API, which is completely equivalent except for the template name and the index template regex. TODO attach unit test.
- Check existing mappings

``` javascript
GET _template
{
   "template_curl": {
      "order": 0,
      "template": "curl-*",
      "settings": {},
      "mappings": {
         "visit": {
            "properties": {
               "visitType": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            }
         }
      },
      "aliases": {}
   },
   "template_java": {
      "order": 0,
      "template": "java-*",
      "settings": {},
      "mappings": {
         "visit": {
            "properties": {
               "visitType": {
                  "index": "not_analyzed",
                  "type": "string"
               }
            }
         }
      },
      "aliases": {}
   }
}
```
- Index the same doc into two indexes that will be created, one matching each index template

``` javascript
POST /curl-1/visit 
{
  "visitType": "REAL_USER"
}
```

``` javascript
POST /java-1/visit 
{
  "visitType": "REAL_USER"
}
```
- Now check the applied mappings:

``` javascript
GET java-1/_mapping
{
   "java-1": {
      "mappings": {
         "visit": {
            "properties": {
               "visitType": {
                  "type": "string" // ***SHOULD include "index": "not_analyzed"***
               }
            }
         }
      }
   }
}
```

``` javascript
GET curl-1/_mapping
{
   "curl-1": {
      "mappings": {
         "visit": {
            "properties": {
               "visitType": {
                  "type": "string",
                  "index": "not_analyzed" // here the mapping was applied
               }
            }
         }
      }
   }
}
```
</description><key id="112740968">14246</key><summary>Java API: Index templates not getting applied correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tkaiser</reporter><labels><label>:Index Templates</label><label>feedback_needed</label></labels><created>2015-10-22T06:40:36Z</created><updated>2016-01-29T19:03:09Z</updated><resolved>2016-01-29T19:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tkaiser" created="2015-10-22T09:58:38Z" id="150166181">Update: Apparently I used a malformed JSON object for the mapping part of the PutIndexTemplateRequestBuilder. 
Still, two index templates that are indistinguishable from the REST interface are applied differently to new indexes.
I have forked the project at branch 1.7 and will investigate further.
</comment><comment author="clintongormley" created="2016-01-29T19:03:09Z" id="176914427">Nothing further. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes broken links.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14245</link><project id="" key="" /><description /><key id="112701210">14245</key><summary>Fixes broken links.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:45:15Z</created><updated>2015-10-22T21:30:01Z</updated><resolved>2015-10-22T21:30:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-22T01:20:37Z" id="150070428">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes broken links.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14244</link><project id="" key="" /><description /><key id="112701189">14244</key><summary>Fixes broken links.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:45:00Z</created><updated>2015-10-22T21:33:47Z</updated><resolved>2015-10-22T21:33:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:33:47Z" id="150362577">Closing with cherrypick from #14245. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes broken links.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14243</link><project id="" key="" /><description /><key id="112700599">14243</key><summary>Fixes broken links.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:40:36Z</created><updated>2015-10-22T21:35:22Z</updated><resolved>2015-10-22T21:35:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:35:20Z" id="150362965">Closed with cherrypick from #14245. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.6 linkfix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14242</link><project id="" key="" /><description>Fixes some bad links.
</description><key id="112700423">14242</key><summary>1.6 linkfix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:38:50Z</created><updated>2015-10-22T21:39:45Z</updated><resolved>2015-10-22T21:39:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:39:45Z" id="150363992">Closed via cherrypick from #14245.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.7 linkfix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14241</link><project id="" key="" /><description>Fixes some bad links.
</description><key id="112700388">14241</key><summary>1.7 linkfix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:38:27Z</created><updated>2015-10-22T21:44:45Z</updated><resolved>2015-10-22T21:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:44:45Z" id="150365068">Closed via cherrypick from #14245.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes bad links.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14240</link><project id="" key="" /><description /><key id="112700344">14240</key><summary>Fixes bad links.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:38:04Z</created><updated>2015-10-22T21:45:20Z</updated><resolved>2015-10-22T21:45:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:45:20Z" id="150365277">Closed via cherrypick from #14245.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.5 linkfix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14239</link><project id="" key="" /><description>Fixes some bad links in 1.5 due to changing URL architectures.
</description><key id="112698905">14239</key><summary>1.5 linkfix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:24:21Z</created><updated>2015-10-22T21:44:01Z</updated><resolved>2015-10-22T21:44:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:44:01Z" id="150364846">Fixed via cherrypick from #14245.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.4 linkfix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14238</link><project id="" key="" /><description>Fixes bad links related to changes in URLs for the 2.0 release.
</description><key id="112696822">14238</key><summary>1.4 linkfix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T23:06:27Z</created><updated>2015-10-22T21:45:46Z</updated><resolved>2015-10-22T21:45:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:45:46Z" id="150365353">Closed via cherrypick from #14245.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.3 linkfix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14237</link><project id="" key="" /><description>Fixes a bad link in 1.3 docs caused by a change in the current URL.
</description><key id="112695988">14237</key><summary>1.3 linkfix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-10-21T22:59:37Z</created><updated>2015-10-22T21:46:20Z</updated><resolved>2015-10-22T21:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="palecur" created="2015-10-22T21:46:20Z" id="150365459">Closed via cherrypick from #14245.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic:strict mapping, not validating fields if value of field is null.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14236</link><project id="" key="" /><description>It appears that this issue was brought up here [here](https://github.com/elastic/elasticsearch/issues/9444), and closed, however I am still seeing the same behavior in 1.7.3. 
As mentioned in the issue from the link, after creating an index with a mapping that includes `"dynamic":"strict"`, Elasticsearch will reject fields not specified in the mapping, only if the fields have a value. Passing a field with a `null` value will not be subject to any validation. Also see my stackoverflow post [here](http://stackoverflow.com/questions/33266982/elasticsearch-strict-mapping-not-working-for-fields-with-null-values)
</description><key id="112675582">14236</key><summary>dynamic:strict mapping, not validating fields if value of field is null.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Bkahler</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-10-21T20:47:04Z</created><updated>2016-01-29T19:02:50Z</updated><resolved>2016-01-29T19:02:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-23T18:03:29Z" id="150649133">It ignores fields with value `null`, because essentially they contain no values.  Not sure if we can do any more here, so marking this as "discuss"
</comment><comment author="clintongormley" created="2016-01-29T19:02:50Z" id="176914349">Really there is nothing to do here - a field with a null value won't create a field, and so can't be rejected. Going to close this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test testNotAnalyzedFields fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14235</link><project id="" key="" /><description>The stack trace:

```
java.lang.AssertionError: 
Expected: &lt;1&gt;
     but: was &lt;2&gt;
    at __randomizedtesting.SeedInfo.seed([5BB8531ACCF5DED4:3B477C6B74871C14]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.marvel.agent.renderer.shards.ShardsTests.testNotAnalyzedFields(ShardsTests.java:127)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1665)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:864)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:900)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:914)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:873)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:775)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:809)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:820)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```

The build failure: https://internal-build.elastic.co/job/es_xplugins_2x_medium/1251/
</description><key id="112659895">14235</key><summary>Test testNotAnalyzedFields fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>test</label></labels><created>2015-10-21T19:17:46Z</created><updated>2015-10-21T19:20:53Z</updated><resolved>2015-10-21T19:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-21T19:20:53Z" id="149999639">@jdconrad this is a failure in the x-plugins project, it should be opened there instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Map log-level 'trace' to JDK-Level 'FINEST'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14234</link><project id="" key="" /><description>This allows to enable the trace-log via JdkESLogger.setLevel(), otherwise both 'debug' and 'trace' are mapped to 'FINE' and there is no way to enable 'FINER'.

Without this, it is not possible to adjust the trace logs when using JDK Logging and trying to adjust the loglevel during runtime, e.g. in Java Clients in order to enable debug-log dynamically.
</description><key id="112658386">14234</key><summary>Map log-level 'trace' to JDK-Level 'FINEST'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">centic9</reporter><labels><label>:Logging</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-21T19:09:30Z</created><updated>2015-11-23T08:59:43Z</updated><resolved>2015-11-23T08:59:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-19T12:56:38Z" id="158049399">Hi @centic9! Thanks for your PR. 

I had a look at it. When you look at `JdkESLogger#isTraceEnabled()`, `JdkESLogger#internalTrace(String)` and `JdkESLogger#internalTrace(String, Throwable)`, they currently use `Level.FINEST`. 

Can you please adjust the log level consistently? I'd change all of them consistently to `Level.FINEST` as it is less invasive and in line with the current implementation which should lead to less surprises.
</comment><comment author="centic9" created="2015-11-19T21:39:16Z" id="158205833">Sure, done now.
</comment><comment author="danielmitterdorfer" created="2015-11-20T07:39:54Z" id="158312680">Hi @centic9!

Thanks for the change. This looks good to me now. Can you please squash your changes into one commit and force-push once again to your repo?

I'll leave this PR open for another day or so to give others a chance to look at the PR too. If nobody objects, I'll merge the PR soon.
</comment><comment author="danielmitterdorfer" created="2015-11-23T08:59:29Z" id="158880024">Thanks for the PR @centic9! I've merged it now to master and 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test testIndexRollingUpgrade fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14233</link><project id="" key="" /><description>It appears records are both being added and deleted during rolling upgrades in 1.X builds

```
java.lang.AssertionError: Count is 119 but 110 was expected.  Total shards: 1 Successful shards: 1 &amp; 0 shard failures:
    at __randomizedtesting.SeedInfo.seed([AE2163553F016965:D85C3953D6D333B3]:0)
    at org.junit.Assert.fail(Assert.java:93)
    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:229)
    at org.elasticsearch.bwcompat.BasicBackwardsCompatibilityTest.testIndexRollingUpgrade(BasicBackwardsCompatibilityTest.java:349)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1627)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:836)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:872)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:886)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:47)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:59)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:845)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:747)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:781)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:792)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:59)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)

```

The build failure is http://build-us-00.elastic.co/job/es_bwc_17/2180/CHECK_BRANCH=tags%2Fv1.2.4,jdk=JDK7,label=bwc/testReport/junit/org.elasticsearch.bwcompat/BasicBackwardsCompatibilityTest/testIndexRollingUpgrade/
</description><key id="112657469">14233</key><summary>Test testIndexRollingUpgrade fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>test</label><label>upgrade</label></labels><created>2015-10-21T19:03:47Z</created><updated>2016-01-29T18:59:20Z</updated><resolved>2016-01-29T18:59:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T18:59:20Z" id="176913418">This appears to be passing now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Properly bind ClassSet extensions as singletons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14232</link><project id="" key="" /><description>The ExtensionPoint.ClassSet binds adds the extension classes to a a Multibinder and binds
the classes and calls the asEagerSingleton method on the multibinder. This does not actually
create a singleton. Instead we first bind the class as a singleton and add then add the class
to the multibinder.

Closes #14194
</description><key id="112657373">14232</key><summary>Properly bind ClassSet extensions as singletons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Internal</label><label>blocker</label><label>bug</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-21T19:03:19Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-21T20:58:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-21T19:10:17Z" id="149996872">is there a change we can add a unittest for this?
</comment><comment author="s1monw" created="2015-10-21T20:58:23Z" id="150022356">I have a unit-test that fails without and passes with the fix... I will pull this one in and push the test afterwards directly
</comment><comment author="s1monw" created="2015-10-21T21:16:21Z" id="150026622">backported to 2.x 2.0 and 2.1 thanks @jaymode 
</comment><comment author="jaymode" created="2015-10-21T23:13:35Z" id="150050493">Thank you for the test @s1monw
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test testKatakanaStemFilter hangs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14231</link><project id="" key="" /><description>The following test is hanging org.elasticsearch.index.analysis.KuromojiAnalysisTests.testKatakanaStemFilter

```
&#1571;&#1603;&#1578; 21, 2015 10:16:47 &#1589; com.carrotsearch.randomizedtesting.ThreadLeakControl$2 evaluate
WARNING: Suite execution timed out: org.elasticsearch.index.analysis.KuromojiAnalysisTests
==== jstack at approximately timeout time ====
"TEST-KuromojiAnalysisTests.testKatakanaStemFilter-seed#[2EFDA19BF9C122F]" ID=13 RUNNABLE
    at org.apache.lucene.util.fst.FST.findTargetArc(FST.java:1195)
    at org.apache.lucene.analysis.ja.dict.TokenInfoFST.cacheRootArcs(TokenInfoFST.java:60)
    at org.apache.lucene.analysis.ja.dict.TokenInfoFST.&lt;init&gt;(TokenInfoFST.java:48)
    at org.apache.lucene.analysis.ja.dict.TokenInfoDictionary.&lt;init&gt;(TokenInfoDictionary.java:57)
    at org.apache.lucene.analysis.ja.dict.TokenInfoDictionary.&lt;init&gt;(TokenInfoDictionary.java:33)
    at org.apache.lucene.analysis.ja.dict.TokenInfoDictionary$SingletonHolder.&lt;clinit&gt;(TokenInfoDictionary.java:72)
    at org.apache.lucene.analysis.ja.dict.TokenInfoDictionary.getInstance(TokenInfoDictionary.java:65)
    at org.apache.lucene.analysis.ja.JapaneseTokenizer.&lt;init&gt;(JapaneseTokenizer.java:212)
    at org.apache.lucene.analysis.ja.JapaneseTokenizer.&lt;init&gt;(JapaneseTokenizer.java:198)
    at org.elasticsearch.index.analysis.KuromojiAnalysisTests.testKatakanaStemFilter(KuromojiAnalysisTests.java:133)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:497)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1638)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:847)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:897)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:856)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:792)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:803)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)

"SUITE-KuromojiAnalysisTests-seed#[2EFDA19BF9C122F]" ID=12 RUNNABLE
    at sun.management.ThreadImpl.dumpThreads0(Native Method)
    at sun.management.ThreadImpl.dumpAllThreads(ThreadImpl.java:446)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.formatThreadStacksFull(ThreadLeakControl.java:671)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.access$900(ThreadLeakControl.java:62)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$2.evaluate(ThreadLeakControl.java:412)
    - locked java.lang.Object@22d8495b
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:662)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.access$200(RandomizedRunner.java:138)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$1.run(RandomizedRunner.java:579)

"JUnit4-serializer-daemon" ID=10 TIMED_WAITING
    at java.lang.Thread.sleep(Native Method)
    at com.carrotsearch.ant.tasks.junit4.events.Serializer$1.run(Serializer.java:47)

"Signal Dispatcher" ID=4 RUNNABLE

"Finalizer" ID=3 WAITING on java.lang.ref.ReferenceQueue$Lock@6d03a20c
    at java.lang.Object.wait(Native Method)
    - waiting on java.lang.ref.ReferenceQueue$Lock@6d03a20c
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143)
    at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164)
    at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

"Reference Handler" ID=2 WAITING on java.lang.ref.Reference$Lock@6bc37c17
    at java.lang.Object.wait(Native Method)
    - waiting on java.lang.ref.Reference$Lock@6bc37c17
    at java.lang.Object.wait(Object.java:502)
    at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)

"main" ID=1 WAITING on com.carrotsearch.randomizedtesting.RandomizedRunner$1@5b7f65a1
    at java.lang.Object.wait(Native Method)
    - waiting on com.carrotsearch.randomizedtesting.RandomizedRunner$1@5b7f65a1
    at java.lang.Thread.join(Thread.java:1245)
    at java.lang.Thread.join(Thread.java:1319)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:589)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.run(RandomizedRunner.java:444)
    at com.carrotsearch.ant.tasks.junit4.slave.SlaveMain.execute(SlaveMain.java:242)
    at com.carrotsearch.ant.tasks.junit4.slave.SlaveMain.main(SlaveMain.java:353)
    at com.carrotsearch.ant.tasks.junit4.slave.SlaveMainSafe.main(SlaveMainSafe.java:10)
```

The failure: http://build-us-00.elastic.co/job/elasticsearch-20-strong/777/testReport/junit/org.elasticsearch.index.analysis/KuromojiAnalysisTests/testKatakanaStemFilter/

I was unable to reproduce the failure locally.
</description><key id="112655790">14231</key><summary>Test testKatakanaStemFilter hangs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jdconrad</reporter><labels><label>:Plugin Analysis Kuromoji</label><label>adoptme</label><label>test</label></labels><created>2015-10-21T18:54:49Z</created><updated>2016-01-29T18:57:34Z</updated><resolved>2016-01-29T18:57:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T18:57:34Z" id="176912933">This appears to be passing now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test testThatDefaultProfilePortOverridesGeneralConfiguration fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14230</link><project id="" key="" /><description>This test is failing testThatDefaultProfilePortOverridesGeneralConfiguration with the following error

```
java.lang.AssertionError: Expected to get exception when connecting to port 56775
    at __randomizedtesting.SeedInfo.seed([52F72E850ED33F58:191F5D910A6DFFCB]:0)
    at org.junit.Assert.fail(Assert.java:88)
    at org.elasticsearch.transport.netty.NettyTransportMultiPortTests.assertConnectionRefused(NettyTransportMultiPortTests.java:218)
    at org.elasticsearch.transport.netty.NettyTransportMultiPortTests.testThatDefaultProfilePortOverridesGeneralConfiguration(NettyTransportMultiPortTests.java:145)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1665)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:864)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:900)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:914)
    at org.elasticsearch.test.junit.rule.RepeatOnExceptionRule$1.evaluate(RepeatOnExceptionRule.java:63)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:809)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:460)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:873)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:775)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:809)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:820)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:367)
    at java.lang.Thread.run(Thread.java:745)
```

Last failure: http://build-us-00.elastic.co/job/es_core_2x_suse/422/testReport/junit/org.elasticsearch.transport.netty/NettyTransportMultiPortTests/testThatDefaultProfilePortOverridesGeneralConfiguration/
</description><key id="112649909">14230</key><summary>Test testThatDefaultProfilePortOverridesGeneralConfiguration fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jdconrad</reporter><labels><label>test</label></labels><created>2015-10-21T18:23:23Z</created><updated>2016-01-29T18:56:27Z</updated><resolved>2016-01-29T18:56:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-29T18:56:27Z" id="176912645">Old test result. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Including inner hits drastically slows down query results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14229</link><project id="" key="" /><description>Running the following query takes &gt; 30 sec with ~13 million property records:

```
{
  "query": {
    "filtered": {
      "query": {
        "has_child": {
          "type": "transaction",
          "score_mode": "max",
          "inner_hits": {},
          "query": {
            "filtered": {
              "query": {
                "function_score": {
                  "functions": [
                    {
                      "gauss": {
                        "dateSold": {
                          "scale": "12w"
                        }
                      }
                    }
                  ]
                }
              },
              "filter": {
                "bool": {
                  "must": [
                    {
                      "has_parent": {
                        "type": "property",
                        "filter": {
                          "geo_bounding_box": {
                            "type": "indexed",
                            "location": {
                              "bottom_left": {
                                "lat": 33.676925635929535,
                                "lon": -84.57515716552734
                              },
                              "top_right": {
                                "lat": 33.86271855861567,
                                "lon": -84.15252685546875
                              }
                            }
                          }
                        }
                      }
                    },
                    {
                      "terms": {
                        "clientId": [
                          0,
                          1
                        ]
                      }
                    }
                  ]
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 500
}
```

Removing the `"inner_hits": {}` drops the execution time to &lt; 1 sec. Is this expected just by including inner hits?
</description><key id="112641878">14229</key><summary>Including inner hits drastically slows down query results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clexmond</reporter><labels><label>:Inner Hits</label><label>adoptme</label><label>enhancement</label></labels><created>2015-10-21T17:39:06Z</created><updated>2017-05-11T15:09:34Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clexmond" created="2015-10-21T18:14:58Z" id="149982536">Updating `"size": 1` with inner_hits on takes roughly the same amount of time as without inner_hits altogether. Increasing the size of the return set increases the query time linearly by roughly 200ms per inner hit.
</comment><comment author="clintongormley" created="2015-10-23T17:57:13Z" id="150647252">It sounds like this query is being rerun for each hit.  I wonder if this could be improved?

@martijnvg what do you think?
</comment><comment author="martijnvg" created="2015-10-26T05:42:45Z" id="151030067">@clintongormley Yes, we run a small search for inner hits (local on the shard during the fetch phase) for each hit we return to include the inner hits per hit. This is okay if only 10 hits (default) are returned, but if the `size` gets increased more time needs to be spent on executing this mini searches to gather the top inner hits per hit. 

This can be improved. In theory we can execute one search in the fetch phase that collects all inner hits for all hits being returned. This reduces the overhead of all these small searches. The fetch phase does have infrastructure to achieve this, so we should look into this.
</comment><comment author="clexmond" created="2015-10-26T20:28:19Z" id="151274737">@martijnvg It sounds like moving in that direction might also allow us to limit the total number of inner hits returned rather than just on a per hit basis. Is that correct?
</comment><comment author="martijnvg" created="2015-10-27T03:35:45Z" id="151362929">@clexmond I think that would require an additional setting too in the query dsl? The number of inner hits being returned is based on: `size *  number_of_inner_hits_definition * size_in_inner_hits`. A global limit can be added and would just stop adding inner hits to search hits in the response if more than the specified time is time or more then the specified inner hits have been added.

But the idea behind this enhancement is to just remove the overhead that comes with the many small searches (which is noticeable when `size` is set to a higher value) by just executing on search for the all inner hits per shard during the fetch phase. This will should speed things up and hopefully listing to the total number of inner hits isn't needed then.
</comment><comment author="SanZhiYuan" created="2016-01-06T08:54:42Z" id="169269914">is this fixed with the latest release? It takes us 10 minutes when querying 50,000+ results -.-
</comment><comment author="martijnvg" created="2016-01-06T09:55:43Z" id="169280868">@SanZhiYuan No, this hasn't been fixed. So you set `size` to 50000 in the search api?
</comment><comment author="SanZhiYuan" created="2016-01-06T10:39:23Z" id="169293944">@martijnvg yep, I was wondering this may work but not working well when querying with size = 50000, which actually too bad, -.-. I have switched to query with 500 results per request, within 10 threads at the same time, and now it is better. It doesn't take much more time than a single request with size = 500, so I guess it is OK for  ES to parallel when running the "small search", and in my case, our qps is pretty low, but with more than 100 million docs. ^.^ 
</comment><comment author="ljcollins25" created="2016-02-17T05:50:29Z" id="185039550">+1. 
</comment><comment author="rpedela" created="2016-02-22T19:50:02Z" id="187350144">+1

I am having similar performance issues with `inner_hits` with a large-ish index and grandchildren. I have a query that takes about 750 ms without inner hits but takes 7-8 seconds with them using a size of 10.
</comment><comment author="ljcollins25" created="2016-02-25T17:32:05Z" id="188893210">What I actually want is the nested objects grouped by their parent document. In theory, this shouldn't even require a separate query during the fetch phase because the original query contains the results that I want. It just needs to find the parent document for each nested document (which is already happening during the fetch phase) and group that into the inner hits.

Should I consider a different approach than inner hits? Nested Aggregations? I was worried about the performance of nested aggregations here since my documents have a very large (&gt; 10000) nested documents per parent document.

Why are nested objects hidden? In theory one could query the nested objects separately and join/group on the client?
</comment><comment author="martijnvg" created="2016-02-29T07:55:19Z" id="190083713">The time it takes to fetch inner hits during the fetch phase depends on the amount documents being fetched. This is directly based on the regular `size` option and `size` option inside an inner hit definition and the number of inner hit definitions in a search request.

@rpedela How many inner hit definitions are specified in your query? and what is the specified `size` in each of this inner hit definition?

&gt; Should I consider a different approach than inner hits? Nested Aggregations? 

You could try to use a `nested` agg with a `top_hits` as metric agg. 

&gt; Why are nested objects hidden? In theory one could query the nested objects separately and join/group on the client?

Because nested docs are inlined with the main document and don't have an id like normal documents have.
</comment><comment author="rpedela" created="2016-02-29T17:07:14Z" id="190292424">I have one `inner_hits` definition for children and one for grandchildren. The `inner_hits` size is set to 1 for both. I just want the top inner child and grandchild for my particular use case.

I originally tried aggregations but the data set has high cardinality and that was much slower.
</comment><comment author="akasper" created="2016-03-24T18:23:16Z" id="200959341">@martijnvg: I've started working on this issue. Can we discuss some of the details at your earliest convenience? 
</comment><comment author="martijnvg" created="2016-03-24T20:49:39Z" id="201015001">Hey @akasper, the best way to discuss this is by opening a PR with the changes you like to make. 
</comment><comment author="mitsuh" created="2016-08-26T02:49:38Z" id="242612602">+1
</comment><comment author="martijnvg" created="2016-08-29T08:25:03Z" id="243062545">Inner hits does no magic, it just executes extra fetch operation (during fetch phase) in order to inline inner hits into the regular hits. If inner hits is enabled on queries to retrieve many hits (lets say more than 100) then inner hits adds a significant performance tax to search request. If size=100 and a single inner hits is enabled then in total upto 400 hits are retrieved (100 for main and 300 for inner hits (default size for inner hit is 3) ). And if more inner hits are enabled and more regular hits are requested then this only gets worse.

Inner hits has been designed to give insight in the top matching nested document or child / parent documents. It wasn't build to retrieve many hits.

I'm leaning towards adding soft limits here. So that when inner hits is enabled and the regular size is higher than 100 we report an request error. Also when on the individual inner hits more than 10 inner hits are requested then we should report a request error as well. And finally the inner hits default size should be 1 instead of 3.

The idea that I shared initially to improve the fetching of inner hits during fetch phase would improve things a bit, but when many hits and inner hits are requested would resolve in similar bad performance.
</comment><comment author="martijnvg" created="2017-05-11T10:07:37Z" id="300744937">Update: An improvement has made to inner hits (#24571) that will improve the slowness that is being reported here when inner hits has been enabled. This improvement will be part of the 5.5 release.

I expect a good improvement with the query shared in the description. The rewrite of the has_parent query is expensive and now this will be executed only once per shard whereas before it would happen upto 500 times.</comment><comment author="clexmond" created="2017-05-11T12:57:47Z" id="300780931">Thanks @martijnvg exciting news! I've largely refactored away from nested / inner hits, but would love to take advantage of it again with these improvements.</comment><comment author="rpedela" created="2017-05-11T15:09:34Z" id="300819641">Excellent! I look forward to future performance improvements discussed in the PR as well.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed typo in range filter documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14228</link><project id="" key="" /><description>The sentence:

"The `fielddata` execution as the same suggests uses field data and therefore requires more memory, so make sure you have sufficient memory on your nodes in order to use this execution mode." 

has a typo in it. The word "same" I believe should read "name".
</description><key id="112619837">14228</key><summary>Fixed typo in range filter documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ByronDDelpinal</reporter><labels /><created>2015-10-21T15:47:56Z</created><updated>2015-10-23T17:38:07Z</updated><resolved>2015-10-23T17:38:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add .dir-locals.el file for Intellij-style indentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14227</link><project id="" key="" /><description>This is the Emacs equivalent of applying settings to every .java file in the directory for devs that use Emacs.
</description><key id="112607741">14227</key><summary>Add .dir-locals.el file for Intellij-style indentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-21T14:52:53Z</created><updated>2015-10-30T15:34:40Z</updated><resolved>2015-10-30T15:34:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-21T14:53:23Z" id="149922126">@mikemccand @jasontedor can you test this? I tested locally with `emacs -q` and it works great, but it would be good to have additional confirmation.
</comment><comment author="jasontedor" created="2015-10-22T16:57:19Z" id="150290577">I took a look. I think it's really close but it seems to not be handling multi-line the same as IntelliJ in many cases. I don't think we need it to match exactly, but some of the formatting is probably not what we want either way.

Here's what I did. I formatted all files with IntelliJ. I use the standard settings. I made a local commit with these changes. Then, I executed:

```
$ find ~/src/elastic/elasticsearch/core \
&gt; -iname \*.java \
&gt; -exec emacs -nw {} \
&gt; --eval \
&gt;   "(progn \
&gt;     (mark-whole-buffer) (indent-region (point-min) (point-max) nil) (save-buffer))" \
&gt; --kill \;
```

and then

```
$ git diff --stat | tail -1
```

gives

```
683 files changed, 9154 insertions(+), 9154 deletions(-)
```

Most of the examples of oddities are on multi-line handling for example `ActionModule.java`, `TransportGetMappingsAction.java`, `TransportPutRepositoryAction.java`, etc.
</comment><comment author="jasontedor" created="2015-10-22T16:59:21Z" id="150291057">Yeah, commit 490cdfac5c441effaa2148a7325404f24d56d3d1 fixes the issues in `ActionModule.java`. :)
</comment><comment author="dakrone" created="2015-10-22T17:00:35Z" id="150291340">@jasontedor don't forget to run with `-q` or `-Q` to make sure your specific config isn't incorrect!
</comment><comment author="dakrone" created="2015-10-22T17:01:57Z" id="150291652">&gt; 683 files changed, 9154 insertions(+), 9154 deletions(-)

Also this is a little dangerous using for an approximation because there are a lot of files that already have incorrect indentation from Intellij's perspective
</comment><comment author="jasontedor" created="2015-10-22T17:02:56Z" id="150291877">&gt; @jasontedor don't forget to run with `-q` or `-Q` to make sure your specific config isn't incorrect!

@dakrone Yeah; instead I just temporarily blew away my `.emacs` and `.emacs.d` because otherwise emacs will keep asking me to confirm whether or not to apply maybe-not-safe changes from your `.dir-locals.el`.
</comment><comment author="dakrone" created="2015-10-22T17:03:35Z" id="150292008">@jasontedor you should be able to hit `!` once to save them as "safe" for future sessions.
</comment><comment author="jasontedor" created="2015-10-22T17:04:27Z" id="150292192">&gt; Also this is a little dangerous using for an approximation because there are a lot of files that already have incorrect indentation from Intellij's perspective

That's why I first applied IntelliJ's settings to every file in `core` and committed it locally first.
</comment><comment author="jasontedor" created="2015-10-22T17:04:53Z" id="150292274">&gt; @jasontedor you should be able to hit `!` once to save them as "safe" for future sessions.

I think the point is exactly that that doesn't keep if you have `-q`?
</comment><comment author="dakrone" created="2015-10-22T17:05:34Z" id="150292403">&gt; That's why I first applied IntelliJ's settings to every file in core and committed it locally first.

I missed that but I see it now in your original messages, good call! Sorry for the mix up!

&gt; I think the point is exactly that that doesn't keep if you have -q?

Yeah, that's the case, the `-q` is just for testing this.
</comment><comment author="dakrone" created="2015-10-28T21:47:31Z" id="152004826">@jasontedor I went down the path of trying to fix every single indentation problem with this, but it will end up being huge due to tons of special cases. I think for now this is a better alternative than the defaults for indentation. How do you feel about merging this and then I can address further problems if/when they arise?
</comment><comment author="jasontedor" created="2015-10-30T01:38:25Z" id="152377204">Commit 2c16adb78430c544dedf02cc8c8306e3dba1e127 helped with a few more. 

&gt; How do you feel about merging this and then I can address further problems if/when they arise?

I think that sounds like a good plan. LGTM.
</comment><comment author="mikemccand" created="2015-10-30T13:04:09Z" id="152518190">Whoa, I missed this, but I played with it just now and it's really nice!  So I can clean up my .emacs now :)

Thanks @dakrone

+1 to push!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove /_optimize REST API endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14226</link><project id="" key="" /><description>The `/_optimize` endpoint was deprecated in 2.1.0 and can now be removed
entirely.

Follow-up of #13778
</description><key id="112605821">14226</key><summary>Remove /_optimize REST API endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Index APIs</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-21T14:45:13Z</created><updated>2016-03-03T19:12:16Z</updated><resolved>2015-10-29T21:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-21T14:46:01Z" id="149919086">Since this is likely to cause other build failures I won't merge this until I'm sitting at home ready to fix failures in other projects :)
</comment><comment author="javanna" created="2015-10-21T16:34:23Z" id="149953870">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Lucene.isEmpty(DocIdSet).</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14225</link><project id="" key="" /><description>After the Filter ban, this method is not used anymore.
</description><key id="112589441">14225</key><summary>Remove Lucene.isEmpty(DocIdSet).</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-21T13:35:25Z</created><updated>2015-10-21T14:02:57Z</updated><resolved>2015-10-21T14:02:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-21T13:43:31Z" id="149899697">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add priority to the threadpool </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14224</link><project id="" key="" /><description>When a bulk request is sent to elasticsearch, if there are other bulk operations simultaneously being sent there is no way to control how they will be handle, hence the requests will be handled randomly/automatically.

My proposal/question is about adding a priority parameter, which will be useful to set at the top of the queue/priority list those bulk (or any other requests) that are more important that the once that are being handled at some point.
</description><key id="112579631">14224</key><summary>Add priority to the threadpool </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>:Bulk</label><label>discuss</label></labels><created>2015-10-21T12:43:01Z</created><updated>2017-07-18T13:12:10Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-21T13:42:34Z" id="149899474">Using a priority queue could help have some entries be taken out of the queue before others, but in the case that the problem is that the threadpool is already full with long-running bulk requests, this would not help as we would still have to wait for these long-running bulk requests to finish before being able to take a new one.

An alternative would be to have several threadpools but I don't like it as elasticsearch already creates hundreds of threads by default (see eg. #12666).
</comment><comment author="clintongormley" created="2015-10-23T17:32:56Z" id="150640891">If your bulk queue is full, then having a priority just indicates which indexing requests you want to throw away first/last.  This doesn't make much sense to me.  
</comment><comment author="ningjun" created="2015-10-29T15:24:18Z" id="152213598">I like the idea of having several bulk thread pools. This way user can set a dedicated thread pool to handle high priority bulk request. 
This is a very useful feature and I think many customers would like to have this feature.
</comment><comment author="Habitats" created="2017-07-18T13:12:10Z" id="316059103">Any progress on this?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms query ignores minimum_should_match when used in must_not clauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14223</link><project id="" key="" /><description>A `terms` query inside a `bool` query's `must` clause will respect the `minimum_should_match` setting, however, when the same terms clause is embedded inside a `must_not` clause the setting is silently ignored.

[Example GIST with a graph-walking scenario ](https://gist.github.com/markharwood/44aadd21cde319234f90)

The bug is because internally the must_not clauses are rewritten to filters and therefore do not execute the usual `minimum_should_match` query logic, in the interests of performance.

@jpountz  suggested throwing an error if the user passes a `miniumum_should_match` setting in a  `terms` query used as part of a must_not clause.
Internally this makes sense for performance and code maintenance reasons but does come at a cost for end users. This would mean inconsistency in the syntax (must vs must_not behaviour of `terms`) and the work-around is verbose (use a bool `should` with array of individual `term` queries followed by minimum_should_match). 
</description><key id="112568597">14223</key><summary>Terms query ignores minimum_should_match when used in must_not clauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2015-10-21T11:44:50Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-21T13:57:46Z" id="149904123">This inconsistency comes from 1.x where the terms query would generate a bool query and the terms filter would generate a terms filter. Now that we merged both, we have to accept the `minimum_should_match` parameter in all cases and I think the bug is that we don't fail when `min_should_match` is set while we are in a `filter` context.

`minimum_should_match` is currently deprecated so that we can make the `terms` query always generate a Lucene TermsQuery instead of a BooleanQuery when used in a query context, which I think is important for the predictability of the query DSL. Otherwise, queries would have very different performance characteristics when used as a filter or a query, would fail when there are more than 1024 clauses when used as a query and not a filter, etc. This will help remove the inconsistency as well when `minimum_should_match` is finally removed.

I agree the bool syntax is a bit verbose, but I don't think that the terms query should be the solution. Maybe we can find ways to simplify the bool syntax when all sub queries are term queries.
</comment><comment author="clintongormley" created="2016-01-29T18:43:00Z" id="176903410">Min should match hasn't been documented in the terms query since 2.0.  Let's deprecate it now and remove it in 3.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor retry logic for TransportMasterNodeAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14222</link><project id="" key="" /><description>Changes:
- Introduce inner class `AsyncSingleAction` to store state used across retry invocations.
- Same as for `TransportInstanceSingleOperationAction` and `TransportReplicationAction`, `onClusterServiceClose` consistently throws a `NodeClosedException` now.
- Added retry logic if master could not publish cluster state or stepped down before publishing (ZenDiscovery). The test `IndexingMasterFailoverIT` shows the issue (but will not be part of the final commit).
- Simplified retry logic by moving bits from different places into shared retry method.
  - Removed boolean flag `retrying` that aborted retrying after a single master node change (now we retry until timeout).
  - Two existing predicates that deal with master node changes unified in a single predicate `masterNodeChangedPredicate`.

Other implementation notes: In contrast to `TransportInstanceSingleOperationAction` and `TransportReplicationAction`, `onTimeout` does not try to execute the action a last time as in this case we wait for specific cluster change events. This keeps the behaviour consistent with the old code that directly failed on timeout.
</description><key id="112555501">14222</key><summary>Refactor retry logic for TransportMasterNodeAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-21T10:14:52Z</created><updated>2015-11-10T19:01:02Z</updated><resolved>2015-11-09T16:59:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-10-21T10:29:48Z" id="149849331">The method `processBeforeDelegationToMaster` can also be removed. AFAICS, it is nowhere used.
</comment><comment author="bleskes" created="2015-10-28T20:48:10Z" id="151985754">This looks great. Left a bunch of comments.
</comment><comment author="ywelsch" created="2015-11-02T12:50:32Z" id="153008551">@bleskes, can you have another look?

I made (all) changes according to the provided suggestions, in particular:
- removed processBeforeDelegationToMaster
- changed methods retryableOrNoBlockPredicate() and masterNodeChangedPredicate() to fields
- changed masterNodeChangedPredicate implementation to only trigger on master node id changes
- moved discovery.zen.NotMasterException to cluster.NotMasterException
- log output in case of timeout while retrying (log-level warn)
- simplified IntegrationTest (took settings from DiscoveryWithServiceDisruptionsIT)
</comment><comment author="bleskes" created="2015-11-04T12:45:43Z" id="153710610">Update looks great. I left some minor comments mostly around testing...
</comment><comment author="ywelsch" created="2015-11-06T16:09:27Z" id="154451295">@bleskes Pushed another round addressing your comments. Rebased on master so I get #14535. I still cannot remove the line `.put("transport.host", "127.0.0.1")` as there seems to be another issue here. Ports are now correct but binding to multiple interfaces seems to mess with the test.
</comment><comment author="bleskes" created="2015-11-09T12:34:33Z" id="155052038">@ywelsch did another round. Let's the leave the networking related code (i.e., `.put("transport.host", "127.0.0.1")` in and change them in another followup. As you indicated other tests have similar code so we can follow up and remove it from them as well.
</comment><comment author="bleskes" created="2015-11-09T14:01:44Z" id="155069761">LGTM! :dancers: 
</comment><comment author="bleskes" created="2015-11-10T12:39:07Z" id="155409737">@ywelsch I think we should back port this to 2.2 . 
</comment><comment author="ywelsch" created="2015-11-10T13:56:05Z" id="155425769">2.2 does not have `FailedToCommitClusterStateException`...
</comment><comment author="bleskes" created="2015-11-10T14:02:24Z" id="155427724">True, but it does have the &#8220;failed while in queue&#8221; logic. And the code is much cleaner.

&gt; On 10 Nov 2015, at 14:56, Yannick Welsch notifications@github.com wrote:
&gt; 
&gt; 2.2 does not have FailedToCommitClusterStateException...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="ywelsch" created="2015-11-10T18:26:22Z" id="155522631">Back ported and pushed to 2.x minus FailedToCommitClusterStateException.
</comment><comment author="bleskes" created="2015-11-10T19:01:02Z" id="155532020">Thanks Yannick!

&gt; On 10 Nov 2015, at 19:26, Yannick Welsch notifications@github.com wrote:
&gt; 
&gt; Back ported and pushed to 2.x minus FailedToCommitClusterStateException.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore previous optimize transport action name for bw comp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14221</link><project id="" key="" /><description>With #13778 we broke backwards compatibility on the transport layer in 2.x and 2.1 branches. We can potentially make breaking changes on the java api in 2.x when it comes to compile errors but we have to be careful with the transport layer. If we change a transport action name, it means that 2.0 won't be able to communicate with 2.1 for what concerns the optimize/forcemerge api.
</description><key id="112550856">14221</key><summary>Restore previous optimize transport action name for bw comp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Index APIs</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-21T09:47:51Z</created><updated>2015-10-21T11:05:21Z</updated><resolved>2015-10-21T11:05:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-21T10:04:45Z" id="149844334">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check exception from query parsers for unknown field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14220</link><project id="" key="" /><description>Most query parsers throw a ParsingException when they trying to parse a field with an unknown name. This adds a generic check for this to the AbstractQueryTestCase so the behaviour
gets tested for all query parsers. The test works by first making sure the test query has a `boost` field and then changing this to an unknown field name and checking for an error.

There are exceptions to this for WrapperQueryBuilder and QueryFilterBuilder, because here the parser only expects the wrapped `query` element. MatchNoneQueryBuilder and MatchAllQueryBuilder so far had setters for boost() and queryName() but didn't render them, which is added here for
consistency.

GeoDistanceQuery, GeoDistanceRangeQuery and GeoHashCellQuery so far treat unknown field names in the json as the target field name of the center point of the query, which so far was handled by
overwriting points previously specified in the query. This is changed here so that an attempt to use two different field names to specify the central point of the query throws a ParsingException

Relates to #10974
</description><key id="112549974">14220</key><summary>Check exception from query parsers for unknown field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-21T09:43:50Z</created><updated>2015-11-08T21:11:52Z</updated><resolved>2015-10-30T15:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-21T10:06:29Z" id="149844629">I like the new test, it helps streamlining the way we parse and validate queries. Left a few comments.
</comment><comment author="cbuescher" created="2015-10-21T13:21:05Z" id="149891683">@javanna I think I found a safer way to insert an unknown fieldname in the base test query that should work for most queries and updated the PR accordingly. I left the questions about MatchNoneQueryBuilder open and am happy to revert those changes in this PR and take this to another issue.

For the changes in GeoDistance/GeoDistanceRange/GeoHashCellQueryBuilder it would be good to get some additional feedback from @nknize. At first glance it seems okay to raise and exception when specifying the point coordinates for those queries more than once, but maybe I'm missing something.
</comment><comment author="javanna" created="2015-10-21T13:53:22Z" id="149902728">&gt; I left the questions about MatchNoneQueryBuilder open and am happy to revert those changes in this PR and take this to another issue

I am happy either way, but if we address comments on match_none we should also make sure that the boost gets applied. Maybe you did that already though by removing the empty `setFinalBoost` setter.
</comment><comment author="cbuescher" created="2015-10-21T14:07:42Z" id="149907078">&gt; Maybe you did that already though by removing the empty setFinalBoost setter

Yes, that was the idea of that change, should be applied and tested now.
</comment><comment author="cbuescher" created="2015-10-26T11:01:44Z" id="151100177">@javanna still waiting for feedback here on the changes in the geo queries by @nknize, but other than that anything to add from your side?
</comment><comment author="javanna" created="2015-10-26T11:04:31Z" id="151100987">LGTM
</comment><comment author="nknize" created="2015-10-30T15:22:57Z" id="152554769">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>install marvel error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14219</link><project id="" key="" /><description>```
denghp@denghp:~/search/elasticsearch-2.0.0-rc1$ bin/plugin install elasticsearch/marvel/latest
-&gt; Installing elasticsearch/marvel/latest...
Trying https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip ...
Downloading ...........................................................................................................................DONE
Verifying https://download.elastic.co/elasticsearch/marvel/marvel-latest.zip checksums if available ...
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip
```
</description><key id="112529279">14219</key><summary>install marvel error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">denghp</reporter><labels /><created>2015-10-21T07:44:44Z</created><updated>2015-10-21T07:55:07Z</updated><resolved>2015-10-21T07:55:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-21T07:55:07Z" id="149808687">Not available yet. You need to wait for 2.0.0 GA
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reject children documents that don't have a parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14218</link><project id="" key="" /><description>Some of our search code assumes that all children documents have a parent, yet we don't enforce this at index time. This assumption looks sane to me (otherwise why would you put a document in a child type at all?) so we should fix our indexing to reject child documents that don't have a parent?

See https://discuss.elastic.co/t/elasticsearch-2-0-bool-query-with-inner-hits-and-parent-query/31706 for more background.
</description><key id="112474182">14218</key><summary>Reject children documents that don't have a parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Parent/Child</label><label>adoptme</label><label>bug</label></labels><created>2015-10-20T22:35:00Z</created><updated>2016-01-29T18:39:59Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-21T04:44:44Z" id="149781870">Just wondering what would happen if you have indexed a parent, some children and then you delete the parent...

I did not test it but I guess the DELETE operation will be successful, so what will happen to the search parts you mentioned earlier?
</comment><comment author="jpountz" created="2015-10-21T10:19:33Z" id="149847643">I agree this can be an issue as well, but a different one? It is different to not have a value for the `_parent` field and to have a value that points to a document that does not exist.
</comment><comment author="bevans88" created="2015-10-22T11:24:58Z" id="150185208">In a very simple Use Case that somewhat mirrors my current applications use of Parent/Child functionality:
- 'Book' Type that is a Child of 'Author' Type.
- 'Book' may, or may not have an Author. For instance we may have a 'Book' that we don't know the   author of .
  
  If it is enforced that all Child documents must have a Parent set then this becomes invalid.

As far as I'm aware having Child documents with no Parent set has also caused no problems with 'standard' search queries, with the current problem only arising due to the use of them in the recent Inner Hits functionality.

At the moment ElasticSearch actually allows for searching on Child documents that don't have a Parent set:

```
  "query": {
    "bool": {
      "must_not": [
        {
          "has_parent": {
            "parent_type": "Author",
            "query": {
              "match_all": {}
            }
          }
        }
      ]
    }
  }
```

Also, wouldn't the following cause problems (In the same way deleting a Parent would I assume):
- Index Child document with Parent that has not yet been indexed. 
- Run search on child documents.
</comment><comment author="clintongormley" created="2015-10-22T14:09:09Z" id="150236191">This puts a burden on users to index parents then children in that order.  If you are (eg) taking a dump from a database then you may get documents in some random order.  This would also mean doing a real-time check for the existence of a parent any time we index a child doc.  Costly, no? 

Also the point of deleting parent docs without also deleting child docs is a valid one.  Either we have to special case the deletion of parent docs to first delete children, or we have to guard against the case where a child doc may not have parents.  

The above is making me lean towards ensuring that we can always handle the case where corresponding parents don't exist.
</comment><comment author="clintongormley" created="2015-10-22T14:15:09Z" id="150237673">I think I may have misunderstood this - it's not about the existence of the parent document, but about whether the child has a parent specified...  in which case I agree, we should enforce this.

&gt; If it is enforced that all Child documents must have a Parent set then this becomes invalid.

In your example, you can simply have an UnknownAuthor parent document.
</comment><comment author="jpountz" created="2015-10-22T14:24:04Z" id="150239960">&gt; I think I may have misunderstood this - it's not about the existence of the parent document, but about whether the child has a parent specified... in which case I agree, we should enforce this.

Exactly, this is what I meant.
</comment><comment author="martijnvg" created="2015-10-26T05:16:52Z" id="151027153">@clintongormley @jpountz the index time check is insufficient. We only check if a routing value has been specified. When the parent id is specified at index we also set the routing value, so that passes the check. If no parent id is specified we fail, but if only a routing value is specified (like was reported in the forum) then we don't fail while we should.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace IndicesLifecycle with a per-index IndexEventListener</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14217</link><project id="" key="" /><description>Today IndicesLifecycle is a per-node class that allows to register
listeners at any time. It also requires to de-register if listeners
are not needed anymore ie. if classes are created per-index / shard etc.
They also cause issues where listeners are registered more than once as in #13259

This commit removes the per-node class and replaces it with an well defined
extension point that allows listeners to be registered at index creation time
without the need to unregister since listeners are go out of scope if the index
goes out of scope. Yet, this still allows to share instances across indices as before
but without the risk of double registering them etc.

All data-structures used for event notifications are now immuatble and can only changes
on index creation time. This removes flexibility to some degree but increases maintainability
of the interface and the code itself dramatically especially with the step by step removal of
the index level dependency injection.

Closes #13259
</description><key id="112465845">14217</key><summary>Replace IndicesLifecycle with a per-index IndexEventListener</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>:Plugins</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T21:36:30Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-21T20:12:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-20T21:38:31Z" id="149710941">@rjernst can you take a look at this
</comment><comment author="bleskes" created="2015-10-21T14:43:22Z" id="149918378">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore support for escaped '/' as part of document id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14216</link><project id="" key="" /><description>With #13691 we introduced some custom logic to make sure that date math expressions like `&lt;logstash-{now/D}&gt;` don't get broken up into two where the slash appears in the expression. That said the only correct way to provide such a date math expression as part of the uri would be to properly escape the '/' instead. This fix also introduced a regression, as it would make sure that unescaped '/' are handled only in the case of date math expressions, but it removed support for properly escaped slashes anywhere else. The solution is to keep supporting escaped slashes only and require client libraries to properly escape them.

This commit reverts 93ad6969669e65db6ba7f35c375109e3f186675b and makes sure that our REST tests runner supports escaping of path parts, which was more involving than expected as each single part of the path needs to be properly escaped. I am not too happy with the current solution but it's the best I could do for now, maybe not that concerning anyway given that it's just test code. I do find uri encoding quite frustrating in java.

Relates to #13691
Relates to #13665

Closes #14177
</description><key id="112448335">14216</key><summary>Restore support for escaped '/' as part of document id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-20T20:01:32Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-21T10:17:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-21T09:48:30Z" id="149840560">LGTM but maybe this would deserve another review from someone who is more familiar with uri escaping than I am
</comment><comment author="spinscale" created="2015-10-21T09:58:24Z" id="149842897">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert GCE test to ESTestCase</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14215</link><project id="" key="" /><description>And work around GCE's annoying security issues that that exposed.

Required for #14069
</description><key id="112437182">14215</key><summary>Convert GCE test to ESTestCase</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugin Discovery GCE</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T18:58:45Z</created><updated>2015-10-20T21:21:25Z</updated><resolved>2015-10-20T21:21:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-20T18:59:03Z" id="149667632">@rmuir would you like to review?
</comment><comment author="rmuir" created="2015-10-20T20:14:37Z" id="149689454">+1, thanks for fixing this to use the base classes for consistency! I only added a question out of curiosity, but lets get this one in. +1 to go to 2.2 as well if its not too much trouble. I wonder if some of the gnarly battles i have had with getting tests working in e.g. IDEs was really due to this guy not extending the correct base class...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ActionFuture.getRootFailure()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14214</link><project id="" key="" /><description>`ActionFuture.getRootFailure()` is not used anywhere.
It has only a single (completely useless) implementation in `AdapterActionFuture`, returning a private field that is never assigned.
</description><key id="112406575">14214</key><summary>Remove ActionFuture.getRootFailure()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T16:21:31Z</created><updated>2015-11-20T14:08:40Z</updated><resolved>2015-10-20T20:23:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-20T16:30:05Z" id="149623889">LGTM.
</comment><comment author="bleskes" created="2015-10-20T16:55:49Z" id="149631857">LGTM2
</comment><comment author="bleskes" created="2015-10-20T16:56:54Z" id="149632147">I'm good with pushing this to 2.1 as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use built-in method for computing hash code of longs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14213</link><project id="" key="" /><description>This commit replaces instances of manually computing a hash code for
primitive longs by XORing the upper bits with the lower bits with a
built-in method for doing the same.
</description><key id="112399317">14213</key><summary>Use built-in method for computing hash code of longs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T15:47:28Z</created><updated>2015-10-20T15:51:38Z</updated><resolved>2015-10-20T15:49:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-20T15:48:37Z" id="149610993">LGTM
</comment><comment author="jasontedor" created="2015-10-20T15:49:40Z" id="149611289">`\(\s*int\s*\)\s*\(\s*([^ ]*)\s*\^\s*\(\1\s*&gt;&gt;&gt;\s*32\s*\)\s*\)` -&gt; `Long.hashCode\($1\)`
</comment><comment author="jasontedor" created="2015-10-20T15:51:38Z" id="149611832">Thanks for the quick review @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add "Best Practices" section to the cloud-aws docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14212</link><project id="" key="" /><description>Per discussion about collecting best practices and various other AWS lore and then including it in the documentation for the AWS Cloud Plugin.
</description><key id="112388487">14212</key><summary>add "Best Practices" section to the cloud-aws docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>docs</label><label>v2.2.0</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T15:02:12Z</created><updated>2016-01-27T14:15:00Z</updated><resolved>2016-01-25T08:06:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-20T15:33:59Z" id="149604350">Looks very good. If you push that in master branch, please could you change cloud plugin with discovery plugin. It has been renamed in 3.0.
</comment><comment author="djschny" created="2015-10-20T19:21:46Z" id="149674391">Thanks for looking @dadoonet 

Yes I will do a different structure organization for master as it doesn't match up. Should I push this to `2.0` and `2.x` or should this wait based upon the current timing with the release?
</comment><comment author="dadoonet" created="2015-10-20T19:24:18Z" id="149674911">Yes. You can push changes to documentation to 2.0 (and 2.1 &amp; 2.x).
</comment><comment author="dakrone" created="2015-12-09T19:45:02Z" id="163369707">@djschny ping reminder for you to merge this (assuming it's still needed)
</comment><comment author="djschny" created="2016-01-08T21:30:14Z" id="170131809">Sorry, but excuse my ignorance, it seems really silly/dangerous, but this actually has to be independently pushed to all the branches labeled above?
</comment><comment author="dadoonet" created="2016-01-08T22:18:33Z" id="170143446">I'd push this to master, 2.x and 2.2 branches.
</comment><comment author="djschny" created="2016-01-13T19:04:15Z" id="171399401">This has been added to the recommended branches:
- 2.x - https://github.com/elastic/elasticsearch/commit/65faab6935a27342ae2442e85c089638285a297b
- 2.2 - https://github.com/elastic/elasticsearch/commit/50b87fd861c97f3afacfbbe119ec1b530187a5e7
- master - https://github.com/elastic/elasticsearch/commit/c42149ebe2a1993db6b6937116d0bd01e7b10c3a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>rpm uses non-portable `--system` flag to `useradd`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14211</link><project id="" key="" /><description>If it used the equivalent `-r` flag instead then it would work on older systems, in particular el5.
Otherwise, the `useradd` fails but the install continues, resulting in a broken installation where everything is owned by root.
</description><key id="112386315">14211</key><summary>rpm uses non-portable `--system` flag to `useradd`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">OrangeDog</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-10-20T14:51:45Z</created><updated>2016-06-09T15:24:10Z</updated><resolved>2016-06-09T15:24:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="girirajsharma" created="2015-11-02T17:54:01Z" id="153099401">Makes sense to me. `--system` flag wasn't introduced in CentOS prior to CentOS 6.0 [http://www.freebsd.org/cgi/man.cgi?query=useradd&amp;apropos=0&amp;sektion=8&amp;manpath=CentOS+6.0&amp;arch=default&amp;format=html]
Although, these days almost all linux distributions have the support of both the flags.
May be I can fix this if the use of flag `r` instead of `system` is preferred.
</comment><comment author="jordansissel" created="2016-06-09T04:47:33Z" id="224799045">This was fixed by https://github.com/elastic/elasticsearch/pull/14596
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove cache concurrency level settings that no longer apply</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14210</link><project id="" key="" /><description>This commit removes some cache concurrency level settings that were
applicable when the cache was backed by the Guava cache implementation,
but no longer apply with the cache implementation completed in #13717.

Relates #7836, relates #13224, relates #13717
</description><key id="112378871">14210</key><summary>Remove cache concurrency level settings that no longer apply</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>docs</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T14:16:54Z</created><updated>2015-10-20T15:51:19Z</updated><resolved>2015-10-20T14:31:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-20T14:31:16Z" id="149584962">LGTM.
</comment><comment author="jasontedor" created="2015-10-20T15:51:19Z" id="149611733">Thanks for the review @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ban oal.search.Filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14209</link><project id="" key="" /><description>Filter has been deprecated in Lucene 5.4 and will be removed in 6.0. We should
stop using this API.
</description><key id="112377495">14209</key><summary>Ban oal.search.Filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T14:10:21Z</created><updated>2015-10-20T16:26:11Z</updated><resolved>2015-10-20T14:22:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-20T14:14:43Z" id="149580795">looks great, thanks for nuking this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add geoip processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14208</link><project id="" key="" /><description>Introduces a GeoIP processor. This processor takes in an ip-address and returns city level information
like `city_name`, `lon`/`lat`, `country_name`, etc. 

TODO items:
- finalize a configuration strategy for where to keep the geo database
- figure out security policy around file permissions for the db and GeoLite reflection
- test depends on local access to the `GeoLite2-City.mmdb` database located at `/tmp/geolite2city.mmdb`. an _actual_ testing strategy needs to be implemented.

Currently, The GeoLite API requires the use of reflection and access to a class type which is not 
supported by the security policy:

reason: uses reflection in a nested fashion when serializing the response json object into a `CityResponse`

``` java
CityResponse response = dbReader.city(ipAddress);
```

exception:

```
java.lang.IllegalArgumentException: Can not access public com.maxmind.geoip2.model.CityResponse() (from class com.maxmind.geoip2.model.CityResponse; failed to set access: access denied ("java.lang.reflect.ReflectPermission" "suppressAccessChecks")
```
</description><key id="112371604">14208</key><summary>Add geoip processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-10-20T13:42:01Z</created><updated>2015-11-03T05:31:56Z</updated><resolved>2015-11-03T05:08:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-28T08:02:17Z" id="151756766">@talevy I've updated the PR with the following changes:
- Rebased with the feature/ingest branch and applied plugin security. The ingest plugin will allow that the geoip2 library uses reflection to deserialise the raw data into the geoip2's own domain classes.
- Temporarily added the city data base to the source tree... 
- Custom database files can only be provided via the config directory. In this directory ES has sufficient permissions to read files from. I believe this is the only sane thing we can do for users with custom geoip databases.
- Replaced the Java integration test with a rest integration test. `mvn clean verify` passes now.

Things to do:
- I think we should add another project where we maintain the geoip2 city database. We can then just pull in the database as an dependency in the elasticsearch project. The database is then available for unit tests, integration tests and when the plugin gets bundled without hacking around and adding custom config to maven or the integration test ant xml stuff.
- Add unit tests. 
- Also support the geoip2 country database.
- Each geoip processor instance will have its own instance of the geoip2 database. We should prevent this and I think there should be an service where we keep the database instance around per file.

Questions:
- There is an todo to add a cache for the database lookups. Is this really something we should do? Reading from the code the database file gets memory mapped into memory so it should quick once it is in the filesystem cache. Did you experience any slowness or maybe have measurements? I didn't yet run any tests, but I will do that later on. Maybe just the initial lookups are slow?
- There is a todo to add the ability to filter out fields retrieved from the geoip database. Instead of adding this feature, maybe users should configure the mutate processor to remove the unwanted fields?
</comment><comment author="talevy" created="2015-10-28T16:01:35Z" id="151892845">In response to the Questions:
- That cache todo for database lookups is just a note to explore the effects of caching. You can see comments about the improved performance of an LRU Cache for `logstash-filter-geoip` here: https://github.com/logstash-plugins/logstash-filter-geoip/pull/40.
- I am ok with removing that TODO about adding ability to filter out fields. This is a feature of the logstash filter, so I didn't want that feature to be forgotten when implementing the processor. I agree, it can be taken care of by a mutate.
</comment><comment author="martijnvg" created="2015-10-28T16:35:10Z" id="151902781">&gt; That cache todo for database lookups is just a note to explore the effects of caching.

Right, I see, lets then open an issue to explore this.
</comment><comment author="martijnvg" created="2015-10-29T11:35:47Z" id="152153828">@talevy I moved the database files to an external dependency and the files are now maintained here: https://github.com/elastic/geolite2-databases

The project is already pushed to the maven repository, so it should be available for everyone.

During plugin installation I put the files under `$ES_HOME/config/ingest/geoip/`instead of embedding the default database files into the jar file. The benefit of this is that both custom db files and the default db files will be handled by the same logic (no exceptions in the logic).
</comment><comment author="martijnvg" created="2015-10-30T10:04:36Z" id="152479584">@talevy  I've updated this PR:
- Rebased with the feature/geoip branch and made it work with gradle.
- Removed the Environment usage from the builder and builder factory. The processors shouldn't rely on any ES classes. Add an extra method to the processor builder factory that allows the environment the ingest framework is running in (ES now and later LS) to pass down the configuration directory.
- Added a service that is private to the geoip processor that builds and caches database readers, so that multiple geoip processors use the same database reader.
- Added basic docs and some more tests.

I think it is ready.
</comment><comment author="martijnvg" created="2015-11-03T02:57:21Z" id="153228156">@talevy I've merged feature/ingest branch into this PR, removed the the maven config and added the annotation.
</comment><comment author="talevy" created="2015-11-03T03:32:40Z" id="153231656">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check "plugin already installed" before jar hell check.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14207</link><project id="" key="" /><description>In the case of a plugin using the deprecated `isolated=false` functionality
this will cause confusion otherwise.

Closes #14205
</description><key id="112368461">14207</key><summary>Check "plugin already installed" before jar hell check.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-20T13:25:38Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-20T13:59:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-20T13:48:13Z" id="149573314">LGTM
</comment><comment author="rmuir" created="2015-10-20T13:56:05Z" id="149575548">i hate shoving before a release, but i will say the fix is IMO low risk and the error could cause a lot of confusion. I'll push to 2.1+ for now.
</comment><comment author="nik9000" created="2015-10-20T14:10:44Z" id="149579882">&gt; i hate shoving before a release, but i will say the fix is IMO low risk and the error could cause a lot of confusion. I'll push to 2.1+ for now.

Agreed.
</comment><comment author="jpountz" created="2015-10-20T14:14:28Z" id="149580743">+1
</comment><comment author="clintongormley" created="2015-10-20T14:18:39Z" id="149581733">We'll be testing this manually before releasing 2.0 anyway, and it is a minor change

++ to pushing to 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor ShardFailure listener infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14206</link><project id="" key="" /><description>Today we leak the notion of an engine outside of the shard abstraction
which is not desirable. This commit refactors the infrastrucutre to use
use already existing interfaces to communicate if a shard has failed and
prevents engine private classes to be implemented on a higher level.
This change is purely cosmentical...
</description><key id="112365002">14206</key><summary>Refactor ShardFailure listener infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T13:09:33Z</created><updated>2015-10-21T11:41:00Z</updated><resolved>2015-10-21T11:40:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-20T14:55:08Z" id="149592100">LGTM. Left one minor comment (next to trivial stuff) about null protection in IndicesClusterStateService
</comment><comment author="s1monw" created="2015-10-21T07:46:22Z" id="149807070">pushed a new commit adressing all comments and I also improved the situation if indexShard is null 
</comment><comment author="bleskes" created="2015-10-21T11:03:57Z" id="149856432">LGTM. Thx @s1monw 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin Installation No Longer Checks for Already Installed Plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14205</link><project id="" key="" /><description>When installing a plugin in ES 1.7, there is a check to verify that the plugin doesn't already exist, and the plugin installation aborts if it does: 

```
bin/plugin install elasticsearch/license/latest
-&gt; Installing elasticsearch/license/latest...
Trying http://download.elasticsearch.org/elasticsearch/license/license-latest.zip...
Downloading ..............................DONE
Installed elasticsearch/license/latest into /path/to/elasticsearch-1.7.0/plugins/license

bin/plugin install elasticsearch/license/latest
-&gt; Installing elasticsearch/license/latest...
Failed to install elasticsearch/license/latest, reason: plugin directory /path/to/elasticsearch-1.7.0/plugins/license already exists. To update the plugin, uninstall it first using --remove elasticsearch/license/latest command

```

In 2.0, this check no longer appears to take place, and the plugin installation appears to be attempted and fails: 

```
bin/plugin install license
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.0.0-rc1/license-2.0.0-rc1.zip ...
Downloading ......DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.0.0-rc1/license-2.0.0-rc1.zip checksums if available ...
Downloading .DONE
Installed license into /path/to/elasticsearch-2.0.0-rc1/plugins/license

bin/plugin install license
-&gt; Installing license...
Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.0.0-rc1/license-2.0.0-rc1.zip ...
Downloading ......DONE
Verifying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/license/2.0.0-rc1/license-2.0.0-rc1.zip checksums if available ...
Downloading .DONE
ERROR: java.lang.IllegalStateException: jar hell!
class: org.elasticsearch.license.plugin.rest.RestGetLicenseAction
jar1: /path/to/elasticsearch-2.0.0-rc1/plugins/license/license-2.0.0-rc1.jar
jar2: /var/folders/1r/t56v_5710ygd6klpsbsrmbsr0000gn/T/4690718986021044046/license/license-2.0.0-rc1.jar
```
</description><key id="112359489">14205</key><summary>Plugin Installation No Longer Checks for Already Installed Plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skearns64</reporter><labels><label>:Plugins</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0</label></labels><created>2015-10-20T12:42:33Z</created><updated>2015-11-22T10:18:13Z</updated><resolved>2015-10-20T13:59:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-20T12:53:38Z" id="149557111">Its actually not really trying to install the plugin again, its just that the jar hell check happens before the "already installed check". Thanks @skearns64 
</comment><comment author="rmuir" created="2015-10-20T13:14:08Z" id="149563764">The issue is unique to this license plugin: it hits this problem because it uses the deprecated `isolated=false`. This kind of thing is not really surprising, its why we deprecated it...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove NotQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14204</link><project id="" key="" /><description>The NotQueryBuilder should be deprecated on the 2.x branches
and can be removed with the next major version. It's equivalent is
the booean query mustNot() clause.

Closes #13761
</description><key id="112357306">14204</key><summary>Remove NotQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-20T12:29:04Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-20T17:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-20T13:56:07Z" id="149575563">LGTM can you add a note to the migration guide please? Also open an issue to the [migration project](https://github.com/elastic/elasticsearch-migration) so we don't forget about it?
</comment><comment author="clintongormley" created="2015-10-20T14:03:03Z" id="149577308">The migration plugin doesn't look at queries, so no need to open an issue there
</comment><comment author="cbuescher" created="2015-10-20T14:58:41Z" id="149593193">@javanna added entry to the `migrate_3_0.asciidoc`, or do you think this better belongs with the deprecation into the corresponding `migrate_2_1.asciidoc`?
</comment><comment author="javanna" created="2015-10-20T15:08:57Z" id="149595997">migrate_3_0.asciidoc is fine, thanks! I left a comment.
</comment><comment author="javanna" created="2015-10-20T17:26:12Z" id="149639631">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate NotQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14203</link><project id="" key="" /><description>This PR deprecates the use of NotQueryBuilder like we
did with And- and OrQueryBuilder before. BooleanQueryBuilder 
is now the only way to combine several filters together, so all 
uses of the not filter can be replaced
with a must_not clause.

Relates to #13761 
</description><key id="112338750">14203</key><summary>Deprecate NotQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>deprecation</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-20T10:44:10Z</created><updated>2015-10-20T13:04:00Z</updated><resolved>2015-10-20T12:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-20T12:31:26Z" id="149550577">thanks Christoph, left some minor comments.
</comment><comment author="bleskes" created="2015-10-20T12:49:22Z" id="149555871">LGTM
</comment><comment author="cbuescher" created="2015-10-20T12:49:35Z" id="149555967">Thanks, added reference to the must not clause in the deprecation notes.
</comment><comment author="cbuescher" created="2015-10-20T12:58:48Z" id="149560243">Also on 2.x with 0127af1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve `min_score` implementation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14202</link><project id="" key="" /><description>This changes how `min_score` is implemented both for `function_score` and the
search request parameter to confirm whether the minimum score is met in the
`matches()` phase of a TwoPhaseIterator.
</description><key id="112314245">14202</key><summary>Improve `min_score` implementation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2015-10-20T08:34:32Z</created><updated>2015-10-26T17:42:52Z</updated><resolved>2015-10-26T13:10:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-20T08:54:19Z" id="149482004">I had a look at this and it looks good to me. That said I may not have enough lucene knowledge to review the lucene bits :) 
I think I understand where this is going but maybe we can highlight what this new implementation of min_score brings compare to the previous collector based one?
</comment><comment author="jpountz" created="2015-10-20T09:02:52Z" id="149484754">This PR doesn't change much for the top-level `min_score` parameter, things are still executed in the same order and performance should be about the same. However, this change is important for `function_score` because if you intersect a `function_score` query with a filter, we will now make sure the filter also matches before verifying the score, while we could sometimes verify the score first before, even if the filter does not match.

Eventually I want to deprecate the top-level `min_score` parameter in favour of `function_score` (which is much more flexible as you can filter any sub query and not only the top-level one), so I worked on this pull request first in order to make `function_score` more efficient when `min_score` is set.
</comment><comment author="javanna" created="2015-10-20T09:06:22Z" id="149485982">sounds good thanks a lot for the explanation @jpountz 
</comment><comment author="nik9000" created="2015-10-20T14:09:39Z" id="149579589">Looks good to me. I had to go read the two phase stuff because it was new to me but this seems right.

Should this go back to lucene?
</comment><comment author="jpountz" created="2015-10-20T16:53:39Z" id="149631319">&gt; Should this go back to lucene?

I'm not confident enough that these function_score APIs are good to open an issue against Lucene, so I like to keep on incubating this functionnality in Elasticsearch for now. Lucene already has some queries that allow to modify scores like BoostedQuery or BoostingQuery and I tend to find them a bit hack-ish...
</comment><comment author="nik9000" created="2015-10-23T18:39:31Z" id="150658981">Makes sense to me.
</comment><comment author="jpountz" created="2015-10-26T17:42:48Z" id="151221763">I found some issues with how this change interacts with caching and normalization when backporting, so I reverted it. I think we still need to implement something similar so that scores are checked in a TwoPhaseIterator but I guess this can only be done at the (Bulk)Scorer level, not Query/Weight.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>restructure processors / addField for Data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14201</link><project id="" key="" /><description>This Commit does the following:
- moves processors into their own sub-packages
- adds ability to add any typed field value into Data

the idea is that each processor will have their own utility classes and keeping them 
contained within a single subpackage for that processor will make traversing the 
ingest directory nicer.
</description><key id="112309948">14201</key><summary>restructure processors / addField for Data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label></labels><created>2015-10-20T08:09:53Z</created><updated>2015-10-20T09:05:01Z</updated><resolved>2015-10-20T09:04:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-20T08:45:28Z" id="149480051">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop ability to execute on Solaris</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14200</link><project id="" key="" /><description>This is very simple to do and recommended by `privileges(5)` documentation:

```
Daemons that never need to exec subprocesses should remove the PRIV_PROC_EXEC privilege from their permitted and limit sets.
```
</description><key id="112248925">14200</key><summary>Drop ability to execute on Solaris</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-19T22:49:04Z</created><updated>2015-10-20T13:01:56Z</updated><resolved>2015-10-20T12:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-20T08:22:07Z" id="149474605">I don't have a Solaris machine to try this out, but what I see looks good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Partial update of an index in Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14199</link><project id="" key="" /><description>I have the following mapping with category as an array.

curl -XPUT http://localhost:9200/cstest/ -d '
{
    "mappings": {
        "csproducts" : {
            "properties" : {             

```
              "productid":{
                "type" : "String"               
              },  
              "productnumber":{
                "type" : "String"               
              },        
              "category":{
                "type": "string",
                "null_value" : "N/A"                  
              }

         } //end properties
      } //end csproducts


} //end mapping
```

}
'
Added 1 product to the index.

curl -XPUT 'http://localhost:9200/cstest/csproducts/1' -d '
{
    "productid":"1",
       "productnumber":"123",
    "category":"software"
}'

When I run the update to add another category,
curl -XPOST 'http://localhost:9200/cstest/csproducts/1/_update' -d'
{
   "script" : "ctx._source.category+=new_category",
   "params" : {
      "new_category" : "engineering"
   }
}'

Category is not indexed as an array of categories for that product. 
What is the command to run the partial updates?

H
curl -XGET 'http://localhost:9200/cstest/csproducts/1?pretty=true'
{
"_index" : "cstest",
"_type" : "csproducts",
"_id" : "1",
"_version" : 4,
"found" : true, "_source" : {"productid":"1","productnumber":"123200","category":"softwaredevelopment"}
}
</description><key id="112236288">14199</key><summary>Partial update of an index in Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malinir123</reporter><labels /><created>2015-10-19T21:24:54Z</created><updated>2015-10-20T12:42:53Z</updated><resolved>2015-10-20T12:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T12:42:52Z" id="149552953">Hi @malinir123 

Please ask questions like these in the forum http://discuss.elastic.co/

If `category` is a string, the `+` operator is going to perform an append operation.  You'd need to make `category` an array in the original document, or detect this case in your script.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch return same score for documents with different freq count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14198</link><project id="" key="" /><description>In the result of one of my searches, Elasticsearch gave two documents the same score, even though one doc has the word appear 3 times vs 12 times in the other doc.  I understand term frequency is only one part of the score calculation, but i don't understand how two docs with such different term frequency got identical score.  I know scores are only relative to the query, for my search, i'm hitting one index, one shard.

Document 1:
{
    "_shard" : 1,
    "_node" : "_BEa2e4ERfaoImOTCR5XMg",
    "_index" : "my_test_index",
    "_type" : "0000000003-nid",
    "_id" : "0001262463-15-000642-1",
    "_score" : 0.020258121, "_source" : {"story_datetime":"20151016T105524"},
    "sort" : [ 1444992924000 ],
    "_explanation" : {
        "value" : 0.020258121,
        "description" : "max of:",
        "details" : [ {
            "value" : 0.020258121,
            "description" : "weight(body:profit in 282422) [PerFieldSimilarity], result of:",
            "details" : [ {
                "value" : 0.020258121,
                "description" : "score(doc=282422,freq=3.0 = termFreq=3.0\n), product of:",
                "details" : [ {
                    "value" : 0.29820275,
                    "description" : "queryWeight, product of:",
                    "details" : [ {
                        "value" : 5.0203834,
                        "description" : "idf(docFreq=135277, maxDocs=7538025)"
                    }, {
                        "value" : 0.0593984,
                        "description" : "queryNorm"
                    } ]
                }, {
                    "value" : 0.06793405,
                    "description" : "fieldWeight in 282422, product of:",
                    "details" : [ {
                        "value" : 1.7320508,
                        "description" : "tf(freq=3.0), with freq of:",
                        "details" : [ {
                            "value" : 3.0,
                            "description" : "termFreq=3.0"
                        } ]
                    }, {
                        "value" : 5.0203834,
                        "description" : "idf(docFreq=135277, maxDocs=7538025)"
                    }, {
                        "value" : 0.0078125,
                        "description" : "fieldNorm(doc=282422)"
                    } ]
                } ]
            } ]
        } ]
    }
}

Document 2:
{
    "_shard" : 1,
    "_node" : "_BEa2e4ERfaoImOTCR5XMg",
    "_index" : "my_test_index",
    "_type" : "my_test_doc",
    "_id" : "0001413507-15-000005-1",
    "_score" : 0.020258121, "_source" : {"story_datetime":"20151016T060653"},
    "sort" : [ 1444975613000 ],
    "_explanation" : {
        "value" : 0.020258121,
        "description" : "max of:",
        "details" : [ {
            "value" : 0.020258121,
            "description" : "weight(body:profit in 463036) [PerFieldSimilarity], result of:",
            "details" : [ {
                "value" : 0.020258121,
                "description" : "score(doc=463036,freq=12.0 = termFreq=12.0\n), product of:",
                "details" : [ {
                    "value" : 0.29820275,
                    "description" : "queryWeight, product of:",
                    "details" : [ {
                        "value" : 5.0203834,
                        "description" : "idf(docFreq=135277, maxDocs=7538025)"
                    }, {
                        "value" : 0.0593984,
                        "description" : "queryNorm"
                    } ]
                }, {
                    "value" : 0.06793405,
                    "description" : "fieldWeight in 463036, product of:",
                    "details" : [ {
                        "value" : 3.4641016,
                        "description" : "tf(freq=12.0), with freq of:",
                        "details" : [ {
                            "value" : 12.0,
                            "description" : "termFreq=12.0"
                        } ]
                    }, {
                        "value" : 5.0203834,
                        "description" : "idf(docFreq=135277, maxDocs=7538025)"
                    }, {
                        "value" : 0.00390625,
                        "description" : "fieldNorm(doc=463036)"
                    } ]
                } ]
            } ]
        } ]
    }
}
</description><key id="112202533">14198</key><summary>Elasticsearch return same score for documents with different freq count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lifo888</reporter><labels /><created>2015-10-19T18:16:44Z</created><updated>2015-10-20T09:13:35Z</updated><resolved>2015-10-20T09:13:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-20T09:13:35Z" id="149487784">This is expected: doc 1 has a factor of 1.7320508 thanks to its doc freq and 0.0078125 because of its length, and doc 2 has a contribution of 3.4641016 for the freq and 0.00390625 for its length. When you multiply contributions, you get 0.0135 in both cases.

So in short, doc 2 is not more relevant because it's longer than doc 1 and gets penalized because of that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add elasticsearch-sql to list of community plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14197</link><project id="" key="" /><description>Thanks!
</description><key id="112198658">14197</key><summary>Add elasticsearch-sql to list of community plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eliranmoyal</reporter><labels /><created>2015-10-19T17:54:04Z</created><updated>2015-10-20T12:38:48Z</updated><resolved>2015-10-20T12:38:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T12:38:48Z" id="149552147">Hi @eliranmoyal 

Thanks for the PR, but this plugin is already listed: https://www.elastic.co/guide/en/elasticsearch/plugins/master/api.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 502s when </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14196</link><project id="" key="" /><description>I was able to complete this tutorial successfully: https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-4-on-centos-71 which let me set up an ELK stack on one box. Now, I'm trying to set up ES / Kibana / nginx on one box and ES / Logstash on another.

On server A, with minimal config (naming the ES cluster, making the ES network.host the server IP, and setting the ES url and host to the server IP), things work. But, when I try to follow this tutorial:
 https://www.elastic.co/guide/en/kibana/current/production.html to load balance across multiple nodes, I get a 502 when changing things to node.data = false OR node.master = false as instructed from curling IP:9200 instead of 200 before the change.

I also don't have much to go on in way of errors. I've tried preloading ES on server A with test data, because I noticed that without some data in ES, kibana4 502s.  Other than that,  I'm not really sure of what else to share.
</description><key id="112158335">14196</key><summary>Elasticsearch 502s when </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshusre</reporter><labels /><created>2015-10-19T14:36:10Z</created><updated>2015-10-20T11:03:59Z</updated><resolved>2015-10-20T05:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-10-20T05:18:35Z" id="149435339">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment><comment author="joshusre" created="2015-10-20T11:03:59Z" id="149526165">I'll pass.  Good talk, bro :\

On Tue, Oct 20, 2015 at 12:19 AM, markwalkom notifications@github.com
wrote:

&gt; Please join us in #elasticsearch on Freenode or at
&gt; https://discuss.elastic.co/ for troubleshooting help or general
&gt; questions.
&gt; 
&gt; We reserve Github for confirmed bugs and feature requests :)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14196#issuecomment-149435339
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cat API uses Content-Type to determine response type </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14195</link><project id="" key="" /><description>Right now _cat API uses Content-Type header to determine response type, instead of Accept header. Therefore GET request should contain Content-Type, but it cannot contain any content, because, well, it's GET request =) I think using Accept header would be much more consistent with http standard.

Example:

```
GET /_cat/nodes HTTP/1.1
Accept: application/json
Content-Type: application/json

HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
```

but

```
GET /_cat/nodes HTTP/1.1
Accept: application/json

HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
```
</description><key id="112145824">14195</key><summary>Cat API uses Content-Type to determine response type </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">crassirostris</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-10-19T13:36:46Z</created><updated>2016-09-23T17:22:17Z</updated><resolved>2016-09-23T17:22:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T12:03:45Z" id="149543428">I had absolutely no idea that this worked!  And I agree that it should be an Accept header instead of Content-Type.

Btw, you can also just add '?format=json'  to achieve the same thing.
</comment><comment author="clintongormley" created="2016-09-23T17:22:17Z" id="249251838">Closed by https://github.com/elastic/elasticsearch/pull/14421
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Low Disk Watermark Logging Oddness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14194</link><project id="" key="" /><description>Using a 2.0.0-snapshot build, I see odd logging behavior for the low disk watermark. 

The free-disk calculation is working fine, but the logging for this is quite odd. After starting a single-node cluster with 5 shards (each with 1 shard, and 1 unassigned replica), I see several log messages about low disk watermark all at once. 30 seconds later, I see another group of low disk watermark messages, but each time there are more messages than previously. First time was 4 identical log lines, then 7, then 10, then 13.

I was able to reproduce several times after restarting the nodes. I inserted newlines into the excerpted logs below. This could become a real problem if the trend continues. 

```
[2015-10-19 09:21:12,202][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:12,202][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:12,203][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:12,203][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node


[2015-10-19 09:21:42,201][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:42,201][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:42,201][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:42,201][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:42,202][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:42,202][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:21:42,202][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node


[2015-10-19 09:22:12,206][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,207][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,207][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,207][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,207][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,207][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,207][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,208][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,208][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:12,208][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node


[2015-10-19 09:22:42,211][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,211][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,211][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,212][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:22:42,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node


[2015-10-19 09:23:12,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,213][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,214][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,214][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,214][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,214][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,214][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,214][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,215][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,215][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,215][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,215][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,215][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:12,215][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.3gb[14.7%], replicas will not be assigned to this node


[2015-10-19 09:23:42,217][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,217][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,217][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,217][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,217][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,218][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,218][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,218][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,218][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,218][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,218][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,219][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
[2015-10-19 09:23:42,220][INFO ][cluster.routing.allocation.decider] [Solara] low disk watermark [85%] exceeded on [XpVtZKlVQFu80lnBEGdh9A][Solara][/skearns/es/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 34.4gb[14.7%], replicas will not be assigned to this node
```
</description><key id="112145255">14194</key><summary>Low Disk Watermark Logging Oddness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">skearns64</reporter><labels><label>:Allocation</label><label>:Logging</label><label>bug</label></labels><created>2015-10-19T13:33:34Z</created><updated>2015-10-21T20:58:26Z</updated><resolved>2015-10-21T20:58:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:52:13Z" id="149538392">@dakrone could you take a look at this please?
</comment><comment author="dakrone" created="2015-10-20T12:38:26Z" id="149552089">@skearns64 When I try to reproduce this, I am getting only a single logging message (extra spacing added by me):

```
~/scratch/elasticsearch-2.0.0-SNAPSHOT &#955; bin/elasticsearch
[2015-10-20 06:33:54,293][INFO ][node                     ] [Aftershock] version[2.0.0-SNAPSHOT], pid[4608], build[e1a7cc2/2015-10-20T12:29:09Z]
[2015-10-20 06:33:54,293][INFO ][node                     ] [Aftershock] initializing ...
[2015-10-20 06:33:54,355][INFO ][plugins                  ] [Aftershock] loaded [], sites []
[2015-10-20 06:33:54,441][INFO ][env                      ] [Aftershock] using [1] data paths, mounts [[/home (/dev/mapper/fedora_thulcandra-home)]], net usable_space [270.9gb], net total_space [396.1gb], spins? [no], types [ext4]
[2015-10-20 06:33:55,850][INFO ][node                     ] [Aftershock] initialized
[2015-10-20 06:33:55,851][INFO ][node                     ] [Aftershock] starting ...
[2015-10-20 06:33:55,976][INFO ][transport                ] [Aftershock] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2015-10-20 06:33:55,985][INFO ][discovery                ] [Aftershock] elasticsearch/4jyUZlbuQJik6bNqb7bxuA
[2015-10-20 06:33:59,021][INFO ][cluster.service          ] [Aftershock] new_master {Aftershock}{4jyUZlbuQJik6bNqb7bxuA}{127.0.0.1}{127.0.0.1:9300}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-10-20 06:33:59,052][INFO ][http                     ] [Aftershock] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200}
[2015-10-20 06:33:59,053][INFO ][node                     ] [Aftershock] started
[2015-10-20 06:33:59,072][INFO ][gateway                  ] [Aftershock] recovered [0] indices into cluster_state
[2015-10-20 06:34:02,803][INFO ][cluster.metadata         ] [Aftershock] [test] creating index, cause [api], templates [], shards [5]/[1], mappings []
[2015-10-20 06:34:29,033][INFO ][cluster.routing.allocation.decider] [Aftershock] low disk watermark [25%] exceeded on [4jyUZlbuQJik6bNqb7bxuA][Aftershock][/home/hinmanm/scratch/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 270.9gb[68.3%], replicas will not be assigned to this node

[2015-10-20 06:34:59,028][INFO ][cluster.routing.allocation.decider] [Aftershock] low disk watermark [25%] exceeded on [4jyUZlbuQJik6bNqb7bxuA][Aftershock][/home/hinmanm/scratch/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 270.9gb[68.3%], replicas will not be assigned to this node

[2015-10-20 06:35:29,028][INFO ][cluster.routing.allocation.decider] [Aftershock] low disk watermark [25%] exceeded on [4jyUZlbuQJik6bNqb7bxuA][Aftershock][/home/hinmanm/scratch/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 270.9gb[68.3%], replicas will not be assigned to this node

[2015-10-20 06:35:59,027][INFO ][cluster.routing.allocation.decider] [Aftershock] low disk watermark [25%] exceeded on [4jyUZlbuQJik6bNqb7bxuA][Aftershock][/home/hinmanm/scratch/elasticsearch-2.0.0-SNAPSHOT/data/elasticsearch/nodes/0] free: 270.9gb[68.3%], replicas will not be assigned to this node
```

You mentioned using a 2.0.0-snapshot build, what SHA are you building? I am building from the 2.0 branch, e1a7cc2.
</comment><comment author="dakrone" created="2015-10-21T19:53:17Z" id="150007165">More info on this. The multiple logging lines is caused by `DiskThresholdDecider` not being bound as a singleton. Usually this wouldn't cause any side effects for Elasticsearch, however, Marvel injects the DTD into its `NodesStatsCollecter` with a `Provider&lt;...&gt;`, which was creating a new instance of `DiskThresholdDecider` every time it collected the stats. The additional instances were then logging, which is why the logging increased in number every time it ran.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update surefire to 2.19, checkstyle to 2.17, clean to 3.0.0, shade to 2.4.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14193</link><project id="" key="" /><description># Release notes for maven-surefire-plugin 2.19:
- new parser of test patterns, let's call it Test Filter API, related to
  parameters: test, ex/includes, ex/includesFile;
- a feature to interrupting the test-set after exceedded certain number of
  errors/failures
- new Doxia Version
- anchoring test class names
- shutdown operations
- command based communication between in-plugin and forked process
- improvements in JUnit and TestNG runners
- etc.

See http://www.mail-archive.com/announce@maven.apache.org/msg00710.html for details.
# Release notes for maven-checkstyle-plugin 2.17:
## Bug
- [MCHECKSTYLE-302] - Using inline configuration does not work with Maven 2.2.1
- [MCHECKSTYLE-304] - Using inline configuration, checkstyle-checker.xml is generated using DTD v1.2
- [MCHECKSTYLE-310] - Parrallel build failing with various errors
- [MCHECKSTYLE-311] - "mvn clean site -Preporting" fails with Could not find resource 'config/maven_checks.xml'
## Improvement
- [MCHECKSTYLE-291] - Change format of violation message
- [MCHECKSTYLE-293] - Update to use non deprecated method Checker.setClassLoader()
## Task
- [MCHECKSTYLE-307] - Upgrade to Checkstyle 6.11
- [MCHECKSTYLE-313] - Upgrade to Checkstyle 6.11.2
# Release Notes - Apache Maven Clean Plugin  Version 3.0.0

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317224&amp;version=12330417
## Improvements:
- [MCLEAN-56] - Make Plugin only 3.X compatible - get rid of Maven 2.
- [MCLEAN-62] - Upgrade to maven-plugins parent version 27
- [MCLEAN-63] - Make naming of properties consistent
- [MCLEAN-65] - Bump version to 3.0.0
- [MCLEAN-66] - Upgrade maven-shared-utils to 0.9
- [MCLEAN-67] - Change package name to org.apache.maven.plugins
- [MCLEAN-69] - Upgrade maven-shared-utils to 3.0.0
# Release Notes - Apache Maven Shade Plugin  Version 2.4.2

https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12317921&amp;version=12333008
## Bugs:
- [MSHADE-172] - "java.lang.ArithmeticException: / by zero" in MinijarFilter
- [MSHADE-190] - Shade does not relocate the contents of META-INF/services files
- [MSHADE-209] - [REGRESSION] "java.lang.ArithmeticException: / by zero" in MinijarFilter (reporter Jon McLean).
## Improvements:
- [MSHADE-205] - Better use of ClazzpathUnit for improved jar minimization (contribution of Benoit Perrot).
- [MSHADE-207] - Replace wrong link to codehaus with correct location
- [MSHADE-210] - Upgrade maven-plugins parent to version 28.
- [MSHADE-211] - Keep Java 1.5
</description><key id="112140275">14193</key><summary>Update surefire to 2.19, checkstyle to 2.17, clean to 3.0.0, shade to 2.4.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label></labels><created>2015-10-19T13:06:14Z</created><updated>2015-11-19T14:01:57Z</updated><resolved>2015-11-19T13:36:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-23T08:31:33Z" id="150511530">I added a new commit for maven clean plugin 3.0.0
</comment><comment author="dadoonet" created="2015-11-19T13:36:47Z" id="158058552">Closing as there is no need to merge that in master now.
PR #14861 opened for 2.x branch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename QueryBuilder objects as they are not builders </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14192</link><project id="" key="" /><description>With the query refactoring we refactored all of our query objects. As a result, they are not builders anymore although we kept the `Builder` suffix to avoid making too many changes at the same time and dealing with merge conflicts. Now that all queries are refactored and everything is in master, it is time to settle on new naming and rename them all. One tricky aspect could be that removing the `Builder` suffix would result in many cases to the same class name as the corresponding lucene query, which sits though in another package. Shall we just remove the suffix or can anybody come up with some better name convention for our own query objects? :)
</description><key id="112121466">14192</key><summary>Rename QueryBuilder objects as they are not builders </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v6.0.0</label></labels><created>2015-10-19T10:55:31Z</created><updated>2017-05-05T15:02:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-19T11:52:52Z" id="149192761">oooh fun! I am all for it but what are we doing with things like `TermQuery` it will have name clashes? I know it's a pain but I already spend a lot of time on this without a good solution so lets see if other can come up with something good?
</comment><comment author="s1monw" created="2015-10-19T11:53:30Z" id="149192841">We can come up with better names ie `KeywordQuery` ? 
</comment><comment author="javanna" created="2015-10-19T12:04:13Z" id="149195958">Quick question, given that the package names would differ, what would be the actual problems if we have name clashes with lucene in practice? 
</comment><comment author="MaineC" created="2015-10-20T08:49:05Z" id="149480939">Simple things first: big +1 on removing the builder suffix - no matter what the final names will look like, these classes aren't builders anymore so they shouldn't be named builders.

As for final names (aka here are my bikeshedding arguments): Just dropping the suffix means that users who are already familiar with our Java API have an easier time switching. Also current names being close to the original Lucene names means that knowing Lucene already helps when adopting Elasticsearch (not sure though whether those knowing Lucene already is still the majority of newcomers to Elasticsearch). Only downside: You have to look closer which class you open in your IDE from a search to not open the wrong one (For me personally that isn't much of a problem, might be different for others). 

Switching to different names looses the link to the original Lucene query - this can be both good and bad: Might make things clearer (after tons of arguments on which name to select), it potentially gives us an option to switch the underlying implementation without confusing users, might make things more complicated for those trying to apply their Lucene knowledge to Elasticsearch.

IMHO: I'd stick with the current names w/o the builder suffix.
</comment><comment author="clintongormley" created="2016-05-03T12:39:46Z" id="216514963">Is this issue now resolved?
</comment><comment author="javanna" created="2016-05-03T13:17:11Z" id="216523954">not resolved, needs to be done.
</comment><comment author="javanna" created="2016-09-08T10:41:45Z" id="245560036">I wonder if this should be done for 6.0 rather than 5.0. That way we could stay with what we have in 5.0 and cause less upgrade pain to java api users. We plan to make java api an internal only thing in 6.0 so renaming is going to be much less problematic at that point. I will label 6.0.0 rather than 5.0.0
</comment><comment author="nik9000" created="2016-09-08T11:01:09Z" id="245563947">I'm comfortable waiting until 6 for this.

On Sep 8, 2016 6:41 AM, "Luca Cavanna" notifications@github.com wrote:

&gt; I wonder if this should be doen for 6.0. That way we could stay with what
&gt; we have in 5.0 and cause less upgrade pain to java api users. We plan to
&gt; make java api an internal only thing in 6.0 so renaming is going to be much
&gt; less problematic at that point. I will label 6.0.0 rather than 5.0.0
&gt; 
&gt; &#8212;
&gt; You are receiving this because you are subscribed to this thread.
&gt; Reply to this email directly, view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14192#issuecomment-245560036,
&gt; or mute the thread
&gt; https://github.com/notifications/unsubscribe-auth/AANLooiPcV03w6JfJnRyLxibNEedFW0kks5qn-ZugaJpZM4GRPvY
&gt; .
</comment><comment author="MaineC" created="2016-09-12T10:29:04Z" id="246308647">Sounds like a good plan to me as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document breaking changes introduced with #13752</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14191</link><project id="" key="" /><description>With #13752 we introduced quite some breaking changes to the java api. We should review what we did and properly document those breaking changes in the migration guide for 3.0. Also, we should consider deprecating in 2.x some the methods that we have removed from `SearchRequest` and `SearchRequestBuilder` etc.
</description><key id="112119280">14191</key><summary>Document breaking changes introduced with #13752</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-10-19T10:41:23Z</created><updated>2016-03-22T15:58:40Z</updated><resolved>2016-03-22T15:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-31T08:24:16Z" id="152714057">Also #14384 and #14270 made the same changes to `ExplainRequest`, `ExplainRequestBuilder`, `ValidateQuery` and `ValidateQueryRequestBuilder`. These should be documented the same way as we do it for the search request.
</comment><comment author="clintongormley" created="2016-01-29T18:36:38Z" id="176901431">@javanna / @colings86 has this been done yet?
</comment><comment author="colings86" created="2016-01-29T18:39:23Z" id="176902098">Not yet no. There are still some small changes being made to the search request so I was waiting for it to settle more. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java api: revise SearchRequest and SearchRequestBuilder object structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14190</link><project id="" key="" /><description>Once we are done with the search refactoring, we should revise the `SearchRequest` object structure, since it has a few problems:
- there are multiple ways to do the same thing e.g. `client.prepareSearch().addSort(sort)` and `client.prepareSearch().setSource(new SearchSourceBuilder().sort(sort)`. Some arguments can be provided directly to the builder, but they become part of the inner source object. 
- confusing behaviour (especially after #13859) e.g. in `client.prepareSearch().addSort(sort).setSource(source)` the setSource wipes the sort criteria that was added in the first place.

After #13859 we parse queries on the coordinating node, but the rest of the `SearchSourceBuilder` is still streamed as json bytes array to the data nodes and parsed once per data node. Once we have refactored all of the search sections (listed in #10217) all of the parsing will happen on the coordinating node, and it will be the right time to revise the `SearchRequest` object structure. It might very well be that the `SearchSourceBuilder` object is not needed anymore and all of its getters/setters can be collapsed to the `SearchRequest`/`SearchRequestBuilder`. Anyways we should have a single consistent way of settings things to the `SearchRequestBuilder` rather than the confusing multiple options that we have now.
</description><key id="112117098">14190</key><summary>Java api: revise SearchRequest and SearchRequestBuilder object structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search Refactoring</label><label>enhancement</label></labels><created>2015-10-19T10:28:40Z</created><updated>2017-07-24T16:25:42Z</updated><resolved>2017-07-24T16:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T12:37:37Z" id="216514531">@javanna @cbuescher is this still relevant?
</comment><comment author="javanna" created="2016-05-03T13:16:46Z" id="216523836">yes still needs to be done.
</comment><comment author="javanna" created="2016-09-08T10:24:28Z" id="245556565">I am looking into this. I would love to remove all of the `SearchRequestBuilder` setter methods that end up setting stuff to the `SearchSourceBuilder` object, which should rather be set explicitly using `setSource` only and using `SearchSourceBuilder` directly to provide query, size etc.

This would remove all the inconsistencies mentioned in this issue, but it turns out to be quite a change for java api users, breaking as most people are using `client.prepareSearch().setQuery(query)` which would have to become `client.prepareSearch().setSource(new SearchSourceBuilder().query(query))`.  I don't think we should break every single java application at the moment. Also once transport client is going to be deprecated, this will not be a problem anymore. What do people think?
</comment><comment author="javanna" created="2017-07-24T16:25:26Z" id="317476916">I am closing this, considering that the high level client only supports requests, not request builders. These inconsistencies will go away once transport client is removed, and request builders are as well.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updates java query dsl documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14189</link><project id="" key="" /><description>This updates the query dsl java documentation to be in line with the current code after query refactoring is through. It also adds a java unit test to reflect the examples in the documentation so developers are pointed to updating the docs when changing the Api.

@clintongormley no idea how feasible it would be to reference real examples in the code base from within our docs to keep the examples in the build cycle...

@colings86 @cbuescher @javanna would be great if you could have another look over the queries you touched before this goes in.

Closes #13800 
</description><key id="112116331">14189</key><summary>Updates java query dsl documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-10-19T10:23:23Z</created><updated>2015-10-20T09:05:18Z</updated><resolved>2015-10-20T08:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-10-19T14:55:06Z" id="149239135">@MaineC left a few very minor comment, otherwise looks good.
</comment><comment author="MaineC" created="2015-10-20T07:48:14Z" id="149465856">Updated. Thanks for your catch wrt. to fuzzy query in particular.

One question: While going over the documentation I realised that it's (currently rightly so) still talking about QueryBuilders. At some point in time we wanted to switch away from this naming scheme. Do we have a plan on which names to switch to when yet?
</comment><comment author="javanna" created="2015-10-20T07:55:43Z" id="149467186">LGTM

@MaineC as for naming, see #14192.
</comment><comment author="dadoonet" created="2015-10-20T07:55:49Z" id="149467203">@MaineC I started to write some "documentation" tests here: https://github.com/elastic/elasticsearch/blob/master/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java

I think it could make sense to have that in one single place.

Also, I started to "annotate" them with "Maven like" comments for future inclusion. See https://github.com/elastic/elasticsearch/blob/master/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java#L54

I saw recently a talk and that might be easier to do than what I initially thought: https://github.com/elastic/docs/issues/4

Hope this could help.
</comment><comment author="MaineC" created="2015-10-20T09:05:18Z" id="149485768">@dadoonet Thanks for your comments. I merged the documentation update for now - shall we open a separate issue that tracks how we want to keep documentation snippets under test*? Seems like both of us had a similar idea/problem there.

&gt; I think it could make sense to have that in one single place.

I'm not quite decided here due to past bad experience: I've been burnt by a project that kept all integration tests in one place separate from the actual code under test. The result was that after a few months of development no-one remembered which test tests which components so when making changes to the codebase there could be random breakages anywhere in the test package. Not saying this is what's going to happen for us just this is where my fear of "move all tests in one place" or "move tests further from the code they test" comes from.

&gt; Also, I started to "annotate" them with "Maven like" comments for future inclusion.

This is great - going through the copy-paste process just for the Java API stuff already was painful enough.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch shutdown randowmly after a while when RejectedExecutionException occured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14188</link><project id="" key="" /><description>ElasticSearch shutdown randowmly after a while.

This is the log of the elasticsearch (1.7.2) set in debug. Tell me if you want to active more specific logs. I don't try to shutdown the elasticsearch at this moment.

[2015-10-19 10:36:30,024][DEBUG][index.merge.scheduler    ] [Baphomet] [logstash-2015.10.19][3] merge segment [_6dd] done: took [27.5s], [285.6 MB], [644,636 docs]
[2015-10-19 10:45:51,459][INFO ][node                     ] [Baphomet] stopping ...
[2015-10-19 10:45:51,609][WARN ][netty.channel.DefaultChannelPipeline] An exception was thrown by an exception handler.
java.util.concurrent.RejectedExecutionException: Worker has already been shutdown
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.registerTask(AbstractNioSelector.java:120)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:72)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.executeInIoThread(AbstractNioWorker.java:56)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.executeInIoThread(NioWorker.java:36)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannelSink.execute(AbstractNioChannelSink.java:34)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.execute(DefaultChannelPipeline.java:636)
        at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaughtLater(Channels.java:496)
        at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:46)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:781)
        at org.elasticsearch.common.netty.channel.Channels.write(Channels.java:725)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:71)
        at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
        at org.elasticsearch.http.netty.pipelining.HttpPipeliningHandler.handleDownstream(HttpPipeliningHandler.java:87)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
        at org.elasticsearch.http.netty.NettyHttpChannel.sendResponse(NettyHttpChannel.java:195)
        at org.elasticsearch.rest.action.support.RestResponseListener.processResponse(RestResponseListener.java:43)
        at org.elasticsearch.rest.action.support.RestActionListener.onResponse(RestActionListener.java:49)
        at org.elasticsearch.action.bulk.TransportBulkAction$2.finishHim(TransportBulkAction.java:360)
        at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:330)
        at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:323)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicationPhase.doFinish(TransportShardReplicationOperationAction.java:963)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicationPhase.doRun(TransportShardReplicationOperationAction.java:831)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.finishAndMoveToReplication(TransportShardReplicationOperationAction.java:525)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:603)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:440)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
[2015-10-19 10:45:52,113][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.09] closing ... (reason [shutdown])
[2015-10-19 10:45:52,113][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.09] closing index service (reason [shutdown])
[2015-10-19 10:45:52,132][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.14] closing ... (reason [shutdown])
[2015-10-19 10:45:52,132][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.14] closing index service (reason [shutdown])
[2015-10-19 10:45:52,137][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.15] closing ... (reason [shutdown])
[2015-10-19 10:45:52,137][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.15] closing index service (reason [shutdown])
[2015-10-19 10:45:52,143][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.12] closing ... (reason [shutdown])
[2015-10-19 10:45:52,143][DEBUG][indices                  ] [Baphomet] [logstash-2015.10.12] closing index service (reason [shutdown])
.... 
</description><key id="112110917">14188</key><summary>ElasticSearch shutdown randowmly after a while when RejectedExecutionException occured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexandrenavarro</reporter><labels /><created>2015-10-19T09:48:41Z</created><updated>2016-03-21T06:28:02Z</updated><resolved>2015-10-19T11:07:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sazary" created="2015-11-23T17:35:31Z" id="159005742">What was the reason?
</comment><comment author="Gerboa" created="2015-11-24T15:50:31Z" id="159310467">Also getting this happen. What was the cause?
</comment><comment author="matteomelani" created="2015-12-10T21:03:36Z" id="163748609">+1. Any clues?
</comment><comment author="awaistune" created="2016-03-21T06:26:09Z" id="199145133">Check your memory (heap / non heap). This happened mostly due to JVM memory usage 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to get apache-flume-1.6.0 to work with ElasticSearch 2.0 beta 2 - Works with Elasticsearch 1.7.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14187</link><project id="" key="" /><description>Hello
I am unable to get apache-flume-1.6.0 to work with ElasticSearch 2.0 beta 2.
The same configuration works fine with Elasticsearch 1.7.3

My flume-en.sh contains
# Give Flume more memory and pre-allocate, enable remote monitoring via JMX

export JAVA_OPTS="-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote"
# Note that the Flume conf directory is always included in the classpath.

FLUME_CLASSPATH="/usr/share/elasticsearch/lib/elasticsearch-2.0.0-beta2.jar"
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.85.x86_64/jre

I have copied the  lucene-core-5.2.1.jar from /usr/share/elasticsearch/lib to /opt/flume/lib

my elastic.conf file is:
elastic.conf
# elastic.conf: A single-node Flume configuration
# Reading avro files via spool dir and use the elastic sink
# Name the components on this agent

a1.sources = avrofld
a1.sinks = k1
a1.channels = c1
# Describe/configure the folder with the avro files from Warehouse Connector

a1.sources.avrofld.type = spooldir
a1.sources.avrofld.channels = c1
a1.sources.avrofld.spoolDir = /var/export
a1.sources.avrofld.fileHeader = true
a1.sources.avrofld.deserializer = avro
# Describe the elasticsearch sink

a1.sinks.k1.type = org.apache.flume.sink.elasticsearch.ElasticSearchSink
a1.sinks.k1.hostNames = 127.0.0.1:9300
a1.sinks.k1.indexName = sa
a1.sinks.k1.indexType = data
a1.sinks.k1.clusterName = elasticsearch
a1.sinks.k1.batchSize = 1000
a1.sinks.k1.ttl = 2d
a1.sinks.k1.serializer = com.rsa.flume.serialization.FlumeAvroEventDeserializer
# Use a channel which buffers events in memory

a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000000
a1.channels.c1.transactionCapacity = 100000
# Bind the source and sink to the channel

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

Elasticsearch 2.0 is successfully started and listening on the port

[root@CentosB lib]# netstat -na |grep 9_00
tcp        0      0 127.0.0.1:9200              0.0.0.0:_                   LISTEN
tcp        0      0 127.0.0.1:9300              0.0.0.0:\*                   LISTEN

However when starting my flume I get the following error
[root@CentosB flume]#  bin/flume-ng agent --conf conf --conf-file conf/elastic.conf --name a1 -Dflume.root.logger=DEBUG,console
2015-10-19 10:24:31,152 (lifecycleSupervisor-1-1) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@5c5dc0a5 counterGroup:{ name:null counters:{} } } - Exception follows.
java.lang.NoSuchMethodError: org.elasticsearch.common.transport.InetSocketTransportAddress.&lt;init&gt;(Ljava/lang/String;I)V
        at org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.configureHostnames(ElasticSearchTransportClient.java:143)
        at org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.&lt;init&gt;(ElasticSearchTransportClient.java:77)
        at org.apache.flume.sink.elasticsearch.client.ElasticSearchClientFactory.getClient(ElasticSearchClientFactory.java:48)
        at org.apache.flume.sink.elasticsearch.ElasticSearchSink.start(ElasticSearchSink.java:357)
        at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)
        at org.apache.flume.SinkRunner.start(SinkRunner.java:79)
        at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Full Log Below
Info: Sourcing environment configuration script /opt/flume/conf/flume-env.sh
Info: Including Hive libraries found via () for Hive access
- exec /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.85.x86_64/jre/bin/java -Xms100m -Xmx2000m -Dcom.sun.management.jmxremote -Dflume.root.logger=DEBUG,console -cp '/opt/flume/conf:/opt/flume/lib/_:/usr/share/elasticsearch/lib/elasticsearch-2.0.0-beta2.jar:/lib/_' -Djava.library.path= org.apache.flume.node.Application --conf-file conf/elastic.conf --name a1
  2015-10-19 10:24:31,017 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:61)] Configuration provider starting
  2015-10-19 10:24:31,020 (lifecycleSupervisor-1-0) [DEBUG - org.apache.flume.node.PollingPropertiesFileConfigurationProvider.start(PollingPropertiesFileConfigurationProvider.java:78)] Configuration provider started
  2015-10-19 10:24:31,023 (conf-file-poller-0) [DEBUG - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:126)] Checking file:conf/elastic.conf for changes
  2015-10-19 10:24:31,023 (conf-file-poller-0) [INFO - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:133)] Reloading configuration file:conf/elastic.conf
  2015-10-19 10:24:31,028 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,028 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1021)] Created context for k1: serializer
  2015-10-19 10:24:31,029 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,029 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:931)] Added sinks: k1 Agent: a1
  2015-10-19 10:24:31,029 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,029 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,029 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,029 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,030 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,030 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,030 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.addProperty(FlumeConfiguration.java:1017)] Processing:k1
  2015-10-19 10:24:31,030 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.isValid(FlumeConfiguration.java:314)] Starting validation of configuration for agent: a1, initial-configuration: AgentConfiguration[a1]
  SOURCES: {avrofld={ parameters:{fileHeader=true, channels=c1, spoolDir=/var/export, type=spooldir, deserializer=avro} }, r1={ parameters:{channels=c1} }}
  CHANNELS: {c1={ parameters:{transactionCapacity=100000, capacity=1000000, type=memory} }}
  SINKS: {k1={ parameters:{clusterName=elasticsearch, indexType=data, serializer=com.rsa.flume.serialization.FlumeAvroEventDeserializer, indexName=sa, batchSize=1000, hostNames=127.0.0.1:9300, type=org.apache.flume.sink.elasticsearch.ElasticSearchSink, ttl=2d, channel=c1} }}
  2015-10-19 10:24:31,039 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateChannels(FlumeConfiguration.java:469)] Created channel c1
  2015-10-19 10:24:31,046 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.validateSinks(FlumeConfiguration.java:675)] Creating sink: k1 using ELASTICSEARCH
  2015-10-19 10:24:31,047 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration$AgentConfiguration.isValid(FlumeConfiguration.java:372)] Post validation configuration for a1
  AgentConfiguration created without Configuration stubs for which only basic syntactical validation was performed[a1]
  SOURCES: {avrofld={ parameters:{fileHeader=true, channels=c1, spoolDir=/var/export, type=spooldir, deserializer=avro} }}
  CHANNELS: {c1={ parameters:{transactionCapacity=100000, capacity=1000000, type=memory} }}
  SINKS: {k1={ parameters:{clusterName=elasticsearch, indexType=data, serializer=com.rsa.flume.serialization.FlumeAvroEventDeserializer, indexName=sa, batchSize=1000, hostNames=127.0.0.1:9300, type=org.apache.flume.sink.elasticsearch.ElasticSearchSink, ttl=2d, channel=c1} }}
  2015-10-19 10:24:31,047 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:136)] Channels:c1
  2015-10-19 10:24:31,048 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:137)] Sinks k1
  2015-10-19 10:24:31,048 (conf-file-poller-0) [DEBUG - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:138)] Sources avrofld
  2015-10-19 10:24:31,048 (conf-file-poller-0) [INFO - org.apache.flume.conf.FlumeConfiguration.validateConfiguration(FlumeConfiguration.java:141)] Post-validation flume configuration contains configuration for agents: [a1]
  2015-10-19 10:24:31,048 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:145)] Creating channels
  2015-10-19 10:24:31,056 (conf-file-poller-0) [INFO - org.apache.flume.channel.DefaultChannelFactory.create(DefaultChannelFactory.java:42)] Creating instance of channel c1 type memory
  2015-10-19 10:24:31,063 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.loadChannels(AbstractConfigurationProvider.java:200)] Created channel c1
  2015-10-19 10:24:31,063 (conf-file-poller-0) [INFO - org.apache.flume.source.DefaultSourceFactory.create(DefaultSourceFactory.java:41)] Creating instance of source avrofld, type spooldir
  2015-10-19 10:24:31,086 (conf-file-poller-0) [INFO - org.apache.flume.sink.DefaultSinkFactory.create(DefaultSinkFactory.java:42)] Creating instance of sink: k1, type: org.apache.flume.sink.elasticsearch.ElasticSearchSink
  2015-10-19 10:24:31,086 (conf-file-poller-0) [DEBUG - org.apache.flume.sink.DefaultSinkFactory.getClass(DefaultSinkFactory.java:63)] Sink type org.apache.flume.sink.elasticsearch.ElasticSearchSink is a custom type
  2015-10-19 10:24:31,115 (conf-file-poller-0) [INFO - org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:114)] Channel c1 connected to [avrofld, k1]
  2015-10-19 10:24:31,124 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:138)] Starting new configuration:{ sourceRunners:{avrofld=EventDrivenSourceRunner: { source:Spool Directory source avrofld: { spoolDir: /var/export } }} sinkRunners:{k1=SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@5c5dc0a5 counterGroup:{ name:null counters:{} } }} channels:{c1=org.apache.flume.channel.MemoryChannel{name: c1}} }
  2015-10-19 10:24:31,134 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c1
  2015-10-19 10:24:31,136 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.
  2015-10-19 10:24:31,136 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started
  2015-10-19 10:24:31,137 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k1
  2015-10-19 10:24:31,138 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.sink.elasticsearch.ElasticSearchSink.start(ElasticSearchSink.java:350)] ElasticSearch sink {} started
  2015-10-19 10:24:31,139 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SINK, name: k1: Successfully registered new MBean.
  2015-10-19 10:24:31,139 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SINK, name: k1 started
  2015-10-19 10:24:31,142 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source avrofld
  2015-10-19 10:24:31,142 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.source.SpoolDirectorySource.start(SpoolDirectorySource.java:78)] SpoolDirectorySource source starting with directory: /var/export
  2015-10-19 10:24:31,147 (lifecycleSupervisor-1-1) [WARN - org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.configureHostnames(ElasticSearchTransportClient.java:136)] [127.0.0.1:9300]
  2015-10-19 10:24:31,152 (lifecycleSupervisor-1-1) [ERROR - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:253)] Unable to start SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@5c5dc0a5 counterGroup:{ name:null counters:{} } } - Exception follows.
  java.lang.NoSuchMethodError: org.elasticsearch.common.transport.InetSocketTransportAddress.&lt;init&gt;(Ljava/lang/String;I)V
      at org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.configureHostnames(ElasticSearchTransportClient.java:143)
      at org.apache.flume.sink.elasticsearch.client.ElasticSearchTransportClient.&lt;init&gt;(ElasticSearchTransportClient.java:77)
      at org.apache.flume.sink.elasticsearch.client.ElasticSearchClientFactory.getClient(ElasticSearchClientFactory.java:48)
      at org.apache.flume.sink.elasticsearch.ElasticSearchSink.start(ElasticSearchSink.java:357)
      at org.apache.flume.sink.DefaultSinkProcessor.start(DefaultSinkProcessor.java:46)
      at org.apache.flume.SinkRunner.start(SinkRunner.java:79)
      at org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:251)
      at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
      at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
      at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
      at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
      at java.lang.Thread.run(Thread.java:745)
  2015-10-19 10:24:31,160 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.sink.elasticsearch.ElasticSearchSink.stop(ElasticSearchSink.java:376)] ElasticSearch sink {} stopping
  2015-10-19 10:24:31,160 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:150)] Component type: SINK, name: k1 stopped
  2015-10-19 10:24:31,160 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:156)] Shutdown Metric for type: SINK, name: k1. sink.start.time == 1445246671139
  2015-10-19 10:24:31,160 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:162)] Shutdown Metric for type: SINK, name: k1. sink.stop.time == 1445246671160
  2015-10-19 10:24:31,160 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.batch.complete == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.batch.empty == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.batch.underflow == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.connection.closed.count == 1
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.connection.creation.count == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.connection.failed.count == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.event.drain.attempt == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.stop(MonitoredCounterGroup.java:178)] Shutdown Metric for type: SINK, name: k1. sink.event.drain.sucess == 0
  2015-10-19 10:24:31,161 (lifecycleSupervisor-1-1) [WARN - org.apache.flume.lifecycle.LifecycleSupervisor$MonitorRunnable.run(LifecycleSupervisor.java:260)] Component SinkRunner: { policy:org.apache.flume.sink.DefaultSinkProcessor@5c5dc0a5 counterGroup:{ name:null counters:{} } } stopped, since it could not besuccessfully started due to missing dependencies
  2015-10-19 10:24:31,155 (lifecycleSupervisor-1-2) [DEBUG - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.&lt;init&gt;(ReliableSpoolingFileEventReader.java:138)] Initializing ReliableSpoolingFileEventReader with directory=/var/export, metaDir=.flumespool, deserializer=avro
  2015-10-19 10:24:31,175 (lifecycleSupervisor-1-2) [DEBUG - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.&lt;init&gt;(ReliableSpoolingFileEventReader.java:160)] Successfully created and deleted canary file: /var/export/flume-spooldir-perm-check-8040653594475164987.canary
  2015-10-19 10:24:31,176 (lifecycleSupervisor-1-2) [DEBUG - org.apache.flume.source.SpoolDirectorySource.start(SpoolDirectorySource.java:111)] SpoolDirectorySource source started
  2015-10-19 10:24:31,177 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: SOURCE, name: avrofld: Successfully registered new MBean.
  2015-10-19 10:24:31,177 (lifecycleSupervisor-1-2) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: SOURCE, name: avrofld started
</description><key id="112107967">14187</key><summary>Unable to get apache-flume-1.6.0 to work with ElasticSearch 2.0 beta 2 - Works with Elasticsearch 1.7.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidjwaugh</reporter><labels /><created>2015-10-19T09:33:21Z</created><updated>2016-04-17T07:47:39Z</updated><resolved>2015-10-20T11:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:42:49Z" id="149535739">Looks like it is due to this change https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_java_api_changes.html#_inetsockettransportaddress_removed

You should open an issue on the Flume list
</comment><comment author="davidjwaugh" created="2015-10-20T11:43:40Z" id="149535872">Thanks very much for your help
</comment><comment author="lucidfrontier45" created="2015-11-12T07:13:36Z" id="156020555">@davidjwaugh 

I made a Custom Sink for Elasticsearch =&gt; 2.0
https://github.com/lucidfrontier45/ElasticsearchSink2

Hope it will help you.
</comment><comment author="GagandeepS" created="2016-04-17T07:47:39Z" id="210971670">Hi @lucidfrontier45 

I am using ES 2.2/2.1 because that is only version compatible with Spark. Please correct me if i am wrong.
Now the problem in my architecture is, for some of the data sources I am using ES in Flume sink so i installed ES 2.0 and use your custom sink. But for some of the data sources I have to use Spark as Flume sink because I have to do some data processing and then read the data from Spark to ES. Now in this case I can only use ES 2.2/2.1 (compatibility issues).

Since no custom Flume sink of ES 2.2 is available. Is it possible to use ES 2.0 with Spark 1.6?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix `minimum should match` in `simple_query_string` for single term and multiple fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14186</link><project id="" key="" /><description>Currently a `simple_query_string` query with one term and multiple fields
gets parsed to a BooleanQuery where the number of clauses is determined
by the number of fields, which lead to wrong calculation of `minimum_should_match`.

This PR adds checks to detect this case and wrap the resulting BooleanQuery into
another BooleanQuery with just one should-clause, so `minimum_should_match`
calculation is corrected.

In order to differentiate between the case where one term is queried across
multiple fields and the case where multiple terms are queried on one field,
we override a simplification step in Lucenes SimpleQueryParser that reduces
a one-clause BooleanQuery to the clause itself.

Closes #13884
</description><key id="112099835">14186</key><summary>Fix `minimum should match` in `simple_query_string` for single term and multiple fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-10-19T08:43:59Z</created><updated>2015-11-03T10:57:35Z</updated><resolved>2015-10-28T11:32:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:34:10Z" id="149534033">@jdconrad is this something that should go into Lucene's simple query string?
</comment><comment author="cbuescher" created="2015-10-26T17:13:43Z" id="151213019">@dakrone would you mind taking a look at this?
</comment><comment author="dakrone" created="2015-10-26T17:25:03Z" id="151216114">@cbuescher Left some minor comments and a clarification where I think I am misunderstanding what you are trying to do
</comment><comment author="cbuescher" created="2015-10-26T23:32:08Z" id="151313860">@dakrone thanks, I adressed the two minor comments and left some comments regarding your questions. Hope this makes the original problem and my attempt to fix a bit clearer. 
</comment><comment author="dakrone" created="2015-10-27T21:44:32Z" id="151654731">Thanks @cbuescher , I think this looks good now.
</comment><comment author="javanna" created="2015-10-30T16:05:23Z" id="152567016">Heya @cbuescher sorry I am late to the party. I believe that the way we fixed the original issue is not future proof as it requires reverse engineering the lucene simple query parser.  I think lucene should rather support minimum should match properly, or we should remove support for it in elasticsearch as it can't be properly supported otherwise. I think we previously discussed what the behaviour should be, but maybe we should re-discuss how to implement that and where that logic should be. Thoughts?
</comment><comment author="cbuescher" created="2015-11-03T10:30:35Z" id="153309875">After feedback from @jpountz and @javanna it became clear that this fix is too fragile and will likely break with future changes in the way lucene parses simple query string. I'll revert this commit on master, reopen the original issue.
</comment><comment author="cbuescher" created="2015-11-03T10:57:35Z" id="153314460">Reverted with 2fe519c3e71c78536b21fa16fe0de27e53544334
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong geo_distance filter behavior</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14185</link><project id="" key="" /><description>I'm working on map search service. On Google Map defined some objects and i need to draw some areas, in whith they can bounds. `geo_shape` search work OK, but with `geo_distance` (e.g. circle search) i have trouble.

![](http://dl2.joxi.net/drive/0004/2001/325585/151016/4854042dd0.png)
If i move my circle above or below these strips, objects will be found. But if I move circle right or left objects, everything works as it should.

That is, if I change the latitude, but longitude affect the objects, ElasticSearch finds them.

Index looks like this:

```
'location' =&gt; [
    'type' =&gt; 'geo_shape',
],
'location_point' =&gt; [
    'type' =&gt; 'geo_point',
],
```

Data:

```
"location": {
    "type": "point",
    "coordinates": [
        25.76029,
        -80.19339
    ]
},
"location_point": [
    25.76029,
    -80.19339
],
```

Search query:

```
{
    "fields":[
        ...
    ],
    "query":{
        ...
    },
    "filter":{
        "and":[
            {
                "and":[
                    {
                        "or":[
                            {
                                "geo_distance":{
                                    "distance":"15294.54531m",
                                    "location_point":[
                                        26.57895,
                                        -80.19496
                                    ]
                                }
                            }
                        ]
                    }
                ]
            },
            {
                ...
            }
        ]
    }
}
```

P.S. the same behavior on `geo_shape` search with `shape type = circle`.
P.S.S. If someone knows English better (e.g. anyone :D), please adjust my description.
</description><key id="112096381">14185</key><summary>Wrong geo_distance filter behavior</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erickskrauch</reporter><labels><label>:Geo</label><label>feedback_needed</label></labels><created>2015-10-19T08:24:39Z</created><updated>2016-01-29T18:34:14Z</updated><resolved>2016-01-29T18:34:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:31:41Z" id="149533626">Hi @ErickSkrauch 

I'm afraid I don't understand what the problem is.  Could you provide a full recreation which demonstrates the data you have, the queries you are running, and what doesn't work as you expect?
</comment><comment author="erickskrauch" created="2015-10-20T13:53:29Z" id="149574777">I can show it to you using TeamViewer on my dev computer.
Can you add me to Skype or Telegram by nickname ErickSkrauch?
</comment><comment author="clintongormley" created="2015-10-20T14:02:02Z" id="149577082">No @ErickSkrauch we can't :)
</comment><comment author="erickskrauch" created="2015-10-20T14:19:14Z" id="149581866">Well, just that it would be the easiest way.

What exactly you not understand?
If I move the circle to the left or right (changing longitude), the objects will be filtered, but it does not affect the latitude.

![](http://dl2.joxi.net/drive/0004/2001/325585/151020/93d0c97e8b.png)
![](http://dl2.joxi.net/drive/0004/2001/325585/151020/65b16d8724.png)
![](http://dl2.joxi.net/drive/0004/2001/325585/151020/1ea3f80e77.png)
![](http://dl1.joxi.net/drive/0004/2001/325585/151020/b3377a1ab8.png)
![](http://dl1.joxi.net/drive/0004/2001/325585/151020/6a7b8043a1.png)
</comment><comment author="clintongormley" created="2015-10-20T14:20:56Z" id="149582268">I repeat: Could you provide a full recreation (ie show the curl requests that get sent to Elasticsearch, and the responses) which demonstrates the data you have, the queries you are running, and what doesn't work as you expect?
</comment><comment author="erickskrauch" created="2015-10-20T14:24:10Z" id="149583019">Ok, I will prepere full recreation.
</comment><comment author="clintongormley" created="2016-01-29T18:34:14Z" id="176900379">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add date processor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14184</link><project id="" key="" /><description>This PR introduces a date processor to help extract datetime values from fields.
</description><key id="112092569">14184</key><summary>add date processor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-10-19T07:59:13Z</created><updated>2015-11-09T13:12:06Z</updated><resolved>2015-11-09T13:12:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-11-06T14:58:12Z" id="154430427">this should be ready for review @talevy @martijnvg 
</comment><comment author="talevy" created="2015-11-06T15:14:04Z" id="154434317">sweet! can you add some short docs for this processor in [docs/plugins/ingest.asciidoc](https://github.com/elastic/elasticsearch/blob/feature/ingest/docs/plugins/ingest.asciidoc) just so we have something there for now?
</comment><comment author="javanna" created="2015-11-06T15:15:07Z" id="154434581">@talevy sure will add docs
</comment><comment author="javanna" created="2015-11-06T16:59:06Z" id="154472140">I pushed some docs, let me know what you think ;)
</comment><comment author="talevy" created="2015-11-06T20:00:53Z" id="154517046">LGTM, just those minor comments
</comment><comment author="martijnvg" created="2015-11-09T06:28:41Z" id="154957618">LGTM2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] terminate_after is not experimental anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14183</link><project id="" key="" /><description>We are relying on terminate_after more and more, replaced the limit filter with it and soon it will also replace the search_exists api. At that point we should make it a stable api rather than experimental.
</description><key id="112090164">14183</key><summary>[DOCS] terminate_after is not experimental anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>docs</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-19T07:51:34Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-19T11:58:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-19T11:51:00Z" id="149192498">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>extended_stats aggregations returning the same value for max and avg</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14182</link><project id="" key="" /><description>running 2.0-rc1

queries return several extended_stats aggregations on various numeric fields. All of the aggregations returned have the same value for max and avg on every extended_stats aggregation returned. This is true when running both top-level aggregations and nested aggregations.

I assume max=avg is only mathematically possible if all values are the same; however, max != min and stdev has non-zero value so assuming that this is a bug. See below for an example of one aggregation:

``` json
avg: 1411.9246253746253
count: 20020
max: 1411.9246253746253
min: 0
stdDeviation: 8944.30792726666
stdDeviationLowerBound: -16476.691229158696
stdDeviationUpperBound: 19300.540479907944
sum: 28266731
sumOfSquares: 1641523392419
variance: 80000644.29776523
```
</description><key id="112076308">14182</key><summary>extended_stats aggregations returning the same value for max and avg</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryanrozich</reporter><labels /><created>2015-10-19T05:40:44Z</created><updated>2015-10-19T05:53:17Z</updated><resolved>2015-10-19T05:53:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ryanrozich" created="2015-10-19T05:53:16Z" id="149108647">closing this issue. It was a bug in my code not in the response from the server
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Envelope causes OOM with GeoHashPrefixTree</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14181</link><project id="" key="" /><description>This looks like a bug, can someone check/confirm please :)

```
curl -X PUT http://localhost:9200/test/

curl -X PUT http://localhost:9200/test/_mapping/object -d '
{
    "object" : {
        "properties" : {
            "bbox" : {
                "type": "geo_shape",
                "precision": "1cm"
            }
        }
    }
}'

curl -X POST http://localhost:9200/test/object -d '
{
    "bbox" : {
        "type" : "envelope",
        "coordinates" : [ [-45.0, 45.0], [45.0, -45.0] ]
    }
}'
```

The logs from my attempt are pretty big so you can see them [here](https://gist.github.com/markwalkom/34a73229d888892fe005).

Reported initially [here](https://discuss.elastic.co/t/indexing-geo-shape-in-2-0-0-rc1-bug/32447).
</description><key id="112041736">14181</key><summary>Envelope causes OOM with GeoHashPrefixTree</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2015-10-18T21:17:06Z</created><updated>2017-03-17T14:50:28Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:17:23Z" id="149530985">1cm resolution for half the world?  Not surprising it OOMs :)

@nknize what can we do here?
</comment><comment author="nknize" created="2015-10-20T13:44:10Z" id="149572396">With that high of resolution this uncovered an issue in Lucene's GeoHashPrefixTree that I had fixed in PackedQuadTree. I'd suggest cutting over to QuadTree with the following mappings:

```
"properties" : {
    "bbox" : {
        "type": "geo_shape",
        "tree" : "quadtree",
        "precision": 29
    }
}
```

This will give 1.11 cm accuracy (but takes ~10 secs on my laptop to index). Ultimately, with shapes the size of the one posted I would suggest lowering that resolution, or at very least, breaking it down into smaller shapes.
</comment><comment author="markwalkom" created="2015-10-20T22:33:31Z" id="149721823">/faceplam

I totally missed that and yes, makes sense
</comment><comment author="nknize" created="2015-10-21T01:28:12Z" id="149752713">No worries Mark! It actually raised some good issues re: max precision. 1.  We throw a healthy IAE when `precision` is explicitly set beyond the max tree level but not when its set using "human readable" form (e.g., "1mm" happily self destructs w/ OOM). 2. GeoHashPrefixTree needs the same surgery procedure as applied to QuadPrefixTree
</comment><comment author="michel-kraemer" created="2015-10-21T04:40:06Z" id="149781458">I agree that the resolution is too high. I initially used a smaller bounding box and got the same error. For the minimum working example to reproduce the bug I just copied the snippet from the docs. Sorry for the confusion.

So do I understand you correctly that there actually is a bug? Because the minimum example works quite well under 1.7. It's done within a couple of milliseconds even with the high resolution and the large bounding box.

Cheers,
Michel
</comment><comment author="michel-kraemer" created="2015-10-21T04:44:05Z" id="149781804">OK. I see. I just tried it again with a smaller bounding box and it's working OK. It seems my application code produced too large bounding boxes and I didn't notice that. I will check my code. Thanks for the pointers.
</comment><comment author="nknize" created="2015-12-08T15:46:14Z" id="162922868">I don't see this making 2.2 since it will be a lucene 5.5+ improvement. Removing version labels for now.
</comment><comment author="michel-kraemer" created="2017-03-17T14:50:28Z" id="287375009">@nknize Now that we have Lucene 6, are there any plans to follow up on this issue?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>About compiler source in IntelliJ IDEA</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14180</link><project id="" key="" /><description>When compiler source failed, as follow:

```
Exception in thread "main" java.lang.IllegalStateException: jar hell!
class: jdk.packager.services.UserJvmOptionsService
jar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/lib/ant-javafx.jar
jar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/lib/packager.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:248)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:152)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:87)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:154)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:266)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:36)
Refer to the log for complete error details.
```

How to solve it?
</description><key id="112026156">14180</key><summary>About compiler source in IntelliJ IDEA</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">feifeiiiiiiiiiii</reporter><labels /><created>2015-10-18T15:42:08Z</created><updated>2015-10-19T01:56:15Z</updated><resolved>2015-10-18T19:38:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-18T16:49:20Z" id="149029703">I manually updated the JDK configuration and removed `ant-javafx.jar` for example.
You have to do it for all conflicting libs.
</comment><comment author="jasontedor" created="2015-10-18T19:38:29Z" id="149041213">The only JDK jars needed on the classpath are the ones listed in this [comment](https://github.com/elastic/elasticsearch/pull/13465#issuecomment-142124441). You can configure this in  Project Structure -&gt; Platform Settings -&gt; JDK -&gt; Classpath. The paths are system and installation dependent, but all these jars are available in the jre/lib folder.
</comment><comment author="feifeiiiiiiiiiii" created="2015-10-19T01:56:15Z" id="149077043">@jakommo @dadoonet tks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search with specific index type is 3 time slower than no index type specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14179</link><project id="" key="" /><description>Version 2.0rc1
Hardware; Core I5 ,8GB of Ram and SSD

---

Here I have index:user and type:test with 40m documents index.
query1: /user/test/_search?q=title:how to install window10 
take around 2300ms to 2500ms

query2: /user/_search?q=title:how to install window10 
take around 700ms to 800ms

---

This is normal behavior of elasticsearch? or bug on 2.0 version?
</description><key id="112022136">14179</key><summary>Search with specific index type is 3 time slower than no index type specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sunchanras</reporter><labels><label>:Search</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-10-18T14:06:53Z</created><updated>2015-11-01T12:28:39Z</updated><resolved>2015-10-20T17:20:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:11:31Z" id="149530100">@sunchanras your first query also has to do a filter on `_type:test`.  Did you run this several times before comparing times? The filter should end up being cached (and fast).
</comment><comment author="sunchanras" created="2015-10-20T13:36:47Z" id="149570411">@clintongormley  Yes, I run it several times.I have just tested with version 1.7 with 200 millions docs, the same result. For single index and single type;search with specific type is much slower than not specified type. I did not do filter on _type:test, I will test with filter and will update result.
</comment><comment author="clintongormley" created="2015-10-20T14:00:10Z" id="149576607">@jpountz any ideas here?
</comment><comment author="sunchanras" created="2015-10-20T14:04:29Z" id="149577648">@clintongormley @jpountz 
I did filter query with type "test"

``` javascript
{
  "query": {
    "filtered": {
      "query": {
        "term": {
          "title": "title:how to install window10"
        }
      },
      "filter": {
        "type": {
          "value": "test"
        }
      }
    }
  }
}
```

Same result.
200million docs, single index "user", single type "test"
single testing node
shard:1
replica:0
Am I missing something ?
</comment><comment author="jpountz" created="2015-10-20T14:35:54Z" id="149586991">@sunchanras can you let us know about the following:
- how many types do you have?
- how many hits do both queries match?
- what does the [validate API](https://www.elastic.co/guide/en/elasticsearch/guide/current/_validating_queries.html) return when `explain` is turned on? (once with the type and once without please)
</comment><comment author="sunchanras" created="2015-10-20T14:44:52Z" id="149589362">@jpountz 
-  only 1 type "test"
-  hit with specific type and non-specific type 6708012 
-  I did not turn `explain` on ( I will try again and will update later)
-  mapping is default dynamic mapping

thanks
</comment><comment author="sunchanras" created="2015-10-20T14:56:49Z" id="149592580">@jpountz 
query body:

``` javascript
I use KOPF
/user/_validate/query
{
  "explain": true,
  "query": {
    "filtered": {
      "query": {
        "match": {
          "title": "how to install cassandra"
        }
      },
      "filter": {
        "type": {
          "value": "test"
        }
      }
    }
  }
}
```

response:

``` javascript
{
  "valid": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  }
}
```

query validation false but no explanation.
Thank
</comment><comment author="jpountz" created="2015-10-20T15:03:03Z" id="149594477">Sorry I was not clear, what I would like to see is the output of:

```
GET /user/test/_validate/query?q=title:how to install window10&amp;explain
```

and

```
GET /user/_validate/query?q=title:how to install window10&amp;explain
```

It looks like the optimization for disjunction we have when they are used at the top-level (usually referred to as BS1) makes things slower than the regular scorer (usually referred to as BS2), which is quite surprising given how BS1 is usually much faster than BS2.
</comment><comment author="sunchanras" created="2015-10-20T15:11:28Z" id="149596638">```
 /user/test/_validate/query?q=title:how to install window10&amp;explain

{
  "valid": true,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "explanations": [
    {
      "index": "user",
      "valid": true,
      "explanation": "+(title:how _all:to _all:install _all:window10) #QueryWrapperFilter(+ConstantScore(_type:test))"
    }
  ]
}
```

```
/user/_validate/query?q=title:how to install window10&amp;explain


{
  "valid": true,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "explanations": [
    {
      "index": "user",
      "valid": true,
      "explanation": "title:how _all:to _all:install _all:window10"
    }
  ]
}
```

Thank
</comment><comment author="sunchanras" created="2015-10-20T15:18:00Z" id="149599229">@jpountz 
Here the captured image from Kopf
1.with specific type 
![e1](https://cloud.githubusercontent.com/assets/6205717/10611856/4af2f316-7778-11e5-998e-7955aa041f0e.jpg)
1. without specific type
   ![e2](https://cloud.githubusercontent.com/assets/6205717/10611860/504bdc9c-7778-11e5-90de-4c5a5d3bf33c.jpg)

Am I missing something?
Thanks
</comment><comment author="jpountz" created="2015-10-20T15:25:36Z" id="149601660">@sunchanras Any chance that you can capture [hot threads](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html) while the slow query (the index-level one) is running? This could help figure out the bottleneck. Please put the content in a gist and link it here.
</comment><comment author="jpountz" created="2015-10-20T15:26:45Z" id="149601992">Also out of curiosity, how many [segments](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-segments.html) does your shard have?
</comment><comment author="sunchanras" created="2015-10-20T15:30:51Z" id="149603287">@jpountz 
For segments

```
"num_committed_segments": 28,
"num_search_segments": 28,
```

Thank
</comment><comment author="sunchanras" created="2015-10-20T15:39:28Z" id="149607995">@jpountz 
For hot thread
https://gist.github.com/sunchanras/e0d2107b62aa884ce561
Sorry I am not sure what "the index-level one" is.
Thank 
</comment><comment author="jpountz" created="2015-10-20T15:46:59Z" id="149610462">Hmm, the hot threads are empty, are you sure you generated them while the query was running? By index-level, I meant the query where the type is not specified.
</comment><comment author="jpountz" created="2015-10-20T15:48:05Z" id="149610819">It might help to run the query in a loop and capture the hot threads several times to get a better idea of what is happening.
</comment><comment author="sunchanras" created="2015-10-20T15:49:23Z" id="149611222">OK, thank. I will run with loop and will update again.
</comment><comment author="sunchanras" created="2015-10-20T15:52:05Z" id="149612012">@jpountz I have just updated the gist, and you view the gist again, please?
https://gist.github.com/sunchanras/e0d2107b62aa884ce561
Thank.
</comment><comment author="sunchanras" created="2015-10-20T16:00:49Z" id="149615882">@jpountz 
Here hot-thread for specific type query

```
 /user/test/_search?q=title:how to install window10&amp;explain
```

https://gist.github.com/sunchanras/7c2ee8b994ed4981f5d7

Thank
</comment><comment author="jpountz" created="2015-10-20T16:16:49Z" id="149620154">Thank you, this is very helpful. So it looks like in your case BS2 is actually faster than BS1, which is quite unusual. Can you please run the following queries to see how many documents contain each individual term?

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "title": "how"
    }
  }
}
```

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "_all": "to"
    }
  }
}
```

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "_all": "install"
    }
  }
}
```

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "_all": "windows10"
    }
  }
}
```
</comment><comment author="sunchanras" created="2015-10-20T16:36:32Z" id="149625439">hit:0 doc

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "title": "how"
    }
  }
}
```

hit:30758725 docs

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "_all": "to"
    }
  }
}
```

hit:0 docs

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "_all": "install"
    }
  }
}
```

hit:0 docs

```
GET /user/_search
{
  "size": 0,
  "query": {
    "term": {
      "_all": "windows10"
    }
  }
}

```

Thank
</comment><comment author="sunchanras" created="2015-10-20T16:40:50Z" id="149626584">@jpountz 
I created a function to generate arbitrary title from wiki docs randomly to index.

```
String string1=[word1,word2,word3...]
String string2=[word1,word2,word3...]
String string3=[word1,word2,word3...]
String string4=[word1,word2,word3...]
String string5=[word1,word2,word3...]
String string6=[word1,word2,word3...]
String title=string1+string2+string3+string1+string5+string6;
```

This might be the problem ? searching with specific type is slow but fast without specific type?
`get /user/test_search?q=title:....` is slower in single term or multiple terms
`get /user/_search?q=title:....` is 2 to 3 times faster in single term or multiple terms
Thank
</comment><comment author="sunchanras" created="2015-10-20T16:45:32Z" id="149629018">@jpountz 
You can setup a test with single index and single type, 40 million docs.
You will see big different between query with or without specific type.

Thank.
</comment><comment author="sunchanras" created="2015-10-20T16:49:44Z" id="149630357">@jpountz 
I am new to ES, thought. I just moved from SOLR.
ES is great in nearly every aspect for distributed search.
</comment><comment author="jpountz" created="2015-10-20T17:20:26Z" id="149638285">Thank you for helping me understand what is happening here. When filtering by a type, Lucene manages to understand that it really only has to run a single term query since there is a single clause that matches documents. However when the query is not filtered, it still tries to apply the logic that we use for disjunctions, which adds unnecessary overhead. I opened an issue against Lucene: https://issues.apache.org/jira/browse/LUCENE-6850 and will close this one.
</comment><comment author="sunchanras" created="2015-11-01T04:14:36Z" id="152792592">@jpountz 
Regarding to https://issues.apache.org/jira/browse/LUCENE-6850 , was it fixed on ES 2.0 ? So I can set up test again.
Thank.
</comment><comment author="nik9000" created="2015-11-01T12:28:39Z" id="152822641">@sunchanras that looks like its in Lucene 6.0, 5.4. Elasticsearch [2.0](https://github.com/elastic/elasticsearch/blob/2.0/pom.xml) uses Lucene 5.2.1. Elasticsearch [2.1](https://github.com/elastic/elasticsearch/blob/2.1/pom.xml) will Lucene use 5.3.0. Elasticsearch [2.2](https://github.com/elastic/elasticsearch/blob/2.x/pom.xml) and [3.0](https://github.com/elastic/elasticsearch/blob/master/gradle.properties) plans to use Lucene 5.4.0. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The boost keyword changed to bool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14178</link><project id="" key="" /><description>There is no query registered for boost, so changing it to bool query. Tested it and it works fine. For example
{
"query": {
"bool": {
"must": {
"match_all": {}
},
"filter": {
"geo_distance_range": {
"from": "100km",
"to": "400km",
"location": [77.42,28.67]
}
}
}
}
}

Please review and merge.
</description><key id="112007705">14178</key><summary>The boost keyword changed to bool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">bharvidixit</reporter><labels><label>docs</label></labels><created>2015-10-18T08:29:36Z</created><updated>2015-10-20T11:34:25Z</updated><resolved>2015-10-19T08:58:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-19T07:46:07Z" id="149129460">@bharvidixit you fix looks good, would you mind signing the [cla](https://www.elastic.co/contributor-agreement) so that we can merge your changes?
</comment><comment author="bharvidixit" created="2015-10-19T08:33:07Z" id="149143738">@jpountz i have already signed CLA.
</comment><comment author="jpountz" created="2015-10-19T08:58:41Z" id="149150983">Indeed I can see it now, it was probably still being processed when I first looked.
</comment><comment author="jpountz" created="2015-10-19T08:59:29Z" id="149151146">Merged, thanks! This will take some minutes to be reflected on the website.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting document by ID with '/' slash in the id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14177</link><project id="" key="" /><description>Trying to insert or get a document with path "/twitter/tweet/a%2Fe" results in a "No handler found for uri [/twitter/tweet/a%2Fe] and method [GET]" response. Response code 400.

It seems it's a regression of previous behaviour unless I'm missing something.

This is running against es-2.0.0-rc1.
</description><key id="112007279">14177</key><summary>Getting document by ID with '/' slash in the id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">andrejserafim</reporter><labels><label>:REST</label><label>low hanging fruit</label><label>regression</label><label>v2.0.0</label></labels><created>2015-10-18T08:25:12Z</created><updated>2015-10-21T16:42:09Z</updated><resolved>2015-10-21T10:17:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-21T10:42:37Z" id="149851238">Hi @andrejserafim thanks a lot for reporting this, it was as you said a regression in 2.0.0-rc1 which will be fixed in 2.0.0 GA.
</comment><comment author="andrejserafim" created="2015-10-21T16:42:09Z" id="149956391">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The boost keyword changed to bool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14176</link><project id="" key="" /><description>There is no query registered for boost, so changing it to bool query. Tested it and it works fine. For example
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "geo_distance_range": {
          "from": "100km",
          "to": "400km",
          "location": [77.42,28.67]
        }
      }
    }
  }
}

Please review and merge.
</description><key id="112007137">14176</key><summary>The boost keyword changed to bool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bharvidixit</reporter><labels /><created>2015-10-18T08:22:32Z</created><updated>2015-10-18T08:29:01Z</updated><resolved>2015-10-18T08:29:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Setup cluster for multiple subscriptions [Azure]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14175</link><project id="" key="" /><description>I currently have machines in more than 1 subscriptions connecting via VirtualNetwork. 
As of now this plugin seems to support only 1 single subscription so it would be nice if it does allow a collection of subscription instead

Issue from old plugin repo:
https://github.com/elastic/elasticsearch-cloud-azure/issues/106
</description><key id="112003033">14175</key><summary>Setup cluster for multiple subscriptions [Azure]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yellow1912</reporter><labels><label>:Plugin Discovery Azure Classic</label><label>adoptme</label></labels><created>2015-10-18T05:55:20Z</created><updated>2016-06-28T09:15:39Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-06-28T09:15:35Z" id="228995791">Just to be sure. It's for discovery, right? Not for snapshot and restore? I mean that azure repositories already [allow multiple accounts](https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-azure-repository.html#cloud-azure-repository).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid LinearRing found on geo_shapes taken from real tweets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14174</link><project id="" key="" /><description>Trying to index twitter data using logstash with `full_tweet =&gt; true` I get the following exception. Since this happens quite frequently, and using fresh Twitter data, I assume the Elasticsearch implementation to either be buggy or too strict.

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [place.bounding_box]
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:263)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:487)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:554)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:487)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
    at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:466)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:418)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:148)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:574)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:440)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchParseException: Invalid LinearRing found (coordinates are not closed)
    at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parseLinearRing(ShapeBuilder.java:892)
    at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parsePolygon(ShapeBuilder.java:903)
    at org.elasticsearch.common.geo.builders.ShapeBuilder$GeoShapeType.parse(ShapeBuilder.java:777)
    at org.elasticsearch.common.geo.builders.ShapeBuilder.parse(ShapeBuilder.java:293)
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:244)
    ... 15 more
```

I have the following defined in my template:

```
               "place":{
                  "properties":{
                     "bounding_box":{
                          "type": "geo_shape"
                     },
```

Here is a sample tweet:

```
{"created_at":"Sat Oct 17 22:22:10 +0000 2015","id":655509130422022145,"id_str":"655509130422022145","text":"Getting Inspired #FriezeArtFair #London #DmitrySholokhov http://t.co/4CVsxImujl","source":"&lt;a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\"&gt;Twitter for iPhone&lt;/a&gt;","truncated":false,"in_reply_to_status_id":null,"in_reply_to_status_id_str":null,"in_reply_to_user_id":null,"in_reply_to_user_id_str":null,"in_reply_to_screen_name":null,"user":{"id":279132601,"id_str":"279132601","name":"Dmitry Sholokhov","screen_name":"DmitrySholokhov","location":"New York","url":"http://dmitrysholokhov.com","description":"NYC based Fashion Designer, TV Personality Winner of both  Project Runway &amp; Project Runway All Stars. Appearances, Business Inquairies: info@dmitrysholokhov.com","protected":false,"verified":false,"followers_count":10509,"friends_count":78,"listed_count":132,"favourites_count":1792,"statuses_count":1463,"created_at":"Fri Apr 08 17:06:09 +0000 2011","utc_offset":-18000,"time_zone":"Quito","geo_enabled":true,"lang":"en","contributors_enabled":false,"is_translator":false,"profile_background_color":"C6E2EE","profile_background_image_url":"http://pbs.twimg.com/profile_background_images/566193376/g1x6ejqbntgpwey1b3hp.jpeg","profile_background_image_url_https":"https://pbs.twimg.com/profile_background_images/566193376/g1x6ejqbntgpwey1b3hp.jpeg","profile_background_tile":false,"profile_link_color":"1F98C7","profile_sidebar_border_color":"C6E2EE","profile_sidebar_fill_color":"DAECF4","profile_text_color":"663B12","profile_use_background_image":false,"profile_image_url":"http://pbs.twimg.com/profile_images/2247167131/563434_698079755757_34601070_34311803_1800819434_n_normal.jpg","profile_image_url_https":"https://pbs.twimg.com/profile_images/2247167131/563434_698079755757_34601070_34311803_1800819434_n_normal.jpg","profile_banner_url":"https://pbs.twimg.com/profile_banners/279132601/1393385297","default_profile":false,"default_profile_image":false,"following":null,"follow_request_sent":null,"notifications":null},"geo":null,"coordinates":null,"place":{"id":"5de8cffc145c486b","url":"https://api.twitter.com/1.1/geo/id/5de8cffc145c486b.json","place_type":"city","name":"Camden Town","full_name":"Camden Town, London","country_code":"GB","country":"United Kingdom","bounding_box":{"type":"Polygon","coordinates":[[[-0.213503,51.512805],[-0.213503,51.572068],[-0.105303,51.572068],[-0.105303,51.512805]]]},"attributes":{}},"contributors":null,"is_quote_status":false,"retweet_count":0,"favorite_count":0,"entities":{"hashtags":[{"text":"FriezeArtFair","indices":[17,31]},{"text":"London","indices":[32,39]},{"text":"DmitrySholokhov","indices":[40,56]}],"urls":[],"user_mentions":[],"symbols":[],"media":[{"id":655509108599033856,"id_str":"655509108599033856","indices":[57,79],"media_url":"http://pbs.twimg.com/media/CRjWD60UEAABJ67.jpg","media_url_https":"https://pbs.twimg.com/media/CRjWD60UEAABJ67.jpg","url":"http://t.co/4CVsxImujl","display_url":"pic.twitter.com/4CVsxImujl","expanded_url":"http://twitter.com/DmitrySholokhov/status/655509130422022145/photo/1","type":"photo","sizes":{"small":{"w":340,"h":340,"resize":"fit"},"thumb":{"w":150,"h":150,"resize":"crop"},"medium":{"w":600,"h":600,"resize":"fit"},"large":{"w":1024,"h":1024,"resize":"fit"}}}]},"extended_entities":{"media":[{"id":655509108599033856,"id_str":"655509108599033856","indices":[57,79],"media_url":"http://pbs.twimg.com/media/CRjWD60UEAABJ67.jpg","media_url_https":"https://pbs.twimg.com/media/CRjWD60UEAABJ67.jpg","url":"http://t.co/4CVsxImujl","display_url":"pic.twitter.com/4CVsxImujl","expanded_url":"http://twitter.com/DmitrySholokhov/status/655509130422022145/photo/1","type":"photo","sizes":{"small":{"w":340,"h":340,"resize":"fit"},"thumb":{"w":150,"h":150,"resize":"crop"},"medium":{"w":600,"h":600,"resize":"fit"},"large":{"w":1024,"h":1024,"resize":"fit"}}}]},"favorited":false,"retweeted":false,"possibly_sensitive":false,"filter_level":"low","lang":"en","timestamp_ms":"1445120530120","@version":"1","@timestamp":"2015-10-17T22:22:10.000Z"}
```
</description><key id="111988126">14174</key><summary>Invalid LinearRing found on geo_shapes taken from real tweets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2015-10-17T22:26:29Z</created><updated>2016-05-06T20:09:25Z</updated><resolved>2015-10-20T11:01:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T11:01:52Z" id="149525445">See https://github.com/elastic/elasticsearch/pull/11161
</comment><comment author="vsilva89" created="2016-05-06T20:09:24Z" id="217545966">Hi synhershko Could you solve this problem? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>F0lha</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14173</link><project id="" key="" /><description>imagens corrigidas
</description><key id="111973524">14173</key><summary>F0lha</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">F0lha</reporter><labels /><created>2015-10-17T17:03:02Z</created><updated>2015-10-17T17:03:41Z</updated><resolved>2015-10-17T17:03:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix javascript/python plugin links</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14172</link><project id="" key="" /><description>I noticed that the repos for the javascript and python plugins say that they have moved to the main elasticsearch repo.  Should I just link to that instead?
</description><key id="111939475">14172</key><summary>Fix javascript/python plugin links</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryanpineo</reporter><labels /><created>2015-10-17T02:33:50Z</created><updated>2015-10-20T10:55:06Z</updated><resolved>2015-10-20T10:55:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T10:55:06Z" id="149523606">Hi @RyanPineo 

Thanks for the PR, however the links in that version of the documentation are correct.  The 2.0 and above docs have the correct links already.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix javascript/python plugin links</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14171</link><project id="" key="" /><description>I noticed that the repo pages say that the plugin has moved to the main ES repo.  I wasn't sure if I should link to the main repo instead.
</description><key id="111939282">14171</key><summary>Fix javascript/python plugin links</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ryanpineo</reporter><labels /><created>2015-10-17T02:26:44Z</created><updated>2015-10-17T02:31:00Z</updated><resolved>2015-10-17T02:31:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Startup script exit status should catch daemonized startup failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14170</link><project id="" key="" /><description>This commit fixes an issue where when starting Elasticsearch in
daemonized mode, a failed startup would not cause a non-zero exit code
to be returned. This can prevent the SysV init system from detecting
startup failures.

Closes #14163
</description><key id="111908042">14170</key><summary>Startup script exit status should catch daemonized startup failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-16T20:47:34Z</created><updated>2015-10-20T15:50:53Z</updated><resolved>2015-10-19T13:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="inqueue" created="2015-10-16T20:52:31Z" id="148833517">LGTM
</comment><comment author="jasontedor" created="2015-10-19T13:32:39Z" id="149214079">Integrated into [2.1](https://github.com/elastic/elasticsearch/commit/57801a4a648d3163b0e957801e9b82da4cd7e29f), [2.x](https://github.com/elastic/elasticsearch/commit/6824299e505d139a109ee2460599c6274b861288) and [master](https://github.com/elastic/elasticsearch/commit/6551875ea043c010188f385bc349e7ffd3158854).
</comment><comment author="jasontedor" created="2015-10-20T15:50:53Z" id="149611622">Thanks for reviewing @inqueue!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to disable closing indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14169</link><project id="" key="" /><description>Closed indices are currently out of scope for snapshots and shard migration,
and can cause issues in managed environments &#8211; where closing an index does
not necessarily make sense, as it still consumes the managed environment's storage quota.

This commit adds an option to dynamically disable closing indices via node or cluster settings.

Closes #14168
</description><key id="111902578">14169</key><summary>Add option to disable closing indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-16T20:13:33Z</created><updated>2015-10-22T18:10:02Z</updated><resolved>2015-10-22T18:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-19T16:40:46Z" id="149275721">You mention that closed indexes consume a significant amount of disk space - but do they use any more disk space than open indexes?
</comment><comment author="s1monw" created="2015-10-20T12:54:01Z" id="149557291">&gt; You mention that closed indexes consume a significant amount of disk space - but do they use any more disk space than open indexes?

no
</comment><comment author="nik9000" created="2015-10-20T13:46:07Z" id="149572859">LGTM then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable close index feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14168</link><project id="" key="" /><description>Closed indices are currently out of scope for snapshots and shard migration, and can cause issues in managed environments &#8211; where closing an index does not necessarily make sense, as it still consumes the managed environment's storage quota.

Making it possible to disable closing of indices node- or cluster wide would be useful.
</description><key id="111895415">14168</key><summary>Disable close index feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">alexbrasetvik</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v2.2.0</label></labels><created>2015-10-16T19:25:10Z</created><updated>2015-10-22T18:09:59Z</updated><resolved>2015-10-22T18:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Bool query: apply minimumShouldMatch when explicitly set regardless of existing clauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14167</link><project id="" key="" /><description>We internally adjust the minimum should match value depending on how many optional clauses we have. That works well in many cases, but we end up overriding an explicitly set value for instance if we have less optional clauses than the current value (see https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/lucene/search/Queries.java#L175). Let's say for example that we have no optional clauses and minimum should match is set to 1, we internally adjust it to 0 right before executing the query. I was discussing this today with @jpountz and this behaviour seemed odd, which is why I am opening this issue for discussion.
</description><key id="111867544">14167</key><summary>Bool query: apply minimumShouldMatch when explicitly set regardless of existing clauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label></labels><created>2015-10-16T16:41:45Z</created><updated>2016-01-05T16:22:18Z</updated><resolved>2016-01-05T16:22:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-21T14:11:33Z" id="149908193">I think we should remove this leniency.
</comment><comment author="jimczi" created="2015-12-30T19:15:24Z" id="168058486">#15571 fixed the min should match when the number of optional clauses is smaller than the provided value. For instance you have 2 optional clauses and min should match is set to 3, in such case the query will now return 0 result. Regarding your example, min should match is ignored when there is no optional clauses so I don't think that #15571 changed anything here. I don't fully understand what you mean by "this behaviour seemed odd". What is the expected behaviour ? Should we translate the non optional clauses into optional ones and take the min should match into account ?
</comment><comment author="javanna" created="2016-01-05T16:22:13Z" id="169049448">I see what you mean @jimferenczi  I think my example wasn't that representative, I had missed that with no optional clauses minimum should match gets ignored, which makes sense to me. What I cared about was that we don't override the explicitly set minimum should match depending on the number of optional clauses, otherwise could return results while it really shouldn't. I am happy, you fixed this, closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the count api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14166</link><project id="" key="" /><description>Closes #13928
</description><key id="111866065">14166</key><summary>Remove the count api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-16T16:32:27Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-19T12:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-19T11:50:43Z" id="149192448">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate the count api in favour of search with size 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14165</link><project id="" key="" /><description>Relates to #13928
</description><key id="111865920">14165</key><summary>Deprecate the count api in favour of search with size 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>deprecation</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-16T16:31:45Z</created><updated>2015-10-20T11:49:13Z</updated><resolved>2015-10-19T12:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-17T19:14:28Z" id="148944972">LGTM - once question, should we use the deprecation logger if somebody uses it?
</comment><comment author="javanna" created="2015-10-19T07:21:24Z" id="149125514">&gt; should we use the deprecation logger if somebody uses it?

I pushed a new commit that makes sure we don't use count from the REST layer, so we can safely use the deprecation logger to signal usages of count from the java api, which is going to be removed. @s1monw can you please have another look?
</comment><comment author="s1monw" created="2015-10-19T11:51:44Z" id="149192593">LGTM
</comment><comment author="javanna" created="2015-10-19T12:29:57Z" id="149200337">Pushed to 2.x and 2.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix a quick documentation typo.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14164</link><project id="" key="" /><description>Fix the object of the sentence to agree with the plural verb.
</description><key id="111859309">14164</key><summary>Fix a quick documentation typo.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Vector241-Eric</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-16T15:54:02Z</created><updated>2016-03-03T23:36:40Z</updated><resolved>2016-03-03T23:36:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-20T10:49:25Z" id="149521106">Hi @Vector241-Eric 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM init.d exits with 0 when JVM fails to start due to not enough memory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14163</link><project id="" key="" /><description>When running the RPM installation of Elasticsearch 1.7.3 (and checking the newer versions of it, likely any release), if you try to give it too much memory, then it claims to succeed to start and it also returns a `0` exit code:

``` shell
[pickypg@localhost init.d]$ sudo ./elasticsearch start
Starting elasticsearch:                                    [  OK  ]
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00007f1abfd30000, 68632248320, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 68632248320 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/jvm-14062/hs_error.log
[pickypg@localhost init.d]$ echo $?
0
```

As noted by @inqueue, if you pass in `-x` to the init.d script, then you will find that the daemon starter appears to be to blame, which "succeeds".
</description><key id="111855492">14163</key><summary>RPM init.d exits with 0 when JVM fails to start due to not enough memory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>:Packaging</label><label>bug</label></labels><created>2015-10-16T15:33:01Z</created><updated>2015-10-19T13:29:47Z</updated><resolved>2015-10-19T13:29:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="inqueue" created="2015-10-16T15:47:51Z" id="148751608">FWIW, systemd behaves correctly:

```
# grep HEAP_SIZE= /etc/sysconfig/elasticsearch
# Set ES_HEAP_SIZE to 50% of available RAM, but no more than 31g
ES_HEAP_SIZE=80g

# systemctl start elasticsearch.service
# ^start^status
systemctl status elasticsearch.service
elasticsearch.service - Elasticsearch
   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled)
   Active: failed (Result: exit-code) since Fri 2015-10-16 11:45:51 EDT; 9s ago
     Docs: http://www.elastic.co
  Process: 11224 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -Des.pidfile=$PID_DIR/elasticsearch.pid -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.config=$CONF_FILE -Des.default.path.conf=$CONF_DIR (code=exited, status=1/FAILURE)
 Main PID: 11224 (code=exited, status=1/FAILURE)

Oct 16 11:45:51 es-mon.zmb.moc systemd[1]: Starting Elasticsearch...
Oct 16 11:45:51 es-mon.zmb.moc systemd[1]: Started Elasticsearch.
Oct 16 11:45:51 es-mon.zmb.moc elasticsearch[11224]: Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_me...=12)
Oct 16 11:45:51 es-mon.zmb.moc systemd[1]: elasticsearch.service: main process exited, code=exited, status=1/FAILURE
Oct 16 11:45:51 es-mon.zmb.moc systemd[1]: Unit elasticsearch.service entered failed state.
Hint: Some lines were ellipsized, use -l to show in full.
```
</comment><comment author="jasontedor" created="2015-10-16T21:11:31Z" id="148837892">With an RPM built and installed from #14170, we now see the following behavior on a Centos 6.7 system:

```
$ cat /etc/redhat-release
CentOS release 6.7 (Final)
$ sudo service elasticsearch start
Starting elasticsearch: Java HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00007fda5a660000, 85724889088, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 85724889088 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /tmp/hs_err_pid12673.log
                                                           [FAILED]
$ echo $?
1
```

instead of the behavior reported in this issue of printing `[  OK  ]` and returning exit status 0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minor fixes to doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14162</link><project id="" key="" /><description>minor fixes
</description><key id="111852179">14162</key><summary>minor fixes to doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">georgicodes</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-16T15:15:03Z</created><updated>2015-10-16T16:26:17Z</updated><resolved>2015-10-16T16:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T15:46:34Z" id="148751334">Thanks for the PR, @GeorgiCodes 

Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="georgicodes" created="2015-10-16T16:05:15Z" id="148755997">Hey @clintongormley hm.... I've signed it twice now... not sure why it's not saying I have? I've even clicked confirm email address in the mail sent to me after signing the e-doc.
</comment><comment author="clintongormley" created="2015-10-16T16:26:01Z" id="148760413">Ah, different email address, so our automated tool isn't finding it.  I've found it manually, thanks @GeorgiCodes 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>no buckets returned by terms aggregation on raw field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14161</link><project id="" key="" /><description>Hi,

I'm getting strange behaviour with a terms aggregation. In the example below, the terms aggregation based on the "fruit" field returns 3 buckets but the aggregation on the "fruit.raw" field returns 0 buckets. I've also tried "fruit.fruit.raw", which returns 3 buckets, as expected.

Am I doing something subtly wrong?

Many Thanks,
Chris

Here is my mapping:

``` json
{
    "my__fruit__1445001899868" : {
        "mappings" : {
            "fruit" : {
                "dynamic" : "strict",
                "properties" : {
                    "color" : {
                        "type" : "string",
                        "fields" : {
                            "raw" : {
                                "type" : "string",
                                "index" : "not_analyzed"
                            }
                        }
                    },
                    "fruit" : {
                        "type" : "string",
                        "fields" : {
                            "raw" : {
                                "type" : "string",
                                "index" : "not_analyzed"
                            }
                        }
                    }
                }
            }
        }
    }
}
```

Here is my data:

``` json
{"index": {"_index": "my__fruit__1445001899868", "_type" : "fruit", "_id" : "2GjU4Yf3R323B9enVSbFuQ"}}
{"fruit":"banana", "color":"yellow"}
{"index": {"_index" : "my__fruit__1445001899868", "_type" : "fruit", "_id" : "mnCDZBNbQ2GgX0JnTIaMxg"}}
{"fruit":"apple", "color":"green"}
{"index": {"_index" : "sand__fruit__1445001899868", "_type" : "fruit", "_id" : "kXlD__CvTk21cicCTncLCQ"}}
{"fruit":"mango", "color":"green"}
```

Here is the working request and response:

``` json
{
    "query" : {
        "match_all" : {}
    },
    "from" : 0,
    "size" : 0,
    "aggs" : {
        "fruit" : {
            "terms" : {
                "field" : "fruit"
            }
        },
        "color" : {
            "terms" : {
                "field" : "color"
            }
        }
    }
}
```

``` json
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "color" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "green",
        "doc_count" : 2
      }, {
        "key" : "yellow",
        "doc_count" : 1
      } ]
    },
    "fruit" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "apple",
        "doc_count" : 1
      }, {
        "key" : "banana",
        "doc_count" : 1
      }, {
        "key" : "mango",
        "doc_count" : 1
      } ]
    }
  }
}
```

Here is the failing request and response:

``` json
{
    "query" : {
        "match_all" : {}
    },
    "from" : 0,
    "size" : 0,
    "aggs" : {
        "fruit" : {
            "terms" : {
                "field" : "fruit.raw"
            }
        },
        "color" : {
            "terms" : {
                "field" : "color.raw"
            }
        }
    }
}
```

``` json
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "color" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ {
        "key" : "green",
        "doc_count" : 2
      }, {
        "key" : "yellow",
        "doc_count" : 1
      } ]
    },
    "fruit" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [ ]
    }
  }
}
```
</description><key id="111845979">14161</key><summary>no buckets returned by terms aggregation on raw field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwithers</reporter><labels /><created>2015-10-16T14:42:19Z</created><updated>2015-10-16T17:02:23Z</updated><resolved>2015-10-16T15:43:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-16T14:47:39Z" id="148736883">Please ask user questions on discuss.elastic.co. We can better help you there.
May be you did not reindex your data after applying your mapping changes?
</comment><comment author="clintongormley" created="2015-10-16T15:43:52Z" id="148750611">@cwithers The type name in your mappings is `incident`, the type name in your docs is `fruit`, so you don't have any `.raw` fields for your data
</comment><comment author="cwithers" created="2015-10-16T15:59:00Z" id="148754623">re: dadoonet, I definitely created the index before populating it so that's not it, unfortunately.

re: clintongormley, I've tried to change the example here to something non customer data specific and missed one of the uses of "incident".
</comment><comment author="cwithers" created="2015-10-16T16:03:04Z" id="148755537">Can this be re-opened? it is a real issue that I can reproduce on a clean 1.7.2 Elasticsearch install.

It looks like having a field name (fruit) that matches the name of the type (fruit) can confuse Elasticsearch in certain cases.
</comment><comment author="dadoonet" created="2015-10-16T16:03:54Z" id="148755691">Can you share your full script which helps to reproduce it?
</comment><comment author="cwithers" created="2015-10-16T16:07:49Z" id="148756488">I create the mapping using curl, then load the data using curl, then make the request using curl:

curl -XPOST http://elasticsearch:9200/fruit -d @C:\src\Curl\fruit-type.json
curl -XPOST http://elasticsearch:9200/fruit/_bulk --data-binary @C:\src\Curl\fruit-data.json
curl -XPOST http://elasticsearch:9200/fruit/_search?pretty -d @C:\src\Curl\ESsearchRequest.json &gt; C:\src\Curl\ESsearchResponse.json
</comment><comment author="dadoonet" created="2015-10-16T16:09:38Z" id="148756897">As Clint said, your example is incorrect. Can you send a full correct script that you run on your end and which reproduces the issue?
</comment><comment author="cwithers" created="2015-10-16T16:41:27Z" id="148764315">The full script is embedded in a lot of Java code, which uses the Elasticsearch REST API to create templates, containg document type metadata and properties, and from the templates timestamped indices. The code also uses the REST API to bulk index the data and to perform searches with lots of aggregations.

I can successfully reproduce the strange behaviour with the above curl command to perform the search with only two aggregations directly against Elasticsearch.

The above mapping was exported using Elasticsearch's REST API, I only removed the metadata, additional field definitions and renamed "incident" with "fruit".

I originally found the issue with Elasticsearch 1.5.1 and to be sure it wasn't already fixed in 1.7.2, I installed 1.7.2 and pointed my Java code at it. Unfortunately, I got the same strange behaviour with one of the terms aggregations.

I didn't know about the discussion forum so thanks for pointing that out.
</comment><comment author="clintongormley" created="2015-10-16T16:42:58Z" id="148764626">&gt; It looks like having a field name (fruit) that matches the name of the type (fruit) can confuse Elasticsearch in certain cases.

Yes it can.  This is a known issue which is fixed in 2.0
</comment><comment author="clintongormley" created="2015-10-16T16:44:41Z" id="148764999">See https://github.com/elastic/elasticsearch/issues/8870
</comment><comment author="cwithers" created="2015-10-16T16:59:44Z" id="148771433">I was aware of that 2.0 change after it being mentioned at Elastic{on} but didn't realise the issue could manifest in this way when there is only one type in an index.

Thanks for all your help &#128522;
</comment><comment author="clintongormley" created="2015-10-16T17:02:23Z" id="148772646">np :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistency between get template and get mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14160</link><project id="" key="" /><description>I was trying to programmatically test, that my template creating code is doing the right thing. Unfortunately I failed, since the get template API and the get mappings API don't agree on the formatting of `dynamic: false`.

The get-mappings returns false as a string (`"false"`), whilst get-template returns a boolean.

```
curl -XPUT localhost:9200/_template/1 -d '{
  "template": "test*",
  "mappings": {
    "test": {
      "dynamic": false,
      "properties": {"field1": {"type": "string"}
      }
    }
  }
}'

curl -XPUT localhost:9200/test/test/1 -d '{"field1": "foo"}'
```

now the template looks like this:

```
curl "localhost:9200/_template/1?pretty"
{
  "1" : {
    "order" : 0,
    "template" : "test*",
    "settings" : { },
    "mappings" : {
      "test" : {
        "dynamic" : false,
        "properties" : {
          "field1" : {
            "type" : "string"
          }
        }
      }
    },
    "aliases" : { }
  }
}
```

and the mapping like this :(

```
curl "localhost:9200/test/_mappings?pretty"
{
  "test" : {
    "mappings" : {
      "test" : {
        "dynamic" : "false",
        "properties" : {
          "field1" : {
            "type" : "string"
          }
        }
      }
    }
  }
}
```

I found this on "1.6.2" -- feel free to dismiss if it was already addressed.

I can see, where this is coming from: since `dynamic` is not really boolean (because of `strict`), it needs to be realized as a string in the mapping. But couldn't the template API apply the same logic, so the `"mappings": {...` blocks in template and index-mappings become comparable?

Note: I "solved" the problem by creating the template with the string `"false"`, but for a user this is a little nagging inconsistency...
</description><key id="111845075">14160</key><summary>Inconsistency between get template and get mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels /><created>2015-10-16T14:37:11Z</created><updated>2015-10-16T15:35:00Z</updated><resolved>2015-10-16T15:35:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T15:35:00Z" id="148748484">Hi @konradkonrad 

The mapping response is generated from actual Java objects. The template, however, is basically just a string that gets parsed only when an index is created.  Currently  you can put what you like in there and the first time it will throw an exception is when you try to create an index with it. 

Even after #2415 is implemented, I'm not sure whether we'll store the original string or the generated JSON.  Either way your current solution will continue to work.

Closing in favour of #2415
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove MetaDataSerivce and it's semaphores</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14159</link><project id="" key="" /><description>MetaDataSerivce tried to protect concurrent index creation/deletion
from resulting in inconsistent indices. This was originally added a
long time ago via #1296 which seems to be caused by several problems
that we fixed already in 2.0 or even in late 1.x version. Indices where
recreated without being deleted and shards where deleted while being used
which is now prevented on several levels. We can safely remove the semaphores
since we are already serializing the events on the cluster state threads.
This commit also fixes some expception handling bugs exposed by the added test
</description><key id="111838441">14159</key><summary>Remove MetaDataSerivce and it's semaphores</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-16T14:03:00Z</created><updated>2015-10-19T11:45:44Z</updated><resolved>2015-10-19T11:45:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-16T14:16:54Z" id="148729552">@bleskes @javanna can you guys take a look ?
</comment><comment author="s1monw" created="2015-10-16T14:17:36Z" id="148729738">I also think https://github.com/elastic/elasticsearch/pull/11258 would become significantly simpler or even obsolete?
</comment><comment author="javanna" created="2015-10-16T14:26:56Z" id="148731938">left a question, LGTM otherwise. I like how it simplifies things, and indeed it helps a lot with #11258.
</comment><comment author="bleskes" created="2015-10-19T09:49:58Z" id="149168664">Change LGTM. Im sure we'll see some fall out from this and fix those but it's the right way to go. Asked some questions about the test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster state and versions - when should we increment which version and how often?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14158</link><project id="" key="" /><description>While reviewing https://github.com/elastic/elasticsearch/pull/14062 I found it somewhat difficult to reason about which version is incremented when in the cluster state. I open this issue to summarize how I think versions work right now and to maybe get some feedback on how this can be handled in a more easy to understand way.

This is the version numbers we maintain:
- cluster state version: 
  - should be incremented only by one each time a cluster state update task is processed and actually yielded any change in the whole cluster state
  - currently only changed in the [cluster state update thread](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L438).
- `MetaData` version: 
  - should increment each time something in the cluster settings or index meta data changes. 
  - currently only changed in the [cluster state update thread](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L443).
- `IndexMetaData` version: 
  - Should increment whenever some index specific property changes (mapping, settings, etc. expect for routing changes which we track in shard version). 
  - changed in many places, look at `IndexMetaData.Builder.version(..)` and where it is called.
- `RoutingTable` version
  - should increment each time any shard routing changes
  - currently changed in the [cluster state update thread](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L440) and `AllocationService.buildChangedResult(..)`.
- Shard version: 
  - incremented each time something in the routing of a single shard changes (add/remove replica, relocate shard etc.). 
  - Shard version is stored in the `ShardRouting`s. It is updated when a single `ShardRouting` changes during allocation and then copied over to all `ShardRouting`s from the same copy in `IndexRoutingTable.normalizeVersions()` once allocation has finished.

Q: Cluster state version and `MetaData` version are updated only once per cluster state update task but the others are not or at least it is not immediately clear if they can potentially be incremented by &gt; 1. It it OK if `IndexMetaData` version,  `RoutingTable` version and shard version increment by more than one between cluster states?

While I think I understand why we do versioning now the way we do it I find it cumbersome to read and wonder if there is a cleaner way to maintain these versions.
For example: we only need the version increments before master sends the new cluster state to the other nodes. Can we build a new cluster state without incrementing any version in any of the components and then [here](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/service/InternalClusterService.java#L438) check the difference between new and old cluster state and update all versions in one go? This would leave no questions open about when versions are updated and where. Chatted very briefly with @s1monw who thinks this will add complexity and not remove any but I have not yet given up hope and will give it a shot. 

Any kind of feedback is more than welcome.
</description><key id="111833542">14158</key><summary>Cluster state and versions - when should we increment which version and how often?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-10-16T13:36:32Z</created><updated>2015-10-19T15:13:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-19T15:13:21Z" id="149244735">Thanks @brwe for the great write up. The way I see it the version are primarily used to allow to check for equality (with one big exception, see further). For example, this is used when updating cluster states on the nodes after receiving a new one from the master where we want to reuse local objects when the didn't change. Longer term I tend to say we should go with implementing `equals` methods where we need to so can avoid using this version variant (as we recently did in shard routing). We should look into how much effort this requires. 

In the cluster state level, the version is also used to indicate "supersedes" relationship where a node can process only the cluster state with highest version from it's pending CS queue and it is guaranteed to include all the changes from the previous ones. It doesn't really matter at the moment if we skip a version, though I don't think it happens now.

Last - ShardRouting's version is used for selecting a primary after a full cluster restart, but we have plans to change that as you know :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error building repository-s3 plugin (tests fail)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14157</link><project id="" key="" /><description>I am trying to build the latest (master) `repository-s3` plugin on a Debian (to use it with ES 2.x). It fail with the error:

```
# mvn clean install -X
(...)
[ERROR] Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.17:junit4 (tests) on project repository-s3: There were test failures: 5 suites, 4 tests, 1 suite-level error, 4 errors [seed: C3725BA8F0B04689] -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal com.carrotsearch.randomizedtesting:junit4-maven-plugin:2.1.17:junit4 (tests) on project repository-s3: There were test failures: 5 suites, 4 tests, 1 suite-level error, 4 errors [seed: C3725BA8F0B04689]
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:216)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
        at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
        at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
        at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307)
        at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193)
        at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106)
        at org.apache.maven.cli.MavenCli.execute(MavenCli.java:862)
        at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:286)
        at org.apache.maven.cli.MavenCli.main(MavenCli.java:197)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
        at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
        at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
        at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.MojoExecutionException: There were test failures: 5 suites, 4 tests, 1 suite-level error, 4 errors [seed: C3725BA8F0B04689]
        at com.carrotsearch.maven.plugins.junit4.JUnit4Mojo.execute(JUnit4Mojo.java:527)
        at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134)
        at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
        ... 20 more
```

Skipping the tests (`-DskipTests=true`) leads to a successful build.
</description><key id="111810418">14157</key><summary>Error building repository-s3 plugin (tests fail)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wsw70</reporter><labels><label>feedback_needed</label><label>test</label></labels><created>2015-10-16T11:13:13Z</created><updated>2016-01-29T14:15:35Z</updated><resolved>2016-01-29T14:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-16T14:41:36Z" id="148735462">May be you can paste which test is in failure and why?
</comment><comment author="clintongormley" created="2016-01-29T14:15:35Z" id="176776176">no more feedback. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent/child reference "cache" cleared when deleting mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14156</link><project id="" key="" /><description>Hi,

A small disaster "happend" yesterday when I removed some non used mappings which had parent type set on their mappings. What happend was that all queries that made use of **has_child** with the same parent type as the mapping that was removed returned zero result.

Example

```
{
   "mappings":{
      "child1":{
         "properties":{
            "value":{
               "type":"string"
            }
         },
         "_parent":"parent"
      },
      "child2":{
         "properties":{
            "value":{
               "type":"string"
            }
         },
         "_parent":"parent"
      }
   }
}
```

Queries are made:

```
POST host:port/index/parent/_search
{
   "filter":{
      "has_child":{
         "type":"child1",
         "filter":{
            .....
         }
      }
   }
}
```

And a delete request is made to **DELETE host:port/*/child2** and suddenly the query above starts to return zero results.

Is this a expected behavior? To me this seems like a bug.
I read that in elasticsearch 2.x.x, delete a mapping will not be supported so this might be a no issue.
</description><key id="111805993">14156</key><summary>Parent/child reference "cache" cleared when deleting mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kuseman</reporter><labels /><created>2015-10-16T10:41:41Z</created><updated>2015-10-16T11:05:58Z</updated><resolved>2015-10-16T11:05:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T11:05:58Z" id="148685051">Hi @kuseman 

Yes it looks like a bug, but one we won't fix as you can no longer delete mappings in 2.0, which should stop this bug from being triggered.

From my testing you can recover from this situation by clearing the caches:

```
curl -XPOST "http://localhost:9200/_cache/clear"
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved building of disco nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14155</link><project id="" key="" /><description>- improved retry policy of ec2 client
- cache results for 10s

original PR was to cloud plugin repo: https://github.com/elastic/elasticsearch-cloud-aws/pull/231
</description><key id="111803727">14155</key><summary>Improved building of disco nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">chaudum</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-16T10:25:15Z</created><updated>2015-11-02T16:35:03Z</updated><resolved>2015-11-02T14:10:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-16T14:54:45Z" id="148738623">As comment, this PR will close #14099 
</comment><comment author="clintongormley" created="2015-10-23T16:31:56Z" id="150626585">CLA has been signed
</comment><comment author="dadoonet" created="2015-10-27T15:20:40Z" id="151538309">Hi @chaudum. Thank you for creating the PR here.
I left some comments. Could you address them and rebase on master?

Thanks!
</comment><comment author="chaudum" created="2015-10-28T13:44:20Z" id="151848415">@dadoonet 
I've pushed a fixup. hope the documentation of the setting is clear enough. any advice how to improve it is welcome :)
</comment><comment author="dadoonet" created="2015-10-28T14:17:58Z" id="151858903">I left a last comment. Can you fix it, then squash your commits, rebase and I'll merge it (after running tests).
I will then adapt it for branch 2.\* as this plugin has been split only in master branch (3.0).

Thanks!
</comment><comment author="chaudum" created="2015-10-29T17:08:43Z" id="152251497">@dadoonet updated docs, rebased and squashed the commit
</comment><comment author="dadoonet" created="2015-11-02T14:40:33Z" id="153034698">Thanks @chaudum. Merged in master, 2.x and 2.1 branches.
</comment><comment author="chaudum" created="2015-11-02T16:34:41Z" id="153075620">@dadoonet thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0.0-rc1 displaying strange data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14154</link><project id="" key="" /><description>I have logstash 2.0.0-beta2 sending AWS detailed billing data into elasticsearch 2.0.0-rc1 so I can test my setup. I've setup a template so all fields are doc_values.

Logstash rubydebug output looks great. Data is as expected. Elasticsearch dynamic mappings look great. Data types look great. Fields are not analysed. All good.

Then I look at the data. Weird, here's a small sample. Hopefully it'll make sense to someone...

```
        {
            "_index": "aws_billing-2015.08.31",
            "_type": "detailed_with_resources_and_tags",
            "_id": "\"60012830\",\"588738232867\",",
            "_score": 1,
            "_source": {
                "@version": "1",
                "@timestamp": "2015-08-31T23:00:00.000Z",
                "host": "PBNEELK4001",
                "type": "detailed_with_resources_and_tags",
                "InvoiceID": "\"6001283",
                "PayerAccountId": "\"60012830\",\"",
                "LinkedAccountId": "\"60012830\",\"",
                "RecordType": "\"6001283",
                "RecordId": "\"60012830\",\"588738232867\",",
                "ProductName": "\"60012830\",\"588738232867\",",
                "RateId": "\"600128",
                "SubscriptionId": "\"60012830",
                "PricingPlanId": "\"60012",
                "UsageType": "\"60012830\",\"58873823286",
                "Operation": "\"600128",
                "AvailabilityZone": "",
                "ReservedInstance": "N",
                "ItemDescription": "\"60012830\",\"588738232867\",\"128881676082",
                "UsageQuantity": 0.00138889,
                "BlendedRate": 1,
                "BlendedCost": 0.00138889,
                "UnBlendedRate": 1,
                "UnBlendedCost": 0.00138889,
                "ResourceId": null,
                "aws_cloudformation_logical_id": null,
                "aws_cloudformation_stack_id": null,
                "aws_cloudformation_stack_name": null,
                "user_Env": null,
                "user_Environment": null,
                "user_Name": null,
                "user_Project": null,
                "user_Purpose": null,
                "user_Role": null,
                "user_User": null,
                "user_Workspace": ""
            }
```

This is what rubydebug looks like (it's not the same record though)...

{
                         "@version" =&gt; "1",
                       "@timestamp" =&gt; "2015-09-07T08:00:00.000Z",
                             "host" =&gt; "HOST4001",
                             "type" =&gt; "detailed_with_resources_and_tags",
                        "InvoiceID" =&gt; "xxx830",
                   "PayerAccountId" =&gt; "xxx867",
                  "LinkedAccountId" =&gt; "xxx944",
                       "RecordType" =&gt; "LineItem",
                         "RecordId" =&gt; "40214935098354950258880757",
                      "ProductName" =&gt; "Amazon Simple Storage Service",
                           "RateId" =&gt; "3510841",
                   "SubscriptionId" =&gt; "112242859",
                    "PricingPlanId" =&gt; "505699",
                        "UsageType" =&gt; "APS2-DataTransfer-Out-Bytes",
                        "Operation" =&gt; "ReadBucketPolicy",
                 "AvailabilityZone" =&gt; "",
                 "ReservedInstance" =&gt; "N",
                  "ItemDescription" =&gt; "$0.140 per GB - up to 10 TB / month data transfer out",
                    "UsageQuantity" =&gt; 1.16e-06,
                      "BlendedRate" =&gt; 0.13724872926531,
                      "BlendedCost" =&gt; 1.6e-07,
                    "UnBlendedRate" =&gt; 0.14,
                    "UnBlendedCost" =&gt; 1.6e-07,
                       "ResourceId" =&gt; "w-001006-emr-admin",
    "aws_cloudformation_logical_id" =&gt; nil,
      "aws_cloudformation_stack_id" =&gt; nil,
    "aws_cloudformation_stack_name" =&gt; nil,
                         "user_Env" =&gt; nil,
                 "user_Environment" =&gt; nil,
                        "user_Name" =&gt; nil,
                     "user_Project" =&gt; nil,
                     "user_Purpose" =&gt; nil,
                        "user_Role" =&gt; nil,
                        "user_User" =&gt; nil,
                   "user_Workspace" =&gt; ""
}

This worked perfectly on logstash 1.5 and elasticsearch &lt; 2.x
</description><key id="111803247">14154</key><summary>Elasticsearch 2.0.0-rc1 displaying strange data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels /><created>2015-10-16T10:21:51Z</created><updated>2015-10-16T10:56:22Z</updated><resolved>2015-10-16T10:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T10:56:05Z" id="148682550">This isn't an Elasticsearch issue. Moving to Logstash
</comment><comment author="clintongormley" created="2015-10-16T10:56:22Z" id="148682603">This issue was moved to elastic/logstash#4048
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't change state of the bool query while converting to lucene query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14153</link><project id="" key="" /><description>We used to change the `minimumShouldMatch` field of the query depending on the context, the final minimim should match should still be applied to the lucene query with the same logic, but the original `minimumShouldMatch` of the query shouldn't change. This was revealed by some recent test failure.
</description><key id="111794525">14153</key><summary>Don't change state of the bool query while converting to lucene query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>review</label></labels><created>2015-10-16T09:26:09Z</created><updated>2015-10-16T12:36:05Z</updated><resolved>2015-10-16T12:36:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-16T09:27:53Z" id="148665083">LGTM
</comment><comment author="javanna" created="2015-10-16T11:58:42Z" id="148698102">@jpountz can you have another look please? I added tests.
</comment><comment author="jpountz" created="2015-10-16T12:31:30Z" id="148704561">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DateFieldMapper fails storing JSON-LD when property name contains `Date`?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14152</link><project id="" key="" /><description>Storing the following fragment as part of a document cause `DateFieldMapper` to throw `IllegalArgumentException` with default setup.
Workaround is to disable "smart mapping" of date/time.

```
...
"ccdm:publicationEndDateTime" : {
  "@type" : "xml:dateTime",
  "@value" : "2015-01-02T11:45:00Z"
},
"ccdm:publicationStartDateTime" : {
  "@type" : "xml:dateTime",
  "@value" : "2015-01-02T11:30:00Z"
},
...
```

Fails at
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/mapper/core/DateFieldMapper.java#L515 

The mapper will fail on the `@type` attribute.
A bit annoying, but auto-indexing of this field (or these documents) aren't crucial for our use, but perhaps such failures during automagic should be avoided?

Best regards, Jan-Helge
</description><key id="111788226">14152</key><summary>DateFieldMapper fails storing JSON-LD when property name contains `Date`?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jhberges</reporter><labels /><created>2015-10-16T08:42:57Z</created><updated>2015-10-16T10:51:34Z</updated><resolved>2015-10-16T10:51:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T10:51:34Z" id="148681642">Indexing just the values that you provided work just fine.  The `@value` field is added as type date.  The problem comes when you try to index a value that isn't a valid date later on.

We can't tell if you intended these values to be indexed or not, so we're not going to silently swallow this exception.  However, you can add a dynamic mapping rule which maps any `@value` field as type:string, or `enabled:false` or whatever you need. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add hint for Array mappings (simple and complex types)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14151</link><project id="" key="" /><description>It seems like a "hint" would be helpful to projects like elasticsearch-hadoop / spark if the mappings exposed when a json **array** is expected.
https://github.com/elastic/elasticsearch-hadoop/issues/484
https://github.com/elastic/elasticsearch-hadoop/issues/482

It may also be useful for tools like Kibana.
A hint such as `"array": true`
- (side note, this hint is unrelated to nested object mapping via `"type: "nested"`:  Similar to but really more of a hint vs. a nested mapping that actually changes how it is indexed.)

such as 

```
  "mappings": {
    "foo": {
      "properties": {
        "bar": {
          "array": true, 
          "properties": {
            "name":    { "type": "string"  },
            "scores":   { "type": "string", "array": true }
          }
        }
      }
    }
  }
}
```

which would provide hints to json documents such as

```
{"foo" : { "bar" : ["name": "jeff", "scores": [10,20,30], "name" : "bob", "scores" : [5,10] ] } }
```
</description><key id="111754371">14151</key><summary>Add hint for Array mappings (simple and complex types)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeffsteinmetz</reporter><labels /><created>2015-10-16T04:00:38Z</created><updated>2015-10-16T10:34:24Z</updated><resolved>2015-10-16T10:34:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T10:34:24Z" id="148679009">Hi @jeffsteinmetz 

Any field in Elasticsearch can be an array or a single value, so I don't like adding the `array` parameter as a hint.  However, this kind of application specific extra mapping info can be stored in the mapping in the [_meta field](https://www.elastic.co/guide/en/elasticsearch/reference/master/mapping-meta-field.html).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running tests with Gradle fails when path to project contains spaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14150</link><project id="" key="" /><description>To reproduce error, clone `elasticsearch` repo into folder with spaces and run `gradle test`.
</description><key id="111729961">14150</key><summary>Running tests with Gradle fails when path to project contains spaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>build</label><label>review</label></labels><created>2015-10-15T23:48:52Z</created><updated>2015-10-26T17:32:57Z</updated><resolved>2015-10-26T17:32:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-26T17:24:02Z" id="151215834">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unassigned shards during cluster start since upgrading to 1.7.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14149</link><project id="" key="" /><description>Hey! I have a two-node Elasticsearch cluster that I upgraded to version 1.7.1 from 1.5.2. All my indexes have 2 shards and no replicas. Whenever I start Elasticsearch now, I get red cluster health with unassigned shards:

```
{
  "cluster_name" : "campfire.production.local",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 214,
  "active_shards" : 214,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 18,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0
}
```

I tried to use the reroute API to assign them or figure out what's wrong, and this happened:

```
curl -XPOST -d '{ "commands" : [ { "allocate" : { "index" : "metadata-jut_internal@16653", "shard" : 1, "node" : "FYEkARYjQm2Bp41qVoxSUg" } } ] }' http://localhost:9200/_cluster/reroute?pretty
{
  "error" : "ElasticsearchIllegalArgumentException[[allocate] trying to allocate a primary shard [metadata-jut_internal@16653][1], which is disabled]",
  "status" : 400
}
```

I tried adding `"allow_primary": true` to the reassign command, and it assigned the shard, but when I tried to query the data from that shard I found that it had been deleted. That wasn't good. After reading https://github.com/elastic/elasticsearch/issues/14001, I reran the command without `allow_primary` but with `explain`, and here's a portion of the response for one of my indexes whose shards are unassigned:

```
        "events-jut_internal@2015.07.31" : {
          "shards" : {
            "0" : [ {
              "state" : "UNASSIGNED",
              "primary" : true,
              "node" : null,
              "relocating_node" : null,
              "shard" : 0,
              "index" : "events-jut_internal@2015.07.31",
              "unassigned_info" : {
                "reason" : "CLUSTER_RECOVERED",
                "at" : "2015-10-15T23:07:55.600Z"
              }
            } ],
            "1" : [ {
              "state" : "UNASSIGNED",
              "primary" : true,
              "node" : null,
              "relocating_node" : null,
              "shard" : 1,
              "index" : "events-jut_internal@2015.07.31",
              "unassigned_info" : {
                "reason" : "CLUSTER_RECOVERED",
                "at" : "2015-10-15T23:07:55.600Z"
              }
            } ]
```

How am I to interpret that `CLUSTER_RECOVERED` reason? How can I get my cluster back to green, preferably without losing any more data? Thanks!
</description><key id="111729647">14149</key><summary>Unassigned shards during cluster start since upgrading to 1.7.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">davidvgalbraith</reporter><labels /><created>2015-10-15T23:45:47Z</created><updated>2016-07-08T07:12:33Z</updated><resolved>2015-10-16T08:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T08:57:46Z" id="148658997">`CLUSTER_RECOVERED` means that the shard was marked as unassigned when you started your cluster.  All shards start off like this.

You can use the `explain` parameter to a reroute command to get explanations of why your shards are unassigned.  In this case, you have disabled allocation.

Check your cluster settings.  You need to reenable allocation.
</comment><comment author="davidvgalbraith" created="2015-10-16T18:37:27Z" id="148799777">That's a brave little theory, but it doesn't seem to be accurate. I did

```
root@staging-llnw-de0:~$ curl -XPUT 'localhost:9200/_cluster/settings' -d '{
  "persistent": {
    "cluster.routing.allocation.enable": "all"
  }
}'
```

and checked the settings had made it with

```
root@staging-llnw-de0:~$ curl localhost:9200/_cluster/settings
{"persistent":{"cluster":{"routing":{"allocation":{"enable":"all"}}}},"transient":{}}
```

and checked the health again like so:

```
root@staging-llnw-de0:/opt/jut# curl localhost:9200/_cluster/health?pretty
{
  "cluster_name" : "campfire.production.local",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 2,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 214,
  "active_shards" : 214,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 18,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0
}
```

I then restarted Elasticsearch on both nodes, and the health stayed the same. Did I run the wrong command to enable allocation? Is there another possible cause of these unassigned shards?
</comment><comment author="s1monw" created="2015-10-16T18:51:19Z" id="148805340">can you run `GET /_cluster/reroute?explain=true` and paste the output? Can you actually tell if the shards that are not allocated exist on disk? like you have 2 nodes, do the nodes have a copy of that shard in their data dir? Can you also for the time being set logging level for `gateway` to `trace`?
</comment><comment author="davidvgalbraith" created="2015-10-16T18:57:01Z" id="148806642">```
root@staging-llnw-de0:/opt/jut# curl localhost:9200/_cluster/reroute?explain=true
{"error":"ElasticsearchIllegalArgumentException[No feature for name [reroute]]","status":400}
```

Doesn't seem like GET on _cluster/reroute is a real API, I did a POST there in OP and pasted a relevant section of the 2.5 MB of output I got.
</comment><comment author="davidvgalbraith" created="2015-10-16T19:17:36Z" id="148810904">The shards are in fact missing from my data dir! What happened to my data?
</comment><comment author="s1monw" created="2015-10-16T19:19:00Z" id="148811174">ok so lemme ask you some more questions? Did you delete those indices by any chance and some shards didn't get deleted so we "re-imported" it on the restart? do you use multiple data paths by any chance? can you see some mentions of dangling indices in the logs?
</comment><comment author="davidvgalbraith" created="2015-10-16T19:26:45Z" id="148812663">Ok so I have an index per day and a retention policy to keep the last 90 days, so the only time I delete indices is after their 90 days are up. Accordingly my indices go back to July 18, and I have the shards from July 18-30, then from July 31 to August 6 I'm missing shards, and from August 7 to today I have the shards again. There's no scenario in my application where I'd delete intermediate indices without deleting all the older indices too. What do you mean by multiple data paths? What would dangling indices look like in the logs? `grep -i dangl elasticsearch.log` returned no lines.
</comment><comment author="s1monw" created="2015-10-16T19:37:01Z" id="148815365">very hard to tell what's going on here.. do you have anything else in the logs that can help me to figure this out?
</comment><comment author="davidvgalbraith" created="2015-10-16T20:14:10Z" id="148822583">I turned on TRACE level logging for gateway and shard recovery. Here's what I find when I search for an index name whose shards got assigned successfully:

```
root@staging-llnw-de0:/opt/jut# grep events-jut_internal@2015.08.07 elasticsearch.log
[2015-09-29 19:42:45,275][DEBUG][action.admin.indices.stats] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][1], node[j0qBnp6BSWmi8sEh9NCP4A], [P], s[STARTED]: failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@291f2e34]
[2015-10-16 20:08:29,538][DEBUG][index.gateway            ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] starting recovery from local ...
[2015-10-16 20:08:29,539][TRACE][index.gateway.local      ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] using existing shard data, translog id [1439439522375]
[2015-10-16 20:08:29,539][TRACE][index.gateway.local      ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] try recover from translog file translog-1439439522375 locations: [/mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-jut_internal@2015.08.07/0/translog]
[2015-10-16 20:08:29,539][TRACE][index.gateway.local      ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] Translog file found in /mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-jut_internal@2015.08.07/0/translog - renaming
[2015-10-16 20:08:29,539][TRACE][index.gateway.local      ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] Renamed translog from translog-1439439522375 to translog-1439439522375.recovering
[2015-10-16 20:08:29,549][TRACE][index.gateway.local      ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] recovering translog file: /mnt/elasticsearch/campfire.production.local/nodes/0/indices/events-jut_internal@2015.08.07/0/translog/translog-1439439522375.recovering length: 17
[2015-10-16 20:08:29,550][TRACE][index.gateway.local      ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] ignoring translog EOF exception, the last operation was not properly written
[2015-10-16 20:08:29,552][TRACE][index.gateway            ] [node-e0c6f82d-5da6-4815-a6ed-0de785125481] [events-jut_internal@2015.08.07][0] recovery completed from local, took [13ms]
```

And here's what I find when I search for one whose shards did not get assigned:

```
root@staging-llnw-de0:/opt/jut# grep events-jut_internal@2015.08.06 elasticsearch.log
root@staging-llnw-de0:/opt/jut#
```

i.e. nothing! So the gateway recovery never even thinks about my index whose shards didn't get assigned. Maybe that's something to go on?
</comment><comment author="s1monw" created="2015-10-16T20:17:07Z" id="148823134">man I don't know where your indices went.. I will talk to others if they can think of a bug that could have caused that...
</comment><comment author="virHappy" created="2016-01-28T07:39:17Z" id="176035960">so what is going on?
</comment><comment author="davidvgalbraith" created="2016-01-28T07:53:11Z" id="176040957">Hey @virHappy! We never found out what happened here. I hope you haven't lost data too.
</comment><comment author="ksou1" created="2016-01-29T22:57:12Z" id="177008974">Hi @davidvgalbraith. I'm experiencing the same issue as we speak. Have a 3 node cluster (1 dedicated master, 2 master/data nodes, set for a primary and 1 replica shard). Set "cluster.routing.allocation.enable" to "none", restarted my nodes (dedicated master 1st). All 3 nodes joined cluster properly (status yellow), so I set "cluster.routing.allocation.enable" to "all".  However, I'm sitting here 1 hour later and all of my "replica" shards are still unassigned. And.. all of my primary shards are assigned to the master/data node that happened to join the cluster first. Reroute?explain shows "CLUSTER RECOVERED" as reason, just as in your case. Will give it a while but I think next time I'll leave routing enabled and live with the thrashing that happens upon cluster start. While I really like ES and what it can do, its difficult to really rely on it with all of the "glitches". 
</comment><comment author="bleskes" created="2016-01-31T20:26:15Z" id="177601746">@ksou1 which ES version are you using?  Also, I'm a bit confused the "CLUSTER RECOVERED" output of reroute. Can you gist the exact output?
</comment><comment author="bleskes" created="2016-01-31T20:28:47Z" id="177602035">scratch the last comment about `Cluster recovered`, re reading the ticket I get what you refer to (the unassigned_info part). Based on your ES version we'll see how to proceed.
</comment><comment author="ksou1" created="2016-02-01T14:06:09Z" id="177986120">Hi @bleskes . ES version is 1.7.2. Replicated shards finally started to allocate about 8 hours after the cluster restart. I have an index deletion script that runs at 1am every morn (to delete indexes 31 days old) and it prints a view of all indexes after the deletions. I noticed in the output that only 7 of the 218 indexes had gone to "green" status even at that point. The rest were in "yellow" status. So... after restarting the cluster at 5pm, only 7 indexes had their replica shards allocated by 1am (8 hours). By 9am the next morning, all replica's were allocated. We will be upgrading to ES 2.1.1 in the next few weeks. Keeping my fingers crossed that we dont see this delay again.
</comment><comment author="bleskes" created="2016-02-02T12:53:43Z" id="178558460">@ksou1 thanks. During the period the cluster was yellow - do you know the replicas were initializing (i.e., recovering) or were they unassigned?
</comment><comment author="ksou1" created="2016-02-02T14:52:05Z" id="178612237">Up until the time the replicas started to allocate to the second data node, they were unassigned. There were 1086 active_primary_shards, and 1086 unassigned_shards (the replicas). There were 0 initializing_shards and, strangely, 0 delayed_unassigned_shards. Cluster health also showed 0 pending_tasks and 0 in_flight_fetch. 
</comment><comment author="bleskes" created="2016-02-02T15:23:01Z" id="178630645">I see. Do you happen to have a cluster health response ? I'm curious to see what the `number_of_in_flight_fetch` field had to say...
</comment><comment author="bleskes" created="2016-02-02T17:24:37Z" id="178699651">@ksou1 not sure whether you edited your comment or I just totally missed the fact that there are 0 in_flght_fetch. I this case my best guess is that you've hit https://github.com/elastic/elasticsearch/pull/14494  and something happened later on triggering another reroute..
</comment><comment author="jessabe" created="2016-03-29T14:39:26Z" id="202928046">@davidvgalbraith having a similar issue here as well. I'm guessing this has resulted in lost data and you've moved on, or? 
</comment><comment author="davidvgalbraith" created="2016-03-29T16:06:52Z" id="202976962">That's right, @jessabe! Never figured out what exactly happened or what we should have done differently =\
</comment><comment author="bleskes" created="2016-03-29T19:15:41Z" id="203059220">@jessabe you may have the same issue (whatever it is) but it may also be different . If you give us more details we can evaluate - these are important matters.
</comment><comment author="maziyarpanahi" created="2016-03-29T22:30:42Z" id="203139288">Hello,

Mine is the latest ES 2.2.1 and I did my upgrade while back when it came out so I am certain it has nothing to do with my upgrade. All the members are the same config and the same version and being restarted multiple times after upgrade. 
This time I did restart each as a rollback cause I updated my elasticsearch.yml (adding repo.path which now is too late!) and 3 indexes has one unassigned shard (all the primary shards) each.

```
"details": "engine failure, reason [corrupt file (source: [flush])], failure FlushFailedEngineException[Flush failed]; nested: CorruptIndexException[verification failed : calculated=p4rbbd stored=1yztcit (resource=VerifyingIndexInput(NIOFSIndexInput(path=\"/data/iscpif-es/nodes/0/indices/scientific/3/index/_f75.cfs\")))];
```

These machines are KVM VM. I am not sure whether this is ext4 corruption and I should do fsck or a bug some where in Lucene/ES.
ps: these indexes are all static so no more index/update just query. It can't be corrupted by not flushing or unexpected shutdown, I'm guessing though!
</comment><comment author="nathan-zhu" created="2016-07-08T07:11:48Z" id="231291281">@davidvgalbraith i got same problem on ES 2.3 in docker, the issue is here #19323 it said i don't have enough disk space for allocation to reroute, not sure this will help something. https://www.elastic.co/guide/en/elasticsearch/reference/current/disk-allocator.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to switch default HttpServerTransport?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14148</link><project id="" key="" /><description>Hi, I&#180;m migrating a plugin from 1.5 to 2.0, it switch default NettyHttpServerTransport to add some sort of securiity, but I confused, since 40f119d85a4eaa39d0a6e594e46f70de725557f0 it seems to me that "http.type" parm doesn&#180;t work (see HttpServerTransport.java)

I&#180;m right? Is there any other way to do that? How is supossed I can call setHttpServerTransport?
</description><key id="111726082">14148</key><summary>How to switch default HttpServerTransport?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ruria</reporter><labels><label>:Network</label><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-10-15T23:10:35Z</created><updated>2015-12-16T19:54:30Z</updated><resolved>2015-12-16T19:54:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T08:50:11Z" id="148657680">@rjernst any ideas?
</comment><comment author="rjernst" created="2015-10-26T17:33:13Z" id="151218912">I think we should add `http.type` back (as a parallel to `transport.type`), and remove `setHttpServerTransport`. I'll try to get that done, not sure when I will get to it though.
</comment><comment author="ruria" created="2015-10-26T19:30:17Z" id="151258273">maybe I can help with this.
</comment><comment author="ruria" created="2015-12-14T12:05:56Z" id="164420917">I&#180;m trying to solve this, but little stuck. Please, see my commit here bbbcc528081a9235146474169ff094291ee8d356  

This works, but my plugin tries to startup a tomcat instance instead netty, so I need to include tomcat (and others) .jar in ES classpath, because by default, this files are included in child plugin class loader when loading my plugin, not in the main thread one.  Is there a way to fix this?
</comment><comment author="rjernst" created="2015-12-15T06:38:34Z" id="164660294">@ruria There were some issues with your commit and some more general cleanup that needed to happen anyways. I opened a PR: #15434
</comment><comment author="rjernst" created="2015-12-15T06:41:27Z" id="164660634">Also, to answer your original question on how to do this: with my PR, you will need to create a plugin that implements `onModule(NetworkModule module)` and then calls `module.registerHttpTransport("awesomeNameOfYourImpl", YourHttpTransportImpl.class)`. You would then set `http.type: awesomeNameOfYourImpl` in either your elasticsearch.yml, or in `additionalSettings()` for your plugin.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update docs for multi-field to have more examples</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14147</link><project id="" key="" /><description>I found some really useful information in this [blog post](https://www.elastic.co/blog/changing-mapping-with-zero-downtime) that I really felt should be within the docs themselves, so I've added in a chunk here.
</description><key id="111712397">14147</key><summary>Update docs for multi-field to have more examples</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">georgicodes</reporter><labels /><created>2015-10-15T21:36:05Z</created><updated>2015-11-09T22:53:00Z</updated><resolved>2015-10-16T08:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="georgicodes" created="2015-10-15T21:49:00Z" id="148531533">the CLA has now been signed but im unsure how to trigger another build.
</comment><comment author="clintongormley" created="2015-10-16T08:46:38Z" id="148657057">Thanks @GeorgiCodes but this info is already in the docs for 2.0 (when these changes arrive)

See https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add local lucene/XGeoPoint* classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14146</link><project id="" key="" /><description>This PR adds the new GeoPoint classes to our local lucene package for supporting geo_point version 2 in the 2.1 release. It also removes some of the redundancy in the existing utility classes. This will help in transitioning from current ES GeoPoint indexing classes to those in lucene. They will be removed once all GeoPoint APIs are finalized in lucene.
</description><key id="111705434">14146</key><summary>Add local lucene/XGeoPoint* classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">nknize</reporter><labels /><created>2015-10-15T20:52:26Z</created><updated>2015-10-23T07:21:36Z</updated><resolved>2015-10-22T18:03:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-15T21:47:05Z" id="148530966">These X\* classes tend to be very painful to maintain when doing Lucene upgrades. Maybe we should use the GeoPoint classes from Lucene directly on master and 2.x? (and upgrade the snapshot when there are changes we want to integrate in elasticsearch?)

Also maybe we shouldn't try to get those geo improvements on the 2.1 branch to avoid having to maintain a fork there?
</comment><comment author="mikemccand" created="2015-10-16T10:32:31Z" id="148678646">Are the X\* classes here identical to what's in Lucene 5.x today?
</comment><comment author="nknize" created="2015-10-16T15:07:44Z" id="148741722">No, these X\* classes are not the same as whats in lucene 5.3 (which is the current version for 2.1). LUCENE-6724 is in 5.4, LUCENE-6778 adds the DistanceRangeQuery that still needs to be committed along with LUCENE-6780 (pre BKD test merge). 
</comment><comment author="s1monw" created="2015-10-19T12:11:31Z" id="149197072">@nknize I wonder if we should push stuff in here that has not yet been committed to lucene. It might cause some incompatibilities and a the overhead seems pretty high as @jpountz said. I wonder if we should push the stuff that is not yet committed to lucene to 2.2 and only get in what is in lucene 5.3 for now?
</comment><comment author="jpountz" created="2015-10-19T13:03:19Z" id="149207012">&gt;  I wonder if we should push the stuff that is not yet committed to lucene to 2.2 and only get in what is in lucene 5.3 for now?

+1
</comment><comment author="nknize" created="2015-10-22T18:03:49Z" id="150307693">&gt; I wonder if we should push the stuff that is not yet committed to lucene to 2.2 and only get in what is in lucene 5.3 for now?

++ Good plan. The GeoPoint encoding was finalized and pushed to branch_5x this morning. So I'm going to wait until 2.2 for the full GeoPointV2 feature merge.
</comment><comment author="s1monw" created="2015-10-23T07:21:36Z" id="150495926">@nknize I unlabeled it...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow parser to move on the START_OBJECT token when parsing search source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14145</link><project id="" key="" /><description>Currently we require parser to be right before the sources START_OBJECT
but if we are parsing embedded search sources this won't work since we potentially
moved already on to the START_OBJECT. This commit make this optional such that
both ways work.
</description><key id="111701289">14145</key><summary>Allow parser to move on the START_OBJECT token when parsing search source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-15T20:31:32Z</created><updated>2015-10-16T08:45:19Z</updated><resolved>2015-10-15T20:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-15T20:31:46Z" id="148511972">@colings86 can you look
</comment><comment author="colings86" created="2015-10-15T20:32:35Z" id="148512136">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Names of indexRouting and searchRouting infos are out of date.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14144</link><project id="" key="" /><description>Hi,

I belive indexRouting should be changed to routing.index and searchRouting to routing.search.
This is what my ES 2.0.0-rc1 instance returns when I hit cat alias api:

![image](https://cloud.githubusercontent.com/assets/2392583/10525856/6b27b198-7387-11e5-81f2-3f4df799fbca.png)

Let me know if I should change other versions of documentation for this issue as well(Looks to me like it always was incorrect in docs).

PS: @clintongormley thanks for pointing right repo :+1: 
</description><key id="111695090">14144</key><summary>Names of indexRouting and searchRouting infos are out of date.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertlyson</reporter><labels><label>docs</label></labels><created>2015-10-15T19:56:19Z</created><updated>2015-10-16T09:02:49Z</updated><resolved>2015-10-16T08:43:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T08:43:24Z" id="148656534">thanks @robertlyson - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start from a random node number so that clients do not overload the first node configured</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14143</link><project id="" key="" /><description>Make sure that the first node listed does not get overloaded when multiple connections/applications/hosts pass the same list to the client. This can, of course, be mitigated by applications randomizing their host config, but this fixes the issue in the client.
</description><key id="111684891">14143</key><summary>Start from a random node number so that clients do not overload the first node configured</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">reversefold</reporter><labels><label>:Java API</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2015-10-15T19:00:03Z</created><updated>2016-07-06T11:20:51Z</updated><resolved>2016-07-05T17:17:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-04-06T20:49:27Z" id="206560649">@reversefold can you explain a little behind the reasoning for this? Is this still needed?
</comment><comment author="reversefold" created="2016-07-04T21:36:38Z" id="230356590">@dakrone The point of this is to make sure that the first node listed does not get overloaded when multiple connections/applications/hosts pass the same list to the client. This can, of course, be mitigated by applications randomizing their host config, but this fixes the issue in the client.
</comment><comment author="jasontedor" created="2016-07-04T22:02:48Z" id="230358465">Your description makes sense to me; can you add the description to the initial PR comment?
</comment><comment author="jasontedor" created="2016-07-05T02:09:35Z" id="230373875">LGTM. I will merge soon. Thanks @reversefold. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0.0~rc1 not available in the relevant APT repo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14142</link><project id="" key="" /><description>When using `deb http://packages.elasticsearch.org/elasticsearch/2.0/debian stable main`, the latest available version is still `2.0.0~beta2` -- did something change in the deployment scripts for `rc1`?
</description><key id="111667923">14142</key><summary>2.0.0~rc1 not available in the relevant APT repo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tianon</reporter><labels /><created>2015-10-15T17:31:43Z</created><updated>2015-10-15T17:38:17Z</updated><resolved>2015-10-15T17:36:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-15T17:36:30Z" id="148467913">Yes, the repo is now `2.x` instead of `2.0`.  See https://www.elastic.co/guide/en/elasticsearch/reference/2.0/setup-repositories.html
</comment><comment author="tianon" created="2015-10-15T17:38:16Z" id="148468325">Ahhhh, thanks @clintongormley; sorry for the bother!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Doc] Fix correct number of slashes when installing a plugin with zip file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14141</link><project id="" key="" /><description /><key id="111642169">14141</key><summary>[Doc] Fix correct number of slashes when installing a plugin with zip file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Plugins</label><label>docs</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-15T15:27:08Z</created><updated>2015-11-08T19:09:35Z</updated><resolved>2015-11-03T09:33:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-03T09:27:07Z" id="153297028">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor Histogram and Date Histogram Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14140</link><project id="" key="" /><description /><key id="111635199">14140</key><summary>Refactor Histogram and Date Histogram Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>review</label></labels><created>2015-10-15T14:57:25Z</created><updated>2015-11-16T14:22:01Z</updated><resolved>2015-11-16T13:03:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T14:05:56Z" id="157039186">I'm a bit late to the party, but LGTM ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor Global Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14139</link><project id="" key="" /><description /><key id="111635021">14139</key><summary>Refactor Global Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label></labels><created>2015-10-15T14:56:43Z</created><updated>2015-11-16T13:02:16Z</updated><resolved>2015-11-16T13:02:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-02T09:15:49Z" id="152963789">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor Geohash Grid Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14138</link><project id="" key="" /><description /><key id="111634632">14138</key><summary>Refactor Geohash Grid Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>review</label></labels><created>2015-10-15T14:55:25Z</created><updated>2015-11-16T14:31:55Z</updated><resolved>2015-11-16T14:31:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T14:28:05Z" id="157047384">LGTM, just found some indentation issues
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor Derivative Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14137</link><project id="" key="" /><description /><key id="111634488">14137</key><summary>Refactor Derivative Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>review</label></labels><created>2015-10-15T14:54:48Z</created><updated>2015-11-16T14:11:49Z</updated><resolved>2015-11-16T14:11:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-11-16T14:08:09Z" id="157040212">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring of Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14136</link><project id="" key="" /><description>This PR refactors all aggregations to be able to be parsed on the coordinating node and serialised as Objects to the shards.
</description><key id="111634277">14136</key><summary>Refactoring of Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-15T14:53:50Z</created><updated>2016-04-05T11:01:50Z</updated><resolved>2016-02-15T11:26:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-30T18:24:44Z" id="152608912">Things look good. Can you just explain why we need this new "init" phase?
</comment><comment author="colings86" created="2015-11-10T11:28:58Z" id="155395291">@jpountz I pushed a some updates. could you re-review?

As to the "init" phase, this is needed to be able to run things that used to be done in the constructor but now can't be because:
1. Not all settings for the aggregator factory will be set in the constructor
2. The aggregator factory will be constructed on the coordinating node but that is some initialisation (see ParentToChildAggregator for an example) that has to be done on the shard where we have the mappings  etc.

I thought it was neater to have an "init" phase than to duplicate calls to initialise in the different create() methods
</comment><comment author="jpountz" created="2015-11-11T21:53:56Z" id="155921531">I left a couple comments but it looks good in general. Thank you for the explanation about init().
</comment><comment author="colings86" created="2015-11-12T15:27:35Z" id="156137933">@jpountz I pushed an update to deal with the review comments including that NOCOMMIT message, could you take another look?
</comment><comment author="jpountz" created="2015-11-16T07:44:39Z" id="156947968">Just left two minor comments, otherwise LGTM
</comment><comment author="jpountz" created="2016-02-12T22:17:51Z" id="183509191">I gave a quick look and could spot a few indentation and naming inconsistencies. Otherwise it looks good!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give a better exception when running from freebsd jail without enforce_statfs=1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14135</link><project id="" key="" /><description>We can't track disk usage in this situation, failing is the correct thing to do.
But we can give a FreeBSD-specific error message, so the user can set the
necessary jail parameters, versus a vague IOException.

Closes #12018
</description><key id="111620500">14135</key><summary>Give a better exception when running from freebsd jail without enforce_statfs=1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-15T13:47:20Z</created><updated>2015-10-24T03:23:56Z</updated><resolved>2015-10-24T03:23:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-15T13:49:26Z" id="148392427">LGTM
</comment><comment author="s1monw" created="2015-10-23T12:38:30Z" id="150560129">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bundle TestShardRouting in test jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14134</link><project id="" key="" /><description>Plugins want to use this too.

Closes #14133
</description><key id="111605368">14134</key><summary>Bundle TestShardRouting in test jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>review</label><label>test</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-15T12:22:21Z</created><updated>2015-10-19T15:05:26Z</updated><resolved>2015-10-19T14:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-15T13:57:17Z" id="148394477">@rjernst, if this is merged here it'll need reflecting in the burn_maven_with_fire branch so... want to review it in both places?
</comment><comment author="nik9000" created="2015-10-15T13:58:25Z" id="148394791">BTW - I think that its weird that we have such intense rules around what we package in our test jar but I think that is a more fundamental issue. IIRC you've (@rjernst) got it rigged differently in gradle so this might not be an issue at all there?
</comment><comment author="s1monw" created="2015-10-17T18:53:30Z" id="148942699">LGTM
</comment><comment author="nik9000" created="2015-10-19T15:05:26Z" id="149241955">Merged to master and backported to 2.x and 2.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch should bundle TestShardRouting in its test jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14133</link><project id="" key="" /><description>`TestShardRouting` is a test utility for building `ShardRouting`, part of the routing state and plugins want to use it to run their tests.
</description><key id="111604680">14133</key><summary>Elasticsearch should bundle TestShardRouting in its test jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-15T12:17:01Z</created><updated>2015-10-19T14:00:22Z</updated><resolved>2015-10-19T14:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[WIP] Introduce the GrokProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14132</link><project id="" key="" /><description>Also moved all processor classes into a subdirectory and introduced a
ConfigException class to be a catch-all for things that can go wrong
when constructing new processors with configurations that possibly throw
exceptions. The GrokProcessor loads patterns from the resources
directory.
# Running your first Grok Pipeline
1. pull changes from this PR
2. launch a single node with ingest plugin using the `IngestRunner`
   
   ``` bash
   cd plugins/ingest
   mvn exec:java -Dexec.mainClass="IngestRunner" -Dexec.classpathScope="test"
   ```
3. Find a log file you wish to parse
   
   ```
   # file: logs
   83.109.8.216 [19/Jul/2015:08:13:42 +0000]
   90.149.9.302 [19/Jul/2015:08:13:44 +0000]
   ...
   ```
4. Create your desired pipeline in Elasticsearch
   
   ``` python
   # file: put_pipeline.py
   import requests
   requests.put("http://localhost:9200/_ingest/pipeline/my_pipeline_id", json={
   "description": "simple_pipeline",
   "processors": [{
       "grok": {
         "field": "message",
         "pattern": '%{IPORHOST:clientip} \[%{HTTPDATE:timestamp}\]'
       }
     }]
   })
   ```
5. Use the elasticsearch python client to ingest your logs
   
   ``` python
   from elasticsearch import Elasticsearch,helpers
   es = Elasticsearch()
   
   # parse line into valid json document
   def parse_file(f):
       for line in f:
           yield { 'message': line.strip() }
   
   # read `logs` file and index into elasticsearch
   with open("logs", "r") as f:
       for ok, result in helpers.streaming_bulk(es, parse_file(f), index="test", doc_type="test", params={"ingest": "my_pipeline_id"}):
           action, result = result.popitem()
           # print response
           print result
   ```
6. your documents should be parsed and ready for searching within Elasticsearch
</description><key id="111601225">14132</key><summary>[WIP] Introduce the GrokProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">talevy</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-10-15T11:50:41Z</created><updated>2015-11-03T05:32:13Z</updated><resolved>2015-11-03T04:31:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-26T07:33:30Z" id="151049662">This looks great! I know it is WIP, but I left a couple of comments.
</comment><comment author="martijnvg" created="2015-10-28T09:44:08Z" id="151782060">Left a couple more comments. I think we should also add docs for the grok processor to the `ingest.asciidoc` file, that contains a minimal description &amp; example and describes the possible named patters that can be used.
</comment><comment author="talevy" created="2015-10-28T20:40:25Z" id="151984016">@martijnvg Similar to your comment about reusing the same GeoIP instance across GeoProcessors. Do you think the same should be done with Grok? As in to use the same Grok instance across all grok processors to avoid re-loading the same configs each time a processor is created?

**UPDATE**: NEVERMIND
</comment><comment author="martijnvg" created="2015-11-03T03:35:55Z" id="153231912">@talevy left two minor comments, other then that LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logstash parse issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14131</link><project id="" key="" /><description>Hello,

We have the following usercase. I have a log containing fields enclosed in "" and separated by ;

The grok filter is set like this:

"%{DATA:1}";"%{DATA:2}";"%{DATA:3}";"%{DATA:4}";"%{DATA:5}";"%{DATA:6}";"%{DATA:7}";"%{DATA:8}";"%{DATA:9}";"%{DATA:10}";"%{DATA:11}";"%{DATA:12}";"%{DATA:13}";"%{DATA:14}";"%{DATA:15}";"%{DATA:16}";"%{DATA:17}";"%{DATA:18}";"%{DATA:19}";"%{DATA:20}";"%{DATA:21}";"%{DATA:22}";"%{DATA:23}";"%{DATA:24}";"%{DATA:25}";"%{DATA:26}";"%{DATA:27}";"%{DATA:28}";"%{DATA:29}";"%{DATA:30}";"%{DATA:31}";"%{DATA:32}";"%{DATA:33}";"%{DATA:34}";"%{DATA:35}";"%{DATA:36}";"%{DATA:37}";"%{DATA:38}";"%{DATA:39}";"%{DATA:40}";"%{DATA:41}";"%{DATA:42}";"%{DATA:43}";"%{DATA:44}";"%{DATA:45}";"%{DATA:46}";"%{DATA:47}";"%{TIMESTAMP_ISO8601:48}";"%{DATA:49}";"%{DATA:50}";"%{DATA:51}";"%{DATA:52}";"%{DATA:53}";"%{DATA:54}";"%{DATA:55}";"%{DATA:56}";"%{DATA:57}"

Everything works perfectly except logstash spits out an error for "%{DATA:28}". I tried DATA, QS, NOTSPACE you name it...

This field contains multiple formats of data, so I need logstash to parse all of them. Somethimes field 28 shows "1", sometimes it looks similar to "AAAAA-A0000A0A-1111-2222-A3AA-A444A4444AA4" and other times it's a number sequence similar to "000-1111-2222-33333". I checked the logs and this is what I get(see below). Basically I tell it to parse some bulk data and it ignores me and tries to parse it how it sees fit. At the bottom of the page is an extras from GrokDebugger and it sheds a little light in my issue.

org.elasticsearch.index.mapper.MapperParsingException: failed to parse [28]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:411)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:706)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:497)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:544)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)
        at org.elasticsearch.index.shard.IndexShard.prepareCreate(IndexShard.java:466)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:418)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:148)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase.performOnPrimary(TransportShardReplicationOperationAction.java:574)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$PrimaryPhase$1.doRun(TransportShardReplicationOperationAction.java:440)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [1383-5494-1131-58264], tried both date format [dateOptionalTime], and timestamp number with locale []
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:617)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:535)
        at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:239)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:401)
        ... 13 more
Caused by: java.lang.IllegalArgumentException: Invalid format: "1383-5494-1131-58264" is malformed at "4-1131-58264"
        at org.elasticsearch.common.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187)
        at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780)
        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:612)
        ... 16 more

GROK DEBUGGER:
 Basically, if I let logstash interpret my field as he consideres fit, it will do something like this. The problem is that even when I define my field as DATA, it treats it as below:

"AAAAA%{BASE16FLOAT}-1111%{ISO8601_TIMEZONE}1-A3AA-A444A4444AA4"
</description><key id="111597407">14131</key><summary>Logstash parse issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ARPLink</reporter><labels /><created>2015-10-15T11:25:30Z</created><updated>2015-10-15T14:58:00Z</updated><resolved>2015-10-15T14:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-15T14:57:59Z" id="148411994">Hi @ARPLink 

I suggest you ask this question in the forums: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Geo] Geo point distance calculations are wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14130</link><project id="" key="" /><description>Function Score query for geo point location and distance giving wrong calculations. 
"location": {
                "origin": "22.242074, 75.936335",
                "scale": "20km",
                "offset": "20km",
                "decay": 0.5
              }
is returning a score of one on co-ordinates 21.360972, 74.855748
</description><key id="111591787">14130</key><summary>[Geo] Geo point distance calculations are wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">neerajgoyal12</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2015-10-15T10:52:22Z</created><updated>2015-10-16T08:38:45Z</updated><resolved>2015-10-16T08:38:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-10-15T12:16:53Z" id="148368911">Can you add the full query here? I cannot reproduce this.
</comment><comment author="neerajgoyal12" created="2015-10-16T07:39:24Z" id="148638844">#### Create Index with Mappings

```
curl -XPUT localhost:9200/test -d '{
    "mappings" : {
      "user_profiles" : {
        "properties" : {
          "about_me" : {
            "type" : "string",
            "analyzer" : "standard"
          },
          "created_at" : {
            "type" : "string"
          },
          "dob" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "first_name" : {
            "type" : "string",
            "analyzer" : "standard"
          },
          "flagged" : {
            "type" : "integer"
          },
          "gender" : {
            "type" : "integer"
          },
          "id" : {
            "type" : "long"
          },
          "last_name" : {
            "type" : "string",
            "analyzer" : "standard"
          },
          "location" : {
            "type" : "geo_point",
            "fielddata" : {
              "format" : "compressed",
              "precision" : "1km"
            }
          },
          "quirky_fact" : {
            "type" : "string",
            "analyzer" : "standard"
          },
          "updated_at" : {
            "type" : "string"
          },
          "user_id" : {
            "type" : "long"
          }
        }
      }
    }
}'

```

#### Add Document 1 to Index

```
curl -XPUT localhost:9200/test/user_profiles/1 -d '{
  "id": 1,
  "first_name": "First_1",
  "last_name": "Last_1",
  "gender": 0,
  "dob": "1986-01-03",
  "about_me": "Veniam optio rerum.",
  "quirky_fact": "Delectus sed.",
  "location": "21.822714, 75.034688",
  "user_id": 1,
  "flagged": 0,
  "created_at": "2015-08-20 17:46:40",
  "updated_at": "2015-08-20 17:46:40"
}'

```

#### Add Document 2 to Index

```
curl -XPUT localhost:9200/test/user_profiles/2 -d '{
  "id": 2,
  "first_name": "First_287",
  "last_name": "Last_287",
  "gender": 1,
  "dob": "1993-04-12",
  "about_me": "Deserunt dolor.",
  "quirky_fact": "Omnis cum.",
  "location": "22.359215, 75.999696",
  "user_id": 287,
  "flagged": 0,
  "created_at": "2015-08-20 17:46:40",
  "updated_at": "2015-08-20 17:46:40"
}'

```

#### Query 1

```
  curl -XGET localhost:9200/test/user_profiles/_search?pretty  -d '{
    "from": 0,
    "size": 100,
    "_source": {
      "include": [
        "user_id"
      ]
    },
    "min_score": 2,
    "query": {
      "function_score": {
        "functions": [
          {
            "linear": {
              "dob": {
                "origin": "now",
                "scale": "1825d",
                "offset": "8395d"
              }
            }
          },
          {
            "linear": {
              "location": {
                "origin": "22.242074, 75.936335",
                "scale": "20km",
                "offset": "20km",
                "decay": 0.5
              }
            }
          }
        ],
        "query": {
          "bool": {
            "must": {
              "term": {
                "gender": "1"
              }
            },
            "must_not": [
              {
                "term": {
                  "user_id": "1"
                }
              },
              {
                "term": {
                  "gender": 0
                }
              }
            ]
          }
        },
        "score_mode": "sum"
      }
    },
    "sort": {
      "_score": {
        "order": "desc"
      },
      "created_at": {
        "order": "desc"
      }
    }
  }'
```

#### Query 2

```
  curl -XGET localhost:9200/test/user_profiles/_search?pretty  -d '{
    "from": 0,
    "size": 100,
    "_source": {
      "include": [
        "user_id"
      ]
    },
    "min_score": 2,
    "query": {
      "function_score": {
        "functions": [
          {
            "linear": {
              "dob": {
                "origin": "now",
                "scale": "1825d",
                "offset": "8395d"
              }
            }
          },
          {
            "linear": {
              "location": {
                "origin": "22.242074, 75.936335",
                "scale": "20km",
                "offset": "20km",
                "decay": 0.5
              }
            }
          }
        ],        
        "score_mode": "sum"
      }
    },
    "sort": {
      "_score": {
        "order": "desc"
      },
      "created_at": {
        "order": "desc"
      }
    }
  }'
```

#### Answer

```

{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : null,
    "hits" : [ {
      "_index" : "test",
      "_type" : "user_profiles",
      "_id" : "2",
      "_score" : 2.0,
      "_source":{"user_id":287},
      "sort" : [ 2.0, "46" ]
    } ]
  }
}
```

The score for the above query is 2.0 which i think is wrong. 
</comment><comment author="clintongormley" created="2015-10-16T08:38:45Z" id="148654723">This is working correctly.  Here's a simpler version of your example:

```
DELETE test

PUT /test
{
    "mappings" : {
      "user_profiles" : {
        "properties" : {
          "location" : {
            "type" : "geo_point",
            "fielddata" : {
              "format" : "compressed",
              "precision" : "1km"
            }
          }
        }
      }
    }
}

PUT /test/user_profiles/1
{
  "location": "21.822714, 75.034688"
}

PUT /test/user_profiles/2
{
  "location": "22.359215, 75.999696"
}

GET /test/user_profiles/_search?_source=0&amp;explain=0
{
  "query": {
    "function_score": {
      "functions": [
        {
          "linear": {
            "location": {
              "origin": "22.242074, 75.936335",
              "scale": "20km",
              "offset": "20km",
              "decay": 0.5
            }
          }
        }
      ],
      "score_mode": "sum"
    }
  },
  "sort": [
    "_score",
    {
      "_geo_distance": {
        "location": "22.242074, 75.936335",
        "unit": "km", 
        "order": "desc"
      }
    }
  ]
}
```

Results:

```
  "hits": [
     {
        "_index": "test",
        "_type": "user_profiles",
        "_id": "2",
        "_score": 1,
        "sort": [
           1,
           14.574812786503244
        ]
     },
     {
        "_index": "test",
        "_type": "user_profiles",
        "_id": "1",
        "_score": 0,
        "sort": [
           0,
           104.04650815165414
        ]
     }
  ]
```

Document 2 is within the `offset` of 20km, and so gets the full score of `1.0`, and document 1 is far enough away that the score has dropped to `0.0`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>feature request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14129</link><project id="" key="" /><description>please add a disable option for cluster.routing.allocation.allow_rebalance for some usecase.
</description><key id="111570240">14129</key><summary>feature request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-10-15T08:42:25Z</created><updated>2015-10-15T13:29:45Z</updated><resolved>2015-10-15T13:29:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2015-10-15T09:59:40Z" id="148339140">usecase:
disable rebalance and then move all shard from one node to another now, and then shutdown the old one
</comment><comment author="clintongormley" created="2015-10-15T13:29:45Z" id="148385932">you can use shard allocation filtering to do that. see https://www.elastic.co/guide/en/elasticsearch/reference/2.0/allocation-filtering.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregation counts mismatch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14128</link><project id="" key="" /><description>Hi Team,

I'm seeing something different in the aggregation functionality.

I have done a search as follows

```
POST myindex/mytype/_search
{
  "size": 0,
  "aggs": {
    "my_agg": {
      "terms": {
        "field": "myfield.mysubfield",
        "size": 200
      }
    }
  }
}
```

I have got the following count in the result set

```
{
       "key": "Engineer",
       "doc_count": 16199
}
```

but expected count is **16388**

When i did something like the following, I'm getting the expected count which is 16388

```
POST myindex/mytype/_search
{
  "size": 0, 
  "query": {
    "match": {
      "myfield.mysubfield": "Engineer"
    }
  }, 
  "aggs": {
    "my_agg": {
      "terms": {
        "field": "myfield.mysubfield",
        "size": 200
      }
    }
  }
}
```

Am I doing something wrong in the first query?

I have experimented one more case in the first query, when i changed size part in the query as follows(**2000**) I'm getting the expected count

```
POST myindex/mytype/_search
{
  "size": 0,
  "aggs": {
    "my_agg": {
      "terms": {
        "field": "myfield.mysubfield",
        "size": 2000
      }
    }
  }
}
```

And I'm saying Boom!!!

Is this expected?
</description><key id="111557900">14128</key><summary>Aggregation counts mismatch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PandiyanCool</reporter><labels /><created>2015-10-15T07:09:57Z</created><updated>2015-10-15T07:21:50Z</updated><resolved>2015-10-15T07:21:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-15T07:21:22Z" id="148303275">Yes. Read this: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-approximate-counts
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ghosts of discovery-ec2 in 2.x branches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14127</link><project id="" key="" /><description>Looks like a test that should be in cloud-aws instead?

https://github.com/elastic/elasticsearch/tree/2.0/plugins/discovery-ec2/src/test/java/org/elasticsearch/discovery/ec2
</description><key id="111551834">14127</key><summary>ghosts of discovery-ec2 in 2.x branches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>test</label></labels><created>2015-10-15T06:08:52Z</created><updated>2015-11-23T12:08:17Z</updated><resolved>2015-11-23T12:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-15T06:59:15Z" id="148298641">Argh! Indeed! That's what happens when you cherry pick from a branch which have been refactored! :p 

Will fix. Thanks for catching it!
</comment><comment author="dadoonet" created="2015-11-23T12:08:17Z" id="158916195">fixed by #14314 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix SeccompTests bug on older kernels / add defense</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14126</link><project id="" key="" /><description>This test failed only on a feature branch, because that feature branch has
a different build randomization script and is the only place
randomizing tests.security.manager (this test cannot run with it enabled).

On old kernels without TSYNC support, the test fails because (surprise to me) the thread that
runs the test is not the same thread that runs static initializers:
https://github.com/randomizedtesting/randomizedtesting/blob/7571489190d677969c768836e1576f4e851f83e8/randomized-runner/src/main/java/com/carrotsearch/randomizedtesting/RandomizedRunner.java#L573-L574

To fix this test (its not an issue in practice, since we do this before creating threadpools),
we just record for testing purposes that we couldn't TSYNC, and re-run the whole thing for the test thread in setUp(), failing if something goes wrong.

Also add a bunch of additional paranoia and narrow our defensive checks better here after reading
through more chrome bug reports: they don't impact us but those linux distros are too cowboy
with the backports and the spirit of the checks makes me feel better.
</description><key id="111551113">14126</key><summary>Fix SeccompTests bug on older kernels / add defense</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-10-15T06:00:54Z</created><updated>2015-10-15T12:37:53Z</updated><resolved>2015-10-15T12:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-15T06:34:01Z" id="148294848">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document _cluster/health?local</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14125</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/cluster-health.html) we make no mention of the `?local` flag, but it seems that [in the code](https://github.com/elastic/elasticsearch/blob/v1.7.2/src/main/java/org/elasticsearch/rest/action/admin/cluster/health/RestClusterHealthAction.java#L52) and also [in the API spec](https://github.com/elastic/elasticsearch/blob/v1.7.2/rest-api-spec/api/cluster.health.json#L21-L24) it's available.

Plus if you tack it onto  a call to nodes in your cluster it returns a valid response.
</description><key id="111547028">14125</key><summary>Document _cluster/health?local</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/palecur/following{/other_user}', u'events_url': u'https://api.github.com/users/palecur/events{/privacy}', u'organizations_url': u'https://api.github.com/users/palecur/orgs', u'url': u'https://api.github.com/users/palecur', u'gists_url': u'https://api.github.com/users/palecur/gists{/gist_id}', u'html_url': u'https://github.com/palecur', u'subscriptions_url': u'https://api.github.com/users/palecur/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1779279?v=4', u'repos_url': u'https://api.github.com/users/palecur/repos', u'received_events_url': u'https://api.github.com/users/palecur/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/palecur/starred{/owner}{/repo}', u'site_admin': False, u'login': u'palecur', u'type': u'User', u'id': 1779279, u'followers_url': u'https://api.github.com/users/palecur/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-10-15T05:20:31Z</created><updated>2015-12-14T13:25:07Z</updated><resolved>2015-12-14T13:25:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fforbeck" created="2015-12-12T00:18:39Z" id="164086622">local:
    If `true` returns the local node information and does not provide
    the state from master node. Default: `false`.

@markharwood, Is this description good enough?
</comment><comment author="clintongormley" created="2015-12-14T13:23:06Z" id="164435148">@fforbeck looks good, want to send a PR?
</comment><comment author="clintongormley" created="2015-12-14T13:24:22Z" id="164435558">ah, I see you have: https://github.com/elastic/elasticsearch/pull/15403
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resync Geopoint hashCode/equals method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14124</link><project id="" key="" /><description>`Geopoint.equals` was modified to consider two points equal if they are within a threshold. This change was done to accept round-off error introduced from GeoHash encoding methods. This PR removes this trappy leniency from `GeoPoint.equals` and instead forces round-off error to be handled at the encoding source. It also fixes the broken contract between the `GeoPoint.hashCode` and `.equals` methods raised in #14083 

closes #14083
</description><key id="111514779">14124</key><summary>Resync Geopoint hashCode/equals method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T23:24:47Z</created><updated>2015-10-19T19:53:54Z</updated><resolved>2015-10-19T19:53:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-17T18:57:13Z" id="148943932">left two comments otherwise LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>List primary copy of each shard first in output of routing table</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14123</link><project id="" key="" /><description>The routing table returned from our apis (like _status) seem to list the copies of a single shard sorted by the node_id.  So the primary copy can appear anywhere in the list, sometimes as the first copy in the list, sometimes as the last copy in the list.  This causes some issues in monitoring scripts I have seen out there where they are expecting the first copy of the shard listed to be the primary copy.  While it is an easy fix in the monitoring script, it may actually be nice if we consistently present the primary copy as the first item in the list of copies.

Example of a shard that has 3 copies (2 replicas):

```
"1" : [ {
          "routing" : {
            "state" : "STARTED",
            "primary" : false,
            "node" : "CUWZHwwjREGElE9zu_PNIg",
            "relocating_node" : null,
            "shard" : 1,
            "index" : "awareness"
          }
        }, {
          "routing" : {
            "state" : "STARTED",
            "primary" : false,
            "node" : "lzBXmSxyScyit_BOMaH4yw",
            "relocating_node" : null,
            "shard" : 1,
            "index" : "awareness"
          }        }, {
          "routing" : {
            "state" : "STARTED",
            "primary" : true,
            "node" : "TFxkZt-lSz-hs-tqGCcdkg",
            "relocating_node" : null,
            "shard" : 1,
            "index" : "awareness"
          }
        } ],
```
</description><key id="111497505">14123</key><summary>List primary copy of each shard first in output of routing table</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Stats</label><label>discuss</label><label>enhancement</label></labels><created>2015-10-14T21:28:59Z</created><updated>2015-10-16T09:12:57Z</updated><resolved>2015-10-16T09:12:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-15T13:14:52Z" id="148382169">Seems reasonable - I assume this wouldn't be too expensive?
</comment><comment author="jasontedor" created="2015-10-15T13:36:27Z" id="148388490">I would think for monitoring it would be better to be using `_cat/shards` which is much more amenable to sorting. For example:

```
curl -sS -XGET localhost:9200/_cat/shards?h=index,shard,prirep,state,id | sort -k1 -k2n -k3
```

would sort each shard by primary/replica status.

To be clear, it's possible, not hard, not expensive, but it seems we already have an API that is better suited to such needs?
</comment><comment author="ppf2" created="2015-10-15T17:18:54Z" id="148463220">Thanks @jasontedor .  In this case, the user is on Windows and using powershell's convert-to-json functionality which makes it easy to iterate the json output and has a pretty complete set of monitoring scripts based on the non-cat apis.  It's certainly easy to update those scripts to workaround this behavior though.  So this is just to gauge the level of effort for us to display the primary copy first which may benefit other folks who use the non-cat apis. Certainly not a high priority item to address, just something nice to have if we have spare time :)
</comment><comment author="bleskes" created="2015-10-15T17:56:49Z" id="148472716">@jasontedor I quickly glanced at things - so I might be missing something but isn't this a question of sorting the shard list in the IndexShardRoutingTable constructor/builder: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/cluster/routing/IndexShardRoutingTable.java#L77

This list is super short, so the overhead should be negligible....
</comment><comment author="jasontedor" created="2015-10-16T09:12:57Z" id="148662527">We discussed this during fix-it-Friday and have come to the conclusion to not favor any one particular sort order over any other. Since it is not difficult to sort externally (especially with the `_cat/shards` API), we will close this issue as won't-fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support numeric/max values for force awareness settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14122</link><project id="" key="" /><description>For some environments (like Azure), force awareness is desirable to handle fault domains and update domains.  The fault and update domain specifications in Azure are in the form of numbers (eg. 0, 1, 2, 3, etc..):

Currently, using force awareness, we will have to specify:

```
    "cluster.routing.allocation.awareness.attributes":"updateDomain,faultDomain",
    "cluster.routing.allocation.awareness.force.updateDomain.values": "0,1,2,3,4"
    "cluster.routing.allocation.awareness.force.faultDomain.values": "0,1"
```

It will be helpful to provide a way for the force awareness settings to handle numeric values.   In this case, it will be nice to be able to just list the maximum number of fault and update domains. That way, when nodes come and go, we will always have the maximum list &#8211; even when we add nodes after the deployment which might use update domains not used yet in that deployment.
</description><key id="111497312">14122</key><summary>Support numeric/max values for force awareness settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>enhancement</label></labels><created>2015-10-14T21:27:48Z</created><updated>2015-10-15T17:02:41Z</updated><resolved>2015-10-15T17:02:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-15T17:02:41Z" id="148457896">Hi @ppf2 

I'm not sure I see the benefit of what you're suggesting here.  The `values` always have to be explicitly listed.  If values are missing then we assume there are nodes missing and take the appropriate action, so there is no auto-adjustment if nodes come and go.  

So then it comes down to having an alternate syntax for a short list of values that happen to be numeric.  This is so easy to handle client side, i don't understand why we would possibly add more complex settings in Elasticsearch to handle this one particular use case that doesn't apply to most users, who will use descriptive values instead (`zone_1`, `region_x`).

Closing as won't fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shards with heavy indexing should get more of the indexing buffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14121</link><project id="" key="" /><description>Today we take the total indexing buffer (default: 10% of heap) and divide it equally across all active shards.

But this is a sub-optimal usage of RAM for indexing: maybe the node has a bunch of small shards (e.g. Marvel) which require hardly any indexing heap, but were assigned a big chunk of heap (which typically goes mostly unused), while other heavy indexing shards were assigned the same indexing buffer but could effectively make use of much more.

This problem is very nearly the same issue `IndexWriter` faces, being told it has an X MB overall indexing buffer to use and then having to manage the N separate in-memory segments (one per thread).

I think we (ES) should take the same approach as `IndexWriter`, except across shards on the node: tell Lucene each shard has an effectively unlimited indexing buffer, but then periodically sum up the actual bytes used across all and when it's over the node's budget, ask the most-heap-consuming shard(s) to refresh to clear the heap.

This should also reduce merge pressure across the node since we'd typically be flushing fewer, larger segments, and helps smooth out IO pressure somewhat (instead of N shards trying to write at once, we stage it over time).

I also removed all configuration associated with the translog buffer (`index.translog.fs.buffer_size`): it's now hardwired to 32 KB.  I don't understand why we need this buffer to be tunable: let the OS manage RAM assigned for IO write buffering / dirty pages?
</description><key id="111491222">14121</key><summary>Shards with heavy indexing should get more of the indexing buffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T20:54:28Z</created><updated>2016-01-12T10:17:18Z</updated><resolved>2016-01-12T09:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-14T21:11:41Z" id="148201795">I wonder if we should ever ask for more refreshes than we need to get below the buffer. I _think_ the old buffer would keep the heap usage of the buffer under or at the buffer's size but this scheme will keep the buffer closer to full all the time. In the most degenerate case of a bunch shards (n) being written to at the same rate this implementation will end up with more "floating".
</comment><comment author="mikemccand" created="2015-10-14T21:36:50Z" id="148209547">&gt;  I think the old buffer would keep the heap usage of the buffer under or at the buffer's size but this scheme will keep the buffer closer to full all the time.

Actually Lucene's `IndexWriter` works just like this change, riding the buffer near full, i.e. it waits for the heap used to cross the limit, then it picks the biggest in-memory segment(s) to write and writes it until heap used is under the budget again.

If we wanted we could add some hysteresis, e.g. when we cross full, we drive the buffer back down to maybe 75% of full, to give it a sharper sawtooth pattern ... but I don't think this is really necessary.

Typically the scheduled refresh (default: every 1s) is going to free the RAM well before `IndexingMemoryController` logic here kicks in.
</comment><comment author="nik9000" created="2015-10-14T21:45:37Z" id="148211510">&gt; Typically the scheduled refresh (default: every 1s) is going to free the RAM well before IndexingMemoryController logic here kicks in.

Yeah - I suspect so. I just thought about it and figured it was worth mentioning.
</comment><comment author="mikemccand" created="2015-10-20T16:28:51Z" id="149623591">I chatted with @s1monw about this ... I think we can add an API to `IndexWriter` to give us more granular control (write just the biggest segment to disk), and more specific control (just write the segment to disk, don't refresh it) to move our dirty bytes to the OS so it can move them to disk.  I opened https://issues.apache.org/jira/browse/LUCENE-6849 to work on this.

I agree the stalling issue is important, so just pretending "0 bytes heap used" as soon as we start moving bytes to disk, is dangerous.  I'll change the PR to track "pending dirty bytes moving to disk", and if that pending bytes is too large vs our budget, we need throttle incoming indexing, hopefully just tapping into the index throttling we already have for when merges are falling behind.  The OS will do its own scary back-pressure here (blocking any thread, or maybe/probably the whole process, that's attempting to write to disk) when its efforts to move dirty bytes to disk are falling behind.
</comment><comment author="mikemccand" created="2015-12-17T18:30:57Z" id="165540164">I finally got back to this PR (merged master = painful!) and addressing feedback from @nik9000 and @s1monw:
- Now we sometimes use the (new) `IndexWriter.flush` API to move indexing buffers to disk, which is cheaper than a refresh since that must also apply all buffered deletes and then open `SegmentReader`s.
- I added back-pressure for when incoming indexing rate exceeds how quickly we can move the indexing buffer to disk: I just tap into the existing single-threaded index throttling we already do when merges are falling behind on a single shard

I think this is ready ... it's a big change, but I think an important one for having ES use its allotted (default 10% of JVM's heap) indexing buffer more efficiently when there are many shards with different indexing rates on one node.
</comment><comment author="s1monw" created="2015-12-21T16:27:07Z" id="166350297">@mikemccand this looks awesome - I left some comments. I wonder if we should put this in a feature branch and let CI chew on it for a while?
</comment><comment author="mikemccand" created="2016-01-06T09:26:05Z" id="169275306">OK I merged master and folded in all feedback, except removing IMC dependency from IndexShard (@s1monw has some pre-cursor cleanup ideas here)... I'm going to push to a feature branch so Jenkins can chew on it.  I'll also run indexing perf tests.
</comment><comment author="mikemccand" created="2016-01-06T18:53:49Z" id="169418795">CI is running here: https://build-us-01.elastic.co/job/elastic-elasticsearch-fair_indexing_buffers/

So far so good!
</comment><comment author="mikemccand" created="2016-01-06T23:40:02Z" id="169498385">I ran an indexing benchmark and it looks like there's no real change to indexing throughput, which is good!

Master got 26.0 K doc/sec, and 26.5 K docs/sec (2nd try), and this PR got 26.9 K docs/sec and 26.3 K docs/sec ... seems like any change is in the noise.

This is really just a "first do no harm" test, because this indexing test is indexing full speed / uniformly into 5 shards, whereas the PR should do better is when indexing heavily into one set of shards and slowly into another ...
</comment><comment author="dakrone" created="2016-01-06T23:48:17Z" id="169501134">&gt; ... whereas the PR should do better is when indexing heavily into one
&gt; set of shards and slowly into another ...

Awesome! I'm curious what the indexing throughput change is if the
Marvel plugin is used to have slowly indexing shards while the benchmark
is running.
</comment><comment author="mikemccand" created="2016-01-08T20:52:25Z" id="170120674">&gt;  I'm curious what the indexing throughput change is if the
&gt; Marvel plugin is used to have slowly indexing shards while the benchmark
&gt; is running.

I tested this and the effect looks minor.  At first I ran the Marvel plugin at defaults, but this only creates 2 lightweight shards (plus my 6 heavily indexing shards) ... so then I set a custom template to have Marvel use 5 shards, and saw maybe a smallish impact:

```
iwbuffer.12.marvel.fast.2.log:Total docs/sec: 32240.0
iwbuffer.12.marvel.fast.log:Total docs/sec: 30976.7
master.12.marvel.fast.2.log:Total docs/sec: 31118.2
master.12.marvel.fast.log:Total docs/sec: 30037.0
```

The `iwbuffer` log is this PR, the 12 is using 12 client-side indexing threads.  So maybe a minor gain in this test, or maybe within measurement noise.  This was using the same settings as "Fast" from https://benchmarks.elastic.co : refresh every 30 sec, 4 gb heap, 4 gb xlog flush threshold.
</comment><comment author="mikemccand" created="2016-01-11T15:17:14Z" id="170584310">I merged master and fixed IMC to "just" be another `IndexOperationListener` ... I think this is ready.

However, a number of the CI jobs have been failing: https://build-us-01.elastic.co/view/Elasticsearch/job/elastic+elasticsearch+fair_indexing_buffers+periodic/

The failures are weird, like something is interrupting the build (`Thread.interrupt`) vs true test failures caused by this change, I think... but I'll dig.
</comment><comment author="s1monw" created="2016-01-11T19:27:14Z" id="170662396">@mikemccand awesome! I left a bunch of comments but this looks fantastic
</comment><comment author="s1monw" created="2016-01-12T08:26:47Z" id="170835895">LGTM
</comment><comment author="mikemccand" created="2016-01-12T10:17:18Z" id="170864141">I removed 2.3.0 from this ... it's a big change, and its precursors haven't been ported to 2.3.0, so I think it should be in our next major release only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test that the lucene "unmap hack" is supported.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14120</link><project id="" key="" /><description>We should know if this is not working for any configuration, otherwise resources such as address space, file handles, and even disk space become tied to Java's garbage collector.
</description><key id="111468287">14120</key><summary>Test that the lucene "unmap hack" is supported.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T19:03:53Z</created><updated>2015-10-15T12:47:54Z</updated><resolved>2015-10-15T12:47:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-14T19:11:16Z" id="148162744">LGTM - this is important indeed!
</comment><comment author="jasontedor" created="2015-10-14T19:12:15Z" id="148163201">LGTM; it might be worth providing a link to the [JDK bug](http://bugs.java.com/bugdatabase/view_bug.do?bug_id=4724038) as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup IndexMetaData</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14119</link><project id="" key="" /><description>This commit cleans up IndexMetaData. In particular, all duplicate
getters (X and getX) have been collapsed into one (getX). Further, the
number of shards and number of replicas settings are now parsed once
and saved off as fields.
</description><key id="111467501">14119</key><summary>Cleanup IndexMetaData</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T19:00:30Z</created><updated>2015-10-15T00:44:26Z</updated><resolved>2015-10-14T23:29:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-14T22:00:21Z" id="148215514">LGTM. I'm +1 on pushing this to 2.x and 2.1
</comment><comment author="jasontedor" created="2015-10-15T00:44:26Z" id="148242533">Integrated into master, 2.1 and 2.x. Thanks for reviewing @bleskes. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API _timeout + _msearch returns no aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14118</link><project id="" key="" /><description>It appears that when I add a timeout to an `_msearch` request in the Java client, all my aggregations are ignored and the filtering doesn't take place.
### Without Timeout

**request**

``` java
MultiSearchRequestBuilder builder = esClient
    .prepareMultiSearch()
    .setIndicesOptions(IndicesOptions.lenientExpandOpen());

SearchRequest req = esClient
                .prepareSearch(req.getIndices())
                .setTypes(req.getType())
                .setSource(mySearchString)
                .setSearchType(SearchType.COUNT)
                .request();

builder.add(req);

MultiSearchResponse response = esClient
                .multiSearch(builder.request()).actionGet();

System.out.println(response.toString());
```

**response**

```
{
  "responses" : [ {
    "took" : 176,
    "timed_out" : false,
    "_shards" : {
      "total" : 12,
      "successful" : 12,
      "failed" : 0
    },
    "hits" : {
      "total" : 4,
      "max_score" : 0.0,
      "hits" : [ ]
    },
    "aggregations" : {
      "agg_name" : {
          "buckets": [
              // etc
          ]
      }
    }
  } ]
```
### With Timeout

**request**

``` java
MultiSearchRequestBuilder builder = esClient
    .prepareMultiSearch()
    .setIndicesOptions(IndicesOptions.lenientExpandOpen());

SearchRequest req = esClient
                .prepareSearch(req.getIndices())
                .setTypes(req.getType())
                .setSource(mySearchString)
                .setSearchType(SearchType.COUNT)
                .setTimeout(TimeValue.timeValueSeconds(60))
                .request();

builder.add(req);

MultiSearchResponse response = esClient
                .multiSearch(builder.request()).actionGet();

System.out.println(response.toString());
```

**response**

```
{
  "responses" : [ {
    "took" : 43,
    "timed_out" : false,
    "_shards" : {
      "total" : 12,
      "successful" : 12,
      "failed" : 0
    },
    "hits" : {
      "total" : 38,
      "max_score" : 0.0,
      "hits" : [ ]
    }
  } ]
}
```

As far as I know, this should work and I can confirm there is no difference in the query being sent (this is in a unit test). Simply adding the timeout changes the response as above. Maybe I'm being stupid, but this looks like it has to be an ES bug?

I'm using ES 1.7.2, both the node and client lib.
</description><key id="111447701">14118</key><summary>Java API _timeout + _msearch returns no aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitfin</reporter><labels /><created>2015-10-14T17:22:55Z</created><updated>2015-10-15T17:18:34Z</updated><resolved>2015-10-15T12:01:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-15T12:01:06Z" id="148366146">Hi @zackehh 

Your call to `setTimeout()` is wiping out the source set by `setSource()`.  Move `setSource()` to the end and it will work.  This annoying behaviour will be fixed by a big search refactoring that is going into 3.0.
</comment><comment author="javanna" created="2015-10-15T12:05:05Z" id="148367092">I am not sure inverting the order of the `setSource` and `setTimeout` helps. The main problem here is that you are specifying the search request as a string. After that you set the timeout, which is under the hood creating a new SearchSourceBuilder which wins against the one provided as a string. You should rather use the SearchSourceBuilder only, to both  set the timeout and provide the request, and avoid providing the request as a string, which is also something that will go away in 3.0.
</comment><comment author="whitfin" created="2015-10-15T15:11:15Z" id="148417270">Hey @clintongormley thanks for the reply - I'll try this out today. 

@javanna unfortunately I can't do that at the moment because everything we have focuses on building the Strings, so it'd be a nightmare to strip out just for this. I'll get back to you on whether the inverting works.
</comment><comment author="javanna" created="2015-10-15T15:22:01Z" id="148420218">I think what might work instead then, as a temporary solution, is using the `setExtraSource` for your string request. That one won't be overridden I believe. Keep in mind that all this is going away in 3.0 though.
</comment><comment author="whitfin" created="2015-10-15T15:41:16Z" id="148427477">Ok @clintongormley reversing the order didn't help, but @javanna's tip of `setExtraSource` did work.

What exactly is the different between `setSource` and `setExtraSource`? I've never seen the latter used anywhere. 

I'll keep in mind that this is going in 3.0 (that's pretty far off though, right?), so I'll set it in motion to move to not using Strings. Thanks for your help guys.
</comment><comment author="javanna" created="2015-10-15T16:38:33Z" id="148449052">Glad you found a solution! extra source is something that we use internally for stuff that can come from both the request body and query_string parameters (e.g. the `query`, `size` etc.). It effectively allows to add additional parameters without touching the original source, handy given that the original source might be a bytes array that would require parsing to be modified.

The reason why this is all going away is that we are moving all the parsing to the coordinating node, while at the moment (2.x too) we still stream json between the different nodes and parse it multiple times, on each data node. At that point once a request is parsed to java objects it is fully represented as a java object (no bytes array or strings, so no parsing needed anymore).
</comment><comment author="whitfin" created="2015-10-15T17:18:34Z" id="148463122">@javanna sweet! Since it's internal, I guess I shouldn't abuse it too much? 

Thank you for the extra info, and for your help with getting me going :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NotSerializableException java.util.Collections$UnmodifiableList when starting new node in cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14117</link><project id="" key="" /><description>We get this stacktrace on the starting node.

```
2015-10-14 16:31:28,448 [elasticsearch[Orb][generic][T#2]] WARN  org.elasticsearch.indices.cluster - [Orb] [[ion][3]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [ion][3]: Recovery failed from [Stature][ZK-Drkc5TNqMGL1d-xqxGA][LONW00071288][inet[/10.102.43.214:9300]]{local=false} into [Orb][_CvekKpITyip5sQuop7oNQ][LONW00071288][inet[/10.102.43.214:9301]]{local=false}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:280) [elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:70) [elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:561) [elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36) [elasticsearch-1.7.1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_65]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]
    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
Caused by: org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:178) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:130) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-1.7.1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_65]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_65]
    at java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
Caused by: java.io.NotSerializableException: java.util.Collections$UnmodifiableList
    at org.elasticsearch.common.io.ThrowableObjectInputStream.verify(ThrowableObjectInputStream.java:139) ~[elasticsearch-1.7.1.jar:na]
    at org.elasticsearch.common.io.ThrowableObjectInputStream.readClassDescriptor(ThrowableObjectInputStream.java:74) ~[elasticsearch-1.7.1.jar:na]
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1601) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500) ~[na:1.7.0_65]
    at java.lang.Throwable.readObject(Throwable.java:914) ~[na:1.7.0_65]
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source) ~[na:na]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_65]
    at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_65]
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500) ~[na:1.7.0_65]
    at java.lang.Throwable.readObject(Throwable.java:914) ~[na:1.7.0_65]
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source) ~[na:na]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_65]
    at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_65]
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) ~[na:1.7.0_65]
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) ~[na:1.7.0_65]
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:176) ~[elasticsearch-1.7.1.jar:na]
    ... 23 common frames omitted

```

It appears that we get two instances of the same Package object (java.util) so the logic here fails:

``` java
        if (aClass.isPrimitive() // primitives are fine
                || aClass.isArray() // arrays are ok too
                || Throwable.class.isAssignableFrom(aClass)// exceptions are fine
                || CLASS_WHITELIST.contains(aClass) // whitelist JDK stuff we need
                || PKG_WHITELIST.contains(aClass.getPackage())
                || pkg.getName().startsWith("org.elasticsearch")) { // es classes are ok
            return streamClass;
        }
        throw new NotSerializableException(aClass.getName());

```

The PKG_WHITELIST.contains() relies on the equals implementation for Package - which is based on identity. The same is true for the CLASS_WHITELIST .... Perhaps we should be using the class or package names (ie list of String) ?
</description><key id="111438396">14117</key><summary>NotSerializableException java.util.Collections$UnmodifiableList when starting new node in cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nickminutello</reporter><labels><label>:Exceptions</label><label>discuss</label><label>v1.7.1</label></labels><created>2015-10-14T16:36:28Z</created><updated>2016-01-30T16:18:38Z</updated><resolved>2016-01-30T16:18:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-14T16:44:57Z" id="148112719">@nickminutello can you tell us which version this is?
</comment><comment author="jasontedor" created="2015-10-14T16:45:43Z" id="148112889">@nik9000 It's visible in `elasticsearch-1.7.1.jar:na` in the stack trace.
</comment><comment author="nickminutello" created="2015-10-14T22:47:33Z" id="148225108">Sorry. Yes 1.7.1
</comment><comment author="clintongormley" created="2015-10-15T13:18:50Z" id="148383293">@nickminutello Is this with a custom plugin that uses its own exceptions?
</comment><comment author="nik9000" created="2015-10-15T13:37:10Z" id="148388794">&gt; Sorry. Yes 1.7.1

Thanks! Sorry. I should have seen it in the stack trace. I'll try and remember to look there next time. Thanks for filing the whole stack trace, btw.

&gt; @nickminutello Is this with a custom plugin that uses its own exceptions?

I don't think so - if I had to guess I'd say its just an exception we didn't test well. IIRC in 2.0 exception passing is done differently so the unserializable exception is probably exclusive to 1.7.

Im' not 100% sure on the root cause. I guess it could be a jar hell thing - I've not looked into the full story of equals and hashcode for package and class objects but if it was easy to do we'd have seen it in testing or production somewhere else. 

So I'm not sure. What does the environment look like where you run this? Do you modify the white and black lists? If so, what do you have them set to?
</comment><comment author="nickminutello" created="2015-10-20T07:40:43Z" id="149464126">No. No custom plugins.
Not sure of the root cause of the issue either.
A possibility is perhaps two different cluster members running on different version jvms?

But a static analysis of the code would suggest its not going to work as expected? (or am I missing something?).
</comment><comment author="jasontedor" created="2015-10-21T15:53:29Z" id="149940987">It is possible for different versions of the JVM to cause serialization issues in the transport layer (addressed in #11910). The most common cause that we've seen is due to changes in 6u45 and 7u21 for the serialization of [`java.net.InetAddress`](http://hg.openjdk.java.net/jdk7u/jdk7u-dev/jdk/rev/7ca8a40795d8). I see that the machine this exception occurred on was running 7u65; are you running any machines in your cluster on a version of the JVM prior to 7u21? That said, the exception message that you present here is slightly different than what we normally see when the differences in serialization lead to issues.
</comment><comment author="nickminutello" created="2015-10-30T17:52:33Z" id="152600909">The issue occurs when one node JVM is 1.8 and the other is 1.7 (see above).

However, is it the case that this is just exposing the _actual_ bug (relying on Object.equals()) above?
</comment><comment author="jasontedor" created="2015-10-30T18:50:25Z" id="152615148">Is it possible for you to either take the 1.7 system to 1.8 or the 1.8 system to 1.7 and see if the issue reproduces? This still feels like it might be a cross JVM version serialization issue so let's confirm that or rule it out. Mixing minor versions is generally okay (not 100%, e.g., 6u45 and 7u21) but crossing major versions seems ripe for trouble.
</comment><comment author="nickminutello" created="2015-11-04T13:36:28Z" id="153721338">Running both on 1.8 reproduces this bug, so the JVM mismatch is a bit of a red herring.
</comment><comment author="nickminutello" created="2015-11-04T13:44:30Z" id="153722888">The root cause exception is:

```
org.elasticsearch.indices.recovery.DelayRecoveryException: source node does not have the shard listed in its state as allocated on the node
    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:108)
    at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:279)
    at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

But I can see now that the bug that is the subject of this issue is not actually in elastic - but in commons-io?

Fundamentally, the **private ObjectStreamClass verify(ObjectStreamClass streamClass)** is broken.
The CLASS_WHITELIST.contains(aClass) and PKG_WHITELIST.contains(aClass.getPackage()) is unlikely to work as the author intends.
</comment><comment author="clintongormley" created="2016-01-29T14:15:15Z" id="176776029">@jasontedor @nickminutello anything more on this bug?  Haven't heard reports of this elsewhere, and 2.0 doesn't use java serialization, so I'm tempted to close this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Size/from limits should be per-request not per-index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14116</link><project id="" key="" /><description>The change in https://github.com/elastic/elasticsearch/pull/13188 added a per-index setting to limit the maximum number of search results that can be requested.  However, it would still be possible to request too many results by querying multiple indices.

Instead, this should be a dynamic cluster-level setting which applies to each search request, regardless of the number of indices involved.
</description><key id="111429973">14116</key><summary>Size/from limits should be per-request not per-index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2015-10-14T15:56:27Z</created><updated>2015-10-15T12:51:55Z</updated><resolved>2015-10-15T12:51:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-14T20:22:18Z" id="148184152">Do you mean you want to limit `${size} x ${number of shards involved in the search request}` instead of just the value of `${size}`?
</comment><comment author="clintongormley" created="2015-10-15T12:51:55Z" id="148375659">OK - appears I misunderstood the current implementation.  I thought that a limit of 10,000 per index when searching 100 indices could result in 1,000,000 hits being returned.  Actually, it will return a max of 10,000.  Closing this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DedicatedClusterSnapshotRestoreIT.restoreIndexWithMissingShards rarely fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14115</link><project id="" key="" /><description>DedicatedClusterSnapshotRestoreIT.restoreIndexWithMissingShards failed twice in the last 10 days with the following error:

```
Expected: &lt;100L&gt; but: was &lt;87L&gt;
    at __randomizedtesting.SeedInfo.seed([5C47F8A2C729A266:D663A36E857381AD]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.junit.Assert.assertThat(Assert.java:832)
    at org.elasticsearch.snapshots.DedicatedClusterSnapshotRestoreIT.restoreIndexWithMissingShards(DedicatedClusterSnapshotRestoreIT.java:525)
```

I didn't manage to reproduce the failure. Seems like after a restore not all expected documents can be found in the index using the count api:

http://build-us-00.elastic.co/job/es_core_master_centos/8019
</description><key id="111418917">14115</key><summary>DedicatedClusterSnapshotRestoreIT.restoreIndexWithMissingShards rarely fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>jenkins</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T15:07:28Z</created><updated>2015-10-29T11:01:43Z</updated><resolved>2015-10-29T11:01:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-20T08:37:48Z" id="149478092">This fails because of the change in https://github.com/elastic/elasticsearch/pull/13766/files#diff-e3d064cfd24df0c1f53af98e6f9bc66dR675  . I'll work on a fix but need to get some other things done first.  If anyone interested in picking this up first , please ping me :)
</comment><comment author="s1monw" created="2015-10-23T07:27:12Z" id="150496696">@bleskes what's the reason for this failure can you comment with more details?
</comment><comment author="bleskes" created="2015-10-25T16:31:22Z" id="150939220">@s1monw see #14276 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Max doc frequency percentage in More Like This query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14114</link><project id="" key="" /><description>More Like This Query has max_doc_freq settings that can be used to remove high-frequency words from search. It is not a frequency but an absolute value. The problem that actual value for index depends on number of documents in the index - larger indexes requires larger values to filter out domain specific stop words. 

I found that XMoreLikeThis actually has `setMaxDocFreqPct(int maxPercentage)` method that recalculates percentage to actual number of documents in index.

However, this method is not available to client. I would be great if you add it to java client and HTTP API.
</description><key id="111401393">14114</key><summary>Max doc frequency percentage in More Like This query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maxcom</reporter><labels><label>:More Like This</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-10-14T13:44:38Z</created><updated>2015-10-15T11:29:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove in-memory fielddata support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14113</link><project id="" key="" /><description>Now that doc values have become the default fielddata implementation, we should progressively deprecate in-memory fielddata so that doc values would be the only way to sort or aggregate on fields.
- [x] #14082 Numerics, dates and booleans
- [x] #16589 Not-analyzed strings
- [x] Geo-points (need to add doc values by default first)
</description><key id="111400496">14113</key><summary>Remove in-memory fielddata support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>Meta</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T13:40:58Z</created><updated>2016-06-28T09:31:33Z</updated><resolved>2016-06-21T17:03:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="astefan" created="2016-01-06T00:40:07Z" id="169179377">This is to track the removal of fielddata support for those fields that can use doc_values, not all fields. So, analyzed string fields will still use in-memory fielddata. The title of the issue kind of suggests the removal of in-memory fielddata overall.

Also, I'm wondering why we don't keep the option of having in-memory fielddata. For those use-cases where fielddata memory pressure is not a problem and the response time should be the fastest possible (considering that doc_values are slower than in-memory fielddata).
</comment><comment author="s1monw" created="2016-01-06T08:15:43Z" id="169263852">&gt; (considering that doc_values are slower than in-memory fielddata).

our benchmarks show that this is not true. The perf tests are in the noise range and that is in an isolated env. ie. if you have pressure it might even be faster?
</comment><comment author="astefan" created="2016-01-06T10:58:08Z" id="169297228">@s1monw did anything change related to the doc_values implementation, so that we can start saying that doc_values are as fast as in-memory fielddata? Or we just performed different tests that show the same speed, and the tests themselves are now targeting different scenarios?
</comment><comment author="jpountz" created="2016-01-06T13:34:40Z" id="169323630">There have been significant improvements in versions 1.3 and 1.4 that made doc values perform within 15% of in-memory fielddata for fielddata-heavy use-cases (meaning it's likely less if the query is not trivial). But doc values have numerous benefits over in-memory fielddata:
- more NRT-friendly
- off-heap
- keep working if the data are larger than the size of the main memory
- don't remove duplicates for numeric fields so that eg sums or averages are correct

In my opinion, the fact that doc values are off-heap alone is worth the move to doc values. Eventually I'd love to see all users move to heaps below 4GB which would mean you would almost not have to worry about garbage collection anymore. This is why eg we also moved norms to disk in 2.1 with no way to go back to memory and reduced the memory usage of multi-valued doc values fields in the upcoming 2.2 release.  
</comment><comment author="s1monw" created="2016-01-06T13:40:47Z" id="169325338">what @jpountz said... ;)
</comment><comment author="mkobrosli" created="2016-02-01T17:09:23Z" id="178072457">I feel like this is too much of an opinionated choice for a system to enforce.  Why force something on people if people can get top speed and scalability that matches their needs, indexing strategy and field design.  It just seems like a strange choice to make.  doc values serve a purpose clearly, but not for everything.  There are iops considerations that come into play in quota enforced environments, as well as more OS context switching which becomes very relevant under high load public facing sites.  

The only way I could see this not angering a lot of people is to complement this in 3.0 by changing aggregation strategies to utilize the filter cache to implement aggregations.  Otherwise you are basically conceding the battle of speed and Solr becomes the only alternative to real scalable faceting.  I really hope we can at least get aggs cached properly if you do decide on killing fielddata.

I really think this shows to much direction starting to move to big data / ELK usage and core search gets burned :( :-1: 
</comment><comment author="clintongormley" created="2016-02-02T14:42:31Z" id="178604579">@mkobrosli see https://github.com/elastic/elasticsearch/issues/16108#issuecomment-173171419
</comment><comment author="lusid" created="2016-04-28T16:20:26Z" id="215482846">I second what @mkobrosli is saying. We have manually disabled doc_values on all 1000+ fields in our ElasticSearch index because the performance loss for our application once doc_values is enabled is completely unacceptable. I haven't had a chance to do extensive testing using Google Cloud local SSDs to determine if doc_values could work in our scenario, but the few tests I have done after some short conversations with Peter Kim on the topic haven't given me much hope that this is going to improve our particular situation where aggregation and sorting performance is imperative across nearly any field in our index. The 10-15% loss of performance on some of our queries seems to take us back to the performance issues we were having with facets back in the v0.90 days.

We hate the fact that we are memory bound and want to be able to use doc_values exclusively, but at the end of the day, we are not yet confident that we can trade memory for disk IO. We also have concerns surrounding the fact we wouldn't have persistent disk to rely on in the case of a server failure since local SSDs would be wiped.

I'm going to continue my testing first chance I get with v5 alpha, but I wanted to be sure I publicly stated our concern before it is too late.
</comment><comment author="jpountz" created="2016-04-28T16:39:49Z" id="215488940">In-memory fielddata will not come back. It only existed because elasticsearch was created at a time when Lucene did not support columnar storage, but now it is part of the index format, just like the inverted index, norms, term vectors or stored fields, which cannot be loaded in memory.

We think enforcing doc values is good because even though it results in slow-downs for requests whose runtime is dominated by sorting or aggregations, the fact that we are not loading gigabytes in RAM anymore improved cluster stability and tail latencies since you are now less likely to have issues because of long garbage collections. Realtimeness also got better since you can now expect to see your indexed data after 1 second rather than 1 second + the time it takes to load fielddata from the inverted index.
</comment><comment author="clintongormley" created="2016-04-28T16:46:28Z" id="215490833">&gt;  the fact that we are not loading gigabytes in RAM 

This should really say: "loading gigabytes into the heap".  Doc values still end up in RAM in the file system cache.  This makes them fast - they are not disk bound.  And you can fit a whole lot more doc values into the file system cache than you could into the limited java heap.
</comment><comment author="jpountz" created="2016-06-21T17:03:11Z" id="227504920">Done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate the `missing` query in favour of a negated `exists` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14112</link><project id="" key="" /><description>The `missing` query is implemented as a negation of the `exists` query, but it is trappy when used with nested documents: 

It can detect docs with nested docs which are missing a field, but cannot detect docs which have no nested docs. Instead, by using an exists filter inside the nested block and placing the negation outside it, things work as expected. (See #3495 for a full example)

To avoid this trap we should deprecate the `missing` filter in favour of using an `exists` filter where the user can place the negation where appropriate.
</description><key id="111395900">14112</key><summary>Deprecate the `missing` query in favour of a negated `exists` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>deprecation</label><label>v2.2.0</label></labels><created>2015-10-14T13:15:37Z</created><updated>2015-12-11T16:20:47Z</updated><resolved>2015-12-11T16:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-14T20:23:10Z" id="148184332">+1
</comment><comment author="jagdish-nasit" created="2015-10-30T10:55:44Z" id="152495344">+1
</comment><comment author="jimczi" created="2015-12-01T13:09:04Z" id="160964928">The missing filter is able to filter (or not) null values (if the user specifies an explicit term for null values), this option is not present in the exists filter (it's in the documentation but not in the code ;) ). Should we add the feature in the exists filter before deprecation ?
</comment><comment author="jpountz" created="2015-12-01T13:25:09Z" id="160967835">+1 to add it. If it's documented but not implemented, this sounds more like a bug to me.
</comment><comment author="jimczi" created="2015-12-01T13:40:45Z" id="160973341">It's not documented sorry, the docs only explain the limitation regarding the filtering of null values ;)
</comment><comment author="jpountz" created="2015-12-01T13:41:49Z" id="160973522">Actually after chatting with @jimferenczi, apparently the feature was never implemented. So maybe it's not worth working on it. Users can still search for `null` values explicitly by doing a `{"term": { "my_field": null }}`. @clintongormley any opinions?
</comment><comment author="clintongormley" created="2015-12-01T13:57:08Z" id="160976783">The `missing` query supports `existence` and `null_value` parameters but honestly these are so hard to understand and explain that I don't think it is worth supporting these any more.  If a field has a `null_value` setting, then an explicit `null` will be replaced with a concrete value (and so will match an `exists` query).  If you want to exclude explicit nulls then you can add a clause which specifically excludes the term that you have used for the null value.  This I think is easier to understand than the `existence`/`null_value` parameters.

In other words: leave the `exists` query as it is today.
</comment><comment author="jimczi" created="2015-12-01T14:01:06Z" id="160977619">Ok no problem, as a side note and as @clintongormley said: " If you want to exclude explicit nulls then you can add a clause which specifically excludes the term that you have used for the null value". Doing a {"term": { "my_field": null }} throws an exception ...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix SimpleQueryString `minimum_should_match` handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14111</link><project id="" key="" /><description>So far we apply the optional `minimum_should_match` parameter
to the top-level query that resulted from parsing the query string
including all of its specified fields which lead to wrong behaviour
in cases like stated in #13884.

Instead we now parse the query string for each field/weight pair
individually, then apply the `minimum_should_match` parameter to
those queries and combine the result into an overall boolean query
with should-clauses for each field.

Closes #13884
</description><key id="111394599">14111</key><summary>Fix SimpleQueryString `minimum_should_match` handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label></labels><created>2015-10-14T13:09:50Z</created><updated>2015-10-31T09:12:31Z</updated><resolved>2015-10-15T11:27:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-10-14T13:40:02Z" id="148052084">I changed one existing integration test because for the following case, the way we handle `minimum_should_match` now changed the behaviour. Given the docs:

```
doc6: 
   body2: bar
   other: foo
doc7: 
   body2: foo bar
   other: foo
doc8: 
   body2: foo baz bar
   other: foo
```

The following query:

```
"query" : {
    "simple_query_string" : {
      "fields" : [ "body2", "other" ],
      "query" : "foo bar baz",
      "minimum_should_match" : "70%"
    }
  }
```

Matched all three documents, because it got rewritten to

```
((body2:foo other:foo) (body2:bar other:bar) (body2:baz other:baz))~2
```

With the change in this PR, doc5 doesn't match any more, because of 

```
((body2:foo body2:bar body2:baz)~2) ((other:foo other:bar other:baz)~2)
```

which to me intuitively makes more sense, so I adapted the existing test.
</comment><comment author="cbuescher" created="2015-10-15T11:27:03Z" id="148359340">Following discussion in #13884 this fix seems to be the wrong approach.
</comment><comment author="javanna" created="2015-10-31T09:12:31Z" id="152717091">@cbuescher I removed the version label as this PR wasn't merged after all.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster get settings does not return settings set in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14110</link><project id="" key="" /><description>If I put the following in my `elasticsearch.yaml`

```
cluster.routing.allocation.enable: none
indices.breaker.total.limit: 50%
discovery.zen.minimum_master_nodes: 1
```

Start my 1 node cluster and do a `GET /_cluster/settings` i'm seeing:

```
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 32

{"persistent":{},"transient":{}}
```

Is this by design? Only if i `PUT /_cluster/settings` these values do they get returned by the get cluster settings.
</description><key id="111379710">14110</key><summary>Cluster get settings does not return settings set in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-10-14T11:34:28Z</created><updated>2015-10-14T11:45:27Z</updated><resolved>2015-10-14T11:45:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T11:45:27Z" id="148025879">@Mpdreamz settings in the config file are per node (and may be different on different nodes).  The cluster settings API today only returns settings set via the API.

We'd like to fix this in #6732 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>tests.rest.load_packaged defaults to true in IDEs, loads all default ES REST tests instead of plugin's tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14109</link><project id="" key="" /><description>Something is wrong with this. A REST IT test case in a project runs against all (or at least a lot) of ES tests, not the project's REST YAMLs.

The examples in ES repository have this:

```
&lt;tests.rest.load_packaged&gt;false&lt;/tests.rest.load_packaged&gt;
```

but for any IDE this has to be passed manually. The problem seems to be here, the origin codebase is hardcoded to FileUtils?

https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/test/rest/ESRestTestCase.java#L243
</description><key id="111372038">14109</key><summary>tests.rest.load_packaged defaults to true in IDEs, loads all default ES REST tests instead of plugin's tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dweiss</reporter><labels><label>bug</label><label>test</label></labels><created>2015-10-14T10:45:01Z</created><updated>2016-01-29T14:07:28Z</updated><resolved>2016-01-29T14:07:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-14T13:50:46Z" id="148055488">Its just a typical IDE test issue. There is no jenkins job running tests from these IDEs or anything, yet they are essentially alternative build systems. 

The general problem is here: https://github.com/elastic/elasticsearch/issues/13914

One possible solution is to avoid system properties for this kind of thing entirely (instead, generate a resource file, as that can then work everywhere).
</comment><comment author="dweiss" created="2015-10-14T13:55:39Z" id="148056713">I understand the problem in general, but this property is replicated across example plugins as well (it's set to false) and I think the only reason why it's set to false in every one of these plugins is because it wouldn't work otherwise (read: it wouldn't be testing the plugin's REST YAML tests).
</comment><comment author="rmuir" created="2015-10-16T14:09:16Z" id="148727393">Right, but there are bigger issues here: 
- rest tests from the IDE do not work in core or plugins at all right now. I believe this was broken sometime when moving stuff to test-resources
- rest resources are applied in an inconsistent way: you get a test-resource folder named `target/dev-tools/rest-api-spec` which means resources are rooted under `/api` and `/test`
- on the other hand plugin rest resources are situated under `src/test/resources` with resources rooted under `rest-api-spec/api` and `rest-api-spec/test` and so on: totally inconsistent.

I am willing to fix all this up, so that it works from IDEs everywhere intuitively out of box, because I do think its important, but it is a significant amount of work and I don't want to just have to do it again in a few weeks because something like gradle is introduced. Things like IDE support are not tested by jenkins so I give a 100% chance I would have to redo everything again.
</comment><comment author="dweiss" created="2015-10-16T18:59:50Z" id="148807225">Hmm... these tests do work for me out of the box on 2.0.0.rc1. I followed an example from plugins/jvm-\* (something, can't remember). 

But I agree with you that it's inconsistent and hard to understand -- I spent a good while trying to figure out what's exactly happening and why (and eventually I gave up on trying to understand it).

This isn't a priority for me anyway as I couldn't figure out the yml "test api" language syntax. Specifically, it didn't seem extensible enough to cover some of the things I wanted to verify (I missed a list-contains-element assertion, for example). Not a big deal, really.
</comment><comment author="clintongormley" created="2016-01-29T14:07:28Z" id="176770584">I believe that, in master, rest tests are only intended to run from the CLI, so I'll close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Decentralize plugin security</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14108</link><project id="" key="" /><description>Today ES core defines all the rules for plugins and special exceptions by name. But this means if a plugin developer needs a similar thing (perhaps just a temporary hack or workaround: maybe in a third party dependency of theirs), it is not possible today, and the only solution is to disable securitymanager entirely.

This is bad for a few reasons:
1. causes core developers stress because they are in the loop for all problems
2. encourages disabling security entirely.

This PR presents an alternative, more like the android model, where plugins can include an optional `plugin-security.policy` file, and users have to confirm the additional requested permissions on installation (if stdin is a controlling terminal, and there is also a new explicit `-b` batch flag for `bin/plugin` too just for completeness):

```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@     WARNING: plugin requires additional permissions     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
* java.lang.RuntimePermission accessClassInPackage.sun.reflect
* java.lang.RuntimePermission closeClassLoader
* java.lang.RuntimePermission createClassLoader
* groovy.security.GroovyCodeSourcePermission /untrusted
See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html
for descriptions of what these permissions allow and the associated risks.

Continue with installation? [y/N]
```

I think its fairly safe, we protect plugins directories pretty well now (even unix filesystem permissions are improved recently). These permissions are _only_ granted to the plugin, like today, which means this stuff is contained pretty well. Of course it forces plugin authors to write proper security code, but I think that is ok, versus it not being possible at all. I added recommendations and an example to the plugin authors documentation to try to help with this. Even in the worst case if a plugin author adds `AllPermission` and screws up all the security code completely, its still better than having no security checking at all for ANY code... That is the major motivation for me wanting to do this, I think it will make things better. It just took me until now to figure out how to make the IDEs work :)

This change also allows tests to work from IDEs for such plugins without as many hacks as before, by configuring some special resources.
</description><key id="111363304">14108</key><summary>Decentralize plugin security</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>feature</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T09:57:04Z</created><updated>2015-10-26T19:34:06Z</updated><resolved>2015-10-14T18:52:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dweiss" created="2015-10-14T10:15:41Z" id="148004078">Not only it looks like an awesome feature, but also a nice cleanup of existing code (I went through the diff out of curiosity). Very cool.
</comment><comment author="clintongormley" created="2015-10-14T10:18:55Z" id="148004983">nice!
</comment><comment author="s1monw" created="2015-10-14T10:44:39Z" id="148010355">this looks awesome! thanks for doing this!
</comment><comment author="dadoonet" created="2015-10-14T10:46:04Z" id="148011015">&lt;3 
</comment><comment author="rmuir" created="2015-10-14T16:57:48Z" id="148116037">@s1monw I pushed changes to improve the docs. I also moved the check after 'plugin already exists' check, so that the user doesn't have to enter Y/N before that case.
</comment><comment author="s1monw" created="2015-10-14T18:25:15Z" id="148144898">LGTM
</comment><comment author="salyh" created="2015-10-19T11:26:52Z" id="149189024">Thats a great idea and i would love to see this as soon as possible released. I am working on a shield kerberos realm (nearly finished) and i had to disable security manager entirely cause all the &lt;pre&gt;Subject.doAs&lt;/pre&gt; stuff is with current ES policy not allowed. Would also be nice to read krb5.conf directly from /etc/. 
</comment><comment author="jprante" created="2015-10-19T21:59:51Z" id="149360216">I hoped for plugin security, thanks! Right now I have to disable security manager completely to get my groovy-based web app plugin things going https://github.com/jprante/elasticsearch-webapp 
</comment><comment author="rmuir" created="2015-10-19T22:57:27Z" id="149370746">Hi @salyh @jprante ... I have set the feature for 2.2 because it depends on a ton of other work that was done in the 2.2 codebase. I'm afraid this is about the soonest we can get this out unfortunately.

Before releasing it I have in mind add some minor improvements to it (e.g. ability to use full policy syntax, possibly rename the configuration file to improve IDE/test support, etc). I am working on some of that at the moment.
</comment><comment author="costin" created="2015-10-26T19:34:06Z" id="151259716">@rmuir Awesome stuff! Thanks for adding this in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure searcher is release if wrapping fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14107</link><project id="" key="" /><description>Today we leak an index searcher if we fail to wrap the seacher in
IndexShard. This commit ensures that the seacher is released if the wrapper
throws an exception.

This commit also restructures some test to be more atomic and only test a single
feature / attribute of the wrapper.
</description><key id="111351836">14107</key><summary>Ensure searcher is release if wrapping fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T08:49:07Z</created><updated>2015-10-14T08:57:37Z</updated><resolved>2015-10-14T08:57:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-14T08:51:45Z" id="147980148">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Spurious File Not Found Exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14106</link><project id="" key="" /><description>Hi ,
We have deployed ES 1.4 on AWS using EBS as storage, we recently started seeing some issues with index in which index remains unassigned , we see a FileNotFoundException in the logs .the file is a lucene file ,  we deleted the shard folder and restarted cluster after which we start getting FIleNotFoundExceptions again while Recovering , so we clean up the whole data path folder and expect that the machine should recover since all primaries are assigned , cluster is in yellow state. However same FileNotFound Exception and recovery fails,

2015-10-14 00:54:47,095 WARN  [org.elasticsearch.indices.cluster](elasticsearch[Captain Germany][generic][T#9]) [Captain Germany] [indexName][2] failed to start shard
org.elasticsearch.indices.recovery.RecoveryFailedException: [indexName][2]: Recovery failed from [Guido Carosella][aBOW6HyxSDG6hkbUrlSwLw][indexName01.xyz.xyz.com][inet:9300]]{zone=us-east-1a} into [Captain Germany][wXPQlst-QyCBx39CoJWcQA][indexName03.xyz.xyz.com][inet[/:9300]]{zone=us-east-1d}
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:308)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$200(RecoveryTarget.java:65)
        at org.elasticsearch.indices.recovery.RecoveryTarget$3.run(RecoveryTarget.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [Guido Carosella][inet[/:9300]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [entity_2015100804][2] Phase[1] Execution failed
        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1120)
        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:654)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:137)
        at org.elasticsearch.indices.recovery.RecoverySource.access$2600(RecoverySource.java:74)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:464)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:450)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [entity_2015100804][2] Failed to transfer [0] files with total size of [0b]
        at org.elasticsearch.indices.recovery.RecoverySource$1.phase1(RecoverySource.java:276)
        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1116)
        ... 9 more
Caused by: java.nio.file.NoSuchFileException: /mnt/data/data/indexName-sep/nodes/0/indices/indexName/2/index/_4wi.si
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
        at java.nio.channels.FileChannel.open(FileChannel.java:287)
        at java.nio.channels.FileChannel.open(FileChannel.java:335)
        at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:81)
        at org.apache.lucene.store.FileSwitchDirectory.openInput(FileSwitchDirectory.java:172)
        at org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)
        at org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)
        at org.apache.lucene.store.Directory.openChecksumInput(Directory.java:113)
        at org.apache.lucene.codecs.lucene46.Lucene46SegmentInfoReader.read(Lucene46SegmentInfoReader.java:51)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:358)
        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:94)
        at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:124)
        at org.elasticsearch.index.store.Store.access$400(Store.java:80)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:575)
        at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:568)
        at org.elasticsearch.index.store.Store.getMetadata(Store.java:186)
        at org.elasticsearch.indices.recovery.RecoverySource$1.phase1(RecoverySource.java:146)
       ... 10 more
</description><key id="111345109">14106</key><summary>Spurious File Not Found Exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankrugold</reporter><labels /><created>2015-10-14T08:05:48Z</created><updated>2015-10-14T08:46:25Z</updated><resolved>2015-10-14T08:46:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T08:46:25Z" id="147977981">hiya

so much has changed since 1.4 was released, especially around recovery and snapshot/restore.  please upgrade.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Netty to 3.10.5.final</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14105</link><project id="" key="" /><description>This fixes a critical bug in SSL handling.

See http://netty.io/news/2015/10/13/3-10-5-Final.html
</description><key id="111343945">14105</key><summary>Upgrade Netty to 3.10.5.final</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Network</label><label>critical</label><label>upgrade</label><label>v1.7.3</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-14T07:56:36Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-14T08:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-10-14T08:55:38Z" id="147981283">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ricardo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14104</link><project id="" key="" /><description>aceita nabo
</description><key id="111340326">14104</key><summary>Ricardo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ricardocerq</reporter><labels /><created>2015-10-14T07:26:54Z</created><updated>2015-10-14T07:28:21Z</updated><resolved>2015-10-14T07:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>cloud-aws plugin: Option to explicitly set x-amz-acl: Private </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14103</link><project id="" key="" /><description>While ACL Private is a default, it would be nice to permit ElasticSearch nodes to explicitly set the desired ACL for the snapshots they create. This can then be validated by an S3 Bucket Policy, which can reject clients that try to upload objects to S3 that aren't set as Private.

http://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html
http://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html
</description><key id="111309789">14103</key><summary>cloud-aws plugin: Option to explicitly set x-amz-acl: Private </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/xuzha/following{/other_user}', u'events_url': u'https://api.github.com/users/xuzha/events{/privacy}', u'organizations_url': u'https://api.github.com/users/xuzha/orgs', u'url': u'https://api.github.com/users/xuzha', u'gists_url': u'https://api.github.com/users/xuzha/gists{/gist_id}', u'html_url': u'https://github.com/xuzha', u'subscriptions_url': u'https://api.github.com/users/xuzha/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1799964?v=4', u'repos_url': u'https://api.github.com/users/xuzha/repos', u'received_events_url': u'https://api.github.com/users/xuzha/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/xuzha/starred{/owner}{/repo}', u'site_admin': False, u'login': u'xuzha', u'type': u'User', u'id': 1799964, u'followers_url': u'https://api.github.com/users/xuzha/followers'}</assignee><reporter username="">JamesBromberger</reporter><labels><label>:Plugin Repository S3</label><label>adoptme</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-14T02:42:14Z</created><updated>2015-10-29T20:20:18Z</updated><resolved>2015-10-29T19:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T08:33:53Z" id="147974051">@dadoonet thoughts?
</comment><comment author="dadoonet" created="2015-10-14T09:07:57Z" id="147984323">It makes sense to me to support such an option.
</comment><comment author="clintongormley" created="2015-10-14T09:10:26Z" id="147984808">@JamesBromberger fancy sending a PR?
</comment><comment author="JamesBromberger" created="2015-10-14T14:31:30Z" id="148067791">Sorry, I'm not a Java dev, so I have no faith that any Java I would try would even compile. But a rough draft as a patch, totally untested, uncompiled, and unsure if this helps:
https://www.james.rcpt.to/misc/elasticsearch-repo-s3-cannedACL.diff
</comment><comment author="JamesBromberger" created="2015-10-27T01:16:53Z" id="151333213">Would be useful to also permit admins to set an "x-amz-acl" header of "bucket-owner-full-control" to enable sending snapshots to an S3 bucket owned by another account for escrow/compliance purposes and letting that separate account own the objects deposited there.

http://docs.aws.amazon.com/AmazonS3/latest/dev/example-walkthroughs-managing-access-example3.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_close?ignore_unavailable=true doesn't seem to work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14102</link><project id="" key="" /><description>[Here](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/indices-open-close.html) mentions you can use `ignore_unavailable=true` to ignore any index that may not exist when closing an index. But if I have a cluster that doesn't have the `test` index and I try `POST /test/_close?ignore_unavailable=true` I get;

```
{
   "error": "IndexMissingException[[test] missing]",
   "status": 404
}
```

Which makes no sense. Not sure if it's the code or the docs that are wrong here.

Via [here](https://discuss.elastic.co/t/close-index-api-does-not-seem-to-provide-option-to-ignore-missing-indexes/31998)
</description><key id="111309423">14102</key><summary>_close?ignore_unavailable=true doesn't seem to work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Index APIs</label><label>bug</label></labels><created>2015-10-14T02:39:10Z</created><updated>2015-10-14T08:43:05Z</updated><resolved>2015-10-14T08:43:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-14T07:34:32Z" id="147962745">did you try this with 2.0 / latest master it seems like an old version of ES?
</comment><comment author="clintongormley" created="2015-10-14T08:43:05Z" id="147976419">The situation is a bit complicated.  For instance:

```
PUT test1
{}


# this throws an exception because no matching index was found
POST test2/_close?ignore_unavailable

# this succeeds, because it ignores test2 and still finds test1
POST test1,test2/_close?ignore_unavailable

# repeating the above command now fails, because test2 doesn't exist and test1 is closed
# so no indices are found to operate on
```

We need to rethink these index options to figure out what the best defaults should be, and how requests with concrete names (or a single concrete name) should behave differently from requests with wildcards

Closing in favour of #9438
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trouble sorting by "bucket_script" in 2.0.0-rc1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14101</link><project id="" key="" /><description>I'm running into some trouble trying to sort by a `bucket_script` aggregation. Attached is an example that illustrates the difficulty.

If we add these fake documents:

```
curl -XPUT http://localhost:9200/test/test/1 -d '
{
    "ind" : 1,
    "my_field" : 1
}
'

curl -XPUT http://localhost:9200/test/test/2 -d '
{
    "ind" : 1,
    "my_field" : 50
}
'

curl -XPUT http://localhost:9200/test/test/2 -d '
{
    "ind" : 2,
    "my_field" : 99
}
'
```

This works:

```
curl -XPOST http://localhost:9200/test/_search -d '{
    "aggs" : {
        "test" : {
            "terms" : {
                "field" : "ind",
                "order" : {
                    "var1.value" : "asc"
                }
            },
            "aggs" : {
                "var1" : {"sum" : {"field" : "my_field"}},
                "var2" : {"sum" : {"field" : "my_field"}},
                "bs"   : {
                    "bucket_script" : {
                        "script" : "var1 + var2",
                        "buckets_path" : {
                            "var1" : "var1",
                            "var2" : "var2"
                        }
                    }
                }
            }
        }
    }
}'
```

but this doesn't:

```
curl -XPOST http://localhost:9200/test/_search -d '{
    "aggs" : {
        "test" : {
            "terms" : {
                "field" : "ind",
                "order" : {
                    "bs.value" : "asc"
                }
            },
            "aggs" : {
                "var1" : {"sum" : {"field" : "my_field"}},
                "var2" : {"sum" : {"field" : "my_field"}},
                "bs"   : {
                    "bucket_script" : {
                        "script" : "var1 + var2",
                        "buckets_path" : {
                            "var1" : "var1",
                            "var2" : "var2"
                        }
                    }
                }
            }
        }
    }
}'
```

Is such an ordering possible at the moment?  Am I just using the wrong syntax? If this is not possible, are there any plans to make it possible in the future?

Thanks!
Ben
</description><key id="111273577">14101</key><summary>Trouble sorting by "bucket_script" in 2.0.0-rc1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">bkj</reporter><labels><label>:Aggregations</label></labels><created>2015-10-13T21:20:51Z</created><updated>2015-10-15T13:23:48Z</updated><resolved>2015-10-15T13:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T08:22:21Z" id="147971331">@colings86 could you comment on this please?
</comment><comment author="colings86" created="2015-10-14T10:47:28Z" id="148011627">In order to be able to use an aggregation for sorting the terms aggregation it must currently be a numeric metric aggregation. Pipeline Aggregations are a different family of aggregations than the Metric Aggregations and cannot be used for sorting. You couldn't sort using a Pipeline aggregation because they are only executed in the reduce phase on the coordinating node and so you do not have the information on the shard in order to be able to sort the shards buckets to pick the top N to send to the coordinating node.
</comment><comment author="bkj" created="2015-10-14T12:40:03Z" id="148037055">Thanks for the answer -- that's sortof what I was guessing.  Do you think it's possible to order the results by using something else?  Like the `scripted_metric` aggregation that allows user defined `map`, `combine`, and `reduce` scripts?  
</comment><comment author="colings86" created="2015-10-15T06:41:16Z" id="148296297">Copied from a similar conversation on the Discuss forums here: https://discuss.elastic.co/t/ordering-terms-aggregation-based-on-pipeline-metric/31839/2

The Scripted Metric Aggregation is part of the Metric Aggregation family but is not a Numeric Metric Aggregation since it returns an arbitrary object. However, there is an issue open in the Elasticsearch repo to add the ability to sort by an attribute of the Scripted Metric Aggregation: https://github.com/elastic/elasticsearch/issues/8486
</comment><comment author="clintongormley" created="2015-10-15T13:23:48Z" id="148384624">Closing in favour of #8486
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mapping lookup order issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14100</link><project id="" key="" /><description>When searching across multiple types, the result of the query changes with the order the types are specified in the URL.  

e.g. 

the field autocomplete exists in 'type a' but not 'type b'.

index/type a,type b/_search?q=autocomplete:Xyx

 query works and returns the correct results from type a

index/type b,type a/_search?q=autocomplete:=Xyx - This fails with no hits - (looks like the query is not correctly analysed).

I can see a whole set of issues here but this does seems odd behaviour.

Thanks

Rob
</description><key id="111265783">14100</key><summary>mapping lookup order issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rob-tice</reporter><labels /><created>2015-10-13T20:38:50Z</created><updated>2015-10-14T08:25:44Z</updated><resolved>2015-10-14T08:15:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T08:15:09Z" id="147969914">@rob-tice I can't replicate your exact issue in 1.7.2, but it sounds like you're running into some of the issues with ambiguous mappings that exist in 1.x.  These problems have been solved in 2.0 but it is not possible to backport the changes.

To work around it, you can try prefixing the field with the type name: `?q=type_a.autocomplete:Xyx`
</comment><comment author="rob-tice" created="2015-10-14T08:25:43Z" id="147972191">Thanks for the reply.  That is what I have done. Roll on 2.0!

Cheers

Rob
On 14 Oct 2015 9:16 am, "Clinton Gormley" notifications@github.com wrote:

&gt; @rob-tice https://github.com/rob-tice I can't replicate your exact
&gt; issue in 1.7.2, but it sounds like you're running into some of the issues
&gt; with ambiguous mappings that exist in 1.x. These problems have been solved
&gt; in 2.0 but it is not possible to backport the changes.
&gt; 
&gt; To work around it, you can try prefixing the field with the type name:
&gt; ?q=type_a.autocomplete:Xyx
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14100#issuecomment-147969914
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>discovery-ec2 plugin needs to throttle itself if it gets a RequestLimitExceeded exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14099</link><project id="" key="" /><description>Occasionally when building a cluster things go into a funny state (not ES's fault) and the discovery plugin (using https://github.com/elastic/elasticsearch-cloud-aws) tries to find the master. If the master can't be found, it appears that the plugin starts spamming the EC2 API with as many lookups per second as it can, which quickly ends with EC2 throwing RequestLimitExceeded exceptions. The proper behaviour in this case is for the plugin to backoff but it doesn't appear to do so. When this occurs, the EC2 console becomes unusable, affecting every EC2 api call for that AWS account.

Unfortunately I can't replicate the issue easily (we caught this when stumbling over a CloudFormation bug) and the CloudFormation stack would eventually roll back or we'd have to destroy the stack manually to try bring it under control. In either case we couldn't get any logs off the instances in time. I can share the CloudTrail logs if that will help. 

This issue was raised here: https://github.com/elastic/elasticsearch-cloud-aws/issues/229

With a PR against that repo: https://github.com/elastic/elasticsearch-cloud-aws/pull/231
</description><key id="111264463">14099</key><summary>discovery-ec2 plugin needs to throttle itself if it gets a RequestLimitExceeded exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rtkmhart</reporter><labels><label>:Plugin Discovery EC2</label><label>enhancement</label></labels><created>2015-10-13T20:31:16Z</created><updated>2015-11-23T14:07:20Z</updated><resolved>2015-11-23T14:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OTTERPUP333" created="2015-10-15T16:59:17Z" id="148456721">easy to reproduce: (discovered by accident)
in your yaml config, set ec2-discovery for some tag (any non-existent tag) and watch es thrash your EC2 console into non-responsiveness
</comment><comment author="dadoonet" created="2015-11-23T14:07:20Z" id="158940685">I think that we can close it now as #14155 has been merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search returns wrong results after Updated. "Slow update"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14098</link><project id="" key="" /><description>Hi!

I have the following code:

``` bash
curl -XGET 'http://localhost:9200/INDEX/MAP/_search?pretty=true&amp;q=active:1'
```

That returns something like this:

``` json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "INDEX",
      "_type" : "MAP",
      "_id" : "AVBDF16ioVCdFXKZcj8T",
      "_score" : 1.0,
      "_source":{"name":"Rodrigo 3","number":133,"active":1}
    }, {
      "_index" : "INDEX",
      "_type" : "MAP",
      "_id" : "AVBDGTnjoVCdFXKZcj8U",
      "_score" : 1.0,
      "_source":{"name":"Rodrigo 4","number":134,"active":1}
    }, {
      "_index" : "INDEX",
      "_type" : "MAP",
      "_id" : "AVBDFYIOoVCdFXKZcj8R",
      "_score" : 1.0,
      "_source":{"name":"Rodrigo 2","number":132,"active":1}
    } ]
  }
}
```

As you can see all of the "hits" have the key "active" set to 1....
So, let's update for example the second "hit" and set "active" to 0.

``` bash
curl -XPOST 'http://localhost:9200/INDEX/MAP/AVBDGTnjoVCdFXKZcj8U/_update' -d '{
    "doc" : {
        "active": 0
    }
}'
```

Now, if I search again, how many results should be returned? 2 right?

``` bash
curl -XGET 'http://localhost:9200/INDEX/MAP/_search?pretty=true&amp;q=active:1'
```

``` json
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "INDEX",
      "_type" : "MAP",
      "_id" : "AVBDF16ioVCdFXKZcj8T",
      "_score" : 1.0,
      "_source":{"name":"Rodrigo 3","number":133,"active":1}
    }, {
      "_index" : "INDEX",
      "_type" : "MAP",
      "_id" : "AVBDGTnjoVCdFXKZcj8U",
      "_score" : 1.0,
      "_source":{"name":"Rodrigo 4","number":134,"active":1}
    }, {
      "_index" : "INDEX",
      "_type" : "MAP",
      "_id" : "AVBDFYIOoVCdFXKZcj8R",
      "_score" : 1.0,
      "_source":{"name":"Rodrigo 2","number":132,"active":1}
    } ]
  }
}
```

**BUT IT RETURNS 3! WHAT?**
How's this possible?
It returns 2 if I wait some time, like 1 second...

What am I doing wrong? Or is a elasticsearch bug?

Note: My index is not called "INDEX", neither the mapping name "MAP"....

Thanks!
</description><key id="111254772">14098</key><summary>Search returns wrong results after Updated. "Slow update"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rodrigograca31</reporter><labels /><created>2015-10-13T19:42:18Z</created><updated>2015-10-13T21:27:03Z</updated><resolved>2015-10-13T19:44:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-13T19:44:10Z" id="147828577">you have to call `_refresh` to make changes visible, it's called periodically every second by default. in the future for questions like this use https://discuss.elastic.co/ rather than the bug / issue tracker
</comment><comment author="dadoonet" created="2015-10-13T19:44:24Z" id="147828629">It's because of the refresh which happens after 1s

You can force a refresh for test purpose
</comment><comment author="rodrigograca31" created="2015-10-13T19:50:32Z" id="147830006">So, between the update and the last search I should run this?

``` bash
curl -XPOST 'http://localhost:9200/INDEX/_refresh'
```

This doesn't make any sense...
</comment><comment author="s1monw" created="2015-10-13T21:27:03Z" id="147859023">&gt; This doesn't make any sense...

this is how ES and all lucene based systems work. They operate on point in time snapshots of the data that makes them very very efficient for fast reads etc. It's not a relational database that makes changes visible once the _transaction_ returns. It makes a boat load of sense if you look how this stuff is used in production and on the technical level. I hope this helps you to understand this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added link to Swagger for Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14097</link><project id="" key="" /><description>This PR is a follow up of #14093

&gt; I wrote this plugin a couple of months ago and have been using it since. I received some messages from people using it and finding it actually useful to discover the API's. Is it suitable to add it to this page?
&gt; 
&gt; The plugin supports Swagger 1.2 docs for Elasticsearch 0.90 and up
</description><key id="111254150">14097</key><summary>Added link to Swagger for Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timschlechter</reporter><labels><label>docs</label></labels><created>2015-10-13T19:39:15Z</created><updated>2015-10-14T08:02:51Z</updated><resolved>2015-10-14T08:02:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T08:02:51Z" id="147967514">thanks @timschlechter - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed a typo ("when when")</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14096</link><project id="" key="" /><description /><key id="111248754">14096</key><summary>Fixed a typo ("when when")</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">speedplane</reporter><labels><label>docs</label></labels><created>2015-10-13T19:11:27Z</created><updated>2015-10-13T19:13:52Z</updated><resolved>2015-10-13T19:13:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T19:13:52Z" id="147819216">thanks @speedplane 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add instrumentation for sparse index detection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14095</link><project id="" key="" /><description>One current item to watch out for with doc values is the notion of "sparse indices". This is the situation where there is a disproportionate ratio of documents that have a value for a field versus those that do. For example if you have 1 million docs but only 1,000 have a particular field actually present in them, the amount of disk consumed will still be large.

In order to help detect this and other such situations, enhancing the instrumentation APIs to help report this on a field by field basis is probably helpful. Similar to what was done for fielddata on a field by field breakdown.

As requested after discussing with @jpountz yesterday while talking about this, suggested adding an issue here to discuss/track. Please any information I may have missed.
</description><key id="111240923">14095</key><summary>add instrumentation for sparse index detection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>:Stats</label><label>discuss</label></labels><created>2015-10-13T18:31:58Z</created><updated>2016-01-29T13:58:22Z</updated><resolved>2016-01-29T13:58:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T09:13:09Z" id="148662564">What would these APIs look like?
</comment><comment author="jpountz" created="2015-10-16T09:30:38Z" id="148665555">For indexed fields, Lucene stores metadata about how many documents have a given field (`Terms.getDocCount()`). One issue however is that this metric is optional, some codecs might not store it. And also it wouldn't work if a field is mapped with doc values only.

Alternatively, monitoring tools could iterate over fields in the mapping and count how many documents contain this field using the `exists` query, which would work for any field that is either indexed or has doc values enabled.

However I'm not sure this is something that could be run on a regular basis as it might be costly if you have many shards or fields, so I think there should be a button in the monitoring UI to run such checks explicitly.
</comment><comment author="clintongormley" created="2016-01-29T13:57:46Z" id="176765282">This sounds like an aggregation with an exists filter per field which can be run by Marvel.  Moving to Marvel
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>shard relocation might leave files behind if deletion unsuccessful</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14094</link><project id="" key="" /><description>When we delete shard content after is has been relocated away from a node and this deletion fails then it will just stay on disk until the whole index is deleted. There seems to be no additional mechanism to delete it later like we have for indices (PendingDelete). In case anyone likes to check, I wrote a test here for WindowsFS: https://github.com/brwe/elasticsearch/commit/b3598ac123f9d9e20726410e221fade6e4905d99
I think this is problematic?
</description><key id="111235155">14094</key><summary>shard relocation might leave files behind if deletion unsuccessful</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Store</label><label>adoptme</label><label>enhancement</label></labels><created>2015-10-13T18:00:19Z</created><updated>2015-10-19T14:54:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T09:17:01Z" id="148663262">FixItFriday agrees with you :)
</comment><comment author="bleskes" created="2015-10-16T20:07:05Z" id="148821266">Agreed this is not nice - but just to make sure I get the scope - the shard will be delete with the next cluster state update, right?
</comment><comment author="brwe" created="2015-10-19T14:54:01Z" id="149238849">Indeed, the description was misleading. The shard content is deleted with next cluster state update but the index directory and the metadata is not. It stays until the whole index is deleted.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added link to Swagger for Elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14093</link><project id="" key="" /><description>I wrote this plugin a couple of months ago and have been using it since. I received some messages from people using it and finding it actually useful to discover the API's. Is it suitable to add it to this page?

The plugin supports Swagger 1.2 docs for Elasticsearch 0.90 and up
</description><key id="111233787">14093</key><summary>Added link to Swagger for Elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">timschlechter</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-10-13T17:52:50Z</created><updated>2015-10-13T19:36:55Z</updated><resolved>2015-10-13T19:36:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T19:07:18Z" id="147816217">Hi @timschlechter 

Sure we can add it.  However, the plugin docs have changed in 2.0 and above. Seehttps://www.elastic.co/guide/en/elasticsearch/plugins/master/index.html  

Could you base your PR off master instead?
</comment><comment author="timschlechter" created="2015-10-13T19:36:55Z" id="147825968">Sure! I tried modifying this PR, but I don't know how to change the target branch.
I'm closing this one and will open a new PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation: max_expansions cannot be set to 0 in fuzzy queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14092</link><project id="" key="" /><description>query-dsl-fuzzy-query.html states that `this query can be very heavy if prefix_length and max_expansions are both set to 0. This could cause every term in the index to be examined!`

Yet actually, `max_expansions` cannot be set to 0 as this results in a `SearchParseException`. See #5017.
</description><key id="111225067">14092</key><summary>Documentation: max_expansions cannot be set to 0 in fuzzy queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cwurm</reporter><labels><label>docs</label></labels><created>2015-10-13T17:04:16Z</created><updated>2015-10-13T19:04:04Z</updated><resolved>2015-10-13T19:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Avoid deadlocks in Cache#computeIfAbsent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14091</link><project id="" key="" /><description>This commit changes the behavior of Cache#computeIfAbsent to not invoke
load for a key under the segment lock. Instead, the synchronization
mechanism to ensure that load is invoked at most once per key is
through the use of a future. Under the segment lock, we put a future in
the cache and through this ensure that load is invoked at most once per
key. This will not lead to the same deadlock situation as before
because a dependent key load on the same thread can not be triggered
while the segment lock is held.

Closes #14090
</description><key id="111224683">14091</key><summary>Avoid deadlocks in Cache#computeIfAbsent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-13T17:01:44Z</created><updated>2015-10-14T01:16:45Z</updated><resolved>2015-10-14T01:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-13T17:02:19Z" id="147778539">@jpountz Can you take a look?
</comment><comment author="jasontedor" created="2015-10-13T20:22:06Z" id="147840949">@jpountz I've pushed another approach.
</comment><comment author="jpountz" created="2015-10-13T23:05:15Z" id="147878523">LGTM
</comment><comment author="jasontedor" created="2015-10-14T01:16:45Z" id="147897956">Thank you for the careful review @jpountz, and for pushing me to find a solution. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cache#computeIfAbsent can lead to deadlocks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14090</link><project id="" key="" /><description>The current implementation of `Cache#computeIfAbsent` can lead to deadlocks in situations where dependent key loading is occurring. This is the case because of the locks that are taken to ensure that the loader is invoked at most once per key. In particular, consider two threads `t1` and `t2` invoking this method for keys `k1` and `k2` which will both trigger dependent calls to `Cache#computeIfAbsent` for keys `kd1` and `kd2`. In cases when `k1` and `kd2`and in the same segment, and`k2`and `kd1` are in the same segment then:
1. `t1` locks the segment for `k1`
2. `t2` locks the segment for `k2`
3. `t1` blocks waiting for the lock for the segment for `kd1`
4. `t2` blocks waiting for the lock for the segment for `kd2`

is a deadlock. This unfortunate situation surfaced in a failed [build](http://build-us-00.elastic.co/job/es_core_master_medium/2473/).

```
"elasticsearch[node_s0][warmer][T#5]" ID=19141 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@2255a366 owned by "elasticsearch[node_s0][warmer][T#4]" ID=19139
    at sun.misc.Unsafe.park(Native Method)
    - waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@2255a366
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)
    at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)
    at org.elasticsearch.common.util.concurrent.ReleasableLock.acquire(ReleasableLock.java:55)
    at org.elasticsearch.common.cache.Cache$CacheSegment.get(Cache.java:187)
    at org.elasticsearch.common.cache.Cache.get(Cache.java:279)
    at org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:300)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:150)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:80)
    at org.elasticsearch.index.fielddata.ordinals.GlobalOrdinalsBuilder.build(GlobalOrdinalsBuilder.java:52)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.localGlobalDirect(AbstractIndexOrdinalsFieldData.java:80)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.localGlobalDirect(AbstractIndexOrdinalsFieldData.java:41)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.lambda$load$27(IndicesFieldDataCache.java:179)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$$Lambda$366/28099691.load(Unknown Source)
    at org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:311)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:174)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.loadGlobal(AbstractIndexOrdinalsFieldData.java:68)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.loadGlobal(AbstractIndexOrdinalsFieldData.java:41)
    at org.elasticsearch.search.SearchService$FieldDataWarmer$3.run(SearchService.java:1019)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Locked synchronizers:
    - java.util.concurrent.ThreadPoolExecutor$Worker@41b4471c
    - java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@23fdb477

"elasticsearch[node_s0][warmer][T#4]" ID=19139 WAITING on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@23fdb477 owned by "elasticsearch[node_s0][warmer][T#5]" ID=19141
    at sun.misc.Unsafe.park(Native Method)
    - waiting on java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@23fdb477
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:967)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1283)
    at java.util.concurrent.locks.ReentrantReadWriteLock$ReadLock.lock(ReentrantReadWriteLock.java:727)
    at org.elasticsearch.common.util.concurrent.ReleasableLock.acquire(ReleasableLock.java:55)
    at org.elasticsearch.common.cache.Cache$CacheSegment.get(Cache.java:187)
    at org.elasticsearch.common.cache.Cache.get(Cache.java:279)
    at org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:300)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:150)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexFieldData.load(AbstractIndexFieldData.java:80)
    at org.elasticsearch.index.fielddata.ordinals.GlobalOrdinalsBuilder.build(GlobalOrdinalsBuilder.java:52)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.localGlobalDirect(AbstractIndexOrdinalsFieldData.java:80)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.localGlobalDirect(AbstractIndexOrdinalsFieldData.java:41)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.lambda$load$27(IndicesFieldDataCache.java:179)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache$$Lambda$366/28099691.load(Unknown Source)
    at org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:311)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:174)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.loadGlobal(AbstractIndexOrdinalsFieldData.java:68)
    at org.elasticsearch.index.fielddata.plain.AbstractIndexOrdinalsFieldData.loadGlobal(AbstractIndexOrdinalsFieldData.java:41)
    at org.elasticsearch.search.SearchService$FieldDataWarmer$3.run(SearchService.java:1019)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
    Locked synchronizers:
    - java.util.concurrent.locks.ReentrantReadWriteLock$NonfairSync@2255a366
    - java.util.concurrent.ThreadPoolExecutor$Worker@42f5ef87
```
</description><key id="111222800">14090</key><summary>Cache#computeIfAbsent can lead to deadlocks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-10-13T16:52:23Z</created><updated>2016-11-25T21:50:29Z</updated><resolved>2015-10-14T01:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-13T17:03:54Z" id="147778977">There are two pull requests open to address this issue. The first is #14068 which relaxes the constraint that `Cache#computeIfAbsent` be called at most once per key. The second is #14091 which changes the synchronization mechanism to be the key itself so that loading does not occur under the segment lock.

Only one of these two pull requests should be merged into master but given the [feedback](https://github.com/elastic/elasticsearch/pull/14068#issuecomment-147525189) on #14068 from @jpountz I wanted to explore a different approach for solving the deadlock issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix access denied for shard deletion with WindowsFS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14089</link><project id="" key="" /><description>We randomly use the WindowsFS mock file system to simulate that
windows does not delete files if they are opened by some other
process and instead gives you java.io.IOException: access denied.
In the tests we also check if the shard was deleted while it is
being deleted. This check loads the mata data of the index
and therefore might hold on to a file while the node
is trying to delete it and deletion will fail then.

Instead we should just check if the directory was removed.

closes #13758
</description><key id="111219489">14089</key><summary>fix access denied for shard deletion with WindowsFS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-10-13T16:36:42Z</created><updated>2015-10-19T12:25:26Z</updated><resolved>2015-10-19T12:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-10-13T16:37:22Z" id="147770745">@s1monw want to take a look?
</comment><comment author="s1monw" created="2015-10-13T19:20:24Z" id="147821983">left some comments
</comment><comment author="brwe" created="2015-10-14T09:40:47Z" id="147991355">Thanks for the review! Addressed all comments just not sure about https://github.com/elastic/elasticsearch/pull/14089#discussion_r41911509
</comment><comment author="s1monw" created="2015-10-19T11:35:52Z" id="149190278">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin script to set proper plugin bin dir attributes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14088</link><project id="" key="" /><description>This commit makes sure that the plugin script looks at user, group and permissions of the elasticsearch bin dir and copies them over to the plugin bin subdirectory, whatever they are, so that they get properly setup depending on how elasticsearch was installed. We also make sure that execute permissions are added for files (we already did this before).

Relates to #11016
</description><key id="111207132">14088</key><summary>Plugin script to set proper plugin bin dir attributes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-13T15:37:11Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-14T13:53:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-13T15:37:51Z" id="147753426">@jaymode @nik9000 can you have a look? This is the last step to complete the work around #11016.
</comment><comment author="jaymode" created="2015-10-14T12:15:38Z" id="148030627">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add methods for variable-length encoding integral arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14087</link><project id="" key="" /><description>This commit adds methods to serialize the elements of int and long
arrays using variable-length encodings. This can be useful for
serializing int and long arrays containing mostly non-negative &#8220;not
large&#8221; values in a compressed form.
</description><key id="111172915">14087</key><summary>Add methods for variable-length encoding integral arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-13T12:48:19Z</created><updated>2015-10-13T17:44:46Z</updated><resolved>2015-10-13T13:12:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-13T13:09:11Z" id="147709373">LGTM. For bonus points write a regex to look for where this can be used and remove some code :)
</comment><comment author="nik9000" created="2015-10-13T17:44:46Z" id="147790665">Might relate to https://github.com/elastic/elasticsearch/issues/13893 ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoMethodError: undefined method `to_iso8601' for nil:NilClass in event.rb</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14086</link><project id="" key="" /><description>Hello,

I used a logstash 1.5.4 instance that processes a few tens of millions messages per day and that often crashes (almost once everyday). In logstash.err, ther is the following trace :

Oct 09, 2015 9:58:17 AM org.apache.http.impl.execchain.RetryExec execute
INFO: Retrying request to {}-&gt;http://10.176.61.21:9200
NoMethodError: undefined method `to_iso8601' for nil:NilClass
         to_s at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-core-1.5.4-java/lib/logstash/event.rb:109
  retry_flush at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-output-elasticsearch-1.0.7-java/lib/logstash/outputs/elasticsearch.rb:729
        times at org/jruby/RubyFixnum.java:275
         each at org/jruby/RubyEnumerator.java:274
          map at org/jruby/RubyEnumerable.java:764
  retry_flush at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-output-elasticsearch-1.0.7-java/lib/logstash/outputs/elasticsearch.rb:724
     register at /opt/logstash/vendor/bundle/jruby/1.9/gems/logstash-output-elasticsearch-1.0.7-java/lib/logstash/outputs/elasticsearch.rb:480

Looking at elasticsearch.rb, the application crashes while trying to log an error :
@logger.error "too many attempts at sending event. dropping: #{next_event}"

Looking at event.rb, the to_s function (called when trying to log the event) assumes there is an existing timestamp field. But from the trace I suspect this field does not always exist. So Logstash crashes when trying to call to_is8601 on a non-existing timestamp.

The fix seems simple : testing if the timestamp field exists before trying to call its to_iso8601 method.
Sorry, I cannot submit a patch since I do not know anything about ruby.
</description><key id="111169059">14086</key><summary>NoMethodError: undefined method `to_iso8601' for nil:NilClass in event.rb</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emmanuelGuiton</reporter><labels /><created>2015-10-13T12:24:33Z</created><updated>2015-10-13T12:53:40Z</updated><resolved>2015-10-13T12:53:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T12:53:39Z" id="147706246">This issue was moved to elastic/logstash#4020
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix numerical issue in function score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14085</link><project id="" key="" /><description>we should sum the weights as double to not lose precision. also,
the tests should simulate exactly what function score does and then test
for equality of scores.

Also, while at it I changed the same thing for explain. 
FiltersFunctionScoreQuery sums up scores and weights and scores as double but when
we explain we cannot get the double scores from the explanation of score
functions. as a result we cannot compute the exact score from the explanations
of the functions alone.
this commit makes the explanation more accurate but also causes the score to be
computed one additional time.
</description><key id="111162017">14085</key><summary>fix numerical issue in function score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Search</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-13T11:37:29Z</created><updated>2015-10-13T17:45:58Z</updated><resolved>2015-10-13T13:21:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-13T12:14:04Z" id="147697705">&gt; also causes the score to be computed one additional time.

This is only the case when calling explain(), right? If so this is no issue at all.
</comment><comment author="brwe" created="2015-10-13T12:37:06Z" id="147702663">&gt; This is only the case when calling explain(), right? 

yes, everything else stays the same

I pushed a new commit.
</comment><comment author="jpountz" created="2015-10-13T13:12:40Z" id="147710165">LGTM
</comment><comment author="brwe" created="2015-10-13T13:21:08Z" id="147711926">@jpountz thanks for the quick review!
</comment><comment author="nik9000" created="2015-10-13T17:45:58Z" id="147791314">Thanks @brwe ! Much better!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Streamline top level reader close listeners and forbid general usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14084</link><project id="" key="" /><description>IndexReader#addReaderCloseListener is very error prone when it comes to
caching and reader wrapping. The listeners are not delegated to the sub readers
nor can it's implementation change since it's final in the base class. This commit
only allows installing close listeners on the top level ElasticsearchDirecotryReader
which is known to work an has a defined lifetime which corresponds to its subreader.
This ensure that caches are cleared once the reader goes out of scope.
</description><key id="111161468">14084</key><summary>Streamline top level reader close listeners and forbid general usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-13T11:33:17Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-14T07:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-13T13:33:34Z" id="147715423">I like the changes, thanks for improving the type safety. when merging master you can drop changes in Versions.java as it now uses a core-closed listener.
</comment><comment author="s1monw" created="2015-10-13T15:29:07Z" id="147750723">@rmuir I found a couple of more bugs, I think you should take another look - this change pays off though.... also @jpountz can you take a look if you have a chance?
</comment><comment author="s1monw" created="2015-10-13T21:33:47Z" id="147860607">@jpountz @rmuir I pushed a new commit cleaning up several things, I think we are pretty ready here
</comment><comment author="jpountz" created="2015-10-13T22:32:25Z" id="147872821">LGTM, just left very minor comments
</comment><comment author="rmuir" created="2015-10-14T00:12:48Z" id="147889305">looks good here too. i have nothing to add to adrien's suggestions. this is a good cleanup!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Geo] Question about Geopoint hashCode/equals contract</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14083</link><project id="" key="" /><description>Hi @nknize, some thing I noticed when working on the Geo-related queries was that the implementation of GeoPoint.hashCode() and equals() does not seem to follow the usual contract that when two objects are equal(), they should return the same hashCode. 
From the current implementation (https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java#L133) it looks like when two points are very close (below the given lat/lon tolerance), they are considered equal, but they will return different hashCode() values. Since we rely on the GeoPoints hashCode/equals implementation in the geo queries, I'm curious about the reasons behind this and wondering if the class should be adapted so the contract holds.
</description><key id="111160298">14083</key><summary>[Geo] Question about Geopoint hashCode/equals contract</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>bug</label></labels><created>2015-10-13T11:24:44Z</created><updated>2015-10-19T19:53:54Z</updated><resolved>2015-10-19T19:53:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-13T11:54:50Z" id="147693881">this sounds like a bug, there shouldn't be any reason for the equals/hashcode contract to be broken other than a bug, maybe the two methods simply got out of sync at some point :)
</comment><comment author="nknize" created="2015-10-14T22:36:34Z" id="148222824">Thanks for the catch @cbuescher! @javanna you're absolutely right, the methods diverged when cutting over to the Lucene Geo encoding which required the tolerance. I'll open a PR to resync the hashCode method. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove "uninverted" and "binary" fielddata support for numeric and boolean fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14082</link><project id="" key="" /><description>Numeric and boolean fields have doc values enabled by default as of
elasticsearch 2.0. This commit removes support for uninverted/in-memory
fielddata, as well as numeric fields encoded in binary doc values which was
the way that elasticsearch stored doc values in a Lucene index before the
1.4 release.

As a consequence, you will only be able to sort and aggregate on numeric and
boolean fields in Elasticsearch 3.0 if doc values have not been switched off.

Relates #14113
</description><key id="111149219">14082</key><summary>Remove "uninverted" and "binary" fielddata support for numeric and boolean fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-10-13T10:12:02Z</created><updated>2016-06-28T09:31:08Z</updated><resolved>2015-10-21T11:03:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-19T14:50:29Z" id="149237998">@nik9000 I pushed a new commit.
</comment><comment author="nik9000" created="2015-10-19T16:32:24Z" id="149272643">Left a minor comment - it looks pretty good to me otherwise. Its nice to see this code go.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document details about config file name suffix impact on parser used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14081</link><project id="" key="" /><description>Config files can be defined in YAML or JSON format.  In most cases the examples are in yaml, but it seems that in some cases a file without a suffix can be interpreted as JSON (even if it is YAML) which will fail.

&gt; @spinscale 
&gt; _Elasticsearch has the notion of a XContent construct, which can either be YAML, JSON, SMILE or CBOR and is usually detected by its first characters_

In specific example, a file was created `shield/user_roles` (note: no `.yml` suffix!) which resulted in this error:

```
[2015-10-11 11:54:23,475][ERROR][shield.authc.support ] [node1] failed to parse role mappings file [/etc/elasticsearch/shield/role_mapping]. skipping/removing all mappings... 
org.elasticsearch.common.settings.SettingsException: Failed to load settings from [/etc/elasticsearch/shield/role_mapping] 
at org.elasticsearch.common.settings.ImmutableSettings$Builder.loadFromStream(ImmutableSettings.java:985) 
at org.elasticsearch.shield.authc.support.DnRoleMapper.parseFile(DnRoleMapper.java:118) 
at org.elasticsearch.shield.authc.support.DnRoleMapper.parseFileLenient(DnRoleMapper.java:102) 
at org.elasticsearch.shield.authc.support.DnRoleMapper.&lt;init&gt;(DnRoleMapper.java:73) 
at org.elasticsearch.shield.authc.ldap.LdapRealm$Factory.create(LdapRealm.java:63) 
at org.elasticsearch.shield.authc.ldap.LdapRealm$Factory.create(LdapRealm.java:43) 
at org.elasticsearch.shield.authc.Realms.initRealms(Realms.java:114) 
at org.elasticsearch.shield.authc.Realms.doStart(Realms.java:56) 
at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85) 
at org.elasticsearch.node.internal.InternalNode.start(InternalNode.java:238) 
at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:128) 
at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:216) 
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32) 
Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Unrecognized token 'admin': was expecting ('true', 'false' or 'null') 
at [Source: admin: 
- "cn=ABC123,ou=Users,dc=corporate" 
; line: 1, column: 6] 
at org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1419) 
at org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508) 
at org.elasticsearch.common.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2300) 
at org.elasticsearch.common.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1459) 
at org.elasticsearch.common.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683) 
at org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:51) 
at org.elasticsearch.common.settings.loader.XContentSettingsLoader.load(XContentSettingsLoader.java:60) 
at org.elasticsearch.common.settings.loader.XContentSettingsLoader.load(XContentSettingsLoader.java:45) 
at org.elasticsearch.common.settings.ImmutableSettings$Builder.loadFromStream(ImmutableSettings.java:982) 
... 12 more 
```

Suggest more explicit documentation around config file type options and the rules governing how the format is guessed if not obvious by file suffix.
</description><key id="111144091">14081</key><summary>Document details about config file name suffix impact on parser used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">robin13</reporter><labels><label>docs</label></labels><created>2015-10-13T09:40:14Z</created><updated>2016-01-15T12:40:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-10-13T11:09:40Z" id="147684749">the config file you are referring to is not an ordinary config file, but one that is not `xContent` compatible, as it basically looks like `/etc/group`. The config file in the exception is another shield file, that indeed is load by `xContent` mechanisms

Checking the exception this is indeed a problem due to `SettingsLoaderFactory.loadFromResource()`, which is checking for the file suffixes and falls back to JSON, see https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/settings/loader/SettingsLoaderFactory.java#L37-L48
</comment><comment author="robin13" created="2015-10-13T12:37:38Z" id="147703031">So is this a problem with the config loader, or with the documentation?  I am confused as to what file formats are possible for which configuration files...
</comment><comment author="spinscale" created="2015-10-13T12:43:34Z" id="147704199">IMO updating the docs to mention the default is JSON (except for files ending in `.yml`, `.yaml` or `.properties` is fine from how I read your problem... but maybe I misunderstood you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ERROR][bootstrap                ] Exception java.lang.Error: The operation completed successfully.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14080</link><project id="" key="" /><description>I get this strange error when trying to start ES on Windows, and am not sure what it means. The entire log is given below. Does anybody know what is the issue?

```
[2015-10-13 00:35:30,772][WARN ][bootstrap                ] jvm uses the client vm, make sure to run `java` with the server vm for best performance by adding `-server` to the command line
[2015-10-13 00:35:30,860][ERROR][bootstrap                ] Exception

java.lang.Error: The operation completed successfully.
at com.sun.jna.Native.open(Native Method)
at com.sun.jna.NativeLibrary.loadLibrary(NativeLibrary.java:171)
at com.sun.jna.NativeLibrary.getInstance(NativeLibrary.java:398)
at com.sun.jna.Native.register(Native.java:1396)
at com.sun.jna.Native.register(Native.java:1156)
at org.elasticsearch.bootstrap.JNAKernel32Library.&lt;init&gt;(JNAKernel32Library.java:53)
at org.elasticsearch.bootstrap.JNAKernel32Library.&lt;init&gt;(JNAKernel32Library.java:37)
at org.elasticsearch.bootstrap.JNAKernel32Library$Holder.&lt;clinit&gt;(JNAKernel32Library.java:47)
at org.elasticsearch.bootstrap.JNAKernel32Library.getInstance(JNAKernel32Library.java:64)
at org.elasticsearch.bootstrap.JNANatives.addConsoleCtrlHandler(JNANatives.java:109)
at org.elasticsearch.bootstrap.Natives.addConsoleCtrlHandler(Natives.java:67)
at org.elasticsearch.bootstrap.Bootstrap.initializeNatives(Bootstrap.java:101)
at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:66)
at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:245)
at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```
</description><key id="111123199">14080</key><summary>[ERROR][bootstrap                ] Exception java.lang.Error: The operation completed successfully.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels><label>:Core</label><label>adoptme</label><label>upgrade</label></labels><created>2015-10-13T07:41:57Z</created><updated>2015-10-16T08:16:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T10:24:22Z" id="147674523">Looks like it may be related to this bug in JNA 4.1.0: https://github.com/java-native-access/jna/issues/517

Do you have Norton security installed by any chance?

Apparently this bug is fixed in 4.2.0.  Any reason for us not to upgrade JNA?
</comment><comment author="rmuir" created="2015-10-13T10:53:35Z" id="147682154">&gt; Apparently this bug is fixed in 4.2.0. Any reason for us not to upgrade JNA?

There is some degree of risk i suppose, but I tested it a few weeks ago and did not encounter any issues (on linux). However, the fact that they don't even know how the fixed the bug in 4.2.0 is not encouraging.
</comment><comment author="ghost" created="2015-10-14T02:46:56Z" id="147908857">Yes, I have Norton Security Suite. Is there something I can do now?
If it is of any help, I encountered this issue on ES 1.5.0 and 1.7.2. Once I moved to 1.4.5 it worked OK.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed Typos</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14079</link><project id="" key="" /><description /><key id="111121527">14079</key><summary>Fixed Typos</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ashwinkumar01</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-13T07:31:35Z</created><updated>2015-10-13T11:53:51Z</updated><resolved>2015-10-13T11:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T10:18:30Z" id="147673298">Hi @ashwinkumar01 
Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="ashwinkumar01" created="2015-10-13T11:09:51Z" id="147684771">Dear Clinton,
I have signed the agreement.

Kind Regards,Ashwin

Date: Tue, 13 Oct 2015 03:19:26 -0700
From: notifications@github.com
To: elasticsearch@noreply.github.com
CC: ashwinkumar01@hotmail.com
Subject: Re: [elasticsearch] Fixed Typos (#14079)

Hi @ashwinkumar01 

Thanks for the PR. Please could I ask you to sign the CLA so that I can merge it in?

http://www.elasticsearch.org/contributor-agreement/

&#8212;
Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2015-10-13T11:53:51Z" id="147693752">thanks @ashwinkumar01 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.action.bulk.BulkRequest force request body has streamSeparator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14078</link><project id="" key="" /><description>I use rabbitmq river to sync log to elasticsearch, but the data I writed do not end with \n ,so it do not add into BulkRequest cause it has no streamSeparator&#12290;
I think end with \n  should not be a necessary condition for.

```
public BulkRequest add(BytesReference data, @Nullable String defaultIndex, @Nullable String defaultType, @Nullable String defaultRouting, @Nullable Object payload, boolean allowExplicitIndex) throws Exception {
        XContent xContent = XContentFactory.xContent(data);
        int line = 0;
        int from = 0;
        int length = data.length();
        byte marker = xContent.streamSeparator();
        while (true) {
            int nextMarker = findNextMarker(marker, from, data, length);
            if (nextMarker == -1) {
                break;
            }

public class JsonXContent implements XContent {
....
@Override
    public byte streamSeparator() {
        return '\n';
    }
```
</description><key id="111113180">14078</key><summary>org.elasticsearch.action.bulk.BulkRequest force request body has streamSeparator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tom916</reporter><labels /><created>2015-10-13T06:18:22Z</created><updated>2015-10-13T06:28:55Z</updated><resolved>2015-10-13T06:27:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-13T06:27:22Z" id="147618087">It must end with \n. By design.
Also rivers have been removed.
</comment><comment author="dadoonet" created="2015-10-13T06:28:55Z" id="147618576">Also you can read this: https://www.elastic.co/guide/en/elasticsearch/guide/current/distrib-multi-doc.html#bulk-format
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> 1.7.1 GROOVY_SCRIPT_BLACKLIST_PATCH /  java.lang.NoSuchFieldError</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14077</link><project id="" key="" /><description>nodeBuilder().clusterName("elasticsearch").client(true).data(false).node();

I use elasticsearch 1.7.1 and Java 8.  This used to work, not sure if that was before Java 8 or a recent upgrade to 1.7.1

Gives the below....pls help why ...

?thrown.cause.localizedMessage        GROOVY_SCRIPT_BLACKLIST_PATCH
?thrown.cause.message         GROOVY_SCRIPT_BLACKLIST_PATCH
?thrown.cause.name        java.lang.NoSuchFieldError

{
  "class": "org.elasticsearch.cluster.settings.ClusterDynamicSettingsModule",
  "method": "&lt;init&gt;",
  "file": "ClusterDynamicSettingsModule.java",
  "line": 105,
  "exact": false,
  "location": "org.elasticsearch-elasticsearch-1.7.1.jar",
  "version": "?"
},
{
  "class": "org.elasticsearch.cluster.ClusterModule",
  "method": "spawnModules",
  "file": "ClusterModule.java",
  "line": 64,
  "exact": false,
  "location": "org.elasticsearch-elasticsearch-1.7.1.jar",
  "version": "?"
},
{
  "class": "org.elasticsearch.common.inject.ModulesBuilder",
  "method": "add",
  "file": "ModulesBuilder.java",
  "line": 44,
  "exact": false,
  "location": "org.elasticsearch-elasticsearch-1.7.1.jar",
  "version": "?"
},
{
  "class": "org.elasticsearch.node.internal.InternalNode",
  "method": "&lt;init&gt;",
  "file": "InternalNode.java",
  "line": 190,
  "exact": false,
  "location": "org.elasticsearch-elasticsearch-1.7.1.jar",
  "version": "?"
},
{
  "class": "org.elasticsearch.node.NodeBuilder",
  "method": "build",
  "file": "NodeBuilder.java",
  "line": 159,
  "exact": false,
  "location": "org.elasticsearch-elasticsearch-1.7.1.jar",
  "version": "?"
},
{
  "class": "org.elasticsearch.node.NodeBuilder",
  "method": "node",
  "file": "NodeBuilder.java",
  "line": 166,
  "exact": false,
  "location": "org.elasticsearch-elasticsearch-1.7.1.jar",
  "version": "?"
},
</description><key id="111098561">14077</key><summary> 1.7.1 GROOVY_SCRIPT_BLACKLIST_PATCH /  java.lang.NoSuchFieldError</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coderlol</reporter><labels /><created>2015-10-13T03:52:33Z</created><updated>2015-10-13T11:55:03Z</updated><resolved>2015-10-13T11:55:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-13T06:32:58Z" id="147619068">I don't have a clear picture of what you did.
I feel like you mixed different versions/jars in the same classloader.
</comment><comment author="coderlol" created="2015-10-13T07:30:59Z" id="147629176">Well, I suspected classloader, but this is the line of code that caused it: nodeBuilder().clusterName("elasticsearch").client(true).data(false).node();

nodeBuilder is from ...

import static org.elasticsearch.node.NodeBuilder.nodeBuilder;

and, the stack trace (in original post) points toward the same jar...
</comment><comment author="dadoonet" created="2015-10-13T07:41:10Z" id="147631701">Sorry. I have absolutely no context. Are you doing that from groovy, Java, Scala?
What is your server version? Is it exactly the same version for the client?

This field exists in 1.7: https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java#L62
</comment><comment author="coderlol" created="2015-10-13T10:28:04Z" id="147675205">Oh, I am using Java on Java 8 ;)  The client and server version is 1.7.1  I've a feeling this may be related to Java 8 unless somehow an incompatible Groovy jar got in the way.
</comment><comment author="coderlol" created="2015-10-13T10:56:17Z" id="147682733">I had the below in my dependency list.  Strange, once upon a time, I had to declare it.  Going to remove the below and see how that goes.

```
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch-lang-groovy&lt;/artifactId&gt;
        &lt;version&gt;2.2.0&lt;/version&gt;
    &lt;/dependency&gt;
```
</comment><comment author="dadoonet" created="2015-10-13T11:55:02Z" id="147693900">Indeed. As you can read on the plugin home page: https://github.com/elastic/elasticsearch-lang-groovy

&gt; Important: This plugin is not needed anymore from elasticsearch 1.4 and is not maintained.

Closing. Feel free to reopen if you think we have an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Long gc pause happened on es1.7.0 plus jdk8u40</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14076</link><project id="" key="" /><description>https://discuss.elastic.co/t/long-gc-pause-happened-on-es1-7-0-plus-jdk8u40/31949
I answered in the discussion forum, but it seems one one can share insight. so I post the problem here to see if can get any help. 
problem:
es1.7.0 plus jdk8u40 and has a very long gc pause in minor gc:
22042 2015-10-10T10:36:44.751+0800: 1954567.631: [GC (Allocation Failure) 1954567.631: [ParNew: 1606667K-&gt;41270K(1763584 K), 1221.6222148 secs] 5895406K-&gt;4332232K(16581312K), 1221.6230128 secs] [Times: user=11780.20 sys=0.00, real=1221 .44 secs]

cluster description:
doc count: 4,695,567
index siez:4.7GB(primary shard + replicas)
index setting:
"number_of_shards": "10"
"number_of_replicas": "1"
node count: 4, 3 of them as master and data node, the other 1 is data node
heap commited: 16GB
GC affect only one node
system load:write with 500tps and search with 300tps
system hardware:256GB ram, 32 core Xeon 2.6GHZ, 1TB spin disk
here is another ES instance with same memory config running on that machine
swap been disabled
</description><key id="111094625">14076</key><summary>Long gc pause happened on es1.7.0 plus jdk8u40</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-10-13T03:09:03Z</created><updated>2015-10-13T09:34:21Z</updated><resolved>2015-10-13T09:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T09:34:21Z" id="147663322">Hi @makeyang 

Let's keep the conversation in one place.  If you find a bug, then we can reopen an issue here. 

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14075</link><project id="" key="" /><description>We don't document this, but there are [some users](https://discuss.elastic.co/t/tribe-node-compatibility/32015) that are federating clusters of different version.

It'd be great if we could document any version restrictions that this has.
</description><key id="111070301">14075</key><summary>Tribe node compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>:Tribe Node</label><label>docs</label></labels><created>2015-10-12T22:50:46Z</created><updated>2017-01-25T15:29:47Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="TinLe" created="2015-10-12T23:10:35Z" id="147545775">+1

We've run into compatibility issues and other problems mixing versions.   As a rule of thumb, it seem safer to use tribe node that lag versions to the clusters it is talking to.

E.g. downstream clusters are running versions M, N and O.   Tribe versions should be &lt; lowest(M,N,O)
</comment><comment author="ppf2" created="2015-11-05T20:13:41Z" id="154177608">+1  esp. since node clients are not compatible between 1.x and 2.x versions.
</comment><comment author="vivekyaji" created="2017-01-25T15:27:59Z" id="275138092">Does this apply for 2.x and 5.x versions? I'm running into issues trying to connect tribe node (2.x) to cluster nodes running on 5.x (Received message from unsupported version: [2.0.0] minimal compatible version is: [5.0.0])
Also tried vise-versa 5.x tribe node and 2.x cluster nodes only to get transport error.
I guess there is no compatibility for tribe nodes between major versions.</comment><comment author="jasontedor" created="2017-01-25T15:29:47Z" id="275138670">&gt; I guess there is no compatibility for tribe nodes between major versions.

There is none.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to lucene-5.4-snapshot-1708254.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14074</link><project id="" key="" /><description /><key id="111069706">14074</key><summary>Upgrade to lucene-5.4-snapshot-1708254.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-12T22:44:56Z</created><updated>2015-10-16T07:44:59Z</updated><resolved>2015-10-16T07:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-10-13T13:46:45Z" id="147719655">LGTM
</comment><comment author="rmuir" created="2015-10-13T13:49:18Z" id="147721135">I don't see regenerated sha1s... did you run `dev-tools/update_lucene.sh` ?
</comment><comment author="jpountz" created="2015-10-14T09:39:06Z" id="147991032">@rmuir good call, I thought I had run verify but apparently not. I just updated the PR
</comment><comment author="rmuir" created="2015-10-14T14:04:59Z" id="148060105">looks good, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to maven-assembly-plugin 2.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14073</link><project id="" key="" /><description>- [https://issues.apache.org/jira/browse/MASSEMBLY-780](MASSEMBLY-780): Snappy supported

Unsure if we want to push it in 2.0 as well.
</description><key id="111053823">14073</key><summary>Update to maven-assembly-plugin 2.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-12T20:53:13Z</created><updated>2015-10-13T08:00:27Z</updated><resolved>2015-10-13T08:00:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-13T07:40:16Z" id="147631415">LGTM but I would suggest not to push to 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused clear(IndexReader) method from IndexFieldData</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14072</link><project id="" key="" /><description>This method is unused can can simply be removed. It's rather confusing
instead since it's another way of invalidating a cache entry but not through
the close listener.
</description><key id="111052448">14072</key><summary>Remove unused clear(IndexReader) method from IndexFieldData</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-12T20:45:27Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-13T09:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-12T20:52:20Z" id="147516316">LGTM
</comment><comment author="jpountz" created="2015-10-12T21:19:10Z" id="147525577">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Never wrap searcher for internal engine operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14071</link><project id="" key="" /><description>there is no need to wrap the searcher for internal operations like version
checks for indexing. In-fact this can even lead to crazy bugs if the wrapper
filters documents that are updated etc. such that we end up with either duplicated docs
if create is used or we update wrong version under certain conditions.

Note: master doesn't have this particular problem since it wraps searchers on the `IndexShard` level
</description><key id="111047946">14071</key><summary>Never wrap searcher for internal engine operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>:Internal</label><label>bug</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-12T20:17:03Z</created><updated>2015-11-22T10:15:44Z</updated><resolved>2015-10-13T09:53:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-12T21:29:46Z" id="147527529">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make PerThreadIDAndVersionLookup per-segment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14070</link><project id="" key="" /><description>Currently this caches some state per-top-level-reader.

This currently has a downside in that it makes managing the cache tricky when readers are wrapped, since the map is cleared by `addReaderClosedListener`. 

As a per-segment cache, things are simpler for that case since we can just use `addCoreClosedListener`, which will evict from this cache when the underlying segment's core is merged away. Its effectively required for reader wrappers to delegate `getCoreCacheKey` for good performance with elasticsearch (otherwise you would e.g. build fielddata and so on per-query), so its not a limitation.

It also addresses the TODO in the code about handling reopen...

If @mikemccand benchmarking is ok, I will add unit tests to this PR, we should be able to easily test this thing.
</description><key id="111022860">14070</key><summary>Make PerThreadIDAndVersionLookup per-segment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>blocker</label><label>enhancement</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-12T17:47:19Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-13T13:01:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-10-12T18:56:23Z" id="147491005">LGTM, this is a nice cleanup (1 TermsEnum per segment core, instead of 1 per top-level reader X segment core).

I benchmarked indexing at defaults and it looks like any performance change is within noise.
</comment><comment author="s1monw" created="2015-10-12T19:15:27Z" id="147495124">LGTM too - I think as a followup we can remove the support for payloads in master since we don't support indices with payloads anymore.
</comment><comment author="rmuir" created="2015-10-13T10:55:18Z" id="147682515">I added some tests for cache eviction of readers and so on here. I think its ready.
</comment><comment author="s1monw" created="2015-10-13T12:32:14Z" id="147701653">LGTM thanks for all the tests
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove @Test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14069</link><project id="" key="" /><description>There are three ways `@Test` was used. Way one:

``` java
@Test
public void flubTheBlort() {
```

This way was always replaced with:

``` java
public void testFlubTheBlort() {
```

Or, maybe with a better method name if I was feeling generous.

Way two:

``` java
@Test(throws=IllegalArgumentException.class)
public void testFoo() {
    methodThatThrows();
}
```

This way of using `@Test` is actually pretty OK, but to get the tools to ban
`@Test` entirely it can't be used. Instead:

``` java
public void testFoo() {
    try {
        methodThatThrows();
        fail("Expected IllegalArgumentException");
    } catch (IllegalArgumentException e ) {
        assertThat(e.getMessage(), containsString("something"));
    }
}
```

This is longer but tests more than the old ways and is much more precise.
Compare:

``` java
@Test(throws=IllegalArgumentException.class)
public void testFoo() {
    some();
    copy();
    and();
    pasted();
    methodThatThrows();
    code();  // &lt;---- This was left here by mistake and is never called
}
```

to:

``` java
@Test(throws=IllegalArgumentException.class)
public void testFoo() {
    some();
    copy();
    and();
    pasted();
    try {
        methodThatThrows();
        fail("Expected IllegalArgumentException");
    } catch (IllegalArgumentException e ) {
        assertThat(e.getMessage(), containsString("something"));
    }
}
```

The final use of test is:

``` java
@Test(timeout=1000)
public void testFoo() {
    methodThatWasSlow();
}
```

This is the most insidious use of `@Test` because its tempting but tragically
flawed. Its flaws are:
1. Hard and fast timeouts can look like they are asserting that something is
faster and even do an ok job of it when you compare the timings on the same
machine but as soon as you take them to another machine they start to be
invalid. On a slow VM both the new and old methods fail. On a super-fast
machine the slower and faster ways succeed.
2. Tests often contain slow `assert` calls so the performance of tests isn't
sure to predict the performance of non-test code.
3. These timeouts are rude to debuggers because the test just drops out from
under it after the timeout.

Confusingly, timeouts are useful in tests because it'd be rude for a broken
test to cause CI to abort the whole build after it hits a global timeout. But
those timeouts should be very very long "backstop" timeouts and aren't useful
assertions about speed.

For all its flaws `@Test(timeout=1000)` doesn't have a good replacement **in**
**tests**. Nightly benchmarks like http://benchmarks.elasticsearch.org/ are
useful here because they run on the same machine but they aren't quick to check
and it takes lots of time to figure out the regressions. Sometimes its useful
to compare dueling implementations but that requires keeping both
implementations around. All and all we don't have a satisfactory answer to the
question "what do you replace `@Test(timeout=1000)`" with. So we handle each
occurrence on a case by case basis.

For files with `@Test` this also:
1. Removes excess blank lines. They don't help anything.
2. Removes underscores from method names. Those would fail any code style
checks we ever care to run and don't add to readability. Since I did this manually
I didn't do it consistently.
3. Make sure all test method names start with `test`. Some used to end in `Test` or start
with `verify` or `check` and they were picked up using the annotation. Without the
annotation they always need to start with `test`.
4. Organizes imports using the rules we generate for Eclipse. For the most part
this just removes `*` imports which is a win all on its own. It was "required"
to quickly remove `@Test`.
5. Removes unneeded casts. This is just a setting I have enabled in Eclipse and
forgot to turn off before I did this work. It probably isn't hurting anything.
6. Removes trailing whitespace. Again, another Eclipse setting I forgot to turn
off that doesn't hurt anything. Hopefully.
7. Swaps some tests override superclass tests to make them empty with
`assumeTrue` so that the reasoning for the skips is logged in the test run and
it doesn't "look like" that thing is being tested when it isn't.
8. Adds an oxford comma to an error message.

Closes #14028
</description><key id="110997655">14069</key><summary>Remove @Test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-12T15:22:44Z</created><updated>2015-10-20T22:33:46Z</updated><resolved>2015-10-20T22:33:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-12T18:21:46Z" id="147481631">I glanced at the changes, but the diff is too large for github UI to handle. Based on the description of the automated changes made, +1.
</comment><comment author="nik9000" created="2015-10-12T18:47:24Z" id="147489114">&gt; I glanced at the changes, but the diff is too large for github UI to handle. Based on the description of the automated changes made, +1.

Thanks. Yeah. Its a mess. You can get the changes with:

```
git diff a3fd6a2ad061bd1cdc845f110614ce737c66b573..
```

I'd probably wait until I remove WIP to do that though - its lots of little things.

I'm doing these changes by hand so I have to glance at the method names so I can avoid stuff like `testVerifyTheThing` and `testSomeStuffTest`. I thought about writing a script to do it but I'm not sure it'd really be faster.....
</comment><comment author="nik9000" created="2015-10-14T13:20:56Z" id="148047389">The total test count doesn't change. I know. I counted.

``` bash
git checkout master &amp;&amp; mvn clean &amp;&amp; mvn install | tee with_test
git no_test_annotation master &amp;&amp; mvn clean &amp;&amp; mvn install | tee not_test
grep 'Tests summary' with_test &gt; with_test_summary
grep 'Tests summary' not_test &gt; not_test_summary
diff with_test_summary not_test_summary
```

These differ somewhat because some tests are skipped based on the random seed.
The total shouldn't differ. But it does!

```
1c1
&lt; [INFO] Tests summary: 564 suites (1 ignored), 3171 tests, 31 ignored (31 assumptions)
---
&gt; [INFO] Tests summary: 564 suites (1 ignored), 3167 tests, 17 ignored (17 assumptions)
```

These are the core unit tests. So we dig further:

``` bash
cat with_test | perl -pe 's/\n// if /^Suite/;s/.*\n// if /IGNOR/;s/.*\n// if /Assumption #/;s/.*\n// if /HEARTBEAT/;s/Completed .+?,//' | grep Suite &gt; with_test_suites
cat not_test | perl -pe 's/\n// if /^Suite/;s/.*\n// if /IGNOR/;s/.*\n// if /Assumption #/;s/.*\n// if /HEARTBEAT/;s/Completed .+?,//' | grep Suite &gt; not_test_suites
diff &lt;(sort with_test_suites) &lt;(sort not_test_suites)
```

The four tests with lower test numbers are all extend `AbstractQueryTestCase`
and all have a method that looks like this:

``` java
@Override
public void testToQuery() throws IOException {
    assumeTrue("test runs only when at least a type is registered", getCurrentTypes().length &gt; 0);
    super.testToQuery();
}
```

It looks like this method was being double counted on master and isn't anymore.
</comment><comment author="nik9000" created="2015-10-14T13:23:27Z" id="148047903">@rjernst or some other unlucky victim, could you scan this? Github's ui doesn't work for it so you'd have to do:

``` bash
git diff a3fd6a2ad061bd1cdc845f110614ce737c66b573..
```

a3fd6a2ad061bd1cdc845f110614ce737c66b573 is what I branched this PR from.

I'm inclined to squash this one once review is done and then rebase on master. As informative as the commit message are the PR's text and the comment above would make a better commit message.
</comment><comment author="javanna" created="2015-10-14T13:57:01Z" id="148057108">&gt; The four tests with lower test numbers are all extend AbstractQueryTestCase. It looks like this method was being double counted on master and isn't anymore.

@nik9000 makes sense to me. 
</comment><comment author="rmuir" created="2015-10-14T14:31:40Z" id="148067833">+1: i quickly sanity checked the 2.4 MB patch and it looks fine. A real test logic improvement for all of our exceptions tests as now the messages are checked in many more places!

&gt; @rjernst or some other unlucky victim, could you scan this? Github's ui doesn't work for it so you'd have to do:

big ones can be done in these two ways: 
- https://github.com/elastic/elasticsearch/pull/14069.patch 
- https://github.com/elastic/elasticsearch/pull/14069.diff
</comment><comment author="nik9000" created="2015-10-14T15:01:33Z" id="148077254">&gt; +1: i quickly sanity checked the 2.4 MB patch and it looks fine. A real test logic improvement for all of our exceptions tests as now the messages are checked in many more places!

Yeah, sorry about that. I thought about doing it some more github friendly way but didn't end up doing it. Sorry.

&gt; big ones can be done in these two ways:
&gt; 
&gt; https://github.com/elastic/elasticsearch/pull/14069.patch
&gt; https://github.com/elastic/elasticsearch/pull/14069.diff

Yeah - those work too!
</comment><comment author="rjernst" created="2015-10-14T16:49:00Z" id="148113685">LGTM too
</comment><comment author="nik9000" created="2015-10-14T17:36:07Z" id="148128605">OK - I'm going to squash and rebase now. It'll probably be a few hours to a day before I can push this though.
</comment><comment author="nik9000" created="2015-10-14T18:47:52Z" id="148154083">I added another commit that fixes a bug that I found in elasticseach that I found while doing this same activity on the x-plugins. I figure its safe here give that this is going in and doesn't need an extra pr.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cache#computeIfAbsent no longer ensures at-most-once invocation of load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14068</link><project id="" key="" /><description>Previously, Cache#computeIfAbsent was implemented to ensure that the
loader for a key was invoked at most once. However, this can lead to
deadlocks in situations where dependent key loading is occurring
because of the locks taken to ensure that the loader is invoked at once
per key. This commit changes the behavior of Cache#computeIfAbsent to
not be atomic and therefore no longer ensures that the loader is
invoked at most once per key.

Closes #14090 
</description><key id="110984105">14068</key><summary>Cache#computeIfAbsent no longer ensures at-most-once invocation of load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>bug</label></labels><created>2015-10-12T14:22:17Z</created><updated>2015-10-14T14:51:32Z</updated><resolved>2015-10-14T01:22:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-12T14:22:29Z" id="147412048">@jpountz or @nik9000 can you take a look?
</comment><comment author="nik9000" created="2015-10-12T14:31:12Z" id="147413929">LGTM. Maybe in another PR add a test that tries to deadlock the thing with recursive loads.
</comment><comment author="jpountz" created="2015-10-12T21:16:55Z" id="147525189">I think this can be a useful feature for a cache? Alternatively maybe we could try to call the loader outside of the lock, for instance by making the map use `Future`s of the values instead of the values themselves?
</comment><comment author="jasontedor" created="2015-10-14T01:22:35Z" id="147898659">Closed in favor of #14091.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Submit coverage reports to Codecov</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14067</link><project id="" key="" /><description>Hey guys, Steve from Codecov here. I discovered this issue #13260 and though Codecov could help with  code coverage. Happy to take any feedback and help with integration. 

I tried to submit coverage reports but tests are not passing atm, but I can rebase if you get those tests passing again to view examples.

Cheers,
Steve
</description><key id="110981125">14067</key><summary>Submit coverage reports to Codecov</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stevepeak</reporter><labels /><created>2015-10-12T14:04:30Z</created><updated>2016-03-10T12:25:33Z</updated><resolved>2016-03-10T12:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-12T14:24:34Z" id="147412478">&gt; I tried to submit coverage reports but tests are not passing atm, but I can rebase if you get those tests passing again to view examples.

Strange! What are you seeing? We've got them running in [CI](https://build.elastic.co/) and they seem to be passing pretty [consistently](http://build-us-00.elastic.co/job/es_core_master_metal/). I'm honestly not sure about running the builds in travis - the last I checked, more than a year ago, the travis workers had real trouble finishing the build before they timed out and it will have only gotten harder on them.

One of the issues with measuring code coverage for Elasticsearch is [randomized testing](http://labs.carrotsearch.com/randomizedtesting.html). Essentially Elasticsearch doesn't try to cover all of the code paths in every test run, rather it covers many interesting code paths in every run against a random backdrop of other code paths. I don't know how much that'd effect code coverage metrics. Certainly some, but it might just be a couple of percentage points overall.
</comment><comment author="stevepeak" created="2015-10-12T14:30:29Z" id="147413769">@nik9000 Thank you for the reply. I can see the issues related to travis, and thats ok. Codecov supports Jenkins and other self-hosted CI solutions.

The randomized testing is interesting... IMO it would be real interesting to see what lines of code got executed during these "random" tests. Codecov can give you that data. You can always disable our comments and statuses too. 

Do you run multiple "random" CI builds on each commit? Codecov does aggregate reports very well. Here is an example: https://codecov.io/github/pyca/cryptography where there over 20 builds being combined per commit.

If you have any suggestions on how Codecov would be a great use case for Elastic Search I would love to chat.

Thank you!
</comment><comment author="clintongormley" created="2016-03-10T12:25:33Z" id="194818553">Hi @stevepeak 

I see this dropped off our radar.  Maybe worth starting this discussion on the forums instead? Given @nik9000 's comment, I'm going to close this PR.  thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>changed ben to been</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14066</link><project id="" key="" /><description /><key id="110977796">14066</key><summary>changed ben to been</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sarwarbhuiyan</reporter><labels><label>docs</label><label>v2.0.0</label></labels><created>2015-10-12T13:43:41Z</created><updated>2015-10-13T07:46:32Z</updated><resolved>2015-10-13T07:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>request circuit breaker estimated size is stuck</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14065</link><project id="" key="" /><description>Hi guys
few days ago circuit breaker started to trip very often and I noticed that estimated size in node stats is "stuck" for 2 out of 4 data nodes that I have. 
with 

```
http://localhost:9200/_nodes/stats-node-0*/stats/breaker
```

I made a graph where you can find estimated size in bytes field over time (red and yellow nodes are stuck)
 ![graph](http://i.imgur.com/fwlJeMt.png) As you can see, there are spikes when I run queries (e.g. 14:11 or 14:23) but on red and yellow nodes the value never drops to 0 again but is stuck at 8.4 and 3.4GB. At 14:33 I decided to restart yellow node and this one works as expected since.

I tried to clear cache but with no success.

Is this a bug or feature? Is the only solution for this to restart service?

I`m using ES 1.7.1, data nodes have 72GB of ram, ES uses 30GB, limits for field data is 40% and for requests 60% of heap
</description><key id="110977549">14065</key><summary>request circuit breaker estimated size is stuck</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">zlosim</reporter><labels><label>:Circuit Breakers</label></labels><created>2015-10-12T13:42:00Z</created><updated>2015-10-16T14:41:13Z</updated><resolved>2015-10-16T14:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-12T22:50:50Z" id="147542946">Hi @zlosim,

Can you post the response for the breaker stats? Is this the request breaker or the field data breaker? Can you provide the output of `curl -s 'http://localhost:9200/_nodes/stats?human&amp;fields=*&amp;pretty'` for me to take a look at?
</comment><comment author="zlosim" created="2015-10-13T06:47:44Z" id="147621823">of course
red node is "stats-node-01"

```
{
  "cluster_name": "stats-es-cluster",
  "nodes": {
    "xyuOtEpKTHadqtG9aT2spw": {
      "timestamp": 1444718665769,
      "name": "stats-master-node-03",
      "transport_address": "inet[/10.0.1.63:9300]",
      "host": "localhost",
      "ip": [
        "inet[/10.0.1.63:9300]",
        "NONE"
      ],
      "attributes": {
        "data": "false",
        "master": "true"
      },
      "indices": {
        "docs": {
          "count": 0,
          "deleted": 0
        },
        "store": {
          "size": "0b",
          "size_in_bytes": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "indexing": {
          "index_total": 0,
          "index_time": "0s",
          "index_time_in_millis": 0,
          "index_current": 0,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "get": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "exists_total": 0,
          "exists_time": "0s",
          "exists_time_in_millis": 0,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 0,
          "query_total": 0,
          "query_time": "0s",
          "query_time_in_millis": 0,
          "query_current": 0,
          "fetch_total": 0,
          "fetch_time": "0s",
          "fetch_time_in_millis": 0,
          "fetch_current": 0
        },
        "merges": {
          "current": 0,
          "current_docs": 0,
          "current_size": "0b",
          "current_size_in_bytes": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0,
          "total_docs": 0,
          "total_size": "0b",
          "total_size_in_bytes": 0
        },
        "refresh": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "flush": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "warmer": {
          "current": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "filter_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 0,
          "memory": "0b",
          "memory_in_bytes": 0,
          "index_writer_memory": "0b",
          "index_writer_memory_in_bytes": 0,
          "index_writer_max_memory": "0b",
          "index_writer_max_memory_in_bytes": 0,
          "version_map_memory": "0b",
          "version_map_memory_in_bytes": 0,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 0,
          "size": "0b",
          "size_in_bytes": 0
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        }
      },
      "os": {
        "timestamp": 1444718665769,
        "uptime": "58.9m",
        "uptime_in_millis": 3534817,
        "load_average": [
          0.29,
          0.24,
          0.23
        ],
        "cpu": {
          "sys": 0,
          "user": 3,
          "idle": 95,
          "usage": 3,
          "stolen": 0
        },
        "mem": {
          "free": "266.3mb",
          "free_in_bytes": 279330816,
          "used": "1.6gb",
          "used_in_bytes": 1819824128,
          "free_percent": 30,
          "used_percent": 69,
          "actual_free": "609.3mb",
          "actual_free_in_bytes": 638951424,
          "actual_used": "1.3gb",
          "actual_used_in_bytes": 1460203520
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718665769,
        "open_file_descriptors": 321,
        "cpu": {
          "percent": 6,
          "sys": "59.4m",
          "sys_in_millis": 3565010,
          "user": "15h",
          "user_in_millis": 54013840,
          "total": "15.9h",
          "total_in_millis": 57578850
        },
        "mem": {
          "resident": "1.2gb",
          "resident_in_bytes": 1312681984,
          "share": "22.2mb",
          "share_in_bytes": 23314432,
          "total_virtual": "3.3gb",
          "total_virtual_in_bytes": 3616681984
        }
      },
      "jvm": {
        "timestamp": 1444718664912,
        "uptime": "17.6d",
        "uptime_in_millis": 1527432453,
        "mem": {
          "heap_used": "671.7mb",
          "heap_used_in_bytes": 704373024,
          "heap_used_percent": 66,
          "heap_committed": "1007.3mb",
          "heap_committed_in_bytes": 1056309248,
          "heap_max": "1007.3mb",
          "heap_max_in_bytes": 1056309248,
          "non_heap_used": "52.9mb",
          "non_heap_used_in_bytes": 55573176,
          "non_heap_committed": "54.2mb",
          "non_heap_committed_in_bytes": 56852480,
          "pools": {
            "young": {
              "used": "121.1mb",
              "used_in_bytes": 127052856,
              "max": "133.1mb",
              "max_in_bytes": 139591680,
              "peak_used": "133.1mb",
              "peak_used_in_bytes": 139591680,
              "peak_max": "133.1mb",
              "peak_max_in_bytes": 139591680
            },
            "survivor": {
              "used": "16.6mb",
              "used_in_bytes": 17432576,
              "max": "16.6mb",
              "max_in_bytes": 17432576,
              "peak_used": "16.6mb",
              "peak_used_in_bytes": 17432576,
              "peak_max": "16.6mb",
              "peak_max_in_bytes": 17432576
            },
            "old": {
              "used": "533.9mb",
              "used_in_bytes": 559887592,
              "max": "857.6mb",
              "max_in_bytes": 899284992,
              "peak_used": "768mb",
              "peak_used_in_bytes": 805358416,
              "peak_max": "857.6mb",
              "peak_max_in_bytes": 899284992
            }
          }
        },
        "threads": {
          "count": 30,
          "peak_count": 36
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 82454,
              "collection_time": "1.8h",
              "collection_time_in_millis": 6712988
            },
            "old": {
              "collection_count": 4058,
              "collection_time": "5.5m",
              "collection_time_in_millis": 333793
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 33,
            "used": "6.7mb",
            "used_in_bytes": 7101880,
            "total_capacity": "6.7mb",
            "total_capacity_in_bytes": 7101880
          },
          "mapped": {
            "count": 0,
            "used": "0b",
            "used_in_bytes": 0,
            "total_capacity": "0b",
            "total_capacity_in_bytes": 0
          }
        }
      },
      "thread_pool": {
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_started": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "listener": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 28
        },
        "index": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "refresh": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "generic": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 7,
          "completed": 1678838
        },
        "warmer": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "search": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "flush": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_store": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 2,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 2,
          "completed": 562403
        },
        "get": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        }
      },
      "network": {
        "tcp": {
          "active_opens": 1353416,
          "passive_opens": 5670346,
          "curr_estab": 235,
          "in_segs": 77303722,
          "out_segs": 63919886,
          "retrans_segs": 972,
          "estab_resets": 41,
          "attempt_fails": 69,
          "in_errs": 6,
          "out_rsts": 5039
        }
      },
      "fs": {
        "timestamp": 1444718665770,
        "total": {
          "total": "9.8gb",
          "total_in_bytes": 10534313984,
          "free": "8.1gb",
          "free_in_bytes": 8787390464,
          "available": "7.7gb",
          "available_in_bytes": 8326406144,
          "disk_reads": 57658,
          "disk_writes": 2125374,
          "disk_io_op": 2183032,
          "disk_read_size": "664.9mb",
          "disk_read_size_in_bytes": 697246720,
          "disk_write_size": "23.7gb",
          "disk_write_size_in_bytes": 25453342720,
          "disk_io_size": "24.3gb",
          "disk_io_size_in_bytes": 26150589440,
          "disk_queue": "0",
          "disk_service_time": "1.3"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/",
            "dev": "/dev/vda1",
            "type": "ext4",
            "total": "9.8gb",
            "total_in_bytes": 10534313984,
            "free": "8.1gb",
            "free_in_bytes": 8787390464,
            "available": "7.7gb",
            "available_in_bytes": 8326406144,
            "disk_reads": 57658,
            "disk_writes": 2125374,
            "disk_io_op": 2183032,
            "disk_read_size": "664.9mb",
            "disk_read_size_in_bytes": 697246720,
            "disk_write_size": "23.7gb",
            "disk_write_size_in_bytes": 25453342720,
            "disk_io_size": "24.3gb",
            "disk_io_size_in_bytes": 26150589440,
            "disk_queue": "0",
            "disk_service_time": "1.3"
          }
        ]
      },
      "transport": {
        "server_open": 117,
        "rx_count": 3146458,
        "rx_size": "559.7gb",
        "rx_size_in_bytes": 601035721849,
        "tx_count": 3146461,
        "tx_size": "210.6mb",
        "tx_size_in_bytes": 220934079
      },
      "http": {
        "current_open": 0,
        "total_opened": 578456
      },
      "breakers": {
        "fielddata": {
          "limit_size_in_bytes": 422523699,
          "limit_size": "402.9mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1.03,
          "tripped": 0
        },
        "request": {
          "limit_size_in_bytes": 633785548,
          "limit_size": "604.4mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 739416473,
          "limit_size": "705.1mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "o-xb7NuLQrSF3ClW7-IWzg": {
      "timestamp": 1444718665772,
      "name": "stats-node-04",
      "transport_address": "inet[/10.0.1.34:9301]",
      "host": "es-dwh-04",
      "ip": [
        "inet[/10.0.1.34:9301]",
        "NONE"
      ],
      "attributes": {
        "master": "false"
      },
      "indices": {
        "docs": {
          "count": 522245081,
          "deleted": 0
        },
        "store": {
          "size": "288.2gb",
          "size_in_bytes": 309471962019,
          "throttle_time": "8.1m",
          "throttle_time_in_millis": 491985
        },
        "indexing": {
          "index_total": 16173453,
          "index_time": "15.4h",
          "index_time_in_millis": 55480382,
          "index_current": 37069,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "get": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "exists_total": 0,
          "exists_time": "0s",
          "exists_time_in_millis": 0,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 4,
          "query_total": 56574,
          "query_time": "4.9h",
          "query_time_in_millis": 17731724,
          "query_current": 0,
          "fetch_total": 22,
          "fetch_time": "7.4s",
          "fetch_time_in_millis": 7486,
          "fetch_current": 0
        },
        "merges": {
          "current": 0,
          "current_docs": 0,
          "current_size": "0b",
          "current_size_in_bytes": 0,
          "total": 19918,
          "total_time": "8.4h",
          "total_time_in_millis": 30569345,
          "total_docs": 155637575,
          "total_size": "105.2gb",
          "total_size_in_bytes": 112974788563
        },
        "refresh": {
          "total": 175230,
          "total_time": "11.2h",
          "total_time_in_millis": 40409145
        },
        "flush": {
          "total": 79124,
          "total_time": "30.1m",
          "total_time_in_millis": 1810492
        },
        "warmer": {
          "current": 0,
          "total": 31331,
          "total_time": "52.2s",
          "total_time_in_millis": 52214
        },
        "filter_cache": {
          "memory_size": "5.8mb",
          "memory_size_in_bytes": 6107200,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "5.2mb",
          "memory_size_in_bytes": 5526224,
          "evictions": 0,
          "fields": {
            "static.gender": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "project": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.sub_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.obj_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.name": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.action": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.login_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "timestamp": {
              "memory_size": "5.2mb",
              "memory_size_in_bytes": 5526224
            },
            "event_data.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.sub_camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.country": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            }
          }
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 5133,
          "memory": "889.4mb",
          "memory_in_bytes": 932620638,
          "index_writer_memory": "230.6mb",
          "index_writer_memory_in_bytes": 241826684,
          "index_writer_max_memory": "7gb",
          "index_writer_max_memory_in_bytes": 7620909295,
          "version_map_memory": "1.1mb",
          "version_map_memory_in_bytes": 1224728,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 333505,
          "size": "17b",
          "size_in_bytes": 17
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "26.3m",
          "throttle_time_in_millis": 1578114
        }
      },
      "os": {
        "timestamp": 1444718666550,
        "uptime": "18.3m",
        "uptime_in_millis": 1103995,
        "load_average": [
          4.22,
          5.69,
          5.38
        ],
        "cpu": {
          "sys": 1,
          "user": 7,
          "idle": 85,
          "usage": 8,
          "stolen": 3
        },
        "mem": {
          "free": "348.6mb",
          "free_in_bytes": 365604864,
          "used": "62.5gb",
          "used_in_bytes": 67197784064,
          "free_percent": 47,
          "used_percent": 52,
          "actual_free": "29.9gb",
          "actual_free_in_bytes": 32132190208,
          "actual_used": "32.9gb",
          "actual_used_in_bytes": 35431198720
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718666562,
        "open_file_descriptors": 28059,
        "cpu": {
          "percent": 179,
          "sys": "6h",
          "sys_in_millis": 21642970,
          "user": "1.1d",
          "user_in_millis": 95697700,
          "total": "1.3d",
          "total_in_millis": 117340670
        },
        "mem": {
          "resident": "31.5gb",
          "resident_in_bytes": 33883381760,
          "share": "523.5mb",
          "share_in_bytes": 549031936,
          "total_virtual": "132.5gb",
          "total_virtual_in_bytes": 142369538048
        }
      },
      "jvm": {
        "timestamp": 1444718667794,
        "uptime": "18.1h",
        "uptime_in_millis": 65439815,
        "mem": {
          "heap_used": "6.1gb",
          "heap_used_in_bytes": 6551270000,
          "heap_used_percent": 20,
          "heap_committed": "29.8gb",
          "heap_committed_in_bytes": 32098877440,
          "heap_max": "29.8gb",
          "heap_max_in_bytes": 32098877440,
          "non_heap_used": "66.7mb",
          "non_heap_used_in_bytes": 70037584,
          "non_heap_committed": "96.8mb",
          "non_heap_committed_in_bytes": 101588992,
          "pools": {
            "young": {
              "used": "375.5mb",
              "used_in_bytes": 393791072,
              "max": "865.3mb",
              "max_in_bytes": 907345920,
              "peak_used": "865.3mb",
              "peak_used_in_bytes": 907345920,
              "peak_max": "865.3mb",
              "peak_max_in_bytes": 907345920
            },
            "survivor": {
              "used": "100.4mb",
              "used_in_bytes": 105292896,
              "max": "108.1mb",
              "max_in_bytes": 113377280,
              "peak_used": "108.1mb",
              "peak_used_in_bytes": 113377280,
              "peak_max": "108.1mb",
              "peak_max_in_bytes": 113377280
            },
            "old": {
              "used": "5.6gb",
              "used_in_bytes": 6052186032,
              "max": "28.9gb",
              "max_in_bytes": 31078154240,
              "peak_used": "28.9gb",
              "peak_used_in_bytes": 31078154240,
              "peak_max": "28.9gb",
              "peak_max_in_bytes": 31078154240
            }
          }
        },
        "threads": {
          "count": 325,
          "peak_count": 523
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 10756,
              "collection_time": "1.8h",
              "collection_time_in_millis": 6746922
            },
            "old": {
              "collection_count": 504,
              "collection_time": "1.3h",
              "collection_time_in_millis": 4805800
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 3058,
            "used": "78.8mb",
            "used_in_bytes": 82628584,
            "total_capacity": "78.8mb",
            "total_capacity_in_bytes": 82628584
          },
          "mapped": {
            "count": 8772,
            "used": "93.4gb",
            "used_in_bytes": 100300504649,
            "total_capacity": "93.4gb",
            "total_capacity_in_bytes": 100300504649
          }
        }
      },
      "thread_pool": {
        "generic": {
          "threads": 6,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 115,
          "completed": 782897
        },
        "index": {
          "threads": 16,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 16,
          "completed": 16250895
        },
        "fetch_shard_store": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 25056
        },
        "get": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 19555
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 75211
        },
        "warmer": {
          "threads": 3,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 126260
        },
        "flush": {
          "threads": 5,
          "queue": 5,
          "active": 5,
          "rejected": 0,
          "largest": 5,
          "completed": 778032
        },
        "search": {
          "threads": 182,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 286,
          "completed": 2046214
        },
        "fetch_shard_started": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "listener": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 150389
        },
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 5,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 5,
          "completed": 29051630
        },
        "refresh": {
          "threads": 8,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 8,
          "completed": 87421
        }
      },
      "network": {
        "tcp": {
          "active_opens": 360308,
          "passive_opens": 1997165,
          "curr_estab": 405,
          "in_segs": 1942043945,
          "out_segs": 2623315725,
          "retrans_segs": 817992,
          "estab_resets": 474,
          "attempt_fails": 5135,
          "in_errs": 74,
          "out_rsts": 2646
        }
      },
      "fs": {
        "timestamp": 1444718667799,
        "total": {
          "total": "1.9tb",
          "total_in_bytes": 2147415490560,
          "free": "1.1tb",
          "free_in_bytes": 1222250770432,
          "available": "1.1tb",
          "available_in_bytes": 1222250770432,
          "disk_reads": 13437443,
          "disk_writes": 73038129,
          "disk_io_op": 86475572,
          "disk_read_size": "874.4gb",
          "disk_read_size_in_bytes": 938942233600,
          "disk_write_size": "3.4tb",
          "disk_write_size_in_bytes": 3824271289344,
          "disk_io_size": "4.3tb",
          "disk_io_size_in_bytes": 4763213522944,
          "disk_queue": "0",
          "disk_service_time": "2.4"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/var/lib/elasticsearch",
            "dev": "/dev/vdb1",
            "type": "xfs",
            "total": "1.9tb",
            "total_in_bytes": 2147415490560,
            "free": "1.1tb",
            "free_in_bytes": 1222250770432,
            "available": "1.1tb",
            "available_in_bytes": 1222250770432,
            "disk_reads": 13437443,
            "disk_writes": 73038129,
            "disk_io_op": 86475572,
            "disk_read_size": "874.4gb",
            "disk_read_size_in_bytes": 938942233600,
            "disk_write_size": "3.4tb",
            "disk_write_size_in_bytes": 3824271289344,
            "disk_io_size": "4.3tb",
            "disk_io_size_in_bytes": 4763213522944,
            "disk_queue": "0",
            "disk_service_time": "2.4"
          }
        ]
      },
      "transport": {
        "server_open": 286,
        "rx_count": 88387564,
        "rx_size": "270.2gb",
        "rx_size_in_bytes": 290228350091,
        "tx_count": 70991481,
        "tx_size": "134.8gb",
        "tx_size_in_bytes": 144778269313
      },
      "breakers": {
        "fielddata": {
          "limit_size_in_bytes": 12839550976,
          "limit_size": "11.9gb",
          "estimated_size_in_bytes": 5526224,
          "estimated_size": "5.2mb",
          "overhead": 1.03,
          "tripped": 0
        },
        "request": {
          "limit_size_in_bytes": 19259326464,
          "limit_size": "17.9gb",
          "estimated_size_in_bytes": 16752,
          "estimated_size": "16.3kb",
          "overhead": 1,
          "tripped": 39
        },
        "parent": {
          "limit_size_in_bytes": 22469214208,
          "limit_size": "20.9gb",
          "estimated_size_in_bytes": 5542976,
          "estimated_size": "5.2mb",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "AaxeWfIiRAq1FScstt6cJA": {
      "timestamp": 1444718665772,
      "name": "stats-node-02",
      "transport_address": "inet[/10.0.1.32:9301]",
      "host": "es-dwh-02",
      "ip": [
        "inet[/10.0.1.32:9301]",
        "NONE"
      ],
      "attributes": {
        "master": "false"
      },
      "indices": {
        "docs": {
          "count": 493913716,
          "deleted": 5
        },
        "store": {
          "size": "262gb",
          "size_in_bytes": 281356821176,
          "throttle_time": "15.4m",
          "throttle_time_in_millis": 926397
        },
        "indexing": {
          "index_total": 354674737,
          "index_time": "3.9d",
          "index_time_in_millis": 342265083,
          "index_current": 250444,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "28.3s",
          "throttle_time_in_millis": 28324
        },
        "get": {
          "total": 379,
          "get_time": "1.3s",
          "time_in_millis": 1397,
          "exists_total": 378,
          "exists_time": "1.3s",
          "exists_time_in_millis": 1397,
          "missing_total": 1,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 2,
          "query_total": 1083986,
          "query_time": "2.2d",
          "query_time_in_millis": 196567728,
          "query_current": 0,
          "fetch_total": 1807,
          "fetch_time": "4.5m",
          "fetch_time_in_millis": 271802,
          "fetch_current": 0
        },
        "merges": {
          "current": 11,
          "current_docs": 4424648,
          "current_size": "2.5gb",
          "current_size_in_bytes": 2785184980,
          "total": 323680,
          "total_time": "4.2d",
          "total_time_in_millis": 370875244,
          "total_docs": 2810023214,
          "total_size": "1.7tb",
          "total_size_in_bytes": 1917495429534
        },
        "refresh": {
          "total": 2608280,
          "total_time": "1.5d",
          "total_time_in_millis": 134196812
        },
        "flush": {
          "total": 828449,
          "total_time": "4.4h",
          "total_time_in_millis": 16114846
        },
        "warmer": {
          "current": 0,
          "total": 40395,
          "total_time": "58.4s",
          "total_time_in_millis": 58469
        },
        "filter_cache": {
          "memory_size": "5.3mb",
          "memory_size_in_bytes": 5636852,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "3.9mb",
          "memory_size_in_bytes": 4126344,
          "evictions": 0,
          "fields": {
            "static.gender": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "project": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.sub_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.obj_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.name": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.action": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.login_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "timestamp": {
              "memory_size": "3.9mb",
              "memory_size_in_bytes": 4126344
            },
            "event_data.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.sub_camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.country": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            }
          }
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 5020,
          "memory": "826.7mb",
          "memory_in_bytes": 866938748,
          "index_writer_memory": "196.5mb",
          "index_writer_memory_in_bytes": 206065212,
          "index_writer_max_memory": "6.3gb",
          "index_writer_max_memory_in_bytes": 6805479589,
          "version_map_memory": "1.7mb",
          "version_map_memory_in_bytes": 1807512,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 137715,
          "size": "17b",
          "size_in_bytes": 17
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 2,
          "current_as_target": 0,
          "throttle_time": "1.7h",
          "throttle_time_in_millis": 6218680
        }
      },
      "os": {
        "timestamp": 1444718667594,
        "uptime": "50.1m",
        "uptime_in_millis": 3007271,
        "load_average": [
          1.76,
          2.15,
          2.39
        ],
        "cpu": {
          "sys": 1,
          "user": 7,
          "idle": 90,
          "usage": 8,
          "stolen": 0
        },
        "mem": {
          "free": "355.7mb",
          "free_in_bytes": 373022720,
          "used": "62.5gb",
          "used_in_bytes": 67190366208,
          "free_percent": 47,
          "used_percent": 52,
          "actual_free": "29.8gb",
          "actual_free_in_bytes": 32036618240,
          "actual_used": "33gb",
          "actual_used_in_bytes": 35526770688
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718667594,
        "open_file_descriptors": 27945,
        "cpu": {
          "percent": 138,
          "sys": "2.5d",
          "sys_in_millis": 219391710,
          "user": "15.6d",
          "user_in_millis": 1351131330,
          "total": "18.1d",
          "total_in_millis": 1570523040
        },
        "mem": {
          "resident": "31.6gb",
          "resident_in_bytes": 34004713472,
          "share": "471.1mb",
          "share_in_bytes": 494014464,
          "total_virtual": "121.6gb",
          "total_virtual_in_bytes": 130633691136
        }
      },
      "jvm": {
        "timestamp": 1444718667712,
        "uptime": "14.9d",
        "uptime_in_millis": 1295384120,
        "mem": {
          "heap_used": "15.6gb",
          "heap_used_in_bytes": 16838359848,
          "heap_used_percent": 52,
          "heap_committed": "29.8gb",
          "heap_committed_in_bytes": 32098877440,
          "heap_max": "29.8gb",
          "heap_max_in_bytes": 32098877440,
          "non_heap_used": "75.7mb",
          "non_heap_used_in_bytes": 79414784,
          "non_heap_committed": "107.3mb",
          "non_heap_committed_in_bytes": 112525312,
          "pools": {
            "young": {
              "used": "299.1mb",
              "used_in_bytes": 313731664,
              "max": "865.3mb",
              "max_in_bytes": 907345920,
              "peak_used": "865.3mb",
              "peak_used_in_bytes": 907345920,
              "peak_max": "865.3mb",
              "peak_max_in_bytes": 907345920
            },
            "survivor": {
              "used": "108.1mb",
              "used_in_bytes": 113377280,
              "max": "108.1mb",
              "max_in_bytes": 113377280,
              "peak_used": "108.1mb",
              "peak_used_in_bytes": 113377280,
              "peak_max": "108.1mb",
              "peak_max_in_bytes": 113377280
            },
            "old": {
              "used": "15.2gb",
              "used_in_bytes": 16411359608,
              "max": "28.9gb",
              "max_in_bytes": 31078154240,
              "peak_used": "28.9gb",
              "peak_used_in_bytes": 31078154240,
              "peak_max": "28.9gb",
              "peak_max_in_bytes": 31078154240
            }
          }
        },
        "threads": {
          "count": 332,
          "peak_count": 591
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 181828,
              "collection_time": "20.1h",
              "collection_time_in_millis": 72652196
            },
            "old": {
              "collection_count": 4875,
              "collection_time": "6.4h",
              "collection_time_in_millis": 23060979
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 12677,
            "used": "228.3mb",
            "used_in_bytes": 239482072,
            "total_capacity": "228.3mb",
            "total_capacity_in_bytes": 239482072
          },
          "mapped": {
            "count": 8781,
            "used": "82.4gb",
            "used_in_bytes": 88582589988,
            "total_capacity": "82.4gb",
            "total_capacity_in_bytes": 88582589988
          }
        }
      },
      "thread_pool": {
        "generic": {
          "threads": 7,
          "queue": 0,
          "active": 2,
          "rejected": 0,
          "largest": 159,
          "completed": 3907208
        },
        "index": {
          "threads": 16,
          "queue": 0,
          "active": 3,
          "rejected": 191,
          "largest": 16,
          "completed": 354986967
        },
        "fetch_shard_store": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 120540
        },
        "get": {
          "threads": 16,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 16,
          "completed": 379
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 312458
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 730471
        },
        "warmer": {
          "threads": 4,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 2274942
        },
        "flush": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 14905948
        },
        "search": {
          "threads": 168,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 429,
          "completed": 40758932
        },
        "fetch_shard_started": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 817
        },
        "listener": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 3094523
        },
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 5,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 5,
          "completed": 598439092
        },
        "refresh": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 1847055
        }
      },
      "network": {
        "tcp": {
          "active_opens": 1056771,
          "passive_opens": 5918728,
          "curr_estab": 404,
          "in_segs": 5428733715,
          "out_segs": 6581247984,
          "retrans_segs": 2253952,
          "estab_resets": 2761,
          "attempt_fails": 13890,
          "in_errs": 1540,
          "out_rsts": 9002
        }
      },
      "fs": {
        "timestamp": 1444718667713,
        "total": {
          "total": "1.9tb",
          "total_in_bytes": 2147415490560,
          "free": "915.7gb",
          "free_in_bytes": 983253540864,
          "available": "915.7gb",
          "available_in_bytes": 983253540864,
          "disk_reads": 24111123,
          "disk_writes": 163990490,
          "disk_io_op": 188101613,
          "disk_read_size": "1.3tb",
          "disk_read_size_in_bytes": 1451648474112,
          "disk_write_size": "7.6tb",
          "disk_write_size_in_bytes": 8377387776512,
          "disk_io_size": "8.9tb",
          "disk_io_size_in_bytes": 9829036250624,
          "disk_queue": "0",
          "disk_service_time": "1.4"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/var/lib/elasticsearch",
            "dev": "/dev/vdb1",
            "type": "xfs",
            "total": "1.9tb",
            "total_in_bytes": 2147415490560,
            "free": "915.7gb",
            "free_in_bytes": 983253540864,
            "available": "915.7gb",
            "available_in_bytes": 983253540864,
            "disk_reads": 24111123,
            "disk_writes": 163990490,
            "disk_io_op": 188101613,
            "disk_read_size": "1.3tb",
            "disk_read_size_in_bytes": 1451648474112,
            "disk_write_size": "7.6tb",
            "disk_write_size_in_bytes": 8377387776512,
            "disk_io_size": "8.9tb",
            "disk_io_size_in_bytes": 9829036250624,
            "disk_queue": "0",
            "disk_service_time": "1.4"
          }
        ]
      },
      "transport": {
        "server_open": 286,
        "rx_count": 1779062802,
        "rx_size": "1.8tb",
        "rx_size_in_bytes": 2007370182487,
        "tx_count": 1562179536,
        "tx_size": "1.3tb",
        "tx_size_in_bytes": 1485653208609
      },
      "breakers": {
        "request": {
          "limit_size_in_bytes": 19259326464,
          "limit_size": "17.9gb",
          "estimated_size_in_bytes": 147456,
          "estimated_size": "144kb",
          "overhead": 1,
          "tripped": 2070
        },
        "fielddata": {
          "limit_size_in_bytes": 12839550976,
          "limit_size": "11.9gb",
          "estimated_size_in_bytes": 4126344,
          "estimated_size": "3.9mb",
          "overhead": 1.03,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 22469214208,
          "limit_size": "20.9gb",
          "estimated_size_in_bytes": 4273800,
          "estimated_size": "4mb",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "SbNXGRHoS0-SJ4zvaGbrkQ": {
      "timestamp": 1444718665769,
      "name": "stats-search-node-02",
      "transport_address": "inet[/10.0.1.64:9300]",
      "host": "dwh-prod-es-search-02",
      "ip": [
        "inet[/10.0.1.64:9300]",
        "NONE"
      ],
      "attributes": {
        "data": "false",
        "master": "false"
      },
      "indices": {
        "docs": {
          "count": 0,
          "deleted": 0
        },
        "store": {
          "size": "0b",
          "size_in_bytes": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "indexing": {
          "index_total": 0,
          "index_time": "0s",
          "index_time_in_millis": 0,
          "index_current": 0,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "get": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "exists_total": 0,
          "exists_time": "0s",
          "exists_time_in_millis": 0,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 0,
          "query_total": 0,
          "query_time": "0s",
          "query_time_in_millis": 0,
          "query_current": 0,
          "fetch_total": 0,
          "fetch_time": "0s",
          "fetch_time_in_millis": 0,
          "fetch_current": 0
        },
        "merges": {
          "current": 0,
          "current_docs": 0,
          "current_size": "0b",
          "current_size_in_bytes": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0,
          "total_docs": 0,
          "total_size": "0b",
          "total_size_in_bytes": 0
        },
        "refresh": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "flush": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "warmer": {
          "current": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "filter_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 0,
          "memory": "0b",
          "memory_in_bytes": 0,
          "index_writer_memory": "0b",
          "index_writer_memory_in_bytes": 0,
          "index_writer_max_memory": "0b",
          "index_writer_max_memory_in_bytes": 0,
          "version_map_memory": "0b",
          "version_map_memory_in_bytes": 0,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 0,
          "size": "0b",
          "size_in_bytes": 0
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        }
      },
      "os": {
        "timestamp": 1444718665769,
        "uptime": "56.8m",
        "uptime_in_millis": 3413553,
        "load_average": [
          0.61,
          0.28,
          0.25
        ],
        "cpu": {
          "sys": 0,
          "user": 1,
          "idle": 98,
          "usage": 1,
          "stolen": 0
        },
        "mem": {
          "free": "2gb",
          "free_in_bytes": 2176294912,
          "used": "13.6gb",
          "used_in_bytes": 14652477440,
          "free_percent": 18,
          "used_percent": 81,
          "actual_free": "2.8gb",
          "actual_free_in_bytes": 3093741568,
          "actual_used": "12.7gb",
          "actual_used_in_bytes": 13735030784
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718665769,
        "open_file_descriptors": 356,
        "cpu": {
          "percent": 4,
          "sys": "12.8m",
          "sys_in_millis": 772050,
          "user": "4.9h",
          "user_in_millis": 17888330,
          "total": "5.1h",
          "total_in_millis": 18660380
        },
        "mem": {
          "resident": "12.5gb",
          "resident_in_bytes": 13449961472,
          "share": "22.1mb",
          "share_in_bytes": 23240704,
          "total_virtual": "15.5gb",
          "total_virtual_in_bytes": 16746700800
        }
      },
      "jvm": {
        "timestamp": 1444718665770,
        "uptime": "7.7d",
        "uptime_in_millis": 667304001,
        "mem": {
          "heap_used": "5gb",
          "heap_used_in_bytes": 5371802152,
          "heap_used_percent": 41,
          "heap_committed": "11.9gb",
          "heap_committed_in_bytes": 12850036736,
          "heap_max": "11.9gb",
          "heap_max_in_bytes": 12850036736,
          "non_heap_used": "52.8mb",
          "non_heap_used_in_bytes": 55467552,
          "non_heap_committed": "53.8mb",
          "non_heap_committed_in_bytes": 56496128,
          "pools": {
            "young": {
              "used": "89mb",
              "used_in_bytes": 93345568,
              "max": "266.2mb",
              "max_in_bytes": 279183360,
              "peak_used": "266.2mb",
              "peak_used_in_bytes": 279183360,
              "peak_max": "266.2mb",
              "peak_max_in_bytes": 279183360
            },
            "survivor": {
              "used": "33.2mb",
              "used_in_bytes": 34865152,
              "max": "33.2mb",
              "max_in_bytes": 34865152,
              "peak_used": "33.2mb",
              "peak_used_in_bytes": 34865152,
              "peak_max": "33.2mb",
              "peak_max_in_bytes": 34865152
            },
            "old": {
              "used": "4.8gb",
              "used_in_bytes": 5243591432,
              "max": "11.6gb",
              "max_in_bytes": 12535988224,
              "peak_used": "8.8gb",
              "peak_used_in_bytes": 9475832688,
              "peak_max": "11.6gb",
              "peak_max_in_bytes": 12535988224
            }
          }
        },
        "threads": {
          "count": 43,
          "peak_count": 46
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 17107,
              "collection_time": "25.8m",
              "collection_time_in_millis": 1553191
            },
            "old": {
              "collection_count": 39,
              "collection_time": "3.7s",
              "collection_time_in_millis": 3786
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 224,
            "used": "16.2mb",
            "used_in_bytes": 17001951,
            "total_capacity": "16.2mb",
            "total_capacity_in_bytes": 17001951
          },
          "mapped": {
            "count": 0,
            "used": "0b",
            "used_in_bytes": 0,
            "total_capacity": "0b",
            "total_capacity_in_bytes": 0
          }
        }
      },
      "thread_pool": {
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_started": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "listener": {
          "threads": 2,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 2,
          "completed": 906173
        },
        "index": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "refresh": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "generic": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 4,
          "completed": 733083
        },
        "warmer": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "search": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "flush": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_store": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 2,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 2,
          "completed": 245950
        },
        "get": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        }
      },
      "network": {
        "tcp": {
          "active_opens": 1291554,
          "passive_opens": 7276997,
          "curr_estab": 234,
          "in_segs": 687114365,
          "out_segs": 1111643405,
          "retrans_segs": 19487,
          "estab_resets": 515,
          "attempt_fails": 87,
          "in_errs": 542,
          "out_rsts": 2809
        }
      },
      "fs": {
        "timestamp": 1444718665770,
        "total": {},
        "data": []
      },
      "transport": {
        "server_open": 117,
        "rx_count": 6456622,
        "rx_size": "248.5gb",
        "rx_size_in_bytes": 266828993296,
        "tx_count": 5797792,
        "tx_size": "620.5mb",
        "tx_size_in_bytes": 650684581
      },
      "http": {
        "current_open": 0,
        "total_opened": 256189
      },
      "breakers": {
        "request": {
          "limit_size_in_bytes": 7710022041,
          "limit_size": "7.1gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        },
        "fielddata": {
          "limit_size_in_bytes": 5140014694,
          "limit_size": "4.7gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1.03,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 8995025715,
          "limit_size": "8.3gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "VdMDfCSRS-eWcll064OKxw": {
      "timestamp": 1444718665772,
      "name": "stats-node-01",
      "transport_address": "inet[/10.0.1.31:9301]",
      "host": "es-dwh-01",
      "ip": [
        "inet[/10.0.1.31:9301]",
        "NONE"
      ],
      "attributes": {
        "master": "false"
      },
      "indices": {
        "docs": {
          "count": 527145746,
          "deleted": 0
        },
        "store": {
          "size": "281gb",
          "size_in_bytes": 301760450733,
          "throttle_time": "2.2h",
          "throttle_time_in_millis": 8251449
        },
        "indexing": {
          "index_total": 330439672,
          "index_time": "3d",
          "index_time_in_millis": 267462135,
          "index_current": 218068,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "1.3m",
          "throttle_time_in_millis": 79451
        },
        "get": {
          "total": 342,
          "get_time": "478ms",
          "time_in_millis": 478,
          "exists_total": 340,
          "exists_time": "478ms",
          "exists_time_in_millis": 478,
          "missing_total": 2,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 2,
          "query_total": 899732,
          "query_time": "1.4d",
          "query_time_in_millis": 126858779,
          "query_current": 0,
          "fetch_total": 1362,
          "fetch_time": "4.5m",
          "fetch_time_in_millis": 272834,
          "fetch_current": 0
        },
        "merges": {
          "current": 4,
          "current_docs": 116895,
          "current_size": "103.9mb",
          "current_size_in_bytes": 108955225,
          "total": 318332,
          "total_time": "3.8d",
          "total_time_in_millis": 328789217,
          "total_docs": 2682297283,
          "total_size": "1.6tb",
          "total_size_in_bytes": 1830261430288
        },
        "refresh": {
          "total": 2497599,
          "total_time": "1.1d",
          "total_time_in_millis": 102341145
        },
        "flush": {
          "total": 809079,
          "total_time": "2.2h",
          "total_time_in_millis": 8224493
        },
        "warmer": {
          "current": 0,
          "total": 242396,
          "total_time": "6.8m",
          "total_time_in_millis": 410701
        },
        "filter_cache": {
          "memory_size": "5.8mb",
          "memory_size_in_bytes": 6155276,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "4.1mb",
          "memory_size_in_bytes": 4308536,
          "evictions": 0,
          "fields": {
            "static.gender": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "project": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.sub_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.obj_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.gems": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.name": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.status": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.action": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.login_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "@timestamp": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "timestamp": {
              "memory_size": "4.1mb",
              "memory_size_in_bytes": 4308536
            },
            "event_data.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.sub_camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.country": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            }
          }
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 4973,
          "memory": "883.9mb",
          "memory_in_bytes": 926873006,
          "index_writer_memory": "129.2mb",
          "index_writer_memory_in_bytes": 135542860,
          "index_writer_max_memory": "7.4gb",
          "index_writer_max_memory_in_bytes": 8045042056,
          "version_map_memory": "569.8kb",
          "version_map_memory_in_bytes": 583480,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 195636,
          "size": "17b",
          "size_in_bytes": 17
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 1,
          "current_as_target": 0,
          "throttle_time": "1.2h",
          "throttle_time_in_millis": 4378435
        }
      },
      "os": {
        "timestamp": 1444718666884,
        "uptime": "20m",
        "uptime_in_millis": 1202269,
        "load_average": [
          3.73,
          3.14,
          3.02
        ],
        "cpu": {
          "sys": 1,
          "user": 8,
          "idle": 88,
          "usage": 9,
          "stolen": 0
        },
        "mem": {
          "free": "382.2mb",
          "free_in_bytes": 400826368,
          "used": "62.5gb",
          "used_in_bytes": 67162562560,
          "free_percent": 47,
          "used_percent": 52,
          "actual_free": "29.6gb",
          "actual_free_in_bytes": 31829549056,
          "actual_used": "33.2gb",
          "actual_used_in_bytes": 35733839872
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718666884,
        "open_file_descriptors": 27875,
        "cpu": {
          "percent": 160,
          "sys": "2.3d",
          "sys_in_millis": 205875840,
          "user": "13.7d",
          "user_in_millis": 1190904850,
          "total": "16.1d",
          "total_in_millis": 1396780690
        },
        "mem": {
          "resident": "31.6gb",
          "resident_in_bytes": 34023280640,
          "share": "497.7mb",
          "share_in_bytes": 521957376,
          "total_virtual": "130gb",
          "total_virtual_in_bytes": 139658158080
        }
      },
      "jvm": {
        "timestamp": 1444718666366,
        "uptime": "13.9d",
        "uptime_in_millis": 1202237297,
        "mem": {
          "heap_used": "15.1gb",
          "heap_used_in_bytes": 16270873936,
          "heap_used_percent": 50,
          "heap_committed": "29.8gb",
          "heap_committed_in_bytes": 32098877440,
          "heap_max": "29.8gb",
          "heap_max_in_bytes": 32098877440,
          "non_heap_used": "71.1mb",
          "non_heap_used_in_bytes": 74621288,
          "non_heap_committed": "102.3mb",
          "non_heap_committed_in_bytes": 107360256,
          "pools": {
            "young": {
              "used": "843.2mb",
              "used_in_bytes": 884190424,
              "max": "865.3mb",
              "max_in_bytes": 907345920,
              "peak_used": "865.3mb",
              "peak_used_in_bytes": 907345920,
              "peak_max": "865.3mb",
              "peak_max_in_bytes": 907345920
            },
            "survivor": {
              "used": "108.1mb",
              "used_in_bytes": 113377280,
              "max": "108.1mb",
              "max_in_bytes": 113377280,
              "peak_used": "108.1mb",
              "peak_used_in_bytes": 113377280,
              "peak_max": "108.1mb",
              "peak_max_in_bytes": 113377280
            },
            "old": {
              "used": "14.2gb",
              "used_in_bytes": 15273306232,
              "max": "28.9gb",
              "max_in_bytes": 31078154240,
              "peak_used": "28.9gb",
              "peak_used_in_bytes": 31078154240,
              "peak_max": "28.9gb",
              "peak_max_in_bytes": 31078154240
            }
          }
        },
        "threads": {
          "count": 352,
          "peak_count": 744
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 165875,
              "collection_time": "19h",
              "collection_time_in_millis": 68594243
            },
            "old": {
              "collection_count": 3517,
              "collection_time": "3h",
              "collection_time_in_millis": 10828416
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 11352,
            "used": "207.9mb",
            "used_in_bytes": 218016676,
            "total_capacity": "207.9mb",
            "total_capacity_in_bytes": 218016676
          },
          "mapped": {
            "count": 8788,
            "used": "90.8gb",
            "used_in_bytes": 97596100042,
            "total_capacity": "90.8gb",
            "total_capacity_in_bytes": 97596100042
          }
        }
      },
      "thread_pool": {
        "generic": {
          "threads": 6,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 165,
          "completed": 2818441
        },
        "index": {
          "threads": 16,
          "queue": 0,
          "active": 0,
          "rejected": 139,
          "largest": 16,
          "completed": 330854025
        },
        "fetch_shard_store": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 117638
        },
        "get": {
          "threads": 16,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 16,
          "completed": 342
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 297312
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 731064
        },
        "warmer": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 2155707
        },
        "flush": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 14259808
        },
        "search": {
          "threads": 189,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 586,
          "completed": 36598119
        },
        "fetch_shard_started": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 674
        },
        "listener": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 5458382
        },
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 5,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 5,
          "completed": 513098616
        },
        "refresh": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 1757920
        }
      },
      "network": {
        "tcp": {
          "active_opens": 429353,
          "passive_opens": 2329813,
          "curr_estab": 426,
          "in_segs": 2251370010,
          "out_segs": 3072778734,
          "retrans_segs": 778448,
          "estab_resets": 552,
          "attempt_fails": 4553,
          "in_errs": 315,
          "out_rsts": 2105
        }
      },
      "fs": {
        "timestamp": 1444718666927,
        "total": {
          "total": "1.9tb",
          "total_in_bytes": 2147412344832,
          "free": "816.3gb",
          "free_in_bytes": 876573933568,
          "available": "816.3gb",
          "available_in_bytes": 876573933568,
          "disk_reads": 16076989,
          "disk_writes": 105478348,
          "disk_io_op": 121555337,
          "disk_read_size": "1001.2gb",
          "disk_read_size_in_bytes": 1075048089600,
          "disk_write_size": "3.7tb",
          "disk_write_size_in_bytes": 4089210678784,
          "disk_io_size": "4.6tb",
          "disk_io_size_in_bytes": 5164258768384,
          "disk_queue": "0",
          "disk_service_time": "0.5"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/var/lib/elasticsearch",
            "dev": "/dev/vdb1",
            "type": "xfs",
            "total": "1.9tb",
            "total_in_bytes": 2147412344832,
            "free": "816.3gb",
            "free_in_bytes": 876573933568,
            "available": "816.3gb",
            "available_in_bytes": 876573933568,
            "disk_reads": 16076989,
            "disk_writes": 105478348,
            "disk_io_op": 121555337,
            "disk_read_size": "1001.2gb",
            "disk_read_size_in_bytes": 1075048089600,
            "disk_write_size": "3.7tb",
            "disk_write_size_in_bytes": 4089210678784,
            "disk_io_size": "4.6tb",
            "disk_io_size_in_bytes": 5164258768384,
            "disk_queue": "0",
            "disk_service_time": "0.5"
          }
        ]
      },
      "transport": {
        "server_open": 307,
        "rx_count": 1645002044,
        "rx_size": "1.5tb",
        "rx_size_in_bytes": 1656378528544,
        "tx_count": 1455730559,
        "tx_size": "1.4tb",
        "tx_size_in_bytes": 1607581996540
      },
      "breakers": {
        "request": {
          "limit_size_in_bytes": 19259326464,
          "limit_size": "17.9gb",
          "estimated_size_in_bytes": 4220779760,
          "estimated_size": "3.9gb",
          "overhead": 1,
          "tripped": 1951
        },
        "fielddata": {
          "limit_size_in_bytes": 12839550976,
          "limit_size": "11.9gb",
          "estimated_size_in_bytes": 4308536,
          "estimated_size": "4.1mb",
          "overhead": 1.03,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 22469214208,
          "limit_size": "20.9gb",
          "estimated_size_in_bytes": 4225088296,
          "estimated_size": "3.9gb",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "KNwV2RkgTwCGK-Q--xZ6cA": {
      "timestamp": 1444718665771,
      "name": "stats-master-node-01",
      "transport_address": "inet[/10.0.1.61:9300]",
      "host": "localhost",
      "ip": [
        "inet[/10.0.1.61:9300]",
        "NONE"
      ],
      "attributes": {
        "data": "false",
        "master": "true"
      },
      "indices": {
        "docs": {
          "count": 0,
          "deleted": 0
        },
        "store": {
          "size": "0b",
          "size_in_bytes": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "indexing": {
          "index_total": 0,
          "index_time": "0s",
          "index_time_in_millis": 0,
          "index_current": 0,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "get": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "exists_total": 0,
          "exists_time": "0s",
          "exists_time_in_millis": 0,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 0,
          "query_total": 0,
          "query_time": "0s",
          "query_time_in_millis": 0,
          "query_current": 0,
          "fetch_total": 0,
          "fetch_time": "0s",
          "fetch_time_in_millis": 0,
          "fetch_current": 0
        },
        "merges": {
          "current": 0,
          "current_docs": 0,
          "current_size": "0b",
          "current_size_in_bytes": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0,
          "total_docs": 0,
          "total_size": "0b",
          "total_size_in_bytes": 0
        },
        "refresh": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "flush": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "warmer": {
          "current": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "filter_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 0,
          "memory": "0b",
          "memory_in_bytes": 0,
          "index_writer_memory": "0b",
          "index_writer_memory_in_bytes": 0,
          "index_writer_max_memory": "0b",
          "index_writer_max_memory_in_bytes": 0,
          "version_map_memory": "0b",
          "version_map_memory_in_bytes": 0,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 0,
          "size": "0b",
          "size_in_bytes": 0
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        }
      },
      "os": {
        "timestamp": 1444718665771,
        "uptime": "58.9m",
        "uptime_in_millis": 3534863,
        "load_average": [
          0.35,
          0.52,
          0.44
        ],
        "cpu": {
          "sys": 1,
          "user": 8,
          "idle": 90,
          "usage": 9,
          "stolen": 0
        },
        "mem": {
          "free": "98.1mb",
          "free_in_bytes": 102899712,
          "used": "1.8gb",
          "used_in_bytes": 1996255232,
          "free_percent": 19,
          "used_percent": 80,
          "actual_free": "394mb",
          "actual_free_in_bytes": 413233152,
          "actual_used": "1.5gb",
          "actual_used_in_bytes": 1685921792
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718665804,
        "open_file_descriptors": 321,
        "cpu": {
          "percent": 17,
          "sys": "5.4h",
          "sys_in_millis": 19549250,
          "user": "1.3d",
          "user_in_millis": 115725120,
          "total": "1.5d",
          "total_in_millis": 135274370
        },
        "mem": {
          "resident": "1.4gb",
          "resident_in_bytes": 1586454528,
          "share": "22.2mb",
          "share_in_bytes": 23298048,
          "total_virtual": "3.4gb",
          "total_virtual_in_bytes": 3662766080
        }
      },
      "jvm": {
        "timestamp": 1444718665805,
        "uptime": "17.6d",
        "uptime_in_millis": 1527307748,
        "mem": {
          "heap_used": "651.4mb",
          "heap_used_in_bytes": 683079560,
          "heap_used_percent": 64,
          "heap_committed": "1007.3mb",
          "heap_committed_in_bytes": 1056309248,
          "heap_max": "1007.3mb",
          "heap_max_in_bytes": 1056309248,
          "non_heap_used": "77.8mb",
          "non_heap_used_in_bytes": 81579632,
          "non_heap_committed": "79.7mb",
          "non_heap_committed_in_bytes": 83632128,
          "pools": {
            "young": {
              "used": "107.4mb",
              "used_in_bytes": 112701880,
              "max": "133.1mb",
              "max_in_bytes": 139591680,
              "peak_used": "133.1mb",
              "peak_used_in_bytes": 139591680,
              "peak_max": "133.1mb",
              "peak_max_in_bytes": 139591680
            },
            "survivor": {
              "used": "11.4mb",
              "used_in_bytes": 12049952,
              "max": "16.6mb",
              "max_in_bytes": 17432576,
              "peak_used": "16.6mb",
              "peak_used_in_bytes": 17432576,
              "peak_max": "16.6mb",
              "peak_max_in_bytes": 17432576
            },
            "old": {
              "used": "532.4mb",
              "used_in_bytes": 558327728,
              "max": "857.6mb",
              "max_in_bytes": 899284992,
              "peak_used": "764.6mb",
              "peak_used_in_bytes": 801747544,
              "peak_max": "857.6mb",
              "peak_max_in_bytes": 899284992
            }
          }
        },
        "threads": {
          "count": 35,
          "peak_count": 6167
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 100896,
              "collection_time": "47.7m",
              "collection_time_in_millis": 2863502
            },
            "old": {
              "collection_count": 979,
              "collection_time": "1.7m",
              "collection_time_in_millis": 107432
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 5369,
            "used": "90.1mb",
            "used_in_bytes": 94507053,
            "total_capacity": "90.1mb",
            "total_capacity_in_bytes": 94507053
          },
          "mapped": {
            "count": 0,
            "used": "0b",
            "used_in_bytes": 0,
            "total_capacity": "0b",
            "total_capacity_in_bytes": 0
          }
        }
      },
      "thread_pool": {
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_started": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "listener": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 4608201
        },
        "index": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "refresh": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "generic": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 6133,
          "completed": 739277
        },
        "warmer": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "search": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "flush": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_store": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 5,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 5,
          "completed": 666034
        },
        "get": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "snapshot": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 612
        }
      },
      "network": {
        "tcp": {
          "active_opens": 1352937,
          "passive_opens": 5688740,
          "curr_estab": 235,
          "in_segs": 1919185060,
          "out_segs": 7197911063,
          "retrans_segs": 88837,
          "estab_resets": 70,
          "attempt_fails": 423,
          "in_errs": 0,
          "out_rsts": 5696
        }
      },
      "fs": {
        "timestamp": 1444718665806,
        "total": {
          "total": "9.8gb",
          "total_in_bytes": 10534313984,
          "free": "8gb",
          "free_in_bytes": 8666959872,
          "available": "7.6gb",
          "available_in_bytes": 8205975552,
          "disk_reads": 340421,
          "disk_writes": 2153868,
          "disk_io_op": 2494289,
          "disk_read_size": "5.7gb",
          "disk_read_size_in_bytes": 6128890880,
          "disk_write_size": "23.8gb",
          "disk_write_size_in_bytes": 25570594816,
          "disk_io_size": "29.5gb",
          "disk_io_size_in_bytes": 31699485696,
          "disk_queue": "0",
          "disk_service_time": "2.9"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/",
            "dev": "/dev/vda1",
            "type": "ext4",
            "total": "9.8gb",
            "total_in_bytes": 10534313984,
            "free": "8gb",
            "free_in_bytes": 8666959872,
            "available": "7.6gb",
            "available_in_bytes": 8205975552,
            "disk_reads": 340421,
            "disk_writes": 2153868,
            "disk_io_op": 2494289,
            "disk_read_size": "5.7gb",
            "disk_read_size_in_bytes": 6128890880,
            "disk_write_size": "23.8gb",
            "disk_write_size_in_bytes": 25570594816,
            "disk_io_size": "29.5gb",
            "disk_io_size_in_bytes": 31699485696,
            "disk_queue": "0",
            "disk_service_time": "2.9"
          }
        ]
      },
      "transport": {
        "server_open": 117,
        "rx_count": 829119804,
        "rx_size": "164.5gb",
        "rx_size_in_bytes": 176731562467,
        "tx_count": 307103224,
        "tx_size": "4.6tb",
        "tx_size_in_bytes": 5058800788921
      },
      "http": {
        "current_open": 0,
        "total_opened": 577695
      },
      "breakers": {
        "fielddata": {
          "limit_size_in_bytes": 422523699,
          "limit_size": "402.9mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1.03,
          "tripped": 0
        },
        "request": {
          "limit_size_in_bytes": 633785548,
          "limit_size": "604.4mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 739416473,
          "limit_size": "705.1mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "S_Hv8WI4TB2-njFrL-y4BA": {
      "timestamp": 1444718665774,
      "name": "stats-master-node-02",
      "transport_address": "inet[/10.0.1.62:9300]",
      "host": "localhost",
      "ip": [
        "inet[/10.0.1.62:9300]",
        "NONE"
      ],
      "attributes": {
        "data": "false",
        "master": "true"
      },
      "indices": {
        "docs": {
          "count": 0,
          "deleted": 0
        },
        "store": {
          "size": "0b",
          "size_in_bytes": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "indexing": {
          "index_total": 0,
          "index_time": "0s",
          "index_time_in_millis": 0,
          "index_current": 0,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "get": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "exists_total": 0,
          "exists_time": "0s",
          "exists_time_in_millis": 0,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 0,
          "query_total": 0,
          "query_time": "0s",
          "query_time_in_millis": 0,
          "query_current": 0,
          "fetch_total": 0,
          "fetch_time": "0s",
          "fetch_time_in_millis": 0,
          "fetch_current": 0
        },
        "merges": {
          "current": 0,
          "current_docs": 0,
          "current_size": "0b",
          "current_size_in_bytes": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0,
          "total_docs": 0,
          "total_size": "0b",
          "total_size_in_bytes": 0
        },
        "refresh": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "flush": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "warmer": {
          "current": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "filter_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 0,
          "memory": "0b",
          "memory_in_bytes": 0,
          "index_writer_memory": "0b",
          "index_writer_memory_in_bytes": 0,
          "index_writer_max_memory": "0b",
          "index_writer_max_memory_in_bytes": 0,
          "version_map_memory": "0b",
          "version_map_memory_in_bytes": 0,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 0,
          "size": "0b",
          "size_in_bytes": 0
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        }
      },
      "os": {
        "timestamp": 1444718665775,
        "uptime": "25.4m",
        "uptime_in_millis": 1526001,
        "load_average": [
          0.42,
          0.36,
          0.27
        ],
        "cpu": {
          "sys": 0,
          "user": 3,
          "idle": 95,
          "usage": 3,
          "stolen": 0
        },
        "mem": {
          "free": "266.1mb",
          "free_in_bytes": 279126016,
          "used": "1.6gb",
          "used_in_bytes": 1820016640,
          "free_percent": 31,
          "used_percent": 68,
          "actual_free": "630.9mb",
          "actual_free_in_bytes": 661565440,
          "actual_used": "1.3gb",
          "actual_used_in_bytes": 1437577216
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718665775,
        "open_file_descriptors": 321,
        "cpu": {
          "percent": 7,
          "sys": "1h",
          "sys_in_millis": 3625460,
          "user": "17h",
          "user_in_millis": 61443090,
          "total": "18h",
          "total_in_millis": 65068550
        },
        "mem": {
          "resident": "1.2gb",
          "resident_in_bytes": 1307852800,
          "share": "20.7mb",
          "share_in_bytes": 21737472,
          "total_virtual": "3.3gb",
          "total_virtual_in_bytes": 3612688384
        }
      },
      "jvm": {
        "timestamp": 1444718665776,
        "uptime": "17.6d",
        "uptime_in_millis": 1525983643,
        "mem": {
          "heap_used": "195mb",
          "heap_used_in_bytes": 204543096,
          "heap_used_percent": 19,
          "heap_committed": "1007.3mb",
          "heap_committed_in_bytes": 1056309248,
          "heap_max": "1007.3mb",
          "heap_max_in_bytes": 1056309248,
          "non_heap_used": "52.7mb",
          "non_heap_used_in_bytes": 55266920,
          "non_heap_committed": "53.7mb",
          "non_heap_committed_in_bytes": 56356864,
          "pools": {
            "young": {
              "used": "1.2mb",
              "used_in_bytes": 1271272,
              "max": "133.1mb",
              "max_in_bytes": 139591680,
              "peak_used": "133.1mb",
              "peak_used_in_bytes": 139591680,
              "peak_max": "133.1mb",
              "peak_max_in_bytes": 139591680
            },
            "survivor": {
              "used": "16.6mb",
              "used_in_bytes": 17431936,
              "max": "16.6mb",
              "max_in_bytes": 17432576,
              "peak_used": "16.6mb",
              "peak_used_in_bytes": 17432576,
              "peak_max": "16.6mb",
              "peak_max_in_bytes": 17432576
            },
            "old": {
              "used": "177.2mb",
              "used_in_bytes": 185839888,
              "max": "857.6mb",
              "max_in_bytes": 899284992,
              "peak_used": "769mb",
              "peak_used_in_bytes": 806446160,
              "peak_max": "857.6mb",
              "peak_max_in_bytes": 899284992
            }
          }
        },
        "threads": {
          "count": 30,
          "peak_count": 34
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 82543,
              "collection_time": "2.2h",
              "collection_time_in_millis": 8270096
            },
            "old": {
              "collection_count": 4161,
              "collection_time": "5.6m",
              "collection_time_in_millis": 340474
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 33,
            "used": "6.7mb",
            "used_in_bytes": 7101868,
            "total_capacity": "6.7mb",
            "total_capacity_in_bytes": 7101868
          },
          "mapped": {
            "count": 0,
            "used": "0b",
            "used_in_bytes": 0,
            "total_capacity": "0b",
            "total_capacity_in_bytes": 0
          }
        }
      },
      "thread_pool": {
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_started": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "listener": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 28
        },
        "index": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "refresh": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "generic": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 1676185
        },
        "warmer": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "search": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "flush": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_store": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 2,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 2,
          "completed": 561757
        },
        "get": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        }
      },
      "network": {
        "tcp": {
          "active_opens": 603899,
          "passive_opens": 2600659,
          "curr_estab": 235,
          "in_segs": 47008813,
          "out_segs": 35656379,
          "retrans_segs": 20,
          "estab_resets": 1,
          "attempt_fails": 11,
          "in_errs": 0,
          "out_rsts": 699
        }
      },
      "fs": {
        "timestamp": 1444718665776,
        "total": {
          "total": "9.8gb",
          "total_in_bytes": 10534313984,
          "free": "8.1gb",
          "free_in_bytes": 8790458368,
          "available": "7.7gb",
          "available_in_bytes": 8329474048,
          "disk_reads": 30752,
          "disk_writes": 1021689,
          "disk_io_op": 1052441,
          "disk_read_size": "427.5mb",
          "disk_read_size_in_bytes": 448300032,
          "disk_write_size": "11.2gb",
          "disk_write_size_in_bytes": 12043771904,
          "disk_io_size": "11.6gb",
          "disk_io_size_in_bytes": 12492071936,
          "disk_queue": "0",
          "disk_service_time": "2.1"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/",
            "dev": "/dev/vda1",
            "type": "ext4",
            "total": "9.8gb",
            "total_in_bytes": 10534313984,
            "free": "8.1gb",
            "free_in_bytes": 8790458368,
            "available": "7.7gb",
            "available_in_bytes": 8329474048,
            "disk_reads": 30752,
            "disk_writes": 1021689,
            "disk_io_op": 1052441,
            "disk_read_size": "427.5mb",
            "disk_read_size_in_bytes": 448300032,
            "disk_write_size": "11.2gb",
            "disk_write_size_in_bytes": 12043771904,
            "disk_io_size": "11.6gb",
            "disk_io_size_in_bytes": 12492071936,
            "disk_queue": "0",
            "disk_service_time": "2.1"
          }
        ]
      },
      "transport": {
        "server_open": 117,
        "rx_count": 3141369,
        "rx_size": "559.7gb",
        "rx_size_in_bytes": 600978138020,
        "tx_count": 3141367,
        "tx_size": "210mb",
        "tx_size_in_bytes": 220220718
      },
      "http": {
        "current_open": 0,
        "total_opened": 577809
      },
      "breakers": {
        "fielddata": {
          "limit_size_in_bytes": 422523699,
          "limit_size": "402.9mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1.03,
          "tripped": 0
        },
        "request": {
          "limit_size_in_bytes": 633785548,
          "limit_size": "604.4mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 739416473,
          "limit_size": "705.1mb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "3xqOiGxaR1atjqjjKPP6wQ": {
      "timestamp": 1444718665770,
      "name": "stats-node-03",
      "transport_address": "inet[/10.0.1.33:9301]",
      "host": "es-dwh-03",
      "ip": [
        "inet[/10.0.1.33:9301]",
        "NONE"
      ],
      "attributes": {
        "master": "false"
      },
      "indices": {
        "docs": {
          "count": 493661559,
          "deleted": 0
        },
        "store": {
          "size": "266gb",
          "size_in_bytes": 285662194466,
          "throttle_time": "14m",
          "throttle_time_in_millis": 844339
        },
        "indexing": {
          "index_total": 147172349,
          "index_time": "2.4d",
          "index_time_in_millis": 210055815,
          "index_current": 187438,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "43.5s",
          "throttle_time_in_millis": 43500
        },
        "get": {
          "total": 65,
          "get_time": "133ms",
          "time_in_millis": 133,
          "exists_total": 65,
          "exists_time": "133ms",
          "exists_time_in_millis": 133,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 3,
          "query_total": 439425,
          "query_time": "21.1h",
          "query_time_in_millis": 76067892,
          "query_current": 0,
          "fetch_total": 533,
          "fetch_time": "2.3m",
          "fetch_time_in_millis": 140740,
          "fetch_current": 0
        },
        "merges": {
          "current": 6,
          "current_docs": 55720,
          "current_size": "38.4mb",
          "current_size_in_bytes": 40347469,
          "total": 165834,
          "total_time": "2.3d",
          "total_time_in_millis": 206459842,
          "total_docs": 1229145371,
          "total_size": "836gb",
          "total_size_in_bytes": 897677136688
        },
        "refresh": {
          "total": 1468193,
          "total_time": "1d",
          "total_time_in_millis": 92847810
        },
        "flush": {
          "total": 663608,
          "total_time": "3.8h",
          "total_time_in_millis": 13831500
        },
        "warmer": {
          "current": 0,
          "total": 34222,
          "total_time": "1m",
          "total_time_in_millis": 62735
        },
        "filter_cache": {
          "memory_size": "5.6mb",
          "memory_size_in_bytes": 5877780,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "6.8mb",
          "memory_size_in_bytes": 7165280,
          "evictions": 0,
          "fields": {
            "static.gender": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "project": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.sub_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.obj_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.data.name": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.platform": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.category": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.url_object.parameters.camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.subject.id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.ref": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.action": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.login_type": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "timestamp": {
              "memory_size": "6.8mb",
              "memory_size_in_bytes": 7165280
            },
            "event_data.device": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "event_data.sub_camp_id": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            },
            "static.country": {
              "memory_size": "0b",
              "memory_size_in_bytes": 0
            }
          }
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 5094,
          "memory": "857.5mb",
          "memory_in_bytes": 899188204,
          "index_writer_memory": "230mb",
          "index_writer_memory_in_bytes": 241264256,
          "index_writer_max_memory": "7.1gb",
          "index_writer_max_memory_in_bytes": 7647205244,
          "version_map_memory": "2.4mb",
          "version_map_memory_in_bytes": 2613248,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 288316,
          "size": "17b",
          "size_in_bytes": 17
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "56.6m",
          "throttle_time_in_millis": 3400219
        }
      },
      "os": {
        "timestamp": 1444718667320,
        "uptime": "49.8m",
        "uptime_in_millis": 2993255,
        "load_average": [
          2.73,
          2.8,
          2.82
        ],
        "cpu": {
          "sys": 1,
          "user": 7,
          "idle": 89,
          "usage": 8,
          "stolen": 0
        },
        "mem": {
          "free": "353.2mb",
          "free_in_bytes": 370458624,
          "used": "62.5gb",
          "used_in_bytes": 67192930304,
          "free_percent": 46,
          "used_percent": 53,
          "actual_free": "29.4gb",
          "actual_free_in_bytes": 31635369984,
          "actual_used": "33.4gb",
          "actual_used_in_bytes": 35928018944
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718667320,
        "open_file_descriptors": 28042,
        "cpu": {
          "percent": 150,
          "sys": "1.3d",
          "sys_in_millis": 116102410,
          "user": "7.7d",
          "user_in_millis": 665947250,
          "total": "9d",
          "total_in_millis": 782049660
        },
        "mem": {
          "resident": "31.6gb",
          "resident_in_bytes": 33941565440,
          "share": "468.3mb",
          "share_in_bytes": 491048960,
          "total_virtual": "125.4gb",
          "total_virtual_in_bytes": 134715592704
        }
      },
      "jvm": {
        "timestamp": 1444718667656,
        "uptime": "6.7d",
        "uptime_in_millis": 586766126,
        "mem": {
          "heap_used": "20.8gb",
          "heap_used_in_bytes": 22359144008,
          "heap_used_percent": 69,
          "heap_committed": "29.8gb",
          "heap_committed_in_bytes": 32098877440,
          "heap_max": "29.8gb",
          "heap_max_in_bytes": 32098877440,
          "non_heap_used": "68.3mb",
          "non_heap_used_in_bytes": 71673992,
          "non_heap_committed": "99mb",
          "non_heap_committed_in_bytes": 103833600,
          "pools": {
            "young": {
              "used": "71.5mb",
              "used_in_bytes": 75050208,
              "max": "865.3mb",
              "max_in_bytes": 907345920,
              "peak_used": "865.3mb",
              "peak_used_in_bytes": 907345920,
              "peak_max": "865.3mb",
              "peak_max_in_bytes": 907345920
            },
            "survivor": {
              "used": "80.1mb",
              "used_in_bytes": 84009552,
              "max": "108.1mb",
              "max_in_bytes": 113377280,
              "peak_used": "108.1mb",
              "peak_used_in_bytes": 113377280,
              "peak_max": "108.1mb",
              "peak_max_in_bytes": 113377280
            },
            "old": {
              "used": "20.6gb",
              "used_in_bytes": 22200084248,
              "max": "28.9gb",
              "max_in_bytes": 31078154240,
              "peak_used": "28.9gb",
              "peak_used_in_bytes": 31078154240,
              "peak_max": "28.9gb",
              "peak_max_in_bytes": 31078154240
            }
          }
        },
        "threads": {
          "count": 286,
          "peak_count": 846
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 87078,
              "collection_time": "10.8h",
              "collection_time_in_millis": 38929996
            },
            "old": {
              "collection_count": 2343,
              "collection_time": "4.1h",
              "collection_time_in_millis": 15041070
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 8804,
            "used": "167.7mb",
            "used_in_bytes": 175852585,
            "total_capacity": "167.7mb",
            "total_capacity_in_bytes": 175852585
          },
          "mapped": {
            "count": 8792,
            "used": "86.3gb",
            "used_in_bytes": 92697122442,
            "total_capacity": "86.3gb",
            "total_capacity_in_bytes": 92697122442
          }
        }
      },
      "thread_pool": {
        "generic": {
          "threads": 7,
          "queue": 0,
          "active": 3,
          "rejected": 0,
          "largest": 104,
          "completed": 2717566
        },
        "index": {
          "threads": 16,
          "queue": 0,
          "active": 1,
          "rejected": 166,
          "largest": 16,
          "completed": 147384437
        },
        "fetch_shard_store": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 76189
        },
        "get": {
          "threads": 16,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 16,
          "completed": 65
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 156962
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 1,
          "completed": 632513
        },
        "warmer": {
          "threads": 3,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 1052482
        },
        "flush": {
          "threads": 5,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 5,
          "completed": 7226593
        },
        "search": {
          "threads": 121,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 702,
          "completed": 17377202
        },
        "fetch_shard_started": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 32,
          "completed": 159
        },
        "listener": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 1414887
        },
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 5,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 5,
          "completed": 248525436
        },
        "refresh": {
          "threads": 8,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 8,
          "completed": 816399
        }
      },
      "network": {
        "tcp": {
          "active_opens": 1045972,
          "passive_opens": 5877021,
          "curr_estab": 445,
          "in_segs": 5516028533,
          "out_segs": 6599146394,
          "retrans_segs": 1934160,
          "estab_resets": 1813,
          "attempt_fails": 21310,
          "in_errs": 1338,
          "out_rsts": 3846
        }
      },
      "fs": {
        "timestamp": 1444718667658,
        "total": {
          "total": "1.9tb",
          "total_in_bytes": 2147415490560,
          "free": "832.7gb",
          "free_in_bytes": 894115389440,
          "available": "832.7gb",
          "available_in_bytes": 894115389440,
          "disk_reads": 23397351,
          "disk_writes": 183329611,
          "disk_io_op": 206726962,
          "disk_read_size": "1.2tb",
          "disk_read_size_in_bytes": 1402784507904,
          "disk_write_size": "7.3tb",
          "disk_write_size_in_bytes": 8121930790400,
          "disk_io_size": "8.6tb",
          "disk_io_size_in_bytes": 9524715298304,
          "disk_queue": "0",
          "disk_service_time": "1"
        },
        "data": [
          {
            "path": "/var/lib/elasticsearch/stats-es-cluster/nodes/0",
            "mount": "/var/lib/elasticsearch",
            "dev": "/dev/vdb1",
            "type": "xfs",
            "total": "1.9tb",
            "total_in_bytes": 2147415490560,
            "free": "832.7gb",
            "free_in_bytes": 894115389440,
            "available": "832.7gb",
            "available_in_bytes": 894115389440,
            "disk_reads": 23397351,
            "disk_writes": 183329611,
            "disk_io_op": 206726962,
            "disk_read_size": "1.2tb",
            "disk_read_size_in_bytes": 1402784507904,
            "disk_write_size": "7.3tb",
            "disk_write_size_in_bytes": 8121930790400,
            "disk_io_size": "8.6tb",
            "disk_io_size_in_bytes": 9524715298304,
            "disk_queue": "0",
            "disk_service_time": "1"
          }
        ]
      },
      "transport": {
        "server_open": 286,
        "rx_count": 753354549,
        "rx_size": "1tb",
        "rx_size_in_bytes": 1155191105193,
        "tx_count": 659942131,
        "tx_size": "475.9gb",
        "tx_size_in_bytes": 511039797964
      },
      "breakers": {
        "request": {
          "limit_size_in_bytes": 19259326464,
          "limit_size": "17.9gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 1161
        },
        "fielddata": {
          "limit_size_in_bytes": 12839550976,
          "limit_size": "11.9gb",
          "estimated_size_in_bytes": 7165280,
          "estimated_size": "6.8mb",
          "overhead": 1.03,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 22469214208,
          "limit_size": "20.9gb",
          "estimated_size_in_bytes": 7165280,
          "estimated_size": "6.8mb",
          "overhead": 1,
          "tripped": 0
        }
      }
    },
    "Hz216J7eTEqgoin4Ll3MEA": {
      "timestamp": 1444718665771,
      "name": "stats-search-node-01",
      "transport_address": "inet[/10.0.1.65:9300]",
      "host": "dwh-prod-es-search-01",
      "ip": [
        "inet[/10.0.1.65:9300]",
        "NONE"
      ],
      "attributes": {
        "data": "false",
        "master": "false"
      },
      "indices": {
        "docs": {
          "count": 0,
          "deleted": 0
        },
        "store": {
          "size": "0b",
          "size_in_bytes": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "indexing": {
          "index_total": 0,
          "index_time": "0s",
          "index_time_in_millis": 0,
          "index_current": 0,
          "delete_total": 0,
          "delete_time": "0s",
          "delete_time_in_millis": 0,
          "delete_current": 0,
          "noop_update_total": 0,
          "is_throttled": false,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        },
        "get": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "exists_total": 0,
          "exists_time": "0s",
          "exists_time_in_millis": 0,
          "missing_total": 0,
          "missing_time": "0s",
          "missing_time_in_millis": 0,
          "current": 0
        },
        "search": {
          "open_contexts": 0,
          "query_total": 0,
          "query_time": "0s",
          "query_time_in_millis": 0,
          "query_current": 0,
          "fetch_total": 0,
          "fetch_time": "0s",
          "fetch_time_in_millis": 0,
          "fetch_current": 0
        },
        "merges": {
          "current": 0,
          "current_docs": 0,
          "current_size": "0b",
          "current_size_in_bytes": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0,
          "total_docs": 0,
          "total_size": "0b",
          "total_size_in_bytes": 0
        },
        "refresh": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "flush": {
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "warmer": {
          "current": 0,
          "total": 0,
          "total_time": "0s",
          "total_time_in_millis": 0
        },
        "filter_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "id_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0
        },
        "fielddata": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0
        },
        "percolate": {
          "total": 0,
          "get_time": "0s",
          "time_in_millis": 0,
          "current": 0,
          "memory_size_in_bytes": -1,
          "memory_size": "-1b",
          "queries": 0
        },
        "completion": {
          "size": "0b",
          "size_in_bytes": 0
        },
        "segments": {
          "count": 0,
          "memory": "0b",
          "memory_in_bytes": 0,
          "index_writer_memory": "0b",
          "index_writer_memory_in_bytes": 0,
          "index_writer_max_memory": "0b",
          "index_writer_max_memory_in_bytes": 0,
          "version_map_memory": "0b",
          "version_map_memory_in_bytes": 0,
          "fixed_bit_set": "0b",
          "fixed_bit_set_memory_in_bytes": 0
        },
        "translog": {
          "operations": 0,
          "size": "0b",
          "size_in_bytes": 0
        },
        "suggest": {
          "total": 0,
          "time": "0s",
          "time_in_millis": 0,
          "current": 0
        },
        "query_cache": {
          "memory_size": "0b",
          "memory_size_in_bytes": 0,
          "evictions": 0,
          "hit_count": 0,
          "miss_count": 0
        },
        "recovery": {
          "current_as_source": 0,
          "current_as_target": 0,
          "throttle_time": "0s",
          "throttle_time_in_millis": 0
        }
      },
      "os": {
        "timestamp": 1444718665771,
        "uptime": "47m",
        "uptime_in_millis": 2820092,
        "load_average": [
          0.12,
          0.34,
          0.44
        ],
        "cpu": {
          "sys": 1,
          "user": 5,
          "idle": 92,
          "usage": 6,
          "stolen": 0
        },
        "mem": {
          "free": "2.2gb",
          "free_in_bytes": 2377859072,
          "used": "13.4gb",
          "used_in_bytes": 14450925568,
          "free_percent": 17,
          "used_percent": 82,
          "actual_free": "2.7gb",
          "actual_free_in_bytes": 2967080960,
          "actual_used": "12.9gb",
          "actual_used_in_bytes": 13861703680
        },
        "swap": {
          "used": "0b",
          "used_in_bytes": 0,
          "free": "0b",
          "free_in_bytes": 0
        }
      },
      "process": {
        "timestamp": 1444718665772,
        "open_file_descriptors": 370,
        "cpu": {
          "percent": 25,
          "sys": "6.9h",
          "sys_in_millis": 25043800,
          "user": "1.3d",
          "user_in_millis": 115340690,
          "total": "1.6d",
          "total_in_millis": 140384490
        },
        "mem": {
          "resident": "12.6gb",
          "resident_in_bytes": 13605203968,
          "share": "22.1mb",
          "share_in_bytes": 23269376,
          "total_virtual": "15.6gb",
          "total_virtual_in_bytes": 16788807680
        }
      },
      "jvm": {
        "timestamp": 1444718665772,
        "uptime": "7.7d",
        "uptime_in_millis": 667669696,
        "mem": {
          "heap_used": "3.2gb",
          "heap_used_in_bytes": 3483699528,
          "heap_used_percent": 27,
          "heap_committed": "11.9gb",
          "heap_committed_in_bytes": 12850036736,
          "heap_max": "11.9gb",
          "heap_max_in_bytes": 12850036736,
          "non_heap_used": "64.1mb",
          "non_heap_used_in_bytes": 67281216,
          "non_heap_committed": "65.8mb",
          "non_heap_committed_in_bytes": 69013504,
          "pools": {
            "young": {
              "used": "2.8mb",
              "used_in_bytes": 2980896,
              "max": "266.2mb",
              "max_in_bytes": 279183360,
              "peak_used": "266.2mb",
              "peak_used_in_bytes": 279183360,
              "peak_max": "266.2mb",
              "peak_max_in_bytes": 279183360
            },
            "survivor": {
              "used": "33.2mb",
              "used_in_bytes": 34865152,
              "max": "33.2mb",
              "max_in_bytes": 34865152,
              "peak_used": "33.2mb",
              "peak_used_in_bytes": 34865152,
              "peak_max": "33.2mb",
              "peak_max_in_bytes": 34865152
            },
            "old": {
              "used": "3.2gb",
              "used_in_bytes": 3445853480,
              "max": "11.6gb",
              "max_in_bytes": 12535988224,
              "peak_used": "11.6gb",
              "peak_used_in_bytes": 12483372976,
              "peak_max": "11.6gb",
              "peak_max_in_bytes": 12535988224
            }
          }
        },
        "threads": {
          "count": 45,
          "peak_count": 424
        },
        "gc": {
          "collectors": {
            "young": {
              "collection_count": 100214,
              "collection_time": "2.7h",
              "collection_time_in_millis": 9956512
            },
            "old": {
              "collection_count": 1915,
              "collection_time": "2.6m",
              "collection_time_in_millis": 156581
            }
          }
        },
        "buffer_pools": {
          "direct": {
            "count": 8269,
            "used": "147.6mb",
            "used_in_bytes": 154771137,
            "total_capacity": "147.6mb",
            "total_capacity_in_bytes": 154771137
          },
          "mapped": {
            "count": 0,
            "used": "0b",
            "used_in_bytes": 0,
            "total_capacity": "0b",
            "total_capacity_in_bytes": 0
          }
        }
      },
      "thread_pool": {
        "percolate": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_started": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "listener": {
          "threads": 2,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 2,
          "completed": 925542
        },
        "index": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "refresh": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "suggest": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "generic": {
          "threads": 1,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 379,
          "completed": 811199
        },
        "warmer": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "search": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 2,
          "completed": 48274
        },
        "flush": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "optimize": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "fetch_shard_store": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "management": {
          "threads": 4,
          "queue": 0,
          "active": 1,
          "rejected": 0,
          "largest": 4,
          "completed": 246036
        },
        "get": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "merge": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "bulk": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        },
        "snapshot": {
          "threads": 0,
          "queue": 0,
          "active": 0,
          "rejected": 0,
          "largest": 0,
          "completed": 0
        }
      },
      "network": {
        "tcp": {
          "active_opens": 1188232,
          "passive_opens": 9283750,
          "curr_estab": 248,
          "in_segs": 3628385592,
          "out_segs": 4672683618,
          "retrans_segs": 50912,
          "estab_resets": 540,
          "attempt_fails": 67,
          "in_errs": 418,
          "out_rsts": 2374
        }
      },
      "fs": {
        "timestamp": 1444718665773,
        "total": {},
        "data": []
      },
      "transport": {
        "server_open": 117,
        "rx_count": 868181600,
        "rx_size": "855.6gb",
        "rx_size_in_bytes": 918741447035,
        "tx_count": 735448909,
        "tx_size": "79gb",
        "tx_size_in_bytes": 84832239088
      },
      "http": {
        "current_open": 14,
        "total_opened": 1248190
      },
      "breakers": {
        "request": {
          "limit_size_in_bytes": 7710022041,
          "limit_size": "7.1gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        },
        "fielddata": {
          "limit_size_in_bytes": 5140014694,
          "limit_size": "4.7gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1.03,
          "tripped": 0
        },
        "parent": {
          "limit_size_in_bytes": 8995025715,
          "limit_size": "8.3gb",
          "estimated_size_in_bytes": 0,
          "estimated_size": "0b",
          "overhead": 1,
          "tripped": 0
        }
      }
    }
  }
}
```
</comment><comment author="dakrone" created="2015-10-15T22:09:59Z" id="148537551">Okay, I see the request breaker is very large for this node:

```
"breakers": {
        "request": {
          "limit_size_in_bytes": 19259326464,
          "limit_size": "17.9gb",
          "estimated_size_in_bytes": 4220779760,
          "estimated_size": "3.9gb",
          "overhead": 1,
          "tripped": 1951
        },
```

Has this node experienced any OutOfMemoryErrors? I'm surprised to see it tripped almost 2k times with a limit of ~18gb.
</comment><comment author="zlosim" created="2015-10-16T07:09:57Z" id="148634096">yes, actually there was one:

```
stats-es-cluster.log.2015-10-10.1.gz:1524-[2015-10-10 16:25:09,229][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
stats-es-cluster.log.2015-10-10.1.gz:1525:java.lang.OutOfMemoryError: Java heap space
stats-es-cluster.log.2015-10-10.1.gz:1526-  at org.elasticsearch.common.netty.buffer.HeapChannelBuffer.&lt;init&gt;(HeapChannelBuffer.java:42)
stats-es-cluster.log.2015-10-10.1.gz:1527-  at org.elasticsearch.common.netty.buffer.BigEndianHeapChannelBuffer.&lt;init&gt;(BigEndianHeapChannelBuffer.java:34)
stats-es-cluster.log.2015-10-10.1.gz:1528-  at org.elasticsearch.common.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)
stats-es-cluster.log.2015-10-10.1.gz:1529-  at org.elasticsearch.common.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:68)
stats-es-cluster.log.2015-10-10.1.gz:1530-  at org.elasticsearch.common.netty.buffer.AbstractChannelBufferFactory.getBuffer(AbstractChannelBufferFactory.java:48)
stats-es-cluster.log.2015-10-10.1.gz:1531-  at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:80)
stats-es-cluster.log.2015-10-10.1.gz:1532-  at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
stats-es-cluster.log.2015-10-10.1.gz:1533-  at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
stats-es-cluster.log.2015-10-10.1.gz:1534-  at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
stats-es-cluster.log.2015-10-10.1.gz:1535-  at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
stats-es-cluster.log.2015-10-10.1.gz:1536-  at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
stats-es-cluster.log.2015-10-10.1.gz:1537-  at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
stats-es-cluster.log.2015-10-10.1.gz:1538-  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
stats-es-cluster.log.2015-10-10.1.gz:1539-  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
stats-es-cluster.log.2015-10-10.1.gz:1540-  at java.lang.Thread.run(Thread.java:745)
```

as for the memory usage, I`m looking into those queries right now
</comment><comment author="dakrone" created="2015-10-16T14:38:44Z" id="148734756">Okay, once an `OutOfMemoryError` has occurred, all bets are off for memory management, even if the node "appears" to have recovered, it can exhibit strange unexplainable behavior later on down the road. I recommend that you restart the node as soon as you can.
</comment><comment author="zlosim" created="2015-10-16T14:41:13Z" id="148735341">thanks, was keeping it alive to debug this issue so its time to restart it :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow tests to run from eclipse on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14064</link><project id="" key="" /><description>Eclipse appends "invalid"-looking classpath entries when running tests, see the last two entries in this example:

```
&lt;other stuff&gt;;C:\Users\dweiss\.m2\repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;C:\Users\dweiss\.m2\repository\commons-codec\commons-codec\1.6\commons-codec-1.6.jar;/C:/Tools/eclipse/Luna/configuration/org.eclipse.osgi/633/0/.cp/;/C:/Tools/eclipse/Luna/configuration/org.eclipse.osgi/632/0/.cp/
```

It is easy to discard this as "invalid stuff eclipse is doing" but java accepts classpath entries like this (it is not clear this is intentional), therefore things like eclipse do it.

Currently anyone trying to run tests from eclipse on windows will get an exception from jarhell with root cause like `java.nio.file.InvalidPathException: Illegal char &lt;:&gt; at index 2: /C:/Tools/eclipse/Luna/configuration/org.eclipse.osgi/633/0/.cp/`

Unfortunately 2.0 is affected, @dweiss discovered this while doing some plugin development with the RC. 
</description><key id="110963157">14064</key><summary>Allow tests to run from eclipse on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v2.0.0</label></labels><created>2015-10-12T12:21:45Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-13T10:58:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dweiss" created="2015-10-12T13:02:22Z" id="147392107">Checked on Windows and it works fine now, thanks Robert.
</comment><comment author="rjernst" created="2015-10-12T18:10:44Z" id="147479020">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyzer unassigned when using 'integer' type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14063</link><project id="" key="" /><description>When assigning an `analyzer` to an `integer` field, the analyzer definition is removed from the mapping.

In the example below I would like to remove non-numeric characters from a house number and store the results as `integer` types in order to use a `range` filter (eg. so `["1A"]` =&gt; `[1]` or `["apt 4"]` =&gt; `[4]`)

When posting the mapping to ES, it is removing the `analyzer` for the `integer` field but not for the `string` field.

This results in an unexpected error such as `MapperParsingException[failed to parse [myInteger]]; nested: NumberFormatException[For input string: \"apartment 1A\"];`.

I looked through the docs and couldn't find mention of this behaviour.

``` bash
#!/bin/bash

################################################
# Analyzer unassigned when using 'integer' type
################################################

ES='localhost:9200';

# drop index
curl -XDELETE "$ES/address?pretty=true";

# create index
curl -XPUT "$ES/address?pretty=true" -d'
{
  "settings": {
    "analysis": {
      "analyzer": {
        "numberify": {
          "type": "custom",
          "tokenizer": "standard",
          "char_filter": ["convert_non_numeric_chars_to_spaces"]
        }
      },
      "char_filter": {
        "convert_non_numeric_chars_to_spaces": {
          "type": "pattern_replace",
          "pattern": "[^0-9]",
          "replacement": " "
        }
      }
    }
  },
  "mappings": {
    "housenumber": {
      "properties": {
        "myString": {
          "type": "string",
          "analyzer": "numberify"
        },
        "myInteger": {
          "type": "integer",
          "analyzer": "numberify"
        }
      }
    }
  }
}';
```

``` bash
# retrieve index mapping
curl -XGET "$ES/address/_mapping?pretty=true"

# !!! analyzer has been removed from 'integer' field but not string field !!!

# "myInteger" : {
#   "type" : "integer"
# },
# "myString" : {
#   "type" : "string",
#   "analyzer" : "numberify"
# }
```

``` bash
# index a doc
curl -XPOST "$ES/address/housenumber/1?pretty=true" -d'
{
  "myInteger": "apartment 1A",
  "myString": "apartment 1A"
}';

# {
#   "error" : "MapperParsingException[failed to parse [myInteger]]; nested: NumberFormatException[For input string: \"apartment 1A\"]; ",
#   "status" : 400
# }
```

``` bash
# expected for input 'apartment 1A'

"myInteger": 1,
"myString": "1"
```

``` bash
{
  "status" : 200,
  "name" : "Mahkizmo",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.7.2",
    "build_hash" : "e43676b1385b8125d647f593f7202acbd816e8ec",
    "build_timestamp" : "2015-09-14T09:49:53Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="110956296">14063</key><summary>Analyzer unassigned when using 'integer' type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">missinglink</reporter><labels /><created>2015-10-12T11:31:31Z</created><updated>2015-10-12T13:06:09Z</updated><resolved>2015-10-12T12:12:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-12T12:12:39Z" id="147378563">Please join us on discuss.elastic.co 

Not an issue here IMO. 
</comment><comment author="missinglink" created="2015-10-12T12:17:55Z" id="147379267">hey @dadoonet where exactly is the discussion going on? I couldn't find this issue over there?
Could you elaborate on why you closed the issue? I took the time to write it.
</comment><comment author="dadoonet" created="2015-10-12T12:29:30Z" id="147382523">Just open the same discussion on discuss. We can help you there.
</comment><comment author="missinglink" created="2015-10-12T13:04:05Z" id="147393104">```
Hello,

Thanks for your recent post contribution!

Akismet has temporarily hidden your post as potential spam.

Please be patient while a staff member manually approves your post; submitting your post again will not expedite this process.

We apologize for the inconvenience.
```

IMO `303 See Other` -&gt; `404 Not Found` -&gt; `201 Created` -&gt; `410 Gone` is not good community management ;(
</comment><comment author="dadoonet" created="2015-10-12T13:05:44Z" id="147393476">Argh... Just fixed it. Will be available soonish! 

https://discuss.elastic.co/t/analyzer-unassigned-when-using-integer-type/32007
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduce Primary Terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14062</link><project id="" key="" /><description>Every shard group in Elasticsearch has a selected copy called a primary. When a primary shard fails a new primary would be selected from the existing replica copies. This PR introduces `primary terms` to track the number of times this has happened. This will allow us, as follow up work and among other things, to identify operations that come from old stale primaries. It is also the first step in road towards sequence numbers.

Relates to #10708
</description><key id="110925611">14062</key><summary>Introduce Primary Terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Sequence IDs</label><label>enhancement</label></labels><created>2015-10-12T07:58:28Z</created><updated>2015-11-18T13:31:08Z</updated><resolved>2015-10-21T16:27:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-12T07:58:49Z" id="147321538">@brwe @jasontedor  care to take a look?
</comment><comment author="brwe" created="2015-10-12T15:06:07Z" id="147426783">Should the primary term also increase when we move a primary from one node to another?
</comment><comment author="brwe" created="2015-10-12T15:33:02Z" id="147439725">When I restart a node then primaryTerm of primaries is incremented by 2. Is this intended?
</comment><comment author="bleskes" created="2015-10-13T13:07:11Z" id="147708954">&gt; Should the primary term also increase when we move a primary from one node to another?

We can maybe later. My feeling is now that this not needed and as this is the "same" primary - it just moved.

&gt; When I restart a node then primaryTerm of primaries is incremented by 2. Is this intended?

Good catch!! fixed and added some testing.
</comment><comment author="brwe" created="2015-10-14T15:59:54Z" id="148098559">I left some nitpicking but in general I wonder this: shard version and primary term should always be same for all copies. We add primary term to index meta data but not the shard version. Also, we write the shard version when we persist the shard meta data but not the primary term. Why do we treat them differently?
I also wonder why we cannot just get the version of the shard and the primary term from the index metadata instead of adding this information to the shard routings? It might be less confusing to have a single source of truth for this information and we write the index meta data now in any case. 
</comment><comment author="bleskes" created="2015-10-15T19:11:17Z" id="148491559">&gt;  shard version and primary term should always be same for all copies.

Note exactly - versions are incremented with every shard routing change, of any shard (primary or not). the terms are only incremented on primary assignment en promotion.

&gt; Why do we treat them differently?

The shard versions are currently used for selected a primary after full cluster restart. Terms will be used for indexing. Long term, once we serialize allocation ids, we can consider removing the versions.

I rebased against master and pushed another commit. @jasontedor,  @brwe can you take another look?
</comment><comment author="brwe" created="2015-10-16T13:47:34Z" id="148720992">&gt; Note exactly - versions are incremented with every shard routing change, of any shard (primary or not). the terms are only incremented on primary assignment en promotion.

What I meant was that they are both always the same for each copy (although primary term and version can of course differ). Shard version is only in the ShardRoutings but shard term is in both and that seems redundant to me.
I was actually hoping we could move the shard term to IndexMetaData only and not store this information in the ShardRouting too because I found it cumbersome to figure out where shard versions are incremented before and now we do the same thing for primary terms. But after some digging I think this is not really easy to do.

However, this is not a problem with this pull request but more with how versioning of MetaData, IndexMetaData, shards etc. works now. I have no good idea how to make this easier to read but opened an issue here to discuss: https://github.com/elastic/elasticsearch/issues/14158
We can leave it now in this pull request as is.
</comment><comment author="s1monw" created="2015-10-16T20:28:34Z" id="148825829">I look at the PR and I wonder if we should introduce a dedicated class for this for several reasons:
- documentation, I think this needs a lot of documentation once used what it is and what it's semantics are.
- we can implement `ToXContent`, `Comparable` and `Writeable`
- we can ensure it's always positive and never decreasing
- we should also use a long just to be on the safe end :) (over paranoid simon)

WDYT?
</comment><comment author="bleskes" created="2015-10-19T18:58:40Z" id="149312768">pushed another commit with a fix for the double version increment issue @brwe found and some(what) beefed java docs.

@s1monw I gave it some more thought and I still think - at least as things stand now - that a wrapper class for the PrimaryTerm will add complexity instead of making things clearer. It will just be a wrapper around an int and would obscure simple operation behind a method. Since it's a gut feeling thing I've asked the group today and @jasontedor tends to agree. We do totally see the importance of documentation. I've beefed up what I could in the current PR and added an explicit docs todo on the seq no meta data issue. I suggest we proceed as is. This is the very first step in a longer journey - as soon as there is more complex logic around the primary term that needs a home we'll wrap it up in a class.

I also moved primary terms to be long. I made them int originally to address concerns people voiced about 16 bytes (term + counter) per doc but I agree we can review it later on and maybe just encode it differently.
</comment><comment author="jasontedor" created="2015-10-20T17:11:25Z" id="149636076">I left a few more comments:
- [should it be a copy?](https://github.com/elastic/elasticsearch/pull/14062#discussion_r42507827)
- [another `long` hash code cleanup](https://github.com/elastic/elasticsearch/pull/14062#discussion_r42509768)
- [unneeded use of `LongArrayList`](https://github.com/elastic/elasticsearch/pull/14062#discussion_r42523524)

I have reservations about the [conversion](https://github.com/bleskes/elasticsearch/commit/21d26543d95c55243bfb33c48e4306fe2d10169d) from `int` to `long` but I think we can continue to think about that as this work progresses. Otherwise, LGTM.
</comment><comment author="bleskes" created="2015-10-21T16:27:47Z" id="149952193">this is pushed to the feature/seq_no branch. Thanks @jasontedor @brwe and @s1monw for the reviews.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GC problems in ES 1.4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14061</link><project id="" key="" /><description>Frequent full GC even if the memory utilization is 30-40%.
*\* We have not changed the default JVM settings

Below are the cluster details -
1. A master node
2. Two data nodes

*\* All the replicas are allocated on one node and all the primaries are on the other node. It should not be a problem, right? Also frequent full GC is observed on one of the 2 data nodes.

Shards - 
We have around 75 shards with replication as 1. I guess the problem is because of shard over allocation.
Most of the queries we hit are aggregation queries.

Thanks,
Rushiraj
</description><key id="110917777">14061</key><summary>GC problems in ES 1.4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rushirajchavan</reporter><labels /><created>2015-10-12T06:59:56Z</created><updated>2015-10-13T08:29:10Z</updated><resolved>2015-10-13T08:29:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T08:29:10Z" id="147646618">Hi @rushirajchavan 

I think the proper place to discuss this is in the forums http://discuss.elastic.co/

Also, check that you have properly disabled swap: https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure more specific analyzer is used independent of the mapping order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14060</link><project id="" key="" /><description>Today in the 1.x series `search_analyzer` or `index_analyzer` will be
overwritten by `analyzer` dependent on the hash map interation order. if
the `analyzer` key comes last all previously set analzyers are lost.
This commit removes the ambigutiy such that `search_analyzer` is always used
if specified.

Closes #14023 

Note: This PR is against branch `1.7` and this issue is already fixed in `2.x` and `master`
</description><key id="110876373">14060</key><summary>Ensure more specific analyzer is used independent of the mapping order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Mapping</label><label>bug</label><label>review</label><label>v1.7.3</label></labels><created>2015-10-11T18:48:54Z</created><updated>2015-10-12T19:23:33Z</updated><resolved>2015-10-12T19:23:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-11T21:01:28Z" id="147245613">left one minor comment.. looks good o.w.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Catch exception when reading corrupted snapshot.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14059</link><project id="" key="" /><description>Single corrupted snapshot file shouldn't prevent listing all other
snapshot in repository.

closes #13887
</description><key id="110838824">14059</key><summary>Catch exception when reading corrupted snapshot.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>review</label></labels><created>2015-10-11T06:48:36Z</created><updated>2015-11-08T19:08:06Z</updated><resolved>2015-11-03T09:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-11-02T13:15:04Z" id="153012984">Great to see this being worked on. The implementation looks good to me. I am unsure however whether the corrupted snapshot should be reported as FAILED.

There are different reasons why a failure occurs here. It can be a permanent failure (data corruption), or it can simply be a temporary issue (cannot be fetched at the moment). Either way, it is possible that the snapshot was once reported as SUCCESS. I see two other possibilities to implement this:
1) Introduce a new state with the semantics of "could not be fetched".
2) Add a flag "ignore_unavailable" to GetSnapshotsRequest similar as for IndicesOptions of GetIndexRequest.

What do you think ?
</comment><comment author="xuzha" created="2015-11-02T19:11:12Z" id="153127727">Thanks @ywelsch for the review.

I agree your ideas to fix the issue. I prefer option 1. I think these errors should be really rarely, option 1 seems simpler. And user could deal with that case.

[Update], right now I think this new state does not make sense. Since if there was a problem happens when reading the snapshot, the problem could be network or hardware issue. This is not really related to the snapshot itself. We should report the error and let user fix the problem.
</comment><comment author="xuzha" created="2015-11-03T09:17:19Z" id="153294460">The branch created into ES repo by mistake. 

Delete this and create a new PR #14471. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename discovery.zen.minimum_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14058</link><project id="" key="" /><description>This setting works irrespective of the discovery method, so it should be named differently.

As it is it could lead to some confusion.
</description><key id="110838802">14058</key><summary>Rename discovery.zen.minimum_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v7.0.0</label></labels><created>2015-10-11T06:47:45Z</created><updated>2017-05-08T10:02:19Z</updated><resolved>2017-05-05T15:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T08:27:17Z" id="147646282">Agreed.  I'd say that this is not urgent, and should be done when we have a proper mechanism to alert the user to renamed or deprecated settings, ie #6732
</comment><comment author="javanna" created="2017-05-05T14:30:40Z" id="299480424">seems like we want to do this, do we want to deprecate the setting and introduce its replacement in 6.0?</comment><comment author="jasontedor" created="2017-05-05T14:50:29Z" id="299485873">I would strongly prefer that we not do this for 6.0.0 (if at all, but that's a separate matter). It's a cosmetic change that has BWC concerns that we do not need to be taking on right now.</comment><comment author="jasontedor" created="2017-05-05T14:51:08Z" id="299486047">Also, related, we have deeper issues with `discovery.zen.minimum_master_nodes` to explore anyway.</comment><comment author="ywelsch" created="2017-05-05T15:02:00Z" id="299488921">The confusion stems from bad naming. The notion of "discovery" is used in two ways:
1) determining the list of nodes that is used for nodes to initially find each other.
2) the implementation that does cluster forming (master election and cluster state publishing).

For 1) we have plugins for GCE, EC2, Azure etc.
For 2) we have a single implementation, called Zen Discovery.

Zen Discovery does both 1) and 2) and allows a pluggable `hosts_provider` implementation that provides the nodes that are used for pinging, i.e., the process of finding other nodes. What we call GCE, EC2, or Azure discovery is nothing more than Zen Discovery with a customized `host_provider`.

One could argue that `minimum_master_nodes` falls into the realm of master election and cluster state publishing and should hence carry the name `zen` as it is specific to that implementation.</comment><comment author="jasontedor" created="2017-05-05T15:07:28Z" id="299490497">Thanks @ywelsch, your elaboration is exactly what I was thinking when I said "if at all, but that's a separate matter" and was going to elaborate when I had more time. &#10084;&#65039;</comment><comment author="jasontedor" created="2017-05-05T15:08:58Z" id="299490908">I feel that we can close this now as won't fix.</comment><comment author="bleskes" created="2017-05-08T09:56:18Z" id="299824533">+1 to close. We don't have enough info now to make the right call. I tend to agree that min master nodes is a zen discovery property and that the plugins are nothing but a host providers. In fact we currently expect people to use the `discovery.zen.hosts_provider` setting for that. I will open a separate issue to remove the BWC logic that still accepts `discovery.type` as a way to configure EC2/GCE/Azure specific unicast host list.</comment><comment author="bleskes" created="2017-05-08T10:02:19Z" id="299825811">issue opened: https://github.com/elastic/elasticsearch/issues/24543</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster concurrent rebalance ignored on node allocation exclusion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14057</link><project id="" key="" /><description>The setting cluster.routing.allocation.cluster_concurrent_rebalance appears to be ignored when moving shard off a node that has been excluded from allocation with the setting cluster.routing.allocation.exclude._ip .

ES Version: 1.7.2

Repeatedly experienced with the following steps:
set cluster.routing.allocation.cluster_concurrent_rebalance to 1
exclude node from allocation with cluster.routing.allocation.exclude._ip

This results in 80 shards rebalancing at a time until all shards are removed from the excluded node.
</description><key id="110813476">14057</key><summary>Cluster concurrent rebalance ignored on node allocation exclusion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">lb425</reporter><labels><label>:Allocation</label><label>bug</label></labels><created>2015-10-10T19:52:41Z</created><updated>2015-10-26T08:27:05Z</updated><resolved>2015-10-26T08:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T09:19:20Z" id="148663587">Investigation required
</comment><comment author="ywelsch" created="2015-10-22T16:05:01Z" id="150272508">I can confirm the issue. The root cause is that rebalancing constraints are not taken into consideration when shards that can no longer remain on a node need to be moved. AFAICS, it affects the following options:
- cluster.routing.allocation.cluster_concurrent_rebalance
- cluster.routing.allocation.allow_rebalance
- cluster.routing.rebalance.enable
- index.routing.rebalance.enable
- rebalance_only_when_active
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bonsai-elasticsearch-rails - cannot require 'elasticsearch/rails/tasks/import'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14056</link><project id="" key="" /><description>When I run: 

```
$ heroku run bundle exec rake environment elasticsearch:import:model CLASS='Artist' FORCE=true
```

my terminal response is 

```
Running bundle exec rake environment elasticsearch:import:model CLASS=Artist FORCE=true on doremi... up, run.3015
Starting up a new ElasticSearch client with https://hexcode.bonsai.io
rake aborted!
NameError: uninitialized constant Elasticsearch
/app/vendor/bundle/ruby/2.0.0/gems/bonsai-elasticsearch-rails-0.0.4/lib/bonsai/elasticsearch/rails.rb:4:in `&lt;top (required)&gt;'
/app/vendor/bundle/ruby/2.0.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:85:in `require'
/app/vendor/bundle/ruby/2.0.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:85:in `rescue in block in require'
/app/vendor/bundle/ruby/2.0.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:68:in `block in require'
/app/vendor/bundle/ruby/2.0.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:61:in `each'
/app/vendor/bundle/ruby/2.0.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:61:in `require'
/app/vendor/bundle/ruby/2.0.0/gems/bundler-1.9.7/lib/bundler.rb:134:in `require'
/app/config/application.rb:16:in `&lt;top (required)&gt;'
/app/Rakefile:4:in `require'
/app/Rakefile:4:in `&lt;top (required)&gt;'
(See full trace by running task with --trace)
```

I have checked that all my elastic-\* gems are their most up to date versions: 0.1.7 for elasticsearch-rails and elasticsearch-model, 0.0.4 for bonsai-elasticsearch-rails, however I did notice here https://rubygems.org/gems/bonsai-elasticsearch-rails that bonsai-elastic-search rails depends on bundler 1.5 and Heroku is using 1.9.7 which is not possible to change as far as I can see. Could this be the crux of the matter?

I also have a similar error in my logs when I try to deploy

```
Oct 09 17:04:27 doremi app/worker.1:  NameError: uninitialized constant Elasticsearch 
Oct 09 17:04:27 doremi app/worker.1:  rake aborted! 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bonsai-elasticsearch-rails-0.0.4/lib/bonsai/elasticsearch/rails.rb:4:in `&lt;top (required)&gt;' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:85:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:85:in `rescue in block in require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:68:in `block in require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:61:in `each' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:61:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler.rb:134:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/config/application.rb:16:in `&lt;top (required)&gt;' 
Oct 09 17:04:27 doremi app/worker.1:  /app/Rakefile:4:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/Rakefile:4:in `&lt;top (required)&gt;' 
Oct 09 17:04:27 doremi app/worker.1:  LoadError: cannot load such file -- bonsai-elasticsearch-rails 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:76:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:76:in `block (2 levels) in require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:72:in `each' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:72:in `block in require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:61:in `each' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler/runtime.rb:61:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/vendor/bundle/ruby/2.2.0/gems/bundler-1.9.7/lib/bundler.rb:134:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/config/application.rb:16:in `&lt;top (required)&gt;' 
Oct 09 17:04:27 doremi app/worker.1:  /app/Rakefile:4:in `require' 
Oct 09 17:04:27 doremi app/worker.1:  /app/Rakefile:4:in `&lt;top (required)&gt;' 
```
</description><key id="110756484">14056</key><summary>bonsai-elasticsearch-rails - cannot require 'elasticsearch/rails/tasks/import'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alfredoliverwillder</reporter><labels /><created>2015-10-10T00:31:00Z</created><updated>2015-10-10T01:44:43Z</updated><resolved>2015-10-10T01:44:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-10T01:44:43Z" id="147021357">I guess you opened that in the wrong repository.

According to this doc: https://rubygems.org/gems/bonsai-elasticsearch-rails, it's provided by https://bonsai.io

I don't think we can fix anything from our side here.
Closing but feel free to reopen if you think I'm wrong.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Guava as a dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14055</link><project id="" key="" /><description>This commit removes Guava as a dependency. Note that Guava will remain
as a test-only dependency (transitively through Jimfs).

Relates #13224
</description><key id="110729518">14055</key><summary>Remove Guava as a dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T20:20:14Z</created><updated>2015-10-20T18:10:06Z</updated><resolved>2015-10-10T07:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-10T03:16:39Z" id="147027731">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Guava as a dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14054</link><project id="" key="" /><description>This commit removes Guava as a dependency. Note that Guava will remain
as a test-only dependency (transitively through Jimfs).

Closes #13224
</description><key id="110716462">14054</key><summary>Remove Guava as a dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T18:50:26Z</created><updated>2015-10-09T20:20:41Z</updated><resolved>2015-10-09T19:52:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-09T18:53:47Z" id="146960265">LGTM
</comment><comment author="nik9000" created="2015-10-09T18:56:24Z" id="146960806">![](http://www.thedailycrate.com/wp-content/uploads/2014/12/tumblr_mfwks2hlNO1qz8x31o1_400.gif)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update mustache.java to version 0.9.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14053</link><project id="" key="" /><description>This commit upgrades mustache.java to version 0.9.1. The primary motive
for this is because version 0.8.13 depends on Guava, but version 0.9.1
does not.

Relates #13224
</description><key id="110708416">14053</key><summary>Update mustache.java to version 0.9.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Search Templates</label><label>review</label><label>upgrade</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T17:58:38Z</created><updated>2015-10-13T07:57:10Z</updated><resolved>2015-10-09T17:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-09T17:59:39Z" id="146948432">LGTM. Now is the right time in the release process to do the upgrade.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unused forbidden-api file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14052</link><project id="" key="" /><description /><key id="110701444">14052</key><summary>Remove unused forbidden-api file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T17:17:47Z</created><updated>2015-10-09T17:22:08Z</updated><resolved>2015-10-09T17:22:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-09T17:19:01Z" id="146937313">@jasontedor I accidentally didn't remove this when I was cleaning up before merging #13939
</comment><comment author="jasontedor" created="2015-10-09T17:19:40Z" id="146937470">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows support to run integration tests with Gradle</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14051</link><project id="" key="" /><description>Implementation note:

The Windows `elasticsearch.bat` start script does not provide a daemon mode (nor is it easy to add one). As Gradle's `Exec` task does not support spawning a process in the background, I had to fall back to Ant's `exec` task. To make things uniform for Windows and Unix, I removed the daemon flag from the Unix shell script invocation. As consequence, the error output is a bit reduced.

Relates to #13930.
</description><key id="110693723">14051</key><summary>Windows support to run integration tests with Gradle</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>build</label></labels><created>2015-10-09T16:29:50Z</created><updated>2015-10-15T14:21:49Z</updated><resolved>2015-10-14T17:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-10-09T16:30:46Z" id="146922300">@rjernst can you have a look?
</comment><comment author="rjernst" created="2015-10-10T09:41:00Z" id="147069621">&gt;  As consequence, the error output is a bit reduced.

This is my main concern. We need to find a way to have the output of the current command (ie starting ES since this is the only one that needs to spawn) output when there is a failure.

There is a longstanding gradle issue for this:
https://issues.gradle.org/browse/GRADLE-1254

And from that, it seems there is a plugin to do exactly what we want?
https://github.com/marcovermeulen/gradle-spawn-plugin
</comment><comment author="rjernst" created="2015-10-10T10:30:55Z" id="147071963">Nevermind about that plugin, it has some inherent flaws in the api (eg it expects the process to emit something on stdout when it is "ready", and it is meant to just support unix).

Maybe it is worthwhile to use the ant impl as a template? It is not much code and self contained.
https://github.com/apache/ant/blob/master/src/main/org/apache/tools/ant/taskdefs/Exec.java

Even better would be to try and fix this in gradle...the code already has an "isDaemon" flag, but it is completely internal (looks like it is used for the gradle daemon).
https://github.com/gradle/gradle/blob/master/subprojects/core/src/main/groovy/org/gradle/process/internal/ExecHandleRunner.java
</comment><comment author="ywelsch" created="2015-10-13T12:26:22Z" id="147700268">My understanding of the original implementation is as follows (please correct me if I'm wrong). The daemon flag is added as a parameter to the Gradle Exec task. This means that the Unix shell script spawns the Elasticsearch Java process with `exec "$JAVA" ... org.elasticsearch.bootstrap.Elasticsearch start "$@" &lt;&amp;- &amp;` and does not wait for the forked process to end. Grade's errorOutput only captures errors in the shell script but not any errors related to the actual Elasticsearch Java process.

The question now is: Why can't we just do the same thing on Windows? First of all, the Windows `.bat` file does not have a daemon option (yet). This could be implemented, however, using `start "Elasticsearch" cmd /c "%JAVA_HOME%\bin\java" ...`. Seems easy at first but does not work unfortunately. The reason is that child processes spawned using `start` inherit filehandles stdin, stdout and stderr. This means that Gradle's exec process cannot exit as filehandles of the Elasticsearch Java child process are still open. Unfortunately, there is no easy way to spawn a proper daemon process from a `.bat` file. The hacky solution for this is to use another bootstrapping executable that starts the actual command using CreateProcess with `bInheritHandles` set to `false`. Possible solutions for this are to write a custom executable in C (1), or to use the windows scripting host cscript.exe (2).
1. http://stackoverflow.com/questions/1536205/running-another-program-in-windows-bat-file-and-not-create-child-process
2. https://wiki.jenkins-ci.org/display/JENKINS/Spawning+processes+from+build and
   https://wiki.jenkins-ci.org/download/attachments/1835010/antRunAsync.js

I see two solutions here:
- Support the same error handling behavior for Windows as for Linux. This means implementing a daemon flag in `elasticsearch.bat`, for example using `cscript.exe`. Still feels like a hack on Windows.
- Not support the same error handling behavior. The implementation for Linux is kept as is. The Windows batch file is started in ClusterFormationTasks.groovy by using Ant's exec task that supports spawn (Future work would be to add the spawn functionality to Gradle's `Exec` task). No error output is captured for Windows.
</comment><comment author="rjernst" created="2015-10-14T16:42:37Z" id="148112127">LGTM, thank you for this!
</comment><comment author="rjernst" created="2015-10-14T16:43:16Z" id="148112317">Maybe add a longer comment in the windows exec block explaining why it is different (and maybe create an issue to follow up on this)?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: extract common blob store tests for reuse in plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14050</link><project id="" key="" /><description>Extracts basic tests that can be shared with snapshot/restore plugins to test their basic functionality.
</description><key id="110693153">14050</key><summary>Tests: extract common blob store tests for reuse in plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>test</label></labels><created>2015-10-09T16:27:42Z</created><updated>2016-01-28T00:12:37Z</updated><resolved>2016-01-28T00:12:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-10-09T16:28:36Z" id="146921089">@tlrx that's what I had in mind. Could you take a look to see if it will help with GCS repo?
</comment><comment author="tlrx" created="2015-10-14T08:54:36Z" id="147980824">@imotov thanks for taking care of this, it looks great. I made few comments, mostly on the form and what we'd like to have in testing classes.
</comment><comment author="imotov" created="2015-10-14T17:06:39Z" id="148119989">@tlrx pushed some changes. when you have a moment, I would like to discuss your suggestion about the content of ESBlobStoreRepositoryIntegTestCase
</comment><comment author="imotov" created="2015-12-21T19:56:18Z" id="166403072">@tlrx I have rebased on the latest master and significantly simplify the common set of tests. Could you take another look and let me know if that makes more sense?
</comment><comment author="tlrx" created="2016-01-15T13:15:08Z" id="171956484">@imotov I left some minor comments but nothing really important.

I really like the latest changes, it's simpler and more meaningful so thanks a lot!
</comment><comment author="imotov" created="2016-01-19T20:35:22Z" id="172977978">@tlrx great catch! Thanks! I have pushed the latest changes after your review.
</comment><comment author="tlrx" created="2016-01-20T09:33:12Z" id="173144652">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ingest Node - enrich data as it gets in via pipelines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14049</link><project id="" key="" /><description>[[currently open issues](https://github.com/elastic/elasticsearch/pulls?q=is%3Aopen+is%3Apr+label%3A%22%3Aingest%22)]

related issues from other projects:
- Beats: https://github.com/elastic/beats/issues/805, Merged! Hurray!
- Kibana: https://github.com/elastic/kibana/issues/5974

There are many use-cases where it is important to enrich incoming data. This enrichment may be something simple like using a regular expression to extract metadata from an existing field, or something more advanced like a geoip lookup or language identification. The filter stage of the [Logstash processing pipeline](https://www.elastic.co/guide/en/logstash/current/pipeline.html#_filters) provides great examples of the ways in which data is often enriched. Node ingest implements a new type of ES node, which performs this enrichment prior to indexing.

Node ingest is a pure Java implementation of the filters in logstash, integrated with Elasticsearch. It works by wrapping the bulk/index APIs, executing a pipeline that is composed of multiple processors to enrich the documents. A processor is just a component that can modify incoming documents (the source before ES turns it into a document). A pipeline is a list of processors grouped under an unique id. If node ingest is enabled then the index and bulk apis can reroute the the request with documents through a pipeline.

The ingest plugin runs on dedicated client nodes and after bulk and index requests have been enriched these index and bulk request continue their way into the cluster.

Node ingest will be a plugin in the elasticsearch project, implementing 2 main aspects:

The first is a pure Java implementation for Pipeline, Processor, as well as initial processor implementation of grok, geoip, kv/mutate, date. This java implementation can then be reused in others places, such as logstash itself, reindex API, and so on. In the first version of the ingest plugin the processor implementations can reside in the ingest plugin, but the framework and processor implementations shouldn&#8217;t rely on any ES specific code, so that later on it can be moved to an isolated library.

The second part is the integration with Elasticsearch. This includes interception of the bulk/index APIs, management APIs (stats and so on in future phase), storage and live reload of the configuration, supporting multiple "live" pipelines, and simulation of pipeline execution.

The goal of the ingest plugin is to make data enrichment easier and it will not replace logstash at all. The ingest plugin should make data enrichment in most of the cases easier when events are only stored in Elasticsearch. For example when only file beat is used to ship logs, a logstash instance will no longer be required. In cases where events are stored in multiple outputs a Logstash installation is required. Also at some point Logstash will reuse the pipeline/processor framework, so the end goal is that both Elasticsearch and Logstash will benefit from the ingest initiative.

Development happens in a feature branch: https://github.com/elastic/elasticsearch/tree/feature/ingest

Current node ingest tasks:
- [x] Hook into the index and bulk APIs. If the ingest plugin is enabled a `pipeline_id` parameter is available to select what pipeline should be used to preprocess the documents before the index/bulk APIs get executed. #13941
- [x] Manage pipeline configuration. Pipelines are stored as a document in an index. Each node ingest node will have the pipelines in memory around to be used when needed. A background process makes sure that an ingest node will eventually get the modifications. #13941
- [x] The pipeline document enrichment shouldn't be non blocking and happen via a dedicated thread pool. #13990
- [x] Add first version of CRUD pipeline APIs. #14047
- [x] Data substructure manipulation. Processors should be able to introduce new nested fields with no pre-existing parent structure. For example, It should be possible to create a new field "location.lat" without "location" existing. #14250
- [x] Add grok processor. #14132
- [x] Add geoip processor. #14208
- [x] Add date processor. #14184 
- [x] Add kv/mutate processor. #14253 
- [x] Strict configuration validation #14552
- [x] geoip processor output fields should be configurable #14582
- [x] Add simulate API. This allows pipelines to be tested out before actually being used. This api accepts a pipeline definition and actual documents and the output is the transformed documents and optionally showing how each document gets modified after each processor. #14572 
- [x] Do not fail whole bulk request if any pipeline for a single document fails #14888
- [x] Split mutate processor into separate processors for each function (e.g. update, remove, etc) #14938
- [x] Add support for setting nested fields in document #14250
- [x] Data and metadata manipulation.  #14644
- [x] Processors and factors should throw exception on the interface.
- [x] Throw exception when grok expression does not match #15132 
- [x] Add ability to provide custom patterns within Grok Processor config definition #15167 
- [x] Reduce number of fields operated on by processors to just one. #15133 
- [x] Support for ingest transient metadata #15036
- [x] on failure pipeline handler #14548 / #15565 (tal)
- [x] append processor #14324 #15577
- [x] Ingest nodes should update pipelines in a sync manner #14998 / #15203 (mvg)
- [x] support for templating in any processor that sets a field value #14990 #15415 (mvg)
- [x] Add support for ingest node boolean flag to enable/disable ingest at the node level. If a node with `node.ingest` set to false receives an ingest request, it should explicitly fail. (mvg) #15610
- [x] Figure out if geoip2 library can be used without suppressing jvm access checks. (mvg) https://github.com/maxmind/GeoIP2-java/pull/52
- [x] Ingest forks a thread for each bulk item in a bulk request. Instead ingest should use one thread to process an entire bulk request. (mvg) #15593 
- [x] Ingest should only try the load the pipelines if the `.ingest` index has been started. (mvg) #15203 
- [x]  Cut over to a non-guice structure. We should at most bind one class to an instance directly rather thatn use the dep-injection framework that we have to un-do once we get rid of juice. #15203 (mvg)
- [x] Change `pipeline_id`param name to `pipeline`. #15618 
- [x] Add index template for the .ingest index, which should be installed by default. #15001 #15631
- [x] Move ingest infrastructure to core
- [x] Move processors with no dependencies to core
- [x] Make grok a module as it has external deps, but it will be installed by default
- [x] Make geoip a plugin which needs to be installed manually
- [x] If node ingest has been disabled then it should redirect a bulk/index request that has the ingest parameter to a node that has ingest enabled.
- [x] Add more descriptive error messages to pipeline factory exceptions (tal) #16010
- [x] In addition to the isolated processor unit tests we should also test combination of processors. #15247
- [x] Benchmark ingest plugin. #14425 (mvg&amp;tal) 
- [x] Move DedotProcessor into its own plugin #16322 
- [x] Add processor `tag`s to `on_failure` metadata #16202 
- [x] Documentation before release #16009

possible v2 tasks:
- Make it possible to let the ingest plugin know what pipeline to use via index settings / index template.
- configuration management
- _simulate w/ verbose should show document diffs between processor results #14698 
- Composite pipelines. It would be convenient to pre-define certain pipelines that process specific things that can be re-used for other documents. For example, there may be a pipeline that processes date + geoip, and these operate on fields that are common to other documents that may require further processing.
- Add ability to show stats in _simulate response to show resource usage and execution times of pipelines/processors
- A pipeline should be able to choose what (custom) thread it uses. Some pipelines just do some simple modifications to the incoming documents while other may reach out to external systems to enrich the incoming documents. #14616 
- Grok discover  api #15041
- Add notion of transactions for processors which mutate multiple fields within a document. This will allow `on_failure` processors to receive a document with a pre-failed-processor state (ref: https://github.com/elastic/elasticsearch/issues/14548#issuecomment-161799133)
- Add compare processor. #14647
- Add json processor, which converts a json string into json.
- Add kv processor.
</description><key id="110689321">14049</key><summary>Ingest Node - enrich data as it gets in via pipelines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>Meta</label></labels><created>2015-10-09T16:08:38Z</created><updated>2016-03-04T15:52:39Z</updated><resolved>2016-03-04T15:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="timini" created="2015-10-20T14:29:15Z" id="149584374">Very interesting feature would use this
</comment><comment author="marcelhallmann" created="2015-12-04T09:04:06Z" id="161912890">When will this plugin ready to use? Sounds very interesting!

Will the plugin be usabel with elastic 1.x?
</comment><comment author="javanna" created="2015-12-04T09:49:00Z" id="161922836">hi @marcelhallmann we don't have a date nor a targeted version for now, but you can monitor the progress in this meta issue. We will have a first release whenever all of the needed features for the first phase are in. We are developing against master (3.x) and considering backporting to 2.x. We will not backport to 1.x though.
</comment><comment author="McStork" created="2016-02-05T11:57:11Z" id="180319516">As many, I am looking forward to this.
On this Ingest Node feature, how does one contribute with custom filters? We plan on developing one and it would be great if it could be linked to Ingest Node, without us needing to develop it for Logstash.
</comment><comment author="timini" created="2016-02-05T12:03:00Z" id="180320657">Update on this?
</comment><comment author="martijnvg" created="2016-02-06T13:16:32Z" id="180763942">@McStork @timini If you're interested in writing your own processor you could take a look at the `geoip` processor which has been developed as a plugin: 
https://github.com/elastic/elasticsearch/tree/master/plugins/ingest-geoip

Beware that this is unreleased code and that things may change. Also it is unknown when ingest gets released.
</comment><comment author="McStork" created="2016-02-08T08:11:49Z" id="181249889">@martijnvg Thanks!
</comment><comment author="mgcrea" created="2016-02-18T16:59:30Z" id="185815316">I'm looking forward to this feature to ingest json and replace `fluentd`. :+1: 
</comment><comment author="ryanmaclean" created="2016-03-03T05:58:23Z" id="191595517">This looks fantastic! Which branch did this end up in? I'm getting a 404 on https://github.com/elastic/elasticsearch/tree/feature/ingest (linked to above, just prior to the checklist) 
</comment><comment author="javanna" created="2016-03-03T06:11:28Z" id="191597764">@ryanmaclean the branch was merged to master and deleted. Ingest node and all of its processors will be released with the next major release (namely 5.0).
</comment><comment author="timini" created="2016-03-03T09:29:52Z" id="191677008">So this is available in master now?

Any documentation out there?
</comment><comment author="javanna" created="2016-03-03T09:57:06Z" id="191687836">The first version of the docs is published as part of our reference: https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest.html .
</comment><comment author="javanna" created="2016-03-03T13:30:09Z" id="191761856">hi @luto65 I feel like this discussion would be better suited for our [discuss forums](https://discuss.elastic.co/). Would you mind posting your questions there? Then if the result of the discussion is a feature request, or a bug, a new issue can be opened on github. Thanks!
</comment><comment author="martijnvg" created="2016-03-04T15:52:39Z" id="192331163">Closing this issue, all required tasks for phase 1 are completed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin script to set proper plugin config dir attributes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14048</link><project id="" key="" /><description>Depending on how elasticsearch is installed, we have two scenarios to take into account that relate to user, group and permissions assigned to the config directory:

1) deb/rpm package: /etc/elasticsearch is root:elasticsearch 750 and the plugin script is run from root user
2) tar/zip archive: es config dir is most likely elasticsearch:elasticsearch and the plugin script is run from elasticsearch user

When the plugin script copies over the plugin config dir within the es config dir, it should take care of setting the proper user, group and permissions, which vary depending on how elasticsearch was installed in the first place. Should be root:elasticsearch 750 if installed from a package, or elasticsearch:elasticsearch if installed from an archive.

This commit makes sure that the plugin script looks at user, group and permissions of the config dir and copies them over to the plugin config subdirectory, whatever they are, so that they get properly setup depending on how elasticsearch was installed in the first place.

Relates to #11016
</description><key id="110675388">14048</key><summary>Plugin script to set proper plugin config dir attributes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugins</label><label>enhancement</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-09T14:55:42Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-13T13:57:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-09T14:56:04Z" id="146895272">@jaymode @tlrx can you have a look?
</comment><comment author="nik9000" created="2015-10-09T15:31:34Z" id="146904428">&gt; 2) tar/zip archive: es config dir is elasticsearch:elasticsearch and the plugin script is run from elasticsearch user

Err, well, tar/zip is wild west. bin/plugin needs to be run as a user that can modify the elasticsearch files but beyond that we can't be sure of anything. Right?
</comment><comment author="nik9000" created="2015-10-09T15:38:23Z" id="146906038">Left a question about +x but otherwise LGTM.
</comment><comment author="javanna" created="2015-10-12T15:07:50Z" id="147427310">I pushed new commits, ready for another review round, @jaymode @nik9000 can you have a look? ;)
</comment><comment author="nik9000" created="2015-10-12T17:52:58Z" id="147474424">LGTM
</comment><comment author="jaymode" created="2015-10-13T12:25:32Z" id="147700139">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ingest CRUD APIs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14047</link><project id="" key="" /><description>Added first version of put, get and delete pipeline APIs.

In order to support deletes the `PipelineStore` needs to also check if all the pipelines it has loaded do still exist.

The documentation is minimal and sits next to all the docs of all other plugins. We may want to move the ingest docs to the plugin itself?
</description><key id="110655015">14047</key><summary>Add ingest CRUD APIs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>review</label></labels><created>2015-10-09T13:05:22Z</created><updated>2015-10-09T16:31:31Z</updated><resolved>2015-10-09T16:31:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve circuit breaking on aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14046</link><project id="" key="" /><description>Unfortunately circuit breaking on aggregations only catches a small subset of out-of-memory errors. I was talking with @dakrone recently who had the interesting idea to also count how many aggregators we create and use it for circuit-breaking. I think it would be a good heuristic to avoid out-of-memory errors in the deeply-nested aggregation case that we often see.
</description><key id="110636821">14046</key><summary>Improve circuit breaking on aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>:Circuit Breakers</label><label>adoptme</label><label>feature</label><label>high hanging fruit</label></labels><created>2015-10-09T11:09:20Z</created><updated>2016-08-30T16:12:48Z</updated><resolved>2016-07-25T20:01:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-12T23:01:55Z" id="147544418">When I discussed this with @jpountz we talked about potentially adding this to the constructor of `AggregatorBase` to increment when it is created (since we can get the breaker from the aggregation context) and decrement it in the `close` method, which I think is a neat idea to try.

Part of this would be determining the "weight" of a single `AggregatorBase`, then we could have a separate `BucketBreaker` where the `overhead` could be tuned for specific use cases (as a mulitplier of the class size).

Thoughts and heap dumps welcome :)
</comment><comment author="dakrone" created="2015-10-28T20:25:10Z" id="151979423">If anyone is curious about this, I'm working on a branch with it here: https://github.com/elastic/elasticsearch/compare/master...dakrone:bucket-circuit-breaker
</comment><comment author="dakrone" created="2016-07-25T20:01:25Z" id="235066887">Closed by https://github.com/elastic/elasticsearch/pull/19394
</comment><comment author="haizaar" created="2016-08-30T06:06:24Z" id="243341863">Will this circuit breaker prevent me from running aggregation that returns zillion buckets? Like generating histogram on file size metrics with step of 5, which can easily result in trillion buckets.
</comment><comment author="dakrone" created="2016-08-30T16:12:48Z" id="243493219">@haizaar that's the intention, yes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>wrong network interface taking when using number networks and bonding</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14045</link><project id="" key="" /><description>Our cluster is running on redhat 6.
We have 2 networks in our cluster. Primary network and backup network. Backup network it's not network for emergency this network used by backup system.
The primary network is using bonding (using bond0 where its slaves are
eth0 &amp; eth1). The backup network is using eth2 .
The issue we are facing is :  when running "get localhost:9200/_nodes the "primary_network" is
eth2 and not bond0. When we try to change the parameter "network.host: _bond0_" we got errors :
"closing connection java.nio.channels.UnresolvedAddressException"
Right now we solve problem by insert bond0 IP address to network.host parameter in yml configuration, but this is pure solution because IP address will be change from time to time. And this is confusing when we see primary_network one IP address and in transport section bound and publish another IP with different mask.
</description><key id="110636781">14045</key><summary>wrong network interface taking when using number networks and bonding</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ozlevka</reporter><labels><label>:Network</label></labels><created>2015-10-09T11:08:58Z</created><updated>2015-10-20T15:52:39Z</updated><resolved>2015-10-20T15:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-09T11:22:43Z" id="146838237">Can you confirm that you are attempting this with `_bond0` instead of `_bond0_` (it should be the latter)? If so, would you be able to try `_bond0_`?
</comment><comment author="ozlevka" created="2015-10-09T12:44:27Z" id="146857850">Sorry, this is only printing mistake. Of course we use _bond0_ as parameter.
If it can help we use version 1.4.4, but I not find some different between
1.4.4 and current stable version.

On Fri, Oct 9, 2015 at 2:23 PM, Jason Tedor notifications@github.com
wrote:

&gt; Can you confirm that you are attempting this with _bond0 instead of
&gt; _bond0_ (it should be the latter)? If so, would you be able to try _bond0_
&gt; ?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14045#issuecomment-146838237
&gt; .

## 

Best regards
Ozeryansky Lev
</comment><comment author="clintongormley" created="2015-10-13T07:42:40Z" id="147632054">Hi @ozlevka 

The networking binding code has been extensively rewritten in 2.0.  Could you try the 2.0.0-rc1 release and see if it suffers from the same problem please?
</comment><comment author="jasontedor" created="2015-10-14T02:33:33Z" id="147906543">@ozlevka I setup a CentOS installation with a bonded interface and Elasticsearch 1.4.4, and I'm not able to reproduce into the issue that you describe.

```
$ ifconfig bond0
bond0     Link encap:Ethernet  HWaddr 08:00:27:6B:E2:E2
          inet addr:192.168.33.12  Bcast:192.168.33.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:feff:1c2a/64 Scope:Link
          UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
          RX packets:15 errors:0 dropped:0 overruns:0 frame:0
          TX packets:31 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0
          RX bytes:918 (918.0 b)  TX bytes:2382 (2.3 KiB)
```

The bonding status:

```
$ cat /proc/net/bonding/bond0
Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)

Bonding Mode: fault-tolerance (active-backup) (fail_over_mac active)
Primary Slave: None
Currently Active Slave: eth3
MII Status: up
MII Polling Interval (ms): 100
Up Delay (ms): 200
Down Delay (ms): 200

Slave Interface: eth3
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 08:00:27:6b:e2:e2
Slave queue ID: 0

Slave Interface: eth2
MII Status: up
Speed: 1000 Mbps
Duplex: full
Link Failure Count: 0
Permanent HW addr: 08:00:27:ff:1c:2a
Slave queue ID: 0
```

The only change that I made to the default configuration is:

```
$ cat /etc/elasticsearch/elasticsearch.yml | grep network.host
network.host: _bond0_
```

and I forced IPv4 by adding 

```
ES_JAVA_OPTS="-Djava.net.preferIPv4Stack=true"
```

to `/etc/sysconfig/elasticsearch`.

From the startup logs:

```
[2015-10-13 22:49:25,150][INFO ][transport                ] [Quentin Quire] bound_address {inet[/192.168.33.12:9300]}, publish_address {inet[/192.168.33.12:9300]}
[2015-10-13 22:49:25,200][INFO ][discovery                ] [Quentin Quire] elasticsearch/f8-Z5kcKQsaz5uRImrXB8g
[2015-10-13 22:49:29,009][INFO ][cluster.service          ] [Quentin Quire] new_master [Quentin Quire][f8-Z5kcKQsaz5uRImrXB8g][localhost][inet[/192.168.33.12:9300]], reason: zen-disco-join (elected_as_master)
[2015-10-13 22:49:29,048][INFO ][http                     ] [Quentin Quire] bound_address {inet[/192.168.33.12:9200]}, publish_address {inet[/192.168.33.12:9200]}
```

And:

```
  $ curl -XGET 192.168.33.12:9200
  {
    "status" : 200,
    "name" : "Quentin Quire",
    "cluster_name" : "elasticsearch",
    "version" : {
      "number" : "1.4.4",
      "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
      "build_timestamp" : "2015-02-19T13:05:36Z",
      "build_snapshot" : false,
      "lucene_version" : "4.10.3"
    },
    "tagline" : "You Know, for Search"
  }
```

As for a get on `_nodes` returning a `primary_interface` of `eth2` on your box, ignore that; that is due to the Sigar library that this depends on and is happening outside of Elasticsearch; it does not mean that Elasticsearch is even using this interface at all. As of Elasticsearch 2.0, we no longer rely on this library and do not even report this information in the `_nodes` endpoint any more.

So, where this leaves us is that we're not concerned by the `primary_interface` reporting, and we are not able to reproduce the issue that you describe. If you're able to provide additional information that will help us reproduce the issue, we would be happy to investigate further. 
</comment><comment author="ozlevka" created="2015-10-15T08:12:01Z" id="148313369">I'll ask system team when I can process upgrade. I think we can wait to GA
version of 2.0

Thank you

On Tue, Oct 13, 2015 at 10:43 AM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @ozlevka https://github.com/ozlevka
&gt; 
&gt; The networking binding code has been extensively rewritten in 2.0. Could
&gt; you try the 2.0.0-rc1 release and see if it suffers from the same problem
&gt; please?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14045#issuecomment-147632054
&gt; .

## 

Best regards
Ozeryansky Lev
</comment><comment author="ozlevka" created="2015-10-15T08:14:08Z" id="148313985">Thank you Jason
About: "get on _nodes returning a primary_interface of eth2 on your box,
ignore that". This is exactly what we do.
I hope it's will be fixed in 2.0 GA version.

Thank you

On Wed, Oct 14, 2015 at 5:34 AM, Jason Tedor notifications@github.com
wrote:

&gt; @ozlevka https://github.com/ozlevka I setup a CentOS installation with
&gt; a bonded interface and Elasticsearch 1.4.4, and I'm not able to reproduce
&gt; into the issue that you describe.
&gt; 
&gt; [vagrant@localhost ~]$ ifconfig bond0
&gt; bond0     Link encap:Ethernet  HWaddr 08:00:27:6B:E2:E2
&gt;           inet addr:192.168.33.12  Bcast:192.168.33.255  Mask:255.255.255.0
&gt;           inet6 addr: fe80::a00:27ff:feff:1c2a/64 Scope:Link
&gt;           UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
&gt;           RX packets:15 errors:0 dropped:0 overruns:0 frame:0
&gt;           TX packets:31 errors:0 dropped:0 overruns:0 carrier:0
&gt;           collisions:0 txqueuelen:0
&gt;           RX bytes:918 (918.0 b)  TX bytes:2382 (2.3 KiB)
&gt; 
&gt; The bonding status:
&gt; 
&gt; [vagrant@localhost ~]$ cat /proc/net/bonding/bond0
&gt; Ethernet Channel Bonding Driver: v3.7.1 (April 27, 2011)
&gt; 
&gt; Bonding Mode: fault-tolerance (active-backup) (fail_over_mac active)
&gt; Primary Slave: None
&gt; Currently Active Slave: eth3
&gt; MII Status: up
&gt; MII Polling Interval (ms): 100
&gt; Up Delay (ms): 200
&gt; Down Delay (ms): 200
&gt; 
&gt; Slave Interface: eth3
&gt; MII Status: up
&gt; Speed: 1000 Mbps
&gt; Duplex: full
&gt; Link Failure Count: 0
&gt; Permanent HW addr: 08:00:27:6b:e2:e2
&gt; Slave queue ID: 0
&gt; 
&gt; Slave Interface: eth2
&gt; MII Status: up
&gt; Speed: 1000 Mbps
&gt; Duplex: full
&gt; Link Failure Count: 0
&gt; Permanent HW addr: 08:00:27:ff:1c:2a
&gt; Slave queue ID: 0
&gt; 
&gt; The only change that I made to the default configuration is:
&gt; 
&gt; [vagrant@localhost ~]$ cat /etc/elasticsearch/elasticsearch.yml | grep network.host
&gt; network.host: _bond0_
&gt; 
&gt; and I forced IPv4 by adding
&gt; 
&gt; ES_JAVA_OPTS="-Djava.net.preferIPv4Stack=true"
&gt; 
&gt; to /etc/sysconfig/elasticsearch.
&gt; 
&gt; From the startup logs:
&gt; 
&gt; [2015-10-13 22:49:25,150][INFO ][transport                ] [Quentin Quire] bound_address {inet[/192.168.33.12:9300]}, publish_address {inet[/192.168.33.12:9300]}
&gt; [2015-10-13 22:49:25,200][INFO ][discovery                ] [Quentin Quire] elasticsearch/f8-Z5kcKQsaz5uRImrXB8g
&gt; [2015-10-13 22:49:29,009][INFO ][cluster.service          ] [Quentin Quire] new_master [Quentin Quire][f8-Z5kcKQsaz5uRImrXB8g][localhost][inet[/192.168.33.12:9300]], reason: zen-disco-join (elected_as_master)
&gt; [2015-10-13 22:49:29,048][INFO ][http                     ] [Quentin Quire] bound_address {inet[/192.168.33.12:9200]}, publish_address {inet[/192.168.33.12:9200]}
&gt; 
&gt; And:
&gt; 
&gt;   [vagrant@localhost ~]$ curl -XGET 192.168.33.12:9200
&gt;   {
&gt;     "status" : 200,
&gt;     "name" : "Quentin Quire",
&gt;     "cluster_name" : "elasticsearch",
&gt;     "version" : {
&gt;       "number" : "1.4.4",
&gt;       "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
&gt;       "build_timestamp" : "2015-02-19T13:05:36Z",
&gt;       "build_snapshot" : false,
&gt;       "lucene_version" : "4.10.3"
&gt;     },
&gt;     "tagline" : "You Know, for Search"
&gt;   }
&gt; 
&gt; As for a get on _nodes returning a primary_interface of eth2 on your box,
&gt; ignore that; that is due ot the Sigar library that this depends on and is
&gt; happening outside of Elasticsearch; it does not mean that Elasticsearch is
&gt; even using this interface at all.
&gt; 
&gt; So, where this leaves us is that we're not concerned by the
&gt; primary_interface reporting, and we are not able to reproduce the issue
&gt; that you describe. If you're able to provide additional information that
&gt; will help us reproduce the issue, we would be happy to investigate further.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14045#issuecomment-147906543
&gt; .

## 

Best regards
Ozeryansky Lev
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fold IndexAliasesService into IndexService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14044</link><project id="" key="" /><description>The IndexAliasesService abstraction only adds unnecessary code and classes
and can be removed. This commit folds the rather simple methods in this
class into IndexService where the IndexAliasesService was obtained from in the past.
</description><key id="110620127">14044</key><summary>Fold IndexAliasesService into IndexService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T09:22:16Z</created><updated>2015-10-13T08:27:51Z</updated><resolved>2015-10-11T20:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-10T09:19:20Z" id="147068053">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request : Closed index </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14043</link><project id="" key="" /><description>Dear Elasticsearch,

It would be nice, if there would be an option that specifies whether the indices which are closed  would be working as deleted indices. By this I mean that if an index has been closed, it shoud not be listed by _aliases or _stats api.  I know blocked indices can be asked by "_cluster/state/blocks" but unfortunatelly my collegaues using open and close function for starting elasticsearch faster. I mean when they start elasticsearch they close each index after they opening them one by one manually. I told them the best solution would be if more machines being used for elasticsearch as cluster, but they stick to their "method". However, they want to search in indices which are closed because the _aliases api got indicies back which are closed (however they are unusable). I know the closed indices are blocked for read write operations, and they can not be used any operation so it would be great if it were an option to use (automatically) closed/blocked indices as deleted indices till they will not be recovered. Could you help me ?  I really don't want to check the indices status before each operation.

I hope this is the right place to solve my issue.

Thank you in advance for any help you can provide.
</description><key id="110615069">14043</key><summary>Feature request : Closed index </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emraxxor</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2015-10-09T08:50:05Z</created><updated>2015-10-13T12:44:43Z</updated><resolved>2015-10-13T07:41:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-09T09:35:45Z" id="146813636">Hi @onthefloorr 

I'm a bit confused as to what you are wanting.  Are you saying that what you want to do is to be able to search on an alias which points to multiple indices, and just ignore the closed indices instead of throwing an exception?

Or is there more to this?
</comment><comment author="emraxxor" created="2015-10-09T12:32:22Z" id="146853401">Hi @clintongormley ,

By default, an  (I don't know what kind of type at this moment) exception is occured if I try to do any (read/write) operation on index which are closed.  Here's where my problem comes in, my application is using indices which are returned in the JSON response body of "_aliases" (of course the number of investigated indexes is limited but I check the existence of index by this way) . For instance, the result of "_aliases" includes indices that are closed, and my application tries to search in these indexes, but an exception will be occurred because the "alias"/"index" is closed.  Basically yes, I would like if it were an option (which would be specified in the elasticsearch.ini) to ignore/skip the closed indices instead of throwing an exception. 
</comment><comment author="clintongormley" created="2015-10-13T07:41:00Z" id="147631665">@onthefloorr This is implemented in 2.0.  The `_aliases` end point doesn't return closed indices by default.
</comment><comment author="emraxxor" created="2015-10-13T12:44:43Z" id="147704406">Thank you very much for the quick reply and the quick help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup the Lucene utility class.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14042</link><project id="" key="" /><description>- removes Lucene.count(IndexSearcher,Query) in favor of IndexSearcher.count(Query)
- removes EarlyTerminatingCollector.reset(): reusing Collector objects does not
  help given that query execution needs to allocate objects (weights, scorers)
  anyway
- adds unit tests to Lucene.exists
</description><key id="110609585">14042</key><summary>Cleanup the Lucene utility class.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>non-issue</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T08:10:04Z</created><updated>2015-10-09T10:04:22Z</updated><resolved>2015-10-09T08:48:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-09T08:19:35Z" id="146794586">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix suggest builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14041</link><project id="" key="" /><description>suggest builder generates wrong content tree
</description><key id="110601463">14041</key><summary>fix suggest builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gruzovator</reporter><labels><label>:Suggesters</label><label>bug</label><label>feedback_needed</label></labels><created>2015-10-09T07:22:39Z</created><updated>2016-03-10T12:23:47Z</updated><resolved>2016-03-10T12:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-09T08:20:43Z" id="146794768">hey, thanks for opening this, can you sign the CLA and add a test for this to make sure it doesn't' happen again?
</comment><comment author="clintongormley" created="2016-03-10T12:23:47Z" id="194818098">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose pending cluster state queue size in node stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14040</link><project id="" key="" /><description>Add stats about the pending cluster state queue: total queue size, number of committed cluster
states, and number of pending cluster states.

closes #13610
</description><key id="110600163">14040</key><summary>Expose pending cluster state queue size in node stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Stats</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-09T07:11:26Z</created><updated>2015-10-30T13:00:29Z</updated><resolved>2015-10-28T18:03:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-20T13:54:05Z" id="149574929">Thanks @xuzha for picking this up. I left some comments. Also, I think it will be great if the xContent output will look like this:

```
...
          "discovery": {
            "cluster_state_queue": {
            "total": 0,
            "pending": 0,
            "committed": 0
         }
```
</comment><comment author="xuzha" created="2015-10-22T06:33:17Z" id="150122362">@bleskes  I pushed another commit. I probably don't get your suggestion. Please take a look and let me know what you think. Thanks again.
</comment><comment author="bleskes" created="2015-10-26T08:10:13Z" id="151055829">This looks great. I left some minor comments. Can we also add a rest test ?
</comment><comment author="xuzha" created="2015-10-27T04:30:07Z" id="151371962">Thanks @bleskes, I updated the PR, please take another look if you have time.
</comment><comment author="bleskes" created="2015-10-27T08:07:30Z" id="151408063">LGTM. Left some minor comments. Feel free to push once addressed. Thanks!
</comment><comment author="xuzha" created="2015-10-28T18:04:15Z" id="151934854">Thanks @bleskes again for the review,  merged.
</comment><comment author="clintongormley" created="2015-10-29T20:33:02Z" id="152310027">@xuzha should we backport to 2.2, 2.x?
</comment><comment author="xuzha" created="2015-10-29T20:43:26Z" id="152314495">@clintongormley hmm, I think we should not. This is PR is based on PendingClusterStatesQueue (https://github.com/elastic/elasticsearch/pull/13303), which is only in master.
</comment><comment author="clintongormley" created="2015-10-30T13:00:29Z" id="152516777">thanks @xuzha 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore an index not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14039</link><project id="" key="" /><description>I have  set up back up of my elastic search index. I am trying to  restore the index but to a new name.
I am running the following command

curl -XPOST 'localhost:9200/_snapshot/backup_snapshots_backup/backup_snapshot_1/_restore
{
    "indices": "csv1", 
    "rename_pattern": "csv(.+)", 
    "rename_replacement": "csv2" 
}'

I keep getting
{"error":"SnapshotRestoreException[[backup_snapshots_backup:backup_snapshot_1] cannot restore index [csv1] because it's open]","status":500}

even after closing the index csv1.

Any ideas what I am doing wrong?
</description><key id="110530301">14039</key><summary>Restore an index not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malinir123</reporter><labels><label>:Snapshot/Restore</label><label>feedback_needed</label></labels><created>2015-10-08T20:02:55Z</created><updated>2015-11-27T16:46:02Z</updated><resolved>2015-11-27T16:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T07:18:17Z" id="147627140">This works fine for me on 1.7.1.   Any more information that you can provide, eg ES version, anything in the logs, etc?
</comment><comment author="ywelsch" created="2015-11-27T16:46:02Z" id="160173340">no feedback -&gt; close
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Let FunctionScoreTests differ a bit more</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14038</link><project id="" key="" /><description>These tests used to attempt to reimplement the FunctionScore stuff to .01%
tolerances and some randomized testing parameters they failed:
http://build-us-00.elastic.co/job/es_g1gc_master_metal/20594/console

This lets the tests differ by up to .1% and still not fail.
</description><key id="110529229">14038</key><summary>Let FunctionScoreTests differ a bit more</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-10-08T19:56:09Z</created><updated>2015-10-13T17:46:10Z</updated><resolved>2015-10-13T17:46:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-08T19:56:21Z" id="146669201">@brwe I think this might be yours to review?
</comment><comment author="brwe" created="2015-10-09T07:41:39Z" id="146781246">This will make the test pass but I digged a little and found that actually there is two things wrong with this test and I think there is a bug too:
What it should do is simulate the exact way that the score is computed and then compare to the float we get from lucene. The score is computed using doubles for summing up and so should the test. We should then cast the expected score we compute in the test to float like scoring in lucene does and compare as floats. The test does not do that. If we change this we can actually do the comparison with `is(1.0f)` instead of `closeTo(1, 1.e-4d)`.
In addition, I think FiltersFunctionScore should sum up the weights for weighted avg as float but does as double here: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/lucene/search/function/FiltersFunctionScoreQuery.java#L344

My bad, I was sloppy when I refactored and did not give it much thought. 

Does that make sense? If so, since you are already at it, want to make that change?
</comment><comment author="nik9000" created="2015-10-12T15:51:53Z" id="147444385">Makes sense to me!

&gt; Does that make sense? If so, since you are already at it, want to make that change?

I'm ok either way. I think it'll be a day or two before I can pick this up again but I'll do what you mentioned when I do. If you want to take it in the mean time then be my guest. If not I'm happy to get it.
</comment><comment author="brwe" created="2015-10-13T11:38:15Z" id="147690875">I made a pr here: #14085
</comment><comment author="nik9000" created="2015-10-13T17:46:10Z" id="147791423">Thanks for fixing this better than I did!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gradle config for forked jvms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14037</link><project id="" key="" /><description>I spent 20 minutes reading gradle docs to figure out how to do this. No one
else should have to.
</description><key id="110523342">14037</key><summary>Gradle config for forked jvms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>v5.0.0-alpha1</label></labels><created>2015-10-08T19:18:36Z</created><updated>2015-11-30T14:18:31Z</updated><resolved>2015-11-30T14:16:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-11-29T01:20:10Z" id="160353644">I'm going through my outdated PRs. Do we want this or should I just close it? I'm fine either way. I still run  the tests on 12 JVMs quite happily.
</comment><comment author="rjernst" created="2015-11-29T01:49:05Z" id="160357493">LGTM
</comment><comment author="nik9000" created="2015-11-30T14:16:47Z" id="160642265">PR is against the wrong branch. I'll close it and just push the changes to master.
</comment><comment author="nik9000" created="2015-11-30T14:18:31Z" id="160642644">b6826bfc78bbae42e39e41189e29f6b4363c710b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>README s/mvn/gradle/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14036</link><project id="" key="" /><description /><key id="110518762">14036</key><summary>README s/mvn/gradle/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2015-10-08T18:54:52Z</created><updated>2015-10-09T12:05:09Z</updated><resolved>2015-10-09T12:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-08T18:57:55Z" id="146654427">Thanks for this! LGTM, a couple minor comments.
</comment><comment author="nik9000" created="2015-10-08T19:16:02Z" id="146660471">&gt; Thanks for this! LGTM, a couple minor comments.

Amended.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use a system property to locate JAVA_HOME</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14035</link><project id="" key="" /><description>It's more reliable than environment variable.
</description><key id="110517607">14035</key><summary>Use a system property to locate JAVA_HOME</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label></labels><created>2015-10-08T18:49:00Z</created><updated>2015-10-09T12:04:56Z</updated><resolved>2015-10-09T12:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-08T18:50:55Z" id="146652254">LGTM, i couple minor suggestions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fresh install ES 1.7.2 bin/plugin fails on El Capitan</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14034</link><project id="" key="" /><description>In trying to confirm if another issue still exists on 1.7.2, I got the following error while trying to install a plugin:

$ ./bin/plugin 
./bin/plugin: line 108: /Library/Internet: is a directory

I resolved the issue for now w/ a symlink /Library/Internet -&gt; /Library/Internet Plug-Ins/ and reset $JAVA_HOME. Guessing this is related to #12610, #12504.
</description><key id="110515627">14034</key><summary>fresh install ES 1.7.2 bin/plugin fails on El Capitan</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dblado</reporter><labels /><created>2015-10-08T18:37:47Z</created><updated>2015-10-13T23:42:38Z</updated><resolved>2015-10-13T07:10:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T07:10:59Z" id="147626123">Agreed.  This has been fixed in 2.0 by moving param parsing to Java from shell.

Seems a bit odd that your JAVA_HOME was pointing to /Library/Internet Plug-Ins/ though?  Mine contains: `/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home`
</comment><comment author="dblado" created="2015-10-13T23:42:38Z" id="147885590">@clintongormley Odd -- I don't recall changing my JAVA_HOME but apparently I had...thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>whitespace allowed in template field name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14033</link><project id="" key="" /><description>If I `PUT` this template definition to `http://localhost:9200/_template/my_new_template` it works, but notice the field name `"some_field "` actually has a trailing whitespace in the String (I think that should be trimmed).

``` json
{
  "template": "some_index_*",
  "mappings": {
    "some_mapping": {
      "properties": {
        "some_field ": {
          "type": "double"
        }
      }
    }
  }
}
```
</description><key id="110502260">14033</key><summary>whitespace allowed in template field name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">podollb</reporter><labels /><created>2015-10-08T17:21:21Z</created><updated>2015-10-13T07:03:05Z</updated><resolved>2015-10-13T07:03:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-13T07:03:05Z" id="147625150">Agreed - closing as duplicate of #9059
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some searches are not returning all the documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14032</link><project id="" key="" /><description>We have configured Elastic Search to have 3 nodes, 3 shards 1 primary and 2 replica shards.
Elastic search health shows that all shards are assigned and running and in green state.
But some searches returns Fetch Failed [Failed to fetch doc id [241839]]]; nested: EOFException[seek past EOF: MMapIndexInput(path=\"/data/elasticsearch/clusterdata/data/abcj/nodes/0/indices/csv1/0/index/_1f9s.fdt\")]; "

error and not all the documents are returned.

Any Idea why I am seeing this? Is the data corrupted?

Thanks
</description><key id="110478842">14032</key><summary>Some searches are not returning all the documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malinir123</reporter><labels /><created>2015-10-08T15:42:24Z</created><updated>2015-10-08T16:41:00Z</updated><resolved>2015-10-08T16:25:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T16:25:20Z" id="146602883">Hi @malinir123 

It looks like you have a corrupt index.  Given that you haven't had any corruption exceptions, I'm guessing that you're on an old version.  I'd recommend upgrading.
</comment><comment author="malinir123" created="2015-10-08T16:29:54Z" id="146604904">I am using elasticsearch-1.1.1.

How do we fix the corrupted index?
</comment><comment author="clintongormley" created="2015-10-08T16:41:00Z" id="146614732">You have to reindex
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the sentence actually understandable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14031</link><project id="" key="" /><description /><key id="110465725">14031</key><summary>Make the sentence actually understandable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>docs</label></labels><created>2015-10-08T14:37:32Z</created><updated>2015-10-08T16:33:32Z</updated><resolved>2015-10-08T16:17:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T16:18:25Z" id="146599375">Nice to see you gearing up for 2.0! - thanks, and merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>move render search template methods to cluster admin client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14030</link><project id="" key="" /><description>In #13971, the RenderSearchTemplateAction was made a cluster level action but the client methods
were not moved from the IndicesAdminClient. This is an inconsistency and this change cleans up the
inconsistency so that the client methods are now part of the ClusterAdminClient since the action is now
considered a cluster level action.
</description><key id="110463979">14030</key><summary>move render search template methods to cluster admin client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>review</label><label>v2.0.0</label></labels><created>2015-10-08T14:29:23Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-08T14:41:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-08T14:34:14Z" id="146564798">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>KnownActionsTests has started failing inconsistently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14029</link><project id="" key="" /><description>It looks like just this morning:
- Passed: https://internal-build.elastic.co/job/x-plugins-20-medium/1774/
- Failed: https://internal-build.elastic.co/job/x-plugins-20-medium/1775/
- Passed: https://internal-build.elastic.co/job/x-plugins-20-medium/1776/
- Failed: https://internal-build.elastic.co/job/x-plugins-20-medium/1777/
</description><key id="110452106">14029</key><summary>KnownActionsTests has started failing inconsistently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label></labels><created>2015-10-08T13:34:46Z</created><updated>2015-10-08T13:36:36Z</updated><resolved>2015-10-08T13:36:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-08T13:35:21Z" id="146546727">Woops - https://internal-build.elastic.co/job/x-plugins-20-medium/1775/ was another failure: #730
</comment><comment author="nik9000" created="2015-10-08T13:36:36Z" id="146547128">Ah! Wrong project!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove @Test annotation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14028</link><project id="" key="" /><description>I vaguely remember a discussion about Elasticsearch preferring tests who's names start `test` rather than using the `@Test` annotation. I've been removing the annotations as I touch test files and remember it and folks seem pretty happy with that approach. The trouble with that approach is that it relies on communication and consistency both of which I think people are bad at. I know I am.

If we don't like the annotation I think we should ban it using forbidden apis and I should just remove them all. If we do like the annotation then we should just close the issue.
</description><key id="110449292">14028</key><summary>Remove @Test annotation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>non-issue</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-08T13:19:35Z</created><updated>2015-10-20T22:33:46Z</updated><resolved>2015-10-20T22:33:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-08T18:33:55Z" id="146647477">+1 to remove. I also remove it anytime I touch a file that has it. It is just an extra line per test for no reason.
</comment><comment author="s1monw" created="2015-10-08T18:34:15Z" id="146647561">+1
</comment><comment author="mikemccand" created="2015-10-08T18:40:50Z" id="146649147">+1
</comment><comment author="jpountz" created="2015-10-09T08:31:24Z" id="146796378">+1
</comment><comment author="martijnvg" created="2015-10-09T12:19:26Z" id="146848306">+1

On 9 October 2015 at 10:31, Adrien Grand notifications@github.com wrote:

&gt; +1
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/14028#issuecomment-146796378
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="nik9000" created="2015-10-09T17:07:02Z" id="146933738">I'll take it!

I'll just do it in master unless anyone has objections.
</comment><comment author="nik9000" created="2015-10-09T19:13:39Z" id="146964439">Lol

```
Scanned 2206 (and 3144 related) class file(s) for forbidden API invocations (in 8.72s), 3145 error(s).
```
</comment><comment author="nik9000" created="2015-10-09T19:13:50Z" id="146964481">**3145**
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate `index.analysis.analyzer.default_index` in favor of `index.analysis.analyzer.default`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14027</link><project id="" key="" /><description>Close #11861
</description><key id="110447547">14027</key><summary>Deprecate `index.analysis.analyzer.default_index` in favor of `index.analysis.analyzer.default`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>deprecation</label><label>review</label><label>v2.0.0</label></labels><created>2015-10-08T13:11:36Z</created><updated>2015-10-12T21:24:23Z</updated><resolved>2015-10-12T21:24:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-08T18:22:27Z" id="146644647">LGTM
</comment><comment author="jpountz" created="2015-10-09T14:13:23Z" id="146883006">@rjernst I pushed more commits to address your comments
</comment><comment author="jpountz" created="2015-10-12T10:27:05Z" id="147359781">@rjernst I pushed a new commit.
</comment><comment author="rjernst" created="2015-10-12T16:59:05Z" id="147460762">LGTM, one very minor comment on the test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Document ES_CLASSPATH removal in breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14026</link><project id="" key="" /><description>Closes #14008
</description><key id="110444257">14026</key><summary>Docs: Document ES_CLASSPATH removal in breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>docs</label><label>review</label><label>v2.0.0</label></labels><created>2015-10-08T12:59:08Z</created><updated>2015-10-08T16:23:37Z</updated><resolved>2015-10-08T16:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-08T15:19:19Z" id="146576940">The cwd on the classpath was just a side effect when ES_CLASSPATH was empty, it didn't always happen, and I dont think it is worth noting. Also, I would use stronger words against putting things in lib dir?
</comment><comment author="clintongormley" created="2015-10-08T16:23:32Z" id="146601771">Reworded and merged - thanks @rjernst 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Seccomp tests fail on some ES Jenkins runs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14025</link><project id="" key="" /><description>The seccomp tests fail pretty consistently on es_feature_geo_point_field_v2 builds:
http://build-us-00.elastic.co/job/es_feature_geo_point_field_v2/2491/console
http://build-us-00.elastic.co/job/es_feature_geo_point_field_v2/2490/console
http://build-us-00.elastic.co/job/es_feature_geo_point_field_v2/2489/console
</description><key id="110437661">14025</key><summary>Seccomp tests fail on some ES Jenkins runs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>jenkins</label><label>test</label></labels><created>2015-10-08T12:26:45Z</created><updated>2015-10-08T12:58:08Z</updated><resolved>2015-10-08T12:43:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-08T12:27:20Z" id="146522676">It looks like this branch has the same version of the test.
</comment><comment author="nik9000" created="2015-10-08T12:29:37Z" id="146523174">The tests pass in both branches on my ubuntu machine when I run:

```
mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.class=org.elasticsearch.bootstrap.SeccompTests -Dtests.security.manager=false
```

Note that you have to disable the security manager for these test to run at all.
</comment><comment author="nik9000" created="2015-10-08T12:39:07Z" id="146525306">Note: that branch doesn't always fail with seccomp test failures. The tests pass here:
http://build-us-00.elastic.co/job/es_feature_geo_point_field_v2/2482/consoleFull
http://build-us-00.elastic.co/job/es_feature_geo_point_field_v2/2481/consoleFull
</comment><comment author="rmuir" created="2015-10-08T12:43:00Z" id="146526349">You need to merge master to it. 
</comment><comment author="nik9000" created="2015-10-08T12:43:10Z" id="146526382">The two builds that passed that I linked to were run on 
http://build-us-00.elastic.co/computer/ubuntu-14-64-12-metal
but the failing builds were run on
http://build-us-00.elastic.co/computer/opensuse-13-64-0-metal
http://build-us-00.elastic.co/computer/ubuntu-14-64-7-metal
</comment><comment author="nik9000" created="2015-10-08T12:44:19Z" id="146526786">&gt; You need to merge master to it.

Fair enough. I didn't see what changed with seccomp tests. Could you link me to the change so I can improve my git-search-foo?
</comment><comment author="rmuir" created="2015-10-08T12:45:11Z" id="146527051">No. i shouldnt have to fix bugs N times because of feature branches.
</comment><comment author="nik9000" created="2015-10-08T12:53:40Z" id="146531269">&gt; No. i shouldnt have to fix bugs N times because of feature branches.

No but if you could help me to find where you fixed it in master then I'd have a better time solving this problem again when it comes up again.

I'll just ignore these failures from here on out.
</comment><comment author="rmuir" created="2015-10-08T12:58:08Z" id="146533384">the whole process is broken here. A little pushing back from me is totally warranted. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove excess comma in code block</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14024</link><project id="" key="" /><description /><key id="110427991">14024</key><summary>Remove excess comma in code block</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>docs</label></labels><created>2015-10-08T11:23:35Z</created><updated>2015-10-08T16:33:16Z</updated><resolved>2015-10-08T16:07:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T16:07:57Z" id="146594534">thanks @ankon - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[pre-2.0.0] Order of analyzer/index_analyzer/search_analyzer is unpredictable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14023</link><project id="" key="" /><description>_This only applies to pre-2.0.0 ES!_

While reviewing the changes in ES 2.x I noticed the removal of the `index_analyzer` in favor of just `analyzer` and `search_analyzer` in a mapping. 
We actually use `index_analyzer` and `search_analyzer`, so I looked for documentation for pre-2.0.0 to tell me which order is used if I just replaced `index_analyzer` with `analyzer` - this way I can use a 2.0.0-compatible mapping already in the application, and decouple the actual update of the ES cluster from the application development cycle.

From my reading of the code that isn't actually possible: pre-2.0.0 ES parses the three settings in whatever order they appear in the map of settings, so when I use `analyzer` and `search_analyzer` in those versions it can happen that the `search_analyzer` setting gets ignored (if the parser first finds `search_analyzer` and then `analyzer`).

afcedb94 (from #9371) fixes that while removing the `index_analyzer` by only applying the settings after having processed the whole map.

It would be really helpful to backport just that part to pre-2.0.0, so that migration doesn't depend on changing both the application code and the ES version at the same time.
</description><key id="110423862">14023</key><summary>[pre-2.0.0] Order of analyzer/index_analyzer/search_analyzer is unpredictable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ankon</reporter><labels><label>:Mapping</label><label>blocker</label><label>bug</label><label>v1.7.3</label></labels><created>2015-10-08T10:53:54Z</created><updated>2015-10-14T07:39:52Z</updated><resolved>2015-10-14T07:39:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T07:39:51Z" id="147963714">Closed by https://github.com/elastic/elasticsearch/pull/14060
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix the link pointing to the repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14022</link><project id="" key="" /><description /><key id="110422381">14022</key><summary>Fix the link pointing to the repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>docs</label></labels><created>2015-10-08T10:44:39Z</created><updated>2015-10-08T16:33:05Z</updated><resolved>2015-10-08T16:04:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T16:04:43Z" id="146593698">thanks @ankon - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Several GeoPoint Cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14021</link><project id="" key="" /><description>this PR includes https://github.com/elastic/elasticsearch/pull/13632 as well as several tests and cleanups related to it.
</description><key id="110420040">14021</key><summary>Several GeoPoint Cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>review</label></labels><created>2015-10-08T10:36:43Z</created><updated>2015-10-08T10:56:04Z</updated><resolved>2015-10-08T10:56:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-08T10:39:34Z" id="146494798">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make license checks a bit less lenient.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14020</link><project id="" key="" /><description>This fixes license checks to apply to all java files under src/ as opposed to
only those in the org.elasticsearch package. It found some license headers that
had to be reformatted. I also added a missing license header to Nullability.java
however this has not be caught by the license checker since it ignores guice
files.

Relates to #13703
</description><key id="110410834">14020</key><summary>Make license checks a bit less lenient.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Packaging</label><label>build</label><label>review</label><label>v2.0.0</label></labels><created>2015-10-08T09:49:36Z</created><updated>2015-10-08T13:44:24Z</updated><resolved>2015-10-08T13:21:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-08T09:52:39Z" id="146477926">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix minor typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14019</link><project id="" key="" /><description /><key id="110410484">14019</key><summary>Fix minor typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankon</reporter><labels><label>docs</label></labels><created>2015-10-08T09:48:07Z</created><updated>2015-10-08T16:32:44Z</updated><resolved>2015-10-08T16:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-08T14:42:55Z" id="146567130">All four of your doc changes look good. I have them open and will merge them when I get a chance.

Or someone else with merge rights will if they get to them before I do.
</comment><comment author="clintongormley" created="2015-10-08T16:19:22Z" id="146599607">Done! thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added correct generic type parameter on ScriptedMetricBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14018</link><project id="" key="" /><description>Make ScriptedMetricBuilder class declaration consistent with all other Builder declarations

Closes https://github.com/elastic/elasticsearch/issues/13986
</description><key id="110405150">14018</key><summary>Added correct generic type parameter on ScriptedMetricBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kufi</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-08T09:19:24Z</created><updated>2015-10-09T09:46:08Z</updated><resolved>2015-10-09T09:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T16:02:23Z" id="146593040">@colings86 please could you review?
</comment><comment author="colings86" created="2015-10-09T09:43:28Z" id="146815552">LGTM, I'll merge this into master, 2.x, and 2.1 branches now
</comment><comment author="s1monw" created="2015-10-09T09:43:45Z" id="146815619">+1
</comment><comment author="colings86" created="2015-10-09T09:46:08Z" id="146816034">Merged. @kufi thanks for raising this issue and contributing the fix :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: change permissions/ownership of config dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14017</link><project id="" key="" /><description>When generating the rpm and dep package we now set proper group (elasticsearch) and permissions (750) to the conf dir (default /etc/elasticsearch). Same for the scripts subdirectory.

Expanded the assert_file bash function to also optionally check the group of files, so we can actually test that the group was set correctly.

Relates to #11016
</description><key id="110399821">14017</key><summary>Packaging: change permissions/ownership of config dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-08T08:44:56Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-08T13:40:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-08T08:47:04Z" id="146460887">@brwe @nik9000 can you have a look please?
</comment><comment author="nik9000" created="2015-10-08T13:02:59Z" id="146536260">LGTM

I remember now! The last time I dealt with it I wasn't a big fan of the error reporting in `assert_file` because you had to go look at the line numbers to figure out which thing failed. Oh well. Not worth fixing here.

Another point that shouldn't block merging this: `assert_file` has too many positional arguments. I wonder if we could convert it to `getopts` or `getopt`. I wonder if that'd actually make it easier to understand or harder.....  Maybe it'd be simpler to just break it into a couple of functions for the different kinds of assertions. I dunno. But 5 positional arguments in bash is really a lot!
</comment><comment author="tlrx" created="2015-10-08T13:47:58Z" id="146551376">Thanks @javanna 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Merge master election with state recovery in the case of a full cluster restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14016</link><project id="" key="" /><description>At the moment we have a two step process - we first elect a master (based on the votes of a min_master_nodes masters). Next the elected master reaches out to at least min_master_nodes master nodes and finds the best last known cluster state. The cluster state will be used as the initial state of the cluster. We can probably merge these two into one, making sure that the elected master have the best state locally (similar to how things work in RAFT, for example). We should watch out for subtleties around honoring `recover_after` settings and their implications (they are mostly meant for shard recovery, so it should, in theory, be OK). 
</description><key id="110390945">14016</key><summary>Merge master election with state recovery in the case of a full cluster restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label></labels><created>2015-10-08T07:44:04Z</created><updated>2016-02-12T15:24:27Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[GEO] Elasticsearch generates InvalidShapeException for geometry it finds invalid but GEOS does not</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14015</link><project id="" key="" /><description>Elasticsearch generates an InvalidShapeException error for a shape that passed GEOS' own validation before being indexed.

```
MapperParsingException[failed to parse [catchment_mpoly]]; nested: InvalidShapeException[Self-intersection at or near point (153.1233139135, -27.587310564, NaN)]; 
```

Could this be related to the precision parameter?
</description><key id="110381333">14015</key><summary>[GEO] Elasticsearch generates InvalidShapeException for geometry it finds invalid but GEOS does not</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">danni</reporter><labels><label>:Geo</label><label>non-issue</label></labels><created>2015-10-08T06:21:51Z</created><updated>2015-11-12T17:48:35Z</updated><resolved>2015-11-12T17:48:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2015-10-08T13:35:31Z" id="146546756">@danni same for this issue if you could, version, field mapping, and source geojson.
</comment><comment author="danni" created="2015-10-13T05:33:00Z" id="147607855">Running on 1.7.2-1.

```
GEO_SHAPE = {
    'type': 'geo_shape',
    'tree': 'quadtree',
    'precision': '15m',
}
```

Having trouble outputting the full polygon due to buffer sizes. I'll need to add a utility that outputs the full shape for me.
</comment><comment author="nknize" created="2015-11-12T17:48:35Z" id="156181974">Closing as non-issue, further discussion can continue on #12325
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[GEO] Ignore duplicate coordinates InvalidShapeException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14014</link><project id="" key="" /><description>Elasticsearch generates an InvalidShapeException from polygons generated by PostGIS:

```
MapperParsingException[failed to parse [catchment_mpoly]]; nested: InvalidShapeException[Provided shape has duplicate consecutive coordinates at: (145.105506775, -37.791859504, NaN)]
```

This particular check seems overzealous.

It's also surprising, given that the polygons are being fed through `ST_SimplifyPreserveTopology`. It is generating errors after its own precision is applied?
</description><key id="110380740">14014</key><summary>[GEO] Ignore duplicate coordinates InvalidShapeException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">danni</reporter><labels><label>:Geo</label><label>non-issue</label></labels><created>2015-10-08T06:16:46Z</created><updated>2016-03-30T20:41:14Z</updated><resolved>2015-11-12T17:47:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T13:30:09Z" id="146545512">@nknize could you take a look please?
</comment><comment author="nknize" created="2015-10-08T13:34:30Z" id="146546542">Thanks @danni can you provide the following so I can reproduce the issue: 
- ES version
- `geo_shape` field mapping
- geojson for the shape
</comment><comment author="danni" created="2015-10-13T05:30:41Z" id="147607625">Running on 1.7.2-1.

```
GEO_SHAPE = {
    'type': 'geo_shape',
    'tree': 'quadtree',
    'precision': '15m',
}
```

Having trouble outputting the full polygon due to buffer sizes.

The start of it is `mpoly: { "type": "MultiPolygon", "coordinates": [ [ [ [ 145.169139998999981, -37.862005047499991 ], [ 145.177806084499991, -37.863051944 ], [ 145.182334310999977, -37.863574976 ], [ 145.195710027999979, -37.865173764499993 ], [ 145.195406457499985, -37.865261251 ], [ 145.194888266999982, -37.865314197999986 ], [ 145.194754786499971, -37.86535698849999 ], [ 145.194700839499973, -37.865423255499991 ], [ 145.194712227499991, -37.865489966499993 ], [ 145.194749858999984, -37.865552071 ], [ 145.194801835, -37.865595139 ], [ 145.194896953999972, -37.865636745499984 ], [ 145.195023791499978, -37.865761047 ], [ 145.195104821499967, -37.865897262499992 ], [ 145.195151212999974, -37.865933226499997 ], [ 145.195197385499938, -37.865933578 ], [ 145.195271881999957, -37.865892711499988 ], [ 145.195398208499938, -37.865758993499995 ], [ 145.195451133499972, -37.865751279 ], [ 145.195508438499985, -37.865777197499995 ], [ 145.195516066999971, -37.865905439499997 ], [ 145.195455403999972, -37.866129678 ], [ 145.195399193999947, -37.866195574999985 ], [ 145.195355284499982, -37.866204547499997 ], [ 145.19527564149999, -37.866187860499991 ], [ 145.195120516499969, -37.866111233499986 ], [ 145.194927102999969, -37.866096951499998 ], [ 145.194564146999966, -37.866204806499994 ], [ 145.194258313499944, -37.866221160499997 ], [ 145.194204183999972, -37.866236275 ], [ 145.19416428949998, -37.866269685999988 ], [ 145.194178925999978, -37.866310626499995 ], [ 145.194227653499979, -37.866341947 ], [ 145.194458698499972, -37.866419998499993 ], [ 145.194729272999979, -37.866446509 ], [ 145.194793841499973, -37.866435390499994 ], [ 145.194865746499971, -37.866452447499988 ], [ 145.194930387999989, -37.866446564499988 ], [ 145.194974735499983, -37.866462641 ], [ 145.195002000999978, -37.866498401499996 ], [ 145.195005322499981, -37.866539138499995 ], [ 145.194946849499985, -37.866660739 ], [ 145.19484950399999, -37.866725229999986 ], [ 145.194675727499941, -37.866763228999986 ], [ 145.194639154499981, -37.866808646499997 ], [ 145.19461962699998, -37.866865071499994 ], [ 145.194706861999947, -37.866881555 ], [ 145.195039194499969, -37.866858023 ], [ 145.195102120499968, -37.866877577499992 ], [ 145.195122925499987, -37.866919776 ], [ 145.195104639, -37.866951189 ], [ 145.195051932999974, -37.866981954499991 ], [ 145.194570351999971, -37.867128548499984 ], [ 145.194461946999979, -37.867188451499992 ], [ 145.19441453349998, -37.867241602 ], [ 145.194291382499983, -37.867470650499989 ], [ 145.194285067999971, -37.867504413 ], [ 145.194306858499971, -37.867532440499993 ], [ 145.194357337999975, -37.867531719 ], [ 145.194617327499969, -37.867405252999987 ], [ 145.194822785999975, -37.867371583 ], [ 145.194954112999966, -37.867412394 ], [ 145.194986159999985, -37.867457737499997 ], [ 145.194984042999977, -37.867507188 ], [ 145.194755078499981, -37.867943381 ], [ 145.194681165999981, -37.868050015 ], [ 145.194619663499964, -37.868085257499992 ], [ 145.19435584149997, -37.868164178499995 ], [ 145.194079390499979, -37.868363442 ], [ 145.194014055499991, -37.868502506499993 ], [ 145.194044678999944, -37.868677609 ], [ 145.194019092499985, -37.868788054 ], [ 145.194043985499974, -37.868820225499995 ], [ 145.194076178499984, -37.868832490999985 ], [ 145.194127314999974, -37.868824073499994 ], [ 145.194172173499993, -37.868789737499995 ], [ 145.194229004, -37.868687432499996 ], [ 145.194270723499983, -37.868644013 ], [ 145.194465998499993, -37.868581779 ], [ 145.194766867999988, -37.868547517 ], [ 145.19482030399999, -37.868576488 ], [ 145.194817237999985, -37.868641996499989 ], [ 145.194741865499992, -37.868704008499989 ], [ 145.194537939999975, -37.868799413 ], [ 145.194293572499987, -37.869032513 ], [ 145.194155857999988, -37.869079854499994 ], [ 145.193937879999964, -37.869132357499993 ], [ 145.193683073499983, -37.869169079999985 ], [ 145.193633871499969, -37.869192723 ], [ 145.19364898249998, -37.869225560499991 ], [ 145.193719135499975, -37.869250683499992 ], [ 145.193806990999974, -37.869316025499991 ], [ 145.193814144999976, -37.869401958 ], [ 145.193729136499968, -37.869512236499986 ], [ 145.193560506499978, -37.869621053499991 ], [ 145.193544008499941, -37.869650579499996 ], [ 145.193601021499973, -37.869699382499995 ], [ 145.193724172499969, -37.869736475 ], [ 145.193789069499985, -37.869840426499991 ], [ 145.193809947499972, -37.869952388499989 ], [ 145.193778082999984, -37.870073933499988 ], [ 145.193673145499986, -37.870244374 ], [ 145.193367713499981, -37.870455902999986 ], [ 145.193209777999954, -37.870538468499987 ], [ 145.192953146499974, -37.870603440499991 ], [ 145.192884782, -37.870657035 ], [ 145.192859998499983, -37.87076378 ], [ 145.192867809499973, -37.870801686499995 ], [ 145.192906900999986, -37.870863532 ], [ 145.192968805, -37.870906766499992 ], [ 145.19299851599996, -37.870903177499997 ], [ 145.193167547499968, -37.870771938499985 ], [ 145.193296611499989, -37.870766888 ], [ 145.193343112499974, -37.870797006 ], [ 145.193340009999986, -37.870828603999989 ], [ 145.193297925499991, -37.870881624999988 ], [ 145.19323850349997, -37.870934035499985 ], [ 145.193121995499979, -37.871000246999984 ], [ 145.193103964499983, -37.871033343499995 ], [ 145.193113965499975, -37.871088251499991 ], [ 145.193281390999971, -37.871472996 ], [ 145.193268652499967, -37.87157593 ], [ 145.193205580499978, -37.871630005499995 ], [ 145.193082392999969, -37.871684025499995 ], [ 145.192925662, -37.871693719499994 ], [ 145.192618550999953, -37.871756564 ], [ 145.192583401499974, -37.871735881 ], [ 145.192596431999988, -37.871581146999986 ], [ 145.192577269499992, -37.871525332499992 ], [ 145.192547266499986, -37.871504649499997 ], [ 145.192403164499979, -37.87151049549999 ], [ 145.192291766499977, -37.871533565 ], [ 145.192218985499977, -37.871566809499988 ], [ 145.192171060999982, -37.871639644 ], [ 145.192150036999976, -37.871841404999984 ], [ 145.192112806999972, -37.871988073 ], [ 145.192061487999979, -37.872083569999987 ], [ 145.192070320999989, -37.872107638499998 ], [ 145.192104229499989, -37.872125528 ], [ 145.192385206499949, -37.87215054 ], [ 145.192601395999958, -37.872302351 ], [ 145.193039323, -37.872474086499984 ], [ 145.19312002449999, -37.872558909 ], [ 145.19312714199998, -37.872612041 ], [ 145.193094766499968, -37.872744094 ], [ 145.193103672499973, -37.872825124 ], [ 145.19313696049997, -37.872886248 ], [ 145.193259052999963, -37.873029104999986 ], [ 145.193275623999966, -37.873085696499984 ], [ 145.193264491499974, -37.873133075 ], [ 145.193128163999972, -37.873313265 ], [ 145.19310093499999, -37.873431442999987 ], [ 145.193127725999972, -37.873450683 ], [ 145.193182293499973, -37.873455289499987 ], [ 145.193411038999983, -37.873397995 ], [ 145.193459364999939, -37.873418326499994 ], [ 145.193476775499988, -37.873463633 ], [ 145.193430821999982, -37.873513953 ], [ 145.193274419499971, -37.873612928 ], [ 145.193055091, -37.873704558499995 ], [ 145.192820687999983, -37.873765700999989 ], [ 145.192751045999984, -37.873834595 ], [ 145.192739657999965, -37.873912036 ], [ 145.192759659999979, -37.873955843999987 ], [ 145.192803605999956, -37.873994268499992 ], [ 145.19287602199995, -37.874021056499991 ], [ 145.192973987999977, -37.874032674499993 ], [ 145.193125353499994, -37.873989791499994 ], [ 145.193259563999987, -37.873884952 ], [ 145.193334388999972, -37.873888652 ], [ 145.193388737499987, -37.873973844499993 ], [ 145.193395307499969, -37.874262019 ], [ 145.193343514, -37.874335464 ], [ 145.19329719549998, -37.874354833499993 ], [ 145.193245693999984, -37.874354426499991 ], [ 145.19310462149997, -37.874287178999985 ], [ 145.192889380999986, -37.874228293499996 ], [ 145.192613732999973, -37.874205538499993 ], [ 145.192351115499946, -37.874067121499998 ], [ 145.192193325999966, -37.874036633499991 ], [ 145.192151679499972, -37.874051341 ], [ 145.192119267499976, -37.874085473499989 ], [ 145.19208287699999, -37.874231586499995 ], [ 145.192112587999986, -37.874258929499995 ], [ 145.192266544999967, -37.874300018 ], [ 145.192368671999986, -37.874371243 ], [ 145.192414661999948, -37.874416142499996 ], [ 145.192416633, -37.87447064349999 ], [ 145.192399696999956, -37.874495655499985 ], [ 145.192283590499983, -37.874558888499998 ], [ 145.192158869999957, -37.874576556 ], [ 145.192036704499969, -37.874541961 ], [ 145.19189471949997, -37.874452883499991 ], [ 145.191852707999971, -37.874442135 ], [ 145.19181055049998, -37.874463076999987 ], [ 145.191841356499992, -37.874541258 ], [ 145.191889646, -37.874593668499998 ], [ 145.192062144999966, -37.874732381499996 ], [ 145.192245411499982, -37.874848875999987 ], [ 145.192321002999989, -37.874859717 ], [ 145.192362649499984, -37.87484948649999 ], [ 145.192465469999973, -37.87474005899999 ], [ 145.192527884999947, -37.874702855499997 ], [ 145.192583583999976, -37.874720356499992 ], [ 145.192629610499978, -37.87`
</comment><comment author="nknize" created="2015-11-03T19:37:30Z" id="153464574">@danni this check is per the [OGC SFA](http://www.opengeospatial.org/standards/sfa) and  [ISO 19107:2003](http://www.iso.org/iso/catalogue_detail.htm?csnumber=26012) standard.

Of course any software package can interpret the standard as they wish. Below is how we interpret:

Particularly:

&gt; A Curve is simple if it does not pass through the same Point twice with the possible exception of the two end points (Reference [1], section 3.12.7.3):
&gt; A Curve is closed if its start Point is equal to its end Point (Reference [1], section 3.12.7.3).
&gt; A Curve that is simple and closed is a Ring.
&gt; The boundary of a Polygon consists of a set of LinearRings that make up its exterior and interior boundaries; 

Once you have exported the offending polygon you can try "repairing" using the following package https://github.com/tudelft3d/prepair
</comment><comment author="nknize" created="2015-11-12T17:47:32Z" id="156181702">@danni, I'm going to go ahead and close this and #14015 as a non issue. For the time being issue #12325 will remain open for further discussion and as a reference for cleaning invalid data.
</comment><comment author="smithsimonj" created="2016-03-30T12:10:17Z" id="203400278">I get this issue with the attached shape for [Antarctica](https://github.com/elastic/elasticsearch/files/195290/Antarctica.txt) in Elasticsearch 2.2.1. The error is:

```
Provided shape has duplicate consecutive coordinates at: (180.0, -90.0, NaN)
```

My mapping bit looks like:

```
"location": {"type": "geo_shape", "precision": "1600m"}
```

The shape I am trying to load is in the attached JSON record (specifically the "geometry" field from within it): [Antarctica.txt](https://github.com/elastic/elasticsearch/files/195435/Antarctica.txt)

I agree with the analysis by @danni that this is a artifact of the shape being simplified before being indexed - the 'duplicate consecutive coordinates' being referred to in the error quite simply don't appear even once in the original data. 
</comment><comment author="smithsimonj" created="2016-03-30T12:14:16Z" id="203402060">Do you want me to raise this as a new issue @nknize?
</comment><comment author="clintongormley" created="2016-03-30T20:41:14Z" id="203626284">@smithsimonj best to open a new issue, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0.0-beta2 bin/plugin --help has cloud-aws as official plugin but not available + fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14013</link><project id="" key="" /><description>bin/plugin install cloud-aws fails to install with the following error...

Trying https://download.elastic.co/elasticsearch/release/org/elasticsearch/plugin/cloud-aws/2.0.0-beta2/cloud-aws-2.0.0-beta2.zip ...
ERROR: failed to download out of all possible locations

I managed to find the file here...

https://oss.sonatype.org/content/repositories/releases/org/elasticsearch/plugin/cloud-aws/2.0.0-beta2/

Installing from this location seems to work ok however when I start elasticsearch I receive the following error...

[2015-10-08 15:28:11,674][WARN ][com.amazonaws.jmx.SdkMBeanRegistrySupport]
java.security.AccessControlException: access denied ("javax.management.MBeanServerPermission" "findMBeanServer")

A couple of questions...

I thought this plugin was for &lt; v2. Should I be using it?
Is this potentially an issue caused by tighter v2 java permissions?

Cheers,

Marty
</description><key id="110377301">14013</key><summary>2.0.0-beta2 bin/plugin --help has cloud-aws as official plugin but not available + fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels /><created>2015-10-08T05:37:36Z</created><updated>2015-10-16T10:03:26Z</updated><resolved>2015-10-08T13:29:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T13:29:03Z" id="146545240">I think you're using the wrong install command, and trying to install the old version.  This works:

```
./bin/plugin install cloud-aws
```
</comment><comment author="Cidan" created="2015-10-13T21:01:17Z" id="147852458">I can confirm this happens to me as well with RC1 -- cloud-aws gives me the same error (while installing properly with plugin install)

Ideas?
</comment><comment author="rmuir" created="2015-10-13T21:24:29Z" id="147858315">&gt; Installing from this location seems to work ok however when I start elasticsearch I receive the following error...
&gt; 
&gt; [2015-10-08 15:28:11,674][WARN ][com.amazonaws.jmx.SdkMBeanRegistrySupport]
&gt; java.security.AccessControlException: access denied ("javax.management.MBeanServerPermission" "findMBeanServer")

That isn't an error, your logging configuration must be incorrect or outdated. Do you have a custom file?

https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/config/logging.yml#L13-L15
</comment><comment author="robertsmarty" created="2015-10-16T10:03:26Z" id="148673351">This works fine for me with rc1. Got a feeling it was not available for beta2.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unknown reason for ElasticsearchException Missing value for field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14012</link><project id="" key="" /><description>I added a geo_distance filter to a query received an error:

ElasticsearchException[Missing value for field [reviews_length]]

I really thought all of my documents had the reviews_length field so I checked, and:

```
curl -XGET http://localhost:9200/vendop/vendor/_search -d '{
  "query": {
    "filtered": {
      "filter": {
        "missing": {
          "field": "reviews_length"
        }
      }
    }
  }
}'
```

produces:

```
{
"took": 1,
"timed_out": false,
"_shards": {
"total": 5,
"successful": 5,
"failed": 0
},
"hits": {
"total": 0,
"max_score": null,
"hits": [ ]
}
}
```

The complete query is:

```
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "geo_distance": {
                "distance": "50mi",
                "location2.coordinates": [
                  -122.4194155,
                  37.7749295
                ]
              }
            }
          ]
        }
      },
      "query": {
        "bool": {
          "minimum_should_match": "60%",
          "should": [
            {
              "function_score": {
                "query": {
                  "match": {
                    "name.lower_raw": "molding"
                  }
                },
                "functions": [
                  {
                    "boost_factor": 20
                  }
                ]
              }
            },
            {
              "function_score": {
                "query": {
                  "match": {
                    "name": {
                      "query": "molding",
                      "fuzziness": "auto"
                    }
                  }
                },
                "functions": [
                  {
                    "boost_factor": 10
                  }
                ]
              }
            },
            {
              "multi_match": {
                "query": "molding",
                "fields": [
                  "_all"
                ]
              }
            },
            {
              "function_score": {
                "functions": [
                  {
                    "field_value_factor": {
                      "field": "reviews_length",
                      "factor": 0.01
                    }
                  }
                ]
              }
            },
            {
              "function_score": {
                "query": {
                  "terms": {
                    "services": [
                      "523e900941233f628f8eee71",
                      "523e900941233f628f8eee72",
                      "54359cfeae304c2f77d6adb6",
                      "523e900941233f628f8eee70",
                      "53dfabeb2a2dfa23de430fcf",
                      "53dfabde2a2dfa23de430fce"
                    ]
                  }
                },
                "functions": [
                  {
                    "boost_factor": 25
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  "size": 20
}
```

If I add this range filter to query.filtered.filter.bool.must, I still get the error but only on one of the shards (so there are search results).

```
            {
              "range": {
                "reviews_length": {
                  "gte": 0
                }
              }
            }
```

the field is defined in the mapping as:

```
        "reviews_length": {
          "type": "long"
        }
```

I tried putting together a full test case but I couldn't reproduce the problem with a very abbreviated data set. I have tried rebuilding my index but still run into this issue. What is interesting is that if I remove the minimum_should_match from the query (or change it to &lt;40% the problem goes away.

I'm sure there is just something I'm missing in the docs. I'm using 1.4.2 in prod so I can't use the 'missing' parameter to 'field_value_factor' yet -- and shouldn't have to since none of my documents have the reviews_length field missing.

If it helps, here is the debug log from ES:

```
[2015-10-07 18:11:26,126][DEBUG][action.search.type       ] [White Rabbit] All shards failed for phase: [query]
org.elasticsearch.search.query.QueryPhaseExecutionException: [vendop_v1][0]: query[filtered(filtered((function score (name.lower_raw:molding,function=boost[20.0]) function score (name:molding~2,function=boost[10.0]) function score (child_filter[review/vendor](filtered(_all:molding)-&gt;cache(_type:review)),function=boost[4.0]) _all:molding function score (ConstantScore(*:*),function=org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction@6e1aca5f) function score (services:523e900941233f628f8eee71 services:523e900941233f628f8eee72 services:54359cfeae304c2f77d6adb6 services:523e900941233f628f8eee70 services:53dfabeb2a2dfa23de430fcf services:547ba915ae304c5e979f40f0 services:5227a9b0c64c8c1739698315 services:5328a646c61e3610d056ca90 services:52450165a4283d7aaa000000 services:533afb1fc61e3662d1cecdc2 services:523e900941233f628f8eee6f services:53dfabcd2a2dfa23de430fcd services:523e900941233f628f8eee74 services:547a7a6d2a2dfa2aaac69fb3 services:523e900941233f628f8eee75 services:53f39ac2ae304c77eaa2289e services:53dfabde2a2dfa23de430fce,function=boost[25.0]))~3)-&gt;BooleanFilter(+GeoDistanceFilter(location2.coordinates, SLOPPY_ARC, 80467.2, 37.7749295, -122.4194155)))-&gt;cache(+_type:vendor +org.elasticsearch.index.search.nested.NonNestedDocsFilter@8b805655)],from[0],size[20]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:289)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:300)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchException: Missing value for field [reviews_length]
    at org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction.score(FieldValueFactorFunction.java:71)
    at org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$FunctionFactorScorer.innerScore(FunctionScoreQuery.java:170)
    at org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer$AnyNextDoc.score(CustomBoostFactorScorer.java:133)
    at org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer.score(CustomBoostFactorScorer.java:71)
    at org.apache.lucene.search.MinShouldMatchSumScorer.evaluateSmallestDocInHeap(MinShouldMatchSumScorer.java:170)
    at org.apache.lucene.search.MinShouldMatchSumScorer.nextDoc(MinShouldMatchSumScorer.java:142)
    at org.apache.lucene.search.FilteredQuery$QueryFirstScorer.nextDoc(FilteredQuery.java:176)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
    ... 8 more
```
</description><key id="110353085">14012</key><summary>unknown reason for ElasticsearchException Missing value for field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dblado</reporter><labels><label>:Search</label><label>bug</label><label>feedback_needed</label></labels><created>2015-10-08T01:14:41Z</created><updated>2016-04-11T14:14:53Z</updated><resolved>2015-10-13T09:37:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T13:16:09Z" id="146541696">Hi @dblado 

So much has changed since then.  Would it be possible to copy the index into a node with 1.7.2 and try the query there, to see if the problem still exists?
</comment><comment author="dblado" created="2015-10-08T19:02:27Z" id="146655616">@clintongormley sure thing. I am using 1.6.0 locally just haven't needed to upgrade prod yet.

Loading my full dataset (~5.5mln documents) into 1.7.2 and running the same query shows 4/5 shards failing.

response error:

```
"failures": [
{
"index": "vendop_v1",
"shard": 0,
"status": 500,
"reason": "QueryPhaseExecutionException[[vendop_v1][0]: query[filtered(filtered((function score (name.lower_raw:molding,function=boost[20.0]) function score (name:molding~2,function=boost[10.0]) _all:molding function score (ConstantScore(*:*),function=org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction@6267d318) function score (services:523e900941233f628f8eee71 services:523e900941233f628f8eee72 services:54359cfeae304c2f77d6adb6 services:523e900941233f628f8eee70 services:53dfabeb2a2dfa23de430fcf services:547ba915ae304c5e979f40f0 services:5227a9b0c64c8c1739698315 services:5328a646c61e3610d056ca90 services:52450165a4283d7aaa000000 services:533afb1fc61e3662d1cecdc2 services:523e900941233f628f8eee6f services:53dfabcd2a2dfa23de430fcd services:523e900941233f628f8eee74 services:547a7a6d2a2dfa2aaac69fb3 services:523e900941233f628f8eee75 services:53f39ac2ae304c77eaa2289e services:53dfabde2a2dfa23de430fce,function=boost[25.0]))~3)-&gt;BooleanFilter(+GeoDistanceFilter(location2.coordinates, SLOPPY_ARC, 80467.2, 37.7749295, -122.4194155)))-&gt;cache(+_type:vendor +org.elasticsearch.index.search.nested.NonNestedDocsFilter@20b3a141)],from[0],size[20]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[Missing value for field [reviews_length]]; "
}
```

log error:

```
[2015-10-08 11:59:40,681][DEBUG][action.search.type       ] [Battletide] [vendop_v1][0], node[6e1xCs4yTU-uzvFvZeWp_A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@67a2443a] lastShard [true]
org.elasticsearch.search.query.QueryPhaseExecutionException: [vendop_v1][0]: query[filtered(filtered((function score (name.lower_raw:molding,function=boost[20.0]) function score (name:molding~2,function=boost[10.0]) _all:molding function score (ConstantScore(*:*),function=org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction@6267d318) function score (services:523e900941233f628f8eee71 services:523e900941233f628f8eee72 services:54359cfeae304c2f77d6adb6 services:523e900941233f628f8eee70 services:53dfabeb2a2dfa23de430fcf services:547ba915ae304c5e979f40f0 services:5227a9b0c64c8c1739698315 services:5328a646c61e3610d056ca90 services:52450165a4283d7aaa000000 services:533afb1fc61e3662d1cecdc2 services:523e900941233f628f8eee6f services:53dfabcd2a2dfa23de430fcd services:523e900941233f628f8eee74 services:547a7a6d2a2dfa2aaac69fb3 services:523e900941233f628f8eee75 services:53f39ac2ae304c77eaa2289e services:53dfabde2a2dfa23de430fce,function=boost[25.0]))~3)-&gt;BooleanFilter(+GeoDistanceFilter(location2.coordinates, SLOPPY_ARC, 80467.2, 37.7749295, -122.4194155)))-&gt;cache(+_type:vendor +org.elasticsearch.index.search.nested.NonNestedDocsFilter@20b3a141)],from[0],size[20]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:301)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:312)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchException: Missing value for field [reviews_length]
    at org.elasticsearch.common.lucene.search.function.FieldValueFactorFunction.score(FieldValueFactorFunction.java:71)
    at org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$FunctionFactorScorer.innerScore(FunctionScoreQuery.java:170)
    at org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer$AnyNextDoc.score(CustomBoostFactorScorer.java:133)
    at org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer.score(CustomBoostFactorScorer.java:71)
    at org.apache.lucene.search.MinShouldMatchSumScorer.evaluateSmallestDocInHeap(MinShouldMatchSumScorer.java:170)
    at org.apache.lucene.search.MinShouldMatchSumScorer.nextDoc(MinShouldMatchSumScorer.java:142)
    at org.apache.lucene.search.FilteredQuery$QueryFirstScorer.nextDoc(FilteredQuery.java:176)
    at org.apache.lucene.search.FilteredQuery$QueryFirstScorer.advance(FilteredQuery.java:187)
    at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.advanceToNextCommonDoc(FilteredQuery.java:274)
    at org.apache.lucene.search.FilteredQuery$LeapFrogScorer.nextDoc(FilteredQuery.java:286)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:192)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
    ... 8 more
```
</comment><comment author="dblado" created="2015-10-08T19:22:04Z" id="146661709">@clintongormley FYI, on 1.7.2, If I add the range filter, there are no errors and hits.total = 33 (expected):

```
            {
              "range": {
                "reviews_length": {
                  "gte": 0
                }
              }
            }
```

On 1.6.0, adding the range filter shows an error on one shard and hits.total = 26
</comment><comment author="jimmytheleaf" created="2015-10-08T22:50:45Z" id="146709964">On 1.7.1, I have a similar error with a query involving function scores and a `geo_bounding_box` filter. I have the same behavior where I get an error that says the field was missing, even though I confirm with a query + "missing" filter that no documents in this doctype have this field missing. (I have documents across other doctypes with this field not present.)

What is strange, is if I removed the `geo_bounding_box` filter, I get results back fine and no longer see the error. Behavior persists after I cleared the cache.

Error root cause was a similar: `Query Failed [Failed to execute main query]]; nested: ElasticsearchException[Missing value for field [a4]];`

@dblado -- kind of curious, if you don't have that range filter, and you remove the `geo_distance` filter, do you still get the error?
</comment><comment author="dblado" created="2015-10-08T23:07:44Z" id="146712401">Hi @jimmytheleaf I do still get the error but not on all shards. I didn't notice the error because some shards succeeded and returned results.

If I remove my minimum_should_match or the field_value_factor should clause, all shards are successful. Playing w/ minimum_shoud_match, values of 1 and 3 do not error, value of 2 does.
</comment><comment author="jimmytheleaf" created="2015-10-09T18:13:36Z" id="146951617">Another datapoint, and suggestion for me that there is something odd going on with the way filters are being applied -- I'm doing a query that involves a `geo_bounding_box` filter and a set of `field_value_factor` functions on an index + doctype. I.E. `POST /&lt;index-name&gt;/&lt;doctype&gt;/_search`.

If I add a term filter for the doctype, i.e.:

``` javascript
{ "term" : { "_type": &lt;doctype&gt;}}
```

I no longer get the spurious `ElasticsearchException[Missing value for field [&lt;fieldname&gt;]];` Issue.

Kinda suggests for me that something about the geo filter is causing my query to funnel documents from other doctypes (where that field is missing) into the function scoring, and causing it to barf when the field I'm scoring on doesn't exist?

My query that triggers the error looks like:

``` javascript
// POST /index-name/doc-type/_search
{
   "query": {
      "filtered": {
         "filter": {
            "bool": {
               "must": [
                  {
                     "nested": {
                        "filter": {
                           "geo_bounding_box": {
                              "_cache": false,
                              "type": "indexed",
                              "point": {
                                 "bottom_right": {
                                    "lat": 36.00621805925558,
                                    "lon": -120.33803214814795
                                 },
                                 "top_left": {
                                    "lat": 38.90084254074442,
                                    "lon": -124.98458605185203
                                 }
                              }
                           }
                        },
                        "path": "foo"
                     }
                  }
               ]
            }
         },
         "query": {
            "nested": {
               "path": "bar",
               "score_mode": "max",
               "inner_hits": {
                  "size": 1
               },
               "query": {
                  "function_score": {
                     "query": {
                        "filtered": {}
                     },
                     "functions": [
                        {
                           "field_value_factor": {
                              "field": "a1",
                              "factor": 0.004342221095126581
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a2",
                              "factor": 0.2212504591677979
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a3",
                              "factor": 0.723225250366149
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a4",
                              "factor": 0.051182069370926446
                           }
                        }
                     ],
                     "score_mode": "sum"
                  }
               }
            }
         }
      }
   }
}
```

Error disappears with either:

``` javascript
// POST /index-name/doc-type/_search
{
   "query": {
      "filtered": {
         "filter": {
            "bool": {
               "must": []
            }
         },
         "query": {
            "nested": {
               "path": "bar",
               "score_mode": "max",
               "inner_hits": {
                  "size": 1
               },
               "query": {
                  "function_score": {
                     "query": {
                        "filtered": {}
                     },
                     "functions": [
                        {
                           "field_value_factor": {
                              "field": "a1",
                              "factor": 0.004342221095126581
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a2",
                              "factor": 0.2212504591677979
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a3",
                              "factor": 0.723225250366149
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a4",
                              "factor": 0.051182069370926446
                           }
                        }
                     ],
                     "score_mode": "sum"
                  }
               }
            }
         }
      }
   }
}
```

or:

``` javascript
// POST /index-name/doc-type/_search
{
   "query": {
      "filtered": {
         "filter": {
            "bool": {
               "must": [
                  { "term": {"_type": "doc-type"} },
                  {
                     "nested": {
                        "filter": {
                           "geo_bounding_box": {
                              "_cache": false,
                              "type": "indexed",
                              "point": {
                                 "bottom_right": {
                                    "lat": 36.00621805925558,
                                    "lon": -120.33803214814795
                                 },
                                 "top_left": {
                                    "lat": 38.90084254074442,
                                    "lon": -124.98458605185203
                                 }
                              }
                           }
                        },
                        "path": "foo"
                     }
                  }
               ]
            }
         },
         "query": {
            "nested": {
               "path": "bar",
               "score_mode": "max",
               "inner_hits": {
                  "size": 1
               },
               "query": {
                  "function_score": {
                     "query": {
                        "filtered": {}
                     },
                     "functions": [
                        {
                           "field_value_factor": {
                              "field": "a1",
                              "factor": 0.004342221095126581
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a2",
                              "factor": 0.2212504591677979
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a3",
                              "factor": 0.723225250366149
                           }
                        },
                        {
                           "field_value_factor": {
                              "field": "a4",
                              "factor": 0.051182069370926446
                           }
                        }
                     ],
                     "score_mode": "sum"
                  }
               }
            }
         }
      }
   }
}
```
</comment><comment author="dblado" created="2015-10-09T21:03:57Z" id="146985622">interesting -- if I update my failed query to include:

```
          "must": [
            {
              "term": {
                "_type": "vendor"
              }
            },
            {
              "geo_distance": {
                "distance": "50mi",
                "location2.coordinates": [
                  -122.4194155,
                  37.7749295
                ]
              }
            }
          ]
```

I still get an error but only on 1 shard instead of all 5.
</comment><comment author="dblado" created="2015-10-10T00:43:48Z" id="147017766">Some more experimenting...

I changed my mapping and rebuilt the index. My reviews_length field is now:

```
        "reviews_length": {
          "type": "short",
        }
```

as long as I add reviews_length range filter:

```
            {
              "range": {
                "reviews_length": {
                  "gte": 0
                }
              }
            }
```

all 5 shards succeed! Without the range filter, 4/5 shards fail with missing value for field.

I built a second index w/ the mapping set as:

```
        "reviews_length": {
          "type": "short",
          "null_value": 0
        }
```

Again, as long as I have the reviews_length range filter, the query succeeds.  Interesting that without the range filter on reviews_length, all shards fail w/ null_value set in the mapping. I find this odd since the field should always exist since there is a null_value specified in the mapping. Did I misunderstand the null_value setting?
</comment><comment author="jpountz" created="2015-10-13T09:37:07Z" id="147663798">These errors are due to the fact that `MinShouldMatchScorer` sometimes evaluates the score on sub scorers even if there might not be enough matching clauses for the boolean query (which has min_should_match set) to match. This scorer was changed in Lucene 5.1 to compute scores lazily (https://issues.apache.org/jira/browse/LUCENE-6201) so it should be fixed in Elasticsearch 2.0, which will be based on Lucene 5.2.1. The only work-around that I can think of for 1.x would be to filter the query wrapped by the `function_score` query that has the `field_value_factor` function so that it cannot match a document that doesn't have a value for `reviews_length`.

Having reindexed with the `null_value` set on the `reviews_length` field should have worked. If it didn't, I guess it is because you have other types on the same index that either don't have the `reviews_length` field or have it but don't have a `null_value` configured.
</comment><comment author="dblado" created="2015-10-13T23:51:08Z" id="147886644">@jpountz thanks for the feedback. I find it odd that the same query works when the reviews_length field is set to a different data type (short vs long).

I do have other types in the same index, but the search is to /index/type/_search...doesn't that mean it shouldn't matter if other types don't have that field defined?
</comment><comment author="clintongormley" created="2015-10-14T08:28:39Z" id="147972886">&gt; I do have other types in the same index, but the search is to /index/type/_search...doesn't that mean it shouldn't matter if other types don't have that field defined?

it's a whole lot more complex than that.  lucene has no concept of types, it just sees a list of fields (which may or may not be in your type).  queries and filters do not necessarily operate on just the subset of documents that other query/filter clauses have included, but may well hit unrelated documents.

This all gets a whole lot neater and cleaner in 2.0
</comment><comment author="EmilAlipiev" created="2016-04-11T12:59:46Z" id="208328651">I am using 2.0.2 version and I have exactly similar query and same error. I went to through my integer field which OrderCount as below. and all have value 0 or greater than 0. How can I solve this problem?

```
{
   "query": {
      "function_score": {
         "query": {
            "bool": {
               "should": [
                  {
                     "multi_match": {
                        "type": "best_fields",
                        "query": "tinte",
                        "fields": [                          
                           "Name^7",
                           "ShortDescription^6"
                        ]
                     }
                  }
               ]
            }
         },
         "field_value_factor": {
            "field": "OrderCount"
         }
      }
   }
}
```

In my Mapping I even added.

```
},
"OrderCount": {
"null_value": 0,
"index": "no",
"store": true,
"type": "integer"
},
```

I tried also "long" but result didnt change for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allocation not complete when only 1 of 2 awareness attributes is set to forced</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14011</link><project id="" key="" /><description>This test uses a similar setup as 14010 (also using ES 1.7.2), the only difference is that for this use case, only 1 of the attributes (updateDomain) is set to be forced, the other attribute (faultDomain) is not set to be forced.

With this configuration, you will see from the video that the first issue I ran into is 14010 where I have to prod it by forcing a cluster reroute in order for it to complete its allocation.  Except that even after prodding, one of the shard is not at the right place.  If I use a manual reroute command to move it, it actually gets moved successfully and does not violate any routing rules.  Given that it does not violate any routing rules, it seems like there is another bug here where it is not moving the shard to the right place automatically as part of awareness.

Repro video:
https://drive.google.com/file/d/0B1rxJ0dAZbQvd3p5Y3F0QzItMWs/view?usp=sharing

Node setup:
https://docs.google.com/document/d/1ktYSzaZh_FlhloSextzCNh3HYYS0OEE8wPU4jhHGPTo/edit?usp=sharing
</description><key id="110347793">14011</key><summary>Allocation not complete when only 1 of 2 awareness attributes is set to forced</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>adoptme</label><label>bug</label><label>discuss</label></labels><created>2015-10-08T00:24:45Z</created><updated>2015-12-14T21:04:25Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T13:11:16Z" id="146539434">Also see #14010
</comment><comment author="ywelsch" created="2015-12-01T18:19:15Z" id="161053092">The issue here is that the AwarenessAllocationDecider uses the total number of shards (which is 1 primary + 2 replicas = 3) as basis to calculate balancedness in faultDomain. However, one of the replicas cannot be allocated due to forced awareness in updateDomain. But this unassigned replica is still used as basis to calculate balancedness in faultDomain.

I'm not sure what we could change here. Any thoughts @bleskes @dakrone?
</comment><comment author="bleskes" created="2015-12-14T21:04:25Z" id="164559202">agreed this will be hard to solve. Basically we need to capture the idea that the third copy can not be assigned under the current setup and this shouldn't be taken into account when balancing on the allocation awareness attribute. My only idea here is to separate allocation deciders into "hard" and "soft" group. The hard one should be run first and should mark any shard copy that is "not assignable" as such. The soft group can the ignore those shards when making decisions. It will require more thoughts and review of current deciders to see how they fit. 

Note: #8178 is another example the result of mixing allocation awareness with a hard rule.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delayed allocation causing partial allocation of shards on allocation awareness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14010</link><project id="" key="" /><description>It is difficult to write out a full repro in words, so I recorded a video of the repro which will help.

The test uses the latest 1.7.2 release.

In short, 6 nodes in cluster, 1 index with 4 shards and 2 replicas (3 copies).  
Each node has 2 awareness attributes (updateDomain and faultDomain) set (both forced).  3 nodes are in 1 updateDomain, the other 3 are in the other updateDomain.  And these nodes are also in different faultDomains.  Test has delayed allocation set to 10s for quicker allocation.  

When an updateDomain is killed (3 nodes gone), the cluster shows partial allocation of shards - until a manual _cluster/reroute command is run (without post body) to prod it, or if a command is issued that updates the cluster state (eg. create an index).  Once a manual reroute (that doesn't change anything) is run or the cluster state is updated, then the remaining shards are immediately allocated successfully based on the awareness settings.

If delayed allocation is turned off entirely, then everything works fine and there is no need to manually prod it to complete the rest of the allocation.

Note that sometimes, with delayed allocation on, it does do the right thing, but if you retest a few times stopping and restarting the 3 nodes, you will see that it doesn't do so consistently.

Repro video:
https://drive.google.com/file/d/0B1rxJ0dAZbQvRUE0SlVxT2pOZFE/view?usp=sharing

Node setup:
https://docs.google.com/document/d/1J5FPSvIA5U41Ou1BNpEN9P7q2L8e7KMxM69IG4dGMkk/edit?usp=sharing
</description><key id="110342311">14010</key><summary>Delayed allocation causing partial allocation of shards on allocation awareness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>adoptme</label><label>bug</label></labels><created>2015-10-07T23:45:35Z</created><updated>2015-11-12T16:58:27Z</updated><resolved>2015-11-12T16:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T13:11:29Z" id="146539529">Also see #14011
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restrict document ID values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14009</link><project id="" key="" /><description>It seems that there are some chars that are not ideal in document IDs, things such as `/`, probably others.
This is being discussed on a different level in #9059 and I thought I'd also raise this specific one.

From [this](https://discuss.elastic.co/t/getting-document-by-document-id/31806) forum post.
</description><key id="110332133">14009</key><summary>Restrict document ID values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:CRUD</label><label>discuss</label></labels><created>2015-10-07T22:35:51Z</created><updated>2016-07-05T14:57:38Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-07T22:56:02Z" id="146356340">+1
</comment><comment author="clintongormley" created="2015-10-08T13:03:02Z" id="146536271">Actually, I think doc ID values can be pretty much unlimited.  As pointed out in the forum post, they just need to be properly encoded in the URL, which all the official clients handle correctly.
</comment><comment author="jpountz" created="2015-10-08T14:03:25Z" id="146555743">I don't think encoding is enough. For instance we allow documents to have `_search` as an id but then you can't GET them.
</comment><comment author="clintongormley" created="2016-01-29T09:09:48Z" id="176653619">You can always fall back to using mget in these cases.

Related to https://github.com/elastic/elasticsearch/issues/9059
</comment><comment author="dr0i" created="2016-07-05T14:56:25Z" id="230502857">&gt; Actually, I think doc ID values can be pretty much unlimited. As pointed out in the forum post, they just need to be properly encoded in the URL 

Right! If it happen that a value of an `_id` field is already percent encoded (e.g. `a%2Fb`) the lookup-query would be  `.../a%252Fb`).
Doing an ID lookup, maybe it would be save  to say: 
"_always_  percent-encode the ID in an ID-query (take the value of `_id` , percent-encode it and query that) ".
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document ES_CLASSPATH changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14008</link><project id="" key="" /><description>In 2.0.0 we are changing the way we handle this and it needs to be adequately documented in breaking changes.

@rjernst mentioned the reasoning here is that if it wasn't specified our shell handling would still add it to the classpath and Java would then see an empty string on the classpath and just add $cwd.
It was decided that it just wasn't worth this because, moving forward plugins should be how things are added to ES. If someone needs to add something to the classpath, they can add it to the ES lib dir manually.
</description><key id="110329702">14008</key><summary>Document ES_CLASSPATH changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-10-07T22:18:13Z</created><updated>2015-10-08T16:23:37Z</updated><resolved>2015-10-08T16:23:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-07T22:47:29Z" id="146354316">FYI, we have a check already in the code so the user gets a "nice message" if they try starting ES with `ES_CLASSAPTH`.
</comment><comment author="clintongormley" created="2015-10-08T12:59:31Z" id="146534359">@rjernst Does #14026 correctly reflect the changes to ES_CLASSPATH?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix ensureNodesAreAvailable's error message</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14007</link><project id="" key="" /><description>listedNodes are the "configured nodes" and not the empty list of nodes that is passed to the method and causes this exception to be thrown

Closes #13957
</description><key id="110328214">14007</key><summary>Fix ensureNodesAreAvailable's error message</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>:Exceptions</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-07T22:07:36Z</created><updated>2015-10-19T11:04:33Z</updated><resolved>2015-10-19T08:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T12:31:27Z" id="146523502">@javanna could you take a look please?
</comment><comment author="javanna" created="2015-10-19T08:42:17Z" id="149147193">thanks @synhershko 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write JVM crash logs to LOG_DIR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14006</link><project id="" key="" /><description>Closes #13982
</description><key id="110306465">14006</key><summary>Write JVM crash logs to LOG_DIR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Logging</label><label>:Packaging</label><label>enhancement</label><label>review</label></labels><created>2015-10-07T20:10:42Z</created><updated>2015-11-29T11:52:02Z</updated><resolved>2015-11-29T01:20:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T20:11:54Z" id="146314641">This PR only covers linux for now. I'll grab windows too if in a bit unless someone has easier access to windows for testing.
</comment><comment author="nik9000" created="2015-10-07T20:12:34Z" id="146314789">Whoops - tests failing with this. I'll fix that.
</comment><comment author="rmuir" created="2015-10-07T22:40:12Z" id="146352784">Some of these things can be dynamically changed thru java code... Jmx i think. Could make things simpler.
</comment><comment author="nik9000" created="2015-10-07T22:52:17Z" id="146355267">Oh!  I'm going to try that in the morning!
On Oct 7, 2015 6:40 PM, "Robert Muir" notifications@github.com wrote:

&gt; Some of these things can be dynamically changed thru java code... Jmx i
&gt; think. Could make things simpler.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/14006#issuecomment-146352784
&gt; .
</comment><comment author="nik9000" created="2015-10-08T14:48:19Z" id="146568681">&gt; Oh!  I'm going to try that in the morning!

I don't _think_ that'll work: `VM Option "ErrorFile" is not writeable`.

I got that by using VisualVM and trying to call `com.sun.management.HotSpotDiagnostic#setVMOption("ErrorFile", "cat")`. You can certainly read the option set on the command line from  `com.sun.management.HotSpotDiagnostic#getVMOption("ErrorFile")`.
</comment><comment author="nik9000" created="2015-11-29T01:20:50Z" id="160353662">Closing as this probably isn't the right way to go about it. If we want it we can revive it later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix small typo in the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14005</link><project id="" key="" /><description /><key id="110306322">14005</key><summary>Fix small typo in the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darkoc</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-07T20:09:46Z</created><updated>2016-03-10T12:22:22Z</updated><resolved>2016-03-10T12:22:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T20:17:00Z" id="146315764">Looks good to me.

@darkoc, can you sign elastic's [cla][https://www.elastic.co/contributor-agreement]? Its needed to merge all changes.
</comment><comment author="clintongormley" created="2016-03-10T12:22:22Z" id="194817808">CLA not signed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing REST spec for `detect_noop`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14004</link><project id="" key="" /><description>Hi,
Looks like there is missing spec for [detect_noop](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html?q=update%20a#_literal_detect_noop_literal) parameter.

Reported https://github.com/elastic/elasticsearch-net/issues/1587
</description><key id="110305411">14004</key><summary>Add missing REST spec for `detect_noop`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertlyson</reporter><labels><label>:REST</label><label>bug</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-07T20:04:10Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-07T20:08:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T20:08:06Z" id="146313757">Looks good! Thanks @robertlyson.

@clintongormley  - I'm not sure I'm labeling this one correctly. Can you correct it if I've made a mistake?
</comment><comment author="nik9000" created="2015-10-07T20:14:01Z" id="146315097">Merged to 2.0 and cherry-picked to 2.1, 2.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: Enforce metadata fields are not passed in documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14003</link><project id="" key="" /><description>We previously removed the ability to specify metadata fields inside
documents in #11074, but the backcompat left leniency that allowed this
to still occur. This change locks down parsing so any metadata field
found while parsing a document results in an exception. This only
affects 2.0+ indexes; backcompat is maintained.

closes #13740
closes #3517
</description><key id="110303791">14003</key><summary>Mappings: Enforce metadata fields are not passed in documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0</label></labels><created>2015-10-07T19:55:16Z</created><updated>2016-03-10T18:15:04Z</updated><resolved>2015-10-07T21:40:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-07T21:37:12Z" id="146340268">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lets talk about switching bin/elasticsearch to bash</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14002</link><project id="" key="" /><description>Right now `bin/elasticsearch` uses `sh`. Would it offend anyone terribly if `bin/elasticsearch` ran in `bash` instead? It'd buy nicer syntax for defaults and sweet, sweet arrays. Arrays would make it much easier to dynamically build command line arguments and not have issues with spaces.
</description><key id="110303388">14002</key><summary>Lets talk about switching bin/elasticsearch to bash</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-10-07T19:52:55Z</created><updated>2016-05-10T19:06:52Z</updated><resolved>2016-05-10T19:06:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T19:53:15Z" id="146309554">@spinscale @tlrx have opinions? @brwe? I'm not sure who else to ask....
</comment><comment author="KevinCathcart" created="2015-10-07T21:21:53Z" id="146333431">Question from a drive-by commenter:
What about the Solaris/SmartOS? Do they install bash by default these days?  I know that Solaris did not historically install it by default, but I've not worked with even remotely recent versions, so that may have changed.
</comment><comment author="markwalkom" created="2015-10-07T23:24:24Z" id="146364081">I'm a +1 on this.

@KevinCathcart  "technically" we don't support Solaris/SmartOS, but it's probably worth keeping it in mind none the less.
</comment><comment author="KevinCathcart" created="2015-10-08T04:34:33Z" id="146417874">@markwalkom: Right. Misread the support matrix because my job's websense
filter blocks portions of the site, so not everything will display right.
</comment><comment author="brwe" created="2015-10-08T07:10:21Z" id="146440105">+1 from me too but only speaking as a developer. I don't know the reason why it is `sh` now but assume it is because of what @KevinCathcart said.
</comment><comment author="spinscale" created="2015-10-08T07:22:58Z" id="146441943">@nik9000 are arrays a bash 4 feature? If so, do all the OS's have bash4 installed by default?

I also agree that the support matrix should not be the only systems ES runs on, as long as we dont die from maintenance hell. The argument for easily parsing spaces is intriguing though... we still do fail in the plugin manager here, right?
</comment><comment author="tlrx" created="2015-10-08T07:29:19Z" id="146442890">I think using `sh` + POSIX compliant scripts help a lot when it comes to install elasticsearch on various operating systems where bash is not installed by default (I have Solaris and FreeBSD in mind).

Since these scripts do not contain a lot of code and logic, I think we can stick to `sh` if it helps non-bash users to use elasticsearch.
</comment><comment author="clintongormley" created="2015-10-09T09:57:52Z" id="146818461">Discussed in FixItFriday: there are a number of users on Solaris/SmartOs, who would need to install bash before using Elasticsearch.  Given that we've shifted command line parsing to Java, do we need the nicer features in bash still?

Consensus in the discussion was to keep on with `sh`, but I'll leave this issue open for a while to garner more opinions.
</comment><comment author="jasontedor" created="2016-04-13T01:35:34Z" id="209182505">I think that we should revisit this. Defaulting to `/bin/sh` is very difficult to support because `/bin/sh` means different things on OS that we _do_ support. For example, on late Centos it means `bash`, but on some Ubuntu it means `dash`. I don't think that the lack of `bash` on _unsupported_ OS should play _strongly_ into our decision making.

&gt; Given that we've shifted command line parsing to Java, do we need the nicer features in bash still?

Yes, I ran into issues on Ubuntu in #17675 after adding a very simple conditional that is fine in `bash` but not `dash`.

&gt; Consensus in the discussion was to keep on with `sh`

This does not make sense to me since `sh` does not mean the same thing across OS, so it means we are supporting everything.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More detail on cause of allocation failure / unassigned shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14001</link><project id="" key="" /><description>I have done a few tests in difference scenarios, in order to find the reason why shards are unassigned.   It seems as if there could be some improvements by providing the real, human readable reason why a shard is in unassigned state.

Possible end reasons (for failed allocation) would include 
- more replicas than nodes
- low disk watermark
- corruption
- forced allocation awareness
- bad routing

In the scenarios that I was able to reproduce, the _Unassigned Reason_, and _Unassigned Details_ did not assist in learning the real cause.  

In **Allocation Awareness** and **Low Disk Watermark** tests, no _unassigned.details_ were available, and the _unassigned.reason_ was not helpful in learning the user error.  Both tests had similar results:

```
GET /_cat/shards?h=node,index,shard,prirep,state,unassigned.reason,unassigned.details
Wolverine test-index 4 p STARTED                       
          test-index 4 r UNASSIGNED CLUSTER_RECOVERED  
...or...
Hazard newone3    4 p STARTED                       
       newone3    4 r UNASSIGNED INDEX_CREATED    
```

A **Node Left** test resulted in a useful _reason_ and a _unassigned.details_ string, but the latter was not human-friendly:

```
Cecelia Reyes test-index 4 p STARTED                                                
              test-index 4 r UNASSIGNED NODE_LEFT node_left[tHfFXWxGS4ao-xGjvkJQAg] 
Cecelia Reyes test-index 3 p STARTED                                                
              test-index 3 r UNASSIGNED NODE_LEFT node_left[tHfFXWxGS4ao-xGjvkJQAg] 
```
</description><key id="110301537">14001</key><summary>More detail on cause of allocation failure / unassigned shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PhaedrusTheGreek</reporter><labels><label>:Stats</label><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-10-07T19:41:39Z</created><updated>2016-11-03T19:15:43Z</updated><resolved>2016-01-12T12:28:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="haiguo" created="2015-10-07T20:07:25Z" id="146313588">+1
</comment><comment author="clintongormley" created="2015-10-08T12:23:56Z" id="146521563">The Unassigned Reason indicates why a shard was marked as unassigned, as opposed to what is preventing it from being assigned (which could be a combination of multiple factors).  

These issues can be diagnosed using the cluster reroute API with the `explain` parameter.

I agree it would be nice to present all of this info a clear human (and machine) readable way.  Any suggestions?
</comment><comment author="clintongormley" created="2015-10-09T10:16:51Z" id="146823815">Discussed in FixitFriday:  We think the biggest thing that is missing here is an explanation of what the unassigned reason codes actually mean.  Rather than trying to include that info in an API, the easiest solution is to just document them.

For the reason a shard cannot BE assigned, we already have human readable output in the cluster reroute API.  

However, it would also help to add documentation to explain how to diagnose these issues.
</comment><comment author="fforbeck" created="2015-12-17T02:29:54Z" id="165317474">Hi @clintongormley,
I've included the description for the reason codes as we have in the UnassignedInfo.java. 
Please, let me know if it is fine.
Thanks!
</comment><comment author="PhaedrusTheGreek" created="2016-11-03T19:15:43Z" id="258245735">This is addressed in 5.x with the [Cluster Allocation Explain API](https://www.elastic.co/guide/en/elasticsearch/reference/5.0/cluster-allocation-explain.html)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't pull translog from shadow engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/14000</link><project id="" key="" /><description>ShadowEngine doesn't have a translog but instead throws an
UOE when it's requested. ShadowIndexShard should not try to pull
stats for the translog either and should return null instead.

Closes #12730
</description><key id="110291899">14000</key><summary>Don't pull translog from shadow engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Shadow Replicas</label><label>bug</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-07T18:50:49Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-08T07:43:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T19:12:54Z" id="146299201">LGTM. Is there an easy way to test this?

So far as I can tell this will just skip make the stats api never return translog stats for ShadowEngine even if requested. Right?
</comment><comment author="s1monw" created="2015-10-07T20:05:12Z" id="146313030">&gt; So far as I can tell this will just skip make the stats api never return translog stats for ShadowEngine even if requested. Right?

yeah that's right - there is no translog for these shards so why would we ever return one?  i would need to write an integration test which I think it a bit over the top for this no?
</comment><comment author="nik9000" created="2015-10-07T20:14:47Z" id="146315257">&gt; i would need to write an integration test which I think it a bit over the top for this no?

Yeah - if its not simple then its not really worth it for this.

LGTM
</comment><comment author="s1monw" created="2015-10-07T20:31:22Z" id="146319146">@nik9000 I added a simple assertion to an existing test - better than nothing and it found another problem
</comment><comment author="nik9000" created="2015-10-07T20:39:31Z" id="146320978">&gt; @nik9000 I added a simple assertion to an existing test - better than nothing and it found another problem

Great! Now it looks much better to me!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document that Debian 8 and Ubuntu 14 systemd</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13999</link><project id="" key="" /><description>Fix #13865
</description><key id="110267843">13999</key><summary>Document that Debian 8 and Ubuntu 14 systemd</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kkirsche</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2015-10-07T16:41:49Z</created><updated>2015-10-08T16:53:22Z</updated><resolved>2015-10-08T16:09:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-08T12:07:56Z" id="146518213">Hi @kkirsche 

Small change to use an internal link instead of an http link

thanks
</comment><comment author="kkirsche" created="2015-10-08T12:34:22Z" id="146524409">@clintongormley fixed &#8212; thanks for sharing how to make that change
</comment><comment author="clintongormley" created="2015-10-08T16:09:26Z" id="146595011">Thanks @kkirsche - merged
</comment><comment author="kkirsche" created="2015-10-08T16:53:22Z" id="146620246">My pleasure. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests for TransportMasterNodeAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13998</link><project id="" key="" /><description>So far, no unit tests existed for this class.
</description><key id="110264740">13998</key><summary>Tests for TransportMasterNodeAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Internal</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-07T16:28:00Z</created><updated>2015-10-13T12:48:40Z</updated><resolved>2015-10-13T12:48:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-10-08T09:04:19Z" id="146464310">I left some comments. Thanks a lot for doing this, we certainly need these kind of tests!

In general it would be great to try to avoid blocking unit tests even if it is only 1s. Most of my comments are about that.
</comment><comment author="ywelsch" created="2015-10-08T12:15:28Z" id="146519537">@brwe thanks for the review. I integrated all your suggestions, please have another look.
</comment><comment author="brwe" created="2015-10-08T13:00:15Z" id="146534927">Left some trivial comments otherwise LGTM. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: a race condition can prevent the initial cluster state from being recovered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13997</link><project id="" key="" /><description>After a full cluster restart, the elected master is tasked with recovery the last known cluster state from disk. To do so, the GatewayService registers it self as a listener to cluster state changes, triggering the recovery if the local node is elected. Sadly the initial post-election cluster state can be missed if it's being processed while the listener is registered (i.e., the listener is too late but the discoveryService.initialStateReceived is not yet set). In this case the cluster state from disk will be recovered with the next change (typically node join).

 In practice this is not a big deal as master election takes at least 3s (by which time the gateway is long started), but it does make some of our tests to fail: http://build-us-00.elastic.co/job/es_core_master_centos/7915/

 To fix this, we submit a cluster state task after the edition of the listener, so we are guaranteed to check things while they are at rest.

 While at it, I removed some left over latch which we don't really wait on anymore.

I marked this PR as going to 2.1, 2.2 and 3.0. Still on the fence w.r.t 2.0. would love some input on that.
</description><key id="110260927">13997</key><summary>Gateway: a race condition can prevent the initial cluster state from being recovered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-07T16:10:21Z</created><updated>2015-10-18T19:39:15Z</updated><resolved>2015-10-18T19:22:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-07T18:32:58Z" id="146286959">I wonder if this is the right way to do it if it's time sensitive? I mean the race is there no matter what so why don't we special case the entire thing and make sure we recover the state before we do anything else if it's a master eligible node? I mean why should go down the road of registering a listener if we know it's potentially broken?
</comment><comment author="s1monw" created="2015-10-17T18:58:39Z" id="148944272">spoke to @bleskes offline and we both agree that we eventually should change how this works but for now the workaround / fix LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rewrite native script documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13996</link><project id="" key="" /><description>Closes #13811
</description><key id="110259053">13996</key><summary>Rewrite native script documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-07T16:00:51Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-07T17:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T16:01:17Z" id="146244026">@clintongormley decent sized docs change to review.
</comment><comment author="clintongormley" created="2015-10-07T16:48:01Z" id="146259270">Minor asciidoc comment, otherwise LGTM
</comment><comment author="clintongormley" created="2015-10-07T16:48:10Z" id="146259296">thanks for writing this
</comment><comment author="nik9000" created="2015-10-07T16:54:46Z" id="146260897">&gt; Minor asciidoc comment, otherwise LGTM

Fixed. Can you check to make sure I didn't break it? I get jumpy because I don't know the right sequence of commands to build all the docs locally and verify them.

&gt; thanks for writing this

Absolutely!
</comment><comment author="clintongormley" created="2015-10-07T16:56:21Z" id="146261271">looks good!
</comment><comment author="nik9000" created="2015-10-07T17:04:36Z" id="146263456">Squashed.
</comment><comment author="nik9000" created="2015-10-07T17:06:40Z" id="146263897">Merged to master and cherry-picked to 2.0, 2.1, and 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecates defaultRescoreWindowSize</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13995</link><project id="" key="" /><description>The window size should be set on each rescorer individually. This method will be removed in 3.0
</description><key id="110230567">13995</key><summary>Deprecates defaultRescoreWindowSize</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Java API</label><label>deprecation</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-07T13:53:50Z</created><updated>2016-01-22T18:35:17Z</updated><resolved>2015-10-07T14:35:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-07T14:05:52Z" id="146205118">LGTM shouldn't we also document the deprecation on the REST layer?
</comment><comment author="colings86" created="2015-10-07T14:08:49Z" id="146205771">This only seems to be a thing for the Java API and doesn't seem to be something you set on the REST layer. Also, I just noticed that SearchRequestBuilder has a similar method that needs deprecating so will push a that change too
</comment><comment author="nik9000" created="2015-10-07T14:12:30Z" id="146206874">LGTM. Sorry for the mess.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exposed hit/miss counters on filter cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13994</link><project id="" key="" /><description>Quick and dirty exposing of hit/miss counters on the filter caches. Comments/suggestions are welcome.
</description><key id="110219961">13994</key><summary>Exposed hit/miss counters on filter cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">favoretti</reporter><labels /><created>2015-10-07T12:55:37Z</created><updated>2015-10-07T16:54:54Z</updated><resolved>2015-10-07T16:54:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="favoretti" created="2015-10-07T13:01:33Z" id="146189834">CLA signed, re-opening PR didn't help CLA validation though..
</comment><comment author="clintongormley" created="2015-10-07T16:34:54Z" id="146255113">Hi @favoretti 

You know the filter cache has been removed in 2.0?
</comment><comment author="favoretti" created="2015-10-07T16:45:17Z" id="146258533">Yes sir. I do :) we are, however, running on 1.7 for now and hitting
interesting CPU sinks, hence just looking at the amount of evictions
doesn't really give insights in cache effectiveness. Exposing hit/miss
doesn't greatly enhance it either though, but gives a tad bit more to look
at.

On Wednesday, 7 October 2015, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @favoretti https://github.com/favoretti
&gt; 
&gt; You know the filter cache has been removed in 2.0?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13994#issuecomment-146255113
&gt; .
</comment><comment author="clintongormley" created="2015-10-07T16:54:54Z" id="146260931">@favoretti 1.7 is our last branch in the 1.x series, so only critical bug fixes will be considered for this branch I'm afraid.

If you need this locally, you'll have to either add it as a plugin or run a custom version.

thanks for the PR anyway!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exposed hit/miss counters on filter cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13993</link><project id="" key="" /><description>Quick and dirty exposing of hit/miss counters on the filter caches. Comments/suggestions are welcome.
</description><key id="110219164">13993</key><summary>Exposed hit/miss counters on filter cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">favoretti</reporter><labels /><created>2015-10-07T12:51:49Z</created><updated>2015-10-07T12:54:38Z</updated><resolved>2015-10-07T12:54:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="favoretti" created="2015-10-07T12:54:38Z" id="146188449">Signing the agreement...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Update repository docs with new major version structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13992</link><project id="" key="" /><description /><key id="110205565">13992</key><summary>Docs: Update repository docs with new major version structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>docs</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-07T11:33:54Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-07T12:44:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>paramters as command line args handled differently by plugin and elasticsearch script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13991</link><project id="" key="" /><description>`./bin/plugin` accepts parameters with a space between name and value like "-Des.path.conf ./path/to/conf" but `./bin/elasticsearch` does not. Seems to me like the two should do the same thing? This goes also for the windows scripts. see also #13989
</description><key id="110203959">13991</key><summary>paramters as command line args handled differently by plugin and elasticsearch script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-10-07T11:25:29Z</created><updated>2017-02-08T13:34:49Z</updated><resolved>2017-02-08T13:34:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-07T21:08:08Z" id="146329876">See my comment on the related issue. We should remove this functionality from `bin/plugin`. The jvm does not take sysprops like that, we are doing some magic to make it happen.
</comment><comment author="ywelsch" created="2015-10-09T10:13:58Z" id="146823362">+1 for removing the functionality that transforms `-Dfoor bar` into `-Dfoo=bar` in `plugin[.bat]`.

Note that the reason as for why system properties get special handling in `plugin[.bat]` is that when you invoke `plugin[.bat]` (in contrast to `elasticsearch[.bat]`) you not only want to specify system properties but also parameters to the main method (such as `install some-plugin -v`). The implementation of `plugin[.bat]` thus has to separate the former from the latter when passing it to the java process.
</comment><comment author="jasontedor" created="2017-02-08T13:34:49Z" id="278329993">Closed by #18207</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add pipeline execution service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13990</link><project id="" key="" /><description>Added pipeline execution service that deals with munging index &amp; bulk requests as they come in using a dedicated thread pool.

Also changed how bulk requests are handled, because before it just didn't work, but added a todo there because it can potentially be handled differently.
</description><key id="110201382">13990</key><summary>Add pipeline execution service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label><label>feature</label><label>review</label></labels><created>2015-10-07T11:07:19Z</created><updated>2015-10-09T16:17:29Z</updated><resolved>2015-10-09T16:17:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-09T08:31:08Z" id="146796326">@uboness @talevy Fixed the typo and removed the todo.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parse Java system properties in plugin.bat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13989</link><project id="" key="" /><description>Fixes #13616
</description><key id="110187227">13989</key><summary>Parse Java system properties in plugin.bat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Plugins</label><label>blocker</label><label>bug</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-07T09:40:02Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-13T11:31:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gmarz" created="2015-10-09T13:52:16Z" id="146876984">LGTM- tested on my Windows laptop.  Thanks for fixing this @ywelsch
</comment><comment author="clintongormley" created="2015-10-13T07:44:02Z" id="147632265">@ywelsch can we get this in?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: only have one CORS allow origin setting string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13988</link><project id="" key="" /><description>We currently have two, which is confusing when you  read the code (especially if one is used with a null default and the other with '*')

Note: this is not a real bug, just a  clean up. We do the right thing...
</description><key id="110179148">13988</key><summary>Internal: only have one CORS allow origin setting string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-07T08:53:10Z</created><updated>2015-10-18T19:47:39Z</updated><resolved>2015-10-18T19:44:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-17T18:59:20Z" id="148944294">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchWhileCreatingIndexIT.testNoReplicas test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13987</link><project id="" key="" /><description>build link: http://build-us-00.elastic.co/job/es_core_master_centos/7914/consoleText

actual failure: 

```
FAILURE 6.77s J0 | SearchWhileCreatingIndexIT.testNoReplicas &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: Hit count is 0 but 1 was expected.  Total shards: 6 Successful shards: 5 &amp; 0 shard failures:
   &gt;    at __randomizedtesting.SeedInfo.seed([1E2B6029CF6723AD:F05D84A395793CC1]:0)
   &gt;    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertHitCount(ElasticsearchAssertions.java:212)
   &gt;    at org.elasticsearch.search.basic.SearchWhileCreatingIndexIT.searchWhileCreatingIndex(SearchWhileCreatingIndexIT.java:81)
   &gt;    at org.elasticsearch.search.basic.SearchWhileCreatingIndexIT.testNoReplicas(SearchWhileCreatingIndexIT.java:47)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

This failure does **not** reproduce for me with: 

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=1E2B6029CF6723AD -Dtests.class=org.elasticsearch.search.basic.SearchWhileCreatingIndexIT -Dtests.method="testNoReplicas" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=el -Dtests.timezone=Pacific/Tarawa
```
</description><key id="110176910">13987</key><summary>SearchWhileCreatingIndexIT.testNoReplicas test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label></labels><created>2015-10-07T08:38:48Z</created><updated>2016-01-28T19:32:52Z</updated><resolved>2016-01-28T19:29:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T19:29:16Z" id="176357769">Old test failure. Closing
</comment><comment author="jasontedor" created="2016-01-28T19:32:52Z" id="176358957">Relates #16208
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Why does the ScriptedMetricBuilder not have an explicit generic type parameter?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13986</link><project id="" key="" /><description>I just noticed a problem when using the elastic4s API, which seems to be a problem with the core elastic API.

It basically boils down to the problem that the ScriptedMetricBuilder class does not set the generic type parameter of the MetricsAggregationBuilder.

The MetricsAggregationBuilder class definition:

```
public abstract class MetricsAggregationBuilder&lt;B extends MetricsAggregationBuilder&lt;B&gt;&gt; extends AbstractAggregationBuilder { ... }
```

A the moment, the ScriptedMetricBuilder is defined as follows:

```
public class ScriptedMetricBuilder extends MetricsAggregationBuilder  { ... }
```

There is also the ValuesSourceMetricsAggregationBuilder class which is defined as follows:

```
public abstract class ValuesSourceMetricsAggregationBuilder&lt;B extends ValuesSourceMetricsAggregationBuilder&lt;B&gt;&gt; extends MetricsAggregationBuilder&lt;B&gt; { ... }
```

Any subclass of the ValueSourceMetricsAggregationBuilder has the following definition:

```
public class StatsBuilder extends ValuesSourceMetricsAggregationBuilder&lt;StatsBuilder&gt; { ... }
public class SumBuilder extends ValuesSourceMetricsAggregationBuilder&lt;SumBuilder&gt; { ... }
...
```

They correctly set the generic type parameter of the MetricsAggregationBuilder class to themself. Only the ScriptedMetricBuilder, for some reason, does take over the more generic `B extends MetricsAggregationBuilder&lt;B&gt;` definition of it's parent.

Is there a reason the ScriptedMetricBuilder is not defined as follows?

```
public class ScriptedMetricBuilder extends MetricsAggregationBuilder&lt;ScriptedMetricBuilder&gt;  { ... }
```

As this would be more consistent with any other definition of builders there are?
</description><key id="110175474">13986</key><summary>Why does the ScriptedMetricBuilder not have an explicit generic type parameter?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kufi</reporter><labels><label>:Aggregations</label><label>bug</label></labels><created>2015-10-07T08:30:21Z</created><updated>2015-10-09T09:46:24Z</updated><resolved>2015-10-09T09:43:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-07T16:19:05Z" id="146250039">@colings86 any idea?
</comment><comment author="colings86" created="2015-10-08T07:10:00Z" id="146440068">Yep, this is a bug. Thanks @kufi for raising this
</comment><comment author="kufi" created="2015-10-08T08:14:16Z" id="146452312">Shall I create a pull request for it?
</comment><comment author="colings86" created="2015-10-08T08:16:05Z" id="146452626">@kufi that would be great, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Revisit defaults for the `cardinality` aggregation?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13985</link><project id="" key="" /><description>The `precision_threshold` parameter of the cardinality aggregation not only has an impact on accuracy but also on memory usage. This is why by default we decide how much memory a cardinality aggregation may use depending on how deep it can be found in the aggregation tree. For instance a top-level cardinality aggregation would use 16KB of memory, a cardinality aggregation under a terms aggregation would use 512 bytes per bucket, and a cardinality aggregation under two (or more) levels of terms aggregation would use 16 bytes per bucket.

Unfortunately, it's not easy to get precise counts with only 16 bytes of memory, which can make the out-of-the-box experience a bit disappointing. I think we have several (non-exclusive) options here:
- increase default memory usage, but I'm nervous about making it even easier to trigger circuit-breaking errors or worse out-of-memory errors. Maybe #9825 could help here: we could decide to always run terms aggs in breadth-first mode if there is a cardinality agg under them so that the cardinality aggregation would be computed on fewer buckets
- better document these defaults
- move parts of the aggs computation to disk so that we can increase our defaults more safely
</description><key id="110173337">13985</key><summary>Revisit defaults for the `cardinality` aggregation?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>discuss</label><label>high hanging fruit</label><label>stalled</label></labels><created>2015-10-07T08:17:57Z</created><updated>2016-10-24T15:01:29Z</updated><resolved>2016-10-24T15:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T09:23:04Z" id="148664163">A few things to investigate:
- Don't change precision if breadth first set
- Automatic use of breadth first when appropriate
- Aggs spilling to disk
</comment><comment author="jpountz" created="2016-10-24T15:01:29Z" id="255766087">Fixed via #19215
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoDistanceRangeQueryTests.testToQuery fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13984</link><project id="" key="" /><description>url: http://build-us-00.elastic.co/job/es_g1gc_master_metal/20357/

actual failure: 

```
Throwable #1: java.lang.AssertionError: query is not equal to its copy after calling toQuery, firstQuery: {
   &gt;   "geo_distance_range" : {
   &gt;     "mapped_geo_point" : [ 180.0, 2.8125 ],
   &gt;     "from" : null,
   &gt;     "to" : "879745km",
   &gt;     "include_lower" : true,
   &gt;     "include_upper" : true,
   &gt;     "unit" : "m",
   &gt;     "distance_type" : "sloppy_arc",
   &gt;     "optimize_bbox" : "memory",
   &gt;     "validation_method" : "COERCE",
   &gt;     "boost" : 1.0,
   &gt;     "_name" : "wNBTigHYQZ"
   &gt;   }
   &gt; }, secondQuery: {
   &gt;   "geo_distance_range" : {
   &gt;     "mapped_geo_point" : [ -180.0, 2.8125 ],
   &gt;     "from" : null,
   &gt;     "to" : "879745km",
   &gt;     "include_lower" : true,
   &gt;     "include_upper" : true,
   &gt;     "unit" : "m",
   &gt;     "distance_type" : "sloppy_arc",
   &gt;     "optimize_bbox" : "memory",
   &gt;     "validation_method" : "COERCE",
   &gt;     "boost" : 1.0,
   &gt;     "_name" : "wNBTigHYQZ"
   &gt;   
```

This failure **does** reproduce with: 
`mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=D9BE49D79D8940D7 -Dtests.class=org.elasticsearch.index.query.GeoDistanceRangeQueryTests -Dtests.method="testToQuery" -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.heap.size=1024m -Dtests.locale=mt_MT -Dtests.timezone=Africa/Ouagadougou`
</description><key id="110169919">13984</key><summary>GeoDistanceRangeQueryTests.testToQuery fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>jenkins</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-10-07T07:54:32Z</created><updated>2016-03-10T18:54:11Z</updated><resolved>2015-10-07T10:55:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Note that plugin removal usually requires node restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13983</link><project id="" key="" /><description>Fix #13301
</description><key id="110127945">13983</key><summary>Note that plugin removal usually requires node restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kkirsche</reporter><labels /><created>2015-10-07T00:09:07Z</created><updated>2015-10-07T16:16:42Z</updated><resolved>2015-10-07T16:11:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-07T16:11:26Z" id="146247311">thanks @kkirsche - merged
</comment><comment author="kkirsche" created="2015-10-07T16:16:41Z" id="146249183">Anytime @clintongormley &#8212; looking for some low hanging fruit ones to get my hands dirty with Elasticsearch proper instead of things like Kibana or the language clients
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>put jvm crash logs in logs/</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13982</link><project id="" key="" /><description>This would be a great improvement over what probably happens today. See issues like https://github.com/elastic/elasticsearch/issues/13600 where the jvm crashes and people are unaware of the log file (most likely in many cases its going to /tmp which is unexpected).

It would be better if we set `-XX:ErrorFile` explicitly...
</description><key id="110126242">13982</key><summary>put jvm crash logs in logs/</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Logging</label><label>:Packaging</label><label>enhancement</label></labels><created>2015-10-06T23:59:03Z</created><updated>2016-04-22T13:46:03Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-07T06:27:36Z" id="146089867">+1
</comment><comment author="jpountz" created="2015-10-07T10:02:27Z" id="146137957">+1
</comment><comment author="nik9000" created="2015-10-07T12:49:18Z" id="146186737">I'll have a look.
</comment><comment author="clintongormley" created="2016-01-28T19:28:46Z" id="176357650">Closed by #14006
</comment><comment author="rjernst" created="2016-01-28T20:34:43Z" id="176401724">I don't think this issue was fixed? The linked PR was closed without merging. The problem, IIRC, is the location must be set when starting the jvm, but we have too many ways to set the log dir location, so determining that from the elasticsearch shell script is problematic.
</comment><comment author="jasontedor" created="2016-01-28T20:35:03Z" id="176401936">@clintongormley I don't think #14006 was integrated, and I still think that this is something that we should do.
</comment><comment author="clintongormley" created="2016-01-29T09:27:18Z" id="176660605">ah thanks - missed that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose nodes operation timeout in REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13981</link><project id="" key="" /><description>Currently it's not possible to specify a timeout for nodes operations (such as node info, node stats, cluster stats and hot threads) via REST-based APIs.
</description><key id="110104413">13981</key><summary>Expose nodes operation timeout in REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v1.7.3</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-06T21:24:01Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-07T22:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-07T08:03:17Z" id="146107219">good catch, LGTM
</comment><comment author="nik9000" created="2015-10-07T13:09:25Z" id="146191503">LGTM as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add numeric sort order field to sorted results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13980</link><project id="" key="" /><description>A customer has requested the ability to add an optional field to sorted results that will show the order of the results. They're doing some operations on the sorted results that are breaking the sorting, and would like to be able to reconstruct them easily with their own systems. A field that shows the original order of the sort would help here.
</description><key id="110098235">13980</key><summary>Add numeric sort order field to sorted results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Search</label><label>feedback_needed</label></labels><created>2015-10-06T20:51:23Z</created><updated>2016-01-28T19:28:23Z</updated><resolved>2016-01-28T19:28:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-07T10:04:34Z" id="146138599">Maybe I'm not understanding the use-case correctly, but isn't it something that would be trivial to do on client side? Ie. when they get the results, they first attach an ordinal to each of them before doing their reordering on client-side?
</comment><comment author="clintongormley" created="2015-10-07T16:25:00Z" id="146251773">besides what @jpountz has suggested, results are already returned with a `sort` array including the values that were used for sorting.  Resorting on the client side would also be trivial.
</comment><comment author="clintongormley" created="2016-01-28T19:28:23Z" id="176357485">Nothing further. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix example about executing filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13979</link><project id="" key="" /><description>a second trial to get github to find my CLA
see https://github.com/elastic/elasticsearch/pull/13960
</description><key id="110098136">13979</key><summary>Fix example about executing filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels><label>docs</label></labels><created>2015-10-06T20:50:40Z</created><updated>2015-10-07T15:52:04Z</updated><resolved>2015-10-07T15:51:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="micpalmia" created="2015-10-06T20:51:32Z" id="145996885">nah, opening the PR again did not work...
</comment><comment author="clintongormley" created="2015-10-07T15:52:04Z" id="146241411">Not sure why it isn't showing up automatically, but I found it.  Merged, thanks @micpalmia 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more sort fields to _percolate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13978</link><project id="" key="" /><description>Hi,

I would like to use the percolator but need to be able to perform a multi-level sort using fields in the .percolator queries.
For example sort matches first by a priority field then by a date-time field.
The documentation says that only sorting by _score is currently supported.
Are there plans to allow sorting on other fields ?

Regards

Colm A
</description><key id="110097347">13978</key><summary>Add more sort fields to _percolate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colmaengus</reporter><labels><label>:Percolator</label><label>discuss</label></labels><created>2015-10-06T20:45:51Z</created><updated>2016-03-21T11:36:42Z</updated><resolved>2016-03-21T11:36:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-07T15:44:50Z" id="146239271">@colmaengus in the meantime you could fake it by using the function score query to combine the fields you need into a `_score`.
</comment><comment author="colmaengus" created="2015-10-07T18:31:32Z" id="146286592">I was thinking of doing that but was wondering how best to mimic the multi level sort.
Is there a plan to add sorting on other    fields ? If so any idea as to the timeline?

&gt; On 7 Oct 2015, at 16:45, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; @colmaengus in the meantime you could fake it by using the function score query to combine the fields you need into a _score.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a note about shard failure in the api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13977</link><project id="" key="" /><description>Closes #13674
</description><key id="110093645">13977</key><summary>Add a note about shard failure in the api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-06T20:25:15Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-07T14:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-06T20:26:11Z" id="145989797">I resisted the urge to be more opinionated in the documentation and make claims like "It is up to you if those failures constitute something worth logging, something worth reporting to the user, or both."
</comment><comment author="nik9000" created="2015-10-07T13:32:20Z" id="146197265">@clintongormley  - want to review this? Its one of the last 2.0 PRs!
</comment><comment author="jpountz" created="2015-10-07T13:56:34Z" id="146202965">LGTM
</comment><comment author="nik9000" created="2015-10-07T14:38:26Z" id="146214577">Merged to master and cherry picked to 2.0, 2.1, and 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make root_cause of field conflicts more obvious</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13976</link><project id="" key="" /><description>Creates a new ElasticsearchException, ConflictingFieldTypesException, that
it then throws on any mapping conflicts.

The error messages for mapping conflicts now look like:

```
{
  "error" : {
    "root_cause" : [ {
      "type" : "mapper_parsing_exception",
      "reason" : "Failed to parse mapping [type_one]: Mapper for [text] conflicts with existing mapping in other types:\n[mapper [text] has different [analyzer], mapper [text] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [text] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]"
    } ],
    "type" : "mapper_parsing_exception",
    "reason" : "Failed to parse mapping [type_one]: Mapper for [text] conflicts with existing mapping in other types:\n[mapper [text] has different [analyzer], mapper [text] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [text] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "Mapper for [text] conflicts with existing mapping in other types:\n[mapper [text] has different [analyzer], mapper [text] is used by multiple types. Set update_all_types to true to update [search_analyzer] across all types., mapper [text] is used by multiple types. Set update_all_types to true to update [search_quote_analyzer] across all types.]"
    }
  },
  "status" : 400
}
```

Closes #12839
</description><key id="110089177">13976</key><summary>Make root_cause of field conflicts more obvious</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-06T20:05:57Z</created><updated>2015-10-19T15:49:02Z</updated><resolved>2015-10-19T15:08:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-09T14:40:43Z" id="146891406">Would it be possible to keep the IllegalArgumentException and still be able to display the root cause of the problem? I think we should only add serialization support for new exceptions when strictly required.
</comment><comment author="nik9000" created="2015-10-09T14:41:56Z" id="146891682">&gt; Would it be possible to keep the IllegalArgumentException and still be able to display the root cause of the problem? I think we should only add serialization support for new exceptions when strictly required.

I can certainly investigate that. If I get it I'll make sure to add some notes about only adding new exceptions to the list if strictly required.
</comment><comment author="rjernst" created="2015-10-10T02:59:08Z" id="147026762">I agree we should only add custom exceptions if necessary, and here an IAE seems perfectly correct. IIRC, the problem was intermediate places in the call stack catching and wrapping. But also that the exception root cause analysis code stops when it finds an ElasticsearchException? Why can't we display the true root cause from the exception?
</comment><comment author="clintongormley" created="2015-10-13T08:23:22Z" id="147644554">@s1monw you know this area best - what's the correct fix here?
</comment><comment author="s1monw" created="2015-10-17T19:08:06Z" id="148944704">I think we should not add a new exception, IMO the main reason for the confusion is that we don't have a good exception message for the `MapperParsingException` can't we just fix the exception message like this: 

``` diff
diff --git a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
index a34d6a3..9b80807 100644
--- a/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
+++ b/core/src/main/java/org/elasticsearch/cluster/metadata/MetaDataCreateIndexService.java
@@ -385,7 +385,7 @@ public class MetaDataCreateIndexService extends AbstractComponent {
                             mapperService.merge(entry.getKey(), new CompressedXContent(XContentFactory.jsonBuilder().map(entry.getValue()).string()), true, request.updateAllTypes());
                         } catch (Exception e) {
                             removalReason = "failed on parsing mappings on index creation";
-                            throw new MapperParsingException("mapping [" + entry.getKey() + "]", e);
+                            throw new MapperParsingException("Failed to parse mapping [" + entry.getKey() + "] - " + e.getMessage(), e);
                         }
                     }
```
</comment><comment author="nik9000" created="2015-10-19T13:13:03Z" id="149208897">&gt; I think we should not add a new exception,

I have to admit when I saw that this could be fixed by adding a new exception I just added it, mostly because I wanted to know if we were against adding them because of the registry thing. I'll do it the way @s1monw suggests- it looks easier and I've had seen it had I not been trying to submit a troll patch anyway.
</comment><comment author="rjernst" created="2015-10-19T13:19:30Z" id="149211093">I don't understand why we need to hide exceptions. Why can't the root cause finder dig to the real root cause? This just makes for confusing stack traces (seeing the message copied).
</comment><comment author="nik9000" created="2015-10-19T13:30:10Z" id="149213477">&gt; I don't understand why we need to hide exceptions. Why can't the root cause finder dig to the real root cause? This just makes for confusing stack traces (seeing the message copied).

I'll think a bit on this - I don't see any reason why the root cause finder code _can't_ dig to the real root cause but I'm not sure the message will always be useful. Say its "null" because its a null pointer exception or something.
</comment><comment author="jpountz" created="2015-10-19T13:30:51Z" id="149213631">I agree copying the error message helps since it will make it more obvious to the user what the error is, as it will be included in the error message of the root exception.

However I share @rjernst's feeling that I would like to have the entire exception.
</comment><comment author="nik9000" created="2015-10-19T13:38:45Z" id="149215765">&gt; However I share @rjernst's feeling that I would like to have the entire exception.

I'd like to get the quick win here and copy the error message so we can backport this to 2.1 and improve the error message we give to the user.

For the root cause detection stuff I think we'll need some more thought.
</comment><comment author="jpountz" created="2015-10-19T13:49:15Z" id="149218514">That works for me.
</comment><comment author="jpountz" created="2015-10-19T14:31:24Z" id="149232243">LGTM.
</comment><comment author="nik9000" created="2015-10-19T15:08:31Z" id="149242961">&gt; LGTM.

Squashed and rebased.
</comment><comment author="nik9000" created="2015-10-19T15:49:02Z" id="149255070">Merged to master and cherry-picked to 2.x and 2.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thinning of historical Data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13975</link><project id="" key="" /><description>we are using the ELK stack to store and view our production monitoring metrics response times/error rates/ resource utilization etc... 

We are looking to come up with a data retention policy. Ideally we want to keep historical data, however we do not need the same resolution of older historical data. For example we don't need to know CPU usage at 10:32AM two years ago...  Ideally we would be able to thin the data of a time-series by removing events within a given interval.  (Granted this would cause an issue with our Count Aggregations, it would be useful to be able to apply a multiplier to tween the data, so we can still see count stats like throughput)

Are there any tools, that people have used with elasticsearch to accomplish this kind of stuff?
</description><key id="110082012">13975</key><summary>Thinning of historical Data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cphoover</reporter><labels><label>:Ingest</label><label>:Reindex API</label><label>discuss</label><label>high hanging fruit</label><label>stalled</label></labels><created>2015-10-06T19:27:46Z</created><updated>2015-10-16T09:30:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cphoover" created="2015-10-07T15:50:17Z" id="146240930">I think I have a solution for this...
</comment><comment author="nik9000" created="2015-10-07T15:52:42Z" id="146241594">&gt; I think I have a solution for this...

https://xkcd.com/979/

But to be serious I'd love to hear about it!
</comment><comment author="cphoover" created="2015-10-07T19:26:22Z" id="146302772">@nik9000 LOL
</comment><comment author="cphoover" created="2015-10-07T19:32:56Z" id="146304179"># APM Data Rentention/Archival Process

## Snapshot, Compression and Archival of Historical Indices

This process will involve using [elastic curator](https://github.com/elastic/curator) in conjunction with the [AWS Cloud Plugin for Elasticsearch](https://github.com/elastic/elasticsearch-cloud-aws#s3-repository) in order to store compressed snapshots of the previous days index. This process should run on a nightly basis. Optionally we can move indexes stored in s3 into Amazon Glacier cold storage, by setting up an [AWS Lifecycle rule](https://aws.amazon.com/blogs/aws/archive-s3-to-glacier/), to be stored at a significantly [cheaper cost](https://aws.amazon.com/s3/pricing/).  The disadvantage being once files are moved to cold storage they take some time to be read and are not immediately accessible. You can read more about [glacier here](https://aws.amazon.com/glacier/).

## Sampling/Thinning out of Historical Indices

This section still needs to be flushed out a bit, and the tooling needs to be created. The general idea is creating a tool, that uses the ES [Scan and Scroll API ](https://www.elastic.co/guide/en/elasticsearch/guide/current/scan-scroll.html) to push documents into a new index based on a sample ratio (e.g `Math.random() &lt;= SAMPLE_RATIO`). This tool should be run to thin the data of older indices out on a nightly basis. In our case we have an alerting system. This sampling process should omit periods of time in which we had active alerts.  In those periods of time we should have full resolution of our data points.  After this process has finished being created and the process has completed the newly created index should be given an index alias of the former index name. Then the old index can be dropped.

Currently we are using a **count** aggregation to visualize traffic/throughput, or rate of events.  This will not work when using sampling of data, because it would appear in our charts that indices that had been sampled would have a lower event count. Instead to fix this we will have to attach a new field onto all events that we care about `"countFactor"`.  This can easily be done with the logstash [mutate filter](https://www.elastic.co/guide/en/logstash/current/plugins-filters-mutate.html). So by default all documents will be indexed into elasticsearch with a field value `"countFactor" : 1`. Now when we run the sampling/thinning process we will have to multiply the countFactor by the divisor e.g. `sample 1/2 of events "countFactor" : 2`   now let's say a month passes and you want to thin that out by another 50% of documents you would multiply by 2 again and now have `"countFactor" : 4`. Now to visualize the event rate, instead of using a count aggregation you will use a sum aggregation.  This would give you the approximate rate of events at any given time.
</comment><comment author="cphoover" created="2015-10-08T13:54:59Z" id="146552976">@nik9000 not sure if there is a better way to handle this. What are your thoughts?
</comment><comment author="nik9000" created="2015-10-08T14:03:26Z" id="146555749">@cphoover I think its a good plan. I've been thinking about this for the past few months too and your ideas seem pretty good.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More documentation around how snapshots are incremental</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13974</link><project id="" key="" /><description>I see about one question a day on the forums, irc, etc about incremental snapshots. I think it might be worth reworking the documentation so its more obvious that the unit of reuse is files. Like with pictures.
</description><key id="110079164">13974</key><summary>More documentation around how snapshots are incremental</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Snapshot/Restore</label><label>docs</label></labels><created>2015-10-06T19:10:21Z</created><updated>2016-01-28T19:27:21Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-06T19:12:22Z" id="145968961">++ :)
</comment><comment author="imotov" created="2015-10-09T19:34:28Z" id="146968738">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add paragraph about removal of conf file option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13973</link><project id="" key="" /><description /><key id="110055037">13973</key><summary>add paragraph about removal of conf file option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>docs</label><label>v2.0.0</label></labels><created>2015-10-06T17:09:02Z</created><updated>2015-10-07T10:31:05Z</updated><resolved>2015-10-06T17:39:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-06T17:16:56Z" id="145932015">How about this:

```
==== Custom config file

It is no longer possible to specify a custom config file with the `CONF_FILE`
environment variable, or the `-Des.config`, `-Des.default.config`, or
`-Delasticsearch.config` parameters.

Instead, the config file must be named `elasticsearch.yml` and must be located
in the default `config/` directory, or in the directory specified in the
`CONF_DIR` environment variable.
```

And I'd put it in the Settings section here: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_setting_changes.html
</comment><comment author="brwe" created="2015-10-06T17:34:41Z" id="145937593">CONF_FILE is actually not an env variable - it is only used when elasticsearch is started as service and the variable is set in `/etc/default/elasticsearch` for deb and `/etc/sysconfig/elasticsearch` for rpm. Or is it then still called environment variable? If not can we do:

```
==== Custom config file

It is no longer possible to specify a custom config file with the `-Des.config`, `-Des.default.config`, or
`-Delasticsearch.config` (starting from command line) or the `CONF_FILE`
setting (for elasticsearch service).

Instead, the config file must be named `elasticsearch.yml` and must be located
in the default `config/` directory, or in the directory specified with `-Des.path.conf` 
(starting from command line) or the `CONF_DIR` setting (for elasticsearch service).
```
</comment><comment author="clintongormley" created="2015-10-06T17:35:49Z" id="145938397">It is still an environment variable.
</comment><comment author="brwe" created="2015-10-06T17:41:43Z" id="145941735">Thanks for the review!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default network.host to _non_loopback_ when using discovery.type:azure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13972</link><project id="" key="" /><description>We should come with good defaults when using `discovery.type: azure` and bind `network.host` to `_non_loopback_` (which is the private IP address - needed to be checked).

Related to #13969 
</description><key id="110052978">13972</key><summary>default network.host to _non_loopback_ when using discovery.type:azure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>won't fix</label></labels><created>2015-10-06T16:56:25Z</created><updated>2015-10-07T14:02:22Z</updated><resolved>2015-10-07T14:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-06T17:08:44Z" id="145929960">I dont think we should ever default to this, its far too heuristicy and hacky. Really it should be deprecated, its totally arbitrary depending on interface ordering. Azure should have its own pseudo names like ec2 and the others... How we resolve them is an impl detail of that plugin.

Moreover i guess i missed where we again start binding to non local interfaces by default, but ill be damned if we leave it to luck with non loopback. If azure is not explicit like ec2 then we should add `_site_local_` or something safer. 
</comment><comment author="dadoonet" created="2015-10-06T17:11:28Z" id="145930628">Yeah. I agree with you Robert. That's why I wrote `which is the private IP address - needed to be checked`. I thought it was. May it's not or too lenient.

I'll try first to see if we can implement `_azure_` interface easily or not which is too me the best option.
</comment><comment author="rmuir" created="2015-10-06T17:17:06Z" id="145932051">I have the same concerns about the gce value in the referenced pr, it seems just as arbitrary, interface zero. If we want to bind to site local addresses for these plugins, then i would rather have a pseudo value that only binds to site local addresses. It seems to solve both these problems and we know it will not bind to anything crazy.
</comment><comment author="dadoonet" created="2015-10-07T14:02:22Z" id="146204351">Closing for the same reasons as exposed in https://github.com/elastic/elasticsearch/issues/13969#issuecomment-146203970
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release: Fix package repo path to only consist of major version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13971</link><project id="" key="" /><description>This is fixes the package repo names from 2.0/2.1/etc to 2.x, so that
all major releases are in a single repository.

Closes #12493
</description><key id="110052772">13971</key><summary>Release: Fix package repo path to only consist of major version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-06T16:55:08Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-07T13:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-06T23:09:31Z" id="146030001">LGTM. I wonder if we need the `.x` or if it should just be eg `2`?
</comment><comment author="spinscale" created="2015-10-07T07:07:19Z" id="146097338">My thought was, that just putting `2` somewhere in the URL will people not make think it is a version and confuse them or accidentally remove it as they think it might be a typo... no strong opinions though, just implemented my personal thought process
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default network.host to _ec2_ when using discovery.type:ec2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13970</link><project id="" key="" /><description>We should come with good defaults when using `discovery.type: ec2` and bind network.host to `_ec2_` (which is the private IP address).

Related to #13969
</description><key id="110052519">13970</key><summary>default network.host to _ec2_ when using discovery.type:ec2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>won't fix</label></labels><created>2015-10-06T16:53:51Z</created><updated>2015-12-17T16:26:06Z</updated><resolved>2015-10-07T14:01:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-07T14:01:34Z" id="146204152">Closing for the same reasons as exposed in https://github.com/elastic/elasticsearch/issues/13969#issuecomment-146203970
</comment><comment author="peterskim12" created="2015-12-17T16:10:13Z" id="165496405">@dadoonet What do you think about logging a warning message suggesting that the user may want to consider changing network.host to _ec2_ if discovery.type: ec2? I gave some feedback to the doc team asking them to better call out the need to set network.host appropriately, but it's easy to miss.
</comment><comment author="dadoonet" created="2015-12-17T16:26:06Z" id="165500663">@peterskim12 Can you open a new issue for this so others can comment?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>default network.host to _gce_ when using discovery.type:gce</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13969</link><project id="" key="" /><description>We should come with good defaults when using `discovery.type: gce` and bind `network.host` to `_gce_` (which is the private IP address).

Related to #13612 
</description><key id="110052255">13969</key><summary>default network.host to _gce_ when using discovery.type:gce</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>:Plugin Discovery GCE</label><label>won't fix</label></labels><created>2015-10-06T16:52:20Z</created><updated>2015-10-07T14:01:04Z</updated><resolved>2015-10-07T14:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-07T14:00:47Z" id="146203970">Here are some tests when running elasticsearch `2.0.0-rc1` on Google Compute Engine without any plugin.
- wget rc1 and unzip
- launch it with all defaults - no plugin `bin/elasticsearch`
- bounds by default to `publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}`
- curl from external to 104.155.10.21:9200 `curl: (7) Failed to connect to 104.155.10.21 port 9200: Connection refused`
- open the firewall 9200 port and curl again. Same error. Which is expected.
- starting with `bin/elasticsearch -Des.network.host=_local_` gave the same result. Bound to `127.0.0.1`. Expected in 2.0.
- starting with `bin/elasticsearch -Des.network.host=_eth0_` (private IP): Bound to `10.240.0.3`. Accessible from outside because of the firewall route we did open previously.

Note that using `_gce_` with the plugin is equivalent of using `_eth0_` here.

What does it mean? It means that it will be unsecured to automatically bound to `_gce_` or `_eth0_` by default.

If users want to create a cluster of private nodes they will have to:
- define `network.host: _gce_` for example
- install `discovery-gce` plugin so they will get automatic node detection within the project

If they want to open one or all of their instances to the public, they will need to be extremely careful with that and probably add restrictions on which machines are really accessible on port 9200. By default, GCE firewall open the port for all machines which have a public IP, and by default all machines have a public IP.

Conclusion: let's close this ticket and the same tickets for azure and ec2 plugins as they should not have another behavior than the default elasticsearch behavior.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build test failure - Jython exceptions cause encoding error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13968</link><project id="" key="" /><description>Build failure here: http://build-us-00.elastic.co/job/es_g1gc_master_metal/20291/

If you call toString() of a python exception, jython tries to "render" it in a pythonisque way, but it seems to have localization problems, if the default locale uses a different number system.

As a workaround we can remove the toString in the assert of PythonSecurityTests.assertFailure() - that will stop build failures, until we have time to dig into jython's problems here.
</description><key id="110018710">13968</key><summary>Build test failure - Jython exceptions cause encoding error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Scripting</label><label>build</label></labels><created>2015-10-06T14:24:30Z</created><updated>2015-10-06T14:33:10Z</updated><resolved>2015-10-06T14:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-06T14:27:08Z" id="145872781">I opened https://github.com/elastic/elasticsearch/issues/13967 for this and pushed workarounds. I can look into it later, my guess is we want to look into the streams we give jython (this is configurable), really these scripts should not be printing to system.out/system.err anyway :) But this might not be the only problem there.
</comment><comment author="markharwood" created="2015-10-06T14:33:10Z" id="145875075">Thanks, @rmuir 
Closing as duplicate.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>jython has localization bugs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13967</link><project id="" key="" /><description>Looks like if the default locale has a different DecimalFormatAndSymbols, it won't be able to render exceptions. Most likely we need to get this fixed in jython itself...

See http://build-us-00.elastic.co/job/es_g1gc_master_metal/20291/ for example.
</description><key id="110016824">13967</key><summary>jython has localization bugs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Lang Python</label><label>:Scripting</label><label>bug</label></labels><created>2015-10-06T14:14:56Z</created><updated>2017-04-19T07:20:34Z</updated><resolved>2017-04-19T07:20:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2017-04-19T07:20:34Z" id="295135492">Python scripting has been removed in master (6.0).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0.0-beta2 fails to start using shield with access denied java.io.FilePermission</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13966</link><project id="" key="" /><description>Hi,

I've just recently updated all my ansible scripts to use all the available 2.0.0-beta2 files, including shield. I'm now receiving the following error on startup...

Exception in thread "main" ElasticsearchException[failed to initialize a TrustManagerFactory]; nested: AccessControlException[access denied ("java.io.FilePermission" "/apps/_elk_assets/certs/xxxxx.jks" "read")];

This was working fine prior to updating packages.

Any ideas?

Cheers,

Marty
</description><key id="109996962">13966</key><summary>2.0.0-beta2 fails to start using shield with access denied java.io.FilePermission</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2015-10-06T12:25:05Z</created><updated>2016-04-05T11:36:35Z</updated><resolved>2015-10-08T01:53:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-10-06T13:28:18Z" id="145855933">Hi @robertsmarty,

Is your JKS file stored in a directory outside of the elaticsearch config directory? If so, you will need to move it to a location within the elasticsearch configuration directory for 2.0.0-beta1 and newer. This is necessary since we use a security manager to limit the directories that the elasticsearch process can read from and write to.
</comment><comment author="robertsmarty" created="2015-10-08T01:53:27Z" id="146391413">Thanks for the answer! Moving the files under the config directory worked. I tried a symbolic link however that didn't seem to work.
</comment><comment author="sumitsengar" created="2016-04-03T09:43:59Z" id="204926110">HI,
I also faced a similar issue while testing ssl security with Shield. I understand ES's limitations in accessing files outside its config directory, but do we also have to live with this constraint in a production deployment scenario? I am using elastic 2.2.0 with same Shield version. Is there a way to override this behaviour?
</comment><comment author="jaymode" created="2016-04-05T11:36:35Z" id="205765223">Hi @sumitsengar,

I think your question will be better answered in our [Shield specific discussion forum ](https://discuss.elastic.co/c/shield). Please join us there so we can learn more about what you need.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Watcher mail body with mustache file template not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13965</link><project id="" key="" /><description>I am trying to load mail body of a watcher from mustache file placed in script folder. Elasticsearch is reading the file ,when i checked in log. if i am trying to give file name test_mail_body or test_mail_body.mustache, then getting error , test_mail_body file not found.
</description><key id="109996246">13965</key><summary>Watcher mail body with mustache file template not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ompanda</reporter><labels /><created>2015-10-06T12:20:08Z</created><updated>2015-10-06T17:40:30Z</updated><resolved>2015-10-06T17:40:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-06T17:40:30Z" id="145941209">Hi @ompanda 

Best place for help on watcher is https://discuss.elastic.co/c/watcher or your support contact
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0 beta testing: Performance difference between similar aggs vs. query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13964</link><project id="" key="" /><description>TLDR: Searches with sorting on a field to get the min/max value of a dataset appears to be a bit faster than aggregating using `min/max` aggs. Maybe just a flaw in my test, feel free to point it out!

While testing 2.0, I did a simple test. Using `makelogs` to index 2 million documents

``` bash
makelogs -c 2m -d 10 --verbose --omit relatedContent
```

Note: You need to install the latest makelogs if you want to use it: `npm install -g makelogs@beta`

Now I executed the following two queries, thinking that they did not have any difference in execution speed

``` json
GET logstash-*/_search
{
  "size": 1, 
  "sort": [
    {
      "@timestamp": {
        "order": "desc"
      }
    }
  ]
}

GET logstash-*/_search?search_type=count
{
  "aggs": {
    "foo": {
      "max": {
        "field": "@timestamp"
      }
    }
  }
}
```

However, the search seems to be a bit faster. I gathered data by running this and taking the `took` time from the responses, including one warmup round and no indexing happening:

`````` bash
for i in {1..500} ; do curl -s 'localhost:9200/logstash-*/_search' -d '{ "size": 1, "sort": [ { "@timestamp": { "order": "desc" } } ] }' | underscore extract 'took'   ; done | tee -a took-search.txt
for i in {1..500} ; do curl -s 'localhost:9200/logstash-*/_search?search_type=count' -d '{ "aggs": { "foo": { "max": { "field": "@timestamp" } } } }' | underscore extract 'took'   ; done | tee -a took-aggs.txt```

Those are the results

```bash
$ cat ../took-search.txt| st --complete
N     min   q1    median  q3  max   sum     mean    stddev    stderr
500   18    24.5  26      28  120     13524   27.048  6.50156     0.290758

$ cat ../took-aggs.txt| st --complete
N     min   q1    median  q3  max   sum     mean    stddev    stderr
500   26    33    35      37  57    17643   35.286  3.77528   0.168835

$ cat ../took-search.txt| ./data_hacks/histogram.py -m 18 -x 50
# NumSamples = 500; Min = 18.00; Max = 50.00
#2 values outside of min/max
# Mean = 27.048000; Variance = 42.185696; SD = 6.495052; Median 26.000000
# each &#8718; represents a count of 2
   18.0000 -    21.2000 [    12]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   21.2000 -    24.4000 [   113]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   24.4000 -    27.6000 [   217]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   27.6000 -    30.8000 [    99]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   30.8000 -    34.0000 [    38]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   34.0000 -    37.2000 [     8]: &#8718;&#8718;&#8718;&#8718;
   37.2000 -    40.4000 [     3]: &#8718;
   40.4000 -    43.6000 [     5]: &#8718;&#8718;
   43.6000 -    46.8000 [     3]: &#8718;
   46.8000 -    50.0000 [     0]:

$ cat ../took-aggs.txt| ./data_hacks/histogram.py -m 18 -x 50
# NumSamples = 500; Min = 18.00; Max = 50.00
#4 values outside of min/max
# Mean = 35.286000; Variance = 14.224204; SD = 3.771499; Median 35.000000
# each &#8718; represents a count of 2
   18.0000 -    21.2000 [     0]:
   21.2000 -    24.4000 [     0]:
   24.4000 -    27.6000 [     6]: &#8718;&#8718;&#8718;
   27.6000 -    30.8000 [    27]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   30.8000 -    34.0000 [   175]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   34.0000 -    37.2000 [   188]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   37.2000 -    40.4000 [    75]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   40.4000 -    43.6000 [    16]: &#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;&#8718;
   43.6000 -    46.8000 [     4]: &#8718;&#8718;
   46.8000 -    50.0000 [     5]: &#8718;&#8718;

# Calculate the 95% time
$ cat ../took-search.txt| ./data_hacks/ninety_five_percent.py
34   
$ cat ../took-aggs.txt| ./data_hacks/ninety_five_percent.py
41
``````

Tools used here: [st](https://github.com/nferraz/st) and [data_hacks](https://github.com/bitly/data_hacks)

The performance difference of those two queries is actually bigger on 1.7.

If there are flaws in my measurements, please close immediately, cant rule it out.

All of these tests happened on my MBP on osx.
</description><key id="109990449">13964</key><summary>Elasticsearch 2.0 beta testing: Performance difference between similar aggs vs. query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2015-10-06T11:42:59Z</created><updated>2017-05-05T14:35:49Z</updated><resolved>2017-05-05T14:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-05-05T14:33:49Z" id="299481229">@spinscale is there anything left to do here?</comment><comment author="spinscale" created="2017-05-05T14:35:49Z" id="299481778">yes, closing the issue.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make strategy optional in GeoShapeQueryBuilder readFrom and writeTo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13963</link><project id="" key="" /><description>The field is optional everywhere else but in the serialization methods, which causes problems. Also expanded tests so that they can catch this type of problem.
</description><key id="109986834">13963</key><summary>Make strategy optional in GeoShapeQueryBuilder readFrom and writeTo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-06T11:18:37Z</created><updated>2015-10-06T11:23:48Z</updated><resolved>2015-10-06T11:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-10-06T11:21:26Z" id="145825747">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix indices.get_field_mapping rest tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13962</link><project id="" key="" /><description>After `field` has been renamed to `fields` in #13902
</description><key id="109986218">13962</key><summary>Fix indices.get_field_mapping rest tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-10-06T11:13:39Z</created><updated>2015-10-06T11:20:13Z</updated><resolved>2015-10-06T11:16:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-06T11:14:21Z" id="145824614">LGTM
</comment><comment author="Mpdreamz" created="2015-10-06T11:20:09Z" id="145825549">merged to master and cherry-picked to `2.0`, `2.1` &amp; `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Convert some messy function score tests to unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13961</link><project id="" key="" /><description>There are many more tests I could convert but I thought I'd better get some feedback first.
</description><key id="109976797">13961</key><summary>Convert some messy function score tests to unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label></labels><created>2015-10-06T10:11:05Z</created><updated>2015-10-07T12:51:23Z</updated><resolved>2015-10-07T12:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-06T15:48:59Z" id="145906850">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix example about executing filters (minor)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13960</link><project id="" key="" /><description /><key id="109975088">13960</key><summary>Fix example about executing filters (minor)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">micpalmia</reporter><labels><label>docs</label></labels><created>2015-10-06T10:01:31Z</created><updated>2015-10-06T20:49:09Z</updated><resolved>2015-10-06T20:49:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-06T12:15:19Z" id="145839195">I think the old way was right. It talking about the `bool` query in the last section.
</comment><comment author="nik9000" created="2015-10-06T16:12:01Z" id="145915139">&gt; I think the old way was right. It talking about the bool query in the last section.

Whoops! I had that backwards. You are right. The doc fix is correct. Sorry for the confusion.

Would you mind signing the elastic [cla][https://www.elastic.co/contributor-agreement]? Its required for all changes. If you sign it, wait 30 minutes, and then comment on this ticket then the tests integrated in github will pass and I can merge it.
</comment><comment author="micpalmia" created="2015-10-06T18:09:21Z" id="145949511">I signed it earlier but did not know I had to comment here. Here it is :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: JsonEscapingMustacheFactory does not escape HTML</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13959</link><project id="" key="" /><description>According to the [mustache docs](https://mustache.github.io/mustache.5.html) all input is HTML escaped by default, unless you use the triple mustache `{{{foo}}}`. However with our own `JsonEscapingMustacheFactory` we break this convention.

Test to reproduce:

``` java
@Test
public void testEscaping() throws Exception {
    HashMap&lt;String, Object&gt; scopes = new HashMap&lt;&gt;();
    scopes.put("foo", "&lt;b&gt;FOO&lt;/b&gt;");

    // DefaultMustacheFactory works as expected
    //MustacheFactory f = new DefaultMustacheFactory();
    MustacheFactory f = new JsonEscapingMustacheFactory();

    String template = "This is spartaaaa or html: {{foo}}";
    Mustache mustache = f.compile(new StringReader(template), "example");
    StringWriter writer = new StringWriter();
    mustache.execute(writer, scopes);
    writer.flush();
    assertThat(writer.toString(), is("This is spartaaaa or html: &amp;lt;b&amp;gt;FOO&amp;lt;/b&amp;gt;"));
}
```

Original source of the issue: https://discuss.elastic.co/t/watchers-mustache-implementation-does-not-appear-to-html-escape-values/31427
</description><key id="109971469">13959</key><summary>Scripting: JsonEscapingMustacheFactory does not escape HTML</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-10-06T09:45:43Z</created><updated>2015-10-06T17:52:16Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aochsner" created="2015-10-06T13:27:05Z" id="145855674">Thanks for tracking this down!

I'd suppose that for the 99% case in ElasticSearch, you don't want to html escape because the content is normally JSON.  I thought about it some more, and I think it's more of an issue with Watcher &amp; the HTML body in an email.  In that case, I want/need to html-escape by default (or use triple mustache if I don't).
</comment><comment author="spinscale" created="2015-10-06T13:35:11Z" id="145857464">As mustache was introduced for the search template feature, it makes sense to have the `JsonEscapingMustacheFactory` - but think about scripted fields or scripted metrics etc, where people still might be confused, when mustache does not follow the expected default behaviour. Let's see if we can find a solution in core two support both ways or alternatively ensure this works in watcher.
</comment><comment author="aochsner" created="2015-10-06T14:31:48Z" id="145874544">Awesome!  Looking forward to it.  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ScriptEngineService.unwrap.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13958</link><project id="" key="" /><description>The ability to unwrap script values is already exposed via ExecutableScript.unwrap.
</description><key id="109959497">13958</key><summary>Remove ScriptEngineService.unwrap.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-06T08:31:54Z</created><updated>2015-10-06T17:18:20Z</updated><resolved>2015-10-06T08:40:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-06T08:38:31Z" id="145783897">LGTM
nice stats :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportNode ensureNodesAreAvailable() doesn't make sense</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13957</link><project id="" key="" /><description>In https://github.com/elastic/elasticsearch/blob/6885ab96805990d83e57dd0baded83252d7a2e0e/core/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java

``` java
    private void ensureNodesAreAvailable(List&lt;DiscoveryNode&gt; nodes) {
        if (nodes.isEmpty()) {
            String message = String.format(Locale.ROOT, "None of the configured nodes are available: %s", nodes);
            throw new NoNodeAvailableException(message);
        }
    }
```

If `nodes` is empty, what's the point in reporting it in the error message?

I'd expect the configured nodes - the ones provided to the TransportNode - and not the ones that we were actually been able to connect to (none of them)
</description><key id="109958103">13957</key><summary>TransportNode ensureNodesAreAvailable() doesn't make sense</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Exceptions</label><label>enhancement</label></labels><created>2015-10-06T08:21:39Z</created><updated>2015-10-19T08:42:28Z</updated><resolved>2015-10-19T08:42:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-06T08:24:36Z" id="145781041">makes sense to me... would you be up for sending a PR? ;)
</comment><comment author="synhershko" created="2015-10-07T22:07:49Z" id="146346720">There you go https://github.com/elastic/elasticsearch/pull/14007
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ScriptEngineService.execute.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13956</link><project id="" key="" /><description>This methods was only used in tests and can be replaced by calling
`ScriptEngineService.executable(compiledScript, vars).run()` instead.
</description><key id="109954820">13956</key><summary>Remove ScriptEngineService.execute.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-06T07:57:57Z</created><updated>2015-10-06T17:33:34Z</updated><resolved>2015-10-06T11:53:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-06T08:48:14Z" id="145787468">I was always confused about the difference between `execute` and `executable` there. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Engine.Create</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13955</link><project id="" key="" /><description>The `_create` API is handy way to specify an index operation should only be done if the document doesn't exist. This is currently implemented in explicit code paths all the way down to the engine. However, conceptually this is no different than any other versioned operation - instead of requiring a document is on a specific version, we require it to be deleted (or non-existent). This PR removes Engine.Create in favor of a slight extension in the VersionType logic.

There are however a couple of side effects:
- DocumentAlreadyExistsException is removed and VersionConflictException is used instead (with an improved error message)
- Update will reject version parameters if the upsert option is used (it doesn't compute anyway).
- Translog.Create is also removed infavor of Translog.Index (that's OK because their binary format was the same, so we can just read Translog.Index of the translog file)
</description><key id="109947956">13955</key><summary>Remove Engine.Create</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Engine</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-06T07:08:25Z</created><updated>2015-10-07T10:47:35Z</updated><resolved>2015-10-07T10:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-06T07:08:53Z" id="145764448">@s1monw @mikemccand care to take a look?
</comment><comment author="mikemccand" created="2015-10-06T19:28:32Z" id="145972588">LGTM, I only left trivial comments, this is a great simplification!
</comment><comment author="bleskes" created="2015-10-07T10:47:35Z" id="146153609">Thx Mike! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow binding to multiple addresses.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13954</link><project id="" key="" /><description>For consistency and simplicity this means that es.network.host/bind_host/publish_host can all take arrays.

Yes, we are still limited to a single publish host, and only the "best" address from publish_host will be used, I do not change that, but its already the case today (even by default!: `_local_`), and its just an internal limitation we can later fix. Instead we just continue with the same logic of "magic selection of publish host", and users can always be explicit, and we are just clear to the user about what is happening.

I did it this way, because network stuff is already overwhelmed, and i mean really overwhelmed with settings. We need to make this easy on the user to do the right thing and not make that worse.

Examples:

`bin/elasticsearch` # defaults

```
[elasticsearch] [2015-10-06 01:15:28,671][WARN ][common.network           ] [Todd Arliss] publish host: [_local_] resolves to multiple addresses, auto-selecting {127.0.0.1} as single publish address
[elasticsearch] [2015-10-06 01:15:28,672][INFO ][transport                ] [Todd Arliss] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
```

`bin/elasticsearch -Des.network.host=_lo0_,_en0_` # multiple interfaces

```
[2015-10-06 01:17:08,970][WARN ][common.network           ] [Torso] publish host: [_lo0_, _en0_] resolves to multiple addresses, auto-selecting {192.168.0.19} as single publish address
[2015-10-06 01:17:08,972][INFO ][transport                ] [Torso] publish_address {192.168.0.19:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}, {[fe80::3e15:c2ff:fee5:d26c]:9300}, {192.168.0.19:9300}
```

`bin/elasticsearch -Des.network.host=192.168.0.19,_local_ -Des.network.publish_host=192.168.0.19` # being explicit

```
[2015-10-06 01:19:28,603][INFO ][transport                ] [Thunderbird] publish_address {192.168.0.19:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}, {192.168.0.19:9300}
```

Closes #13592
</description><key id="109939223">13954</key><summary>Allow binding to multiple addresses.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Network</label><label>feature</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-06T05:45:27Z</created><updated>2015-10-24T03:48:45Z</updated><resolved>2015-10-24T03:48:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-06T18:34:40Z" id="145958017">I pushed another commit that fixes the `_local_` to behave more sanely, instead of "all the addresses of loopback interfaces" it means "all the loopback addresses". This is an important difference, e.g. it means we no longer confusingly bind to link-local addresses like `fe80::1%lo0` on the mac.

I added `_site_` and `_global_` to match `_local_`. This means users can configure network binding by reachability scope(s), interface(s), address(es), or any combination of those. 

e.g. stuff like `-Des.network.host=_local_,_site_` will bind to 127.0.0.1, ::1, and 192.168.1.1, but not public addresses like 92.1.2.3

I am hoping this is enough to prevent confusing options in issues such as https://github.com/elastic/elasticsearch/pull/13612 and https://github.com/elastic/elasticsearch/issues/13972

If we need to fix this for 2.0 in order to fix those issues, then fine, lets do things right. But our network configuration needs to be easy and make logical sense :)
</comment><comment author="nik9000" created="2015-10-06T18:47:18Z" id="145961754">&gt; ```
&gt; [2015-10-06 01:17:08,972][INFO ][transport                ] [Torso] publish_address {192.168.0.19:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}, {[fe80::3e15:c2ff:fee5:d26c]:9300}, {192.168.0.19:9300}
&gt; ```

Nice!

&gt; If we need to fix this for 2.0 in order to fix those issues, then fine, lets do things right. But our network configuration needs to be easy and make logical sense :)

I'm +1 on getting this back to 2.1. I'd love to get it into 2.0.1 when that comes around too.
</comment><comment author="rmuir" created="2015-10-06T21:30:53Z" id="146007818">@nik9000 and for the record i don't really want to rush this stuff in. I just mention that in response to things like https://github.com/elastic/elasticsearch/issues/13972 which are set for 2.0 

Maybe it is those that need to be reconsidered instead as far as versioning? I am heavily concerned about being able to fix this stuff, I don't want to hear whimpering and whining about how fixing it in 2.2 is "breaking bwc", and i also don't want 2.0 to go out with massive security problems: I am doing everything I can to prevent that.
</comment><comment author="dadoonet" created="2015-10-06T21:36:23Z" id="146008913">@rmuir There is IMO absolutely no rush to fix #13969 #13970 and #13972 although I marked them initially for 2.0.0. Just because people can use `_local_` or `_ec2_` for now so they have a workaround.

So if there is any security concern, let's move those issues to a later version. It's only a nice to have I think.
</comment><comment author="rmuir" created="2015-10-06T21:39:14Z" id="146009422">OK I just feel like i missed something bigtime: some discussion that we are reverting the "bind-to-localhost" default in terms of ease of use.

This should really be discussed in more detail and planned more thoroughly. I saw a bunch of 2.0 issues come up suddenly and just got super worried about it.

On the other hand binding to localhost only as a default always, its very safe and easy to understand.
</comment><comment author="nik9000" created="2015-10-06T21:46:31Z" id="146010943">&gt; Maybe it is those that need to be reconsidered instead as far as versioning? I am heavily concerned about being able to fix this stuff, I don't want to hear whimpering and whining about how fixing it in 2.2 is "breaking bwc", and i also don't want 2.0 to go out with massive security problems: I am doing everything I can to prevent that.

If we're concerned about breaking backwards compatibility we can make those network addresses either strings or arrays. That seems simple enough and might be worth doing anyway.

&gt; On the other hand binding to localhost only as a default always, its very safe and easy to understand.

Yeah. I think it'd be nice to have some "this is how you make a real cluster" docs somewhere for users who want a helping hand going from playing around on one box to multiple boxes.
</comment><comment author="nik9000" created="2015-10-06T21:46:51Z" id="146010999">&gt; Yeah. I think it'd be nice to have some "this is how you make a real cluster" docs somewhere for users who want a helping hand going from playing around on one box to multiple boxes.

But not as part of this PR. That'd be silly.
</comment><comment author="rmuir" created="2015-10-06T21:54:25Z" id="146012765">I'm removing 2.2 and will just remove the garbage like `_non_loopback_` in our network configuration completely here in additional commits. It is far more important to clean up master. Otherwise things like deprecation and leniency hang around for too long.
</comment><comment author="rmuir" created="2015-10-15T22:45:14Z" id="148544373">I pushed additional commits with the cleanups I wanted to do. I think this is good for master. if we want to backport it (e.g. to 2.2) then we should deprecate+warn about the heuristicy `_non_loopback_` that will be removed.
</comment><comment author="rmuir" created="2015-10-23T12:27:09Z" id="150558408">I'm not going to maintain this branch, its too much work
</comment><comment author="s1monw" created="2015-10-23T12:37:23Z" id="150559960">I looked through it and it LGTM I am sorry I missed the last pings 8 days ago, can you merge it when it's up-to-date?
</comment><comment author="rmuir" created="2015-10-23T13:28:49Z" id="150571411">i resynced this to master.
</comment><comment author="jaymode" created="2015-10-23T13:40:59Z" id="150575468">LGTM too
</comment><comment author="s1monw" created="2015-10-23T19:39:29Z" id="150671764">LGTM thanks rob
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide custom logging appender that supports time based rolling and maxbackupindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13953</link><project id="" key="" /><description>As part of https://github.com/elastic/elasticsearch/issues/7927, we implemented RollingFileAppender in log4j.  However, this is implemented via log4j-extras and the RollingFileAppender there does not support MaxBackupIndex for time based rolling.  

```
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.rolling.TimeBasedRollingPolicy.
log4j:WARN No such property [maxBackupIndex] in org.apache.log4j.rolling.RollingFileAppender.
```

This is a request for an out of the box appender that supports all of the following features commonly requested by administrators out there:
- Time based rolling (eg. logs rolled daily)
- Compression on logs
- Ability to define the max number of logs to retain (i.e. MaxBackupIndex), eg. retain the last N days of logs.

Also see discussion [here](https://github.com/elastic/puppet-elasticsearch/issues/34)
</description><key id="109932701">13953</key><summary>Provide custom logging appender that supports time based rolling and maxbackupindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Logging</label><label>discuss</label><label>enhancement</label></labels><created>2015-10-06T04:16:13Z</created><updated>2016-05-20T20:35:34Z</updated><resolved>2016-05-20T20:35:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tomchlee" created="2016-03-17T20:33:25Z" id="198069987">Hi,

By updating ES to log4j 2, I think all of the features indicated in this request will be supported and with many more options via RollingFileAppender and DefaultRolloverStrategy:

https://logging.apache.org/log4j/2.x/manual/appenders.html

Thanks.
</comment><comment author="haiguo" created="2016-05-20T20:09:20Z" id="220706492">+1 
</comment><comment author="jasontedor" created="2016-05-20T20:34:15Z" id="220711768">Closing in favor of #17697.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OR statement braking the "NOT" statement ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13952</link><project id="" key="" /><description>Hi Everyone,
I'm here to give back to community, i spent hours looking for an answer for my problem without luck, so I decided to share my findings with you all, so we can all take a look on the mystic of elasticsearch.

My problem was very simple, but not easy as it seems:
i needed to search for a field where the fields match a keyword or it were simply blank either null
ex:

SELECT \* FROM bucket1 WHERE country="canada" AND (language=en OR language=NULL)

simple right?

for my query i constructed the following:
bucket/document/_search [POST]
{
  "size": 10,
  "query": {
    "query_string": {
      "query": "country:ca AND region1:quebec AND (language:en OR -language:*)"
    }
  }
}

It brought me 0 (zero) results what made me think.. because when i did the split query, it broght me around 35k results
ex:
{
  "size": 10,
  "query": {
    "query_string": {
      "query": "country:ca AND region1:quebec AND (language:en)"
    }
  }
}
32k results
THEN 
{
  "size": 10,
  "query": {
    "query_string": {
      "query": "country:ca AND region1:quebec AND (-language:*)"
    }
  }
}
3k results

so i decided go for different approach
{
  "size": 10,
  "query": {
    "query_string": {
      "query": "(region1:quebec AND -geo_city:\* ) OR (region1:quebec AND geo_city:montreal )"
    }
  }
}
35K results
conclusion NOT statement is broking everything if set with an OR statement.
after that i finally did the following:
{
  "size": 10,
  "query": {
    "query_string": {
      "query": "country:ca AND region1:quebec AND ((language:en) OR (-language:*))"
    }
  }
}
35k results, and it was working

now im looking for a different approach to make this query more efficient, cuz the NOT statement is taking really long time to excecute, around x20 factor than without the NOT statement.

i hope to have some feedbacks on this =)

thanks in advance
</description><key id="109881163">13952</key><summary>OR statement braking the "NOT" statement ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ACV2</reporter><labels /><created>2015-10-05T20:38:29Z</created><updated>2015-10-08T19:38:41Z</updated><resolved>2015-10-05T20:48:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-05T20:48:43Z" id="145664058">Please ask your questions on discuss.elastic.co.

We can help you there.

Note that when using wildcards text is not analyzed. Might be your problem here. But let's talk about it on discuss.
</comment><comment author="ACV2" created="2015-10-08T19:38:41Z" id="146665435">@dadoonet Hi thanks for your reply, you're right that seems to be the right place.

you can follow in: 
https://discuss.elastic.co/t/or-statement-braking-the-not-statement-elasticsearch/31875
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support unsigned number types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13951</link><project id="" key="" /><description>There are a lot of cases, where you want to store a byte/short/int/long in an elastic field, but it's unsigned by nature. With byte/short/int, it's always possible to use a bigger type (at the cost of increased storage space? I haven't measured that), but not with long.
Splitting numbers or subtracting/adding the "magic number" from and to the value is a nightmare.
I think it would be much more convenient to support unsigned types natively.

Theoretically on elasticsearch side it wouldn't be too hard: if a field is marked as unsigned (ubyte/ushort/uint/ulong?), every value which "comes in" needs to be decreased by 2^(bitsize(type)-1), and when "going out", increased by the same amount. Otherwise elasticsearch internals wouldn't change.
Of course representing the unsigned values with Java native signed types can be tricky (BigInteger or Java 8 native unsigned types?).
</description><key id="109865111">13951</key><summary>Support unsigned number types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bra-fsn</reporter><labels><label>:Mapping</label><label>discuss</label></labels><created>2015-10-05T19:10:32Z</created><updated>2017-01-11T15:26:59Z</updated><resolved>2015-10-16T09:56:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-05T21:09:36Z" id="145669195">The values are mostly already compressed in lucene. For example, if your values range from -1 to 253, only a single byte should be used (depends on what features you have enabled for the field eg indexed, stored, doc values).

I don't think we should complicate the api with more types. Other than input validation (which the user can do on their end), the different types for long/int/short/byte don't matter to lucene. They are simply numeric and we will represent them according to the values seen. I would rather see the ability to set the valid range of values in the mapping, and limit to just "integral" and "floating point" field types. (Granted the types do matter for fielddata, but with doc values by default for 2.0 this doesn't really matter anymore).
</comment><comment author="jpountz" created="2015-10-06T13:47:54Z" id="145861177">&gt; Granted the types do matter for fielddata

Actually they don't, so even on 1.x things are just fine.
</comment><comment author="clintongormley" created="2015-10-06T17:45:22Z" id="145942705">I think the only time that type may be relevant is when you want to use an unsigned long that doesn't fit into the range of a signed long?
</comment><comment author="rjernst" created="2015-10-06T17:55:04Z" id="145945987">Once we get BigDecimal support it becomes irrelevant I think? I would rather wait for that (hopefully not too long now with BKD going into lucene core soon) than add something the the api right now that would require complex internal changes (how do you represent an unsigned long without BigDecimal in java?).
</comment><comment author="clintongormley" created="2015-10-06T18:00:22Z" id="145947257">++
</comment><comment author="bra-fsn" created="2015-10-06T18:25:20Z" id="145954972">@clintongormley: Currently, I have to use cases:
1. I have several unsigned int fields, which I can't express with signed integer. If I set them to long, the "binding" part of the ES schema (mapping) disappears, the devs think that it's a long and handle it accordingly. Apps break if anybody writes a value which can't be expressed in unsigned int space.
2. Yes, unsigned long is another (big :) problem. I have to do math or express them as strings, but I lose the arithmetic operators then (search for ranges etc)
And third (which I couldn't think ES will ever support): arbitrary size numbers. For example I store several 128, 256, 384 and 512 bit hash values.
Now I can only do this with:
1. split numbers (into 64 bits and I have to do unsigned-signed-unsigned conversions)
2. num to string encoding
3. hex storage as string
4. binary storage (base64)
Instead of writing a 512 bit number...

As far as I can remember, 1 and 2 was the most space efficient (storing several billions, so space matters), but having an arbitrary sized integer would be the best.

So my point is:
1. having different (unsigned/signed) types helps in keeping consistency (value range enforcement) across different users - sometimes there are just too much devs hanging around
2. having a bigdecimal would also be a big advantage

Thanks,
</comment><comment author="bra-fsn" created="2015-10-06T18:30:26Z" id="145956721">@rjernst: "(how do you represent an unsigned long without BigDecimal in java?)"
With BigInteger? :)
</comment><comment author="rjernst" created="2015-10-06T18:34:51Z" id="145958076">&gt; 1. having different (unsigned/signed) types helps in keeping consistency (value range enforcement) across different users - sometimes there are just too much devs hanging around

See my comments before. Would the problem you have with storing an unsigned int in a long be handled by allowing you to set the minimum and maximum value allowed on numeric types? This way it could be a long, but you set 0 as the min and max unsigned int as the max. The only limitation (for now, until we have bigint/decimal support) would be you still could not represent anything outside of min/max signed long.

&gt; 1. having a bigdecimal would also be a big advantage

See the issue #5683, and the latest lucene issue to support this, https://issues.apache.org/jira/browse/LUCENE-6825
</comment><comment author="bra-fsn" created="2015-10-06T18:43:44Z" id="145960259">@rjernst: yes, for me having a min/max (and an arbitrary precision integer :) would be enough, given that it won't explode the storage space needs.
</comment><comment author="jpountz" created="2015-10-16T09:56:46Z" id="148671702">We just discussed this issue in FixitFriday: we like specifying types using byte,short,int,long better than min and max values.

And given that doc values compute dynamically how many bits per value are required, space requirements would be pretty similar between an unsigned int field and a long, so implementing support for unsigned numeric types would not help.
</comment><comment author="pjcard" created="2017-01-11T15:26:59Z" id="271897936">@rjernst Thanks for your suggestion, however it won't help us or anyone else indexing 64-bit hashes, our workaround for those at the moment is to store them as a string.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] remove index= from catch: /regex/ in date math tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13950</link><project id="" key="" /><description>This breaks on some clients and is not required for the actual test

Should be back-ported to `2.x` and `2.0`
</description><key id="109862764">13950</key><summary>[test] remove index= from catch: /regex/ in date math tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels><label>:REST</label><label>test</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-05T18:56:22Z</created><updated>2016-03-10T18:15:03Z</updated><resolved>2015-10-05T19:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-05T19:14:25Z" id="145635765">LGTM
</comment><comment author="HonzaKral" created="2015-10-06T15:47:12Z" id="145906362">Pushed to `master`, `2.0`, `2.1` and `2.x`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>lucene-expressions fail at compiling a simple script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13949</link><project id="" key="" /><description>If you look at this build failure: http://build-us-00.elastic.co/job/es_core_master_regression/3629/testReport/junit/org.elasticsearch.script.expression/MoreExpressionTests/testSpecialValueVariable/ Elasticsearch failed to compile an aggregation script on one shard (only one) due to the following error:

```
ScriptException[Failed to compile inline script [_value * 3] using lang [expression]]; nested: ArrayIndexOutOfBoundsException[7];
    at org.elasticsearch.script.ScriptService.compileInternal(ScriptService.java:312)
    at org.elasticsearch.script.ScriptService.compile(ScriptService.java:256)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:455)
    at org.elasticsearch.search.aggregations.support.ValuesSourceParser.createScript(ValuesSourceParser.java:230)
    at org.elasticsearch.search.aggregations.support.ValuesSourceParser.config(ValuesSourceParser.java:224)
    at org.elasticsearch.search.aggregations.metrics.NumericValuesSourceMetricsAggregatorParser.parse(NumericValuesSourceMetricsAggregatorParser.java:66)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:198)
    at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:103)
    at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:752)
    ... 10 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 7
    at org.antlr.v4.runtime.atn.ParserATNSimulator.addDFAEdge(ParserATNSimulator.java:1956)
    at org.antlr.v4.runtime.atn.ParserATNSimulator.computeTargetState(ParserATNSimulator.java:609)
    at org.antlr.v4.runtime.atn.ParserATNSimulator.execATN(ParserATNSimulator.java:485)
    at org.antlr.v4.runtime.atn.ParserATNSimulator.adaptivePredict(ParserATNSimulator.java:424)
    at org.apache.lucene.expressions.js.JavascriptParser.expression(JavascriptParser.java:494)
    at org.apache.lucene.expressions.js.JavascriptParser.compile(JavascriptParser.java:113)
    at org.apache.lucene.expressions.js.JavascriptCompiler.getAntlrParseTree(JavascriptCompiler.java:216)
    at org.apache.lucene.expressions.js.JavascriptCompiler.compileExpression(JavascriptCompiler.java:189)
    at org.apache.lucene.expressions.js.JavascriptCompiler.compile(JavascriptCompiler.java:122)
    at org.elasticsearch.script.expression.ExpressionScriptEngineService$1.run(ExpressionScriptEngineService.java:107)
    at org.elasticsearch.script.expression.ExpressionScriptEngineService$1.run(ExpressionScriptEngineService.java:102)
    at java.security.AccessController.doPrivileged(Native Method)
    at org.elasticsearch.script.expression.ExpressionScriptEngineService.compile(ExpressionScriptEngineService.java:102)
    at org.elasticsearch.script.ScriptService.compileInternal(ScriptService.java:310)
```

The failure does not reproduce so I'm wondering there might be a concurrency issue here?
</description><key id="109842694">13949</key><summary>lucene-expressions fail at compiling a simple script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Scripting</label><label>bug</label></labels><created>2015-10-05T17:05:19Z</created><updated>2015-10-19T10:50:18Z</updated><resolved>2015-10-19T10:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-05T17:46:52Z" id="145610554">I think its this bug: https://github.com/antlr/antlr4/issues/804

Lets upgrade to antlr 4.5.1, please fix this in lucene!
</comment><comment author="clintongormley" created="2015-10-06T14:01:05Z" id="145864834">@jdconrad see ^^
</comment><comment author="jdconrad" created="2015-10-06T22:52:57Z" id="146026952">Just saw this.  Will take a look at fixing this tonight.
</comment><comment author="jdconrad" created="2015-10-06T23:28:15Z" id="146032949">@rmuir, Good find on the bug.  Thanks!  Attached a patch here (https://issues.apache.org/jira/browse/LUCENE-6830) for Lucene.
</comment><comment author="clintongormley" created="2015-10-19T10:50:18Z" id="149183495">Fixed in Lucene 5.4
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportNodesAction shouldn't hold on to cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13948</link><project id="" key="" /><description>Long running TransportNodesAction requests can retain old cluster states in memory for much longer than needed. This can cause nodes with frequent cluster state updates and long running requests to run out of memory.

This problem is especially pronounced on large tribe nodes that send a large number of node requests and have frequently updated cluster state.
</description><key id="109842261">13948</key><summary>TransportNodesAction shouldn't hold on to cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>:Tribe Node</label><label>enhancement</label><label>v1.7.3</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-05T17:02:37Z</created><updated>2016-03-10T18:15:35Z</updated><resolved>2015-10-08T00:51:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-06T08:58:10Z" id="145790270">left one question... 
</comment><comment author="imotov" created="2015-10-06T16:21:08Z" id="145917706">@bleskes is this what you had in mind?
</comment><comment author="bleskes" created="2015-10-06T19:18:22Z" id="145970301">LGTM
</comment><comment author="s1monw" created="2015-10-07T18:54:27Z" id="146292859">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove UpdateTests' dependency on groovy.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13947</link><project id="" key="" /><description>These tests had to be moved to lang-groovy when groovy has been made a plugin.
I refactored them a bit to use mock plugins instead so that groovy is not
necessary anymore and they can come back to core.
</description><key id="109841118">13947</key><summary>Remove UpdateTests' dependency on groovy.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T16:55:36Z</created><updated>2015-10-08T14:42:25Z</updated><resolved>2015-10-08T14:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-05T16:57:27Z" id="145598256">Woops @javanna worked on AliasesRoutingTests in parallel (and has a better solution). So you can just focus on UpdateTests for now.
</comment><comment author="javanna" created="2015-10-06T07:09:28Z" id="145764534">Left a comment on `AliasRoutingIT`. The update test looks great, thanks for taking the time to fix it.
</comment><comment author="jpountz" created="2015-10-06T13:14:13Z" id="145852597">I simplified a bit the script engine API in master (removing two methods) and rebased this PR so that less ceremony is required to implement a script engine. Is it good enough? I'm a bit on the fence to extend MockScriptEngine as it might encourage to add hacks to the base class when needed for some tests, so I tend to like keeping things separated. I suspect part of the issue is that our scripting API is a bit too complicated for what it does?
</comment><comment author="javanna" created="2015-10-06T13:42:07Z" id="145859845">I agree with you @jpountz I would merge as-is.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add options for indices.get feature</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13946</link><project id="" key="" /><description>As described here https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-index.html?q=get%20index#_filtering_index_information
</description><key id="109839914">13946</key><summary>Add options for indices.get feature</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:REST</label></labels><created>2015-10-05T16:48:43Z</created><updated>2015-10-06T18:28:14Z</updated><resolved>2015-10-06T18:28:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-05T17:04:23Z" id="145599891">LGTM
</comment><comment author="clintongormley" created="2015-10-06T14:00:03Z" id="145864592">Hmmm doesn't this clash with eg the `GET index/_settings` API etc?
</comment><comment author="gmarz" created="2015-10-06T14:38:32Z" id="145876670">I think the difference is that indices.get allows you to retrieve multiple features (_settings, _aliases, etc..) all in the same call, which isn't possible with the other APIs since they are feature specific.
</comment><comment author="clintongormley" created="2015-10-06T17:54:11Z" id="145945694">ah right, i wasn't aware of that!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] move back some test from groovy plugin to core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13945</link><project id="" key="" /><description>relates to #13837
</description><key id="109834398">13945</key><summary>[TEST] move back some test from groovy plugin to core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T16:17:04Z</created><updated>2015-10-06T07:34:38Z</updated><resolved>2015-10-06T07:29:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-05T16:43:36Z" id="145594832">LGTM - left a minor comment.

Heroic effort, btw
</comment><comment author="jpountz" created="2015-10-06T07:25:41Z" id="145766795">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unneeded Module abstractions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13944</link><project id="" key="" /><description>These abstractions don't really do anything nor can they be extended.
We can just fold them into IndexModule for now. There are more but they
are tricky due to some test dependencies which I need to resolve first.
</description><key id="109823960">13944</key><summary>Remove unneeded Module abstractions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T15:34:54Z</created><updated>2015-10-05T15:43:24Z</updated><resolved>2015-10-05T15:43:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-10-05T15:36:04Z" id="145572842">LGTM ++
</comment><comment author="nik9000" created="2015-10-05T15:36:06Z" id="145572851">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>shard failed on field indexing failure (elasticsearch 1.7.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13943</link><project id="" key="" /><description>Upgrading from elastiscearch 1.7.1 to 1.7.2 shards started failing/initializing on field data index failure instead of just failing to index. This results in constant ongoing shard failures and initialization.

Elasticsearch 1.7.2 on Ubuntu Trusty with Oracle JDK 1.8u40

[error] field containing bad data:

```
[2015-10-02 21:24:18,761][WARN ][cluster.action.shard     ] [elk-master003] [logstash-main-index-3-2015.10.02][3] received shard failed for [logstash-main-index-3-2015.10.02][3], node[85YLlAjAQT-q1dg1m3__aQ], relocating [RnkZoiSXTSiEhiKc4i3pZA], [P], s[INITIALIZING], indexUUID [WGwZFLMDSRaQEU69td2mFA], reason [shard failure [failed recovery][RecoveryFailedException[[logstash-main-index-3-2015.10.02][3]: Recovery failed from [elk-data009][RnkZoiSXTSiEhiKc4i3pZA][elk-data009.acme.com][inet[/10.143.15.236:9300]]{tag_type=prod, max_local_storage_nodes=1, tag_dc=dal09, master=false, tag_tier=elk-data} into [elk-data010][85YLlAjAQT-q1dg1m3__aQ][elk-data010.acme.com][inet[/10.143.15.242:9300]]{tag_type=prod, max_local_storage_nodes=1, tag_dc=dal09, master=false, tag_tier=elk-data}]; nested: RemoteTransportException[[elk-data009][inet[/10.143.15.236:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[logstash-main-index-3-2015.10.02][3] Phase[2] Execution failed]; nested: RemoteTransportException[[elk-data010][inet[/10.143.15.242:9300]][internal:index/shard/recovery/translog_ops]]; nested: MapperParsingException[object mapping for [app86] tried to parse field [error] as object, but got EOF, has a concrete value been provided to it?]; ]]
```

[activity_id] field was automatically set as "date" by first doc ingested when that field is usually a string resulting in subsequent field data failing to parse as date. This should Not fail shards and never did before upgrading to 1.7.2

```
[2015-10-05 12:31:03,699][WARN ][cluster.action.shard     ] [elk-master003] [logstash-main-index-2015.10.05][29] received shard failed for [logstash-main-index-2015.10.05][29], node[-B4M-bYsTiuxeGsyoCJ13A], [R], s[INITIALIZING], unassigned_info[[reason=ALLOCATION_FAILED], at[2015-10-05T12:30:53.962Z], details[shard failure [failed recovery][RecoveryFailedException[[logstash-main-index-2015.10.05][29]: Recovery failed from [elk-data006][tN1yZkfQTE-NeYp9SpwZtw][elk-data006.acme.com][inet[/10.143.15.166:9300]]{tag_type=prod, max_local_storage_nodes=1, tag_dc=dal09, master=false, tag_tier=elk-data} into [elk-data008][I6ikDzyoQZC2SJmQeKg5Jw][elk-data008.acme.com][inet[/10.143.15.181:9300]]{tag_type=prod, max_local_storage_nodes=1, tag_dc=dal09, master=false, tag_tier=elk-data}]; nested: RemoteTransportException[[elk-data006][inet[/10.143.15.166:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[logstash-main-index-2015.10.05][29] Phase[2] Execution failed]; nested: RemoteTransportException[[elk-data008][inet[/10.143.15.181:9300]][internal:index/shard/recovery/translog_ops]]; nested: MapperParsingException[failed to parse [activity_id]]; nested: MapperParsingException[failed to parse date field [01B56E68-901E-4256-ABB5-0537FB51AE99], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: "01B58E68-921E-4256-ACB5-0537FB21AE99" is malformed at "B58E68-921E-4256-ACB5-0537FB21AE99"]; ]]], indexUUID [5vHsvD-aSk2cLw_IzTO3XA], reason [shard failure [failed recovery][RecoveryFailedException[[logstash-main-index-2015.10.05][29]: Recovery failed from [elk-data006][tN1yZkfQTE-NeYp9SpwZtw][elk-data006.acme.com][inet[/10.143.15.166:9300]]{tag_type=prod, max_local_storage_nodes=1, tag_dc=dal09, master=false, tag_tier=elk-data} into [elk-data001][-B4M-bYsTiuxeGsyoCJ13A][elk-data001.acme.com][inet[/10.143.15.244:9300]]{tag_type=prod, max_local_storage_nodes=1, tag_dc=dal09, master=false, tag_tier=elk-data}]; nested: RemoteTransportException[[elk-data006][inet[/10.143.15.166:9300]][internal:index/shard/recovery/start_recovery]]; nested: RecoveryEngineException[[logstash-main-index-2015.10.05][29] Phase[2] Execution failed]; nested: RemoteTransportException[[elk-data001][inet[/10.143.15.244:9300]][internal:index/shard/recovery/translog_ops]]; nested: MapperParsingException[failed to parse [activity_id]]; nested: MapperParsingException[failed to parse date field [01B58E68-921E-4256-ACB5-0537FB21AE99], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: "01B58E68-921E-4256-ACB5-0537FB21AE99" is malformed at "B58E68-921E-4256-ACB5-0537FB21AE99"]; ]]
```
</description><key id="109806463">13943</key><summary>shard failed on field indexing failure (elasticsearch 1.7.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thomasquatorze</reporter><labels /><created>2015-10-05T14:19:53Z</created><updated>2015-10-06T17:51:11Z</updated><resolved>2015-10-06T13:50:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-06T13:50:54Z" id="145862027">Hi @thomasquatorze 

The first exception indicates that you have a corrupted translog.  You'll need to delete it in order to continue.  2.0 no longer allows translog corruptions.

The second error is probably a result of two shards trying to add the same field at the same time, one as a date and one as a string. The date version won, but the string version continued to be used on the losing shard.  Now, when moving the shard to another node, official mapping has been applied.

This has also been fixed in 2.0: dynamic mappings are now added synchronously.
</comment><comment author="thomasquatorze" created="2015-10-06T13:56:24Z" id="145863548">Hi @clintongormley 
Thanks for following up. The second condition went on for 12 hours with all shards continuously failing and initializing. This was not a one shard one time incident. It also does not explain why a shard would fail on indexing instead of just failing to index the document.
</comment><comment author="clintongormley" created="2015-10-06T17:51:11Z" id="145944752">@thomasquatorze here are the important bits of the exception:

```
[RecoveryFailedException[[logstash-main-index-2015.10.05]
[internal:index/shard/recovery/translog_ops]
; nested: MapperParsingException[failed to parse [activity_id]
; nested: MapperParsingException[failed to parse date field [01B56E68-901E-4256-ABB5-0537FB51AE99]
, tried both date format [dateOptionalTime]
, and timestamp number with locale []
]
; nested: IllegalArgumentException[Invalid format: "01B58E68-921E-4256-ACB5-0537FB21AE99" is malformed at "B58E68-921E-4256-ACB5-0537FB21AE99"]
```

This exception fails while trying to recover a shard replica, during the translog phase.  The document in the translog has a string field where the mapping expects a date field.

However this occurred (and the scenario I described above is the most likely), this would cause a recovery exception.  Elasticsearch would then try to recover this shard on another node and have the same problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify similarity module and friends</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13942</link><project id="" key="" /><description>SimilarityModule was binding two different interfaces SimilaritySerivce and SimilarityLookupSerice.
Both used a class Similarities which was holding some default impls etc. Thit commit folds
all the logic into a rather simplified SimilarityService which has not construction time dependency to
any other service in the system anymore. It's soely constructued from custom similarities, the index name
and index settings and SimilarityModule is just trivial glue code with out much logic.
This also adds a simple unittest for basic test coverage of the service.
</description><key id="109806300">13942</key><summary>Simplify similarity module and friends</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T14:19:01Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-07T18:51:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-05T15:45:31Z" id="145576016">LGTM - didn't do a super thorough review but it looks right.
</comment><comment author="rjernst" created="2015-10-05T20:04:10Z" id="145652069">Fantastic! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>First cleanup of the WIP commit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13941</link><project id="" key="" /><description>1. only update pipeline if the content has been changed
2. split the actual fetching of pipeline docs from the pipeline store to make unit testing easier
3. replaced hardcoded processor lookups with simple map registry
</description><key id="109805483">13941</key><summary>First cleanup of the WIP commit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Ingest</label></labels><created>2015-10-05T14:14:26Z</created><updated>2015-10-06T13:03:39Z</updated><resolved>2015-10-06T13:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-10-06T11:41:02Z" id="145830839">left some comments, OTT LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Removed obsoleted MVEL paragraph</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13940</link><project id="" key="" /><description /><key id="109795388">13940</key><summary>Docs: Removed obsoleted MVEL paragraph</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>docs</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T13:25:09Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-10-06T16:56:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-05T13:30:58Z" id="145528093">LGTM
</comment><comment author="nik9000" created="2015-10-05T13:32:17Z" id="145528611">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and ban ImmutableMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13939</link><project id="" key="" /><description>Removes and bans `ImmutableMap`. This is one of the last blockers for #13224.
</description><key id="109794476">13939</key><summary>Remove and ban ImmutableMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T13:20:05Z</created><updated>2015-10-09T16:56:17Z</updated><resolved>2015-10-09T16:56:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-05T13:22:38Z" id="145526094">I'm trying to do this in a minimally invasive way - no switching to `Streams` API.

I've added a forbidden-apis check to stop us from using `unmodifiableMap` in the `org.elasticsearch.cluster` package, with the intent of forcing us into using ImmutableOpenMap that is similarly compact.
</comment><comment author="nik9000" created="2015-10-05T13:31:47Z" id="145528431">I should not that there are two kinds of concerns around using `unmodifiableMap` in the `org.elasticsearch.cluster` package:
1. Space: `unmodifiableMap` wrapping a `HashMap` presumably takes more space than `ImmutableMap`. `ImmutableMap` uses open addressing and `HashMap` uses chains and/or balanced trees to resolve conflicts.
2. Heap churn: cluster state is immutable - meaning that an object reference into cluster state will always reference the same cluster state. To change the cluster state build copies of it. This involves a lot of copying, not copying the entire thing, but still lots of copying. Larger data structures like `HashMap` involve more allocations which in turn will cause more stop the world pauses even in a healthy environment.

In this PR I aim to keep cluster state's maps in ones that use open addressing to keep them small-ish. So to not make number 1 above worse.

It would be interesting to investigate [persistent data structures](https://en.wikipedia.org/wiki/Persistent_data_structure) like `CopyOnWriteHashMap` which is a [hash array mapped trie](http://en.wikipedia.org/wiki/Hash_array_mapped_trie). But that should await another pull request.
</comment><comment author="nik9000" created="2015-10-06T16:32:25Z" id="145920614">@jasontedor ready for review! Tons of little changes.
</comment><comment author="jasontedor" created="2015-10-08T18:40:07Z" id="146648974">@nik9000 Any chance that you could rebase on master? There are a lot of unrelated changes because of the merge of master into this branch.
</comment><comment author="nik9000" created="2015-10-08T18:42:38Z" id="146649604">&gt; @nik9000 Any chance that you could rebase on master? There are a lot of unrelated changes because of the merge of master into this branch.

I can certainly merge master into it again if that'll help. Rebasing on master would be harder I think and I thought that wasn't the recommended thing anyway when you have large-ish branches like this.
</comment><comment author="jasontedor" created="2015-10-08T18:45:22Z" id="146650723">@nik9000 Merging master in won't help, it will just add more noise to this diff. It's rebasing that will eliminate the noise because then the diff between what you merged in and this branch will be zero except for the changes that you made.

You'll have to rebase on master at some point anyway. :)

That said, if you think that rebasing on master will slow down the review process, then can you revert the commits ba68a8df63f45af12642b0874ac64dd4841fc832 and bb2611d2f55f8a07e6a33cb1c40059eed50f33c1?

My desire is to remove the noise that isn't from your change, but is instead from changes that took place in master.
</comment><comment author="nik9000" created="2015-10-09T13:38:07Z" id="146873584">&gt; @nik9000 Merging master in won't help, it will just add more noise to this diff.

I talked with @jasontedor over video and chat and we came to the conclusion that github is broken for pull requests of this size. Really the only way to handle it is to pull the patch locally and diff it against master. We just don't trust github for PRs this large. We'll need to avoid pull requests of this size in future. Luckily they are reasonably rare for single features.

The talk about merging master into the pull request was a red herring - the pull request was really just that big.
</comment><comment author="jasontedor" created="2015-10-09T14:16:17Z" id="146883759">@nik9000 This is great work, and very much appreciated. I left a few comments but otherwise LGTM. Let's get this home.
</comment><comment author="nik9000" created="2015-10-09T15:40:30Z" id="146906533">&gt; @nik9000 This is great work, and very much appreciated. I left a few comments but otherwise LGTM. Let's get this home.

I believe I've addressed you points. Have a look at  bfb9054, 56318df, and 9492223.

I'm going to merge master into this again to see if I can get a clean merge back to master when it comes time to merge it.
</comment><comment author="jasontedor" created="2015-10-09T16:03:14Z" id="146911781">Yes, looks great. Just one more thing. I think the forbidden-api changes were helpful for development, but let's take them out before taking this PR to master?
</comment><comment author="nik9000" created="2015-10-09T16:06:21Z" id="146912436">&gt; Yes, looks great. Just one more thing. I think the forbidden-api changes were helpful for development, but let's take them out before taking this PR to master?

Done in beeea14.
</comment><comment author="jasontedor" created="2015-10-09T16:07:53Z" id="146912785">LGTM.
</comment><comment author="nik9000" created="2015-10-09T16:09:29Z" id="146913135">&gt; LGTM.

Cool. I'll rerun test yet another time (the merge didn't go as well as I thought) and if they pass I'll merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: simplify IndexQueryParserService parse methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13938</link><project id="" key="" /><description>Query refactoring: simplify IndexQueryParserService parse methods and prepare the field for #13859

Relates to #13859
</description><key id="109789379">13938</key><summary>Query refactoring: simplify IndexQueryParserService parse methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T12:51:36Z</created><updated>2015-10-06T13:44:23Z</updated><resolved>2015-10-05T13:06:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-10-05T12:54:34Z" id="145519159">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IdsQueryBuilder to accept only non null ids and types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13937</link><project id="" key="" /><description>Types are still optional, but if you do provide them, they can't be null. Split the existing constructor that accepted nnull into two, one that accepts no arguments, and another one that accepts the types argument, which must be not null.

Also trimmed down different ways of setting ids, some were misleading as they would always add the ids to the existing ones and not set them, the add prefix makes that clear. Left `addIds` method that accepts a varargs argument. Added check for ids not be null.

Note that this change is breaking for the java api.
</description><key id="109785647">13937</key><summary>IdsQueryBuilder to accept only non null ids and types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T12:29:05Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-05T13:15:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-10-05T12:53:11Z" id="145518737">LGTM
</comment><comment author="javanna" created="2015-10-05T13:15:05Z" id="145523794">Merged https://github.com/elastic/elasticsearch/commit/e8653f51569580f23ca28a91106d90b07e989167
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dumps to elasticsearch from ntopng have incorrect timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13936</link><project id="" key="" /><description>Only 3 out of 400.000 dumped flows have the correct timestamp. 
I dont know why specifically 28th of August as it does not relate to any milestone (i.e installing something or starting a service etc)

root@nTop:/etc/ntopng# cat ntopng.conf
-w=3000
-W=0
-m="192.168.0.0/24"
-F=es;flows;ntopng-%Y.%m.%d;http://localhost:9200/_bulk;password
-d=/storage/ntopng
-G=/var/tmp/ntopng.pid
-i=tcp://127.0.0.1:5556

Some dumped flows

October 5th 2015, 02:45:17.000 @timestamp:October 5th 2015, 02:45:17.000 type:flows IPV4_SRC_ADDR:140.205.159.56 L4_SRC_PORT:0 IPV4_DST_ADDR:0.0.0.0 L4_DST_PORT:0 PROTOCOL:6 TCP_FLAGS:0 IN_PKTS:5 IN_BYTES:405 OUT_PKTS:0 OUT_BYTES:0 FIRST_SWITCHED:1,440,789,636 LAST_SWITCHED:1,440,789,636 json.5:0 json.9:0 json.10:0 json.13:0 json.14:0 json.15:0.0.0.0 json.16:0 json.17:0 json.42:531762 CLIENT_NW_LATENCY_MS:0 SERVER_NW_LATENCY_MS:0 _id:AVA1dbrJKInKjCg_uSAI _type:flows _index:ntopng-2015.10.05

October 5th 2015, 02:15:05.000 @timestamp:October 5th 2015, 02:15:05.000 type:flows IPV4_SRC_ADDR:121.214.55.2 L4_SRC_PORT:0 IPV4_DST_ADDR:0.0.0.0 L4_DST_PORT:0 PROTOCOL:17 L7_PROTO:0 L7_PROTO_NAME:Unknown IN_PKTS:36 IN_BYTES:1,779 OUT_PKTS:0 OUT_BYTES:0 FIRST_SWITCHED:1,440,789,635 LAST_SWITCHED:1,440,789,636 json.5:0 json.9:0 json.10:0 json.13:0 json.14:0 json.15:0.0.0.0 json.16:1221 json.17:0 json.42:258897 SRC_IP_COUNTRY:AU SRC_IP_LOCATION:144.967, -37.817 _id:AVA1Wg8kKInKjCg_tfu9 _type:flows _index:ntopng-2015.10.05

October 5th 2015, 02:05:09.000 @timestamp:October 5th 2015, 02:05:09.000 type:flows IPV4_SRC_ADDR:62.210.205.156 L4_SRC_PORT:0 IPV4_DST_ADDR:0.0.0.0 L4_DST_PORT:0 PROTOCOL:6 L7_PROTO:0 L7_PROTO_NAME:Unknown TCP_FLAGS:0 IN_PKTS:104 IN_BYTES:146,553 OUT_PKTS:0 OUT_BYTES:0 FIRST_SWITCHED:1,440,789,635 LAST_SWITCHED:1,440,789,635 json.5:0 json.9:0 json.10:0 json.13:0 json.14:0 json.15:0.0.0.0 json.16:12876 json.17:0 json.42:170919 CLIENT_NW_LATENCY_MS:0 SERVER_NW_LATENCY_MS:0 SRC_IP_COUNTRY:FR SRC_IP_LOCATION:2.333, 48.867 _id:AVA1UPf4KInKjCg_tPkX _type:flows _index:ntopng-2015.10.05

August 28th 2015, 21:20:36.000 @timestamp:August 28th 2015, 21:20:36.000 type:flows IPV4_SRC_ADDR:46.188.121.201 L4_SRC_PORT:0 IPV4_DST_ADDR:0.0.0.0 L4_DST_PORT:0 PROTOCOL:17 IN_PKTS:1 IN_BYTES:99 OUT_PKTS:0 OUT_BYTES:0 FIRST_SWITCHED:1,440,789,636 LAST_SWITCHED:1,440,789,636 json.5:0 json.9:0 json.10:0 json.13:0 json.14:0 json.15:0.0.0.0 json.16:0 json.17:0 json.42:965 _id:AVA1QBnEKInKjCg_sw0B _type:flows _index:ntopng-2015.10.04

August 28th 2015, 21:20:36.000 @timestamp:August 28th 2015, 21:20:36.000 type:flows IPV4_SRC_ADDR:203.215.120.85 L4_SRC_PORT:0 IPV4_DST_ADDR:0.0.0.0 L4_DST_PORT:0 PROTOCOL:17 IN_PKTS:1 IN_BYTES:48 OUT_PKTS:0 OUT_BYTES:0 FIRST_SWITCHED:1,440,789,636 LAST_SWITCHED:1,440,789,636 json.5:0 json.9:0 json.10:0 json.13:0 json.14:0 json.15:0.0.0.0 json.16:6648 json.17:0 json.42:724 SRC_IP_COUNTRY:PH SRC_IP_LOCATION:121.05, 14.633 _id:AVA1QBnsKInKjCg_sw0T _type:flows _index:ntopng-2015.10.04
</description><key id="109783094">13936</key><summary>dumps to elasticsearch from ntopng have incorrect timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kthkaya</reporter><labels /><created>2015-10-05T12:10:45Z</created><updated>2015-10-06T13:43:06Z</updated><resolved>2015-10-06T13:43:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-06T13:43:06Z" id="145860033">I'm afraid I have no idea what you're talking about, and this doesn't seem to be the right place to ask the question.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hyphenation decompounder malfunction?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13935</link><project id="" key="" /><description>Hi,
I'm trying to use the hyphenation decompounder as described here to split the Norwegian word "chaplinpris" into its two word-parts "chaplin" and "pris", but having no luck: Following instructions as put down in this page:
https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-compound-word-tokenfilter.html

I handcrafted my own hyphenation pattern file:

```
&lt;hyphenation-info&gt;
&lt;hyphen-min before="2" after="2"/&gt;
&lt;patterns&gt;
chaplin7pris
&lt;/patterns&gt;
&lt;/hyphenation-info&gt;
```

The 7 between chaplin and pris should mean that this place is highly eligible for a word-split. (some documentation for the patterns found here http://xmlgraphics.apache.org/fop/1.1/hyphenation.html#patterns)

The analysis settings:

```
PUT / wiki / _settings 
{
    "settings": {
        "analysis": {
            "analyzer": {
                "hyph_decoumpound_list": {
                    "tokenizer": "standard",
                    "filter": [
                        "standard",
                        "lowercase",
                        "hyph_decompound_list"
                    ]
                }
            },
            "filter": {
                "hyph_decompound_list": {
                    "type": "hyphenation_decompounder",
                    "word_list": ["pris", "chaplin"],
                    "hyphenation_patterns_path": "config/lang/hyphenation/test.xml"
                }
            }
        }
    }
}
```

Testing analysis

```
GET /wiki/_analyze?analyzer=hyph_decoumpound_list&amp;text=chaplinpris
```

returns no splits:

```
{
   "tokens": [
      {
         "token": "chaplinpris",
         "start_offset": 0,
         "end_offset": 11,
         "type": "&lt;ALPHANUM&gt;",
         "position": 1
      }
   ]
}
```

Using the same analysis setting just replacing type hyphenation decompounder with the dictionary decompounder works fine. So it seems the hyphenation decompounder doesn't do what it's supposed to? 
</description><key id="109778020">13935</key><summary>Hyphenation decompounder malfunction?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">babadofar</reporter><labels><label>:Analysis</label><label>adoptme</label><label>bug</label></labels><created>2015-10-05T11:33:49Z</created><updated>2016-10-19T12:38:05Z</updated><resolved>2016-03-11T14:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-06T13:33:16Z" id="145857065">Hi @babadofar 

I've just had a play with it in 2.0 and am seeing the same thing.  I haven't used it before, so may be missing something obvious.  Requires investigation...
</comment><comment author="babadofar" created="2015-10-12T22:35:47Z" id="147540871">Thanks! Glad to not be the only one with this experience ;) 
But really, even if the hyphenation decompounder did do what it says, I'm not sure if it would be helpful in regards to splitting up compound words in sensible places. The hyphenation patterns from TeX are meant as guidelines to where words should be split in order to make text flow in a readable manner across the screen. The word parts produced by the hyphenation algorithm do not quite correspond to meaningful words.  The dictionary decompounder actually comes closer, with it's brute force logic. To me it seems that the dictionary decompounder would do a great job with Scandinavian compound words with only two additions: 
1. Don't try to find all possible words. Try to match enough words to completely fill the original, nothing more or less. Except...
2. Allow a (short) predefined list of [interfixes](https://en.wikipedia.org/wiki/Interfix) that may be found in between sub-words.

Example for 1:

Dictionary contains:  `arbeider, arbeid, er, partiet, parti, et`
Compound word: `arbeiderpartiet`
Should be split into: `arbeider, partiet` 
Do not add: `er, et, parti`

Example for 2:

Dictionary contains:  `barn, hage, arn, arne, age`
Interfix list contains: `e, s`
Compound word: `barnehage`
Should be split into: `barn, hage` 
Remove interfix `e`
Do not add: `arn, arne, age`
</comment><comment author="babadofar" created="2015-11-23T08:02:07Z" id="158873009">Idea: remove mentions of the hyphenation decompounder from the documentation 
</comment><comment author="udit7590" created="2016-01-07T09:42:02Z" id="169611110">@babadofar There is an option for that(Do not add: arn, arne, age): **only_longest_match: true**
But that's not working as well for me. Donno if I am getting it wrong.
Also hyphenation_decompounder is not working in my case as well.
</comment><comment author="clintongormley" created="2016-03-11T14:11:20Z" id="195380852">@babadofar Found the answer.  The token filter in Lucene only supports FOP v1.2 compatible hyphenation patterns.  v2.0 and above is not supported.  I've updated the docs to link to the supported format: https://github.com/elastic/elasticsearch/commit/9a851a58b95b17250f2c6bb4543370371b193fc4
</comment><comment author="clintongormley" created="2016-03-11T14:12:00Z" id="195381188">Hopefully Lucene will add support for later versions, then we will inherit that
</comment><comment author="iamjochem" created="2016-10-19T12:38:05Z" id="254799796">I just tried using the `hyphenation_decompounder` filter with the v1.2 FOP files and found that it still does nothing (ES logs no errors either), changing the filter type to `dictionary_decompounder` made it work. 

| Info | ... |
| --- | --- |
| ES version | 2.2 |
| FOP XML file | nl.xml |
| Dictionary Words | mes, messen, pot, vis |

my filter config looks like so:

``` json
{
    "analysis"    : {
        "filter"        : {
            "nl_dehyphenate": {
                "type"                      : "dictionary_decompounder",
                "word_list_path"            : "/path/to/relevant/config/files/nl.txt",
                "hyphenation_patterns_path" : "/path/to/relevant/config/files/nl.xml",
                "min_word_size"             : 6,
                "min_subword_size"          : 3,
                "max_subword_size"          : 15
            }
        }
    }
}
```

possibly (probably?) I'm doing something wrong ... but maybe this is an indication that the `hyphenation_decompounder` filter is borked somehow.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Settings in log config file should not overwrite custom parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13934</link><project id="" key="" /><description /><key id="109777818">13934</key><summary>Settings in log config file should not overwrite custom parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Logging</label><label>bug</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-05T11:32:07Z</created><updated>2015-11-22T10:16:43Z</updated><resolved>2015-10-05T11:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-05T11:42:13Z" id="145502656">LGTM can you please label the PR? ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin cli tool should not create empty log files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13933</link><project id="" key="" /><description>Plugin cli tools configures logging with whatever is in the logging.yml.
If a file appender is configured for any of the logs this will cause creation
of an empty log file. If a plugin was for example installed as root it will
create empty logs at es.home/logs.
This is problematic when for example plugins are installed as root and es is run
as service. Logs will then be created in /usr/share/elasticsearch/logs
and can later not be removed by for example dpkg -r or -purge.

To avoid this, configure the logger to use an appender that writes to the same
output that plugin cli tool does. This allows other components that are called
from Plugin cli tool to write to the same terminal that plugin cli tool writes to
by using the logging mechanism already in place.
The logging conf is not read at all pb plugin cli tool.

As a side effect, the loging level for components that are called
from the plugin command such as the jar hell check can now be configured
with -Des.logger.level which makes it easier to debug the jar hell check.
</description><key id="109776376">13933</key><summary>Plugin cli tool should not create empty log files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Logging</label><label>bug</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-05T11:20:48Z</created><updated>2015-11-22T10:16:43Z</updated><resolved>2015-10-06T12:13:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-10-05T12:52:24Z" id="145518404">left some minor comments, agreeing with the others, but this looks good. thx for digging!
</comment><comment author="brwe" created="2015-10-05T15:38:34Z" id="145574025">@dadoonet @nik9000 @spinscale thanks a lot for the review! I addressed all comments. 
</comment><comment author="nik9000" created="2015-10-05T15:46:59Z" id="145576391">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filtering in non indexed field, as well as non existing field should return an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13932</link><project id="" key="" /><description>I have opened this pull request because to discuss if we can return an exception when filtering/querying a field that is not indexed, or that does not exist. As an example of this:
## Querying a non-indexed field

```
PUT trial
{
  "mappings": {
    "document_type1": {
      "properties": {
        "prop1": {
          "index": "no",
          "type": "string"
        }
      }
    }
  }
}

POST trial/document_type1
{
  "prop1": "sure"
}

POST trial/_search
{
  "query": {
    "match": {
      "prop1": "sure"
    }
  }
}
```

This returns:

```
{
   "took": 13,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```
## Querying a non-existing field

```
POST trial/_search
{
  "query": {
    "match": {
      "prop2": "some text"
    }
  }
}
```

This returns:

```
{
   "took": 13,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 0,
      "max_score": null,
      "hits": []
   }
}
```

Both cases could return an exception, indicating that the field is not indexed, or does not exist. I've marked this issue as a discussion, since there are some cases such as querying multiple indices that could be a bit tricky to add consistency to the result of mixed results of existing fields on certain indices plus non-indexed/existing fields on other indices.
</description><key id="109765931">13932</key><summary>Filtering in non indexed field, as well as non existing field should return an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>discuss</label><label>enhancement</label></labels><created>2015-10-05T10:03:53Z</created><updated>2017-05-15T23:22:01Z</updated><resolved>2015-10-06T13:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tonyd" created="2015-10-05T15:01:05Z" id="145559338">Returning no results without any indication that the query could not be answered seems semantically incorrect - given a data store and a well-formed query, "no results" should mean that there are no matches to the query in that store; the alternative seems like an invitation to latent defects. I understand your point about multiple indices, but that case seems even worse - if one index is unable to respond to the query, you might get results, just not all of them. Perhaps there could be an option to allow it if, for example, you are migrating to a new index with a different mapping.
</comment><comment author="clintongormley" created="2015-10-06T13:53:36Z" id="145862649">Duplicate of #12016
</comment><comment author="helenzBV" created="2017-05-15T22:09:27Z" id="301619302">@tonyd Sorry to bother you but I just have a question regarding searching non-indexed field. I have informed by a colleague that it is possible to search non-indexed field, and he stated that he have seen this was being done by an elastic search expert. I'm curious if this is true. (Interestingly, this question is also a very controversial question all over internet). If this happens to be true, would you mind educate me on how to do it? </comment><comment author="tonyd" created="2017-05-15T23:22:01Z" id="301632049">@helenzBV Hi, no, I don't know of any way to do that - which doesn't mean there isn't one, but see  #17748, which adds an option to give you an error if you search an unindexed field.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot restore and index creates should keep index settings and cluster blocks in sync</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13931</link><project id="" key="" /><description>Restoring an index from a snapshot or creating a new index can bring the index settings index.blocks.read_only, index.blocks.read, index.blocks.write and index.blocks.metadata out-of-sync with the corresponding cluster blocks.

For an example, see #13213.
</description><key id="109753426">13931</key><summary>Snapshot restore and index creates should keep index settings and cluster blocks in sync</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ywelsch</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-05T08:43:58Z</created><updated>2015-10-13T11:37:08Z</updated><resolved>2015-10-13T11:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-10-05T19:49:17Z" id="145647845">Left a couple of comments.
</comment><comment author="ywelsch" created="2015-10-06T14:13:34Z" id="145869344">@imotov Can you have another look? I simplified the whole thing now :-)
</comment><comment author="imotov" created="2015-10-07T13:15:52Z" id="146193214">LGTM
</comment><comment author="tlrx" created="2015-10-07T15:18:22Z" id="146227534">Left minor comments
</comment><comment author="ywelsch" created="2015-10-09T11:23:58Z" id="146838398">@clintongormley Targeting this for 2.1 and above would be ok here I guess?
</comment><comment author="clintongormley" created="2015-10-13T07:31:06Z" id="147629204">@ywelsch ++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to gradle build system</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13930</link><project id="" key="" /><description>We currently have a very large maven build system, comprising of 6k lines of xml. While maven parent POMs have been very useful in de-duplicating build logic across elasticsearch and plugins, the rigidity of maven has required us to begin implementing most of our test logic through `maven-antrun-plugin`. Another major problem with multi-module builds is trap: building a module requires explicitly listing an upstream module in order to get a local build of the dependency, or building from a higher level module (which implicitly builds all sub-modules). And finally, the maven model is incapable of decoupling test dependencies from main dependencies, in that tests for module A, cannot depend on module B, which in turn depends on module A (this is _not_ a cycle).

I have a branch which contains elasticsearch switched to using gradle:
https://github.com/elastic/elasticsearch/tree/burn_maven_with_fire

This is a tracking issue for completing the migration to gradle. The tasks are separated into two groups: those remaining required before pushing to master (as to not completely break development), and those required for the migration to be complete, which must happen before the next release (tentatively 2.2).

Required for master push:
- [x] Maven publishing
- [x] Windows integ test support
- [x] Setup publishing of buildSrc (ie ES gradle build plugins)
- [x] Publish "attach project plugin" (allows dynamically including a multi-project build inside another build)

Required for release:
- [x] Copyright header check
- [x] Vagrant qa tests
- [x] Multi node qa tests (#14921)
- [x] Rpm/deb building complete
- [ ] Rpm signing
- [x] Distribution tests
- [x] Update plugins not in elasticsearch repo to use ES gradle build plugins (mapper attachment: elastic/elasticsearch-mapper-attachments#184)
- [ ] Document new ES gradle build plugins (for ES plugin authors)
- [ ] Document building/testing in README/TESTING (this is done except for jacoco reports)
- [x] integ tests parity: #14464

Addtional nice to haves:
- [x] Make qa plugins work in eclipse (need to configure custom classpath)
- [x] Add `run` task for elasticsearch and plugins to replace `run.sh`/`run.bat`: #14451
</description><key id="109752898">13930</key><summary>Switch to gradle build system</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>adoptme</label><label>build</label><label>Meta</label><label>v2.3.0</label></labels><created>2015-10-05T08:40:08Z</created><updated>2016-01-28T19:10:09Z</updated><resolved>2016-01-28T19:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-05T12:36:46Z" id="145514794">&gt; Document building/testing in README/TESTING

You should probably add _some_ of this to the pre-merge checklist to. Maybe just the some basic build instructions and a big note saying "`![under construction][underconstruction.gif]` All references to maven in this file are obsolete and will be replaced with gradle soon. We're very sorry for the inconvenience. `![under construction][underconstruction.gif]`"

&gt; Document building/testing in README/TESTING
&gt; Rpm/deb building complete
&gt; Distribution tests
&gt; Vagrant qa tests
&gt; Rpm signing

I'm cool with grabbing these later this week. If anyone that wants them before I get to them then please take them with my best wishes.

&gt; Document new ES gradle build plugins (for ES plugin authors)

I'm cool with grabbing this later, maybe porting github.com/wikimedia/search-highlighter or something to prove it out.
</comment><comment author="rjernst" created="2015-10-05T22:25:28Z" id="145686882">I published a snapshot of the `project-attachment-plugin`. See the repo here:
https://github.com/elastic/gradle-project-attachment-plugin
And the snapshot is here:
https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/gradle/project-attachment-plugin/1.0.0-SNAPSHOT/
</comment><comment author="dadoonet" created="2015-10-06T03:39:23Z" id="145734063">I was wondering if we can contribute this plugin to Gradle as it sounds like to me a useful one for Gradle community. Is it doable at some point (when stable enough)?
</comment><comment author="rjernst" created="2015-10-06T03:41:35Z" id="145734241">The gradle folks are working on a longer term solution for combining multiple multi-project builds. This is just a semi-temporary solution.
</comment><comment author="dadoonet" created="2015-10-06T06:32:43Z" id="145753760">&gt; The gradle folks are working on a longer term solution for combining multiple multi-project builds.

Great news! 
</comment><comment author="nik9000" created="2015-10-21T01:39:33Z" id="149754002">&gt; I'm cool with grabbing these later this week. If anyone that wants them before I get to them then please take them with my best wishes.

And now that I'm finally done with `@Test` I can think of helping again! Hurray!
</comment><comment author="mrsolo" created="2015-10-21T17:17:22Z" id="149966893">I would add 'ability to do third party license analysis' as part of release requirement.
currently 3rd party uses [license maven plugin](http://www.mojohaus.org/license-maven-plugin/)
with 'mvn -P license site' call

gradle seems to have [one](https://github.com/hierynomus/license-gradle-plugin)
</comment><comment author="rjernst" created="2015-10-21T18:26:12Z" id="149985281">@mrsolo afaik, we only use the maven license plugin to check files have a license header?
</comment><comment author="mrsolo" created="2015-10-21T18:41:58Z" id="149989340">@rjernst we use http://code.mycila.com/license-maven-plugin/ to check the license header which is a different license plug in than http://www.mojohaus.org/license-maven-plugin/

we use depends on maven license plugin to do  third party library license auditing
</comment><comment author="mrsolo" created="2015-10-21T20:36:57Z" id="150017349">Hmm .. since 2.x, the licenses are managed in https://github.com/elastic/elasticsearch/tree/2.1/distribution/licenses and some if not all libraries are not shaded, this pretty much invalidate pre 2.x auditing workflo, gradle 3rd party dependencies therefore is nice to have but not necessary.

Auditing will be
1) Cross check content of distribution *.zip against information under distribution/licenses
2) Generate information for external consumption
</comment><comment author="rjernst" created="2015-10-21T20:41:48Z" id="150018489">#1 is already done in the port of the license checker (perl script on master). Not sure what you mean by #2
</comment><comment author="mrsolo" created="2015-10-21T20:45:55Z" id="150019430">#2 basically mean generation of google spreadsheet, currently not automated
</comment><comment author="nik9000" created="2015-10-30T16:55:41Z" id="152585660">Ok - I'm going to look at the vagrant tests.
</comment><comment author="nik9000" created="2015-11-05T19:33:38Z" id="154166241">&gt; Ok - I'm going to look at the vagrant tests.

I've got the vagrant tests working as well as I can without rpm and deb packages so I'm going to start work on fixing the deb package. 
</comment><comment author="nik9000" created="2015-11-18T19:18:44Z" id="157827848">&gt; except for instructions for vagrant tests and jacoco reports

Ok - I'm going to see about these.
</comment><comment author="clintongormley" created="2016-01-28T19:10:09Z" id="176349072">This is done, bar the docs for plugin authors (#15280). Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test failure: org.elasticsearch.indices.recovery.RecoverySourceHandlerTests.testHandleCorruptedIndexOnSendSendFiles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13929</link><project id="" key="" /><description>This test failure is reproducible: `mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=E325370F88FD9068 -Dtests.class=org.elasticsearch.indices.recovery.RecoverySourceHandlerTests -Dtests.method="testHandleCorruptedIndexOnSendSendFiles" -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.heap.size=1024m -Dtests.locale=sr -Dtests.timezone=America/Porto_Velho`.

See http://build-us-00.elastic.co/job/es_g1gc_master_metal/20088/testReport/junit/org.elasticsearch.indices.recovery/RecoverySourceHandlerTests/testHandleCorruptedIndexOnSendSendFiles/
</description><key id="109752708">13929</key><summary>Test failure: org.elasticsearch.indices.recovery.RecoverySourceHandlerTests.testHandleCorruptedIndexOnSendSendFiles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>jenkins</label></labels><created>2015-10-05T08:38:26Z</created><updated>2015-10-05T12:09:05Z</updated><resolved>2015-10-05T12:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-05T12:09:05Z" id="145507800">fixed by https://github.com/elastic/elasticsearch/pull/13923
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shall we remove the count api?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13928</link><project id="" key="" /><description>The count api is currently a shortcut to the search api with `size` set to `0`. We should discuss whether it makes sense to remove it given that one can simply use the `_search` api instead and set `size` to `0`.

When it comes to the REST layer, the lines of code required to maintain the shortcut to search are not many, but the java api requires a bit more, it would be nice to get rid of that and simplify things.

I see a few disadvantages on the client side:
- it's handy to have the _count endpoint for ease of use, although it is super easy to move to `_search?size=0`
- if you forget to set the `size` to `0` everything works, but more work is done under the hood
- last I can think of is the different response:

```
{
    "count" : 1,
    "_shards" : {
        "total" : 5,
        "successful" : 5,
        "failed" : 0
    }
}
```

compared to the search api response which is a bit more convoluted:

```
{
    "_shards":{
        "total" : 5,
        "successful" : 5,
        "failed" : 0
    },
    "hits":{
        "total" : 1
    }
}
```
</description><key id="109752634">13928</key><summary>Shall we remove the count api?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>enhancement</label></labels><created>2015-10-05T08:37:48Z</created><updated>2015-10-19T12:46:35Z</updated><resolved>2015-10-19T12:46:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T09:58:25Z" id="148672186">The `_count` API can be replaced easily by `_search?size=0`.  Removing it from the REST layer would remove some comvenient simple sugar, but removing it from the Java API really doesn't impact convenience.

Let's remove from the Java API and leave the REST endpoint in place, and keep the response the same as today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enhance plugin-descriptor.properties guide</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13927</link><project id="" key="" /><description>- extract doc from the descriptor
- make obvious that plugin authors will have to release a new version for each elasticsearch version
</description><key id="109749757">13927</key><summary>Enhance plugin-descriptor.properties guide</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>docs</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-10-05T08:13:59Z</created><updated>2016-03-10T18:15:35Z</updated><resolved>2015-10-06T20:17:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-05T08:16:19Z" id="145460339">@rmuir or @clintongormley do you agree with this doc?
</comment><comment author="clintongormley" created="2015-10-06T13:09:36Z" id="145851673">Added one comment.  Also, in https://github.com/dadoonet/elasticsearch/blob/doc/add-elasticsearch-version/dev-tools/src/main/resources/plugin-metadata/plugin-descriptor.properties#L28 the example uses an elasticsearch version of "2.0", which isn't correct. It should use a full version number.  Could you update that as well please?
</comment><comment author="dadoonet" created="2015-10-06T20:18:38Z" id="145987059">Thanks @clintongormley for the review.
Merged in 2.0 to master branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic search master branch test case failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13926</link><project id="" key="" /><description>Hi,

I have been to to build elastic search from source from the following master branch
https://github.com/elastic/elasticsearch.git

Without test cases the build is sucessfull .. following is teh instruction to build
mvn install -DskipTests=true

With test cases there are failures and i am unable to figure out the cause 
mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=93CDDBA6C9651B87 -Dtests.class=org.elasticsearch.index.similarity.SimilarityTests -Dtests.method="testResolveSimilaritiesFromMapping_DFR"

Requesting your inputs to resolve this
Below are the debug logs

Debug LOgs

Suite: org.elasticsearch.index.similarity.SimilarityTests
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=93CDDBA6C9651B87 -Dtests.class=org.elasticsearch.index.similarity.SimilarityTests -Dtests.method="testResolveSimilaritiesFromMapping_DFR" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=es_DO -Dtests.timezone=America/Argentina/ComodRivadavia
FAILURE 31.2s | SimilarityTests.testResolveSimilaritiesFromMapping_DFR &lt;&lt;&lt;

&gt; Throwable #1: java.lang.AssertionError:
&gt; Expected: a value less than or equal to &lt;YELLOW&gt;
&gt;      but: &lt;RED&gt; was greater than &lt;YELLOW&gt;
&gt;    at __randomizedtesting.SeedInfo.seed([93CDDBA6C9651B87:79B60E8EF528003F]:0)
&gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
&gt;    at org.elasticsearch.test.ESSingleNodeTestCase.createIndex(ESSingleNodeTestCase.java:212)
&gt;    at org.elasticsearch.test.ESSingleNodeTestCase.createIndex(ESSingleNodeTestCase.java:192)
&gt;    at org.elasticsearch.test.ESSingleNodeTestCase.createIndex(ESSingleNodeTestCase.java:181)
&gt;    at org.elasticsearch.index.similarity.SimilarityTests.testResolveSimilaritiesFromMapping_DFR(SimilarityTests.java:104)
&gt;    at java.lang.Thread.run(Thread.java:745)
&gt;   2&gt; NOTE: leaving temporary files on disk at: /home/elasticsearch/core/target/J0/temp/org.elasticsearch.index.similarity.SimilarityTests_93CDDBA6C9651B87-005
&gt;   2&gt; NOTE: test params are: codec=Asserting(Lucene53): {}, docValues:{}, sim=RandomSimilarityProvider(queryNorm=false,coord=no): {}, locale=es_DO, timezone=America/Argentina/ComodRivadavia
&gt;   2&gt; NOTE: Linux 3.10.0-229.1.2.el7.s390x s390x/Oracle Corporation 1.8.0_60 (64-bit)/cpus=1,threads=1,free=390243056,total=518979584
&gt;   2&gt; NOTE: All tests run in this JVM: [SimilarityTests]
&gt; Completed [1/1] in 42.91s, 1 test, 1 failure &lt;&lt;&lt; FAILURES!

[WARNING] JVM J0: stderr was not empty, see: /home/elasticsearch/core/target/junit4-J0-20151005_074526_147.syserr

Tests with failures:
- org.elasticsearch.index.similarity.SimilarityTests.testResolveSimilaritiesFromMapping_DFR

Ketan Kunde
</description><key id="109745121">13926</key><summary>Elastic search master branch test case failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ketan22584</reporter><labels><label>feedback_needed</label></labels><created>2015-10-05T07:46:40Z</created><updated>2016-01-18T10:49:41Z</updated><resolved>2016-01-18T10:49:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-05T12:31:29Z" id="145513002">This test failure suggests that there is something that prevented the index to be created on some nodes resulting in a red state. Do you have local modifications of elasticsearch? If not can you try to pull latest changes and run "mvn clean" before trying to build the package again?
</comment><comment author="ketan22584" created="2015-10-08T03:50:17Z" id="146411701">Thanks for the reply
I do not have local changes made to the source

I went by your suggestion to checkout fresh source  clean it and build it still the build fails
pasting the logs below

this are the instructions i followed to build it
Also just want to add that i am using jdk 1.8 and maven 3.3.3
mvn clean
mvn install

Your inputs will be helpful to get this test cases passing

Thanks in advance

Tests with failures:
- org.elasticsearch.test.rest.Rest7IT.test {yaml=index/10_with_id/Index with ID}

[INFO] JVM J0:     0.50 .. 10861.16 = 10860.66s
[INFO] Execution time total: 3 hours 1 minute 1 second
[INFO] Tests summary: 287 suites (1 ignored), 2108 tests, 1 error, 57 ignored (57 assumptions)
[INFO]
[INFO] --- maven-antrun-plugin:1.8:run (integ-tests-top-hints) @ elasticsearch ---
[INFO] Executing tasks

main:
 [tophints] 681.82s | org.elasticsearch.recovery.RecoveryWhileUnderLoadIT
 [tophints] 540.89s | org.elasticsearch.bwcompat.OldIndexBackwardsCompatibilityIT
 [tophints] 509.27s | org.elasticsearch.index.engine.InternalEngineMergeIT
 [tophints] 452.84s | org.elasticsearch.cluster.ClusterStateDiffIT
 [tophints] 415.85s | org.elasticsearch.cluster.allocation.ClusterRerouteIT
[INFO] Executed tasks
[INFO]
[INFO] --- maven-failsafe-plugin:2.18.1:verify (verify) @ elasticsearch ---
[INFO] Failsafe report directory: /home/elasticsearch/core/target/failsafe-reports
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Build Tools and Resources .......................... SUCCESS [ 20.128 s]
[INFO] Rest API Specification ............................. SUCCESS [  3.493 s]
[INFO] Elasticsearch: Parent POM .......................... SUCCESS [05:42 min]
[INFO] Elasticsearch: Core ................................ FAILURE [  04:08 h]
[INFO] Distribution: Parent POM ........................... SKIPPED
[INFO] Distribution: TAR .................................. SKIPPED
[INFO] Distribution: ZIP .................................. SKIPPED
[INFO] Distribution: Deb .................................. SKIPPED
[INFO] Plugin: Parent POM ................................. SKIPPED
[INFO] Plugin: Analysis: ICU .............................. SKIPPED
[INFO] Plugin: Analysis: Japanese (kuromoji) .............. SKIPPED
[INFO] Plugin: Analysis: Phonetic ......................... SKIPPED
[INFO] Plugin: Analysis: Smart Chinese (smartcn) .......... SKIPPED
[INFO] Plugin: Analysis: Polish (stempel) ................. SKIPPED
[INFO] Plugin: Cloud: Google Compute Engine ............... SKIPPED
[INFO] Plugin: Delete By Query ............................ SKIPPED
[INFO] Plugin: Discovery: Azure ........................... SKIPPED
[INFO] Plugin: Discovery: EC2 ............................. SKIPPED
[INFO] Plugin: Discovery: Multicast ....................... SKIPPED
[INFO] Plugin: Language: Expression ....................... SKIPPED
[INFO] Plugin: Language: Groovy ........................... SKIPPED
[INFO] Plugin: Language: JavaScript ....................... SKIPPED
[INFO] Plugin: Language: Python ........................... SKIPPED
[INFO] Plugin: Mapper: Murmur3 ............................ SKIPPED
[INFO] Plugin: Mapper: Size ............................... SKIPPED
[INFO] Plugin: Repository: Azure .......................... SKIPPED
[INFO] Plugin: Repository: S3 ............................. SKIPPED
[INFO] Plugin: Store: SMB ................................. SKIPPED
[INFO] Plugin: JVM example ................................ SKIPPED
[INFO] Plugin: Example site ............................... SKIPPED
[INFO] QA: Parent POM ..................................... SKIPPED
[INFO] QA: Smoke Test Plugins ............................. SKIPPED
[INFO] QA: Smoke Test Multi-Node IT ....................... SKIPPED
[INFO] QA: Smoke Test Client .............................. SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 04:14 h
[INFO] Finished at: 2015-10-07T14:53:13+00:00
[INFO] Final Memory: 69M/247M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-failsafe-plugin:2.18.1:verify (verify) on project elasticsearch: There are test failures.
[ERROR]
[ERROR] Please refer to /home/elasticsearch/core/target/failsafe-reports for the individual test results.
[ERROR] -&gt; [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &lt;goals&gt; -rf :elasticsearch
</comment><comment author="ketan22584" created="2015-10-23T10:12:06Z" id="150535332">Hi can anyone give me any inputs on the above issue
</comment><comment author="dadoonet" created="2015-10-23T10:18:58Z" id="150536343">Not really an answer but you could skip the tests if your goal is just to build a version.
</comment><comment author="ketan22584" created="2015-10-23T10:26:55Z" id="150537740">Thanks for the reply .. yes on skipping test my build is sucess but i need to get the test cases passing .
I see that some of you have been able to get it passing .. i just want to know is there something that i have been missed to get this test cases passed?
</comment><comment author="javanna" created="2015-10-23T10:33:50Z" id="150538759">I've seen this `index/10_with_id/Index with ID` failure before, it was due to the characters that are used as part of the index name. I think the fix was to export encoding related environment variables: `LANG=en_US.UTF-8` and `LC_ALL=en_US.UTF-8`. Have also a look at this similar issue: #8548 .
</comment><comment author="clintongormley" created="2016-01-18T10:49:41Z" id="172495818">No more feedback.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to create Location object in Groovy/Java</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13925</link><project id="" key="" /><description>I am getting latitude and longitude values from a rest API and i want to create a location object in groovy/java(may be using double arrays) and store it to Elasticsearch with other parameters. I know how to do this through logstash and wondering how to achieve it from groovy/java..

how to created the below object using groovy/java.
"location": {
"lat": "50.68262437",
"lon": "7.5592885"
}

Thanks in advance!!
</description><key id="109718190">13925</key><summary>How to create Location object in Groovy/Java</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rajeshdommati</reporter><labels /><created>2015-10-05T01:39:17Z</created><updated>2015-10-05T02:44:55Z</updated><resolved>2015-10-05T02:44:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-05T02:44:55Z" id="145414306">Please join us on discuss.elastic.co for questions.

Not an issue. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lock down javascript and python script engines better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13924</link><project id="" key="" /><description>These should run with minimal permissions like groovy does. 
</description><key id="109705191">13924</key><summary>Lock down javascript and python script engines better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Scripting</label><label>breaking</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-04T21:17:45Z</created><updated>2016-02-01T11:30:32Z</updated><resolved>2015-10-05T15:29:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-05T09:08:02Z" id="145469883">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Record all bytes of the checksum in VerifyingIndexOutput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13923</link><project id="" key="" /><description>The fix in #13848 has an off by one issue where the first byte of the checksum
was never written. Unfortunately most tests shadowed the problem and the first
byte of the checksum seems to be very likely a 0 which causes only very rare
failures.

Relates to #13896
Relates to #13848
</description><key id="109699084">13923</key><summary>Record all bytes of the checksum in VerifyingIndexOutput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-04T19:23:22Z</created><updated>2015-10-05T12:05:08Z</updated><resolved>2015-10-05T12:03:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-04T19:23:59Z" id="145379440">here is a test that reproduces and fails without the fix: `http://build-us-00.elastic.co/job/es_g1gc_master_metal/20034/`
</comment><comment author="jpountz" created="2015-10-05T09:01:28Z" id="145467984">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sample</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13922</link><project id="" key="" /><description>N/a
</description><key id="109695394">13922</key><summary>sample</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">emolifeinc</reporter><labels /><created>2015-10-04T18:31:25Z</created><updated>2015-10-04T18:50:12Z</updated><resolved>2015-10-04T18:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-10-04T18:50:12Z" id="145372855">Probably an error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>setting network.bind_host: localhost and publish_host: _ens192:ipv4_ uses localhost only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13921</link><project id="" key="" /><description>We wanted to set the REST API of an elasticsearch cluster to localhost and proxy all requests with a web proxy (apache), but to let the cluster find the nodes via the main interface. But it looks like it will bind to the local interface only.

Those are the ports opened by elasticsearch process:

```
# netstat -tulpn | grep 20438
tcp6       0      0 127.0.0.1:9200          :::*                    LISTEN      20438/java
tcp6       0      0 127.0.0.1:9300          :::*                    LISTEN      20438/java
udp6       0      0 :::54328                :::*                                20438/java
```

This is the relevant configuration from /etc/elasticsearch/es-01/elasticsearch.yml:

```
network:
  publish_host: _ens192:ipv4_
  bind_host: localhost
```

Those are the startup log messages:

```
starting ...
bound_address {inet[/127.0.0.1:9300]}, publish_address {inet[/10.220.10.105:9300]}
optymyze_Romania/XPSgcKuQSteNJfNwyWIsTg
new_master [v-so-repo-05.synygy.net-es-01][XPSgcKuQSteNJfNwyWIsTg][v-so-repo-05.synygy.net][inet[/10.220.10.105:9300]]{host=v-so-repo-05.synygy}, reason: zen-disco-join (elected_as_master)
delaying initial state recovery for [5m]. recover_after_time was set to [5m]
bound_address {inet[/127.0.0.1:9200]}, publish_address {inet[/10.220.10.105:9200]}
started
```

Here is an exception thrown shortly after starting:

```
[inet[/10.220.10.105:9300]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:825)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:758)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:731)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:216)
    at org.elasticsearch.cluster.service.InternalClusterService$ReconnectToNodes.run(InternalClusterService.java:584)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.ConnectException: Connection refused: /10.220.10.105:9300
    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:701)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:152)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    ... 3 more
```
</description><key id="109680702">13921</key><summary>setting network.bind_host: localhost and publish_host: _ens192:ipv4_ uses localhost only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cristifalcas</reporter><labels /><created>2015-10-04T13:15:14Z</created><updated>2015-10-06T12:47:04Z</updated><resolved>2015-10-06T12:47:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-06T12:47:03Z" id="145846304">@cristifalcas Elasticsearch only listens on the bind host, whatever you set publish host to.  If you only listen to bind host, then your nodes won't be able to contact each other.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function_score query bug using boolean filter with script_score function</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13920</link><project id="" key="" /><description>(elasticsearch 2.0.0-beta2)
Running this, what would you expect the result to be?

``` python
test='test'
doc='doc'
body = {
    'id': 1,
    'shiny': True,
}
es.index(test,doc,body)
body = {
    'id': 2,
    'shiny': True,
    'happy': True,
}
es.index(test,doc,body)
body = {
    'query': {
        'function_score': {
            'functions': [{
                'filter': {
                    'bool': {
                        'should': {
                            'term': { 'shiny': True},
                            'term': { 'happy': True},
                        },
                    }
                },
                'script_score' : {
                    'script': """
                        3*doc['shiny'].value +
                        2*doc['happy'].value
                        """
                }
            }
        ]}
    }
}
```

Doc 1 should have a score of 5 and doc 2 should have a score of 3. However instead doc 1 has a score of 5 and doc 2 has a score of 1 indicating that is was filtered out. So it seems like the `should` is acting like a `must`.

However, if I replace the `bool` filter with this:

```
'query_string': {
    'query': 'shiny:* happy:*'
}
```

Then I get the expected results.
</description><key id="109660394">13920</key><summary>function_score query bug using boolean filter with script_score function</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JnBrymn-EB</reporter><labels /><created>2015-10-04T03:21:37Z</created><updated>2015-10-05T15:18:25Z</updated><resolved>2015-10-05T09:59:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-05T09:59:53Z" id="145484982">I suspect this is due to the fact that

```
                        'should': {
                            'term': { 'shiny': True},
                            'term': { 'happy': True},
                        }
```

should be replaced with:

```
                        'should': [
                            {'term': { 'shiny': True}},
                            {'term': { 'happy': True}}
                        }
```

Otherwise your json document has twice the same key, which might confuse either python or elasticsearch, and in the end only one of the clauses that you defined is considered. Please reopen if it does not fix the issue.
</comment><comment author="JnBrymn-EB" created="2015-10-05T15:18:25Z" id="145566554">ugh... sorry for the noise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add _cat/snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13919</link><project id="" key="" /><description>Containing things like repos and current snapshots would be awesome :)
</description><key id="109652350">13919</key><summary>Add _cat/snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label></labels><created>2015-10-03T23:56:53Z</created><updated>2015-10-26T17:40:54Z</updated><resolved>2015-10-26T17:40:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>When shard becomes active again, immediately increase its indexing buffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13918</link><project id="" key="" /><description>Spinoff from #13802.

Today, when an index goes from inactive to active, because indexing ops suddenly arrive to an inactive index, we (IndexingMemoryController) take up to 30 seconds to notice this and then increase the indexing buffer from the tiny idle 512KB to "its fair share".  This is somewhat dangerous because it could write many, many segments in those 30 seconds...

This PR changes that so the inactive -&gt; active transition instead causes us to immediately re-visit the indexing buffer for all shards.

It also shifts responsibility of tracking active/inactive into `IndexShard`, and no longer uses the translog ID/ops count to check for changes (I think this is more error prone?  E.g. #13802), simplifying it instead to timestamp (System.nanoTime) of the last indexing op.
</description><key id="109646945">13918</key><summary>When shard becomes active again, immediately increase its indexing buffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-03T21:33:57Z</created><updated>2015-10-08T07:53:32Z</updated><resolved>2015-10-08T07:53:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-10-06T08:54:24Z" id="145789523">Thanks @nik9000 I folded in the feedback.
</comment><comment author="bleskes" created="2015-10-06T11:13:22Z" id="145824477">I love how this turned out, especially around moving away from the translog checks. I left some suggestions here and there. I also think we should  move the controller to work on IndexShard objects rather than shard ids (sorry for the scope creep - can be done in a different change). At some point I got worried that a new shard will not be seen as "added" if the original shard was destroyed and a new one created. I think it's OK now (because we only use current state when allocating memory) but it's more correct to use physical instances. 
</comment><comment author="mikemccand" created="2015-10-06T20:51:02Z" id="145996782">&gt; I also think we should move the controller to work on IndexShard objects rather than shard ids

++

But I thought we used `ShardId` all over to make this more easily unit-tested?  But anyway let's do this on a separate change ... this one is big already :)
</comment><comment author="mikemccand" created="2015-10-06T21:20:00Z" id="146005367">OK I folded feedback @bleskes, thank you!
</comment><comment author="bleskes" created="2015-10-07T11:52:00Z" id="146169279">This is getting great. I left some final comments.
</comment><comment author="mikemccand" created="2015-10-07T14:20:59Z" id="146208944">OK thanks @nik9000 and @bleskes, I folded in the last round of feedback.
</comment><comment author="bleskes" created="2015-10-07T16:13:55Z" id="146248377">LGTM. Awesome mike.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>processors setting is not reflected in nodes info API ("os.available_processors")</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13917</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html#processors

The end of that page seems to imply the `processors` value passed in settings will be displayed in the nodes info API with the `os` flag. However, it seems to always display information retrieved from the OS.

I know the setting is going through due to the change in thread pool values.

Should the "available_processors" value display what is passed in settings, or is it assumed to be actual OS information?

(on 1.7.1)
</description><key id="109602316">13917</key><summary>processors setting is not reflected in nodes info API ("os.available_processors")</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">FestivalBobcats</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-03T04:59:45Z</created><updated>2015-11-04T22:57:35Z</updated><resolved>2015-11-03T17:55:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T10:00:59Z" id="148672841">I think we should keep the `available_processors` key as it is today, but that we should include an `allocated_processors` key which shows actual number of processors that we are using.
</comment><comment author="FestivalBobcats" created="2015-10-16T15:14:21Z" id="148743264">Definitely agree with that approach.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.Iterators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13916</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.Iterators across the codebase. This is one of
the final steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="109581577">13916</key><summary>Remove and forbid use of com.google.common.collect.Iterators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T22:18:49Z</created><updated>2015-10-07T17:05:59Z</updated><resolved>2015-10-07T16:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-05T12:32:49Z" id="145513591">LGTM
</comment><comment author="jasontedor" created="2015-10-07T17:05:58Z" id="146263738">Thanks for reviewing @jpountz and @s1monw.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggestion is not refreshing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13915</link><project id="" key="" /><description>ES: Version: 1.7.2, Build: e43676b/2015-09-14T09:49:53Z, JVM: 1.7.0_79

I created an index like this:

curl -XDELETE 'http://localhost:9200/my_db.index_map/'
curl -XPUT 'http://localhost:9200/my_db.index_map/' -d '
{
  "settings": {
    "analysis": {
      "analyzer": {
        "case_insensitive": {
          "tokenizer": "keyword",  
          "filter":  [ "lowercase" ] 
        }
      }
    }
  }
}'

curl -XPUT 'http://localhost:9200/my_db.index_map/string/_mapping/' -d '
{
  "string" : {
    "properties" : {
      "keywords_tags" : {
        "type": "completion"
      }
    }
  }
}'

Then i added document with keywords_tags as "jabalpur". Later i removed it. 

Now in my search i am not getting jabalpur as results

curl -X GET http://127.0.0.1:9200/my_db.index_map/_search?pretty=true -d '{
    "query": {
        "match": {
            "keywords_tags": "jabalpur"
        }
    }
}'

Response:

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}

But if i make a suggest query like this:
curl -X GET http://127.0.0.1:9200/my_db.index_map/_suggest?pretty=true -d '{
    "suggestions" : {
        "text" : "jabalpur",
        "completion" : {
            "field" : "keywords_tags"
        }
    }
}'

{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "suggestions" : [ {
    "text" : "jabalpur",
    "offset" : 0,
    "length" : 8,
    "options" : [ {
      "text" : "jabalpur",
      "score" : 1.0
    } ]
  } ]
}

why is the suggest index not getting updated and how can i fix this ??
</description><key id="109570075">13915</key><summary>Suggestion is not refreshing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vi3k6i5</reporter><labels /><created>2015-10-02T20:52:20Z</created><updated>2016-05-02T08:41:58Z</updated><resolved>2015-10-03T13:13:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-03T13:13:18Z" id="145246938">Hi @vi3k6i5 

This is a documented limitation of the current completion suggester.  The new completion suggester coming in 2.1 doesn't have this limitation
</comment><comment author="vi3k6i5" created="2015-10-03T13:40:27Z" id="145249617">@clintongormley : Thanks for the update.

Is there any way around this? any temporary solution that might work?

Btw, Where can i track release plans ? and by when will 2.1 be released?

Thanks,
Vikash
</comment><comment author="sudheendrach" created="2015-10-06T07:10:45Z" id="145764685">:+1: @clintongormley would like to know the release plans for 2.1
</comment><comment author="clintongormley" created="2015-10-06T17:08:23Z" id="145929879">@vi3k6i5 optimizing the shard with expunge_deletes would remove the deleted (updated) documents.  

We don't announce release dates ahead of time.
</comment><comment author="vi3k6i5" created="2015-10-07T06:15:15Z" id="146088639">@clintongormley Thanks this helps. :+1: 
</comment><comment author="Alex1sz" created="2016-02-17T23:41:47Z" id="185461148">@clintongormley according to the documentation for 2.2 https://www.elastic.co/guide/en/elasticsearch/reference/2.2/search-suggesters-completion.html#querying[](url)  this isn't fixed for 2.2 (see: "The suggest data structure might not reflect deletes on documents immediately. You may need to do an Optimize for that. You can call optimize with the only_expunge_deletes=true to only target deletions for merging.") Did the 'new completion suggester' not make it into 2.2? Or this is the 'new completion suggester' and this just didn't get fixed?   
</comment><comment author="clintongormley" created="2016-02-28T21:55:59Z" id="189952017">@Alex1sz We backed it out of 2.2 as it was a breaking change, so it will only arrive in 5.0 unfortunately
</comment><comment author="trompx" created="2016-05-02T08:41:58Z" id="216146988">Hello @clintongormley,

I use the completion-suggester to display data that are returned with a payload updated every hour. But with the current suggester the update are not showing up, only the first version of the document is returned which is really annoying. So the only solution is to optimize the whole index to just update a value ? Or is it possible to force an update just for a specific document ?

Will the new suggester correct this problem ?

Thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>more consistency with IDE configuration for tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13914</link><project id="" key="" /><description>People want to be able to run tests from their IDE (and maybe use the debugger and so on). I get it.

But tests behave differently depending on how you run them, and its a huge hassle to deal with this stuff.

The problem is we have to support too many runtime environments:
- bin/elasticsearch (aka running for real). distribution and plugin REST tests go against an external server in this environment. They are the most realistic tests.
- "unit test environment from maven". in this situation, things are less realistic. We have multiple nodes within the same jvm, and we just do our best to approximate reality. With maven there are even differences depending on whether its in a "reactor build" or not.
- Intellij IDE. There might even be multiple environments, depending on how people configure it (e.g. mvn idea:idea versus something within the IDE itself). This is less realistic even than unit tests.
- Eclipse IDE. Currently this is the least realistic of all.

Having to make all these configurations "work" or "kind of work" is incredibly frustrating and complex. In many cases I throw out designs completely, or push back against changes, because it won't work in all these scenarios. Instead, I think we should put a little work into the IDE configuration to make it easier.

For example, eclipse puts _all generated classes_ piled together in `eclipse/build`. Instead of:

```
&lt;classpathentry kind="src" path="src/main/java"/&gt;
&lt;classpathentry kind="src" path="src/test/java"/&gt;
```

We can do:

```
&lt;classpathentry kind="src" path="src/main/java" output="eclipse-build/target/classes"/&gt;
&lt;classpathentry kind="src" path="src/test/java output="eclipse-build/target/test-classes"/&gt;
```

See how now its closer to the maven build? For example when testing lang-groovy plugin it means test-framework can assign src/main/java all the scary permissions groovy needs, but not src/test/java code. This means tests will fail in the IDE if security logic is wrong.

I'm setting adopt me: folks come to me with questions if their IDE does not do what they want, but I basically have to throw up my hands until we simplify some stuff: improve these IDE configs.
</description><key id="109543892">13914</key><summary>more consistency with IDE configuration for tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>build</label></labels><created>2015-10-02T18:14:19Z</created><updated>2016-01-28T18:39:40Z</updated><resolved>2016-01-28T18:39:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-10-02T18:16:31Z" id="145112886">I also wonder if these IDEs can be closer to unit tests as far as which system properties are provided to tests and things like that.
</comment><comment author="clintongormley" created="2016-01-28T18:39:40Z" id="176330926">i think this has been resolved going forward, so I'll close for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Copy a snapshot to another repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13913</link><project id="" key="" /><description>I have a repository with 4-hour snapshots going back 8 months. Most of those I'd like to delete, but 10 are useful and should be kept. I have to delete ~1500 snapshots. At 30 minutes each, that will take about 25 days.

If there were a way to copy snapshots between repositories, this would be a much faster operation: copy those 10 snapshots, delete the old repository.

Any plans to support this? I think it would reduce the pain of cleaning up large repositories substantially.
</description><key id="109533756">13913</key><summary>Copy a snapshot to another repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantr</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>enhancement</label></labels><created>2015-10-02T17:18:25Z</created><updated>2016-01-28T18:36:35Z</updated><resolved>2016-01-28T18:36:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T17:47:13Z" id="145097179">Related to #12587?
</comment><comment author="imotov" created="2015-10-09T19:43:02Z" id="146970284">@clintongormley in #12587 we added support for bulk delete operations within snapshot. Basically deleting multiple snapshot files in one request to S3 or azure. I think what @grantr needs is to have an ability to delete multiple snapshots at the same time.
</comment><comment author="clintongormley" created="2016-01-28T18:36:35Z" id="176329009">thanks @imotov - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make weighted centroid an optional parameter for geohash_grid aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13912</link><project id="" key="" /><description>#13433 added a weighted centroid to the `geohash_grid` aggregator.  #13846 adds a stand alone `geo_centroid` metric aggregator for combining with other aggregations (e.g., terms aggregator). This enhancement adds an optional `compute_centroid` parameter (default to `false`) to the `geohash_grid` aggregator to provide a shortcut for computing the weighted centroid on geohash grids without requiring `geo_centroid` as a separate sub-aggregation.
</description><key id="109530904">13912</key><summary>Make weighted centroid an optional parameter for geohash_grid aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>discuss</label><label>enhancement</label></labels><created>2015-10-02T16:58:57Z</created><updated>2015-10-15T15:12:37Z</updated><resolved>2015-10-15T14:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-02T17:48:11Z" id="145097710">Adding a sub aggregation is so easy, why should we have this `compute_centroid` parameter?
</comment><comment author="nknize" created="2015-10-02T18:30:07Z" id="145116325">Everything's already available in GridAggregator leafCollector and doReduce to compute the weighted centroid while accumulating each bucket without having to create separate GeoCentroidAggregator objects and make a second pass per bucket. Maybe negligible savings?
</comment><comment author="jpountz" created="2015-10-02T18:39:34Z" id="145118925">OK I see your point. I would be in favour of not having this `compute_centroid` option for now, and then potentially reconsider our options in the future if we identify `geo_centroid` under `geohash_grid` as a frequent source of out-of-memory errors.
</comment><comment author="nknize" created="2015-10-02T18:52:11Z" id="145124492">+1 I'm good w/ the "lets fix it if its broke".

Just a quick observation from the singleValueFieldAsSubAggToGeohashGrid() test which indexes 2K documents. Using `geo_centroid` as a subaggregator makes 2*numDocs calls to `collect` (one for the GeoGrid and one for the Centroid) where as accumulating centroid as a part of GeoHashGrid makes numDocs calls to `collect`.  I don't suspect we'll have OOM issues?  But do we think this savings is worth it as index size grows?  Or a non issue with a good shard configuration?
</comment><comment author="jpountz" created="2015-10-02T18:57:18Z" id="145125924">I think this is expected? We create a tree of aggregators that delegate the collect method. So the framework calls collect `numDocs` times on the `geohash_grid` aggregation, and part of what collect() on the `geohash_grid` aggregator does is calling collect() on the `geo_centroid` aggregator.
</comment><comment author="nknize" created="2015-10-02T19:20:11Z" id="145130437">Right, I'm just wondering if adding this centroid option to the grid aggregator is worth it at the moment for cutting this process in half.  I can either leave the issue open and address later or close and reopen if it becomes a problem?
</comment><comment author="nknize" created="2015-10-15T14:36:50Z" id="148405817">Closing for now as a non-issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove search exists api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13911</link><project id="" key="" /><description>With #13910 we deprecate `_search/exists`, we can then remove the api from the master branch.

Closes #13682 
</description><key id="109529882">13911</key><summary>Remove search exists api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T16:51:59Z</created><updated>2015-10-21T15:39:57Z</updated><resolved>2015-10-21T15:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-03T16:45:15Z" id="145263897">LGTM
</comment><comment author="jpountz" created="2015-10-03T16:46:08Z" id="145263934">Should we add a note to the migration guide?
</comment><comment author="javanna" created="2015-10-19T08:37:44Z" id="149145607">I added the missing note to the migration guide and rebased, should be ready
</comment><comment author="jpountz" created="2015-10-21T13:26:13Z" id="149893166">LGTM again
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate _search/exists in favour of regular _search with size 0 and terminate_after 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13910</link><project id="" key="" /><description>Deprecate `_search/exists` in favour of regular `_search` with `size` set to  `0` and `terminate_after` set to `1`.

Relates to #13682
</description><key id="109528769">13910</key><summary>Deprecate _search/exists in favour of regular _search with size 0 and terminate_after 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>deprecation</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-10-02T16:44:05Z</created><updated>2015-10-21T13:34:35Z</updated><resolved>2015-10-21T13:34:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:59:30Z" id="145086277">Nice.  Perhaps add a section to the search API docs entitled "Fast check for any matching docs" or similar?
</comment><comment author="jpountz" created="2015-10-02T17:02:27Z" id="145086890">+1 on documenting this "recipe"
</comment><comment author="s1monw" created="2015-10-17T19:09:18Z" id="148944765">+1 maybe @debadair can help documenting that?
</comment><comment author="javanna" created="2015-10-19T07:33:02Z" id="149127226">I pushed a new commit to log a deprecation warning when search exists is used. Agreed on the additional documentation needed, I would open a follow-up PR though as that one would need to go to master too. Is that ok?
</comment><comment author="jpountz" created="2015-10-21T13:19:56Z" id="149891433">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.ImmutableCollection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13909</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.ImmutableCollection across the codebase. This
is one of the final steps in the eventual removal of Guava as a
dependency.

Relates #13224
</description><key id="109526326">13909</key><summary>Remove and forbid use of com.google.common.collect.ImmutableCollection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T16:29:33Z</created><updated>2015-10-02T19:18:49Z</updated><resolved>2015-10-02T19:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-02T16:38:00Z" id="145081703">LGTM this was easy too :)
</comment><comment author="jasontedor" created="2015-10-02T19:18:49Z" id="145130205">Thanks again for reviewing @javanna!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.io.Resources</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13908</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.io.Resources across the codebase. This is one of the
few remaining steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="109518624">13908</key><summary>Remove and forbid use of com.google.common.io.Resources</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T15:47:49Z</created><updated>2015-10-02T16:09:29Z</updated><resolved>2015-10-02T16:09:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-02T16:05:07Z" id="145072395">LGTM this one was easy ;)
</comment><comment author="jasontedor" created="2015-10-02T16:09:29Z" id="145073354">Thanks for the quick review @javanna!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.hash.*</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13907</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.hash.HashCode, com.google.common.hash.HashFunction,
and com.google.common.hash.Hashing across the codebase. This is one of
the few remaining steps in the eventual removal of Guava as a
dependency.

Relates #13224
</description><key id="109513549">13907</key><summary>Remove and forbid use of com.google.common.hash.*</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T15:23:44Z</created><updated>2015-10-04T20:44:03Z</updated><resolved>2015-10-04T20:26:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-02T17:53:49Z" id="145101743">LGTM
</comment><comment author="jasontedor" created="2015-10-04T20:44:03Z" id="145385911">Thanks for the review @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated indices.get_aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13906</link><project id="" key="" /><description>This has been deprecated since the first release of Elasticsearch 1.0
</description><key id="109498694">13906</key><summary>Remove deprecated indices.get_aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>:Aliases</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T14:14:53Z</created><updated>2016-03-31T13:57:04Z</updated><resolved>2016-03-31T13:57:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-02T15:05:42Z" id="145053486">LGTM thanks @Mpdreamz . I had to do some digging to figure out when we had deprecated the api, it happened as part of #4539. I think it's ok to remove it, but I'd confirm with @rashidkpc to double check that kibana fully relies on the get index api now instead.
</comment><comment author="Mpdreamz" created="2015-10-06T17:13:38Z" id="145931153">It is still used in kibana: https://github.com/elastic/kibana/blob/0186c55d4e6e02b6f2cf593636ae21a6c45eaa41/src/ui/public/index_patterns/_mapper.js#L76
</comment><comment author="javanna" created="2015-11-02T09:24:01Z" id="152967452">@rashidkpc @simianhacker can you please comment on whether this is safe to remove in 3.0?
</comment><comment author="dadoonet" created="2016-01-13T12:55:57Z" id="171282420">+1 to remove. It gives inconsistent results in some cases. See #15817.
</comment><comment author="clintongormley" created="2016-03-10T19:10:13Z" id="195004682">Chatted to @rashidkpc who says that they will change kibana to use get_alias instead of get_aliases, so we can go ahead with this PR.
</comment><comment author="clintongormley" created="2016-03-10T19:10:44Z" id="195004838">@Mpdreamz do you want to pick this up again?
</comment><comment author="Mpdreamz" created="2016-03-10T19:21:28Z" id="195008161">@clintongormley self assigned, will rebase tomorrow.
</comment><comment author="epixa" created="2016-03-10T20:49:58Z" id="195041032">Kibana is now using get_alias, go nuts!
</comment><comment author="rashidkpc" created="2016-03-29T15:40:23Z" id="202963794">@Mpdreamz is this going in to 5.0.0?
</comment><comment author="Mpdreamz" created="2016-03-29T19:16:53Z" id="203060022">I rebased and resolved conflcts, @clintongormley is this OK to merge in master?
</comment><comment author="clintongormley" created="2016-03-30T19:16:04Z" id="203590946">Fine by me.  @javanna could you have a look please?
</comment><comment author="javanna" created="2016-03-31T11:23:42Z" id="203885263">I have failing tests with this change. There are REST tests for indices.get_aliases that need to be removed I think, plus these seem to use the api too:

```
  - org.elasticsearch.test.rest.RestIT.test {p0=indices.update_aliases/20_routing/Search Routing}
  - org.elasticsearch.test.rest.RestIT.test {p0=indices.update_aliases/20_routing/Index, Search, Default Routing}
  - org.elasticsearch.test.rest.RestIT.test {p0=indices.update_aliases/20_routing/Index, Default Routing}
  - org.elasticsearch.test.rest.RestIT.test {p0=indices.update_aliases/20_routing/Routing}
  - org.elasticsearch.test.rest.RestIT.test {p0=indices.update_aliases/20_routing/Search, Default Routing}
  - org.elasticsearch.test.rest.RestIT.test {p0=indices.update_aliases/20_routing/Index Routing}
```

@Mpdreamz can you run tests or do you want me to take this over? let me know what's best for you!
</comment><comment author="Mpdreamz" created="2016-03-31T12:30:22Z" id="203908339">@javanna doh! fixing this right now
</comment><comment author="Mpdreamz" created="2016-03-31T13:32:59Z" id="203937611">@javanna rebased, removed the rest tests for get indices and updated `update_aliases` tests and reran those. Apologies for missing those in the first place!
</comment><comment author="javanna" created="2016-03-31T13:35:41Z" id="203939095">LGTM @Mpdreamz if tests are green, go merge it ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.net.InetAddresses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13905</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.net.InetAddresses across the codebase. This is one of
the few remaining steps in the eventual removal of Guava as a
dependency.

Relates #13224
</description><key id="109490817">13905</key><summary>Remove and forbid use of com.google.common.net.InetAddresses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T13:28:29Z</created><updated>2015-10-08T00:51:55Z</updated><resolved>2015-10-08T00:29:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-07T22:03:01Z" id="146345877">LGTM
</comment><comment author="rmuir" created="2015-10-08T00:44:52Z" id="146383194">ipv6 representation is fairly complex (see RFC 5952)... i don't see any problems in the code looking at it, but can we add a random test that round trips? Of course, this doesn't ensure that certain rules are followed (e.g. only compress when number of zeros &gt; 2, only compress the longest run, ...) but its a simple nice sanity check.

In other words, any compressed form we generate for a random 128 bits should parse back to the same 128 bits via InetAddress.getByName.
</comment><comment author="rmuir" created="2015-10-08T00:51:55Z" id="146384775">Suggested test... it passes with the old guava code at least :)

```
    public void testRoundTrip() throws Exception {
        byte bytes[] = new byte[16];
        Random random = random();
        for (int i = 0; i &lt; 10000; i++) {
            random.nextBytes(bytes);
            InetAddress expected = Inet6Address.getByAddress(bytes);
            String formatted = NetworkAddress.formatAddress(expected);
            InetAddress actual = InetAddress.getByName(formatted);
            assertEquals(expected, actual);
        }
    }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SharedClusterSnapshotRestoreIT. deleteSnapshotWithMissingIndexAndShardMetadataTest test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13904</link><project id="" key="" /><description>I hit this failing seed on master ... not sure why it's failing:

```
Started J0 PID(19725@Michaels-MacBook-Pro-2.local).
Suite: org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT
  2&gt; REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=8EEF26FBB2DE1802 -Dtests.class=org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT -Dtests.method="deleteSnapshotWithMissingIndexAndShardMetadataTest" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=fr_FR -Dtests.timezone=Canada/Yukon
FAILURE 4.62s | SharedClusterSnapshotRestoreIT.deleteSnapshotWithMissingIndexAndShardMetadataTest &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;11&gt;
   &gt;      but: was &lt;9&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([8EEF26FBB2DE1802:FC339B914AD34F7]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT.deleteSnapshotWithMissingIndexAndShardMetadataTest(SharedClusterSnapshotRestoreIT.java:835)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /Users/mike/src/es.clean/core/target/J0/temp/org.elasticsearch.snapshots.SharedClusterSnapshotRestoreIT_8EEF26FBB2DE1802-002
  2&gt; NOTE: test params are: codec=Asserting(Lucene53): {_field_names=PostingsFormat(name=Asserting), foo=PostingsFormat(name=Asserting), _type=PostingsFormat(name=Asserting), _uid=PostingsFormat(name=Asserting), _all=PostingsFormat(name=Asserting)}, docValues:{_version=DocValuesFormat(name=Asserting)}, sim=RandomSimilarityProvider(queryNorm=false,coord=yes): {}, locale=fr_FR, timezone=Canada/Yukon
  2&gt; NOTE: Mac OS X 10.10.4 x86_64/Oracle Corporation 1.8.0_40 (64-bit)/cpus=4,threads=1,free=400935864,total=514850816
  2&gt; NOTE: All tests run in this JVM: [SharedClusterSnapshotRestoreIT]
Completed [1/1] in 5.81s, 1 test, 1 failure &lt;&lt;&lt; FAILURES!
```
</description><key id="109484745">13904</key><summary>SharedClusterSnapshotRestoreIT. deleteSnapshotWithMissingIndexAndShardMetadataTest test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>test</label></labels><created>2015-10-02T12:48:24Z</created><updated>2016-05-03T12:37:08Z</updated><resolved>2016-05-03T12:37:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-05-03T12:37:07Z" id="216514454">old report - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.EvictingQueue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13903</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.EvictingQueue across the codebase. This is
one of the few remaining steps in the eventual removal of Guava as a
dependency.

Relates #13224
</description><key id="109472215">13903</key><summary>Remove and forbid use of com.google.common.collect.EvictingQueue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-10-02T11:03:16Z</created><updated>2015-10-08T01:17:15Z</updated><resolved>2015-10-08T01:05:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-02T18:08:12Z" id="145109151">Wouldn't it be better to change our pipeline aggs to use a regular queue and evict explicitly instead of adding this new queue impl? This would avoid this new abstraction.
</comment><comment author="jasontedor" created="2015-10-07T16:46:21Z" id="146258791">@jpountz I took a look at whether or not manually managing evictions in the pipeline aggregations code would simplify things or not, and frankly I think it doesn't and I think it would lead to code that is harder to maintain. In this case, I think that this (relatively simple) abstraction simplifies the pipeline aggregations code (so that there isn't the noise of manually managing the evictions there) and makes it more maintainable (future maintainers have to take care to properly manage any changes to the add logic). I would be interested in seeing if you have a simple and maintainable way that I'm not seeing?
</comment><comment author="jpountz" created="2015-10-07T22:07:33Z" id="146346679">I haven't really tried or even looked, I was just thinking out loud. If it looks easier with this abstraction, I'm good with the change. Can you just add some minimal javadocs to EvictingQueue before pushing?
</comment><comment author="jasontedor" created="2015-10-08T01:04:56Z" id="146386243">@jpountz Added Javadocs in e61e7463e3e4da13e481de20671c42f196154f1a.
</comment><comment author="jasontedor" created="2015-10-08T01:17:15Z" id="146387430">Thank you for reviewing @jpountz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get field mapping documented fields as field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13902</link><project id="" key="" /><description>This PR brings this spec more inline with other api's and minimizes the known route variables in the API

See: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/rest/action/admin/indices/mapping/get/RestGetFieldMappingAction.java#L66
</description><key id="109471407">13902</key><summary>Get field mapping documented fields as field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>breaking</label><label>bug</label></labels><created>2015-10-02T10:55:12Z</created><updated>2015-10-06T10:01:20Z</updated><resolved>2015-10-06T09:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:48:37Z" id="145083968">LGTM
</comment><comment author="Mpdreamz" created="2015-10-06T10:01:20Z" id="145807810">merged to master and cherry-picked to `2.0`. `2.1`, `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> indices.close takes a list of indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13901</link><project id="" key="" /><description> not single index
</description><key id="109468621">13901</key><summary> indices.close takes a list of indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label></labels><created>2015-10-02T10:30:44Z</created><updated>2015-10-06T10:01:14Z</updated><resolved>2015-10-06T09:27:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-02T10:46:31Z" id="144988540">LGTM seems like we have no REST tests using a comma separated list of indices though, shall we add a quick one? ;)
</comment><comment author="Mpdreamz" created="2015-10-02T13:13:41Z" id="145016566">+1 but I rather sent another PR for it, want to get the doc changes in master and 2.x before GA :)
</comment><comment author="Mpdreamz" created="2015-10-06T10:01:14Z" id="145807776">merged to master and cherry-picked to `2.0`. `2.1`, `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices.open takes a list of indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13900</link><project id="" key="" /><description>not single index
</description><key id="109468414">13900</key><summary>indices.open takes a list of indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label></labels><created>2015-10-02T10:28:56Z</created><updated>2015-10-06T10:01:07Z</updated><resolved>2015-10-06T09:27:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-02T10:46:17Z" id="144988514">LGTM seems like we have no REST tests using a comma separated list of indices though, shall we add a quick one?
</comment><comment author="Mpdreamz" created="2015-10-02T13:13:14Z" id="145016482">+1 but I rather sent another PR for it, want to get the doc changes in master and 2.x before GA :)
</comment><comment author="Mpdreamz" created="2015-10-06T10:01:07Z" id="145807735">merged to master and cherry-picked to `2.0`. `2.1`, `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug with simple_query_string, multiple fields and prefix query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13899</link><project id="" key="" /><description>## Abstract

-- Adding a '*' after the query is the difference between an error and a search
-- Tested on v1.7.2. and v2.0.0.beta2
## Setup
### Delete the index

```
curl -XDELETE localhost:9200/testing
```
### Add data

```
curl -XPUT 'http://localhost:9200/testing/test/1' -d '{"name":"test1","list":{"id":1,"color" : "orange"}}'
```
## Querying
### 1) First working query

```
curl -XPOST 'http://localhost:9200/testing/test/_search' -d '{
  "_source":true,
      "query":{
        "simple_query_string":{
          "fields":[
            "list.color"
          ],
          "query":"orange"
        }
      }
}'
```
### 2) This query gives error: "...nested: NumberFormatException[For input string: \"orange\"]..."

```
curl -XPOST 'http://localhost:9200/testing/test/_search' -d '{
  "_source":true,
      "query":{
        "simple_query_string":{
          "fields":[
            "list.*"
          ],
          "query":"orange"
        }
      }
}'
```
### 3) but this works again?!?

```
curl -XPOST 'http://localhost:9200/testing/test/_search' -d '{
  "_source":true,
      "query":{
        "simple_query_string":{
          "fields":[
            "list.*"
          ],
          "query":"orange*"
        }
      }
}'
```
## Notes

Notice that the only change between 2) and 3) is that I put a `*` after the "orange" =&gt; "orange*"
The error mentions nesting, but there's no nesting in the mapping?!?
</description><key id="109458968">13899</key><summary>Bug with simple_query_string, multiple fields and prefix query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">LeonardGC</reporter><labels /><created>2015-10-02T09:25:01Z</created><updated>2015-10-02T16:43:26Z</updated><resolved>2015-10-02T16:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:43:26Z" id="145082943">The prefix query (`orange*`) is a term level query which attempts no analysis, and so can be run on a numeric field (without finding any results).  The first query tries to analyze `orange` but that doesn't work with a numeric field.

You're looking for the `lenient` option, which will ignore fields of the wrong type.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent/child inner hits not containing the fields values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13898</link><project id="" key="" /><description>Hello,

The parent/child inner hits seem to support requesting only the '_source' field. Inner hits on nested objects on the contrary support asking for any field.

The documentation is not extremely clear on the subject, but I think it should work.

I have this behavior on ElasticSearch 1.7.2.

This script reproduces the problem in the more concise way i could think of:

``` shell
printf "Delete index:\n\n"
curl -XDELETE localhost:9200/test

printf "\n\nPrepare mappings with parent/child relationship and some nested object:\n\n"
curl -XPUT localhost:9200/test -d '{
  "mappings": {
    "company": {
      "properties": {
        "addresses": {
          "type": "nested"
        }
      }
    },
    "employee": {
      "_parent": {
        "type": "company"
      }
    }
  }
}'

printf "\n\nInsert a company with nested address:\n\n"
curl -XPUT localhost:9200/test/company/mgdis -d '{
  "companyName": "MGDIS",
  "addresses": [{
    "city": "Vannes",
    "country": "France"
  }]
}'

printf "\n\nInsert a employee child of the company:\n\n"
curl -XPUT localhost:9200/test/employee/alban?parent=mgdis -d '{
  "name": "Alban Mouton"
}'

printf "\n\nFlush index:\n\n"
curl -XPOST 'http://localhost:9200/test/_flush'

printf "\n\nSearch a company with nested addresses in inner hits. The field 'city' IS PRESENT:\n\n"
curl -XPOST localhost:9200/test/company/_search -d '{
  "query": {
    "nested": {
      "path": "addresses",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "fields": ["city"]
      }
    }
  }
}'

printf "\n\nSearch an employee with parent company in inner hits. The field 'companyName' SHOULD BE PRESENT:\n\n"
curl -XPOST localhost:9200/test/employee/_search -d '{
  "query": {
    "has_parent": {
      "parent_type": "company",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "fields": ["companyName"]
      }
    }
  }
}'

printf "\n\nSearch a company with child employee in inner hits. The field 'name' SHOULD BE PRESENT:\n\n"
curl -XPOST localhost:9200/test/company/_search -d '{
  "query": {
    "has_child": {
      "type": "employee",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "fields": ["name"]
      }
    }
  }
}'
```
</description><key id="109458204">13898</key><summary>Parent/child inner hits not containing the fields values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">albanm</reporter><labels /><created>2015-10-02T09:20:54Z</created><updated>2015-10-02T12:17:27Z</updated><resolved>2015-10-02T12:17:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-10-02T12:07:10Z" id="144999369">Hi @albanm if you remove the company/employee type from the url then I expect the last two queries to work. 

There is an issue with the inner query wrapped has_child/has_parent can't access mapping information from the child or parent type if a type is specified in the url. This has been fixed in 2.0.
</comment><comment author="albanm" created="2015-10-02T12:17:27Z" id="145001135">Indeed it works !

Thanks a lot for the quick response.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update `cat allocation` doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13897</link><project id="" key="" /><description>Cat allocation` doc is outdated. 

Related to #13529 #13783
</description><key id="109442943">13897</key><summary>Update `cat allocation` doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>docs</label></labels><created>2015-10-02T07:35:53Z</created><updated>2015-10-02T17:27:20Z</updated><resolved>2015-10-02T17:27:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:35:54Z" id="145081245">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verify Checksum once it has been fully written to fail as soon as possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13896</link><project id="" key="" /><description>Today we are relying on calling Store.verify on the closed stream to validate the
checksum. This is still necessary to catch file truncation but for an actually corrupted
file or checksum we can fail early and check the checksum against the actual metadata
once it's been fully written to the VerifyingIndexOutput.

This failure is fixed with this PR:

```
http://build-us-00.elastic.co/job/es_core_master_strong/5179/testReport/junit/org.elasticsearch.indices.recovery/RecoverySourceHandlerTests/testHandleCorruptedIndexOnSendSendFiles/
```
</description><key id="109404298">13896</key><summary>Verify Checksum once it has been fully written to fail as soon as possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-01T23:27:29Z</created><updated>2015-10-02T19:51:45Z</updated><resolved>2015-10-02T19:50:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-10-02T19:41:43Z" id="145136858">LGTM.
</comment><comment author="s1monw" created="2015-10-02T19:51:45Z" id="145139501">I will wait a bit until I backport this just to make sure it doesn't break anything
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ExtendableQueryParser equivalent in ElasticSearch?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13895</link><project id="" key="" /><description>I have a usecase to intercept a Lucene query from a user, then do query manipulation for certain fields so that we can make their query a bit more "smart".  For example, if they send us an IPv4 CIDR, we want to convert this to a numeric range.

We tried using the ExtendableQueryParser, then using the subsequent query in the QueryStringQueryBuilder.  This works for the most part, but gets tricky once special characters are introduced.  Lucene will "drop" these once the toString is called on the Query object that is created from parsing the string query.  When we feed this in to the QueryStringQueryParser, the query that is generated causes ES to fail because things aren't escaped properly anymore (because Lucene's QueryParser's toString removed them).

My hope is that if ExtendableQueryParser (or equivalent) was built into ES, we could just modify specific key/values in the query, and let everything else pass through.

I can provide more info if needed, please let me know.

Edit: we are using ES 1.7.2

Thanks!
</description><key id="109402300">13895</key><summary>ExtendableQueryParser equivalent in ElasticSearch?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tad9ab</reporter><labels><label>:Query DSL</label><label>discuss</label></labels><created>2015-10-01T23:09:26Z</created><updated>2015-10-16T10:06:39Z</updated><resolved>2015-10-16T10:06:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-16T10:06:39Z" id="148674024">Hi @tad9ab 

We've discussed this in FixItFriday and we're not keen to add these hooks.  This feels like it should be handled in a query pre-processing phase before sending the query to Elasticsearch.

For the concrete example of converting the CIDR to a range, there is this issue that has been open for a while https://github.com/elastic/elasticsearch/issues/7464. Nobody has implemented it yet, but we'd be in favour of implementing it if you'd like to send a PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update query-string-syntax.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13894</link><project id="" key="" /><description>Incorrect word "bares" replaced with correct word "bears"
</description><key id="109397036">13894</key><summary>Update query-string-syntax.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dmland</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-01T22:26:11Z</created><updated>2015-12-30T20:18:42Z</updated><resolved>2015-11-18T14:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:30:33Z" id="145079690">Thanks for the PR @dmland. Please could I ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="dmland" created="2015-11-17T04:51:54Z" id="157266997">I apologize for how long it took me to do so, but I finally signed the CLA. Thanks for your patience.
</comment><comment author="clintongormley" created="2015-11-18T14:50:45Z" id="157737856">better late than never :)

merged, thanks @dmland 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify reading sets, maps, and lists</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13893</link><project id="" key="" /><description>While working on removing `ImmutableMap` and `ImmutableSet` I've seen a ton of different fun ways to read immutable maps, sets, and lists. If we're willing to use Java 8's function pointer things we could remove lots of duplicated code by creating `readMap`, `readSet`, and `readList` in `StreamInput`. We could probably also do something similar for writing these but I haven't thought as much about it.
</description><key id="109391498">13893</key><summary>Simplify reading sets, maps, and lists</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2015-10-01T21:44:39Z</created><updated>2016-03-10T18:52:53Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-10-02T17:00:16Z" id="145086416">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ImmutableGeoPoint Class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13892</link><project id="" key="" /><description>The current GeoPoint class provides `reset` utility methods making all GeoPoint instance volatile. This is particularly dangerous for existing Query objects but necessary for Grid and Centroid metric aggregations (situations that would otherwise create many transient GeoPoint objects). This enhancement adds an ImmutableGeoPoint class for guarding against potential query corruption.
</description><key id="109387403">13892</key><summary>Add ImmutableGeoPoint Class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>discuss</label><label>low hanging fruit</label></labels><created>2015-10-01T21:17:24Z</created><updated>2017-05-26T16:01:02Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-10-01T22:45:53Z" id="144869000">Can we just make GeoPoint immutable without adding another class?
</comment><comment author="nknize" created="2015-10-02T14:17:03Z" id="145033454">After thinking a bit more, how about a simple mutable GeoPoint class that extends an immutable Point base class? Since we want to eventually support 3 or 4 dimensions this would give us both the read-only protection we need for Query state and the foundation for extending into higher dimensions - without creating a class for each dimension like other libraries do (e.g., Point2D, Point3D).  This would be very useful for coordinate reprojection and 3rd party plugin support.
</comment><comment author="danielmitterdorfer" created="2015-11-16T14:07:07Z" id="157039670">Although it seems practical, this breaks the Liskov Substitution Principle and can lead to surprises:

```
//MutablePoint inherits from ImmutablePoint
MutablePoint pm = new MutablePoint(0, 0);
ImmutablePoint pi = pm;

// in another method far, far away, the x coordinate is cached since pi is immutable

final int x = pi.getX();

// now we update the value somewhere else again ...
pm.setX(3);

// ... and the cached value 'x' is wrong although 'pi' was advertised as immutable :(
```

It is also tricky to implement concurrent updates correctly. I think you would have to reimplement the mutable point class from scratch anyway and also provide an API that allows for atomic updates of a point coordinate (assuming concurrent updates are supported).

I am also not sure how your design would help extending to more than two dimensions.
</comment><comment author="nknize" created="2015-11-17T03:32:53Z" id="157257605">This is how I was planning: https://github.com/nknize/elasticsearch/blob/immutableGeopoint/core/src/main/java/org/elasticsearch/common/geo/GeoPoint.java

The following would throw a ClassCastException:

``` java
    GeoPoint mp = GeoPoint.mutable(-89.0, 125.0);
    GeoPoint gp2 = (Immutable)mp;
    ((Mutable)mp).reset(-87.0, 122.0);
```

Good point on the concurrency issue. It was not planned so we could start by updating the javadocs with this warning, and then (if needed) update for concurrency support.

But this may be OBE as I believe we can make GeoPoint immutable (as Ryan mentioned) with low impact and less risk.
</comment><comment author="danielmitterdorfer" created="2015-11-17T07:31:39Z" id="157299061">Thanks for the code example. This makes it much clearer.

However, as the `GeoPoint` fields are not final, the class is not immutable but just effectively immutable. This will be no problem as long as `GeoPoint` is only used from a single thread but you (might) get into trouble if you have multiple threads and you publish `GeoPoint` with a data race. If the fields of `GeoPoint` would be final, you'd be even safe in this case (due to the guarantees defined in [JLS 17.5](https://docs.oracle.com/javase/specs/jls/se8/html/jls-17.html#jls-17.5)).

Long story short, this would work perfectly fine in a single threaded case but it is not safe for multithreading.
</comment><comment author="nknize" created="2015-11-17T15:39:23Z" id="157407006">Thanks for the review and great input @danielmitterdorfer  If/when this lands I'll give you a heads up to review the PR. @colings86 how urgent is GeoPoint immutability?
</comment><comment author="colings86" created="2015-11-17T15:52:36Z" id="157411072">@nknize its not that urgent. More like a nice to have really. It just means we won't need to do any copying of points to make sure the don't change
</comment><comment author="danielmitterdorfer" created="2015-11-17T15:57:13Z" id="157412528">@nknize Thanks, I'm glad I could help. :)
</comment><comment author="javanna" created="2017-05-05T14:45:31Z" id="299484437">@nknize Is this something yet to be done? Anything left to discuss on the matter?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update link to Jepsen related test class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13891</link><project id="" key="" /><description /><key id="109377891">13891</key><summary>Update link to Jepsen related test class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">josegonzalez</reporter><labels><label>docs</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-10-01T20:34:30Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-10-01T20:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="josegonzalez" created="2015-10-01T20:37:23Z" id="144840523">I've signed the CLA, how do I get the check to pass now?
</comment><comment author="nik9000" created="2015-10-01T20:50:48Z" id="144844298">There are a few async processes that have to happen first and then every time a new comment comes in it should rerun. I just confirmed that you signed it manually though so I'll merge it.
</comment><comment author="nik9000" created="2015-10-01T20:51:32Z" id="144844464">And my comment triggered the check to rerun and it passed. Its neat when things like that work.
</comment><comment author="josegonzalez" created="2015-10-01T21:03:45Z" id="144847706">&lt;3
</comment><comment author="nik9000" created="2015-10-01T21:26:29Z" id="144855006">Thanks for catching it! A link checker is just one more thing to add to the list of doc validations.....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clarify that aliases and indices can't share names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13890</link><project id="" key="" /><description /><key id="109365740">13890</key><summary>Clarify that aliases and indices can't share names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">veatch</reporter><labels><label>docs</label></labels><created>2015-10-01T19:23:12Z</created><updated>2015-10-02T16:12:45Z</updated><resolved>2015-10-02T16:11:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:12:45Z" id="145074051">thanks @veatch - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices get template accepts a list of names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13889</link><project id="" key="" /><description>not just string
</description><key id="109357145">13889</key><summary>indices get template accepts a list of names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label></labels><created>2015-10-01T18:32:25Z</created><updated>2015-10-06T10:00:58Z</updated><resolved>2015-10-06T09:28:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:09:13Z" id="145073287">LGTM
</comment><comment author="Mpdreamz" created="2015-10-06T10:00:58Z" id="145807694">merged to master and cherry-picked to `2.0`. `2.1`, `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename fields to field to be consistent with other api's</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13888</link><project id="" key="" /><description>namely indices.get_field_mapping
</description><key id="109324326">13888</key><summary>Rename fields to field to be consistent with other api's</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-10-01T15:29:31Z</created><updated>2015-10-06T09:24:25Z</updated><resolved>2015-10-02T10:46:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2015-10-02T10:46:44Z" id="144988569">closing this in favor of documenting indices.get_gield_mapping as `fields`, this to resemble to querystring option present on several API's.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Single corrupted snapshot file shouldn't prevent listing all other snapshot in repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13887</link><project id="" key="" /><description>To reproduce - create several snapshots, truncate one of the snapshot files and try getting the list of snapshots by running `curl -XGET localhost:9200/_snapshot/my_repo/_all`
</description><key id="109305462">13887</key><summary>Single corrupted snapshot file shouldn't prevent listing all other snapshot in repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-10-01T14:01:34Z</created><updated>2015-11-19T06:09:08Z</updated><resolved>2015-11-19T06:09:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix typo - datehistogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13886</link><project id="" key="" /><description>date_histogram in place of datehistogram
</description><key id="109283960">13886</key><summary>Fix typo - datehistogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PoOoZaQ</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-10-01T11:49:28Z</created><updated>2015-10-06T17:22:58Z</updated><resolved>2015-10-06T17:22:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-02T13:08:09Z" id="145015693">Hi @PoOoZaQ thanks for opening this PR. May I ask you to sign our [CLA](https://www.elastic.co/contributor-agreement) so we can merge it in?
</comment><comment author="PoOoZaQ" created="2015-10-06T10:32:35Z" id="145817041">Signed
</comment><comment author="clintongormley" created="2015-10-06T17:22:42Z" id="145933343">thanks @PoOoZaQ - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RangeQueryBuilder format in 1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13885</link><project id="" key="" /><description>Hi there!

I noticed something strange today with `RangeQueryBuilder` with regard to the `format` parameter in 1.7.

Although [`format` is documented here](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/query-dsl-range-query.html#_date_options), there is no sign of it in the [Java source of 1.7](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java). However, it [exists in 2.0](https://github.com/elastic/elasticsearch/blob/2.0/core/src/main/java/org/elasticsearch/index/query/RangeQueryBuilder.java#L39).

Is it possible that the `format` parameter is silently ignored in 1.7? Am I missing something?

Thanks for your time,
Oliver
</description><key id="109261641">13885</key><summary>RangeQueryBuilder format in 1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">olivere</reporter><labels /><created>2015-10-01T09:28:29Z</created><updated>2015-10-02T16:07:17Z</updated><resolved>2015-10-02T16:07:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T16:07:17Z" id="145072876">Yeah, it likes like it was only exposed via the REST API in 1.7.  This commit added it to the builder: https://github.com/elastic/elasticsearch/commit/23cf9af495ef5066c6a5d88218ed3bf4ceca9db3#diff-73c1518c0dd10aebabe2bc79b2587a14
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug with simple_query_string, minimum_should_match, and multiple fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13884</link><project id="" key="" /><description>```
# With a one-word query and minimum_should_match=-50%, adding extra non-matching fields should not matter.
# Tested on v1.7.2.

# delete and re-create the index
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test

echo 

 # insert a document
curl -XPUT 'http://localhost:9200/test/test/1' -d '
 { "title": "test document"}
 '
curl -XPOST 'http://localhost:9200/test/_refresh'

echo 

 # this correctly finds the document (f1 is a non-existent field)
 curl -XGET 'http://localhost:9200/test/test/_search' -d '{
  "query" : {
    "simple_query_string" : {
      "fields" : [ "title", "f1" ],
      "query" : "test",
      "minimum_should_match" : "-50%"
    }
  }
 }
'

echo 

# this incorrectly does not find the document (f1 and f2 are non-existent fields)
curl -XGET 'http://localhost:9200/test/test/_search' -d '{
  "query" : {
    "simple_query_string" : {
      "fields" : [ "title", "f1", "f2" ],
      "query" : "test",
      "minimum_should_match" : "-50%"
    }
  }
 }
'

echo
```
</description><key id="109217921">13884</key><summary>Bug with simple_query_string, minimum_should_match, and multiple fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">davidlbowen</reporter><labels><label>:Query DSL</label><label>bug</label></labels><created>2015-10-01T02:34:48Z</created><updated>2016-02-05T08:55:03Z</updated><resolved>2016-02-04T18:11:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T15:53:05Z" id="145069490">This does look like a bug.  The min_should_match is being applied at the wrong level:

```
GET /test/test/_validate/query?explain
{
  "query" : {
    "simple_query_string" : {
      "fields" : [ "title", "f1", "f2" ],
      "query" : "test",
      "minimum_should_match" : "-50%"
    }
  }
}
```

Returns an explanation of:

```
+((f1:test title:test f2:test)~2)
```

While the `query_string` and `multi_match` equivalents return:

```
+(title:test | f1:test | f2:test)
```
</comment><comment author="cbuescher" created="2015-10-14T10:09:12Z" id="148000511">I had a look and saw that `simple_query_string` iterates over all fields for each token in the query string and combines them all in boolean query `should` clauses, and we apply the `minimum_should_match` on the whole result. 

@clintongormley As far as I understand you, we should parse the query string for each field separately, apply the `minimum_should_match` there and then combine the result in an overall Boolean query. This however raised another question for me. Suppose we have two terms like `"query" : "test document"` instead, then currently we we get:

```
((f1:test title:test f2:test) (f1:document title:document f2:document))~1
```

If we would instead create the query per field individually we would get something like

```
((title:test title:document)~1 (f1:test f1:document)~1 (f2:test f2:document)~1)
```

While treating the query string for each field individually looks like the right behaviour in this case, I wonder if this will break other cases. wdyt?
</comment><comment author="clintongormley" created="2015-10-14T11:31:18Z" id="148023418">@cbuescher the `query_string` query takes the same approach as your first output, ie:

```
((f1:test title:test f2:test) (f1:document title:document f2:document))~1
```

I think the bug is maybe a bit more subtle.  A query across 3 fields for two terms with min should match 80% results in:

```
bool:
    min_should_match: 80% (==1)
    should:
      bool:
        should: [ f1:term1, f2:term1, f3:term1]
      bool:
        should: [ f1:term2, f2:term2, f3:term2]
```

however with only one term it is producing:

```
bool:
  min_should_match: 80% (==2) 
  should: [ f1:term1, f2:term1, f3:term1]
```

In other words, min should match is being applied to the wrong `bool` query.  Instead, even the one term case should be wrapped in another `bool` query, and the min should match should be applied at that level.
</comment><comment author="cbuescher" created="2015-10-14T12:03:12Z" id="148028555">@clintongormley Yes, I think thats what I meant. I'm working on a PR that applies the `minimum_should_match` to sub-queries that only target one field. That way your examples above would change to something like

```
bool:    
    should:
      bool:
        min_should_match: 80% (==1)
        should: [ f1:term1, f1:term2]
      bool:
        min_should_match: 80% (==1)
        should: [ f2:term1, f2:term2]
      bool:
        min_should_match: 80% (==1)
        should: [ f3:term1, f3:term2]
```

and for one term

```
bool:    
    should:
      bool:
        min_should_match: 80% (==0)
        should: [ f1:term1]
      bool:
        min_should_match: 80% (==0)
        should: [ f2:term1]
      bool:
        min_should_match: 80% (==0)
        should: [ f3:term1]
```

In the later case we already additionally simplify one-term bool queries to TermQueries.
</comment><comment author="clintongormley" created="2015-10-15T11:22:45Z" id="148358711">@cbuescher I think that is incorrect.  The simple query string query (like the query string query) is term-centric rather than field-centric.  In other words, min should match should be applied to the number of terms (regardless of which field the term is in).

I'm guessing that there is an "optimization" for the one term case where the field-level bool clause is not wrapped in an outer bool clause.  Then the min should match is applied at the field level instead of at the term level, resulting in the wrong calculation.
</comment><comment author="cbuescher" created="2015-10-15T12:49:18Z" id="148374958">That guess seems right, there is an optimization in lucenes
SimpleQueryParser for boolean queries with 0 or 1 clauses that seems to be
the problem. I think we can overwrite that.

On Thu, Oct 15, 2015 at 1:23 PM, Clinton Gormley notifications@github.com
wrote:

&gt; @cbuescher https://github.com/cbuescher I think that is incorrect. The
&gt; simple query string query (like the query string query) is term-centric
&gt; rather than field-centric. In other words, min should match should be
&gt; applied to the number of terms (regardless of which field the term is in).
&gt; 
&gt; I'm guessing that there is an "optimization" for the one term case where
&gt; the field-level bool clause is not wrapped in an outer bool clause. Then
&gt; the min should match is applied at the field level instead of at the term
&gt; level, resulting in the wrong calculation.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13884#issuecomment-148358711
&gt; .

## 

Christoph B&#252;scher
</comment><comment author="clintongormley" created="2015-10-15T15:04:19Z" id="148414875">If this is in Lucene, perhaps it should be fixed there?

@jdconrad what do you think?
</comment><comment author="cbuescher" created="2015-10-15T15:15:15Z" id="148418387">@clintongormley I don't think SimpleQueryParser#simplify() is at the root of this anymore. The problem seems to be that SimpleQueryParser parses term by term-centric, but only starts wrapping the resulting queries when combining more than two of them. For one search term and two fields I get a Boolean query with two TermQuery clauses (without enclosing Boolean query), for two terms and one field I get an enclosing Boolean query with two Boolean query subclauses. I'm not sure yet how this can be distiguished from outside of the Lucene parser without inspecting the query, and if a solution like that holds for more complicated cases.
</comment><comment author="cbuescher" created="2015-10-19T08:58:58Z" id="149151047">Althought it would be nice if Lucene SimpleQueryParse would output a Boolquery with one should-clause and three nested Boolqueries for the 1-term/multi-field case, I think we can detect this case and do the wrapping in the additional Boolquery in the SimpleQueryStringBuilder. I just opened a PR.
</comment><comment author="jdconrad" created="2015-10-19T17:15:49Z" id="149285553">The SQP wasn't really designed around multi-field terms, but needed to have it added afterwards for use as a default field which is why the min-should-match never gets applied down at that the level.  I don't know if the correct behavior is to make it work on multi-fields.  I'll have to give that some thought given that it really is as @cbuescher described as term-centric, and it sort of supposed to be disguised from the user.  One thing that will make this easier to fix, though, I believe is #4707, since it will flatten the parse tree a bit.
</comment><comment author="cbuescher" created="2015-10-20T09:10:12Z" id="149486734">@jdconrad thanks for explaining, in the meantime I opened #14186 which basically tries to distinguish the one vs. multi-field cases and tries wraps the resulting query one more time to get a correct min-should-match. Please leave comment there if my current approach will colide the plans regarding #4707.
</comment><comment author="cbuescher" created="2015-11-03T11:01:39Z" id="153315208">Reopening this issue since the fix proposed in #14186 was too fragile. Discussed with @javanna and @jpountz, at this point we think the options are either fixing this in lucenes SimpleQueryParser so that we can apply minimum_should_match correctly on the ES side or remove this option from `simple_query_string` entirely because it cannot properly supported.
</comment><comment author="cbuescher" created="2015-11-03T11:57:40Z" id="153330850">Trying to sum up this issue so far: 
- the number of should-clauses returned by `SimpleQueryParser` is not 1 for one search term and multiple fields, so we cannot apply `minimum_should_match` correctly in `SimpleQueryStringBuilder`. e.g. for `"query" : "term1", "fields" : [ "f1", "f2", "f3" ]` SimpleQueryParser returns a BooleanQuery with three should-clauses. As soon  as we add more search terms, the number of should-clauses is the same as the number of search terms, e.g. `"query" : "term1 term2", "fields" : [ "f1", "f2", "f3" ]` returns a BooleanQuery with two subclauses, one per term.
- it is difficult to determine the number of terms from the query string upfront, because the tokenization depends on the analyzer used, so we really need `SimpleQueryParser#parse()` for this.
- it is hard to determine the correct number of terms from the returned lucene query without making assumptions about the inner structure of the query (which is subject to change, reason for #14186 beeing reverted). e.g. currently `"query" : "term1", "fields" : [ "f1", "f2", "f3" ]` and `"query" : "term1 term2 term3", "fields" : [ "f1" ]` will return a BooleanQuery with same structure (three should-clauses, each containing a TermQuery). 
</comment><comment author="jimczi" created="2016-02-04T18:11:51Z" id="179975012">@cbuescher this issue is fixed by https://github.com/elastic/elasticsearch/pull/16155. 
@rmuir has pointed out a nice way to distinguish between a single word query with multiple fields against a multi word query with a single field: we just have to check if the coord are disabled on the top level BooleanQuery, the simple query parser disables the coord when the boolean query for multiple fields is built.
Though I tested the single word with multiple fields case only, if you think of other issues please reopen this ticket or open a new one ;).
</comment><comment author="cbuescher" created="2016-02-04T21:08:13Z" id="180049856">@jimferenczi thats great, I just checked this with a test which is close to the problem desciption here. I'm not sure if this adds anything to your tests, but just in case I justed opened  #16465 which adds this as an integration test for SimpleQueryStringQuery. Maybe you can take a look and tell me if it makes sense to add those as well.
</comment><comment author="jimczi" created="2016-02-05T08:55:03Z" id="180256823">@cbuescher thanks, a unit test in SimpleQueryStringBuilderTest could be useful as well. The integ test does not check the minimum should match that is applied (or not) to the boolean query. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update version incompatibility message for plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13883</link><project id="" key="" /><description>When the plugin manager does not find in `plugin-descriptor.properties` the exact same elasticsearch version it was built on
as the current elasticsearch version, it fails with a message like:

```
ERROR: Elasticsearch version [2.0.0-beta1] is too old for plugin [elasticsearch-mapper-attachments]
```

Actually, the message should be:

```
Plugin [elasticsearch-mapper-attachments] is incompatible with Elasticsearch [2.0.0.beta2]. Was designed for version [2.0.0.beta1].
```

The opposite is true. If you try to install a version of a plugin which was built with a newer version of elasticsearch, it will fail the same way:

```
Plugin [elasticsearch-mapper-attachments] is incompatible with Elasticsearch [2.0.0.beta1]. Was designed for version [2.0.0.beta2].
```
</description><key id="109196431">13883</key><summary>Update version incompatibility message for plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>non-issue</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T22:49:24Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-10-01T05:22:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-30T22:55:23Z" id="144568781">+1 this is great. it will handle "too new" as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Early terminate high disk watermark checks on single data node cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13882</link><project id="" key="" /><description>Right now, we allow allocation if there is only a single node in the
cluster. it would be nice to fail open when there is only one data node
(instead of only one node total).

closes #9391
</description><key id="109187406">13882</key><summary>Early terminate high disk watermark checks on single data node cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T21:55:20Z</created><updated>2015-10-04T16:45:11Z</updated><resolved>2015-10-03T23:18:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-03T13:28:12Z" id="145248919">Left one minor comment but otherwise this LGTM. Can you label for where this goes? Perhaps 2.1.0+
</comment><comment author="xuzha" created="2015-10-03T17:46:24Z" id="145271968">@dakrone thanks very much  for the review, just updated the comment.
Will merge into 2.1, 2.x, and master shortly if there is no other concern.
</comment><comment author="xuzha" created="2015-10-03T23:40:38Z" id="145299456">2.1: https://github.com/elastic/elasticsearch/commit/f37efce417bcb5db3e86cf617049ded6fe97fbbf
2.x: https://github.com/elastic/elasticsearch/commit/89f52bc8603e409e8b1668679c19fb79cde7b1fa
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove shard-level injector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13881</link><project id="" key="" /><description>Today we use a hierarchical injector on the shard level for each shard
created. This commit removes the shard level injector and replaces
it with good old constructor calls. This also removes all shard level plugin
facilities such that plugins can only have node or index level modules.
For plugins that need to track shard lifecycles they should use the relevant
callback from the lifecycle we already provide.
</description><key id="109181409">13881</key><summary>Remove shard-level injector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Plugins</label><label>breaking-java</label><label>PITA</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T21:17:19Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-05T12:26:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-30T21:50:56Z" id="144552304">Fantastic!! LGTM!
</comment><comment author="mikemccand" created="2015-10-01T05:42:50Z" id="144626767">LGTM!
</comment><comment author="s1monw" created="2015-10-01T08:54:31Z" id="144660110">@rjernst @mikemccand pushed some updates
</comment><comment author="rjernst" created="2015-10-01T09:12:05Z" id="144665253">LGTM
</comment><comment author="rjernst" created="2015-10-01T19:02:50Z" id="144817851">New commit still LGTM
</comment><comment author="mikemccand" created="2015-10-02T04:19:39Z" id="144912787">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nuke ES_CLASSPATH appending, JarHell fail on empty classpath elements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13880</link><project id="" key="" /><description>Old ES 1.x startup scripts were buggy, adding empty classpath elements. But this can be horrible: it means "CWD" to java, and when starting from a service maybe that is /, and now JarHell is scanning your entire computer (like #13864). 

It would be better to just fail hard on a bogus classpath, and hint at the possible cause `(outdated shell script from a previous version?)`

Additionally, users can still override ES_CLASSPATH, which caused the whole bug in the first place for the 1.x scripts, but we should fail on that too. Its not going to work and you will just get a securityexception. Instead we can tell the user how to do it better.

Maybe we want to clean this up for earlier versions (e.g. 2.1 or maybe even 2.0) too, as some of it is "our fault" (the old broken scripts) and possible still "our fault" (maybe packaging is not upgrading them properly?)

Relates to #13864
Closes #13812
</description><key id="109178987">13880</key><summary>Nuke ES_CLASSPATH appending, JarHell fail on empty classpath elements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T21:03:55Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-09-30T22:59:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-30T21:37:41Z" id="144549760">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace Guava cache with simple concurrent LRU cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13879</link><project id="" key="" /><description>This pull request replaces the Guava cache with a simple concurrent LRU cache with flexible eviction policies.
</description><key id="109170069">13879</key><summary>Replace Guava cache with simple concurrent LRU cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T20:14:14Z</created><updated>2015-10-09T15:56:19Z</updated><resolved>2015-10-09T15:56:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-06T21:35:45Z" id="146008799">We once had a use case that wanted to remove all entries in the cache who's keys started with something - it was in filter caching. The idea was to drop all entries form the cache when a segment was no longer valid. I don't know if we still have that use case though.
</comment><comment author="nik9000" created="2015-10-06T21:43:19Z" id="146010278">Oh yeah, `IndicesRequestCache$Reaper` still does it. Fun times. Anyway, its probably not worth doing something crazy in this but I always thought that there must be a simpler way to handle this use case. Something that doesn't require iterating over the entire cache every once in a while.
</comment><comment author="nik9000" created="2015-10-07T15:11:28Z" id="146225229">Its probably worth adding a note that there is only one operation that acquires multiple locks at one time, invalidateAll, and that it acquires all the segments locks first, then the lruLock.
</comment><comment author="jpountz" created="2015-10-08T21:58:44Z" id="146698294">it looks good to me but I would like someone else (@nik9000 ?) to also do another round of review before we merge
</comment><comment author="jasontedor" created="2015-10-09T14:21:15Z" id="146885244">@nik9000 Would you be able to take another look?
</comment><comment author="nik9000" created="2015-10-09T14:40:39Z" id="146891394">&gt; @nik9000 Would you be able to take another look?

Sure! I'll have a look soon.
</comment><comment author="nik9000" created="2015-10-09T15:28:38Z" id="146903702">Anything else I do will be knit picking. LGTM. Lets get it in and kick the tires.

I think its worth adding a test for the hit and miss stats. I scanned for one and didn't see it but I could have just missed it.

Is it worth adding a stat for additions?
</comment><comment author="jasontedor" created="2015-10-09T15:34:56Z" id="146905269">&gt; I think its worth adding a test for the hit and miss stats. I scanned for one and didn't see it but I could have just missed it.

@nik9000 Did you have something in mind that is different than what is in `CacheTests#testCacheStats`? I think this tests both hit and miss stats, plus evictions.

&gt; Is it worth adding a stat for additions?

I don't think so (am I being silly in thinking that it's just `Cache.count() + Cache.stats().getEvictions()`?
</comment><comment author="nik9000" created="2015-10-09T15:37:53Z" id="146905951">You are right on both counts there.

LGTM
</comment><comment author="jasontedor" created="2015-10-09T15:45:37Z" id="146907676">@nik9000 @jpountz @dakrone @rmuir Thanks for reviewing. Bringing it home!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed some messy tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13878</link><project id="" key="" /><description /><key id="109165881">13878</key><summary>fixed some messy tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>test</label></labels><created>2015-09-30T19:51:24Z</created><updated>2015-09-30T20:13:56Z</updated><resolved>2015-09-30T20:13:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-30T19:58:53Z" id="144522362">+1 for fixing these, thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES_HOME is set to null in windows service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13877</link><project id="" key="" /><description>I'm running a single instance of Elasticsearch 1.7.1 on Windows 10. I use some custom Elasticsearch plugins which need to read the value of `ES_HOME` environment variable. I use `System.getenv("ES_HOME")` to read it. I've not set the value of `ES_HOME` environment variable on this machine. When I start Elasticsearch by executing `elasticsearch.bat`, the value of `ES_HOME` is correctly read. But when I install Elasticsearch service and start it using `service.bat install` and `service.bat start`, the value of `ES_HOME` is `null`. If I set `ES_HOME` environment variable on my machine, then there is no problem at all but I want to minimize the dependency on environment variables as much as possible.

This looks like a bug to me as there is discrepancy in behaviour between two launching machanisms. Thoughts?
</description><key id="109153164">13877</key><summary>ES_HOME is set to null in windows service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">bittusarkar</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-09-30T18:36:19Z</created><updated>2016-02-10T05:56:08Z</updated><resolved>2016-02-10T05:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T15:19:07Z" id="145059218">@Mpdreamz could you take a look at this please?
</comment><comment author="bittusarkar" created="2015-10-09T00:10:21Z" id="146721654">@Mpdreamz were you able to make any progress on this?
</comment><comment author="bittusarkar" created="2015-12-10T07:47:41Z" id="163526081">Pinging again
</comment><comment author="gmarz" created="2016-02-09T21:49:46Z" id="182092846">@bittusarkar ES_HOME isn't set globally, so the service won't pick up on it since it's ran separately in it's own process.  Even if we set it globally (which is probably a bad idea) you're still relying on environment variables- the only difference being local vs global.

What if you try `System.getProperty("es.path.home")` instead?
</comment><comment author="bittusarkar" created="2016-02-10T05:56:08Z" id="182210072">Awesome! `System.getProperty("es.path.home")` works for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to forbidden-apis 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13876</link><project id="" key="" /><description>This is just to update to forbidden-apis 2.0. I know the gradle build is coming, but it is still good to update.

New features:
This is the major 2.0 release of the forbidden-apis plugin. The main new
feature is native support for the [Gradle](https://gradle.org/) build system (minimum requirement is Gradle 2.3).
But also Apache Ant and Apache Maven build systems got improved support: Ant can now load signatures from
arbitrary resources by using a new XML element `&lt;signatures&gt;&lt;/signatures&gt;` that may contain
any valid ANT resource, e.g., ivy's cache-filesets or plain URLs. Apache Maven now supports
to load signatures files as artifacts from your repository or Maven Central (new `signaturesArtifacts`
Mojo property).

It also adds missing java.time API signatures to the famous "unsafe" bundled signatures.

There was one change in Maven: the default lifecycle phase is now "verify", but this has no effect on ES, because it explicitely declares lifecycle phases.
</description><key id="109149604">13876</key><summary>Update to forbidden-apis 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uschindler</reporter><labels><label>build</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T18:17:18Z</created><updated>2015-09-30T19:31:13Z</updated><resolved>2015-09-30T18:35:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-30T18:27:29Z" id="144499561">Thanks Uwe, I'll test this out real quick and push :)
</comment><comment author="rmuir" created="2015-09-30T18:35:39Z" id="144501676">This is great, thanks @uschindler 

Also this update is nice since it works with java 9 jigsaw rather than disabling itself, though I have not looked into where these warnings come from. 

```
[INFO] --- forbiddenapis:2.0:check (check-forbidden-apis) @ elasticsearch ---
[INFO] Scanning for classes to check...
[INFO] Reading bundled API signatures: jdk-unsafe-1.8
[INFO] Reading bundled API signatures: jdk-deprecated-1.8
[WARNING] Method not found while parsing signature: java.awt.Component#getPeer() [signature ignored]
[WARNING] Method not found while parsing signature: java.awt.Font#getPeer() [signature ignored]
[WARNING] Method not found while parsing signature: java.awt.MenuComponent#getPeer() [signature ignored]
[WARNING] Method not found while parsing signature: java.awt.Toolkit#getFontPeer(java.lang.String,int) [signature ignored]
[WARNING] Method not found while parsing signature: java.util.jar.Pack200$Packer#addPropertyChangeListener(java.beans.PropertyChangeListener) [signature ignored]
[WARNING] Method not found while parsing signature: java.util.jar.Pack200$Packer#removePropertyChangeListener(java.beans.PropertyChangeListener) [signature ignored]
[WARNING] Method not found while parsing signature: java.util.jar.Pack200$Unpacker#addPropertyChangeListener(java.beans.PropertyChangeListener) [signature ignored]
[WARNING] Method not found while parsing signature: java.util.jar.Pack200$Unpacker#removePropertyChangeListener(java.beans.PropertyChangeListener) [signature ignored]
[WARNING] Method not found while parsing signature: java.util.logging.LogManager#addPropertyChangeListener(java.beans.PropertyChangeListener) [signature ignored]
[WARNING] Method not found while parsing signature: java.util.logging.LogManager#removePropertyChangeListener(java.beans.PropertyChangeListener) [signature ignored]
[INFO] Reading bundled API signatures: jdk-system-out
[INFO] Reading API signatures: /home/rmuir/workspace/elasticsearch/core/target/dev-tools/forbidden/core-signatures.txt
[INFO] Reading API signatures: /home/rmuir/workspace/elasticsearch/core/target/dev-tools/forbidden/all-signatures.txt
[INFO] Reading API signatures: /home/rmuir/workspace/elasticsearch/core/target/dev-tools/forbidden/third-party-signatures.txt
[INFO] Loading classes to check...
[INFO] Scanning classes for violations...
[INFO] Scanned 5622 (and 1543 related) class file(s) for forbidden API invocations (in 2.06s), 0 error(s).
```
</comment><comment author="uschindler" created="2015-09-30T18:41:58Z" id="144503200">The warnings are fine. They are caused by the fact that Java 9 actually REMOVED deprecated APIs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: fix test by making copy of geopoints in assertLuceneQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13875</link><project id="" key="" /><description>This avoids modifying the point in a query builder shell during assertions in `doAssertLuceneQuery'
</description><key id="109129516">13875</key><summary>Tests: fix test by making copy of geopoints in assertLuceneQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>test</label></labels><created>2015-09-30T16:25:00Z</created><updated>2015-10-12T10:28:48Z</updated><resolved>2015-10-12T10:28:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-30T18:29:46Z" id="144500074">LGTM
</comment><comment author="cbuescher" created="2015-10-12T10:28:48Z" id="147360028">Merged with master with 97db3d5
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need support for four digit locales</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13874</link><project id="" key="" /><description>Currently ELK only supports two digit language information: en, fr, es etc. We would like support for four digit locale support: en-us, fr-fr. es-es, es-mx etc
</description><key id="109119623">13874</key><summary>Need support for four digit locales</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">evernon9681</reporter><labels><label>feedback_needed</label></labels><created>2015-09-30T15:40:55Z</created><updated>2016-01-28T18:29:32Z</updated><resolved>2016-01-28T18:29:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-30T17:41:09Z" id="144487155">Can you be more specific about where you are providing elasticsearch with these locales?
</comment><comment author="clintongormley" created="2016-01-28T18:29:32Z" id="176324610">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic: Failed to resolve config path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13873</link><project id="" key="" /><description>I'm banging my head against the screen with this problem.  I'm trying to configure stopwords for an index.  This works on my local system, an Ubuntu system.

Using Elastic 1.7.2

```
curl -X POST -H "Content-Type: application/json" -H "Cache-Control: no-cache" -H "Postman-Token: 34bbe264-ad79-de46-d5cd-293c5a9a1584" -d '{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english": {
          "type":           "english",
          "stopwords_path": "stopwords_en.txt" 
        }
      }
    }
  }
}' 'http://10.40.1.55:9200/my_index'
```

This is the trace caused from using the Java API (both cURL and Java produce the error)

```
Caused by: org.elasticsearch.env.FailedToResolveConfigException: Failed to resolve config path [/stopwords/stopwords_en.txt], tried file path [/stopwords/stopwords_en.txt], path file [/etc/elasticsearch/stopwords/stopwords_en.txt], and classpath
        at org.elasticsearch.env.Environment.resolveConfig(Environment.java:213)
        at org.elasticsearch.index.analysis.Analysis.getWordList(Analysis.java:230)
        at org.elasticsearch.index.analysis.Analysis.parseWords(Analysis.java:165)
        at org.elasticsearch.index.analysis.Analysis.parseStopWords(Analysis.java:185)
        at org.elasticsearch.index.analysis.Analysis.parseStopWords(Analysis.java:181)
        at org.elasticsearch.index.analysis.StandardAnalyzerProvider.&lt;init&gt;(StandardAnalyzerProvider.java:53)
        at sun.reflect.GeneratedConstructorAccessor135.newInstance(Unknown Source)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.InjectorImpl$5$1.call(InjectorImpl.java:781)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
        at org.elasticsearch.common.inject.InjectorImpl$5.get(InjectorImpl.java:777)
        at org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:221)
        at com.sun.proxy.$Proxy18.create(Unknown Source)
```

Permissions on /etc/elasticsearch are wide open.

```
drwxrwxrwx   3 root root  4096 Sep 30 15:03 ./
drwxr-xr-x 111 root root  4096 Sep 18 16:01 ../
-rwxrwxrwx   1 root root 13476 Apr 27 10:07 elasticsearch.yml*
-rwxrwxrwx   1 root root  2054 Sep 14 11:51 logging.yml*
-rwxr--r--   1 root root  4588 Sep 30 15:03 stopwords_en.txt*
```

There is nothing in the `elasticsearch.log`

Any insight would be greatly appreciated.
</description><key id="109091948">13873</key><summary>Elastic: Failed to resolve config path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">itonics-tbeauvais</reporter><labels /><created>2015-09-30T13:47:25Z</created><updated>2015-09-30T15:07:35Z</updated><resolved>2015-09-30T15:07:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="itonics-tbeauvais" created="2015-09-30T15:07:35Z" id="144440346">Ugh.. it was automatically clustering.. _face palm_
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] Prefix option for getting index settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13872</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-settings.html#_prefix_option

This documentation is wrong. The correct way to do this is by:

```
GET /test/_settings/index.translog.*
```
</description><key id="109090849">13872</key><summary>[Docs] Prefix option for getting index settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmoskovicz</reporter><labels><label>adoptme</label><label>bug</label><label>docs</label></labels><created>2015-09-30T13:43:04Z</created><updated>2015-10-02T14:47:18Z</updated><resolved>2015-10-02T14:47:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Vagrant tests: Non-reactor builds breaks bats tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13871</link><project id="" key="" /><description>Running full vagrant tests like `mvn clean verify` from the root results in copying this dependency

```
[INFO] Copying elasticsearch-3.0.0-SNAPSHOT.deb to /Users/alr/devel/elasticsearch/qa/vagrant/target/testroot/elasticsearch-3.0.0-SNAPSHOT.deb
```

where as running mvn with the `-pl` or the `-rf` parameter copies the debian package with a timestamp at the end, like this

```
[INFO] Copying elasticsearch-3.0.0-SNAPSHOT.deb to /Users/alr/devel/elasticsearch/qa/vagrant/target/testroot/elasticsearch-3.0.0-20150930.001002-241.deb
```

this leads to the debian package not being found, because we check for the strict version

```
     [exec] trusty: # dpkg: error processing archive elasticsearch-3.0.0-SNAPSHOT.deb (--install):
     [exec] trusty: #  cannot access archive: No such file or directory
```

The bug is in `./src/test/resources/packaging/scripts/30_deb_package.bats` and `./src/test/resources/packaging/scripts/40_rpm_package.bats` because we refer to the packages like this: `elasticsearch-$(cat version).deb`

Also we could upgrade to beta2 for the upgrade test
</description><key id="109078856">13871</key><summary>Vagrant tests: Non-reactor builds breaks bats tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>test</label></labels><created>2015-09-30T12:43:17Z</created><updated>2016-01-29T08:32:43Z</updated><resolved>2016-01-29T08:32:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-10-05T15:35:45Z" id="145572688">I cannot reproduce this. I tried 
`mvn clean install -DskipTests`
`mvn -Dtests.vagrant -pl qa/vagrant pre-integration-test`
`mvn -Dtests.vagrant=all -pl qa/vagrant verify` 
and all works as expected.
Can you give the exact order of commands you execute?
</comment><comment author="spinscale" created="2015-10-06T08:00:22Z" id="145775706">I did not run `mvn install`, which makes this fail reliably for me
</comment><comment author="spinscale" created="2016-01-29T08:32:43Z" id="176639515">closing this due to gradle switch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Close TokenStream in finally clause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13870</link><project id="" key="" /><description>We have a couple places where we might fail to close a `TokenStream` on exception ...

Maybe closes #11947
</description><key id="109063485">13870</key><summary>Close TokenStream in finally clause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-30T11:01:05Z</created><updated>2015-10-02T13:43:26Z</updated><resolved>2015-10-02T13:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-30T11:20:29Z" id="144364619">LGTM, would be great if we had a nice mock token stream thingy that could keep track of the open streams like we do for thread leaks and unclosed contexts :)
</comment><comment author="johtani" created="2015-09-30T13:00:12Z" id="144388603">Are there same issues?
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/highlight/PlainHighlighter.java#L112
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java#L490
https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/apache/lucene/queryparser/classic/MapperQueryParser.java#L621
</comment><comment author="rmuir" created="2015-09-30T16:39:06Z" id="144470592">&gt; LGTM, would be great if we had a nice mock token stream thingy that could keep track of the open streams like we do for thread leaks and unclosed contexts :)

We have MockTokenizer in lucene for this.
</comment><comment author="mikemccand" created="2015-09-30T17:14:18Z" id="144480378">Thanks @johtani, I pushed a few more commits to fix those and some additional places.

I changed a few private APIs to take Analyzer + field + contents, instead of TokenStream, so I could use try-with-resources.

I had to push the close back down into `SuggestUtils.analyze` for a crazy case where the suggester consumed a token stream but recursed on itself and would then try to illegally reuse the same `TokenStream` instance again (while still iterating that `TokenStram` up above).  So the high-level try-with-resources did the close "too late".
</comment><comment author="johtani" created="2015-09-30T18:31:05Z" id="144500417">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update inner-hits.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13869</link><project id="" key="" /><description>Fix a glitch in inner_hits feature documentation (though I'm not absolutely sure of the final version)
</description><key id="109046273">13869</key><summary>Update inner-hits.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tcucchietti</reporter><labels><label>docs</label></labels><created>2015-09-30T09:08:32Z</created><updated>2015-09-30T09:16:56Z</updated><resolved>2015-09-30T09:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-30T09:16:50Z" id="144335398">thanks @tcucchietti!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update example with parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13868</link><project id="" key="" /><description>Update to show how to use the 'Request Parameters', not as a javascript object, but as query arguments.
</description><key id="109022511">13868</key><summary>Update example with parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hafkensite</reporter><labels><label>docs</label></labels><created>2015-09-30T06:23:32Z</created><updated>2015-10-02T14:24:36Z</updated><resolved>2015-10-02T14:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-30T08:14:09Z" id="144322613">@hafkensite thanks! Can you sign the [CLA](https://www.elastic.co/contributor-agreement) and I will merge this in?
</comment><comment author="clintongormley" created="2015-10-02T14:24:36Z" id="145036414">thanks @hafkensite - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>compile-time scan for accessibility issues on 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13867</link><project id="" key="" /><description>Simon solved most of this with java 8 method references in trunk, but we need another solution for java 7. There are two things ideally to check:
- `@Inject`ed methods/constructors are public and their classes are public.
- subclasses of `TransportRequest` are public and have a public no-arg constructor

I tried to make sure those things delivered good exceptions when fixing the code this way initially. I also used grep and IDE class hierarchy and stuff, and spent a lot of time. But 2.x is a different codebase, which will change too, and it would be nicer if the build failed with a static scan somehow. It could be a separate evil tool we launch from ant even. :)

Here is the meat of my changes so you get the idea:

https://github.com/elastic/elasticsearch/commit/e1efd5f6baf8dec1c23c79b346b8a29f234a98bb
</description><key id="109011224">13867</key><summary>compile-time scan for accessibility issues on 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>adoptme</label><label>enhancement</label></labels><created>2015-09-30T04:18:19Z</created><updated>2016-11-06T07:58:13Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add warning on cluster.routing.allocation.balance settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13866</link><project id="" key="" /><description>https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html#_balanced_shards

Have yet to see a successful usage of these balance settings.  On the contrary, have seen end users from different deployments trying to alter these from the defaults and ended up causing more unbalancing issues in their cluster and had to revert to the defaults as the resolution.

Can we add a warning on these settings and discourage users from changing them?
</description><key id="108985371">13866</key><summary>Add warning on cluster.routing.allocation.balance settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-09-30T00:14:50Z</created><updated>2016-01-15T12:40:56Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-30T08:16:03Z" id="144322892">&gt; Can we add a warning on these settings and discourage users from changing them?

I think they should be removed from the documentation (ie, expert-only settings!)
</comment><comment author="ppf2" created="2015-09-30T08:19:15Z" id="144323388">+1 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch service not starting on Debian 8.2 boot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13865</link><project id="" key="" /><description>Elasticsearch 1.7.2 is installed (using the Debian repo you guys provide) and works properly. When I run `sudo service elasticsearch start`, it starts up and runs as expected. I've set it up to start on boot by issuing `sudo update-rc.d elasticsearch defaults 95 10`. `elasticsearch` exists in both `/etc/init.d` and `/etc/rc[0-6].d`. Every time I boot my server, though, it doesn't start; I have to boot it manually. The only thing that exists in `/var/log/elasticsearch/elasticsearch.log` is this:

```
[2015-09-29 17:27:10,696][INFO ][node                     ] [Changeling] stopping ...
[2015-09-29 17:27:10,748][INFO ][node                     ] [Changeling] stopped
[2015-09-29 17:27:10,748][INFO ][node                     ] [Changeling] closing ...
[2015-09-29 17:27:10,754][INFO ][node                     ] [Changeling] closed
```

What might be causing this? I'm not sure where to look. I've grepped the log files in `/var/log/` without success.
</description><key id="108981801">13865</key><summary>elasticsearch service not starting on Debian 8.2 boot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gfairchild</reporter><labels><label>adoptme</label><label>docs</label><label>low hanging fruit</label></labels><created>2015-09-29T23:40:42Z</created><updated>2015-10-08T16:09:29Z</updated><resolved>2015-10-08T16:09:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-09-30T08:05:32Z" id="144320791">Debian 8 uses systemd to manage services, can you please use systemd commands (see https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html#_using_systemd) instead to check if it works better?
</comment><comment author="gfairchild" created="2015-09-30T17:11:39Z" id="144479787">That was indeed the problem! Thanks very much! You might want to see about updating the documentation to clarify that. The "Debian/Ubuntu" header doesn't give any indication that the new versions of Debian don't use update-rc.d.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception in 2.0.0-beta2: "java.nio.file.AccessDeniedException: /var/cache/ldconfig"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13864</link><project id="" key="" /><description>While trying to start up elasticsearch 2.0.0 beta2 I'm running into:

```
[2015-09-29 16:00:30,019][ERROR][org.elasticsearch.bootstrap] Exception
java.nio.file.AccessDeniedException: /var/cache/ldconfig
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newDirectoryStream(UnixFileSystemProvider.java:426)
    at java.nio.file.Files.newDirectoryStream(Files.java:413)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:179)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:199)
    at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:69)
    at java.nio.file.Files.walkFileTree(Files.java:2602)
    at java.nio.file.Files.walkFileTree(Files.java:2635)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:137)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:86)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:154)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:266)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```
</description><key id="108979428">13864</key><summary>Exception in 2.0.0-beta2: "java.nio.file.AccessDeniedException: /var/cache/ldconfig"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DevBOFH</reporter><labels /><created>2015-09-29T23:18:13Z</created><updated>2015-10-02T16:11:23Z</updated><resolved>2015-10-02T16:11:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-29T23:40:44Z" id="144221611">Why is this in your classpath?
</comment><comment author="rmuir" created="2015-09-29T23:43:50Z" id="144222058">And if you run with -Des.logger.level=DEBUG, you will get a lot of information, that might tell us why the system is misconfigured like that, the classpath shouldn't contain directories like that, that you don't have access to.
</comment><comment author="DevBOFH" created="2015-09-30T01:01:46Z" id="144231760">Why is /var/cache/ldconfig in the classpath? It isn't as far as I can tell.

ES_CLASSPATH is `ES_CLASSPATH=$ES_CLASSPATH:$ES_HOME/lib/*:$ES_HOME/lib/sigar/*`

If I open up permissions on that dir it keeps trying to access various directories and fails with the same exception.

Directories it's complained about:

```
/var/cache/ldconfig
/var/lib/polkit-1
/var/lib/sudo
```

Also, I put `-Des.logger.level=DEBUG` into my elasticsearch-env.sh file and it doesn't appear to show any additional output in the log file.
</comment><comment author="rmuir" created="2015-09-30T01:43:26Z" id="144252525">You aren't running 2.0-beta2, but some frankenstein installation:

rmuir@beast:~/Downloads/elasticsearch-2.0.0-beta2$ fgrep -r sigar .
rmuir@beast:~/Downloads/elasticsearch-2.0.0-beta2$ 
</comment><comment author="nik9000" created="2015-09-30T19:39:45Z" id="144517225">&gt; You aren't running 2.0-beta2, but some frankenstein installation:

Can you post the steps you took to get here?
</comment><comment author="DevBOFH" created="2015-10-01T18:49:08Z" id="144814580">It looks like this was an issue caused by our config management system, chef in this case, mixing older ES 1.7 scripts with ES 2.0.
</comment><comment author="clintongormley" created="2015-10-02T16:11:23Z" id="145073765">thanks for letting us know @BCWilsonDotCom 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: set has_parent &amp; has_child types context properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13863</link><project id="" key="" /><description>While refactoring has_child and has_parent query we lost an important detail around types. The types that the inner query gets executed against shouldn't be the main types of the search request but the parent or child type set to the parent query. We used to use QueryParseContext#setTypesWithPrevious as part of XContentStructure class which has been deleted, without taking care though of setting the types and restoring them as part of the innerQuery#toQuery call.
</description><key id="108968517">13863</key><summary>Query refactoring: set has_parent &amp; has_child types context properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T21:58:45Z</created><updated>2015-10-02T14:32:19Z</updated><resolved>2015-09-30T10:21:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-29T23:04:50Z" id="144216356">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid index name `.` and `..`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13862</link><project id="" key="" /><description>Fixes #13858
</description><key id="108941380">13862</key><summary>Forbid index name `.` and `..`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Index APIs</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T19:24:45Z</created><updated>2015-10-06T12:53:51Z</updated><resolved>2015-10-05T03:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-29T19:29:07Z" id="144164316">@xuzha - it looks good to me. Did you verify @divanikus's report first? I trust @divanikus but I figure its good to verify things like this.
</comment><comment author="rmuir" created="2015-09-29T19:48:40Z" id="144168990">other possibilities like .., /, and \ could probably cause the same issue.
</comment><comment author="divanikus" created="2015-09-29T20:04:17Z" id="144174911">I also think that other cases are possible. Most dangerous could be ".." or something like that.
I'm running 1.7.1 if any.
</comment><comment author="xuzha" created="2015-09-29T21:37:53Z" id="144198698">@nik9000  Thanks, I did verify this via TransportClient. 
Our Rest API treat . and .. as current path and parents path.

I could create index with `.`, but when I tried to delete the index, ES give me the exception: 

```
java.io.IOException: Could not remove the following files (in the order of attempts):
  ...core/data/elasticsearch/nodes/1/indices/.: java.nio.file.FileSystemException: /Users/Xu/workspace/elasticsearch/core/data/elasticsearch/nodes/1/indices/.: Invalid argument

    at org.apache.lucene.util.IOUtils.rm(IOUtils.java:295)
    at org.elasticsearch.env.NodeEnvironment.deleteIndexDirectoryUnderLock(NodeEnvironment.java:424)
    at org.elasticsearch.env.NodeEnvironment.deleteIndexDirectorySafe(NodeEnvironment.java:406)
    at org.elasticsearch.indices.IndicesService.deleteIndexStore(IndicesService.java:522)
    at org.elasticsearch.indices.IndicesService.removeIndex(IndicesService.java:429)
    at org.elasticsearch.indices.IndicesService.deleteIndex(IndicesService.java:471)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.deleteIndex(IndicesClusterStateService.java:797)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedIndices(IndicesClusterStateService.java:234)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:171)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:493)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:231)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:194)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-29 14:33:57,953][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:57,965][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:57,990][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:58,032][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:58,115][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:58,281][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:58,606][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:33:59,251][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying
[2015-09-29 14:34:00,535][WARN ][indices                  ] [Zarek] [.] still pending deletes present for shards [[.]] - retrying

```

In any case, we should forbid name with `.` , `..`  and others. I just cause problems at file system
</comment><comment author="spinscale" created="2015-09-30T15:52:31Z" id="144455778">I dug a little bit into this... my idea was to check in `NodeEnvironment.deleteIndexDirectoryUnderLock()`, which calls `indexPaths(index)` to get the different paths of an index. We could check in that method that resolving to an absolute path actually ends with the index name. Sth like

```
if (false == nodePaths[i].indicesPath.resolve(index.name()).toAbsolutePath().normalize().toString().endsWith(index.getName())) {
 throw some fancy exception here?
}
```

this would prevent any weird path there/is it sufficient?

**Update**: This is still not sufficient as I thought, sorry for the noise.
</comment><comment author="xuzha" created="2015-09-30T17:53:21Z" id="144490026">Thanks @spinscale, I think we still need to forbid these `.` and `..` index names, just because ES need to create a dir using index name.
</comment><comment author="xuzha" created="2015-10-01T08:09:45Z" id="144651206">@nik9000, would you like to take another look ? 
I'm not sure if this is appropriate I just forbid  `.` and `..`  here. Or should we wait for a more concrete solution for https://github.com/elastic/elasticsearch/issues/9059 
</comment><comment author="nik9000" created="2015-10-01T23:51:39Z" id="144879790">I'm fine with this as is - I don't think its strong enough but it gets us further and prevents some silly mistakes.
</comment><comment author="nik9000" created="2015-10-01T23:54:16Z" id="144880105">Like - we should still think about preventing more bad index names in a more holistic way - but this isn't going to stop that.
</comment><comment author="xuzha" created="2015-10-02T18:39:56Z" id="145119102">Like Nik said, this is definitely not strong enough, it just a start. If there is no objection, I would merge it in tomorrow.
</comment><comment author="xuzha" created="2015-10-05T03:23:56Z" id="145419784">2.1 https://github.com/elastic/elasticsearch/commit/7edac7ea27acba02ae1e0809472b30e5ed794d1b
2.x https://github.com/elastic/elasticsearch/commit/24f6809112ea93e53378e8ed89267afa49fd655e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>install groovy plugin before running script test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13861</link><project id="" key="" /><description>groovy moved to a plugin but the tests rely on it
see #13834
I also wonder why we test scripting here at all.

@nik9000 can you take a look? also, sorry I did not run all test before #13856
</description><key id="108927093">13861</key><summary>install groovy plugin before running script test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T18:15:50Z</created><updated>2015-10-06T12:19:59Z</updated><resolved>2015-10-06T12:18:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-29T19:21:07Z" id="144161491">Fine by me. I'll be happy when we have a scripting language enabled by default again!
</comment><comment author="brwe" created="2015-09-30T12:34:18Z" id="144383467">@nik9000 I messed something up while testing. Test don't actually pass yet with my fix, they just fail at a later stage. Sorry, I'll figure out what is wrong and update shortly...
</comment><comment author="brwe" created="2015-10-05T11:35:48Z" id="145501703">I know why the tests fail now. I made a pr here: https://github.com/elastic/elasticsearch/pull/13933 Once this is in the vagrant tests all pass with this pr.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to get all indices that ever existed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13860</link><project id="" key="" /><description>Hello,

is there a way to find out the names of all the indices ever created? Even after the index might have been deleted. Does elastic store such historical info?

Thanks
</description><key id="108926563">13860</key><summary>How to get all indices that ever existed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2015-09-29T18:13:59Z</created><updated>2015-09-29T20:17:26Z</updated><resolved>2015-09-29T19:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-29T19:22:29Z" id="144161986">It doesn't. The best place to look is the logs. 

I believe the right place to ask things like this is discuss.elastic.co. 
</comment><comment author="abtpst" created="2015-09-29T20:17:26Z" id="144179646">thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor SearchRequest to be parsed on the coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13859</link><project id="" key="" /><description>This change refactors the SearchRequest so it is parsed on the coordinating node and then sent as a serialised object to the shards. The SearchSourceBuilder encompasses the state of the bpdy of the search request (know in the code as source) and is embedded in the SearchRequest. On the shard the SearchSourceBuilder is used to populate the SearchContext where we used to parse the source JSON.

A few things to note:
- The Java API has been changed to only accept builders in the SearchRequest, SearchRequestBuilder, SearchSourceBuilder and other similar classes. This means that all methods that accepted BytesReference, byte[], String, BytesArray etc. in the search API (and derivatives) should have been removed
- Some variables inside the SearchSourceBuilder are stored as BytesReference object as their builder objects have not yet been refactored. The SearchSourceBuilder converts the incoming Builder into the BytesReference to serialise to the shards and the parsing of this JSON is still handled in SearchService.parseSource(). Over time these will be refactored so these builders can also be stored as objects in SearchSourceBuilder and serialised to the shards properly.
</description><key id="108911723">13859</key><summary>Refactor SearchRequest to be parsed on the coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Internal</label><label>breaking-java</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T16:52:50Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-10-15T14:00:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-10-08T10:41:06Z" id="146495812">@javanna I think this is ready for another review
</comment><comment author="s1monw" created="2015-10-15T08:20:24Z" id="148315084">@colings86 this LGTM but I wonder what happened to all the `ParseElements` like `HighlighterParseElement` since we used to have all the parsing in there I wonder if we still need id and separately why we don't reuse the code from them to parse the actual sources at some point. I might just miss something but I though we are parsing all that stuff now already or is that still opaque (which is ok!) I just wonder if we miss something here....
</comment><comment author="javanna" created="2015-10-15T12:50:25Z" id="148375171">we decided with @colings86 to make the source builder not nullable in the SearchRequest, but to do it after we merge this PR to master. I just did a final round and this LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unsafe index names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13858</link><project id="" key="" /><description>I have accidently created an index with name "." (dot). It was created without any problems. Tried to delete it and elastic did deleted whole my data on whole cluster.
</description><key id="108911142">13858</key><summary>Unsafe index names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">divanikus</reporter><labels><label>:Index APIs</label><label>bug</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T16:50:28Z</created><updated>2015-10-05T03:24:40Z</updated><resolved>2015-10-05T03:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove the disabled autogenerated id optimization from InternalEngine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13857</link><project id="" key="" /><description>If a document is indexed into ES with no id, ES will generate one for it. We used to have an optimization for this case where the engine will not try to resolve the ids of these request in the existing index but immediately try to index them. This optimization has proven to be the source of brittle bugs (solved!) and we disabled it in 1.5, preparing for it to be removed if no performance degradation was found. Since we haven't seen any such degradation we can remove it.

Along with the removal of the optmization, we can remove the autogenerate id flag on indexing requests and the can have duplicate flag. The only downside of the removal of the canHaveDuplicate flag is that we can't make sure any more that when we retry an autogenerated id create operation we will ignore any document already exists exception (See https://github.com/elasticsearch/elasticsearch/pull/9125  for background and discussion). To work around this, we don't set the operation to CREATE any more when we generate an id, so the resulting request will never fail when it finds an existing doc but do return a version of 2. I think that's acceptable. 
</description><key id="108907197">13857</key><summary>Remove the disabled autogenerated id optimization from InternalEngine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Engine</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T16:30:35Z</created><updated>2015-09-30T17:36:15Z</updated><resolved>2015-09-30T14:18:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-29T16:30:47Z" id="144112378">@s1monw can you take a look?
</comment><comment author="mikemccand" created="2015-09-29T18:53:13Z" id="144154954">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add lang-groovy to plugin vagrant test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13856</link><project id="" key="" /><description>plugin tests fail otherwise
</description><key id="108900501">13856</key><summary>add lang-groovy to plugin vagrant test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T15:57:28Z</created><updated>2015-10-02T15:56:50Z</updated><resolved>2015-09-29T16:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-29T15:58:09Z" id="144102662">@nik9000 can you take a look?
</comment><comment author="nik9000" created="2015-09-29T16:23:00Z" id="144110539">Fine by me!
On Sep 29, 2015 5:58 PM, "Britta Weber" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 can you take a look?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13856#issuecomment-144102662
&gt; .
</comment><comment author="rmuir" created="2015-10-01T06:53:16Z" id="144637691">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trailing JSON object should throw invalid JSON exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13855</link><project id="" key="" /><description>The following document is accepted with throwing an invalid json exception:

```
PUT t/t/1
{
  "foo": "bar"
}{}
```

Apparently this is not being caught by the changes in https://github.com/elastic/elasticsearch/pull/11414
</description><key id="108899163">13855</key><summary>Trailing JSON object should throw invalid JSON exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:CRUD</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2015-09-29T15:50:27Z</created><updated>2015-10-27T18:52:39Z</updated><resolved>2015-10-02T14:20:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-09-30T05:49:44Z" id="144288274">@clintongormley , I just tested this against 2.0, seems changes in #11414 do catch this and throw `mapper_parsing_exception` [here](https://github.com/elastic/elasticsearch/blob/6885ab96805990d83e57dd0baded83252d7a2e0e/core/src/main/java/org/elasticsearch/index/mapper/DocumentParser.java#L142): 
</comment><comment author="clintongormley" created="2015-10-02T14:20:50Z" id="145035034">This is very curious.  #11414 was merged into beta1, but beta1 still fails to catch this malformed document.  However beta2 does throw an exception.  Not sure what else changed, but this appears to be fixed so I'll close
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add SpecialPermission to guard exceptions to security policy.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13854</link><project id="" key="" /><description>In some cases (e.g. buggy cloud libraries, scripting engines), we must
grant dangerous permissions to contained cases. Those AccessController blocks
are dangerous, since they truncate the stack, and can allow privilege escalation.

This PR adds a simple permission to check before each one, so that unprivileged code
like groovy scripts, can't do anything they shouldn't be allowed to do otherwise.
</description><key id="108898673">13854</key><summary>Add SpecialPermission to guard exceptions to security policy.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T15:48:24Z</created><updated>2015-10-02T15:58:31Z</updated><resolved>2015-09-29T21:34:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-09-29T15:54:44Z" id="144101770">LGTM
</comment><comment author="rmuir" created="2015-09-29T16:01:22Z" id="144103574">I added another commit with additional paranoia for two scripting cases.
</comment><comment author="jdconrad" created="2015-09-29T16:03:48Z" id="144104169">Still LGTM.
</comment><comment author="kimchy" created="2015-09-29T16:16:01Z" id="144108477">@rmuir is my understanding correct that we should always call this now instead of mistakenly only called `AccessController` without it? If so, maybe in a future change, we can forbid calling AccessController except in a wrapper class, that does the security manager special permission check automatically?
</comment><comment author="rmuir" created="2015-09-29T16:19:17Z" id="144109274">That is the basics of it. See my second commit though, its just as bad if e.g. you get a SomethingEvil in a block but then expose it to another method unprotected. 

As far as a helper method, i don't think we should do it, because it will corrupt the stack with additional codebases and will not work :)

I have tried really hard to set things up, in such a way that we avoid code like this in general, the security stuff should be transparent:  but we have these exceptions to the rules, and I want to prevent "race to the bottom" where the least secure plugin you have drags down the entire system.
</comment><comment author="rmuir" created="2015-09-29T16:26:55Z" id="144111496">Some of these things are explained in a better way than I can, via http://www.oracle.com/technetwork/java/seccodeguide-139067.html#9

Especially ones like 9.5, which is what groovyscriptingengine does (it caches that classloader).
</comment><comment author="rmuir" created="2015-09-29T16:31:21Z" id="144112512">A great example is groovy scripts today, we give them minimal permissions (https://github.com/elastic/elasticsearch/blob/master/core/src/main/resources/org/elasticsearch/bootstrap/groovy.policy#L24-L31), but if you had the python plugin installed, all the script has to do is get ahold of scripting service and compile a python plugin that has no "sandboxing" (using the word lightly), and now it has elevated its privileges. This check will prevent it, because it will not have SpecialPermission.

Going forward I would like to improve the other engines to be more limited, just like we do with groovy, but today that is just not the case.
</comment><comment author="kimchy" created="2015-09-29T16:38:42Z" id="144114170">ahh, I see, thanks for the explanation !
</comment><comment author="rmuir" created="2015-09-29T16:40:19Z" id="144114554">the helper method idea is still a good one, we can look into it in the future. we might be able to do something "sneaky" (e.g. with accesscontrolcontext) and then like you say, we can use forbidden apis...

methods like that are tricky business though... so I went with the straightforward approach here as a start.
</comment><comment author="rmuir" created="2015-10-01T06:53:46Z" id="144637749">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ClusterSerivce and IndexSettingsService dependency from IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13853</link><project id="" key="" /><description>We have two unneeded heavy dependencies on IndexShard that are unneeded and only cause
trouble if you try to mock index shard. This commit removes IndexSettingsService as well as
ClusterSerivce from IndexShard to simplify future mocking and construction.
</description><key id="108898488">13853</key><summary>Remove ClusterSerivce and IndexSettingsService dependency from IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T15:47:46Z</created><updated>2015-10-02T14:34:07Z</updated><resolved>2015-09-30T12:20:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-29T15:55:37Z" id="144102024">LGTM .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When setting multi zone configuration, elasticsearch fails to start</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13852</link><project id="" key="" /><description>Settings in elasticsearch.yml  : 

cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone: zone1,zone2
node.zone: zone1

Once these lines were commented out in config, it started properly.

Error reported in logs : 

[2015-09-29 14:43:30,693][ERROR][org.elasticsearch.bootstrap] Exception
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, SettingsException[Failed to get setting group for [cluster.routing.a
llocation.awareness.force.] setting prefix and setting [cluster.routing.allocation.awareness.force.z
one] because of a missing '.']
  at org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider.&lt;init&gt;(Unknown
Source)
  while locating org.elasticsearch.cluster.routing.allocation.decider.AwarenessAllocationDecider
    for parameter 1 at org.elasticsearch.cluster.routing.OperationRouting.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.cluster.routing.OperationRouting
    for parameter 2 at org.elasticsearch.cluster.service.InternalClusterService.&lt;init&gt;(Unknown Sourc
e)
</description><key id="108896470">13852</key><summary>When setting multi zone configuration, elasticsearch fails to start</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ajaybhatnagar</reporter><labels /><created>2015-09-29T15:37:29Z</created><updated>2015-09-29T15:55:04Z</updated><resolved>2015-09-29T15:55:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ajaybhatnagar" created="2015-09-29T15:38:06Z" id="144096988">This is for beta 2 release.
</comment><comment author="jpountz" created="2015-09-29T15:53:43Z" id="144101484">You should replace `cluster.routing.allocation.awareness.force.zone`with `cluster.routing.allocation.awareness.force.zone.values`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make floating-point numbers dynamically mapped as floats instead of doubles by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13851</link><project id="" key="" /><description>For numbers around 1, precision is about 1e-7. My sentiment is that it would be enough for 99% of use-cases so maybe we should switch to floats to cut disk requirements by half when using the default configuration.
</description><key id="108896369">13851</key><summary>Make floating-point numbers dynamically mapped as floats instead of doubles by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>discuss</label></labels><created>2015-09-29T15:36:56Z</created><updated>2015-12-09T07:38:27Z</updated><resolved>2015-12-09T07:38:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-29T20:46:28Z" id="144186723">+1
</comment><comment author="rmuir" created="2015-09-29T20:50:52Z" id="144187743">+1
</comment><comment author="polyfractal" created="2015-09-30T16:40:58Z" id="144471068">+1!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test distributions against SLES 11 SP3/4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13850</link><project id="" key="" /><description>In order to be sure that packages (tar.gz, RPM does not work at the moment, same issues as with CentOS 5), we should add SLES 11 to the list of tested distributions with vagrant.

However there does not seem to be a working SLES11 distro available. The only existing and working one is SP1, which is really old (SP is Service Pack and kind of a release).

In order to run the tests we first need a SP3 vagrant image and then run the tests against it. @jjfalling offered to take a look here. After this we can run the tests on SLES, first the tar.gz distribution and later, once we decide to create a second RPM supporting older distros, we could run that one against the image.
</description><key id="108895061">13850</key><summary>Test distributions against SLES 11 SP3/4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label></labels><created>2015-09-29T15:30:23Z</created><updated>2016-04-22T12:55:58Z</updated><resolved>2016-04-22T12:55:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-04-22T12:55:56Z" id="213417417">Closing in favor of #17690.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot easily specify multiple number values for match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13849</link><project id="" key="" /><description>[Match query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html) allows only one value to be passed. If multiple values are required for the match then one can aggregate them into a sentence which is going to be analyzed.
However if the field the match queries against is a _number_, it is not possible to pass in multiple values:

``` json
"match" : { "participants" : "1" }
```

works however

``` json
"match" : { "participants" : "1 2" }
```

doesn't and will throw a `NumberFormatException`. Using an array (as `terms` query allows) is also not supported.

``` json
"match" : { "participants" : [1 2] }
```

The workaround is to break down the query into `bool` manually.
</description><key id="108889790">13849</key><summary>Cannot easily specify multiple number values for match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels><label>:Query DSL</label><label>adoptme</label></labels><created>2015-09-29T15:05:25Z</created><updated>2015-10-02T14:02:31Z</updated><resolved>2015-10-02T14:02:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2015-09-29T15:08:05Z" id="144089069">@dakrone was kind enough to try it out and replicate the issue.
</comment><comment author="jpountz" created="2015-09-29T15:57:34Z" id="144102535">Wouldn't it be easier to build a `terms` query instead of a `match` if you already know which terms you would like to query?
</comment><comment author="costin" created="2015-09-29T16:06:16Z" id="144105063">Should work but I would argue it's still a workaround. Just like `in` is a synonym for `terms`, ES could automatically translate the query from `match` to `terms`. The user doesn't always know the values passed are numbers or that the target field is numeric hence why applying the workaround would require additional calls and effort.
</comment><comment author="clintongormley" created="2015-10-02T14:02:31Z" id="145030258">@costin i don't think we should support this.  The semantics for the `terms` query is simple: include any document which contains at least one of the listed terms.  

The `match` query is a very different story.  We've got min should match, fuzziness, cutoff frequencies, tf/idf, phrases and positions, what happens if each entry in the array generates multiple terms? how do we combine the scores from each entry, and how do we combine the scores from the terms from each entry. should we use bool, dis_max, blended?  etc etc etc

We provide the tools for you to specify the logic you require explicitly yourself, eg:

```
{
  "query": {
    "bool": {
      "should": [
        {
          "match": {
            "participants": "1"
          }
        },
        {
          "match": {
            "participants": "2"
          }
        }
      ]
    }
  }
}
```

or (depending on your needs) perhaps:

```
{
  "query": {
    "constant_score": {
      "filter": {
        "bool": {
          "should": [
            {
              "match": {
                "participants": "1"
              }
            },
            {
              "match": {
                "participants": "2"
              }
            }
          ]
        }
      }
    }
  }
}
```

I don't think we should further complicate an already complicated query when there is a perfectly usable alternative.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Verify actually written checksum in VerifyingIndexOutput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13848</link><project id="" key="" /><description>today we don't verify that the actual checksum written to VerifyingIndexOutput
is the actual checksum we are expecting.
</description><key id="108887644">13848</key><summary>Verify actually written checksum in VerifyingIndexOutput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T14:55:05Z</created><updated>2015-10-04T19:25:35Z</updated><resolved>2015-09-30T13:36:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-29T16:04:18Z" id="144104303">I pushed a new commit @jasontedor @dakrone @rmuir @mikemccand 
</comment><comment author="s1monw" created="2015-09-30T08:13:31Z" id="144322499">@mikemccand @rmuir pushed a new commit
</comment><comment author="s1monw" created="2015-09-30T11:15:31Z" id="144363578">@mikemccand next iteration pushed
</comment><comment author="mikemccand" created="2015-09-30T13:31:06Z" id="144397534">LGTM, thanks @s1monw!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Workaround JDK-8056984</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13847</link><project id="" key="" /><description>This commit works around JDK bug [JDK-8056984](https://bugs.openjdk.java.net/browse/JDK-8056984) in the`javac` compiler.
This bug is impacting CI compilations on JDK 8u25.
</description><key id="108885852">13847</key><summary>Workaround JDK-8056984</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>jvm bug</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T14:46:53Z</created><updated>2016-03-10T18:53:09Z</updated><resolved>2015-09-29T14:49:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-29T14:48:57Z" id="144083611">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds geo_centroid metric aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13846</link><project id="" key="" /><description>This PR adds a new metric aggregator for computing the geo_centroid over a set of geo_point fields. This can be combined with other aggregators (e.g., geohash_grid, significant_terms) for computing the geospatial centroid based on the document sets from other aggregation results.

closes #13621 
</description><key id="108881305">13846</key><summary>Adds geo_centroid metric aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>feature</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T14:26:18Z</created><updated>2015-10-15T08:02:10Z</updated><resolved>2015-10-14T21:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-02T13:47:45Z" id="145026125">Hiya @nknize 

I note this PR is missing documentation?  Also, is the plan to remove the geohash_grid centroid added in https://github.com/elastic/elasticsearch/pull/13433 in favour of this agg? (I may have missed it in this PR). 
</comment><comment author="nknize" created="2015-10-02T14:28:43Z" id="145037898">@clintongormley Thanks for the documentation reminder. I'll finish that up. 

re: removal of #13433 I was thinking we make it optional (e.g., `weighted_centroid: true`) and default to `false`.  This way, we can avoid requiring a subaggregatoin if the user only wants the centroid of the `geo_grid`.  There's less overhead for this more common use case.
</comment><comment author="colings86" created="2015-10-05T10:37:22Z" id="145491751">@nknize I left some comments but still want to try this out on some data too.

In terms of this replacing the weighted centroid in the geohash-grid agg, I am a bit torn. On the one hand, I agree its is going to be a common use-case, but on the other hand I don't like having the same implementation in two different places as it adds a maintenance overhead.
</comment><comment author="nknize" created="2015-10-05T13:48:21Z" id="145532393">@colings86 Thanks for the feedback. I don't like having it in both places either. I opened #13912 to facilitate a discussion for whether we want to keep it "native" but make it optional in `geohash_grid`. I like this idea because it gives the best performance for the common-use case. If we decide to go forward with that approach I'll decouple the weighted average logic so its not duplicated.
</comment><comment author="nknize" created="2015-10-09T14:34:05Z" id="146889537">@colings86 removed centroid calculation from GeoHashGridAggregation. Centroid is a standalone metric aggregator. /cc @jpountz 
</comment><comment author="colings86" created="2015-10-14T07:42:10Z" id="147964107">LGTM
</comment><comment author="s1monw" created="2015-10-15T08:02:10Z" id="148311346">this PR has been merged into master and cherry-picked into 2.1 but not int 2.x @nknize can you please make sure you are cherry-picking it into 2.x as well. also don't forget cherry-picking the serialization fix here: 5b1ee8bd749ea553a4aad57962a798a4ff98da77
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mark id as required for delete_template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13845</link><project id="" key="" /><description /><key id="108876949">13845</key><summary>Mark id as required for delete_template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:REST</label><label>bug</label></labels><created>2015-09-29T14:05:41Z</created><updated>2015-10-06T18:29:01Z</updated><resolved>2015-10-06T18:25:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-29T16:01:50Z" id="144103695">LGTM
</comment><comment author="gmarz" created="2015-10-06T18:25:53Z" id="145955233">Closing, this is a duplicate of https://github.com/elastic/elasticsearch/pull/13711
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up scripting permissions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13844</link><project id="" key="" /><description>Now that groovy is factored out, we contain this dangerous stuff there.

TODO: look into those test hacks inspecting class protection domains, maybe we can
clean that one up too.

TODO: generalize the GroovyCodeSourcePermission to something all script engines check,
before entering accesscontrollerblocks. this way e.g. groovy script cannot coerce
python engine into creating something with more privs if it gets ahold of it... we
should probably protect the aws/gce hacks in the same way.
</description><key id="108875615">13844</key><summary>Clean up scripting permissions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T13:59:37Z</created><updated>2015-10-02T15:56:09Z</updated><resolved>2015-09-29T14:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-29T14:06:36Z" id="144071415">NICE!
</comment><comment author="rmuir" created="2015-10-01T06:52:51Z" id="144637642">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Current issues with (meta) data upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13843</link><project id="" key="" /><description>Reviewing https://github.com/elastic/elasticsearch/pull/13799, I found it hard to follow the upgrade paths in our code. Here are couple of suggestion to make it simpler. This issue is meant capture them and we can spawn other tickets for each of the things we decide to do.
- We rely both on Lucene versions and Elasticsearch versions in our code. I think this is confusing. We should just rely on the Elasticsearch version.
- We capture the current status in index settings but we don't have clear separation in our naming between data upgrades (i.e., Lucene segments) and metadata upgrades. We have the following settings:
  - `index.version.upgraded` -&gt; meta data was upgraded 
  - `index.version.minimum_compatible` -&gt; lucene segements were upgraded (and uses lucene versions)
    they should be renamed to  `index.version.metadata_upgrade` &amp; `index.version.data_upgrade` (or similar)
- For closed indices we ignore the minimum data requirement (see MetaDataIndexUpgradeService#checkSupportedVersion) as we aren't intending to go and read the lucene files. However, we _do_ upgrade the metadata files. This puts us in place where people can't downgrade back to a previous version, open the index and upgrade it. I _think_ that we we have closed indices which can't be opened we should refuse to start. The alternative is to leave the metadata alone - but then we can't guarantee that the Elasticsearch code of the new version can deal with it.
- The upgrade API sends upgrade commands to all the shards and when they respond successfully,we send a command to the master to mark the index as (data) upgraded. It would be nice to send the allocation IDs of the physical copies that were upgraded so the master can decide wether these are indeed contain all the _current_ primaries.

Other confusions &amp; minor things:
- we have MetaDataUpdateSettingsService#upgradeIndexSettings which marks the index after upgrading the shard. We have MetaDataIndexUpgradeService which does all the metadata upgrading. We should move upgradeIndexSettings() into  MetaDataIndexUpgradeService
- MetaDataUpdateSettingsService#upgradeIndexSettings (or it's equivalent) should stay away from the meta data upgrade settings (it's generally OK because the cluster state was already upgraded at that point)
- MetaDataIndexUpgradeService#checkSupportedVersion ignores closed indices. That should be made explicit (either by the name or a parameter).
</description><key id="108865384">13843</key><summary>Current issues with (meta) data upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Core</label><label>adoptme</label><label>discuss</label><label>enhancement</label></labels><created>2015-09-29T13:07:55Z</created><updated>2015-10-02T13:38:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Workaround JDK-8056014</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13842</link><project id="" key="" /><description>This commit works around JDK bug [JDK-8056014](https://bugs.openjdk.java.net/browse/JDK-8056014) in the `javac` compiler.
This bug is impacting CI compilations on JDK 8u11 and 8u25.
</description><key id="108864766">13842</key><summary>Workaround JDK-8056014</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>jvm bug</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T13:04:24Z</created><updated>2016-03-10T18:53:15Z</updated><resolved>2015-09-29T13:43:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-29T13:07:45Z" id="144053165">Lgtm. I don't imagine it's easy to detect this automatically somehow. 
</comment><comment author="jasontedor" created="2015-09-29T13:43:22Z" id="144063584">I've verified this compiler bug presented on 8u11, 8u20, 8u25, and 8u31 and this simple fix works around the issue on all these versions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change logging settings dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13841</link><project id="" key="" /><description>Hello,

it would be nice to have possibility to dynamically change logging settings without restarting elasticsearch (like watchdog on logger config).

Here is some hints about log4j:

http://www.theserverlabs.com/blog/2010/04/22/dynamically-changing-log-level-with-weblogic-log4j-jmx-and-wlst/
http://stackoverflow.com/questions/4598702/dynamically-changing-log4j-log-level
</description><key id="108858459">13841</key><summary>Change logging settings dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vad1m</reporter><labels /><created>2015-09-29T12:23:58Z</created><updated>2015-10-01T06:20:07Z</updated><resolved>2015-10-01T06:20:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-10-01T01:32:17Z" id="144590495">Changing the logging level dynamically at run-time is available through cluster-wide settings: https://www.elastic.co/guide/en/elasticsearch/guide/current/logging.html

If you want to change the root logger (e.g. to change all the loggers at once), the syntax is `_root`:

```
PUT /_cluster/settings
{
    "transient" : {
        "logger._root" : "DEBUG"
    }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start making RecoverySourceHandler unittestable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13840</link><project id="" key="" /><description>This commit shuffels and rewrites some code in RecoverySourceHandler to make it
simpler and more unittestable. This commit doesn't change all parts of this class
neither is it fully tested yet. It's an important part of the infrastrucutre so I started
to make it better tested but I don't want to change everything in one go since it makes
review simpler and more detailed. Future commits will continue cleaning up the class and
add more tests.
</description><key id="108853537">13840</key><summary>Start making RecoverySourceHandler unittestable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T11:51:05Z</created><updated>2015-10-02T14:57:23Z</updated><resolved>2015-09-30T16:19:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-29T12:10:58Z" id="144039721">Left some comments, being able to unit test this is great! Now we can simulate corrupting network streams in unit tests!
</comment><comment author="s1monw" created="2015-09-29T14:59:11Z" id="144086527">pushed a new commit @dakrone 
</comment><comment author="s1monw" created="2015-09-29T14:59:39Z" id="144086651">I have to wait for https://github.com/elastic/elasticsearch/pull/13848 since this is where I ran into this
</comment><comment author="s1monw" created="2015-09-30T14:39:15Z" id="144432237">@dakrone can you take another look?
</comment><comment author="dakrone" created="2015-09-30T15:29:44Z" id="144446810">Looks good, thanks! +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13839</link><project id="" key="" /><description>Hi,
I face an invalid JSON answer with elasticsearch 1.4.5 with agregations. Without aggregation, query is fine.

My query:

```
curl -XGET 'http://localhost:9200/toxsign/signature/_search' -d '{
"query" : {
"term" : { "contributors" : "osallou" }
},
"aggs": {
      "organism": {
          "terms": {
             "field": "title"
          }
      }
 }
}
'



{"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"toxsign","_type":"signature","_id":"560a38bf2e71a81fb95bccfb_1414e60a-a2bb-40ce-959e-7e67b4eaff92_48dae30e-3524-4a3e-8185-2486ebf35b95","_score":1.0,"_source":{"submission_date": 1443503263, "status": "private", "confidence": "", "last_updated": 1443503263, "description": "data1", "contributors": "osallou", "title": "test2", "treatments": [{"title": "treat2", "signatures": [{"genomic": [{"tissue": "genomic1", "id": "08cde85f827848e0a3f7a997a4c5db1c"}], "physio": [{"tissue": "tissue1", "id": "4c62413a7232455c948a7ba723795527"}], "id": "48dae30e-3524-4a3e-8185-2486ebf35b95"}], "organism": "human", "description": "sample", "id": "1414e60a-a2bb-40ce-959e-7e67b4eaff92"}], "dataset": "560a38bf2e71a81fb95bccfb", "owner": "xxx"}{ "index" : { "_index" : "toxsign", "_type": "signature" , "_id" : "560a38bf2e71a81fb95bccfb_1414e60a-a2bb-40ce-959e-7e67b4eaff92_d8560884-9fa3-431f-9739-04228dc728cb" } }}]},"aggregations":{"organism":{"doc_count_error_upper_bound":0,"sum_other_doc_count":0,"buckets":[{"key":"test2","doc_count":1}]}}}
```

The problem is here:

```
"owner": "xxx"}{ "index" : {
```

The following is added directly in the hit answer, with an invalid json structure

```
{ "index" : { "_index" : "toxsign", "_type": "signature" , "_id" : "560a38bf2e71a81fb95bccfb_1414e60a-a2bb-40ce-959e-7e67b4eaff92_d8560884-9fa3-431f-9739-04228dc728cb" } }
```
</description><key id="108850980">13839</key><summary>Invalid json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">osallou</reporter><labels /><created>2015-09-29T11:33:28Z</created><updated>2015-09-29T15:55:07Z</updated><resolved>2015-09-29T15:55:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-29T15:55:07Z" id="144101876">Hi @osallou 

You have indexed a document with invalid JSON, and that is exactly what you are getting back. 

This should be caught by https://github.com/elastic/elasticsearch/pull/11414 in 2.0, but apparently this is still broken: https://github.com/elastic/elasticsearch/issues/13855
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch creates same log entry multiple times</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13838</link><project id="" key="" /><description>Hello,

I have a problem with Elasticsearch creating same entry in DB 2 or more times.

Here is the search of the entry before it appears in DB:

```
root@elk:/home/tigran# curl -XGET localhost:9200/appdev-2015.09.29/jibeqa/_search -d '{"query":{"bool":{"must":[{"term":{"jibeqa.date":"2015-09-29T07:10:50+0000"}}],"must_not":[],"should":[]}},"from":0,"size":10,"sort":[],"facets":{}}' | json_pp
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   270  100   122  100   148  19473  23623 --:--:-- --:--:-- --:--:-- 24666
{
   "timed_out" : false,
   "hits" : {
      "total" : 0,
      "hits" : [],
      "max_score" : null
   },
   "took" : 1,
   "_shards" : {
      "successful" : 5,
      "total" : 5,
      "failed" : 0
   }
}
root@elk:/home/tigran#
```

As you can see it is empty for now.

Now I send json to the log file which will be forwarded with logstash-forwarder

```
{"appId":"imas-dev-10.240.21.82","date":"2015-09-29T07:10:50+0000","category":"IMAS_CHATSESSION_MESSAGE_RECEIVED_121","user":"+11000040488","toUser":"+11000040466","messageType":"imdn","contributionId":"FACfIAV6DA","size":265,"extras":{}}
```

logstash rubydebug shows that it was parsed and sent to Elasticsearch just once:

```
{
             "appId" =&gt; "imas-dev-10.240.21.82",
              "date" =&gt; "2015-09-29T07:10:50+0000",
          "category" =&gt; "IMAS_CHATSESSION_MESSAGE_RECEIVED_121",
              "user" =&gt; "+11000040488",
            "toUser" =&gt; "+11000040466",
       "messageType" =&gt; "imdn",
    "contributionId" =&gt; "FACfIAV6DA",
              "size" =&gt; 265,
            "extras" =&gt; {},
          "@version" =&gt; "1",
        "@timestamp" =&gt; "2015-09-29T07:10:50.000Z",
              "file" =&gt; "/opt/jibe/jetty_master/reporting_MASTER_local/logs/reporting.log",
              "host" =&gt; "app3-us",
            "offset" =&gt; "2390",
              "tags" =&gt; "appdev",
              "type" =&gt; "jibeqa"
}
```

But new search in Elasticsearch shows that it was created 2 times:

```
root@elk:/home/tigran# curl -XGET localhost:9200/appdev-2015.09.29/jibeqa/_search -d '{"query":{"bool":{"must":[{"term":{"jibeqa.date":"2015-09-29T07:10:50+0000"}}],"must_not":[],"should":[]}},"from":0,"size":10,"sort":[],"facets":{}}' | json_pp
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  1350  100  1202  100   148   3304    406 --:--:-- --:--:-- --:--:--  3311
{
   "_shards" : {
      "total" : 5,
      "successful" : 5,
      "failed" : 0
   },
   "timed_out" : false,
   "took" : 361,
   "hits" : {
      "total" : 2,
      "max_score" : 1,
      "hits" : [
         {
            "_index" : "appdev-2015.09.29",
            "_score" : 1,
            "_type" : "jibeqa",
            "_id" : "AVAYqahQZ2RePRz37-kE",
            "_source" : {
               "appId" : "imas-dev-10.240.21.82",
               "messageType" : "imdn",
               "toUser" : "+11000040466",
               "offset" : "2390",
               "contributionId" : "FACfIAV6DA",
               "@timestamp" : "2015-09-29T07:10:50.000Z",
               "user" : "+11000040488",
               "type" : "jibeqa",
               "@version" : "1",
               "date" : "2015-09-29T07:10:50+0000",
               "file" : "/opt/jibe/jetty_master/reporting_MASTER_local/logs/reporting.log",
               "tags" : "appdev",
               "host" : "app3-us",
               "category" : "IMAS_CHATSESSION_MESSAGE_RECEIVED_121",
               "size" : 265,
               "extras" : {}
            }
         },
         {
            "_score" : 1,
            "_type" : "jibeqa",
            "_source" : {
               "file" : "/opt/jibe/jetty_master/reporting_MASTER_local/logs/reporting.log",
               "date" : "2015-09-29T07:10:50+0000",
               "@version" : "1",
               "type" : "jibeqa",
               "extras" : {},
               "size" : 265,
               "category" : "IMAS_CHATSESSION_MESSAGE_RECEIVED_121",
               "tags" : "appdev",
               "host" : "app3-us",
               "messageType" : "imdn",
               "appId" : "imas-dev-10.240.21.82",
               "contributionId" : "FACfIAV6DA",
               "@timestamp" : "2015-09-29T07:10:50.000Z",
               "user" : "+11000040488",
               "offset" : "2390",
               "toUser" : "+11000040466"
            },
            "_id" : "AVAYqahQZ2RePRz37-kD",
            "_index" : "appdev-2015.09.29"
         }
      ]
   }
}
```

The ID is different but the rest is same.

My input is:

```
  lumberjack {
    port =&gt; 5120
    #type =&gt; "logs"
    #tags =&gt; "appdev"
    ssl_certificate =&gt; "/etc/pki/tls/certs/logstash-forwarder_appdev.crt"
    ssl_key =&gt; "/etc/pki/tls/private/logstash-forwarder_appdev.key"
    codec =&gt; "json"
  }
```

Filter is:

```
filter {
  if [type] == "jibeqa" {
    json{
        source =&gt; "message"
    }
    mutate {
        remove_field =&gt; "message"
    }
    date {
      match =&gt; [ "date", "ISO8601"]
    }
  }
}
```

And the output is:

```
output {
  if "appdev" in [tags] {
    elasticsearch { host =&gt; localhost protocol =&gt; "http" cluster =&gt; Jibe_ELK index =&gt; "appdev-%{+YYYY.MM.dd}" workers =&gt; 5 }
  }
#  stdout { codec =&gt; rubydebug }
}
```

Versions:

```
ii  elasticsearch                       1.5.2                               all          Open Source, Distributed, RESTful Search Engine
ii  logstash                            1.4.5-1-a2bacae                     all          An extensible logging pipeline
ii  logstash-contrib                    1.4.5-1-2bad350                     all          Community supported plugins for Logstash
```
</description><key id="108842118">13838</key><summary>Elasticsearch creates same log entry multiple times</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tigranterteryan</reporter><labels /><created>2015-09-29T10:36:33Z</created><updated>2015-09-29T13:57:55Z</updated><resolved>2015-09-29T12:57:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-29T12:55:17Z" id="144049996">I think that elasticsearch got this index operation 2 times from logstash.
So to me, that's more a logstash issue than an elasticsearch one.
</comment><comment author="dadoonet" created="2015-09-29T12:57:16Z" id="144050389">If you want to make sure that the same event is not indexed twice, then you need to tell logstash to use a specific `_id` which will be extracted from your logstash event.

Closing as I don't think it's an elasticsearch issue here.

Feel free to reopen and comment.
</comment><comment author="tigranterteryan" created="2015-09-29T13:37:06Z" id="144060517">I updated logstash to override the _id:

```
  if "appdev" in [tags] {
    elasticsearch { host =&gt; localhost protocol =&gt; "http" cluster =&gt; Jibe_ELK index =&gt; "appdev-%{+YYYY.MM.dd}" document_id =&gt; "%{contributionId}-%{offset}-%{date}" workers =&gt; 5 }
  }
```

And logstash.log shows that logstash sends to elasticsearch only one document:

```
{
             "appId" =&gt; "imas-dev-10.240.21.82",
              "date" =&gt; "2015-09-29T07:10:52+0000",
          "category" =&gt; "IMAS_CHATSESSION_MESSAGE_RECEIVED_121",
              "user" =&gt; "+11000040488",
            "toUser" =&gt; "+11000040466",
       "messageType" =&gt; "imdn",
    "contributionId" =&gt; "FACfIAV6DA",
              "size" =&gt; 265,
            "extras" =&gt; {},
          "@version" =&gt; "1",
        "@timestamp" =&gt; "2015-09-29T07:10:52.000Z",
              "file" =&gt; "/opt/jibe/jetty_master/reporting_MASTER_local/logs/reporting.log",
              "host" =&gt; "app3-us",
            "offset" =&gt; "2868",
              "type" =&gt; "jibeqa",
              "tags" =&gt; "appdev"
}
```

But in Elasticsearch it appears with _id I assigned and with default one:

```
{
   "hits" : {
      "max_score" : 1,
      "total" : 2,
      "hits" : [
         {
            "_source" : {
               "offset" : "2868",
               "date" : "2015-09-29T07:10:52+0000",
               "messageType" : "imdn",
               "user" : "+11000040488",
               "contributionId" : "FACfIAV6DA",
               "file" : "/opt/jibe/jetty_master/reporting_MASTER_local/logs/reporting.log",
               "host" : "app3-us",
               "extras" : {},
               "@version" : "1",
               "toUser" : "+11000040466",
               "category" : "IMAS_CHATSESSION_MESSAGE_RECEIVED_121",
               "appId" : "imas-dev-10.240.21.82",
               "type" : "jibeqa",
               "size" : 265,
               "@timestamp" : "2015-09-29T07:10:52.000Z",
               "tags" : "appdev"
            },
            "_index" : "appdev-2015.09.29",
            "_type" : "jibeqa",
            "_id" : "AVAZT-DeZ2RePRz3MKsB",
            "_score" : 1
         },
         {
            "_source" : {
               "appId" : "imas-dev-10.240.21.82",
               "category" : "IMAS_CHATSESSION_MESSAGE_RECEIVED_121",
               "toUser" : "+11000040466",
               "@version" : "1",
               "tags" : "appdev",
               "size" : 265,
               "@timestamp" : "2015-09-29T07:10:52.000Z",
               "type" : "jibeqa",
               "user" : "+11000040488",
               "messageType" : "imdn",
               "date" : "2015-09-29T07:10:52+0000",
               "offset" : "2868",
               "extras" : {},
               "host" : "app3-us",
               "file" : "/opt/jibe/jetty_master/reporting_MASTER_local/logs/reporting.log",
               "contributionId" : "FACfIAV6DA"
            },
            "_index" : "appdev-2015.09.29",
            "_id" : "FACfIAV6DA-2868-2015-09-29T07:10:52+0000",
            "_type" : "jibeqa",
            "_score" : 1
         }
      ]
   },
   "_shards" : {
      "total" : 5,
      "failed" : 0,
      "successful" : 5
   },
   "timed_out" : false,
   "took" : 3
}
```
</comment><comment author="dadoonet" created="2015-09-29T13:40:45Z" id="144062370">Did you clean the index first? Any chance those are older data?
</comment><comment author="tigranterteryan" created="2015-09-29T13:42:36Z" id="144063307">It's not old data, I checked the search before sending. I can remove everything completely and try again.
But did that many times.
Data gets there manually when I send it.
</comment><comment author="dadoonet" created="2015-09-29T13:43:48Z" id="144063834">Ok. I see. If you can reproduce it and describe your full scenario, then open an issue in logstash project.
</comment><comment author="tigranterteryan" created="2015-09-29T13:44:33Z" id="144064099">OK Thank you!
</comment><comment author="tigranterteryan" created="2015-09-29T13:57:55Z" id="144068917">For a refference: https://github.com/elastic/logstash/issues/3975
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rip the grooviness out of 'messy tests' in groovy plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13837</link><project id="" key="" /><description>These these tests belong in core, and were just moved temporarily.

We should clean them up one by one...
</description><key id="108835030">13837</key><summary>Rip the grooviness out of 'messy tests' in groovy plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>test</label></labels><created>2015-09-29T09:54:54Z</created><updated>2016-08-01T15:02:31Z</updated><resolved>2016-08-01T15:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-29T09:55:42Z" id="144010328">See https://github.com/elastic/elasticsearch/pull/13834 for more information. The referred package-info is https://github.com/rmuir/elasticsearch/blob/groovy_factor_out/plugins/lang-groovy/src/test/java/org/elasticsearch/messy/tests/package-info.java
</comment><comment author="tlrx" created="2016-08-01T15:02:31Z" id="236607133">Many messy tests have been cleaned recently in #19280, #19302, #19336 and #19621. Most of the tests have been moved back in their original places and converted back again into integration tests while the dependency on Groovy has been replaced by mocked scripts. It's not perfect since some tests would have benefit to be changed into unit or simplier tests but I think that's still a good move.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow for base16 encoded binary fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13836</link><project id="" key="" /><description>I've [searched for issues mentioning](https://github.com/elastic/elasticsearch/search?q=base16&amp;type=Issues&amp;utf8=%E2%9C%93) "base16" and didn't find any, so I'm daring to create a new issue.

I want to post binary data to ES, but that data is not base64 encoded. It is rather base16 encoded.

I tried to define a mapping with `{"type": "binary", "encoding":"base16"}` but that didn't work.

I'd appreciate if it was possible to define the encoding of the binary data posted.
</description><key id="108823749">13836</key><summary>Allow for base16 encoded binary fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">muelli</reporter><labels /><created>2015-09-29T08:53:30Z</created><updated>2015-10-02T14:57:04Z</updated><resolved>2015-10-02T14:57:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-29T15:35:07Z" id="144096121">Hi @muelli 

Yeah, this is the first request we've had for base 16.  I very much doubt we're going to support it, not least of all because of the 4x cost of storage.  

I'd build the conversion from base16-&gt;base64 into your indexing process.
</comment><comment author="muelli" created="2015-09-29T15:49:32Z" id="144100303">Hah. I thought POSTing my data to ES was my indexing process ;-)

My na&#239;ve suggestion is to only treat the "encoding" property of the mapping's field as a hint to decode the data to then store in whatever way ES pleases. In a way to softcode the implicit base64 decoding of a binary field.

I don't have a need for the exact base16 data. It's just that I have a lot of JSON documents with data already base16 encoded. And I was hoping to have a more friendly way of getting my data into ES than  deserialising everything to JSON, decoding base16, encoding base64, and serialising everything back to JSON.

I know it's doable and I'm in the process of doing that. But maybe it makes a future me a tiny bit happier if I could load not only base64, but also base16, or pretty much anything into a binary field as long as there is an appropriate decoding function (think deflate, rot13, base2, ...).
</comment><comment author="jpountz" created="2015-09-30T12:38:04Z" id="144384080">I'd rather settle on base64 and make sure this is what se use all the time to encode binary data.
</comment><comment author="jprante" created="2015-09-30T16:18:19Z" id="144465179">@muelli you have the misconception that field type binary for Lucene can also drive the JSON string processing which happens before.

I have created a plugin that can process incoming base16 encoded JSON strings for buk indexing. The idea is to decode the base16 transparently in the XContent API and map them to a byte array that is passed on to the indexing where base64 is used internally.

See this plugin for an example 

https://github.com/jprante/elasticsearch-hex

Because the XContent API is not easy to extend when it comes to parse field strings, I implemented my own version by copy/paste, with a custom bulk action.

I think parsing incoming JSON strings for binary data encoding could take away some burden from users, not only for hexadecimal encodings, but also for processing "ASCII armored" data, like Ascii85/Base85, quoted-printable MIME strings, or percent encodings:

https://en.wikipedia.org/wiki/Binary-to-text_encoding
</comment><comment author="clintongormley" created="2015-10-02T14:57:04Z" id="145050922">More and more we're trying to move transformations like this out of elasticsearch core to make the core functionality simple to maintain and bullet proof. The right place for things like this is in an ETL pipeline before core.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add response into ClearScrollResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13835</link><project id="" key="" /><description>When deleting an individual scroll ID, ES does produce a 200 in the
header if successful and a 404 if the scroll ID wasn't found, but
returns empty response body. 

It would be more user friendly to provide
some information on whether the scroll deletion is successful.

closes #13817
</description><key id="108820945">13835</key><summary>Add response into ClearScrollResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Search</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T08:40:46Z</created><updated>2015-10-09T03:49:07Z</updated><resolved>2015-10-09T03:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-10-07T18:29:54Z" id="146286169">Just saw the review, thanks @nik9000. Change the boolean to number freed.
</comment><comment author="nik9000" created="2015-10-07T19:09:21Z" id="146297846">Left minor comment. Otherwise LGTM.
</comment><comment author="nik9000" created="2015-10-07T19:14:22Z" id="146299861">LGTM
</comment><comment author="xuzha" created="2015-10-07T19:17:42Z" id="146300755">@nik9000 Thanks very much for the view.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Factor groovy out of core into lang-groovy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13834</link><project id="" key="" /><description>See https://github.com/elastic/elasticsearch/issues/13725 for the motivation.

Note that many core tests (approximately 50) depend on groovy. Rather than try to refactor all of these at once, I simply moved them all to this plugin under the `org.elasticsearch.messy.tests` package, so we can deal with them iteratively. This preserves them in their entirety, so we don't lose coverage, or go back and forth on how each one should be fixed: instead this way we can clean up over time.

Last time when factoring out expressions, @rjernst stayed up all night to fix a bunch of tests, but we can't do things this way, that is why I did what I did. 

The package-info.java has guidelines about what I think we should do, to get them back in core. It also has the list of renames so you know where each file came from (in case its easy to fix and move back to core). 

I made no security changes here to not bury in this noise.
</description><key id="108772035">13834</key><summary>Factor groovy out of core into lang-groovy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>PITA</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-29T00:23:30Z</created><updated>2015-10-01T06:52:17Z</updated><resolved>2015-09-29T09:58:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-09-29T00:56:09Z" id="143913707">LGTM.  Glad to see this moved to a plugin.
</comment><comment author="s1monw" created="2015-09-29T09:51:29Z" id="144009622">+1 on moving the tests into the plugin - We can gracefully move them back or to unittests while we go.. I think we should just open a issue that we have to do that so we don't forget! Thanks for working on this! 
</comment><comment author="rmuir" created="2015-10-01T06:52:17Z" id="144637382">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_score value doesn&#8217;t match the return value of a script_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13833</link><project id="" key="" /><description>When the following `script_score` is executed the explain shows a different _score value than the one returned by the script

```
{
  "query":{
    "function_score":{
      "functions":[
        { "script_score":{ "script":"1443150000000" } }
      ]
    }
  }
}
```

This is what the explain shows:

```
value: 1443150040000
description: "script score function, computed with script:"1443150000000"
```
</description><key id="108740461">13833</key><summary>_score value doesn&#8217;t match the return value of a script_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cldsnchz</reporter><labels /><created>2015-09-28T20:31:41Z</created><updated>2015-09-29T19:43:54Z</updated><resolved>2015-09-29T14:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-29T14:08:20Z" id="144071875">`_score` is a floating point.  Your number is too high to be represented accurately by a floating point
</comment><comment author="cldsnchz" created="2015-09-29T19:43:54Z" id="144167685">Is there any plan to change `_score` from float to double? That will allow to use timestamps as scores (I'm working on a use case that needs that feature).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to take snapshot of a index type instead of all types in the indexes?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13832</link><project id="" key="" /><description>We have 2 different types in the index but we don't need to take backup of whole index including all types but to one specific type only. How can we do this?
</description><key id="108726596">13832</key><summary>How to take snapshot of a index type instead of all types in the indexes?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lokeshhctm</reporter><labels /><created>2015-09-28T19:10:30Z</created><updated>2015-09-29T08:06:05Z</updated><resolved>2015-09-29T08:06:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-29T08:06:05Z" id="143979233">Snapshots can only work at the index level, not type. If you want finer granularity you need to index your documents in different indices instead of types. In the future please use [the forums](http://discuss.elastic.co) to ask questions, we try to only use github issues for confirmed bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make REST test logging more verbose</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13831</link><project id="" key="" /><description>This logs all requests and responses sent during the REST tests with the
intent of making it simpler to debug REST test failures. This is especially
important if the REST test makes the same assertion twice because its lets
you deduce which instance of the assertion failed.
</description><key id="108696243">13831</key><summary>Make REST test logging more verbose</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>review</label><label>test</label></labels><created>2015-09-28T16:32:29Z</created><updated>2015-11-29T01:21:00Z</updated><resolved>2015-11-29T01:21:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-28T16:49:24Z" id="143801532">Does it really need to be enabled by default? Why can't the user just pass -Des.logger.level ?
</comment><comment author="rmuir" created="2015-09-28T17:07:57Z" id="143806285">Personally I would solve the problem in a different way, by improving exceptions. This is always the preferred way, logging just adds noise.

Yes its bad today, look at this:

```
FAILURE 0.08s | AnalysisKuromojiRestIT.test {yaml=analysis_kuromoji/10_basic/Analyzer} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: field [tokens.2.token] doesn't match the expected value
   &gt; Expected: "&#39365;bogus"
   &gt;      but: was "&#39365;"
   &gt;    at __randomizedtesting.SeedInfo.seed([23704C1A2BA2025F:AB2473C0855E6FA7]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.rest.section.MatchAssertion.doAssert(MatchAssertion.java:71)
   &gt;    at org.elasticsearch.test.rest.section.Assertion.execute(Assertion.java:69)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:373)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Instead I think when we throw an exception from rest tests, we should add a StackTraceElement for the yaml file itself: if we populate line number, method, etc etc then the exceptions will be very intuitive and you know exactly where to go in the yaml file.
</comment><comment author="nik9000" created="2015-09-28T20:52:41Z" id="143870497">I looked at line numbers. The parser made that difficult. I'd prefer doing
line numbers but didn't see an simple way.
On Sep 28, 2015 7:08 PM, "Robert Muir" notifications@github.com wrote:

&gt; Personally I would solve the problem in a different way, by improving
&gt; exceptions. This is always the preferred way, logging just adds noise.
&gt; 
&gt; Yes its bad today, look at this:
&gt; 
&gt; FAILURE 0.08s | AnalysisKuromojiRestIT.test {yaml=analysis_kuromoji/10_basic/Analyzer} &lt;&lt;&lt;
&gt; 
&gt; &gt; Throwable #1: java.lang.AssertionError: field [tokens.2.token] doesn't match the expected value
&gt; &gt; Expected: "&#39365;bogus"
&gt; &gt;      but: was "&#39365;"
&gt; &gt;    at __randomizedtesting.SeedInfo.seed([23704C1A2BA2025F:AB2473C0855E6FA7]:0)
&gt; &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
&gt; &gt;    at org.elasticsearch.test.rest.section.MatchAssertion.doAssert(MatchAssertion.java:71)
&gt; &gt;    at org.elasticsearch.test.rest.section.Assertion.execute(Assertion.java:69)
&gt; &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:373)
&gt; &gt;    at java.lang.Thread.run(Thread.java:745)
&gt; 
&gt; Instead I think when we throw an exception from rest tests, we should add
&gt; a StackTraceElement for the yaml file itself: if we populate line number,
&gt; method, etc etc then the exceptions will be very intuitive and you know
&gt; exactly where to go in the yaml file.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13831#issuecomment-143806285
&gt; .
</comment><comment author="jpountz" created="2015-10-09T14:48:52Z" id="146893310">I agree having line numbers from the yaml file would be very helpful but in the meantime I think this change is already helpful. Should we merge it and open another issue about getting line numbers back in the exception?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Determine if the index has been modified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13830</link><project id="" key="" /><description>Today, it is difficult to determine if any index has been updated (e.g., in _mostly_ read only scenarios where the answer becomes more interesting). If you have a simple, outside cache, then it would be ideal to know when an index has been modified without jumping through hoops to figure it out.

Conceptually, this could be as simple as a _shard_ metadata field that indicates when the last document was _modified_ (created, updated, or deleted). I do not think that it is worth modifying the cluster state by adding it at the index level (less of a hassle with cluster state diffs, but still quite noisy).

This could live as a secondary API next to `_field_stats` (`_index_stats` or just an addition to the `&lt;index&gt;/_stats`) as a way to show how many _actual_ modified documents exist, how many deleted documents exist (preferably as an absolute number, rather than somewhat relative like `_version`, but actual deletions will most likely be harder to track), and document count. Some of this is available in other ways (e.g., deleted docs), but there's no simple way to know that "indexA has these stats relative to its documents" and there's no way to know "those stats last changed at time `X`".
</description><key id="108689056">13830</key><summary>Determine if the index has been modified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Index APIs</label><label>discuss</label></labels><created>2015-09-28T15:56:21Z</created><updated>2015-10-02T14:29:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-29T12:44:19Z" id="144047297">It's got to be persisted somewhere.  Data needs to be part of the cluster/index state in order to be persisted.  So now we need to write this to disk every time  document is indexed.

Elasticsearch already has a bunch of caching built in.  Most of the time you can just run your queries/aggs and rely on Elasticsearch to do the caching for you.  In the small number of cases where you want external caching, you could use a simple max agg on a timestamp field to check whether the external cache should be purged.  Enable result caching on the max agg and even that check becomes very light.

I think existing functionality is a better answer than adding an index timestamp.
</comment><comment author="clintongormley" created="2015-10-02T14:29:30Z" id="145038308">It just occurred to me that this could be implemented in a much simpler lighter way: most recent refresh time.  Each index (or shard) could just record the time of the most recent refresh of a new segment.  This value need not be persistent, just held in memory.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve seccomp syscall filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13829</link><project id="" key="" /><description>- Add OS X support via "seatbelt" mechanism. This gives consistency across dev and prod, since many devs use OS X.
- block execveat system call: it may be new, but we should not allow it.
</description><key id="108685352">13829</key><summary>improve seccomp syscall filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-28T15:38:10Z</created><updated>2015-10-02T13:35:07Z</updated><resolved>2015-09-29T12:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jdconrad" created="2015-09-29T01:00:16Z" id="143914289">LGTM.  I think moving away from the blacklist to just the three commands is a lot simpler.
</comment><comment author="rjernst" created="2015-09-29T12:52:30Z" id="144049031">LGTM too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot restore operations throttle more than specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13828</link><project id="" key="" /><description>Lucene's RateLimiter can do too much sleeping on small values (see also #6018).
The issue here is that calls to "pause" are not properly guarded in "restoreFile".

Instead of simply adding the guard, this commit uses the RateLimitingInputStream similar as for "snapshotFile".
</description><key id="108665190">13828</key><summary>Snapshot restore operations throttle more than specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">ywelsch</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v1.7.3</label><label>v2.0.0</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-09-28T14:08:44Z</created><updated>2016-03-10T18:15:35Z</updated><resolved>2015-10-03T14:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ywelsch" created="2015-09-29T11:51:08Z" id="144036011">@mikemccand can you have a look at this PR?
</comment><comment author="mikemccand" created="2015-09-30T17:30:58Z" id="144484212">LGTM, this is a very bad bug!  That small `4096` buffer means we are way over-throttling recovery :(  We should push this back to 1.7.x?
</comment><comment author="mikemccand" created="2015-09-30T17:32:29Z" id="144484581">Are we directly calling `RateLimiter.pause` anywhere else in ES?
</comment><comment author="ywelsch" created="2015-09-30T17:34:40Z" id="144485123">I checked the other places, and they are all properly guarded. Maybe RateLimiter.pause could have an assertion?
</comment><comment author="mikemccand" created="2015-09-30T17:41:26Z" id="144487210">&gt; Maybe RateLimiter.pause could have an assertion?

Well, it's called (correctly) by `RateLimitingInputStream` ... maybe we could ban ES from directly calling `RateLimiter.pause` using our forbidden APIs?
</comment><comment author="ywelsch" created="2015-09-30T17:58:38Z" id="144491301">I'm very much in favour of back porting this to 1.7.x, as I saw a 10x slowdown on a production cluster.

I added your suggestions. Not closing the RateLimitingInputStream is ok, as it just delegates the close to PartSliceStream (which is now auto-closed).

Just banning RateLimiter.pause does not work as it is used in other places of ES (e.g. RecoverySourceHandler).
</comment><comment author="mikemccand" created="2015-09-30T20:23:23Z" id="144530353">&gt; Just banning RateLimiter.pause does not work as it is used in other places of ES (e.g. RecoverySourceHandler).

OK thanks, and it looks like it does the right thing (checks `getMinPauseCheckBytes`).
</comment><comment author="mikemccand" created="2015-09-30T20:23:52Z" id="144530566">LGTM, thanks @ywelsch!
</comment><comment author="ywelsch" created="2015-10-01T21:55:40Z" id="144860959">@clintongormley which ES versions should we push this back to?
</comment><comment author="clintongormley" created="2015-10-02T16:29:44Z" id="145079531">@ywelsch i'm good with 1.7.x and above
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add doc match score support to percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13827</link><project id="" key="" /><description>The [percolate API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-percolate.html#_percolate_api) documentation mentions `track_scores` and `sort` options in the _Additional request body options_ section. More precisely, we can read the following notes, respectively:

&gt; The _score is based on the query and represents how the query matched the **percolate query&#8217;s metadata, not** how the document (that is being percolated) matched the query.
&gt; [...]
&gt; Like `track_score` the score is based on the query and represents how the query matched to the percolate query&#8217;s metadata and **not** how the document being percolated matched to the query.

I'm wondering why there is nothing in the API about the score of the document itself (being percolated) among the various percolator queries?

I read somewhere that this would be due to hit score being relative to its resultset, and that in a percolator query, we only consider one document at a time, so the score on each percolator query is not really relevant.

I was expecting #3506 to be the solution, but it's not related after having a look. Such a feature is also mentioned [here](http://elasticsearch-users.115913.n3.nabble.com/Percolate-Query-Scoring-and-Fields-td4050649.html) in the mailing list but there is no technical answer.

Can you confirm? Is there any way/plan to get a percolation score?
</description><key id="108655758">13827</key><summary>Add doc match score support to percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ncolomer</reporter><labels><label>:Percolator</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2015-09-28T13:15:54Z</created><updated>2016-03-31T14:06:17Z</updated><resolved>2016-03-31T08:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-28T16:05:51Z" id="143790409">@ncolomer Scoring how percolator queries match with the document being percolated requires different scoring then exists today in search which is based tf/idf. But this feature is interesting and I think this should be made possible in the percolate api. I think the score should be based on proximity, how close the matches are in the document being percolated. 
</comment><comment author="ncolomer" created="2015-09-29T07:28:41Z" id="143972554">@martijnvg thanks for your answer!

You mean that, the way percolation is implemented today, it does not benefit from internal scoring tools (that the regular search API does for instance)?

Actually, this score would be useful to compare 2 consecutive docs being percolated: if they both match the same percolation query, it might be interesting to give them a score (ie. how well they match the query) to compare them.
</comment><comment author="martijnvg" created="2015-09-29T12:10:16Z" id="144039621">&gt; You mean that, the way percolation is implemented today, it does not benefit from internal scoring tools (that the regular search API does for instance)?

Right now there is just no scoring on how a query matches with the document being percolated. So I think this should be added at some point. The score that is being returned by the percolate api is just how the percolator query in the percolate api matches with the metadata on the percolator query.
</comment><comment author="martijnvg" created="2016-02-03T19:06:15Z" id="179409672">Now that the core percolator has been refactored to be a Lucene query adding scoring to the percolator has been made much easier. And with the upcoming refactoring in #16349 (replacing the percolate api with percolator query in query dsl) exposing the scores from how queries match with the MemoryIndex becomes a low hanging fruit. (right now the percolator just ignores that score)
</comment><comment author="ncolomer" created="2016-03-31T14:06:16Z" id="203955036">Thanks a lot @martijnvg! :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integration tests for running ES with plugins could be collapsed into running with a single ES startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13826</link><project id="" key="" /><description>Currently for each module in the ES repo we install the plugin, start up ES, wait for it to start, and then execute the tests. Instead of doing this N times for each module, we could probably execute the tests against the running ES that has all of the plugins installed, reducing the amount of times we have to start ES externally and wait for it to be available.
</description><key id="108643530">13826</key><summary>Integration tests for running ES with plugins could be collapsed into running with a single ES startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label></labels><created>2015-09-28T11:58:32Z</created><updated>2015-09-30T08:05:28Z</updated><resolved>2015-09-30T08:05:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-28T11:59:31Z" id="143723625">+1

Its not the "maven way" but it could save a ton of time.
</comment><comment author="rmuir" created="2015-09-28T12:46:16Z" id="143732718">-1

This would be nearly impossible to debug.
</comment><comment author="dakrone" created="2015-09-28T12:50:26Z" id="143735170">@rmuir shouldn't we test that our installing multiple of our plugins don't interfere with each other?
</comment><comment author="rmuir" created="2015-09-28T12:52:29Z" id="143736007">Integration tests need to be as simple as possible so that they can be debugged. Its like the golden rule, otherwise they will cause time to be wasted.

Its obvious that this has not been understood, if you look at the ones in `elasticsearch/core`, but that does not mean we have to keep doing things this way, and spread that bad behavior into the plugins world.
</comment><comment author="nik9000" created="2015-09-28T14:23:35Z" id="143758028">&gt; Integration tests need to be as simple as possible so that they can be debugged. Its like the golden rule, otherwise they will cause time to be wasted.

I don't 100% agree with this but my quibble isn't relevant. In this case the proposal is only to sacrifice cleanliness of tests for speed. I think in this case I think its a tradeoff worth making. I also agree with you that in other cases the integration tests have gotten too complicated and should be simplified.
</comment><comment author="dadoonet" created="2015-09-28T14:34:36Z" id="143760948">I'm +1. I always thought that Integration tests are about "integration".
So how all the stuff we are building integrates with each other.

I think it's part of the qa basically.

On the other hand, I agree with Robert on the fact that as a plugin developer I prefer to detect super quickly that my plugin simply does not start anymore.

This is a tradeoff I believe.
</comment><comment author="rmuir" created="2015-09-28T14:39:11Z" id="143762370">&gt; I don't 100% agree with this but my quibble isn't relevant. In this case the proposal is only to sacrifice cleanliness of tests for speed. I think in this case I think its a tradeoff worth making. I also agree with you that in other cases the integration tests have gotten too complicated and should be simplified.

I do not think this is the right tradeoff at all. Plugins integration tests are very fast, and don't need optimizing. Its the distribution and core integration tests that are dog slow.
</comment><comment author="rmuir" created="2015-09-28T14:40:01Z" id="143762565">&gt; On the other hand, I agree with Robert on the fact that as a plugin developer I prefer to detect super quickly that my plugin simply does not start anymore.

Please consider other situations like when we do lucene upgrades, we need to be able to debug this stuff, and not have 87 plugins failing at once.
</comment><comment author="rmuir" created="2015-09-28T14:43:38Z" id="143763572">&gt; I'm +1. I always thought that Integration tests are about "integration".
&gt; So how all the stuff we are building integrates with each other.
&gt; 
&gt; I think it's part of the qa basically.

If you want to test that, then improve the qa/smoke-test-plugins test. But individual units/modules need to be tested first, with contained test cases, so that you never have to debug a crazy complex configuration with multiple plugins unless _its actually an integration problem across them_

That is how tests work, it is the same reason why unit tests are valuable. 
</comment><comment author="jpountz" created="2015-09-28T14:49:22Z" id="143765034">Maybe the issue that is surfacing here is that elasticsearch startup is too slow and we should try to understand why and fix it.
</comment><comment author="rjernst" created="2015-09-28T14:49:38Z" id="143765106">This is a bad idea. We are not gaining any time back when the time to debug problems is more than the "time saved" in builds. Tests should be isolated.
</comment><comment author="dadoonet" created="2015-09-28T14:50:08Z" id="143765236">@jpountz hot reload of plugins ? :)
</comment><comment author="rmuir" created="2015-09-28T14:56:26Z" id="143766926">This issue is optimizing the wrong thing. I have the feeling this happens often not just around tests, but with code too. 

Look at the simplest plugin:

```
[INFO] Plugin: Example site ............................... SUCCESS [  9.293 s]
```

On the other hand:

```
[INFO] Elasticsearch: Core ................................ SUCCESS [08:17 min]
```

In fact I think server startup is something like 2s out of that 9.293s,  but still the fact remains, this server startup in plugin builds is not the slow part of the build.
</comment><comment author="rmuir" created="2015-09-28T15:05:45Z" id="143769244">The difference is even more pronounced on my mac laptop, which I only got a little over a year ago:

```
[INFO] Plugin: Example site ............................... SUCCESS [ 12.014 s]
[INFO] Elasticsearch: Core ................................ SUCCESS [25:29 min]
```
</comment><comment author="javanna" created="2015-09-30T07:21:14Z" id="144312082">I would love to have faster tests too, but I agree that going back to a shared cluster isn't the right solution... and if we want to test that different plugins don't interfere with each other we should have specific tests for that, but in general each plugin should have isolated and easy to debug tests.
</comment><comment author="dakrone" created="2015-09-30T08:05:28Z" id="144320772">Okay, I can understand the reasoning to have plugins isolated for easier debuggingi.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add stack traces to logged exceptions where missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13825</link><project id="" key="" /><description>Switches to .stackTrace(...) for logging statements. Eliminates another 11 calls to .getMessage()

(So much for the trivial/obvious changes.)

Relates to #10021 
</description><key id="108635365">13825</key><summary>Add stack traces to logged exceptions where missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-28T11:00:09Z</created><updated>2016-03-30T10:31:45Z</updated><resolved>2016-03-30T10:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-28T12:01:53Z" id="143724058">LGTM
</comment><comment author="MaineC" created="2015-11-17T13:18:07Z" id="157368496">@s1monw Thanks for the thorough review and for pointing out those flaws. Updated.
</comment><comment author="MaineC" created="2016-01-19T09:55:55Z" id="172797862">@s1monw please ping me once you have time to have another look at the changes you requested so I can get this updated beforehand. (Or let me know if it should just go in after rebasing).
</comment><comment author="s1monw" created="2016-03-10T13:16:47Z" id="194836101">LGTM sorry for the delay
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>An inactive shard is activated by triggered synced flush (backport of #13802)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13824</link><project id="" key="" /><description>When a shard becomes in active we trigger a sync flush in order to speed up future recoveries. The sync flush causes a new translog generation to be made, which in turn confuses the IndexingMemoryController making it think that the shard is active. If no documents comes along in the next 5m, the shard is made inactive again , triggering a sync flush and so forth.

To avoid this, the IndexingMemoryController is changed to ignore empty translogs when checking if a shard became active. This comes with the price of potentially missing indexing operations which are followed by a flush. This is acceptable as if no more index operation come in, it's OK to leave the shard in active.

A new unit test is introduced and comparable integration tests are removed.

Relates #13802
Includes a backport of #13784
</description><key id="108627487">13824</key><summary>An inactive shard is activated by triggered synced flush (backport of #13802)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>non-issue</label></labels><created>2015-09-28T10:13:15Z</created><updated>2015-09-28T12:43:42Z</updated><resolved>2015-09-28T12:43:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-28T10:32:19Z" id="143703378">LGTM!  Thanks @bleskes 
</comment><comment author="bleskes" created="2015-09-28T12:43:35Z" id="143732257">push to 1.7 ( https://github.com/elastic/elasticsearch/commit/d7faab1143106d0b6cd035bd98fff7b2d0d47599 ) . Thx @mikemccand  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin should be able to resolve dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13823</link><project id="" key="" /><description>If you have one plugin that depends on another you have to install both of them now. It'd be nice if you could just install the one you want and both come down and be installed. 

I admit this is a bit weird with classloader isolation for plugins, but its still certainly possible. Imagine a plugin that does dictionary based analysis and each language dictionary is another plugin. Or the watcher and license plugins. 
</description><key id="108625592">13823</key><summary>bin/plugin should be able to resolve dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Plugins</label><label>discuss</label></labels><created>2015-09-28T09:59:30Z</created><updated>2015-09-29T11:52:26Z</updated><resolved>2015-09-29T11:52:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-28T10:08:21Z" id="143699600">Is it the same as #11404 ?
</comment><comment author="rjernst" created="2015-09-28T23:15:00Z" id="143899204">This simply won't work right now. We should get the ability for isolated plugins to communicate working before we think about dependencies at all.  Skipping isolation for plugins is deprecated (it was immediately when it was added), and it needs to be removed before doing anything else with this.
</comment><comment author="rmuir" created="2015-09-28T23:32:31Z" id="143901597">its too complex for plugins to have hard dependencies on each other, we are straining as is to support it, it needs to be removed ASAP.
</comment><comment author="nik9000" created="2015-09-29T11:52:26Z" id="144036201">I agree its a dupe. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>sum aggregation got wrong answer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13822</link><project id="" key="" /><description>I'm using `1.3.4`. Java version `1.7.0_71-b14`.
Sum aggregation goes wrong in my production environment.

The mapping is:

``` json
{
  "activity": {
    "date_detection": false,
    "properties": {
      "partnerCode": {
        "type": "string",
        "fields": {
          "raw": {
            "type": "string",
            "index": "not_analyzed"
          }
        }
      }
      "payAmount": {
        "type": "long"
      }
    },
    ...
  }
  ...
}
```

When I send a aggregation query:

``` json
{
        "query":{
                "filtered":{
                        "filter":{
                                "bool":{
                                        "must":[
                                                {"term": {"activity.partnerCode.raw":"xxx"}}
                                        ]
                                }
                        }
                }
        },
        "aggs": {
                "a": {
                        "sum": {
                                "field": "activity.payAmount"
                        }
                }
        },
        "size": 0
}
```

I got the response:

``` json
{
    "took":9562,
    "timed_out":false,
    "_shards":{"total":2,"successful":2,"failed":0},
    "hits":{"total":1911,"max_score":0.0,"hits":[]},
    "aggregations":{
        "a":{"value":5.083666008154969E20}
    }
}
```

The `payAmount` stores how much money was spend in a trade. No customer had that much money.

I use scan&amp;scroll to export documents, and sum `payAmount' getting 15094439.

It seems index is corrupted and I don't know how to fix it.
</description><key id="108586009">13822</key><summary>sum aggregation got wrong answer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whitejava</reporter><labels><label>:Aggregations</label><label>feedback_needed</label></labels><created>2015-09-28T04:04:38Z</created><updated>2015-10-13T08:29:32Z</updated><resolved>2015-10-13T08:29:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-28T10:21:35Z" id="143701800">@caipeichao hmmm.. i'd check the values of the payAmount field that is being used for this calculation.  Perhaps something like using a comma instead of a period for numbers?

if you think the index is corrupted, can you reindex?
</comment><comment author="whitejava" created="2015-09-30T08:30:26Z" id="144326926">@clintongormley Thanks.

The payAmount is really integers like 10000, 12000, 1999. no comma no decimal.

I have reindexed but not working: I read from the index and write back to the same index. some indexes is corrected, but some indexes is still.
</comment><comment author="justjico" created="2015-10-02T08:09:41Z" id="144953833">@caipeichao I faced a similar issue once, my problem was at least one mapping in one of the indices was wrong. You should verify that across all your indices there's no other field named "payAmount" that has a  different type than "long", it doesn't matter if it's a different type, or is nested differently, if it's the same name it will cause something like this.
</comment><comment author="clintongormley" created="2015-10-02T16:39:01Z" id="145081970">@justjico good thinking - most likely the cause
</comment><comment author="whitejava" created="2015-10-04T05:51:08Z" id="145319683">Thanks! I would check the mapping these days and give you a feedback.
</comment><comment author="whitejava" created="2015-10-10T11:30:02Z" id="147076921">I checked the mapping, there is only one `payAmount` in my mapping.
</comment><comment author="whitejava" created="2015-10-10T11:31:31Z" id="147076961">So, that's likely a index corruption. Then I will try to reindex and check whether fixed.
</comment><comment author="whitejava" created="2015-10-12T09:04:01Z" id="147336205">OK, I confirm it's index corruption. Because I reindexed by copy data to another index, and sum aggregation goes correct.
</comment><comment author="clintongormley" created="2015-10-13T08:29:31Z" id="147646675">thanks for letting us know @caipeichao 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test that Jayatana is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13821</link><project id="" key="" /><description>Installs javatana in vivid, emulates its on-login actions when starting
elasticsearch and verifies that elasticsearch turns off javatana.

Relates to #13813
</description><key id="108574263">13821</key><summary>Test that Jayatana is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-28T01:35:27Z</created><updated>2016-03-10T18:15:35Z</updated><resolved>2015-10-06T13:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-28T01:41:57Z" id="143617363">awesome
</comment><comment author="nik9000" created="2015-09-29T19:35:58Z" id="144165994">@brwe or @spinscale, want to review?
</comment><comment author="spinscale" created="2015-09-30T09:37:10Z" id="144339099">nice. one. Ran the tests

LGTM
</comment><comment author="nik9000" created="2015-10-06T13:17:19Z" id="145853454">Merged to 3.0 and cherry-picked to 2.x, 2.1, and 2.0. So much cherry-pick.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Finish banning ImmutableSet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13820</link><project id="" key="" /><description>Ban ImmutableSet$Builder because that let you sneak some `ImmutableSet`s in.

Remove all remaining imports of ImmutableSet.

Another step in #13224
</description><key id="108568706">13820</key><summary>Finish banning ImmutableSet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-27T23:51:33Z</created><updated>2015-09-29T13:34:34Z</updated><resolved>2015-09-29T13:34:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-29T08:26:19Z" id="143983870">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should forced merges be IO throttled?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13819</link><project id="" key="" /><description>With the new auto-IO throttling in Lucene's ConcurrentMergeScheduler as of Lucene 5.0 (ES 2.0), forced merges (optimize) are never throttled by default.

I think this is fair: the user asked for the index to merge down to 1 (default) segment so it should happen as quickly as possible.  And users generally should force merge only when the index is otherwise idle ... 5.0 has been out for a while now and I haven't heard user complaints about this.

CMS has a setting to change the default MB/sec for forced merges, but we haven't exposed it in ES, and this is a behavior change from before (in ES 1.x) when all merges IO (natural and forced) were held to the shared default of 20 MB/sec.

Note that users can still set the old IO store throttling (`indices/index.store.throttle.max_bytes_per_sec`) and it will still apply to all merges.

So we could do nothing here, and if that turns out to be a problem, users can still use the old way (but we need to re-document this if so).

Or we can open up a MB/sec setting for forced merges ... I don't really like that: ES has too many settings, and "MB/sec" is too raw/low-level for a user who's kicking off a forced merge to necessarily know the right value for.

Or we could hardwire the force merge MB/sec to be a function of the current natural merge MB/sec auto IO throttle, maybe just `==`.
</description><key id="108538714">13819</key><summary>Should forced merges be IO throttled?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2015-09-27T14:17:02Z</created><updated>2017-01-25T10:07:47Z</updated><resolved>2015-10-02T13:42:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-27T16:01:54Z" id="143571158">&gt; I think this is fair: the user asked for the index to merge down to 1 (default) segment so it should happen as quickly as possible.

+1, they asked to do this.
</comment><comment author="nik9000" created="2015-09-27T16:34:04Z" id="143575520">I'm fine with the default being no throttle but an option to throttle might
still be useful for folks serving queries from one index while force
merging another. Or maybe they are force merging on  yesterday's index but
they still want to keep their index rate high for today's data.

Nik
On Sep 27, 2015 6:01 PM, "Robert Muir" notifications@github.com wrote:

&gt; I think this is fair: the user asked for the index to merge down to 1
&gt; (default) segment so it should happen as quickly as possible.
&gt; 
&gt; +1, they asked to do this.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13819#issuecomment-143571158
&gt; .
</comment><comment author="jpountz" created="2015-09-27T21:44:30Z" id="143596956">+1 to not throttle explicit merges. Like Mike said, we are already drowning under settings so I think it's important not to introduce a new one.
</comment><comment author="bleskes" created="2015-09-28T07:09:54Z" id="143657140">I understand the concerns about not adding a settings. I also agree that in some cases one might want to get the force merge done as soon as possible, as there is a human waiting on it. I think the most common one here is when people actually run the upgrade api. On the other hand in many other cases, operator do a maintenance job in the background while users of the cluster keep on searching. For example - and I believe that's practically the single case where we _recommend_ running a force merge - when ones moves yesterday's data to cold nodes in the time based data scenario. I think it will be a great shame if this stress nodes for their IO/CPU
</comment><comment author="clintongormley" created="2015-09-28T10:31:12Z" id="143703209">A merge (forced or otherwise) should never kill your search application. Ideally, a forced merge run on a quiet box should be able to use all available resources, but when run on a busy box, it should know how to play nice with the other resource consumers.

That'd be ideal but, if i understand correctly, then the auto-IO throttling only works with indexing, not with search?
</comment><comment author="mikemccand" created="2015-09-28T10:50:02Z" id="143707806">&gt; That'd be ideal but, if i understand correctly, then the auto-IO throttling only works with indexing, not with search?

Correct: it watches for natural merge "backlog" (more than one merge of nearly the size wants to run).  It doesn't look at any search metrics.

In the ideal world, `ionice` would be available from Java on all OS's per-thread, so we'd be able to just set low priority for natural merges, slightly higher priority for forced ones, and top priority for search IO ... but we are not there yet :)

Maybe we could add a boolean setting, `throttle_force_merges` or something, default to `false`, or maybe a dynamic default based on `IOUtils.spins`, and if you change that to `true`, all forced merges will be throttled at the same rate as natural merges?
</comment><comment author="nik9000" created="2015-09-28T10:59:57Z" id="143712386">I'd be happy with a parameter that defaulted to not throttling. That is a
breaking change from 1.0 and needs docs and stuff but its cool.
On Sep 28, 2015 12:50 PM, "Michael McCandless" notifications@github.com
wrote:

&gt; That'd be ideal but, if i understand correctly, then the auto-IO
&gt; throttling only works with indexing, not with search?
&gt; 
&gt; Correct: it watches for natural merge "backlog" (more than one merge of
&gt; nearly the size wants to run). It doesn't look at any search metrics.
&gt; 
&gt; In the ideal world, ionice would be available from Java on all OS's
&gt; per-thread, so we'd be able to just set low priority for natural merges,
&gt; slightly higher priority for forced ones, and top priority for search IO
&gt; ... but we are not there yet :)
&gt; 
&gt; Maybe we could add a boolean setting, throttle_force_merges or something,
&gt; default to false, or maybe a dynamic default based on IOUtils.spins, and
&gt; if you change that to true, all forced merges will be throttled at the
&gt; same rate as natural merges?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13819#issuecomment-143707806
&gt; .
</comment><comment author="clintongormley" created="2015-09-29T12:27:27Z" id="144044305">We do tell people not to do forced merges on hot boxes (and I wasn't aware that they were throttled in current versions). I'd be tempted to leave the setting for now.  It can always be added if users ask for it.
</comment><comment author="mikemccand" created="2015-09-29T18:58:28Z" id="144156536">&gt; I'd be tempted to leave the setting for now. It can always be added if users ask for it.

OK let's just leave it as is for now (forced merges not throttled) and revisit this if it proves to be a problem
</comment><comment author="joedj" created="2016-07-12T04:24:51Z" id="231934044">I would very much like the ability to throttle forced merges.

I just want to merge indices that aren't going to receive any more writes down to 1 segment - I don't care how long that takes.  It is lower priority than natural merges, even.

Moving shards around is a hassle, and has significant costs associated with it (extra nodes, storage, data transfer).

At the moment, we pick a quiet(er) time of day, disable all alerting for half an hour (!), hit optimize and hope for the best, which is obviously a terrible solution.
</comment><comment author="arpanshah29" created="2016-09-15T07:59:21Z" id="247262378">^ + 1
</comment><comment author="centic9" created="2017-01-25T10:07:47Z" id="275068084">I also would love to be able to throttle this, we have a number of small clusters, so we do not have separate nodes for old data. As there is also no way to see/cancel ongoing merges, we are pretty blind here. Start off an optimize and hope it will not kill search/index operations... 

I know it is documented this way, but it still is not ideal as we also have older indices that we would like to force-merge to one segment but we do not care how long this takes, for us this is also even lower prio than natural merges.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch 2.0.0-beta2 does not accept field names with periods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13818</link><project id="" key="" /><description>I set up an instance of Elasticsearch 1.7.2 that has an index where many of the field names contain periods, e.g. "field.a.x", "field.b.x", etc.  I was able to index and retrieve data from this index with no trouble. When updated to 2.0.0-beta2, Elasticsearch failed to start giving me an error message indicating that the 1.7.2 index could not be upgraded because the field names have periods.
</description><key id="108505944">13818</key><summary>Elasticsearch 2.0.0-beta2 does not accept field names with periods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vichargrave</reporter><labels /><created>2015-09-27T01:07:34Z</created><updated>2015-09-27T10:31:12Z</updated><resolved>2015-09-27T03:49:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-09-27T02:13:10Z" id="143512930">I think this is on purpose. See more in this commit: https://github.com/elastic/elasticsearch/commit/aed1f68e494c65ad50b98a3e0a7a2b6a794b2965
</comment><comment author="vichargrave" created="2015-09-27T03:27:28Z" id="143515133">As far as I'm concerned it is a bug.  However, if that is the way it is going to be then I'll have to live with it.  Thanks for the response.
</comment><comment author="dadoonet" created="2015-09-27T03:49:12Z" id="143515652">Yes. See here the documentation: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_mapping_changes.html#_field_names_may_not_contain_dots

I think that the migration plugin would tell you that.
</comment><comment author="clintongormley" created="2015-09-27T10:31:12Z" id="143540414">&gt; As far as I'm concerned it is a bug. 

Allowing them in the first place was a bug.  It was impossible to distinguish between objects and fields with embedded dots.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete scroll returns empty response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13817</link><project id="" key="" /><description>When calling delete scroll api (eg. curl -XDELETE localhost:9200/_search/scroll/_all or against individual scroll ids), the api response (using 1.7.1) returns an empty response.  It will be more user friendly to provide some information on whether the scroll deletion is successful, etc..
</description><key id="108498952">13817</key><summary>Delete scroll returns empty response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-09-26T22:13:52Z</created><updated>2015-10-09T03:48:33Z</updated><resolved>2015-10-09T03:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-27T09:43:28Z" id="143533419">When deleting an individual scroll ID, ES does produce a 200 in the header if successful and a 404 if the scroll ID wasn't found (doesn't exist, was already deleted)
</comment><comment author="ppf2" created="2015-09-27T16:18:16Z" id="143573909">Yah it does, it will be nice to provide something in the response body though just to be consistent with our other apis :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure XContent is consistent across platforms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13816</link><project id="" key="" /><description>Today we generate XContent with platform dependent linefeeds. This
commit makes the pretty-printed json etc. consistent with \n across all
platforms.
</description><key id="108492792">13816</key><summary>Ensure XContent is consistent across platforms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:REST</label><label>bug</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-26T20:06:38Z</created><updated>2016-03-10T18:15:35Z</updated><resolved>2015-09-29T12:28:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-26T21:21:53Z" id="143497066">+1, looks good.
</comment><comment author="tlrx" created="2015-09-28T07:47:30Z" id="143662659">Left a very minor comment
</comment><comment author="s1monw" created="2015-09-29T12:06:26Z" id="144039065">@tlrx I pushed a new commit can you look?
</comment><comment author="tlrx" created="2015-09-29T12:13:47Z" id="144041038">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename cloud-gce plugin to discovery-gce plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13815</link><project id="" key="" /><description>Follow up azure and aws splits, we need to be consistent and rename `cloud-gce` to `discovery-gce`.
</description><key id="108453248">13815</key><summary>Rename cloud-gce plugin to discovery-gce plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>:Plugin Discovery GCE</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-09-26T10:01:11Z</created><updated>2015-10-08T07:46:07Z</updated><resolved>2015-10-08T04:54:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-26T16:51:11Z" id="143469450">LGTM. Don't we want to document this as a breaking change somewhere...?
</comment><comment author="dadoonet" created="2015-09-26T19:20:10Z" id="143480847">Indeed! Thanks. Will add this.
</comment><comment author="dadoonet" created="2015-09-27T22:46:27Z" id="143599589">I pushed a new version and was almost ready to go but GCE tests are failing when running from Maven. And they pass when ran from the IDE... :(

Need to fix that first obviously.
</comment><comment author="dadoonet" created="2015-09-28T21:30:47Z" id="143879944">So I added some traces, tried to debug that but I have no clue of what is happening.

It sounds like this method is never called when you run tests from the command line (mvn install): https://github.com/dadoonet/elasticsearch/blob/pr/rename-gce/plugins/discovery-gce/src/test/java/org/elasticsearch/discovery/gce/GceComputeServiceMock.java#L60

When you run it from the IDE this method is called.

@rmuir Any idea?
</comment><comment author="bleskes" created="2015-10-06T08:26:19Z" id="145781324">@dadoonet can I help here somehow?
</comment><comment author="dadoonet" created="2015-10-06T08:28:02Z" id="145781607">@bleskes If you have any idea why the branch fails on Maven, then yes you can definitely help.
I did not find the cause for now.
</comment><comment author="dadoonet" created="2015-10-07T21:59:59Z" id="146345272">@bleskes sounds like after rebasing everything looks better. Will run all `mvn install` now and will merge if success.

Thanks!
</comment><comment author="bleskes" created="2015-10-08T07:46:07Z" id="146447305">great.

&gt; On 08 Oct 2015, at 00:00, David Pilato notifications@github.com wrote:
&gt; 
&gt; @bleskes sounds like after rebasing everything looks better. Will run all mvn install now and will merge if success.
&gt; 
&gt; Thanks!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery-ec2 plugin should check `discovery.type`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13814</link><project id="" key="" /><description>As done in #13809 and in Azure, we should check that `discovery.type` is set to `ec2` before starting services.

Closes #13581
</description><key id="108451602">13814</key><summary>Discovery-ec2 plugin should check `discovery.type`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>blocker</label><label>bug</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-26T09:18:02Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-02T18:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-26T13:05:35Z" id="143434242">left a couple of minor suggestions. 
</comment><comment author="dadoonet" created="2015-09-30T10:53:39Z" id="144359354">@bleskes I updated the PR. Let me know.

Marking it as a blocker for 2.0 as we have seen issues in discovery when the old `cloud-aws` is added to a cluster "only for snapshot/restore" purposes.

cc @alexbrasetvik @beiske 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't let ubuntu try to install its crazy jayatana agent.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13813</link><project id="" key="" /><description>By default, our security stuff will reject this (as do other apps).
See https://bugs.launchpad.net/ubuntu/+source/jayatana/+bug/1441487

However its not really the user's fault, ubuntu screws up here by
installing this agent by default. We don't want any agents.

So instead, we drop it like this:

```
$ bin/elasticsearch
Warning: Ignoring JAVA_TOOL_OPTIONS=-Bogus1 -Bogus2
Please pass JVM parameters via JAVA_OPTS instead
[2015-09-25 23:34:39,777][INFO ][node                     ] [Doctor Bong] version[3.0.0-SNAPSHOT], pid[19044], build[2f5b6ea/2015-09-26T03:18:16Z]
...
```

Closes #13785
</description><key id="108436971">13813</key><summary>Don't let ubuntu try to install its crazy jayatana agent.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>bug</label><label>PITA</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-26T03:36:46Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-09-26T03:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-26T03:39:04Z" id="143394774">LGTM
</comment><comment author="dadoonet" created="2015-09-26T03:39:09Z" id="143394777">It looks good to me
</comment><comment author="kno10" created="2015-09-29T08:28:17Z" id="143984706">This should be fixed in Ubuntu (by getting rid of this hack), and not require workaround in Elasticsearch etc.
</comment><comment author="rmuir" created="2015-09-29T09:52:29Z" id="144009788">Don't complain to me, I don't work on ubuntu.
</comment><comment author="nik9000" created="2015-09-29T10:22:26Z" id="144016289">I don't have a link but when I looked around it looked like ubuntu is
moving away from it. But they'll be slow and we got this into the first
version that suffers from it.
On Sep 29, 2015 11:52 AM, "Robert Muir" notifications@github.com wrote:

&gt; Don't complain to me, I don't work on ubuntu.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13813#issuecomment-144009788
&gt; .
</comment><comment author="rmuir" created="2015-09-29T10:23:18Z" id="144016425">I linked to the bug on this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>nuke ES_CLASSPATH appending</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13812</link><project id="" key="" /><description>Out of box, ES expects its stuff to be in particular places. We should not be appending to ES_CLASSPATH, allowing users to specify stuff there, like we do in elasticsearch.bin.sh

If the user sets it, its not going to work out of box.
</description><key id="108435521">13812</key><summary>nuke ES_CLASSPATH appending</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>breaking</label><label>bug</label><label>v2.0.0-rc1</label></labels><created>2015-09-26T02:51:01Z</created><updated>2015-10-07T22:04:33Z</updated><resolved>2015-09-30T22:59:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-26T04:05:28Z" id="143397508">+1 to nuke this. Users shouldnt be adding jars to the classpath. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>native scripts docs are out of date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13811</link><project id="" key="" /><description>I don't think this is going to work:

```
Note, the scripts need to be in the classpath of elasticsearch. One simple way to do it is to create a directory under plugins (choose a descriptive name), and place the jar / classes files there. They will be automatically loaded.
```

We should recommend something else, e.g. to put the jar in lib/, if they are supposed to be in the main classpath.
</description><key id="108435324">13811</key><summary>native scripts docs are out of date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>docs</label><label>v2.0.0</label></labels><created>2015-09-26T02:45:38Z</created><updated>2015-10-30T10:46:48Z</updated><resolved>2015-10-07T17:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-26T02:51:11Z" id="143393187">Nice catch. I wonder if we should look by default in a specific user dir so upgrading elasticsearch would be easier?

Load scripts from plugins._scripts for example?
</comment><comment author="rmuir" created="2015-09-26T02:56:43Z" id="143393369">If we do that, then the fix is more complex than a doc one. I think we should do the simple doc fix for 2.0? what percentage of users are using native scripts?

Separately, I tend to agree it would be cleaner, there is something to be said for "improving" native script support by loading the user classes in a child loader (versus encouraging them to put them in lib/ which will be the main classpath). The downside is that if we do that, we should really add e.g. a qa scenario that tests it, since its more involved. 

I'm also hesitant to overhaul native script support, since scripting engine support is currently also undergoing refactoring in master, and we might want to do things completely differently there (e.g. perhaps native scripting is a plugin or something, i dont know, have not even looked at it).
</comment><comment author="rjernst" created="2015-09-26T03:42:48Z" id="143394902">I think we should just stick to a doc fix for now. At the moment, native scripts must now be loaded through a plugin. We can think about how we could make it easier for 2.1 or 2.2 (although I personally think a plugin is the right thing here), but that's the way it is for 2.0. 
</comment><comment author="dadoonet" created="2015-09-26T03:50:26Z" id="143395125">Agreed
</comment><comment author="ppf2" created="2015-09-28T14:45:05Z" id="143763903">+1.  If they follow the current procedure in the native script documentation today, they will not be able to start up the ES node in 2.x (as expected).  

```
Exception in thread "main" java.lang.IllegalStateException: Unable to initialize plugins
Likely root cause: java.nio.file.NoSuchFileException: /ELK/elasticsearch-2.1.0-SNAPSHOT/plugins/test/plugin-descriptor.properties
    at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
    at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
    at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
    at java.nio.file.Files.newByteChannel(Files.java:315)
    at java.nio.file.Files.newByteChannel(Files.java:361)
    at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:380)
    at java.nio.file.Files.newInputStream(Files.java:106)
    at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
    at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:301)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:107)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:148)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:268)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

As part of the doc resolution to this ticket, let's also include this in our breaking changes documentation (https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_plugin_and_packaging_changes.html and/or https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_scripting_changes.html). Thx!
</comment><comment author="harpreetsb" created="2015-10-30T01:48:45Z" id="152379305">has this been fixed
We upgraded to the ES2.0 and tried restarting it and

```
[2015-10-30 01:34:45,391][INFO ][node                     ] [Spitfire] version[2.0.0], pid[5824], build[de54438/2015-10-22T08:09:48Z]
[2015-10-30 01:34:45,392][INFO ][node                     ] [Spitfire] initializing ...
[2015-10-30 01:34:45,405][ERROR][bootstrap                ] Exception
java.lang.IllegalStateException: Unable to initialize plugins
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:115)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:144)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:170)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:270)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Caused by: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/plugins/_site/plugin-descriptor.properties
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
        at java.nio.file.Files.newByteChannel(Files.java:361)
        at java.nio.file.Files.newByteChannel(Files.java:407)
        at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384)
        at java.nio.file.Files.newInputStream(Files.java:152)
        at org.elasticsearch.plugins.PluginInfo.readFromProperties(PluginInfo.java:86)
        at org.elasticsearch.plugins.PluginsService.getPluginBundles(PluginsService.java:306)
        at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:112)
        ... 5 more

```
</comment><comment author="dadoonet" created="2015-10-30T03:23:43Z" id="152402654">Did you create the descriptor file? https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugin-authors.html#_plugin_descriptor_file
</comment><comment author="harpreetsb" created="2015-10-30T03:42:04Z" id="152406205">No nothing, but i already had few plugins
</comment><comment author="harpreetsb" created="2015-10-30T03:45:19Z" id="152406614">Weird that things work fine on my local machine,
But on my local machine  when i do
`sudo service elasticsearch status`
I get 
`elasticsearch is not running`
 BUT
'http://localhost:9200/' return 

``` json
{
  "status" : 200,
  "name" : "Orphan",
  "cluster_name" : "elasticsearch",
  "version" : {
    "number" : "1.4.4",
    "build_hash" : "c88f77ffc81301dfa9dfd81ca2232f09588bd512",
    "build_timestamp" : "2015-02-19T13:05:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.3"
  },
  "tagline" : "You Know, for Search"
}

```
</comment><comment author="dadoonet" created="2015-10-30T08:09:08Z" id="152456025">I don't understand. You wrote elasticsearch 2.0 but here it's 1.4.4.

Feel free to ask your questions on discuss.elastic.co.
</comment><comment author="harpreetsb" created="2015-10-30T10:46:48Z" id="152490383">oh well i had to delete all previous node data to get the new version running..
I wil post here later onces i configure mine and see if our online server does not give same error.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unreproducible failure of MinimumMasterNodesIT#simpleMinimumMasterNodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13810</link><project id="" key="" /><description>`MinimumMasterNodesIT` failed for me today but I wasn't able to reproduce the failure by rerunning it. Here is the rerun command:

``` shell
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=A956287610A0C2F7 -Dtests.class=org.elasticsearch.cluster.MinimumMasterNodesIT -Dtests.method="simpleMinimumMasterNodes" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=lv -Dtests.timezone=America/Costa_Rica
```

[This](https://gist.github.com/nik9000/f7e02c84da51eab18c51) is the output of the failure. When I compare it to a test run that passes it looks like the cluster isn't able to connect to itself properly. The only other difference I see is the port numbers - the passes connect on `94xx` but the failures try `99xx`.
</description><key id="108434815">13810</key><summary>Unreproducible failure of MinimumMasterNodesIT#simpleMinimumMasterNodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Cluster</label><label>bug</label><label>test</label></labels><created>2015-09-26T02:31:36Z</created><updated>2016-03-18T13:24:42Z</updated><resolved>2016-01-28T18:16:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-26T22:00:25Z" id="143499328">@nik9000 it's hard to tell what's going on - it looks like the log are cut off in half, I don't see any of the logs in the test (info level). Als I'm not sure I follow what you mean with the `94xx`. I presume you don't have a complete dump lying somewhere/this is what the test produced?
</comment><comment author="nik9000" created="2015-09-27T23:03:03Z" id="143600684">&gt; @nik9000 it's hard to tell what's going on - it looks like the log are cut off in half

That was the output that mvn logged for the failure.

&gt; Also I'm not sure I follow what you mean with the 94xx

The port were different.
</comment><comment author="clintongormley" created="2016-01-28T18:16:14Z" id="176316644">This failure is old. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cloud-gce plugin should check `discovery.type`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13809</link><project id="" key="" /><description>closes #13614 
This commmit adds check `discovery.type` and other required parameters before
loading gce plugin.

This change make gce plugin behave like Azure:

if user doesn't set discovery type to `gce`, ES would not load gce modules. 
if user does set discovery type to `gce` but without all parameters we need, es would fail to start.
</description><key id="108424924">13809</key><summary>cloud-gce plugin should check `discovery.type`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud GCE</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T23:43:11Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-09-29T17:51:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-26T02:47:12Z" id="143393003">Thanks for doing this!
Left some comments. It looks very good to me.
Actually I should do the same test for discovery.type in azure and AWS now projects are split.
</comment><comment author="rjernst" created="2015-09-26T03:55:04Z" id="143395329">Is this really necessary? I don't like plugins checking a setting that is not theirs. The gce plugin registers itself as a discovery type. It is the discovery modules job to decide which discovery type to use.
</comment><comment author="xuzha" created="2015-09-26T04:42:10Z" id="143398723">Thanks @dadoonet and @rjernst. I don't have strong opinion.

But I think we could remove checking `discovery.type` here. If `gce` have all required paras, then we could go ahead and register. What do you think @dadoonet. 
</comment><comment author="dadoonet" created="2015-09-26T06:27:46Z" id="143403322">Don't have a strong opinion as well.

Just thinking of what would be the consequences when we will remove the need for setting any cloud.gce property. Not a problem I guess.

Let's apply Ryan suggestion.
</comment><comment author="dadoonet" created="2015-09-26T08:47:49Z" id="143412434">Actually, I like the fact we don't bind unnecessary modules when discovery is not set to `gce`.

That's what we do today for azure: https://github.com/elastic/elasticsearch/blob/master/plugins/discovery-azure/src/main/java/org/elasticsearch/cloud/azure/AzureDiscoveryModule.java#L79-L83

I think it's cleaner.

If we don't do that, in Azure, we will automatically start `AzureComputeServiceImpl` which will complain that some settings are not set. But may be in that case `AzureComputeServiceImpl` should detect that `discovery.type` is not set to `azure` and just don't log any warning in that case.
</comment><comment author="bleskes" created="2015-09-26T17:02:39Z" id="143470001">@rjernst I agree with your sentiment. Do you see any way of allowing the plugin to bind extra services if the DiscoveryModule activated it ? there are things like network address resolution, adding unicast host providers etc. 

@dadoonet long term it will be awesome if we can decouple things such that we won't need guice. The network part of the plugin can be registered all the time (people need to active it using _gce_ in their setting) and maybe the unicast host provider can be create in GCEDiscovery on start (GCEDiscovery is now kinda empty)
</comment><comment author="rjernst" created="2015-09-26T17:33:37Z" id="143473334">@bleskes I'm not sure we need extra services? I have two thoughts:
- Can the unicast hosts list be provided through subclassing zen discovery?
- Does zen discovery cover to many "things"? Should we have separate pluggable apis for master election (election.type?), ping service (ping.type?), and host discovery? Could this split be done now or are there tight couplings right now blocking a split like that?
</comment><comment author="bleskes" created="2015-09-26T17:40:46Z" id="143474992">@rjernst  re splitting discovery - yeah and I'm still thinking on what's the right split. these things are tied together. Breaking them up are not trivial at all. Note though that GCE only supplies configuration (i.e., list of hosts to ping), which makes it creating it from the discovery implementation an option (see note to david from earlier :))
</comment><comment author="rjernst" created="2015-09-28T05:52:17Z" id="143645924">I'm fine for now with moving the hosts list to a protected method that plugins override on their subclass of zen (I think that's what you are suggesting)?
</comment><comment author="bleskes" created="2015-09-28T06:58:48Z" id="143655921">I looked at the code and I think it will be a biggish change to move the unicast host providers from Guice into a method zen discovery. It will also imply that UnicastZenPing needs to move ZenDiscovery and ZenPingService need to be either moved away from Guice as well or have an extra method for late addition of ping types. 

For the sake of moving forward, I suggest we proceed with the current approach where we check settings for _now_ and try to work towards a better way of doing this in ZenDiscovery/DiscoveryModule.

&gt; On 28 Sep 2015, at 07:52, Ryan Ernst notifications@github.com wrote:
&gt; 
&gt; I'm fine for now with moving the hosts list to a protected method that plugins override on their subclass of zen (I think that's what you are suggesting)?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="xuzha" created="2015-09-28T08:26:48Z" id="143673504">Thanks a lot guys.
Updated and addressed some concerns @dadoonet. 
</comment><comment author="dadoonet" created="2015-09-29T13:27:23Z" id="144058428">It looks good to me
</comment><comment author="dadoonet" created="2015-09-29T17:31:13Z" id="144129516">@xuzha I think we need it as well in 2.x and 2.0 branches. See https://github.com/elastic/elasticsearch/blob/2.0/plugins/cloud-gce/src/main/java/org/elasticsearch/plugin/cloud/gce/CloudGcePlugin.java#L74

We hit a similar issue with ec2 today on 2.0 branch. Could you check it and cherry pick this commit if needed to 2.0, 2.x branches? And add 2.0.0 and 2.1.0 labels to this PR?
</comment><comment author="xuzha" created="2015-09-29T17:33:22Z" id="144130040">@dadoonet sure, I saw your message in hipchat, I think we should do it.
</comment><comment author="dadoonet" created="2015-09-29T17:34:32Z" id="144130325">thanks for confirming. I added labels to this PR then.
</comment><comment author="xuzha" created="2015-09-29T17:55:12Z" id="144135451">In 2.x and 2.0 now,

2.0 https://github.com/elastic/elasticsearch/commit/21092d94c2ed2492fbc6914a2f9f5d24d839e4ec
2.x https://github.com/elastic/elasticsearch/commit/9654e089f571b345d675d208ed2fb4b9e4ec1259
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Search cluster not auto repairing (1.7.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13808</link><project id="" key="" /><description> I have es 1.7.2 and have a cluster of 10 nodes. I wanted to scale down to ten, so I terminated one of the boxes and watched hte cluster status change from green to yellow. Before the upgrade to 1.7.2 from 1.4.5, it will automatically relocate all the shards.
Now it sits with a bunch of unassigned_shards forever. I found a fix where I run the following:

curl -XPUT 'localhost:9200/_cluster/settings' -d '{
    "transient" : {
        "cluster.routing.allocation.enable" : "all"
    }
}'

This fixed my issue and my cluster went back to green. However, I need to run that command every time I teardown a node on my way down to 5. Before I never had to run that command and I could safely destroy one node, one by one, going from green to yellow back to green on its own. 

I would eventually like this to autoscale, but cannot do that if I need to enable allocation each time i drop a node. 

Any thoughts? 

Thanks
</description><key id="108414646">13808</key><summary>Elastic Search cluster not auto repairing (1.7.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trikosuave</reporter><labels /><created>2015-09-25T22:00:00Z</created><updated>2015-09-28T07:02:41Z</updated><resolved>2015-09-27T09:55:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-27T09:55:30Z" id="143534115">@trikosuave please ask these questions on the forum instead: http://discuss.elastic.co/

Presumably you had allocation disabled for some reason (maybe you set it during the upgrade). Once you reset it to `all`, things work as expected.
</comment><comment author="trikosuave" created="2015-09-28T04:13:24Z" id="143637493">Yes, I when I set it to 'all', it started assigning the unassigned shards. However, the next time a node went down, its shards went to unassigned where they stayed until I set it to 'all' again. Hence the _issue_. I won't always be there when a node goes down to "manually" tell it to assign all the unassigned shards. I should only have to set it once and have it take care of itself from then on. 

I'm not treating github/issues as a forum. I am reporting an issue I have that I did not have before the upgrade to 1.7.2

Thank you.
</comment><comment author="bleskes" created="2015-09-28T07:02:40Z" id="143656312">@trikosuave just double checking - how long did you wait for shard to be allocated after a node went down? In 1.7.2 we wait for 1m to give the node a chance to come back up (and avoid unneeded recoveries on start up). See [documentation here](https://www.elastic.co/guide/en/elasticsearch/reference/current/delayed-allocation.html?q=node%20left%20allocation#delayed-allocation).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch is deleting some documents by itself </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13807</link><project id="" key="" /><description>I'm having a real hard issue, elasticsearch on a production server is deleting a lot of documents, when the index pass over 1.4 GB begins deleting a lot of documents, until now 2477 documents, help please!
</description><key id="108410264">13807</key><summary>Elasticsearch is deleting some documents by itself </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jorgadan</reporter><labels /><created>2015-09-25T21:22:28Z</created><updated>2015-09-27T10:32:31Z</updated><resolved>2015-09-27T10:32:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-26T20:32:18Z" id="143492468">@jorgadan Can you give more information about the issue you are seeing (elasticsearch version, what the actual symptoms are, etc.)
</comment><comment author="jorgadan" created="2015-09-27T00:53:47Z" id="143507558">Ok, I had an index and I created a copy for fixing some mapping errors and then I reindex from the copy to the original with the reindex plugin, but when I do localhost:9200/_cat/indices/v my original index is 1.4 GB and my copy is 1.7 GB, and the original says that there are 2270 deleted documents but I haven't deleted any doc.
</comment><comment author="mikemccand" created="2015-09-27T04:24:38Z" id="143516800">Did you replace any already previously indexed document?  This is a delete + add under the hood.

Or, did you hit any exceptions while indexing some documents?  In some cases this results in marking the failed document as deleted.
</comment><comment author="clintongormley" created="2015-09-27T10:32:31Z" id="143540455">@jorgadan this sounds like simple misunderstanding.  The best place for discussions like this is in the forum.  http://discuss.elastic.co/

if you find a bug, feel free to reopen
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add match count scoring option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13806</link><project id="" key="" /><description>Feature Request:

I would like to be able to run a query that simply returns a score indicating the number of terms that matched my query. As far as I can tell after poring over the documentation (&amp; internet), this is not currently supported.

Say my index contains the following records:
- `{ "name": "john jacob smith junior 3rd" }`
- `{ "name": "john jacob smith junior" }`
- `{ "name": "john smith 3rd" }`
- `{ "name": "fred smith" }`

These are the scores I'd want to get back for the following `match` queries:

search terms: `john smith`
- `john jacob smith junior 3rd` =&gt; 2
- `john jacob smith junior` =&gt; 2
- `john smith 3rd` =&gt; 2
- `fred smith` =&gt; 1

search terms: `fred smith 3rd`
- `john jacob smith junior 3rd` =&gt; 2
- `john jacob smith junior` =&gt; 1
- `john smith 3rd` =&gt; 1
- `fred smith` =&gt; 2

etc.

This should support all the semantics of a standard `match` query, e.g. `fuzziness`, `minimum_should_match`, etc., and I should be able to set a `boost` so that I may weight these queries in a `should` clause.

I feel like such an approach would fill what seems to me to be a gap between the infinite granularity of standard `match` scores (closeness of the search term to the searched field normalized by TF-IDF), and `constant_score` queries that collapse everything down to a simple yes or no.

The rationale for this is that in our application we don't actually care how many terms in the searched field _don't_ match, we just care how many _do_ match, and we don't care how rare or important a search term is in the index. We want to give every single record that matches the same number of search terms the precisely same score, and then use our own business logic to boost ranking appropriately based on other fields, e.g. recency, popularity, promoted status, etc.

Using the examples above, a search for `smith` would give every record a 1, and we would use our other fields (not shown for brevity) in a `function_score` query to boost the ones we feel are most relevant to the top of the pack.

I feel like it would make sense to implement this as a parameter that could be added to the existing query types. Something like `score_mode` that accepted values like `similarity` or `match_count`, where `similarity` would be the default and represents how scoring currently works, and `match_count` would be my proposed addition.

So a simple query might look like:

```
{
    "query": {
        "match": {
            "display_name": {
                "fuzziness": 0.75,
                "query": "smith",
                "scoring_mode": "match_count"
            }
        }
    }
}
```

In our use case we'd want to bundle several of these up in a `should` clause, which does seem to normalize the overall score (I wrote a query with two `constant_score` clauses with `boost: 1` and matches on both clauses had score of 1.4142135 and matches on one clause had score of 0.35355338). That's not ideal, but I could work with it. In a perfect world (or my perfect world!) a `should` clause containing multiple of these `match_count` queries would emit the sum of all those scores without modification.
</description><key id="108395226">13806</key><summary>Add match count scoring option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpotisch</reporter><labels /><created>2015-09-25T19:41:54Z</created><updated>2016-03-17T06:52:07Z</updated><resolved>2015-09-27T10:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpotisch" created="2015-09-25T21:06:24Z" id="143354816">**UPDATE**: In going over this with a colleague I realized that only knowing how many search terms matched a record in the index could obscure the fact that it really was a low quality match. e.g. the query terms `john jacob smith junior` would score a 2 against `john smith`, but the query `john smith` would produce the exactly same score against that record despite having no term misses.

Perhaps in addition to or instead of `match_count` it could have `match_percent`. In that case `john smith` would get a score of 0.5 against the query `john jacob smith junior`, while the same query against the record `john smith` would produce a score of 1.

This is a little closer to Elasticsearch's behavior today, but with the important removal of the TF-IDF normalization and the exceedingly fine granularity that overwhelms our custom `function_score` ranking.
</comment><comment author="s1monw" created="2015-09-26T20:25:20Z" id="143492181">so basically what you are saying is you just want to wrap each term in the boolean query in a constant score query that returns `1` on a match?
</comment><comment author="clintongormley" created="2015-09-27T10:25:26Z" id="143540222">You have a few options.  You can disable term stats completely by setting [`index_options`](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/index-options.html) to `docs`.

You can use BM25 similarity and [tune `k1` and `b`](https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html#bm25-tunability) to control term frequency saturation and field length normalization.

Use a constant score query per term, wrapped in a `bool` query (with `disable_coord` set to `true`).

You're already using function score, so you could just filter on the terms you're interested in and score each one with the `weight` you desire.

Alternatively, use the `weight` and  `boost_mode` parameters in function score to tune how query weights are combined with your function score weights.

Write your own custom similarity that does exactly what you want.

i think there are enough options here that we don't need to further complicate the match query.
</comment><comment author="jpotisch" created="2015-09-28T16:24:11Z" id="143796098">@clintongormley thanks for the info. I definitely don't want to gum up the works if this is already possible. I will look into the options you describe but based on your reply I want to be sure I explained this clearly. You mentioned using `constant_score` but that would collapse all those scores down to the same value (value of `boost` or default of 1), no? And re: `function_score` I don't want to "filter on the terms [I'm] interested in", I want to include everything.

I'm not looking to filter results or get precise weighting across multiple `should` clauses, I'm trying to get a score that tells me how many search terms matched the searched record, without normalizing for term frequency, etc. Something along the lines of having the search terms `a b c d` produce a score of:
- 1 against the field `"A"`
- 2 against the record `"A B x y z"`
- 4 against the record `"A x B x C x D foo bar guacamole"`
- etc.

My apologies if my initial description was misleading, or if I've misunderstood your reply. I will of course defer to you completely on this but I just want to make sure we're not talking past each other.
</comment><comment author="clintongormley" created="2015-09-29T14:04:38Z" id="144070838">Here's what I mean:

You can tune BM25 to ignore term frequency (k1) and field length normalization (b) as follows:

```
PUT t
{
  "similarity": {
    "only_idf": {
      "type": "BM25",
      "k1": 0,
      "b": 0
    }
  },
  "mappings": {
    "my_type": {
      "properties": {
        "field": {
          "type": "string",
          "similarity": "only_idf"
        }
      }
    }
  }
}

POST t/my_type/_bulk
{"index": {}}
{"field": "A"}
{"index": {}}
{"field": "A B x y z"}
{"index": {}}
{"field": "A x B x C x D foo bar guacamole"}
```

This query will add up the IDF for each matching term (you mentioned the requirement in your second comment to differentiate between high and low quality terms):

```
GET t/_search
{
  "query": {
    "bool": {
      "disable_coord": true,
      "should": [
        {
          "term": {
            "field": "a"
          }
        },
        {
          "term": {
            "field": "b"
          }
        },
        {
          "term": {
            "field": "c"
          }
        },
        {
          "term": {
            "field": "d"
          }
        }
      ]
    }
  }
}
```

You can use constant scores to count each match as 1 (although the final score for each document is normalised so you get 0.5, 1, and 2, instead of 1, 2, and 4:

```
GET t/_search
{
  "query": {
    "bool": {
      "disable_coord": true,
      "should": [
        {
          "constant_score": {
            "filter": {
              "term": {
                "field": "a"
              }
            }
          }
        },
        {
          "constant_score": {
            "filter": {
              "term": {
                "field": "b"
              }
            }
          }
        },
        {
          "constant_score": {
            "filter": {
              "term": {
                "field": "c"
              }
            }
          }
        },
        {
          "constant_score": {
            "filter": {
              "term": {
                "field": "d"
              }
            }
          }
        }
      ]
    }
  }
}
```

And here's a function score query which gives you 1, 2, and 4:

```
GET t/_search
{
  "query": {
    "function_score": {
      "query": {
        "match": {
          "field": "a b c d"
        }
      },
      "boost_mode": "replace",
      "score_mode": "sum",
      "functions": [
        {
          "filter": {
            "term": {
              "field": "a"
            }
          },
          "weight": 1
        },
        {
          "filter": {
            "term": {
              "field": "b"
            }
          },
          "weight": 1
        },
        {
          "filter": {
            "term": {
              "field": "c"
            }
          },
          "weight": 1
        },
        {
          "filter": {
            "term": {
              "field": "d"
            }
          },
          "weight": 1
        }
      ]
    }
  }
}
```
</comment><comment author="jpotisch" created="2015-09-30T21:39:57Z" id="144550175">Thanks @clintongormley. This is _exceedingly_ helpful. I didn't realize I'd effectively have to shatter my search term into individual `should` clauses (`"field": "a"`, `"field": "b"`, etc.) So I guess I need to tokenize my search terms on white space and build the query dynamically, one `should` array item per term?

And I didn't mean I need to distinguish between high and low quality _terms_ (like `the` vs. `guacamole`), I meant I want to distinguish high quality matches (all my search terms match) from low quality matches (some terms were found, some were not). For example if I search for `a b`, the record `a b c d` would be a high quality match, as it contains all my search terms, and would have a score of 2 using your function_score query.

My concern is that the query `a b c d` against the record `a b` would also produce a score of 2 even though half my search terms missed. To be clear I still want them in the results, I'd just want them to score lower. But now that I think it through I wonder if this actually would cause issues in practice. I don't actually care about correctly ranking results for search `a b` against those for search `a b c d`, I only care that all results that contain `a` and `b` and `c` and `d` are ranked identically at the top, followed by all records that are missing one term, then missing two, all the way down to the `minimum_should_match` I specify.

Any parting words of advice/caution regarding the above? Otherwise you're free to go. ;-) (And regardless, I owe you a beer or three!)
</comment><comment author="clintongormley" created="2015-10-02T15:28:40Z" id="145062557">&gt; My concern is that the query a b c d against the record a b would also produce a score of 2 even though half my search terms missed. To be clear I still want them in the results, I'd just want them to score lower. 

That's where query coordination comes in handy  (ie the thing I disabled with `disable_coord`).  See https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html#coord for more
</comment><comment author="q11112345" created="2016-03-17T03:32:08Z" id="197676795">what if i cant analyze the query string myself ?
for query string: "iphone5s"
i need the analyzer analyzing the query string and having the same effect as jpotisch need.

for example:
query string: "iphone5s"
terms analyzed by index analyzer: "iphone" "5s"
score result expecting: 2, 1 or 0

can this be possible?
@clintongormley 

and the best is that i can know which term is matched to the document in script score, then i can do something to adjust the weight of different term, 
i may build up a mapping from term to weight in script.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ElasticsearchException ids for 2.0 consistent with master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13805</link><project id="" key="" /><description>This commit reworks the ElasticsearchException ids so that they are
consistent with master at f40ae25352f842f188ce833406f8aeb500ae64ca.
</description><key id="108395157">13805</key><summary>Make ElasticsearchException ids for 2.0 consistent with master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Exceptions</label><label>:Internal</label><label>enhancement</label></labels><created>2015-09-25T19:41:30Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-09-25T20:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-25T19:57:28Z" id="143337445">left one comment
</comment><comment author="s1monw" created="2015-09-25T20:49:54Z" id="143349845">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fork Lucene PatternTokenizer to apply LUCENE-6814 (closes #13721)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13804</link><project id="" key="" /><description>While LUCENE-6814 fixes #13721, it will only make it into Lucene 5.4. This will make it only available to elasticsearch 2.0.

There appears to be a very low chance of LUCENE-6814 getting fixed in 4.10, which elasticsearch 1.x uses. I've made a copy of Lucene 4.10.4 PatternTokenizer with the fix so that 1.x versions will also be able to benefit.
</description><key id="108393526">13804</key><summary>Fork Lucene PatternTokenizer to apply LUCENE-6814 (closes #13721)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">achow</reporter><labels><label>:Core</label><label>bug</label></labels><created>2015-09-25T19:31:30Z</created><updated>2015-12-16T14:51:48Z</updated><resolved>2015-11-05T23:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-11-05T20:22:12Z" id="154179570">Hi @achow, I think it's reasonable to fork this fix into ES 1.7.x, with LUCENE-6814 fixed, since Lucene is unlikely to do another 4.10.x release soon and another ES 1.7.x will likely happen and the memory leak is sizable for your usage case.

Can you please add a comment to the forked class explaining that it's a copy from Lucene 4.10.3, but with LUCENE-6814 applied?  Also, can you rename the classes to XPatternAnalyzer/Tokenizer (this is our convention for a forked-from-Lucene class) and leave in the original `org.apache.lucene.*` namespace?  Thanks.
</comment><comment author="achow" created="2015-11-05T22:21:31Z" id="154213661">@mikemccand updated. Looks like I opened this up against 1.6 though.. Should I just create another pull against 1.7?
</comment><comment author="mikemccand" created="2015-11-05T23:31:45Z" id="154229928">&gt; Should I just create another pull against 1.7?

Oh yes please, @achow, thank you!
</comment><comment author="achow" created="2015-11-05T23:58:45Z" id="154237906">Opened https://github.com/elastic/elasticsearch/pull/14571
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can NOT do template query on updated search template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13803</link><project id="" key="" /><description>Use ElasticSearch 1.5.
Create a search template, then update it.
Confirm the search template is updated by retrieving it: GET /_search/template/myTemplate
We can find that the search template is correctly updated.
However, when we try to do template query using 'myTemplate', ES still uses the old template.
</description><key id="108386631">13803</key><summary>can NOT do template query on updated search template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blingwang</reporter><labels /><created>2015-09-25T18:50:58Z</created><updated>2015-09-27T09:50:45Z</updated><resolved>2015-09-27T09:50:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-27T09:50:45Z" id="143533626">Fixed in https://github.com/elastic/elasticsearch/pull/10526
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>An inactive shard is activated by triggered synced flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13802</link><project id="" key="" /><description>When a shard becomes inactive we trigger a sync flush in order to speed up future recoveries. The sync flush causes a new translog generation to be made, which in turn confuses the IndexingMemoryController making it think that the shard is active. If no documents come along in the next 5m, the shard is made inactive again , triggering a sync flush and so forth.

To avoid this, the IndexingMemoryController is changed to ignore empty translogs when checking if a shard became active. This comes with the price of potentially missing indexing operations which are followed by a flush. This is acceptable as if no more index operation come in, it's OK to leave the shard inactive.

A new unit test is introduced and comparable integration tests are removed.

I think we need to push this all the way to 1.7.3 ... 
</description><key id="108383273">13802</key><summary>An inactive shard is activated by triggered synced flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>review</label><label>v1.7.3</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T18:29:15Z</created><updated>2015-11-22T10:11:24Z</updated><resolved>2015-09-27T05:25:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-26T09:35:35Z" id="143414924">This was a great catch @bleskes, and the cutover from IT to unit test is wonderful.

I left a bunch of small comments, but otherwise LGTM.

I still find that massive if statement in `updateShardStatuses` confusing but I can't quickly see an easy way to improve it...
</comment><comment author="bleskes" created="2015-09-26T18:58:10Z" id="143479578">@mikemccand thx for the review. I pushed another round. Tried to simplify the 'big if'
</comment><comment author="mikemccand" created="2015-09-26T19:21:12Z" id="143480888">LGTM, thanks @bleskes!
</comment><comment author="bleskes" created="2015-09-27T05:35:32Z" id="143521138">push this to master. will give CI some time before pushing it all the way back to 1.7...
</comment><comment author="bleskes" created="2015-09-27T09:19:09Z" id="143532683">&gt; Pre-existing issue: I think 30s default is too large, because an index that was inactive and suddenly becomes active with a 512 KB indexing

I was thinking about this some more (morning run, he he) - I think we should fold all the decisions and state about being active or not into the indexshard. Then it will be easy to reset on every index operation (immediately). We also already have the notion of an inactivity listener - we can extend it to have an active event as well. We can also add an checkIfInactive method on IndexShard which check if there was any indexing ops (we already have stats) since the last time it was checked. To avoid making IndexShard more complex, we can push this functionality ShardIndexingService - which is already in charge of stats.
</comment><comment author="mikemccand" created="2015-09-27T14:39:31Z" id="143563355">&gt;  I think we should fold all the decisions and state about being active or not into the indexshard.

+1, this would be cleaner.  E.g. I don't like how `IndexShard.updateBufferSize` "infers" that it's going inactive by looking if the new size is 512 KB ... would be better it was more directly aware it's going inactive.

I can try to tackle this.

&gt;  To avoid making IndexShard more complex, we can push this functionality ShardIndexingService - which is already in charge of stats.

Or, why not absorb `ShardIndexingService` into `IndexShard`?  All it does is stats gathering and calling listeners (which I think is only used by percolator?).  It's also confusing how it has `postCreate` and `postCreateUnderLock`.  Seems like maybe an excessive abstraction at this point...
</comment><comment author="bleskes" created="2015-09-27T15:03:29Z" id="143565350">I personally don't mind folding it into indexshard though having a dedicate stats/active component has benefit (clearer testing suite) . I know Simon has an opinion on this.

On 27 sep. 2015 4:39 PM +0200, Michael McCandlessnotifications@github.com, wrote:

&gt; &gt; I think we should fold all the decisions and state about being active or not into the indexshard.
&gt; 
&gt; +1, this would be cleaner. E.g. I don't like howIndexShard.updateBufferSize"infers" that it's going inactive by looking if the new size is 512 KB ... would be better it was more directly aware it's going inactive.
&gt; 
&gt; I can try to tackle this.
&gt; 
&gt; &gt; To avoid making IndexShard more complex, we can push this functionality ShardIndexingService - which is already in charge of stats.
&gt; 
&gt; Or, why not absorbShardIndexingServiceintoIndexShard? All it does is stats gathering and calling listeners (which I think is only used by percolator?). It's also confusing how it haspostCreateandpostCreateUnderLock. Seems like maybe an excessive abstraction at this point...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/pull/13802#issuecomment-143563355).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Omit current* stats for OldShardStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13801</link><project id="" key="" /><description>Closes #13386
</description><key id="108373150">13801</key><summary>Omit current* stats for OldShardStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">achow</reporter><labels><label>:Stats</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T17:32:09Z</created><updated>2015-11-20T09:33:17Z</updated><resolved>2015-10-19T09:48:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-10-18T20:40:32Z" id="149045253">this looks good to me! thanks for opening the PR
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update java api docs after query refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13800</link><project id="" key="" /><description>On the query-refactoring branch we made some breaking changes to the java api. We need to update the java api docs accordingly.
</description><key id="108348664">13800</key><summary>Update java api docs after query refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>docs</label></labels><created>2015-09-25T15:34:13Z</created><updated>2015-10-20T08:56:43Z</updated><resolved>2015-10-20T08:56:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove support for pre 2.0 indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13799</link><project id="" key="" /><description>This commit removes all index level compatibilty and upgrade paths for
pre 2.0 indices. This includes:
- Remove leftover from delete_by_query to replay translog records,
  since in 3.x all pending delelte_by_query instances are applied on the
  upgrade to 2.x we can remove the bwc layer now.
- Remove Elasticsearch090PostingsFormat - we maintained our own posting format
  until 2.0 this is now removed since folks need to upgrade to 2.x first before going
  to 3.0
- Remove BloomFilterPostingFormat - this was only used for ID fields in the Elasticsearch090PostingsFormat
- Remove upgrade methods to pre 2.0 translogs without checkpoints
</description><key id="108347636">13799</key><summary>Remove support for pre 2.0 indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T15:27:58Z</created><updated>2015-10-13T07:03:32Z</updated><resolved>2015-10-08T18:33:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-25T20:05:39Z" id="143338859">@rjernst @bleskes @rmuir  can you take a look / raise your opinion?
</comment><comment author="bleskes" created="2015-09-29T12:29:31Z" id="144044666">Left some minor comments.
</comment><comment author="jpountz" created="2015-10-08T14:51:44Z" id="146569590">LGTM
</comment><comment author="s1monw" created="2015-10-08T15:09:47Z" id="146574466">@bleskes can you take another look - this get's stale quickly so I'd like to move on here soonish
</comment><comment author="s1monw" created="2015-10-08T18:19:02Z" id="146643818">I spoke to @bleskes and he is ok with moving forward given @jpountz LGTM since he has limited availability. I will push this in a bit
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tests: Avoid unicode strings in alternative query builder tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13798</link><project id="" key="" /><description>When testing alternative versions of the json syntax for the term queries (SpanTermQueryBuilder and TermQueryBuilder) we don't go trough the doXContent json rendering, so having a unicode value creates problems for the parser later, causing those tests to fail rarely. Since Unicode values are generally tested elsewhere, I suggest we make sure we always have ascii here.
</description><key id="108346696">13798</key><summary>Tests: Avoid unicode strings in alternative query builder tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T15:22:36Z</created><updated>2015-10-12T11:03:10Z</updated><resolved>2015-10-12T11:03:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-09-29T09:34:19Z" id="144006175">I think we should only escape `"` and control chars as required by the JSON specification and allow unicode to be part of the randomized string value.
</comment><comment author="cbuescher" created="2015-09-29T14:43:27Z" id="144082044">@tlrx thanks, added quoting for unicode strings in the test setup. Care to take another look?
</comment><comment author="tlrx" created="2015-10-07T12:53:46Z" id="146188173">LGTM
</comment><comment author="cbuescher" created="2015-10-12T11:03:09Z" id="147366139">Merged with affe2f2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove IndexService dep. from IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13797</link><project id="" key="" /><description>there is no reason for the index shard to hold on to it's corresponding
index service. This dependency is unnecessary.
</description><key id="108345337">13797</key><summary>Remove IndexService dep. from IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T15:14:21Z</created><updated>2015-09-29T12:21:00Z</updated><resolved>2015-09-29T12:20:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-28T12:49:37Z" id="143734630">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove reflection hacks from ElasticsearchException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13796</link><project id="" key="" /><description>Today we use reflection where it's not needed anymore since java8 can
pass ctors around. This commit replaces runtime checks with compile time
checks which is always preferrable.
</description><key id="108339856">13796</key><summary>Remove reflection hacks from ElasticsearchException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T14:44:19Z</created><updated>2015-09-25T19:02:50Z</updated><resolved>2015-09-25T19:02:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title> Add support of field boost in prefix query #5418 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13795</link><project id="" key="" /><description>I am facing very similar issue as described in #5418. Having and index and mapping from #5418, following query:

`curl -XPOST 'http://localhost:9200/messages/_search' -d '{ "query" : { "query_string" : { "query" : "tes*" , "rewrite" : "top_terms_10" } } , "explain" : true }' | python -mjson.tool`

returns results with no boost applied for document with ID 1 (mind the wildcard). I tried using `analyze_wildcard` and all sort of combinations of `top_terms...N` as @clintongormley suggested - no luck.

ES 1.7.2.

Output:

```

    "_shards": {
        "failed": 0,
        "successful": 5,
        "total": 5
    },
    "hits": {
        "hits": [
            {
                "_explanation": {
                    "description": "weight(_all:test in 0) [PerFieldSimilarity], result of:",
                    "details": [
                        {
                            "description": "fieldWeight in 0, product of:",
                            "details": [
                                {
                                    "description": "tf(freq=1.0), with freq of:",
                                    "details": [
                                        {
                                            "description": "termFreq=1.0",
                                            "value": 1.0
                                        }
                                    ],
                                    "value": 1.0
                                },
                                {
                                    "description": "idf(docFreq=1, maxDocs=1)",
                                    "value": 0.30685282
                                },
                                {
                                    "description": "fieldNorm(doc=0)",
                                    "value": 0.5
                                }
                            ],
                            "value": 0.15342641
                        }
                    ],
                    "value": 0.15342641
                },
                "_id": "1",
                "_index": "messages",
                "_node": "CYrdARWXRwSA2lX-yTU8uw",
                "_score": 0.15342641,
                "_shard": 2,
                "_source": {
                    "comment": "whatever",
                    "message": "test message",
                    "user": "user1"
                },
                "_type": "message"
            },
            {
                "_explanation": {
                    "description": "weight(_all:test in 0) [PerFieldSimilarity], result of:",
                    "details": [
                        {
                            "description": "fieldWeight in 0, product of:",
                            "details": [
                                {
                                    "description": "tf(freq=1.0), with freq of:",
                                    "details": [
                                        {
                                            "description": "termFreq=1.0",
                                            "value": 1.0
                                        }
                                    ],
                                    "value": 1.0
                                },
                                {
                                    "description": "idf(docFreq=1, maxDocs=1)",
                                    "value": 0.30685282
                                },
                                {
                                    "description": "fieldNorm(doc=0)",
                                    "value": 0.4375
                                }
                            ],
                            "value": 0.13424811
                        }
                    ],
                    "value": 0.13424811
                },
                "_id": "2",
                "_index": "messages",
                "_node": "CYrdARWXRwSA2lX-yTU8uw",
                "_score": 0.13424811,
                "_shard": 3,
                "_source": {
                    "comment": "test comment",
                    "message": "hello world",
                    "user": "user2"
                },
                "_type": "message"
            }
        ],
        "max_score": 0.15342641,
        "total": 2
    },
    "timed_out": false,
    "took": 1
}
```
</description><key id="108332589">13795</key><summary> Add support of field boost in prefix query #5418 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jCalamari</reporter><labels><label>:Query DSL</label><label>bug</label><label>discuss</label></labels><created>2015-09-25T14:01:23Z</created><updated>2016-01-28T18:15:39Z</updated><resolved>2016-01-28T18:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-27T09:17:34Z" id="143532648">Yeah, it looks like the payload is not being applied in the case of the prefix.  

Question for me is: does it actually make sense to apply boost here?  Prefix queries are rewritten as constant scores for a good reason: we have no idea if it is the word the user is interested in.
</comment><comment author="clintongormley" created="2016-01-28T18:15:39Z" id="176316299">Query time boosts are a more flexible and useful alternative here.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ES internal deletion policies in favour of Lucenes implementations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13794</link><project id="" key="" /><description>These classes are really duplicates and are just here for historical reasons.
We don't need these anymore since the same classes exist in lucene today.
This also removes the guice injection for DeletionPolicy and make them shard private.
</description><key id="108331319">13794</key><summary>Remove ES internal deletion policies in favour of Lucenes implementations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T13:54:24Z</created><updated>2015-09-25T14:13:31Z</updated><resolved>2015-09-25T14:12:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-25T14:03:36Z" id="143232628">Wow :)  LGTM, I just left one nano comment.
</comment><comment author="jpountz" created="2015-09-25T14:03:42Z" id="143232662">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cannot contact master (trying on 127.0.0.1?)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13793</link><project id="" key="" /><description>I have a cluster of servers which were migrated from 1.7 to 2.0.0-beta2. 

The first server (`eu4`) started correctly.
The second server (`eu3`) does not start, it looks like it is trying to join the cluster by querying the master on `127.0.0.1`:

```
[2015-09-25 15:32:18,855][INFO ][org.elasticsearch.discovery.zen] [eu3] failed to send join request to master [{eu4}{9EHVwkenTCCM0Fkc27J-qQ}{127.0.0.1}{127.0.0.1:9300}{master=true}], reason [RemoteTransportException[[eu3][127.0.0.1:9300][internal:discovery/zen/join]]; nested: IllegalStateException[Node [{eu3}{d8S6fBqjSg2XDEkaksvyjA}{127.0.0.1}{127.0.0.1:9300}{master=true}] not master for join request]; ]
```

Both servers are started with a default binding (`0.0.0.0`). The first one (which starts fine) is reachable on his IP (as well as on `127.0.0.0`). Both are masters, and both are advertised as such as such in the configuration.

This setup worked fine in 1.7
</description><key id="108328630">13793</key><summary>cannot contact master (trying on 127.0.0.1?)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wsw70</reporter><labels /><created>2015-09-25T13:38:15Z</created><updated>2015-09-25T13:49:24Z</updated><resolved>2015-09-25T13:49:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T13:49:24Z" id="143229348">@wsw70 Please read the breaking changes in 2.0 docs, specifically the [networking changes](https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_network_changes.html)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure equivalent geohashCellQueries are equal after toQuery called</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13792</link><project id="" key="" /><description>Previous to this change if to equal geohash cell query builders were created and then toQuery was called on one, they would no longer be equal.

This change also adds a test to AbstractQueryTestCase to make sure calling toQuery on any query builder does not affect the query builder's equality
</description><key id="108327283">13792</key><summary>Make sure equivalent geohashCellQueries are equal after toQuery called</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>bug</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T13:31:50Z</created><updated>2015-09-29T12:35:58Z</updated><resolved>2015-09-29T12:35:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-25T13:35:24Z" id="143225565">LGTM
</comment><comment author="s1monw" created="2015-09-29T12:26:12Z" id="144044075">LGTM
</comment><comment author="javanna" created="2015-09-29T12:28:22Z" id="144044449">looks great thanks a lot
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename the RenderSearchTemplateAction to be a cluster level action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13791</link><project id="" key="" /><description>The RenderSearchTemplateAction is currently a "indices" action, but the action does not really apply to
indices, it is more of a general action that is used to validate the search template. With our current
categorization of actions, `cluster:` and `indices:`, `cluster:` is a more appropriate type as this action
is not associated with indices. The classes are also moved to a cluster package.
</description><key id="108323986">13791</key><summary>Rename the RenderSearchTemplateAction to be a cluster level action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>non-issue</label><label>review</label><label>v2.0.0</label></labels><created>2015-09-25T13:14:46Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-08T12:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T13:57:04Z" id="143231162">@colings86 thoughts?
</comment><comment author="colings86" created="2015-09-29T13:56:23Z" id="144068489">Sounds good to me
</comment><comment author="s1monw" created="2015-10-08T08:47:55Z" id="146461064">@jaymode can I merge this?
</comment><comment author="s1monw" created="2015-10-08T12:07:28Z" id="146518118">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create a script designed to fill Elasticsearch's heap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13790</link><project id="" key="" /><description>Adds a script that spins up an elasticsearch instance and attempts to crash it by filling it "too full". Once its filled the script dumps the heap for the hung elasticsearch and it can then be analyzed with MAT or whatever your favorite tools are.

To fill Elasticsearch too full of Lucene index stuff:

``` shell
./dev-tools/many_indexes.sh -y 100000
```

To fill elasticsearch too full of unassigned shards:

``` shell
./dev-tools/many_indexes.sh -n -s 1000 -r 10
```
</description><key id="108320682">13790</key><summary>Create a script designed to fill Elasticsearch's heap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>WIP</label></labels><created>2015-09-25T12:55:23Z</created><updated>2016-03-08T13:12:52Z</updated><resolved>2016-03-08T13:12:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2015-09-25T14:05:41Z" id="143233099">From our meeting earlier, some more potentially useful "abusive" behaviors to test:
- Many aliases
- Many fields and/or deeply nested fields (probably equivalent once it is decoded into the cluster state?)
- Indices with many shards, instead of many indices with a few shards?  Not sure if that would be any different
- Index templates, dynamic templates
</comment><comment author="jpountz" created="2015-10-09T14:41:33Z" id="146891590">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename `_doc` to `_any`?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13789</link><project id="" key="" /><description>This sort option has no practical interest besides being the fastest sort option. Renaming to `_any` would allow to have more flexibility in the future and eg. return shards sequentially instead of in parallel, ...
</description><key id="108317533">13789</key><summary>Rename `_doc` to `_any`?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-09-25T12:36:49Z</created><updated>2016-01-28T18:14:11Z</updated><resolved>2016-01-28T18:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T13:46:04Z" id="143228508">What about `_disable` or `_disabled`?
</comment><comment author="jpountz" created="2015-09-25T14:04:22Z" id="143232844">I'm good with it.
</comment><comment author="clintongormley" created="2016-01-28T18:14:11Z" id="176315218">Actually, I'm good with `_doc` - let's leave this as is.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: split parse phase into fromXContent and toQuery for all queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13788</link><project id="" key="" /><description>This PR includes the work that was done in the query-refactoring branch in the last few months. All queries were refactored to split the current parse phase into tho phases:
1) `fromXContent` that returns an intermediate elasticsearch object which can be streamed over the wire
2) `toQuery` that converts the intermediate elasticsearch object into the corresponding lucene query

The first phase can be moved to the coordinating node, while the second phase will stay on the data node.

All queries have been refactored, plus lots of tests have been added for each query. Once this is merged we can start using this new infra and effectively move the parse phase to the coordinating node as the next step.

For all the detailed changes, see pull requests labelled with the [`:Query Refactoring` label](https://github.com/elastic/elasticsearch/pulls?q=is%3Aclosed+is%3Apr+label%3A%22%3AQuery+Refactoring%22).

Relates to #10217.
</description><key id="108317377">13788</key><summary>Query refactoring: split parse phase into fromXContent and toQuery for all queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Search</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T12:35:41Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-09-25T12:41:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-25T12:36:00Z" id="143207877">LGTM LOL
</comment><comment author="javanna" created="2015-09-25T12:59:29Z" id="143215023">Marked this as breaking as it breaks backwards compatibility for plugins and the java api. Changes are included in the migrate guide. Also each PR that introduced breakage is [marked as breaking](https://github.com/elastic/elasticsearch/pulls?q=is%3Apr+label%3A%22%3AQuery+Refactoring%22+is%3Aclosed+label%3Abreaking).
</comment><comment author="dadoonet" created="2015-09-25T13:49:08Z" id="143229304">@javanna do you plan to also change documentation at https://github.com/elastic/elasticsearch/tree/master/docs/java-api/query-dsl ?
</comment><comment author="javanna" created="2015-09-25T15:34:23Z" id="143253619">yea we will need to do that, thanks for the reminder @dadoonet . opened #13800
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java api: remove TermsLookupQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13787</link><project id="" key="" /><description>TermsLookupQueryBuilder was left around only for bw comp reasons, but TermsQueryBuilder is its replacement. We can remove it now that it is clear query refactoring goes in master (3.0).
</description><key id="108294759">13787</key><summary>Java api: remove TermsLookupQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>breaking</label><label>review</label></labels><created>2015-09-25T09:45:27Z</created><updated>2015-09-25T10:06:33Z</updated><resolved>2015-09-25T10:06:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-25T09:52:02Z" id="143174863">LGTM, theres also still a shortcut in QueryBuilders#termsLookupQuery(), anybody using that should notive the builder is gone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move ShardTermVectorService to be on indices level as TermVectorService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13786</link><project id="" key="" /><description>There is no need to have term vectors service on the shard level where it's
created for every shard. This commit moves it to a higher level which makes
shard creation slightly simpler and reduces the number of long living objects.
</description><key id="108286618">13786</key><summary>Move ShardTermVectorService to be on indices level as TermVectorService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T08:46:50Z</created><updated>2015-09-25T09:03:14Z</updated><resolved>2015-09-25T09:03:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-25T08:54:39Z" id="143163950">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic search 2.0.0-beta2  zip package does not start on ubuntu</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13785</link><project id="" key="" /><description>I'm running several installations of elasticsearch 1.X on ubuntu laptop without any issues, that installatios are made from zip. I downloaded version and when I try to run  I found this error.

Exception in thread "main" java.lang.IllegalStateException: failed to load bundle [] due to jar hell
Likely root cause: java.security.AccessControlException: access denied ("java.io.FilePermission" "/usr/share/java/jayatanaag.jar" "read")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:372)
    at java.security.AccessController.checkPermission(AccessController.java:559)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:206)
    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:145)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:154)
    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:91)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:116)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:345)
    at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:112)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:144)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:266)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.

Jayatana is installed by default on ubuntu to support global menu.
</description><key id="108286079">13785</key><summary>Elastic search 2.0.0-beta2  zip package does not start on ubuntu</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">silvestrelosada</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.0.0-rc1</label></labels><created>2015-09-25T08:42:00Z</created><updated>2015-10-01T11:17:56Z</updated><resolved>2015-09-26T03:46:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-25T08:57:27Z" id="143164378">I think we should unset JAVA_TOOL_OPTIONS in our startup script...
</comment><comment author="silvestrelosada" created="2015-09-25T09:00:09Z" id="143164795">If I disabe jayatana on my ubuntu seems to work well
</comment><comment author="rmuir" created="2015-09-25T09:00:43Z" id="143164883">Yeah, I think we should just do it for you so people don't experience problems, and don't need to disable it (e.g. we disable it for elasticsearch, because we don't want it)
</comment><comment author="s1monw" created="2015-09-25T09:05:34Z" id="143165596">+1 @rmuir 
</comment><comment author="rmuir" created="2015-09-25T09:05:37Z" id="143165607">And warn if it contains anything. Basically saying, hey look, we are ignoring this stuff, if you want to pass stuff to java, use JAVA_OPTS. That way there are no surprises.
</comment><comment author="rmuir" created="2015-09-26T03:47:02Z" id="143395035">Thank you for testing the beta and reporting this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ByteSizeValue.equals should normalize units</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13784</link><project id="" key="" /><description>currently ByteSizeValue.parse("1GB") is not equal to ByteSizeValue.parse("1024MB")
</description><key id="108274113">13784</key><summary>ByteSizeValue.equals should normalize units</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Settings</label><label>bug</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T06:55:00Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-25T18:09:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-25T06:55:20Z" id="143144818">@javanna can you take a look please?
</comment><comment author="bleskes" created="2015-09-25T10:59:48Z" id="143187384">@jpountz pushed another commit.
</comment><comment author="jpountz" created="2015-09-25T11:10:41Z" id="143188857">LGTM
</comment><comment author="bleskes" created="2015-09-25T11:12:04Z" id="143189039">thx @jpountz are you +1 on pushing this to 2.0 as well?
</comment><comment author="jpountz" created="2015-09-25T11:17:36Z" id="143189727">It looks like a low-risk fix, so 2.0 works for me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds disk used by indices to _cat/allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13783</link><project id="" key="" /><description>This PR adds a column to _cat/allocation with the disk space used by ES indices in the node.

Closes #13529 
</description><key id="108253436">13783</key><summary>Adds disk used by indices to _cat/allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrestc</reporter><labels><label>:CAT API</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-25T03:06:58Z</created><updated>2015-10-01T18:47:27Z</updated><resolved>2015-10-01T18:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrestc" created="2015-09-29T20:20:08Z" id="144180437">@xuzha thanks for the comment. I`ve update this with your suggestion
</comment><comment author="xuzha" created="2015-09-30T22:50:32Z" id="144567893">@andre Sorry for the delay. @bleskes could you also take a look ? I think this is good. 
</comment><comment author="bleskes" created="2015-10-01T06:30:36Z" id="144633090">LGTM. Thx @andrestc  . @uxha will you merge this in? I think it can go into 2.1, 2.2 and 3.0 (can you please tag it appropriately? )
</comment><comment author="xuzha" created="2015-10-01T06:33:44Z" id="144633427">@bleskes thanks, will do. @andrestc could you rebase and squash the PR ?
</comment><comment author="andrestc" created="2015-10-01T11:19:02Z" id="144700293">@xuzha done!
</comment><comment author="xuzha" created="2015-10-01T18:47:27Z" id="144814209">Thanks @andrestc for the PR,  merged ;-)

And back ported to 2.1 and 2.x
2.1 https://github.com/elastic/elasticsearch/commit/f28cc8896c09c4d168c099786d5c8475eea7a573
2.X https://github.com/elastic/elasticsearch/commit/c69d6e9fe0f29fea69560d32599dc7eb2014f130
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't log multi-megabyte guice exceptions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13782</link><project id="" key="" /><description>Instead just log the same thing we print to the startup console for that case (magic logic),
it sucks to do this, but guice exceptions are too much.

All other non-guice exceptions will still be fully logged.

cc: @drewr 
</description><key id="108222986">13782</key><summary>Don't log multi-megabyte guice exceptions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Logging</label><label>bug</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T22:05:20Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-25T19:02:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2015-09-24T22:07:42Z" id="143062904">:+1: 
</comment><comment author="rmuir" created="2015-09-24T22:12:03Z" id="143063660">For something like a bad network host, i get a 2KB log (the same magic logic going to console) instead of a 4MB one:

```
015-09-24 18:10:50,968][INFO ][node                     ] [Vapor] version[3.0.0-SNAPSHOT], pid[21417], build[a14a525/2015-09-24T22:08:16Z]
[2015-09-24 18:10:50,969][INFO ][node                     ] [Vapor] initializing ...
[2015-09-24 18:10:51,047][INFO ][plugins                  ] [Vapor] loaded [], sites []
[2015-09-24 18:10:51,060][INFO ][env                      ] [Vapor] using [1] data paths, mounts [[/ (/dev/disk/by-uuid/1b3a9e9c-3d5f-412e-be52-a7e64433e25d)]], net usable_space [154.1gb], net total_space [229.1gb], spins? [no], types [ext4]
[2015-09-24 18:10:52,151][ERROR][bootstrap                ] Guice Exception: java.lang.IllegalArgumentException: Failed to resolve address for [_bogus_]
Likely root cause: java.net.UnknownHostException: _bogus_: unknown error
        at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
        at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:907)
        at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1302)
        at java.net.InetAddress.getAllByName0(InetAddress.java:1255)
        at java.net.InetAddress.getAllByName(InetAddress.java:1171)
        at java.net.InetAddress.getAllByName(InetAddress.java:1105)
        at org.elasticsearch.transport.netty.NettyTransport.parse(NettyTransport.java:708)
        at org.elasticsearch.transport.netty.NettyTransport.addressesFromString(NettyTransport.java:660)
        at org.elasticsearch.transport.TransportService.addressesFromString(TransportService.java:399)
        at org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.&lt;init&gt;(UnicastZenPing.java:160)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
        at &lt;&lt;&lt;guice&gt;&gt;&gt;
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:201)
        at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:128)
        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:177)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:277)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

I am the first to be against hiding exceptions in any way, but 4MB is over the top anywhere.
</comment><comment author="rmuir" created="2015-09-24T22:24:58Z" id="143065997">I'm setting this as a bug for 2.0, because I think it is too important. Feel free to uncheck that if you think it is wrong to do.
</comment><comment author="drewr" created="2015-09-24T22:27:21Z" id="143066325">I think it's fairly critical as the experience for the operator wasn't great, and 2.0 is all about fixing that.
</comment><comment author="rjernst" created="2015-09-24T22:32:12Z" id="143067116">LGTM
</comment><comment author="rmuir" created="2015-09-25T15:40:14Z" id="143255329">Does anyone else have feelings/concerns on this? I know i do, about hiding exception stuff that may be important. That is why I always hate hiding any part of an exception/stacktrace in any way.

But 4MB is crazy, the way I see it, there has to be a line somewhere.
</comment><comment author="jpountz" created="2015-09-25T15:42:04Z" id="143255990">+1
</comment><comment author="mikemccand" created="2015-09-25T15:53:11Z" id="143259595">LGTM
</comment><comment author="sherry-ger" created="2015-11-19T01:53:01Z" id="157922420">This still seems to be an issue in ES 2.0.  It generated 40 GB of logs with messages like the following.

```
[2015-11-17 17:00:07,435][WARN ][org.elasticsearch        ] Exception cause unwrapping ran for 10 levels...
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested:
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
 ...
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: NullPointerException;
Caused by: RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-2][10.1.108.103:9300][indices:data/write/bulk[s]]]; nested: 
RemoteTransportException[[mon-esd-3][10.1.108.111:9300][indices:data/write/bulk[s]]]; nested: 
```
</comment><comment author="rmuir" created="2015-11-19T02:58:44Z" id="157933850">That is because this is not a fix. The fix is to remove guice.
</comment><comment author="bleskes" created="2015-11-19T08:53:00Z" id="157992567">@sherry-ger I wonder whether this is guice driven - do you have the bottom of the stack trace / something else? do you  know where is that NullPointerException coming from?
</comment><comment author="sherry-ger" created="2015-11-19T17:52:55Z" id="158135680">@bleskes No, this probably is not guice driven.  This is more akin to https://github.com/elastic/elasticsearch/issues/4639, which seems to suggest this feature may have  resolved it.  I do have the rest of the stack trace.  I will post this to the right issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix typo "Seting" to "Setting"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13781</link><project id="" key="" /><description>I found a small misspelling in doc.
How about this?
</description><key id="108220425">13781</key><summary>fix typo "Seting" to "Setting"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KimTaehee</reporter><labels><label>docs</label></labels><created>2015-09-24T21:46:37Z</created><updated>2015-09-25T13:28:24Z</updated><resolved>2015-09-25T13:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T13:28:13Z" id="143222685">thanks @KimTaehee - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a way to disable allocation awareness for specific indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13780</link><project id="" key="" /><description>Details on the use case is documented in https://github.com/elastic/elasticsearch/issues/12431.  Providing an option to disable allocation awareness at the index level will help address such use case.
</description><key id="108209189">13780</key><summary>Provide a way to disable allocation awareness for specific indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:Allocation</label><label>discuss</label><label>enhancement</label></labels><created>2015-09-24T20:35:26Z</created><updated>2015-09-25T13:26:35Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add support for secondary azure storage account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13779</link><project id="" key="" /><description>In #13228, @craigwi proposed a PR to support multiple storage accounts and secondary endpoints (which are readonly).

As per discussion on the thread I started to revisit the PR.

This PR rebased @craigwi work on master and modify the way you define secondary storage account.

In `elasticsearch.yml`:

``` yml
cloud:
    azure:
        storage:
            my_account1:
                account: your_azure_storage_account1
                key: your_azure_storage_key1
                default: true
            my_account2:
                account: your_azure_storage_account2
                key: your_azure_storage_key2
```

When creating a repository, you can choose which azure account you want to use for it:

``` sh
# This one will use the default account (my_account1)
curl -XPUT localhost:9200/_snapshot/my_backup1?pretty -d '{
  "type": "azure"
}'

# This one will use the second account (it will make it readonly)
curl -XPUT localhost:9200/_snapshot/my_backup2?pretty -d '{
  "type": "azure",
  "settings": { 
    "account" : "my_account2",
    "location_mode": "secondary_only"
  }
}'
```

@craigwi I ran some manual tests and it seems to work as expected but I'd love if you could confirm that.
Do you think you can build my branch and run a test on your side?

I don't think it's totally ready for a review though (I want at least to squash/reword some of commits).
</description><key id="108206635">13779</key><summary>Add support for secondary azure storage account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T20:19:55Z</created><updated>2015-11-18T16:03:48Z</updated><resolved>2015-11-18T15:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="craigwi" created="2015-09-25T17:12:44Z" id="143290745">Thanks @dadoonet.  I'll review and get back to you shortly.
</comment><comment author="dadoonet" created="2015-10-12T10:38:03Z" id="147362648">@craigwi FYI I added a new commit and rebased on master.
</comment><comment author="dadoonet" created="2015-10-27T14:32:52Z" id="151518661">@imotov Do you think you could review this PR?
</comment><comment author="craigwi" created="2015-10-27T16:18:42Z" id="151556153">Thanks @dadoonet; as mentioned in a separate note, I have reviewed this and we are good to go.
</comment><comment author="imotov" created="2015-11-10T14:37:46Z" id="155436918">Left one minor comment. The snapshot/restore side looks good to me.
</comment><comment author="dadoonet" created="2015-11-18T16:03:47Z" id="157760259">Merged also in 2.x branch with 04b51d4fb3230bf928ec95207e0d260bd41dd312
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Force Merge API, deprecate Optimize API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13778</link><project id="" key="" /><description>This adds an API for force merging lucene segments. The `/_optimize` API is now
deprecated and replaced by the `/_forcemerge` API, which has all the same flags
and action, just a different name.

This is intended to be merged to both 3.x (master) and 2.1.0, and then I will follow it up with an additional PR to remove `/_optimize` from 3.x only.
</description><key id="108203395">13778</key><summary>Add Force Merge API, deprecate Optimize API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Index APIs</label><label>breaking</label><label>deprecation</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T20:03:36Z</created><updated>2016-02-29T16:38:09Z</updated><resolved>2015-10-21T03:48:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-24T21:02:30Z" id="143050231">@nik9000 addressed your comments
</comment><comment author="mikemccand" created="2015-09-24T21:19:34Z" id="143053822">LGTM, thanks @dakrone.
</comment><comment author="s1monw" created="2015-09-25T07:16:42Z" id="143147291">@dakrone why don't we just rename existing classes and add delegating methods to the client using empty subclasses with different name? I also would not worry about java API and just make REST API backwards compatible?
</comment><comment author="dakrone" created="2015-09-25T15:46:54Z" id="143258226">@s1monw I did it just to maintain backwards compatibility with the Java API, if we're okay with breaking it for 2.1.0 I can do what you mentioned now.
</comment><comment author="dakrone" created="2015-09-28T10:42:29Z" id="143705236">@s1monw pushed changes to this to remove the optimize Java bits (leaving the rest/documentation bits, which I'll remove in a subsequent PR)
</comment><comment author="s1monw" created="2015-10-17T19:10:51Z" id="148944823">LGTM
</comment><comment author="javanna" created="2015-10-21T08:30:19Z" id="149816659">I see that this has gone to 2.1 and 2.x too, meaning that we removed optimize from the java api without deprecating it first. I think we could have been kinder to our users here to be honest, like deprecate in 2.1 and remove in 2.x or master. That said I guess compile errors will tell java api users what to do...
</comment><comment author="s1monw" created="2015-10-21T09:06:57Z" id="149828327">@javanna that's the way to go on the java end... the overhead is minor since you only change a couple of line AND the compiler forces you which is awesome!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move ShardPercolateService creation into IndexShard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13777</link><project id="" key="" /><description>This commit removes an unnecessarily guice injected service and
simply creates an instance where it's needed using plain old java ctors
</description><key id="108198125">13777</key><summary>Move ShardPercolateService creation into IndexShard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T19:32:08Z</created><updated>2015-09-25T08:51:54Z</updated><resolved>2015-09-25T08:51:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-24T19:59:33Z" id="143033165">LGTM. Long term we should probably clean up the naming of these "services" because I know some implement the lifecycle component, and others not. I double checked and this one does not, so it does not need the lifecycle handling our injection code does. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`repositories.azure.base_path` is not used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13776</link><project id="" key="" /><description>When looking at azure code, `repositories.azure.base_path` is actually never read although we said it is in documentation.

See also https://github.com/elastic/elasticsearch-cloud-azure/issues/98
</description><key id="108197581">13776</key><summary>`repositories.azure.base_path` is not used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>bug</label></labels><created>2015-09-24T19:28:58Z</created><updated>2015-12-29T09:45:10Z</updated><resolved>2015-12-29T09:44:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Custom plugin can no longer access parseObject method in 2.0 beta</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13775</link><project id="" key="" /><description>While updating our elasticsearch plugin to support the upcoming 2.0 release, we noticed that the parseObject method which used to be publicly accessible, has become package-private in a package-private class (DocumentParser.java) See #10802. 

Any reason this can't be public anymore?
</description><key id="108185531">13775</key><summary>Custom plugin can no longer access parseObject method in 2.0 beta</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bwsawyer</reporter><labels><label>:Plugins</label><label>discuss</label></labels><created>2015-09-24T18:18:10Z</created><updated>2016-01-28T18:11:37Z</updated><resolved>2016-01-28T18:11:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T13:03:06Z" id="143215657">@rjernst any ideas?
</comment><comment author="rjernst" created="2015-09-25T14:12:05Z" id="143234618">I would need to know more about what the plugin is trying to do. @bwsawyer can you explain your intended use of parseObject?
</comment><comment author="bwsawyer" created="2015-09-25T14:20:12Z" id="143236390">The plugin adds fuzzy name matching (for more info see the slides posted here http://www.meetup.com/Elasticsearch-Boston/events/222682277/)
We have a custom Mapper that generates additional fields based on the indexed name string.  These get indexed as a nested object. Currently in 1.x this is a simple matter of creating an ObjectMapper and calling parse, but in 2.0 this no longer seems possible.
</comment><comment author="rjernst" created="2015-11-09T02:06:59Z" id="154902567">&gt; a simple matter of creating an ObjectMapper
&gt; @bwsawyer Could you elaborate on this? Were you somehow overriding ObjectMapper? 

Part of the refactorings were to decouple parsing for a single field type, from parsing the entire document. Before, parsing the document was mixed back and forth between "object" parsing, and concrete fields. This decoupling meant moving part of ObjectMapper to a new class, DocumentParser, which was dedicated to parsing the actual document.

While there is still work left to do with mappings, have you considered moving this logic out of mappings altogether? This seems like something that belongs more at the document construction before sending to elasticsearch, or perhaps with a custom sub-plugin in the future of the upcoming ingest plugin (#14049).
</comment><comment author="clintongormley" created="2016-01-28T18:11:36Z" id="176313101">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>testDeserializeCorruptionException fails with java9 jigsaw build</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13774</link><project id="" key="" /><description>I added assume()'s for these problems before, but new ones are creeping in. We should try to plan for the situation. The basics are that StackTraceElement gets a new member with java 9 jigsaw, the module, and we (obviously) lose it during serialization, this causes the current testing scheme to fail: note that `java.base@9.0` that gets lost. Can we test it better so we don't have this issue?

```
Suite: org.elasticsearch.index.store.StoreTests
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=DAD1B0AD9C20C7 -Dtests.class=org.elasticsearch.index.store.StoreTests -Dtests.method="testDeserializeCorruptionException" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=hu_HU -Dtests.timezone=America/Atka
FAILURE 0.06s J1 | StoreTests.testDeserializeCorruptionException &lt;&lt;&lt;
   &gt; Throwable #1: org.junit.ComparisonFailure: expected:&lt;...ccessorImpl.invoke0([java.base@9.0/Native Method)
   &gt;    at java.lang.Thread.run(java.base@9.0/]Thread.java:746)
   &gt; &gt; but was:&lt;...ccessorImpl.invoke0([Native Method)
   &gt;    at java.lang.Thread.run(]Thread.java:746)
   &gt; &gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([DAD1B0AD9C20C7:A8459C7FBBB8F26E]:0)
   &gt;    at org.elasticsearch.index.store.StoreTests.testDeserializeCorruptionException(StoreTests.java:1295)
   &gt;    at java.lang.Thread.run(java.base@9.0/Thread.java:746)
```
</description><key id="108165613">13774</key><summary>testDeserializeCorruptionException fails with java9 jigsaw build</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>adoptme</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T16:26:34Z</created><updated>2015-11-17T14:36:14Z</updated><resolved>2015-11-17T14:36:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-17T14:36:14Z" id="157387514">Closed by https://github.com/elastic/elasticsearch/pull/14723
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>warning when creating an azure repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13773</link><project id="" key="" /><description>When registering a new `azure` repository, we get this message:

```
[2015-09-24 18:14:27,892][INFO ][org.elasticsearch.repositories] [azure1] put repository [my_backup1]
[2015-09-24 18:14:29,684][WARN ][org.elasticsearch.cloud.azure.blobstore] [azure1] can not remove [tests-iAT5HBuQQkeNosa6OzgTOw/] in container {elasticsearch-snapshots}: The specified blob does not exist.
```

Actually, files are created:
- https://account.blob.core.windows.net/elasticsearch-snapshots/tests-iAT5HBuQQkeNosa6OzgTOw/data-9WVhbEsnQum700y9u0IVcA.dat
- https://account.blob.core.windows.net/elasticsearch-snapshots/tests-iAT5HBuQQkeNosa6OzgTOw/master.dat

Tested on 2.0.0-beta2
</description><key id="108165009">13773</key><summary>warning when creating an azure repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>adoptme</label><label>bug</label></labels><created>2015-09-24T16:23:25Z</created><updated>2016-05-03T13:47:42Z</updated><resolved>2016-05-03T13:47:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-05-03T13:47:42Z" id="216532454">This issue was happening in 2.0.0-beta2 and does not have been reported since then.
Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove option to configure custom config file via CONF_FILE or -Des.default.conf</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13772</link><project id="" key="" /><description>It is rarely used and was not consistently handled by different distributions anyway.
This commit also adds a test for specifying CONF_DIR when installing plugins and
starting elasticsearch.

relates to #12712  and #12954
closes #5329
closes #13715

Would be great if someone with a windows machine could test the bat file, I unfortunately can't.
</description><key id="108162906">13772</key><summary>Remove option to configure custom config file via CONF_FILE or -Des.default.conf</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label><label>breaking</label><label>v2.0.0</label></labels><created>2015-09-24T16:10:51Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-07T08:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-24T18:12:12Z" id="143008229">LGTM, I left a couple very minor comments.
</comment><comment author="nik9000" created="2015-09-24T18:20:31Z" id="143010464">I also left some pretty minor comments.
</comment><comment author="electrical" created="2015-09-24T21:31:41Z" id="143056382">Where does the systemd pre-exec file come from ?

edit: ignore me; found it now. seems i missed it when browsing on my mobile.
</comment><comment author="brwe" created="2015-09-25T09:36:57Z" id="143171991">@nik9000 @rjernst thanks for the review! addressed all comments.
@electrical I added the file to check if CONF_FILE was defined and not start elasticsearch if so. I don't know systemd too well so if there is another way to have this check before we start elasticsearch I'd be happy to change that.
</comment><comment author="brwe" created="2015-09-25T16:40:22Z" id="143272408">Chatted with @spinscale who wanted the check for `-Des` params to be moved from bin/elasticsearch script to Bootstrap. I added a commit for that too. I [reordered logging conf initialization](https://github.com/elastic/elasticsearch/pull/13772/files#diff-40fe9ad3b9a0769a26fe81051263be3dR240) so that we get a proper message, not sure though if this is problematic?
</comment><comment author="brwe" created="2015-09-29T15:39:46Z" id="144097447">@rjernst or @nik9000 or @spinscale  can you take another look?
</comment><comment author="brwe" created="2015-09-29T15:46:57Z" id="144099388">Forgot to add that I tried out the service.bat on windows now too. Thanks @dliappis for the help!
</comment><comment author="nik9000" created="2015-10-06T13:32:11Z" id="145856805">LGTM
</comment><comment author="brwe" created="2015-10-06T18:41:27Z" id="145959697">@nik9000 addressed all comments. I made the timeout now dependent on if we run in background or not but my bash is that of a 3 year old so if you have any suggestion to improve readability don't hold back.
</comment><comment author="nik9000" created="2015-10-06T19:12:32Z" id="145968996">LGTM
</comment><comment author="bstascavage" created="2015-11-13T16:52:53Z" id="156485818">This was actually a feature we used and this required me to re-architecture our cluster deployment.  I have no idea why, in 2015, you would remove the ability to specify a config file, and instead for the user to have a config file be a specific filename.  Obviously the damage is done, but I would dispute the claim that it is 'It is rarely used'
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard rebalancing too aggressive when adding new nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13771</link><project id="" key="" /><description>Cluster:
- 3 dedicated masters
- 2 client nodes
- 65 data nodes, each running 2 instances of ES
- ES version 1.5.2, but tests on a smaller 1.7 cluster seem to behave the same way
- Shard balancing based on disk usage
- 64 indices, 1757 primary shards, 1757 replica shards (3514 shards total)

Upon adding a new node to the cluster, the expectation is that this node will get populated quickly.
In reality, we end up with shards rebalancing across the entire cluster.

Example:
I added `logdb41` to the cluster today, and the result was primary shards getting relocated and replicas being rebuilt everywhere, and very few going to the new node at any one time:

```
index               shard time   type       stage source_host          target_host
logstash-2015.08.29 26    268206 relocation index logdb74              logdb52
logstash-2015.08.23 36    268138 replica    index logdb70              logdb74
logstash-2015.08.23 15    268340 replica    index logdb26              logdb38
logstash-2015.08.25 15    267976 replica    index logdb39              logdb26
logstash-2015.08.25 28    268094 replica    index logdb32              logdb27
logstash-2015.09.08 6     245140 replica    index logdb48              logdb41
logstash-2015.08.25 12    268262 relocation index logdb40              logdb65
logstash-2015.09.19 25    264782 relocation index logdb65              logdb41
```

Does the rebalancing algorithm try to get disk usage as even as possible across the entire cluster?
Or does it allow some amount of variance between nodes?
It feels like the cluster is coming up with a new map of where all the shards should end up, but instead of initially putting as many shards as possible on the new node, it's prioiritising the rebalancing some other way.
This seems less efficient in terms of production operations. Do all these shards really need to move?

Thanks!
</description><key id="108161770">13771</key><summary>Shard rebalancing too aggressive when adding new nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>:Allocation</label><label>feedback_needed</label></labels><created>2015-09-24T16:04:27Z</created><updated>2016-01-28T18:10:53Z</updated><resolved>2016-01-28T18:10:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-24T16:14:41Z" id="142976301">&gt; Does the rebalancing algorithm try to get disk usage as even as possible across the entire cluster?

No, disk is actually not a factor in the "weight" of a node+shard combination. The only thing disk is used for is a "yes/no" for whether the shard can be allocated based on the watermarks.

&gt; It feels like the cluster is coming up with a new map of where all the shards should end up, but instead of initially putting as many shards as possible on the new node, it's prioiritising the rebalancing some other way.

This is likely because the `BalancedShardsAllocator` has determined a new, better, weight for other nodes since new nodes have been added. Basically, it tries to give weight to a node for a shard given the following criteria:
- _index balance_ meaning that the shards for a particular index are distributed more evenly (instead of clustered around a small number of nodes)
- _shard balance_ meaning that the number of shards is roughly equivalent across all the nodes (when possible with regard to the allocation deciders)

The `index` factor defaults to 0.55 and the `shard` factor defaults to 0.45 (for their relative importance).

However, in your case, since you want the rebalancing to not be as aggressive, you could consider the `cluster.routing.allocation.balance.threshold` setting (defaults to 1.0f), which means that the weight for the evaluated node must be above this for a relocation to be performed. If you raise it, relocations will occur less frequently at the potential cost of a more unbalanced (with regard to the two weights I mentioned above) cluster.
</comment><comment author="clintongormley" created="2016-01-28T18:10:53Z" id="176312508">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>azure repository type is not registered anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13770</link><project id="" key="" /><description>I'm testing on master branch (3.0.0-SNAPSHOT).
- Install repository-azure plugin
- start elasticsearch
- create a repository

``` sh
curl -XPUT localhost:9200/_snapshot/my_backup1?pretty -d '
{
    "type": "azure"
}
'
```

It gives:

``` json
{
  "error" : {
    "root_cause" : [ {
      "type" : "repository_exception",
      "reason" : "[my_backup1] failed to create repository"
    } ],
    "type" : "repository_exception",
    "reason" : "[my_backup1] failed to create repository",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "Unknown [repository] type [azure]"
    }
  },
  "status" : 500
}
```
</description><key id="108161128">13770</key><summary>azure repository type is not registered anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository Azure</label></labels><created>2015-09-24T16:01:01Z</created><updated>2015-09-24T16:11:00Z</updated><resolved>2015-09-24T16:10:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-24T16:07:40Z" id="142974650">Just tested with 2.0.0-beta2 and got the same error so I'm marking this as a blocker for 2.0 (cc @clintongormley) 
</comment><comment author="dadoonet" created="2015-09-24T16:10:47Z" id="142975396">False alarm. I did a mistake in my configuration. Closing. Sorry for the noise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: last //norelease addressed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13769</link><project id="" key="" /><description>Some methods had default implementation while queries were going to be refactored, now that they are all refactored all those methods can be made abstract.
</description><key id="108158152">13769</key><summary>Query refactoring: last //norelease addressed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-24T15:45:41Z</created><updated>2015-09-24T16:13:39Z</updated><resolved>2015-09-24T16:13:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-24T15:57:39Z" id="142972204">LGTM
</comment><comment author="javanna" created="2015-09-24T16:13:39Z" id="142976046">Merged db9c2796b34943d62df46074c85e8f6acf48f79e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: minor cleanups before we merge into master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13768</link><project id="" key="" /><description>Remove ParseFieldMatcher duplication query in both contexts. QueryParseContext is still contained in QueryShardContext, as parsing still happens in the shards here and there. Most of the norelease comments have been removed simply because the scope of the refactoring has become smaller. Some could only be removed once everything, the whole search request, gets parsed on the coordinating node. We will get there eventually.
</description><key id="108153606">13768</key><summary>Query refactoring: minor cleanups before we merge into master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-24T15:23:30Z</created><updated>2015-09-24T16:01:33Z</updated><resolved>2015-09-24T16:01:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-24T15:51:27Z" id="142970275">LGTM
</comment><comment author="javanna" created="2015-09-24T16:01:33Z" id="142973249">Merged aae7faa88e55db81d3fa61a6fdb91fe14558efbd
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch doesn't provide any query by which you can get distinct documents based on a field value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13767</link><project id="" key="" /><description>There is no query in ElasticSearch by which we can remove some documents from a set of documents which have same key value pair.  
</description><key id="108141892">13767</key><summary>ElasticSearch doesn't provide any query by which you can get distinct documents based on a field value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">temaniarpit27</reporter><labels /><created>2015-09-24T14:26:37Z</created><updated>2015-09-25T11:54:21Z</updated><resolved>2015-09-25T11:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T11:54:21Z" id="143194931">There are many many definitions of what is considered to be a duplicate document.  You could use a top_hits aggregation to find duplicates, or perhaps a more-like-this query, or the [carrot2](https://github.com/carrot2/elasticsearch-carrot2) or [Entity Resolution](https://github.com/YannBrrd/elasticsearch-entity-resolution) plugins
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor StoreRecoveryService to be a simple package private util class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13766</link><project id="" key="" /><description>StoreRecoveryService used to be a pretty heavy class with lots of dependencies.
This class was basically not testable in isolation and had an async API with a listener.
This commit refactors this class to be a simple utility classs with a sync API hidden behind
the IndexShard interface. It includes single node tests and moves all the async properities to
the caller side.
Note, this change also removes the mapping update on master from the store recovery code since
it's not needed anymore in 3.0 because all stores have been subject to sync mapping updates such
that the master already has all the mappings for documents that made it into the transaction log.
</description><key id="108140097">13766</key><summary>Refactor StoreRecoveryService to be a simple package private util class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T14:16:43Z</created><updated>2015-09-29T13:59:47Z</updated><resolved>2015-09-29T13:59:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-24T14:17:16Z" id="142943266">@bleskes if you have time :)
</comment><comment author="dakrone" created="2015-09-29T08:40:20Z" id="143988787">I left mostly cosmetic comments, I know I'm not @bleskes but this LGTM :)
</comment><comment author="dakrone" created="2015-09-29T08:50:27Z" id="143992465">Also, I think this is much cleaner than the previous code, so thanks @s1monw!
</comment><comment author="bleskes" created="2015-09-29T09:12:36Z" id="144001173">Looks good to me because @dakrone said so . I don't need to look ;)

On 29 sep. 2015 10:50 AM +0200, Lee Hinmannotifications@github.com, wrote:

&gt; Also, I think this is much cleaner than the previous code, so thanks@s1monw(https://github.com/s1monw)!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly orview it on GitHub(https://github.com/elastic/elasticsearch/pull/13766#issuecomment-143992465).
</comment><comment author="dakrone" created="2015-09-29T13:51:26Z" id="144067071">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Give vagrant boxes 2gb of ram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13765</link><project id="" key="" /><description>This lets them run our tests without oomkiller getting angry.
</description><key id="108138412">13765</key><summary>Give vagrant boxes 2gb of ram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T14:08:02Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-24T18:03:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-24T14:10:12Z" id="142941695">+1, our vagrant setup is very useful if you want to try out changes on different linux versions. Just needs a bit more ram to be able to e.g. compile the source code and so on :)

I tested this change and it fixes problems i had on trusty.
</comment><comment author="nik9000" created="2015-09-24T17:05:24Z" id="142990496">Any objections, @brwe or @spinscale or @tlrx ?
</comment><comment author="brwe" created="2015-09-24T17:06:57Z" id="142990930">ok for me
</comment><comment author="nik9000" created="2015-09-24T18:22:05Z" id="143010816">Merged to master and cherry-picked to 2.0 and 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docu typo: Fixed parameter value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13764</link><project id="" key="" /><description>Parameter "auto_import_dangled" accepts 'closed' not 'close'
</description><key id="108129994">13764</key><summary>Docu typo: Fixed parameter value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">franklanganke</reporter><labels><label>docs</label></labels><created>2015-09-24T13:29:27Z</created><updated>2015-09-25T11:45:04Z</updated><resolved>2015-09-25T11:44:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="franklanganke" created="2015-09-24T13:38:32Z" id="142929716">Signed CLA.
</comment><comment author="clintongormley" created="2015-09-25T11:45:04Z" id="143193725">thanks @franklanganke - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed scripts can't be used for sorting by using the Java API due to hardcoded parameter name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13763</link><project id="" key="" /><description>ScriptSortBuilder uses the hardcoded String "script" for the script/script_id/file parameter when using a script to sort query responses.
"script" works for both an inline/dynamic script and when referring to a script file name.
"file" works for referring to a script file name.
"script_id" NEEDS to be used when referring to an indexed script.

When using the SortBuilders.scriptSort it uses ScriptSortBuilder which always sets the "script" parameter name, this makes it impossible to use an indexed script as the parameter would have to be "script_id".
</description><key id="108129891">13763</key><summary>Indexed scripts can't be used for sorting by using the Java API due to hardcoded parameter name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mmelitzer</reporter><labels /><created>2015-09-24T13:28:53Z</created><updated>2015-09-25T11:42:19Z</updated><resolved>2015-09-25T11:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T11:42:19Z" id="143193020">Hi @mmelitzer 

This has been fixed in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Map from CodecService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13762</link><project id="" key="" /><description>Replaces an unmodifiableMap in CodecService with a switch. This has the advantage
of saving a few kb per open shard....
</description><key id="108120222">13762</key><summary>Remove Map from CodecService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label></labels><created>2015-09-24T12:24:58Z</created><updated>2016-02-13T12:28:23Z</updated><resolved>2016-02-10T14:46:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-24T17:05:03Z" id="142990334">@jpountz I've pushed something removing `availableCodes` - have a look at the test stuff that I removed. I _think_ it doesn't break anything. There were comments about random selection but they didn't seem to jibe with the code. So I tried blasting the checks in the tests and everything seemed to work - so it looks like `Codec.getDefault().getName()` doesn't have any chance of getting readonly codecs any more.
</comment><comment author="jpountz" created="2015-09-25T13:07:00Z" id="143216321">LGTM

This comment about read-only codecs is weird. Indeed we make old codecs read-only when we move to a new default codec, but write is still supported for tests so it shouldn't be an issue.
</comment><comment author="jpountz" created="2015-09-25T13:11:45Z" id="143217347">Oh sorry I get the problem now, we take the codec name and put it as a setting for the engine, so it will try to look up the codec and get the read-only one. So I think we should keep the logic, just using `Codec.availableCodecs()` instead of `codecService.availableCodecs()`.
</comment><comment author="nik9000" created="2015-09-25T15:59:13Z" id="143260956">@jpountz I added the check back. I'm fighting in the dark on this a bit because I'm not sure when it'd come up. The old way didn't break the tests so far as I could tell so its hard for me to figure out what is going on.
</comment><comment author="jpountz" created="2015-10-09T14:43:10Z" id="146891990">Could it be just because alternative codecs are only picked rarely? Can you try eg. many iterations or by passing "-Dtests.codec=SomeOldCodecName"?
</comment><comment author="nik9000" created="2015-10-15T21:00:23Z" id="148519949">Rebased.

&gt; Could it be just because alternative codecs are only picked rarely? Can you try eg. many iterations or by passing "-Dtests.codec=SomeOldCodecName"?

Not that I can tell. `Codec.getDefault()` can be set to something arbitrary but `-Dtests.codec=` will only set it some a supported codec. I'm fine with committing as is with that code being a bit of a mystery though.
</comment><comment author="s1monw" created="2015-10-17T19:12:40Z" id="148944916">@jpountz can you revisit this?
</comment><comment author="nik9000" created="2015-10-23T13:54:00Z" id="150579171">Addressed an open comment.
</comment><comment author="nik9000" created="2016-02-10T14:46:00Z" id="182403365">Too stale and not a big deal. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not Query: Investigate if we can deprecate short version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13761</link><project id="" key="" /><description>Currently the Not Query can have two version, either directly starting with the inner query or nesting it in an additinal `filter` element:

```
"not" : {
                "range" : {
                    "postDate" : {
                        "from" : "2010-03-01",
                        "to" : "2010-04-01"
                    }
                }
          }
```

or 

```
"not" : {
                "filter" :  {
                    "range" : {
                        "postDate" : {
                            "from" : "2010-03-01",
                            "to" : "2010-04-01"
                        }
                    }
                }
          }
```

The current NotQueryParser supports both versions, but it would be beneficial to be able to remove one version, so this issue is here to discuss whether and how we can deprecate the short version.
</description><key id="108104350">13761</key><summary>Not Query: Investigate if we can deprecate short version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query DSL</label><label>enhancement</label></labels><created>2015-09-24T10:41:10Z</created><updated>2015-10-20T17:44:45Z</updated><resolved>2015-10-20T17:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T10:59:10Z" id="143187304">The only purpose of `not` these days is as a short cut for `bool.must_not`.  I'd keep the short version and remove the long version.
</comment><comment author="jpountz" created="2015-09-27T21:59:38Z" id="143597639">I think we should deprecate all versions of `not`. I remember some people arguing that `not` was important for the query DSL so that you could do things like:

``` java
{
  "query": {
    "filtered": {
      "query": /* some_query */
      "filter": {
        "not": { /* some filter */ }
      }
    }
  }
}
```

However in the meantime we deprecated the `filtered`, `and` and `or` queries and `bool` is now the only way to combine several filters together, so all uses of the `not` filter can be replaced with a `must_not` clause (which will also execute faster). For instance the above query becomes:

``` java
{
  "query": {
    "bool": {
      "must": /* some_query */
      "must_not": { /* some filter */ }
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-09-28T10:02:08Z" id="143698628">I'm good with that
</comment><comment author="cbuescher" created="2015-10-20T09:51:30Z" id="149499287">So are we good with deprecating the complete `not` query from 2.x on and delete in 3.0 like with `and` and `or`? I can start with that.
</comment><comment author="javanna" created="2015-10-20T10:30:50Z" id="149516131">+1
</comment><comment author="cbuescher" created="2015-10-20T10:48:07Z" id="149520227">@jpountz @javanna opened PR against 2.1 for deprecation, mind to take a look if this needs additional changes in the docs? Otherwise I'll also do PR for deletion on master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>filter data by using @timestamp field of  elasticsearch query dsl ,but it dont work, please help</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13760</link><project id="" key="" /><description># mapping info of index

{
"test_php_log-2015.09.23": {
"aliases": { },
"mappings": {
"php_error": {
"properties": {
"@timestamp": {
"type": "date",
"format": "dateOptionalTime"
},
"@version": {
"type": "string"
},
"host": {
"type": "string"
},
"message": {
"type": "string"
},
"path": {
"type": "string"
},
"type": {
"type": "string"
}
}
},
"php_fpm": {
"properties": {
"@timestamp": {
"type": "date",
"format": "dateOptionalTime"
},
"@version": {
"type": "string"
},
"host": {
"type": "string"
},
"message": {
"type": "string"
},
"path": {
"type": "string"
},
"type": {
"type": "string"
}
}
}
},

and this is my elasticsearch query dsl ,but seem in result ,it is dont work

curl -XGET http://192.168.9.99:9200/meipai_php_log-2015.09.23/php_error/_search/ '
{
    "query" : {
        "filtered" : {
            "filter" : {
                "range" : {
                    "@timestamp" : {
                        "gt": "now - 2h"
                    }
                }
            }
        }
    }
}'

===there are part of result =====

{
"took": 6,
"timed_out": false,
"_shards": {
"total": 5,
"successful": 5,
"failed": 0
},
"hits": {
"total": 54592,
"max_score": 1,
"hits": [
{
"_index": "test_php_log-2015.09.23",
"_type": "php_error",
"_id": "I7ver7QFQk2XZPN8_CvNQg",
"_score": 1,
"_source": {
"message": "[23-Sep-2015 10:19:01 Asia/Shanghai] PHP Warning: PDOStatement::execute(): Error reading result set's header in /www/xxxx.com/libraries/Db.php on line 484",
"@version": "1",
"@timestamp": "2015-09-23T02:19:01.483Z",
"type": "php_error",
"host": "xxx72",
"path": "/www/log/php_error.log"
}
}
,
</description><key id="108089714">13760</key><summary>filter data by using @timestamp field of  elasticsearch query dsl ,but it dont work, please help</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adastudy</reporter><labels /><created>2015-09-24T09:12:53Z</created><updated>2015-09-24T09:45:26Z</updated><resolved>2015-09-24T09:45:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-24T09:45:26Z" id="142874771">Please join us on https://discuss.elastic.co/ 
We will help you there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pending operations in the translog prevent shard from being marked as inactive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13759</link><project id="" key="" /><description>The IndexingMemoryController checks periodically if there is any indexing activity on the shard. If no activity is seen for 5m (default) the shard is marked as inactive allowing it's indexing buffer quota to be  given to other active shards.

Sadly the current check is bad as it checks for 0 translog operations. This makes the inactive wait for a flush to happen - which used to take 30m and since #13707 doesn't happen at all (as we rely on the synced flush triggered by inactivity). This commit fixes the check so it will work with any translog size.
</description><key id="108075327">13759</key><summary>Pending operations in the translog prevent shard from being marked as inactive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>bug</label><label>review</label><label>v1.7.3</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-24T07:37:54Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-24T10:09:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2015-09-24T08:06:25Z" id="142847987">LGTM
</comment><comment author="mikemccand" created="2015-09-24T08:06:47Z" id="142848227">LGTM, thanks @bleskes!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CI] MetaDataWriteDataNodesIT.testMetaIsRemovedIfAllShardsFromIndexRemoved failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13758</link><project id="" key="" /><description>Fails with the following generic message (for this test, as this is what it tests):

expected index to not be in meta state of node node_t1

see http://build-us-00.elastic.co/job/es_core_2x_centos/334/testReport/junit/org.elasticsearch.gateway/MetaDataWriteDataNodesIT/testMetaIsRemovedIfAllShardsFromIndexRemoved/

More detailed to be filled in as they are found.
</description><key id="108074797">13758</key><summary>[CI] MetaDataWriteDataNodesIT.testMetaIsRemovedIfAllShardsFromIndexRemoved failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>jenkins</label><label>test</label></labels><created>2015-09-24T07:33:33Z</created><updated>2015-10-19T12:25:26Z</updated><resolved>2015-10-19T12:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-10-13T13:20:22Z" id="147711755">This failed four times within five days (19-24 Sep) but not before or after. I was unable to reproduce it too.
It seems like whatever caused this failure is fixed now. It is unsatisfying to not know what was going on but don't think it makes much sense to spend time on an analysis so I will just close this issue. Feel free to reopen if you disagree.
</comment><comment author="javanna" created="2015-10-13T13:26:18Z" id="147713359">funny timing, I was looking into this as it just failed on the 2.1 branch...reopening :)

http://build-us-00.elastic.co/job/es_core_21_centos/76/
</comment><comment author="brwe" created="2015-10-13T15:13:49Z" id="147745536">chatted with @s1monw and we found the problem: in the test we use the WindowsFS mock file system to simulate that windows does not delete files if they are opened by some other process and instead gives you `java.io.IOException: access denied`. In the test we also check if the shard was deleted while it is being deleted. This check might hold on to a file while the node is trying to delete it and deletion will fail then. I am trying to figure out if this is a bug. We might end up having shards lying around forever if for example a virus scanner is opening files while shards are relocated away from a node on windows.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"now" is not affected by "time_zone" in range queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13757</link><project id="" key="" /><description>As discussed in #13607, `now` ignores the `time_zone` parameter in range queries. This PR adds this to docs.

I decided to not throw an exception for this case because this can be still useful when there are two dates provided and one of them isn't `now`. Is this enough or we should:

(a) Throw an exception every time there is a `now` with `time_zone` even when there is another date parameter (e.g the example in the docs).
(b) Throw an exception if the only date provided is `now`.
(c) The same as (b) but instead of throwing an exception, just log it (warning?).

Closes #13607.
</description><key id="108037381">13757</key><summary>"now" is not affected by "time_zone" in range queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrestc</reporter><labels><label>docs</label></labels><created>2015-09-24T01:13:56Z</created><updated>2015-09-25T11:55:31Z</updated><resolved>2015-09-25T11:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrestc" created="2015-09-24T01:14:36Z" id="142775383">Since @clintongormley  suggested the change, I`m pinging for review.
Thanks!
</comment><comment author="jpountz" created="2015-09-24T14:22:13Z" id="142944432">I think this pull request is the right way to make it clear that `time_zone` will not apply to `now`. I will merge it if there are no objections.
</comment><comment author="andrestc" created="2015-09-24T14:31:11Z" id="142946685">I updated the text from your comment.
Thanks!
</comment><comment author="clintongormley" created="2015-09-25T11:55:31Z" id="143195074">Thanks @andrestc - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow node to start up even if script.disable_dynamic is configured in 2.x+</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13756</link><project id="" key="" /><description>From 2.x branch, if script.disable_dynamic is set in the configuration, we throw the following exception:

```
Exception in thread "main" java.lang.IllegalArgumentException: script.disable_dynamic is not a supported setting, replace with fine-grained script settings. 
Dynamic scripts can be enabled for all languages and all operations by replacing `script.disable_dynamic: false` with `script.inline: on` and `script.indexed: on` in elasticsearch.yml
    at org.elasticsearch.script.ScriptService.&lt;init&gt;(ScriptService.java:146)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at &lt;&lt;&lt;guice&gt;&gt;&gt;
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:202)
    at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:129)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:145)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:168)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:268)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
Refer to the log for complete error details.
```

But then the ES process quits and will not start up.  A better behavior will be for it to just throw the exception, ignore the script.disable_dynamic setting and still allow the ES node to start up.
</description><key id="108036186">13756</key><summary>Allow node to start up even if script.disable_dynamic is configured in 2.x+</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels /><created>2015-09-24T01:01:43Z</created><updated>2016-07-27T11:41:51Z</updated><resolved>2015-09-25T10:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-24T01:38:00Z" id="142779391">I dont understand why we should be lenient, I think its good to fail on configuration errors.
</comment><comment author="ppf2" created="2015-09-24T01:52:35Z" id="142781020">I am more thinking about potential surprise when folks upgrade to 2.x.  As much as we hope that admins out there will thoroughly test and read the documentation on breaking changes, this is unfortunately not always the case, so I am just wondering if it is better to still allow the node to start up and just continue to throw these exceptions to the log on every start up (or attempt to invoke scripting when it is disabled) just to remind the admins to do some clean up.  But I do see your point here, this will simply shift the surprise to when they actually have to use scripting/runtime :)
</comment><comment author="dadoonet" created="2015-09-24T02:43:03Z" id="142787359">Can the migration plugin catch that?
</comment><comment author="clintongormley" created="2015-09-25T10:35:53Z" id="143184051">No it can't.  The error message is self explanatory, and it is an easy fix.  Also the fact that it is an error that prevents startup means the user won't miss it.

Nothing to do here.
</comment><comment author="magec" created="2016-07-27T11:41:51Z" id="235561300">I'm facing that error and came across this PR, agree that I should read the doc, and that it should fail.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fork lucene PatternTokenizer to not reuse a StringBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13755</link><project id="" key="" /><description>To resolve https://github.com/elastic/elasticsearch/issues/13721. Doesn't seem anyone's looked at that, though.

PatternTokenizer and tests are based off Lucene 5.3.0.
</description><key id="108035202">13755</key><summary>Fork lucene PatternTokenizer to not reuse a StringBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">achow</reporter><labels /><created>2015-09-24T00:51:22Z</created><updated>2015-09-25T10:53:27Z</updated><resolved>2015-09-25T10:53:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-24T08:13:39Z" id="142850977">I agree PatternTokenizer shouldn't hold onto heap unnecessarily ... but instead of forking into ES, I think we should fix this in Lucene for its next release (either 5.3.2 or 5.4.0)?
</comment><comment author="mikemccand" created="2015-09-24T08:51:17Z" id="142860560">OK I opened this Lucene issue: https://issues.apache.org/jira/browse/LUCENE-6814
</comment><comment author="clintongormley" created="2015-09-25T10:53:27Z" id="143186528">Thanks @achow and @mikemccand - closing in favour of https://issues.apache.org/jira/browse/LUCENE-6814
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes and bans ImmutableSet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13754</link><project id="" key="" /><description>The next step in #13224.
</description><key id="108016613">13754</key><summary>Removes and bans ImmutableSet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T22:02:22Z</created><updated>2015-09-26T01:46:18Z</updated><resolved>2015-09-26T01:46:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T22:09:40Z" id="142745453">Note to any reviewers: be very very careful. I made a one line mistake and broke parent/child for a while during this process. The tests all pass but I'm a bit afraid.

Also, some of these changes might change the memory usage of the cluster state. ImmutableSet uses open addressing and a single array but this change switches to HashSets which resolves collisions with [trees](http://openjdk.java.net/jeps/180) - so more memory usage.
</comment><comment author="nik9000" created="2015-09-23T23:18:22Z" id="142757856">In other odd news, this pr shaves a minute off of a clean compile and test of the core module. I'm not sure why.
</comment><comment author="jpountz" created="2015-09-24T15:19:13Z" id="142960332">It looks good to me overall. I think the fact that the PR both moves to the stream API and forbids Immutable makes the review a bit harder, for future refactorings maybe we should try to do it in different pull requests. As far as I am concerned, I think I would find the use of the stream API a bit more readable if the chaining was performed on a separate line for every operation.
</comment><comment author="nik9000" created="2015-09-24T17:55:38Z" id="143004287">&gt; I think the fact that the PR both moves to the stream API and forbids Immutable makes the review a bit harder, for future refactorings maybe we should try to do it in different pull requests.

I imagine we wouldn't really move to Stream at all if we did that - which is ok I guess.

&gt; I think I would find the use of the stream API a bit more readable if the chaining was performed on a separate line for every operation.

I've done that a few places. I also reverted back to imperative style in a few places as well.
</comment><comment author="nik9000" created="2015-09-24T17:58:25Z" id="143004944">@jpountz I believe this is ready for round two at your leisure. Thanks for reviewing it!
</comment><comment author="nik9000" created="2015-09-24T20:48:28Z" id="143046605">Sorry about that! I had unpushed local changes that had addressed some of the formatting.
</comment><comment author="jpountz" created="2015-09-24T20:56:35Z" id="143048886">LGTM
</comment><comment author="nik9000" created="2015-09-25T13:52:56Z" id="143230050">I've merged master into this to pick up the query refactoring changes that were just merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Block process execution with seccomp on linux/amd64</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13753</link><project id="" key="" /><description>On newer linux kernels, we can use prctl/seccomp to lock down the process and prevent the worst of the worst, like execution. This is used by e.g. chrome/firefox sandbox and so on.

This is just another level of security, java's security manager is not perfect, and there are often bugs in java itself, so it would good to have another level of defense.

See https://en.wikipedia.org/wiki/Seccomp for more information

This PR blocks execve(), fork(), and vfork(), returning EACCES instead, so even with security manager disabled, process execution is still prevented. 

```
            java.io.IOException: Cannot run program "ls": error=13, Permission denied
                    at __randomizedtesting.SeedInfo.seed([65E6C4BED11899E:FC6E1CA6AA2DB634]:0)
                    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
                    at java.lang.Runtime.exec(Runtime.java:620)
                    ...
                  Caused by: java.io.IOException: error=13, Permission denied
                    at java.lang.UNIXProcess.forkAndExec(Native Method)
                    at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:248)
                    at java.lang.ProcessImpl.start(ProcessImpl.java:134)
                    at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
```
</description><key id="108014551">13753</key><summary>Block process execution with seccomp on linux/amd64</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T21:53:28Z</created><updated>2016-06-04T14:30:53Z</updated><resolved>2015-09-24T17:55:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-23T21:55:30Z" id="142742277">nice!
</comment><comment author="dakrone" created="2015-09-23T22:20:22Z" id="142747239">Tested this locally and it looks to be working, very neat!
</comment><comment author="rmuir" created="2015-09-24T02:54:13Z" id="142789085">I fixed all the TODOs and gripes I had with this and added documentation, checks here. I think its ready...
</comment><comment author="jaymode" created="2015-09-24T12:25:35Z" id="142913957">left a minor comment about a log message, other than that LGTM! This is really nice.
</comment><comment author="rmuir" created="2015-09-24T12:46:30Z" id="142917602">Thanks for looking, I added that debug, good idea.
</comment><comment author="s1monw" created="2015-09-24T13:00:45Z" id="142920222">LGTM FWIW I don't know that linux feature in particular
</comment><comment author="samcday" created="2016-06-04T01:58:58Z" id="223729818">Hey so it turns out this secure computing thing completely breaks `-XX:OnOutOfMemoryError`, since the JVM attempts to `execs`the supplied command(s).

Should I raise a separate issue for that?
</comment><comment author="rmuir" created="2016-06-04T06:02:37Z" id="223738813">More like, launching processes is bullshit for a daemon process to do. Under any circumstances. Manage this stuff with startup scripts, etc. If you don't agree with me, open an issue if you like. But this is an important piece, to ensure remote execution is something that doesn't happen again.
</comment><comment author="jasontedor" created="2016-06-04T14:30:53Z" id="223758669">&gt; Hey so it turns out this secure computing thing completely breaks `-XX:OnOutOfMemoryError`, since the JVM attempts to `execs`the supplied command(s).

For others that might not click through to #18736, if all you want to do is kill on `OutOfMemoryError`, just use `ExitOnOutOfMemoryError` available starting in 8u92.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor SearchRequest to be parsed on the coordinating node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13752</link><project id="" key="" /><description>This change refactors the SearchRequest so it is parsed on the coordinating node and then sent as a serialised object to the shards. The SearchSourceBuilder encompasses the state of the bpdy of the search request (know in the code as `source`) and is embedded in the SearchRequest. On the shard the SearchSourceBuilder is used to populate the SearchContext where we used to parse the source JSON.

A few things to note:
- The Java API has been changed to only accept builders in the SearchRequest, SearchRequestBuilder, SearchSourceBuilder and other similar classes. This means that all methods that accepted BytesReference, byte[], String, BytesArray etc. in the search API (and derivatives) should have been removed
- Some variables inside the SearchSourceBuilder are stored as BytesReference object as their builder objects have not yet been refactored. The SearchSourceBuilder converts the incoming Builder into the BytesReference to serialise to the shards and the parsing of this JSON is still handled in SearchService.parseSource(). Over time these will be refactored so these builders can also be stored as objects in SearchSourceBuilder and serialised to the shards properly.
</description><key id="108011983">13752</key><summary>Refactor SearchRequest to be parsed on the coordinating node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Search Refactoring</label><label>breaking</label><label>enhancement</label><label>review</label></labels><created>2015-09-23T21:37:48Z</created><updated>2016-01-18T07:50:08Z</updated><resolved>2015-10-19T10:17:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-29T16:54:19Z" id="144119020">This has been replaced by https://github.com/elastic/elasticsearch/pull/13859
</comment><comment author="javanna" created="2015-10-19T10:17:44Z" id="149175117">This was merged, we can close. the only thing that's left to-do is documenting all of the specific breaking changes and eventually deprecate those methods removal in 2.x branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Docs] System properties trump settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13751</link><project id="" key="" /><description>For clients, it can sometimes be the case that users will specify their own system properties that happen collide with Elasticsearch's settings. In at least one case, I've come across the scenario when client-side code was attempting to talk to two separate clusters (one feeding from another).

On the client-side, a system property was being generically set like `elasticsearch.cluster.name` (or `es.cluster.name`) because the application's property management made it easier to do this and decouple from the data store. This led to the problem where the secondary cluster's `cluster.name` setting was being silently ignored.

To counter this problem, it's as easy as telling the Elasticsearch settings to ignore system properties by setting `config.ignore_system_properties` to `false`. It would be better to not have system properties sneaking into your settings as well, but this is rarely an intentional problem so it can go unnoticed for awhile unless something actually stops working entirely.

I'd be happy to make the PR myself, but I am not sure where it should go, so I'm hoping that someone will have a good suggestion.

Also, perhaps we should warn within the settings builder if a property gets overwritten for any reason? It seems like a bad approach for someone to set, then reset a setting _intentionally_.
</description><key id="107989531">13751</key><summary>[Docs] System properties trump settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">pickypg</reporter><labels><label>discuss</label><label>docs</label></labels><created>2015-09-23T19:27:41Z</created><updated>2016-09-27T15:54:21Z</updated><resolved>2016-09-27T15:54:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-23T20:08:19Z" id="142714541">Loading system properties are specifically meant to allow overrides. Why would we warn on that? This sounds like a user problem. 
</comment><comment author="pickypg" created="2015-09-23T20:13:07Z" id="142715562">I agree that it's a user problem, especially with clients (where it's far less expected in my opinion), but perhaps "warn" should be translated to "DEBUG".
</comment><comment author="dakrone" created="2016-09-27T15:54:21Z" id="249908932">This was fixed in 5.0 with https://github.com/elastic/elasticsearch/pull/18198
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: remove IndexQueryParseService#parse methods used only in tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13750</link><project id="" key="" /><description> SimpleIndexQueryParserTests was the main responsible: deleted lots of duplicated tests, moved the ones that made sense to keep to their corresponding unit tests (note they were ESSingleNode tests before while are now converted to unit tests).
</description><key id="107966586">13750</key><summary>Query refactoring: remove IndexQueryParseService#parse methods used only in tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-23T17:20:48Z</created><updated>2015-09-24T12:08:39Z</updated><resolved>2015-09-24T12:08:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-23T18:41:53Z" id="142692835">just the diffs get a LGTM  here :)
</comment><comment author="cbuescher" created="2015-09-24T10:55:03Z" id="142892548">@javanna did a few spot checks on where the tests were moved, looks good to me too.
</comment><comment author="javanna" created="2015-09-24T12:08:38Z" id="142911321">Merged 8976934d3b5a171c85cfaa0a9e88c1a1e9e3f90a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing: Remove smoke tester verbose option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13749</link><project id="" key="" /><description>The verbose option was buggy and with the run() option imported
from the prepare_release_candidate not really needed, as it is now
easy to spot the commands due to color coding.
</description><key id="107960782">13749</key><summary>Testing: Remove smoke tester verbose option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>test</label></labels><created>2015-09-23T16:48:43Z</created><updated>2015-10-07T13:00:55Z</updated><resolved>2015-10-07T13:00:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-10-07T13:00:55Z" id="146189709">committed with https://github.com/elastic/elasticsearch/commit/21950dd2aa2a2f418a31f3eeade27c153d994176
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update put-mapping.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13748</link><project id="" key="" /><description>Small typo.
</description><key id="107950818">13748</key><summary>Update put-mapping.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bcoughlin</reporter><labels><label>docs</label></labels><created>2015-09-23T15:52:40Z</created><updated>2015-09-25T10:12:59Z</updated><resolved>2015-09-25T10:12:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T15:54:29Z" id="142646443">Looks good to me. @bcoughlin, can you sign the [cla](https://www.elastic.co/contributor-agreement)? Its required even for awesome typo fixes like this.
</comment><comment author="bcoughlin" created="2015-09-23T16:00:49Z" id="142648023">Signed. OCD made me do it.
</comment><comment author="clintongormley" created="2015-09-25T10:12:57Z" id="143180911">thanks @bcoughlin - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] add lang-expression plugin to vagrant tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13747</link><project id="" key="" /><description>we need this now that #13726 has been merged
</description><key id="107950427">13747</key><summary>[test] add lang-expression plugin to vagrant tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T15:50:48Z</created><updated>2015-10-01T04:01:00Z</updated><resolved>2015-09-23T16:01:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T15:52:36Z" id="142646001">Left minor comments otherwise LGTM.
</comment><comment author="brwe" created="2015-09-23T15:56:20Z" id="142646942">ok, reordered
</comment><comment author="rmuir" created="2015-10-01T04:01:00Z" id="144613965">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More Endless refresh-mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13746</link><project id="" key="" /><description>As per https://github.com/elastic/elasticsearch/issues/10318 i'm seeing endless refresh-mappings happening as a result of ordering issues vs the master mapping.  The aforementioned article provides a work around (that says, should not be used in newer releases of ES).  On 1.7.2 the cluster stopped thrashing once the data nodes had the 'indices.cluster.send_refresh_mapping: false.

Please advise.
</description><key id="107947808">13746</key><summary>More Endless refresh-mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blackhole-em</reporter><labels><label>:Mapping</label><label>feedback_needed</label></labels><created>2015-09-23T15:37:48Z</created><updated>2016-01-28T18:08:36Z</updated><resolved>2016-01-28T18:08:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-23T17:45:35Z" id="142676581">@blackhole-em please provide us with a copy of your mapping - something isn't being serialized properly.  It may already have been fixed in 2.0
</comment><comment author="clintongormley" created="2016-01-28T18:08:36Z" id="176310774">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: remove deprecated methods and temporary classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13745</link><project id="" key="" /><description>After all queries now have a `toQuery` method and the parsers all
support `fromXContent` it is possible to remove the following
workarounds and deprecated methods we kept around while doing the
refactoring:
- remove the BaseQueryParser and BaseQueryParserTemp. All parsers
  implement QueryParser directly now
- remove deprecated methods in QueryParseContext that either returned
  a Query or a Filter.
- remove the temporary QueryWrapperQueryBuilder

Relates to #10217
</description><key id="107942429">13745</key><summary>Query Refactoring: remove deprecated methods and temporary classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-23T15:11:21Z</created><updated>2016-03-11T11:50:44Z</updated><resolved>2015-09-24T11:01:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-23T15:22:52Z" id="142638094">left a couple of comments but this looks great, thanks for picking this up
</comment><comment author="s1monw" created="2015-09-23T18:45:54Z" id="142693775">LGTM though
</comment><comment author="cbuescher" created="2015-09-24T10:43:14Z" id="142888873">@javanna addressed your comment concerning not query parser and opened follow up issue, want to have another look or are you okay with this now?
</comment><comment author="javanna" created="2015-09-24T10:48:52Z" id="142889882">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds the gradle wrapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13744</link><project id="" key="" /><description>It's generally a good practice to use the gradle wrapper so everyone is building with the same version of gradle. Instead of `gradle someTask` commands become `./gradlew someTask` (assuming a nix environment)
</description><key id="107931219">13744</key><summary>Adds the gradle wrapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adrianbk</reporter><labels /><created>2015-09-23T14:23:10Z</created><updated>2016-03-10T16:12:55Z</updated><resolved>2016-03-10T16:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-23T17:27:12Z" id="142671717">@adrianbk Thanks for the PR. I have been apprehensive about using the gradle wrapper for a number of reasons. Maybe you can assuage them.
1. This means I have to run all gradle tasks from the root project? Or at least always refer back to it like `../../gradlew`
2. Autocomplete does not work because the gradle wrapper lib goes into `gradle/`. So typing `./g&lt;tab&gt;` autocompletes to the dir (just requires typing an extra `w`, but still, a pain and seems to be counter to the point of the wrapper, ie easy running)
3. (most important) It means adding binary files (a jar) to git history. This means every version we generate the wrapper for will be permanently in the history (pruning aside). In Lucene, all dependencies used to be checked into source control, and this has proved to be a huge pain when converting the repo to git (makes cloning take an enormous amount of time). Obviously this is a small jar, but it is still binary data.

What I would rather do is enforce the version of gradle within the build script itself. For example, the current build is written against the 2.6 api, but 2.6 or 2.7 should work fine. I haven't figured out an easy way to do this though (it does not appear, at least to me, that the gradle version is exposed programmatically).

FWIW, I would be completely fine with having this _and_ the desired version check above, so that casual users trying to work on PRs would have less burden to have gradle set up. But I would probably not do it as long as there is a jar needed in source control. I have no idea what the technical limitations might be, but perhaps the wrapper could be bootstrapped my having a .java file instead of jar, and use javac first? It looks like the wrapper works with a JRE, but this seems silly since you can't run a build with a jre?
</comment><comment author="big-guy" created="2015-10-01T05:50:28Z" id="144627392">I tried to reply to your concerns below.  I think the wrapper is a good idea for a few reasons:
1. Low barrier to building the project for first time contributors
2. Switching branches/projects with the wrapper means you don't have to worry about $PATH pointing to the correct Gradle version
3. When an upgrade occurs, you don't have to worry about some contributors lagging behind
4. Testing a new Gradle version just means pointing the wrapper to a new URL
5. If it becomes convenient to create a custom "Gradle with ES plugins" distribution, making sure everyone is using it is just an update to the URL

&gt; This means I have to run all gradle tasks from the root project? Or at least always refer back to it like ../../gradlew

Yes and no. Most of the time, I work from root, but I usually have a flatter project structure.  To build from a subproject, you'd have to use relative paths to gradlew or use a script like gdub (https://github.com/dougborg/gdub).  

You can also look at something like sdkman (http://sdkman.io/) if you're used to tools like rvm, but that raises the bar for contributors.

&gt; Autocomplete does not work because the gradle wrapper lib goes into gradle/. So typing ./g&lt;tab&gt; autocompletes to the dir (just requires typing an extra w, but still, a pain and seems to be counter to the point of the wrapper, ie easy running)

The point of the wrapper is to ensure everyone is building with the right version of Gradle. That could be a custom version of Gradle (bundled with plugins) or a particular release from gradle.org.  In hindsight, it probably would have been nicer to call the script 'gw' or something unambiguous.

&gt; (most important) It means adding binary files (a jar) to git history. This means every version we generate the wrapper for will be permanently in the history (pruning aside). In Lucene, all dependencies used to be checked into source control, and this has proved to be a huge pain when converting the repo to git (makes cloning take an enormous amount of time). Obviously this is a small jar, but it is still binary data.

I looked at the history of the Gradle repo where we've updated the wrapper ~100 times.  This makes up less than 5MB in the repo (assuming no compression), so I don't think this is a big issue.  The wrapper can be used across many versions, so unless there are new wrapper features, you would never have to update it again.

&gt; What I would rather do is enforce the version of gradle within the build script itself. For example, the current build is written against the 2.6 api, but 2.6 or 2.7 should work fine. I haven't figured out an easy way to do this though (it does not appear, at least to me, that the gradle version is exposed programmatically).

The Gradle version is exposed as a property on the Gradle object: https://docs.gradle.org/current/dsl/org.gradle.api.invocation.Gradle.html#org.gradle.api.invocation.Gradle:gradleVersion

Enforcing (or complaining loudly about) the version of Gradle used within your build seems fine to me (to prevent the accidental use of another version), I think the wrapper is a nicer way to do that because you don't have to worry about telling someone how to go get Gradle.

If you start to publish your own Gradle plugins for consumption outside your build, the other thing to keep in mind is that you'll probably want to use the Gradle TestKit and plan for testing against multiple versions of Gradle (or just declare that version X is the supported one). At most, I'd have the plugins complain if they're used with an untested version of Gradle (I wouldn't fail the build).

&gt; FWIW, I would be completely fine with having this and the desired version check above, so that casual users trying to work on PRs would have less burden to have gradle set up. But I would probably not do it as long as there is a jar needed in source control. I have no idea what the technical limitations might be, but perhaps the wrapper could be bootstrapped my having a .java file instead of jar, and use javac first? It looks like the wrapper works with a JRE, but this seems silly since you can't run a build with a jre?

The wrapper can start with the JRE and then find a JDK to run the build with (e.g., setting org.gradle.java.home to a JDK).  When Groovy was moving over to Apache, I think there was a minor fracas about a binary artifact in the repository, so they considered some different things.  I think they were eventually allowed to keep the wrapper, but I think one of the suggestions was to download the gradle-wrapper.jar from somewhere in the gradlew/gradlew.bat scripts.

HTH
</comment><comment author="uschindler" created="2015-10-01T08:57:01Z" id="144661317">Hi,
Ryan was talking with me about this as I am the "Apache Guy". I wrote the gradle plugin for forbiddenapis, released yesterday as version 2.0. I also investigated using the gradle wrapper to test the plugin in an gradle environment (the build of forbiddenapis is ANT and that is fine for multi-build-system plugin: we have Ant, Maven, Gradle, CLI in forbiddenapis and bugs like gradleApi() introducing tons of bullshit into your compile environment, including incomaptible ASM versions stopped me from migrating the whole build to gradle - so gradle not always have good sides). The most horrible thing is: You cannot get Gradle and all its internal dependencies on Maven Central! I have no idea why this is like that, but this makes developing plugins a pain, unless you use Gradle to build your plugin with gradleApi() f*cking up your classpath,

While reviewing Gradle wrapper to have an easy test environment for the plugin, I just noticed the following problems: In Lucene and Solr we are not allowed to put JAR files into source distributions or commit them to SVN. There are checks of the download archives regularily that enforce this. This was a big issue before Lucene's build moved to Ant/Ivy, because we also had compile time dependencies committed to SVN, and this was a blocker at some point! On top of that: Recently Groovy got into Apache Incubation as it wants to join as a full fledged ASF project. One of the first commits after importing the Git repo into ASF was - you might guess it - removing the wrapper jar and wrapper scripts (see https://mail-archives.apache.org/mod_mbox/incubator-groovy-notifications/201505.mbox/%3Cc28c218e8f694efbaa5fa774ceb26b72@git.apache.org%3E). In the meantine they seem to had some discussions (also on the legal forums) and I think it was added back as a "special case", but having binary files in a source distrbution is still a no-go.

Here was the discussion: http://mail-archives.apache.org/mod_mbox/groovy-dev/201504.mbox/%3C552F3B8F.6030207@gmx.org%3E

Other Apache projects use custom shell scripts to download the gradle wrapper. It would all be simple if the gradle wrapper would be deployed to Maven central, but it isn't. In Lucene we have similar code that downloads the IVY version to use with Ant using "ant ivy-bootstrap".

As Elasticsearch is no ASF project, the situation might be different here, but my personal opinion is also: Don't do this! The Gradle people want to push you to this, but this is not real open-source like (you know there is a company behind, looks like they want to push their binary into every source repo).

I don't see a problem requiring a Gradle installation to build Elasticsearch. Where is the difference to Maven or Ant required to build a project?
</comment><comment author="melix" created="2015-10-01T09:16:13Z" id="144666013">To be clear we _never_ removed the wrapper. I think it's a problem that the ASF doesn't allow it (the Eclipse Foundation, which is even worse than ASF in terms of IP compliance, allows it). What we did is removing the wrapper from the _source zip_ that we produce. With my Gradle hat on, I would _never_ recommand not to use the wrapper. You _have_ to use it.

As for `gradleApi()` polluting your classpath, I'm not sure what you mean but it might be worth pulling this on the forums. The thing is that `gradleApi()` is for a Gradle plugin, so it will bring everything on classpath that Gradle needs, and it is expected I think.
</comment><comment author="uschindler" created="2015-10-01T09:30:03Z" id="144669032">&gt; With my Gradle hat on, I would never recommand not to use the wrapper. You have to use it.

No, you don't have to use it. You can download the source distribution and install gradle in parallel (as I did) and run the build by entering "gradle task". Why do I need a wrapper for that? I don't need one for Maven or Ant, whats different here for Gradle?

&gt; As for gradleApi() polluting your classpath, I'm not sure what you mean but it might be worth pulling this on the forums. The thing is that gradleApi() is for a Gradle plugin, so it will bring everything on classpath that Gradle needs, and it is expected I think.

It is this issue: https://issues.gradle.org/browse/GRADLE-1715

I have no idea why there is no work on resolving this. This is a blocker for using Gradle to build stuff that relies on for example on ASM (and shaded later). There are also issues to build a plugin for 3 build systems: Gradle, Maven and Ant. The classpath is polluted also with Ant and Maven dependencies (of course in wrong versions). Elasticsearch also has a custom plugin (see buildSrc), so it affects uses here, too.

I would say, this is not related to this issue, I just brought it in here, because Gradle has problems as a build system like others have, too. I just wanted to bring in the whole story.

At the moment I would still argue against using Gradle to build Lucene. I am happy with Ant.
</comment><comment author="melix" created="2015-10-01T09:35:08Z" id="144670256">You _can_ download the distribution, but you should not have to. Ideally your CI server or your users shouldn't have any prerequisite to build ElasticSearch (apart from a JDK). So using the wrapper, you're locking things down, including the version of the tool which has been validated to work. If tomorrow someone upgrades something in the build, and that this little something requires an upgrade of Gradle, without the wrapper, it can easily be unnoticed. Using the wrapper you don't have such a problem. If your build requires an upgrade of the toolchain (Gradle), the build knows. The CI configuration will not need to be updated, and you won't have to know, it's all transparent.
</comment><comment author="uschindler" created="2015-10-01T09:38:13Z" id="144672595">I would be fine if all this could be done with a simple shell script that downloads gradle-wrapper.jar (_from Maven Central, please_), but having a binary JAR in your source distro is a big no-go for me.
</comment><comment author="melix" created="2015-10-01T09:41:24Z" id="144675099">You can do exactly like what we did for Groovy: have it in Git, exclude it from the source zip. It's a pity to have to do this, but that's what the ASF requires, nothing more. Having it in Git is important because a shell script that downloads the wrapper is easy to write, but it has problems too:
- dependency on `curl` or whatever tool you would use to download the jar. (`wget`, ...), so you add another pre-requisite
- there's still _no_ way to do such a thing under Windows, and Gradle has to support it.
</comment><comment author="uschindler" created="2015-10-01T09:51:43Z" id="144681508">As said before, I have no problem in documenting that you need a specific version of Gradle to build Elasticsearch and put a link in README. This is similar to Maven today (or Ant/Ivy in terms of Lucene). If you put something into the build.gradle that complains if the version is wrong (like `&lt;antversion later=.../&gt;` in Ant) I am happy. And for Jenkins it is perfectly fine to declare this as a "Gradle build for version XY", refer to build.gradle. Very easy for the CI manager; and similar to statement "dowload from Github URL xy before starting gradle build". It is just a simple GUI configuraion in your CI server.
</comment><comment author="melix" created="2015-10-01T10:04:55Z" id="144685405">You can do it, but doing this you're doing too much work. You are asking people to install a build tool, which they can avoid. You are asking them to check the README if something goes wrong because you happened to upgrade Gradle, which we can avoid. You are also putting more work on the CI configuration side, where again you could avoid it. And last but not least, if you have several branches and that they use different versions of Gradle, you have to configure it when again you could avoid it.

I don't say you _must_ use the wrapper. I'm saying there is no good reason not to use it.
</comment><comment author="adrianbk" created="2015-10-01T10:05:38Z" id="144685514">Gradle is far more than both Maven and Ant, you simply will not get the same capabilities and flexibility. Gradle is very much designed with automation in mind, the gradle Wrapper being a single example. 

&gt; just a simple GUI configuraion in your CI server 

is the antithesis of automation. Consider having hundreds or even thousands of CI servers.
</comment><comment author="uschindler" created="2015-10-01T10:53:16Z" id="144694403">Elasticsearch master already has minimum Java 8 as requirement. Lucene trunk is same and branch_5.x may migrate soon. Java 8 requires to have the "jjs" (the command line tool of nashorn) scripting engine in JDK available, so I would prefer to have a simple Javascript for beginners available. Gradle should also think about using this instead of binaries like gradle-wrapper.jar as "internet &#228;hm build engine downloader".
</comment><comment author="nik9000" created="2015-10-01T16:18:49Z" id="144776901">I'd be supper happy with a shell script and a bat script and sad about a jar file. For what that is worth. 
</comment><comment author="ywelsch" created="2015-10-01T21:52:51Z" id="144860443">The Gradle wrapper makes it much simpler to get started working on Elasticsearch without having to worry about a locally installed Gradle version. I think that ease of use should be a determining factor here. Having the Gradle wrapper still gives you the freedom of not using it and running the build from a custom Gradle installation.

About the wrapper jar specifically: I fail to see the relevance of ASF policies here. Pivotal, Netflix and many others are using the Gradle wrapper in their OSS. As for the wrapper jar itself, it is very small and can, if need be, directly be built from the Gradle sources.

@rjernst The directory of the wrapper jar does not need to be named `gradle`, see property `jarFile` of `Wrapper` task. As for why Gradle does not rely on the JDK but only JRE, note that Gradle is not only used to build Java projects (and other Java compilers like ecj do not need a JDK).
</comment><comment author="clintongormley" created="2016-03-10T16:12:55Z" id="194929615">Given the desire not to have a binary blob in our git repo, and the fact that we've settled into our gradle installation now, I'm going to close this PR.

thanks for submitting, and for the interest
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds a version number to every dependency on org.apache.httpcomponent&#8230;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13743</link><project id="" key="" /><description>&#8230;s:httpclient
</description><key id="107922667">13743</key><summary>Adds a version number to every dependency on org.apache.httpcomponent&#8230;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adrianbk</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T13:44:01Z</created><updated>2015-09-23T16:54:14Z</updated><resolved>2015-09-23T16:54:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T13:49:57Z" id="142607196">@rjernst this one is for you I think.

@adrianbk can you sign the [cla](https://www.elastic.co/contributor-agreement)? Its required for all changes.
</comment><comment author="adrianbk" created="2015-09-23T14:01:41Z" id="142610681">@nik9000 Yep, I'd signed the CLA before submitting the PR. Confirmed it just now.
</comment><comment author="adrianbk" created="2015-09-23T14:03:14Z" id="142611354">Here's the background for this PR: https://discuss.gradle.org/t/dependency-resolution-extremely-slow-with-s3-repositories/ 
</comment><comment author="clintongormley" created="2015-09-23T14:13:49Z" id="142614312">CLA has been signed
</comment><comment author="rjernst" created="2015-09-23T16:54:14Z" id="142660806">Thank you @adrianbk!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `ExpressionScriptCompilationException` and `ExpressionScriptExecutionException`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13742</link><project id="" key="" /><description>These exceptions are useless and unused, since we are on a major verison we should remove
them. This commit also makes it easier to remove exceptions in the future.
</description><key id="107915779">13742</key><summary>Remove `ExpressionScriptCompilationException` and `ExpressionScriptExecutionException`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T13:11:49Z</created><updated>2015-09-25T10:17:18Z</updated><resolved>2015-09-23T18:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T13:14:27Z" id="142595636">Left a question. LGTM though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`-` command line arguments must be passed before `--`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13741</link><project id="" key="" /><description>It looks like right now `-` command line arguments must be passed before `--` but that is confusing. Things like `ls` don't impose an order on the arguments so people aren't used to there being an order and it trips them up. Can Elasticsearch just allow them in any order?
</description><key id="107908436">13741</key><summary>`-` command line arguments must be passed before `--`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-09-23T12:23:36Z</created><updated>2016-09-27T15:51:55Z</updated><resolved>2016-09-27T15:51:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2016-09-27T15:51:54Z" id="249908131">I believe this is fixed with the refactor of CLI parameters so I am closing this, feel free to re-open if I'm mistaken.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing reserved fields inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13740</link><project id="" key="" /><description>If I create an index by indexing a document which contains a reserved field (eg `_source`, `_id`) then I get an exception.  However, indexing the same document again works, eg:

```
DELETE t

PUT t/t/1
{
  "_id": 12345
}
```

returns:

```
    {
        "type": "illegal_argument_exception",
        "reason": "Mapper for [_id] conflicts with existing mapping in other types:\n[mapper [_id] cannot be changed from type [_id] to [long]]"
     }
```

But indexing into an existing index works:

```
PUT t/t/1
{
  "_id": 12345
}
```

This means that the following bulk request would throw an exception for the first document, but accept any following documents:

```
DELETE t
POST t/t/_bulk
{"create": {}}
{"_id":12345}
{"create": {}}
{"_id":12345}
```

Related to #10456
</description><key id="107908301">13740</key><summary>Indexing reserved fields inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>bug</label><label>v2.0.0</label></labels><created>2015-09-23T12:22:36Z</created><updated>2015-10-08T09:41:50Z</updated><resolved>2015-10-07T21:40:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-10-07T14:10:40Z" id="146206265">Is it ever legal to pass a metadata field (_id,_version, _ttl etc) in the body of a doc? 
</comment><comment author="clintongormley" created="2015-10-07T16:38:42Z" id="146256809">No, not any more.  
</comment><comment author="markharwood" created="2015-10-07T16:45:32Z" id="146258585">OK. Trying out a patch that checks MapperService.isMetadataField() for legal fieldnames in the doc body. Anything else we need to tighten up in naming?
</comment><comment author="markharwood" created="2015-10-07T17:04:23Z" id="146263431">I added a simple check in DocumentParser:

```
if (MapperService.isMetadataField(currentFieldName)) {
    throw new MapperParsingException("Reserved field name [" + currentFieldName + "] passed in document body");
}
```

The tests that failed were mainly "testIncludeInObjectBackcompat".
- org.elasticsearch.index.mapper.ttl.TTLMappingTests.testIncludeInObjectBackcompat
- org.elasticsearch.index.mapper.parent.ParentMappingTests.testParentNotSet
- org.elasticsearch.index.mapper.parent.ParentMappingTests.testParentSetInDocBackcompat
- org.elasticsearch.index.mapper.all.SimpleAllMapperTests.testIncludeInObjectBackcompat
- org.elasticsearch.index.mapper.routing.RoutingTypeMapperTests.testIncludeInObjectBackcompat
- org.elasticsearch.index.mapper.timestamp.TimestampMappingTests.testIncludeInObjectBackcompat
- org.elasticsearch.index.mapper.id.IdMappingTests.testIncludeInObjectBackcompat

They look to check metadata-fields-in-the-body are safely ignored i.e have a comment like this:

```
        // _routing in a document never worked, so backcompat is ignoring the field
```

So do we break backcompat and now no longer tolerate these metadata fields by throwing exceptions?
</comment><comment author="rjernst" created="2015-10-07T17:42:02Z" id="146273624">@markharwood No, we should not break backcompat. Here is the original PR that was supposed to block this: #11074. Prior to that PR, each meta field could override `includeInObject()` to return `true`, which would cause the mapper for that meta field to be added to the rest of the mappers for document types. Thus when encountering the field while parsing the document, it would find the metadata mapper, and parse. There was also logic inside the metadata mapper that allowed this parsing.

For backcompat, I kept the parsing logic in metadata mappers that supported it before. This can be removed in master, as well as the backcompat check to add select metadata fields to the regular mappers.  What must be happening here is the metadata mappers are still getting added to the regular mappers map. This shouldn't be happening on a new index. I'm still investigating why that is happening...
</comment><comment author="rjernst" created="2015-10-07T17:46:28Z" id="146274945">One thing I notice is we should have an extra check similar to what you propose (but the impl of isMetadataField needs to be updated, because the list of metadata fields should no longer be static, because they can be added by plugins, as _size is now a plugin). So we should add your check, but guard with a backcompat check.
</comment><comment author="rjernst" created="2015-10-07T17:47:27Z" id="146275157">This will block the action earlier, since right now, from the "good" case in Clint's example, you can see the field was actually parsed and it is trying to add the field to the mappings. This is too late.
</comment><comment author="rjernst" created="2015-10-07T17:52:39Z" id="146276519">Note that fixing `isMetadataField` will require a little plumbing. This should not be a static method, which means passing down the metadata fields list when creating `GetResult`, and then doing the check before creating `GetField` (so get field should have a flag, instead of the current `isMetadataField()` which calls the static `MapperService` method.
</comment><comment author="rjernst" created="2015-10-07T19:32:40Z" id="146304112">I have a change I'm testing that is minimally invasive for backport to 2.0. I will make a followup to cleanup `isMetadataField` to work with metadata fields added by plugins, but that requires a larger change to how metadata fields are added.
</comment><comment author="clintongormley" created="2015-10-08T09:41:50Z" id="146474834">Closed by #14003
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal: Elasticsearch doctest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13739</link><project id="" key="" /><description>Elasticsearch has lots examples sitting in asciidoc but doesn't have a mechanism for making sure that they work. I propose it grow some way to test these either by running them against a cluster or by comparing them to the rest-api-specs.
</description><key id="107905332">13739</key><summary>Proposal: Elasticsearch doctest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label><label>docs</label><label>test</label></labels><created>2015-09-23T12:05:04Z</created><updated>2016-05-12T13:40:37Z</updated><resolved>2016-05-12T10:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-23T12:25:41Z" id="142583776">++ IMO we should extract the documentation from existing tests.

That's why I opened https://github.com/elastic/docs/issues/4 to make Java documentation always consistent.

See an example here of what I can imagine for a Java Client doc: https://github.com/elastic/elasticsearch/blob/master/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java#L39
</comment><comment author="nik9000" created="2015-09-23T12:28:49Z" id="142584319">&gt; See an example here of what I can imagine for a Java Client doc: https://github.com/elastic/elasticsearch/blob/master/qa/smoke-test-client/src/test/java/org/elasticsearch/smoketest/SmokeTestClientIT.java#L39

Oh that is cool! I like that.

I was thinking of the other way around - running the json docs that we have through something to make sure they work.
</comment><comment author="danielmitterdorfer" created="2016-05-11T21:35:33Z" id="218597526">@nik9000 Isn't this covered by #18075 (i.e. can we close this ticket?)?
</comment><comment author="clintongormley" created="2016-05-12T10:27:25Z" id="218718862">Closed by #18075
</comment><comment author="nik9000" created="2016-05-12T13:40:36Z" id="218760228">Man, I've been talking about this for a while now. I just keep forgetting all the tickets I've made.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Order of command line flags</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13738</link><project id="" key="" /><description>specifying the order required for cli -Des notation arguments
</description><key id="107901420">13738</key><summary>Order of command line flags</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nellicus</reporter><labels><label>docs</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T11:41:30Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-23T12:26:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T11:51:03Z" id="142576978">Relates to e27ede48ce6f275f80e8b59bc6dc7ad2849c64fc and #13737.
</comment><comment author="nik9000" created="2015-09-23T11:52:37Z" id="142577181">Looks good to me. Now that I look at it I wonder why you have to specify the parameters in that order.
</comment><comment author="nellicus" created="2015-09-23T12:11:41Z" id="142580939">@nik9000 I am not sure why but it caused my existing config to fail
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More helpful error message on parameter order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13737</link><project id="" key="" /><description>This commit addresses a confusing error message that arises when a
property parameter (e.g. -D) is after a double-dash parameter. The
current error message reports to the user that the parameter does not
start with &#8220;--". Adding the second dash as the error message suggests
causes the parameter to be silently ignored. This is confusing for the
user. With this commit, the user is now informed that the parameter
order is violated.

Relates e27ede48ce6f275f80e8b59bc6dc7ad2849c64fc
</description><key id="107900414">13737</key><summary>More helpful error message on parameter order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T11:33:42Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-23T14:14:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T11:49:55Z" id="142576830">LGTM.

I'd probably add a test for this to BootstrapCliParserTests.
</comment><comment author="nik9000" created="2015-09-23T12:25:14Z" id="142583694">Added 2.1 and 2.0 to this - I think its worth getting a better error message in for 2.0.
</comment><comment author="jasontedor" created="2015-09-23T13:58:10Z" id="142609757">@nik9000 I added a unit test in 72b67d17a90049d0aea4fa325922d814d4d59ba9.
</comment><comment author="nik9000" created="2015-09-23T14:01:16Z" id="142610565">LGTM.
</comment><comment author="jasontedor" created="2015-09-23T14:28:11Z" id="142618502">Thanks for reviewing @clintongormley and @nik9000. This is integrated into [master](8d1d8f9c46e03dcf553f879e399486f4430e7cd9), [2.0](4faaaa021e82f61c9f76b126503d023d19739487) and [2.x](d9e7193a7efd8c9679e281fcfbd2c37fbe4b7392).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Embedded node on-demand thread creation leaks caller context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13736</link><project id="" key="" /><description>For the local node requests get directly submitted to a TheadPoolExecutor which, if not exhausted, will create new Workers/Threads on demand. Thread initialization leaks the original callers TCCL, AccessControlContext and InheritableThreadLocals. Depending on the environment this is a first class classloader leak.
As the worker can be reused from a totally different context I don't think there is any legitimate use for this context, but this might be different for the server threads.

For this usage pattern, the ThreadFactory should make sure not to leak any caller context. The first two leaks can be circumvented by unsetting or setting the TCCL, running in doAsPrivileged with a new AccessControlContext. Unfortunately there is no API to reset InheritableThreadLocals, that can only be achieved using reflection.

As I'm not sure about potential side effects compatability and performance-wise, what I would suggest is making the thread factory configurable. 
</description><key id="107892712">13736</key><summary>Embedded node on-demand thread creation leaks caller context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mbechler</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2015-09-23T10:38:41Z</created><updated>2015-09-24T06:38:54Z</updated><resolved>2015-09-23T13:48:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-23T13:28:10Z" id="142599616">Hi @mbechler 

Which version are you referring to?
</comment><comment author="clintongormley" created="2015-09-23T13:28:39Z" id="142599757">@rmuir this sounds like your territory?
</comment><comment author="rmuir" created="2015-09-23T13:48:42Z" id="142606866">there is no plan to allow permissions to setContextClassLoader anywhere in elasticsearch at the moment, it is denied, given that createClassLoader is also allowed. Maybe it changes in the future, but the combination of the two is too much.
</comment><comment author="mbechler" created="2015-09-23T14:23:13Z" id="142617217">I'm not really sure I understand what you mean. This is not about the standalone server (where the described issue most certainly does not matter at all) but an embedded instance. Also it's not about having the ability to set a context classloader for some piece of code but about not leaking one from calling code. Also it's not only the context classloader that is a problem.

As I said, I was not really aware of the side-effects, if standalone is running with a SecurityManager (as I understand your comment) doing so generally is certainly out of the question.

But, right now I do have to wrap all my calls into the node client with a construct that drops TCCL, ACC and InheritableThreadLocals or otherwise I end up with massive classloader leaks. Scenario is OSGI where I'm running an elasticsearch node and a webapp container side-by-side - admittedly exotic - but the current code is conceptually wrong (one might very well argue this is a design flaw in the standard library - but fixing that will probably take years). 

If one was able to override the ThreadFactory this could be fixed for environments where it matters in just a few lines of code (one might even ship an alternative ThreadFactory implementation).
</comment><comment author="rmuir" created="2015-09-23T14:47:41Z" id="142625199">Standalone is running with security manager and that is certainly the case we are trying to improve.

Honestly we don't test running any other way (container or anything) and I do not even know if it is possible or works anymore at all. 

I don't think the code is "wrong" because I don't think elasticsearch should be this wonder-do-it-all-jar-file, that is a standalone server to some people, a java client to other people, and then N exotic cases (like embedded servers) to others. It is too many pressures tugging at too many directions and it causes the server to be too flaky, too lenient, and too complicated. This is just my opinion.
</comment><comment author="mbechler" created="2015-09-24T06:38:54Z" id="142827800">Well, embedded node is the mode of operation suggested by the docs for java clients, so that cannot be too exotic. Everything else it takes to get hit by this is client code that is more short-lived (e.g. get redeployed) than the node.

I'd still argue that the code is wrong, it is only correct by accident if there is a single calling context. Processing one callers requests with the AccessControlContext, TCCL, ... of another caller most certainly sounds not correct to me.

Again, I would already be very happy if I'm able to fix this for myself more easily which can be achieved by making the ThreadFactory configurable. I would even be willing to provide a patch, given it generally would be considered for merging.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reduced the number of ClusterStateUpdateTask variants</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13735</link><project id="" key="" /><description>In the past ClusterStateUpdateTask was an interface and we had various derived marker interfaces to control behavior. Since then we moved ClusterStateUpdateTask to be an abstract class but we kept the old hierarchy of implementations. All of those (but the AckedClusterStateUpdateTask) can be folded into ClusterStateUpdateTask, adding correct default behavior.
</description><key id="107891755">13735</key><summary>Reduced the number of ClusterStateUpdateTask variants</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T10:33:28Z</created><updated>2015-09-23T13:31:46Z</updated><resolved>2015-09-23T11:47:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-23T10:33:46Z" id="142560342">@jasontedor can you take a look?
</comment><comment author="jasontedor" created="2015-09-23T11:13:32Z" id="142570935">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>no_match_size ignored using the fast-vector-highlighter on a field containing an array of strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13734</link><project id="" key="" /><description>As demonstrated in the gist, no_match_size setting is ignored and no highlight field is returned when the field on which highlighting is requested has no match _and_ the field contains an array of strings instead of a single string.
https://gist.github.com/edwardsmit/2ec8506db2e4402024dd
</description><key id="107884800">13734</key><summary>no_match_size ignored using the fast-vector-highlighter on a field containing an array of strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">edwardsmit</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2015-09-23T09:53:31Z</created><updated>2016-11-25T16:02:00Z</updated><resolved>2016-11-25T16:02:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T13:08:54Z" id="142593883">Confirmed. I don't know when it'll get a fix but it certainly is a bug.

I believe `no_match_size` was supposed to return the first n characters of the first entry in the array if. At least that is how I remember it but its been a long while since I wrote it.

Your gist is wonderful! If all issues had such a thing I'd be very happy.

For next time its: add the `?pretty` option to the requests so they are a little easier to read and/or end them in `;echo`. Without the trailing newline things get messy.

I usually prefer to put the `curl -XDELETE localhost:9200/indexname` as the first line rather than the last. It'll fail the first time unless I have an index with that name in which case it'll clean up the last index.

Thanks for filing this.
</comment><comment author="nik9000" created="2015-09-23T13:12:28Z" id="142594880">I added 3.0.0 because I confirmed it on my local checkout which happened to be the master branch. Did you confirm this on 1.7?
</comment><comment author="edwardsmit" created="2015-09-23T13:14:32Z" id="142595657">Yes I've verified this on 1.7.2
</comment><comment author="nik9000" created="2015-09-23T13:22:03Z" id="142597831">&gt; Yes I've verified this on 1.7.2

So marked. And I also added 2.0.0 and 2.1.0 because I don't imagine its in 1.7 and 3.0 and not in 2.x.
</comment><comment author="clintongormley" created="2016-11-25T15:00:43Z" id="262973548">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update core-types.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13733</link><project id="" key="" /><description>Fix a trivial typo hes --&gt; has
</description><key id="107884503">13733</key><summary>Update core-types.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mcku</reporter><labels><label>docs</label><label>v1.7.3</label></labels><created>2015-09-23T09:51:10Z</created><updated>2015-09-25T12:23:45Z</updated><resolved>2015-09-25T12:17:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-23T12:36:49Z" id="142585579">Nice! Thanks for that.

@mcku our CLA checker is choking on this because you used a different email to sign the commit than you signed the CLA. Which I'm told is fine if you put the email that you used on the commit in your github profile. Can you try that and leave a comment to trigger the check again?

Sorry for the trouble.
</comment><comment author="mcku" created="2015-09-24T19:11:57Z" id="143022453">Hi Nik! 

Could you check that again? I liked the CLA and stuff. You guys are doing serious things.. hope to send more useful commits in the future. 

Take care..
</comment><comment author="nik9000" created="2015-09-24T19:19:48Z" id="143024072">Cool! Yes, the CLA check passed. Thanks so much. I'll try to merge this this afternoon.
</comment><comment author="nik9000" created="2015-09-25T12:20:59Z" id="143201827">Thakns @mcku! I've merged it.
</comment><comment author="nik9000" created="2015-09-25T12:23:45Z" id="143202926">Looks like it doesn't apply to 2.x or master because those docs have been rewritten. So no cherry-picking this morning.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function_score's score_mode of `avg` doesn't seem to work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13732</link><project id="" key="" /><description>The `avg` `score_mode` for a `function_score` that has a single `function` with only a `filter` doesn't seem to actually compute the average.

If I replace the `avg` `score_mode` with `sum` I get the expected result.

This is a gist demonstrating the issue: https://gist.github.com/astefan/cb200eab777ded2b25df
Tested with ES 1.7.1.
</description><key id="107884433">13732</key><summary>function_score's score_mode of `avg` doesn't seem to work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">astefan</reporter><labels><label>:Query DSL</label><label>feedback_needed</label></labels><created>2015-09-23T09:50:39Z</created><updated>2016-01-28T18:07:02Z</updated><resolved>2016-01-28T18:06:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-10-06T11:04:04Z" id="145823122">At first I thought it is a bug too but this is actually expected. The avg is not computed as the avg of the scores like

```
(x1*w1 + x2*w2 + ... + xn*wn)/(n)
```

but instead we compute the weighted average:

```
(x1*w1 + x2*w2 + ... + xn*wn)/(w1 + w2 + ... + wn)
```

This was changed here: https://github.com/elastic/elasticsearch/pull/9004
The weighted average in a way computes the regular average of the `x`s without the weights but _pulls_ it a little closer to values that have a high weight.

I drew an image to explain what the weighted average does:

![avg-explain](https://cloud.githubusercontent.com/assets/4320215/10306908/7e259de6-6c2a-11e5-8ff9-12bb05f542c8.jpg)

It seems counterintuitive at first but it seems to make more sense and so far we only had a request for the weighted average but none for plain average. Do you need something different?
</comment><comment author="clintongormley" created="2016-01-28T18:06:56Z" id="176310061">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate locale parameters in query_string and simple_query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13731</link><project id="" key="" /><description>The configured analysis chain should be used instead.

Relates to #13229
</description><key id="107876643">13731</key><summary>Deprecate locale parameters in query_string and simple_query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2015-09-23T09:04:44Z</created><updated>2015-10-06T08:49:28Z</updated><resolved>2015-10-05T11:50:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-23T17:28:41Z" id="142672034">@rmuir can you have a look? if this looks good I will put up another PR to remove `locale` for 3.0.
</comment><comment author="javanna" created="2015-10-05T09:51:01Z" id="145483582">I updated the PR to add some docs, which I had forgotten about in the first place. Reviews are welcome.
</comment><comment author="javanna" created="2015-10-05T10:03:10Z" id="145485474">while looking into removing these as previously discussed, I bumped into `org.apache.lucene.queryparser.classic.QueryParserBase#setLocale` in lucene. I am then wondering why we shouldn't expose this setting if lucene allows to set it. Are we sure this is the best way forward? It is hard for me to tell. Will leave a comment also on the original issue.
</comment><comment author="rmuir" created="2015-10-05T11:03:08Z" id="145496028">I think the issue is that elasticsearch does not in fact use the configured analysis chain for wildcards, range queries, fuzzy queries, etc.

Lucene factories are annotated with the information needed to do this properly. I have explained the issue here: https://github.com/elastic/elasticsearch/issues/9978
</comment><comment author="javanna" created="2015-10-05T11:50:57Z" id="145504388">thanks for the explanation @rmuir that makes sense to me. I am closing this PR then, we can't move on till we have implemented what is described in #9978.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>doc() context for script field on inner hit is restricted to type(s) of top level / outer request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13730</link><project id="" key="" /><description>When adding a script field to an inner hit, I'd expect to be able to access the fields / reader for in the context of that inner hit. Alas, the result of `doc().types` (method of `AbstractSearchScript`) only contains the type of the top level hit, and calls like `docFieldLongs("child_attr")` result in errors of the form _No field found for [child_attr] in mapping with types [parent_type]_.

Creating a full working example is a bit involved as it includes a server-side plugin with a native script, but the search request from the client looks like this:

``` java
new SearchRequestBuilder(client)
    .setSearchType(SearchType.QUERY_THEN_FETCH)
    .setIndices("an-index")
    .setTypes("parent_type")
    .addInnerHit("child", new InnerHit()
        .setType("child")
        .addScriptField("scripted", "native", "my_script");
```

(In reality, there's a sort and size and from here and there)

The following then triggers the error for me:

``` java
@Override
public Object run() {
    return docFieldLongs("child_attr").getValue();
}
```

Is the result of `doc()` (what `docFieldLongs()` uses) as intended here or should that refer to the doc of type `child` that was matched as an inner hit?
</description><key id="107869408">13730</key><summary>doc() context for script field on inner hit is restricted to type(s) of top level / outer request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akaIDIOT</reporter><labels /><created>2015-09-23T08:06:34Z</created><updated>2015-09-23T13:07:06Z</updated><resolved>2015-09-23T13:07:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="akaIDIOT" created="2015-09-23T09:16:27Z" id="142537718">Update: the doc id for `doc()` seems correct: it refers to the doc that has the inner hit. The `types` arg to `DocLookup` contains only the type supplied with the top level search request (`parent_type` in the example), _not_ the type explicitly set on the inner hit (`child` in the example).
</comment><comment author="clintongormley" created="2015-09-23T13:07:06Z" id="142593444">Hi @akaIDIOT 

This has been fixed in 2.0.0-beta1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop old template syntax BWC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13729</link><project id="" key="" /><description>today we have a gazilion ways to specify templates and the parsing is a nightmare. We should drop the 1.x syntax in 3.0 but it might have implications on scripts store in the `.script` indes. We should find out if we need to `upgrade` those scripts in 2.x
</description><key id="107868518">13729</key><summary>Drop old template syntax BWC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Scripting</label><label>adoptme</label><label>blocker</label><label>enhancement</label><label>v5.0.0-alpha5</label></labels><created>2015-09-23T07:58:31Z</created><updated>2016-07-13T13:28:38Z</updated><resolved>2016-07-13T13:28:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MaineC" created="2015-09-24T09:33:13Z" id="142868006">We discussed this yesterday - @colings86 @s1monw I think the conclusion was that this is not an issue as only the template itself is stored in the index, but not the rest of the request. Correct?
</comment><comment author="clintongormley" created="2015-09-25T10:54:34Z" id="143186681">@MaineC has the bwc layer already been removed in master? If not, then this issue should still be open.
</comment><comment author="colings86" created="2015-09-25T10:58:45Z" id="143187239">I think this should still be open. The PUT index script API is unaffected by the old/new style of specifying scripts, but the APIs that use scripts (aggs, update, template search etc.) still have code in the parser to parser the old style. I think that is what we need to remove the resolve this issue?
</comment><comment author="clintongormley" created="2015-11-29T11:56:14Z" id="160410828">Related to https://github.com/elastic/elasticsearch/issues/14837
</comment><comment author="clintongormley" created="2016-02-14T17:30:12Z" id="183933681">Related to #16651
</comment><comment author="martijnvg" created="2016-07-07T09:08:35Z" id="231024333">When upgraded to 5.x, the indexed scripts have to be [re-added manually](https://www.elastic.co/guide/en/elasticsearch/reference/master/breaking_50_scripting.html#_python_migration_script) already. So if we just drop the 1.x syntax now, that wouldn't be a big of a deal?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix position-increment-gap doc example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13728</link><project id="" key="" /><description>Old example would throw an exception when using `slop` option, change it to use `query` element. 
</description><key id="107864356">13728</key><summary>Fix position-increment-gap doc example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>docs</label></labels><created>2015-09-23T07:22:00Z</created><updated>2015-09-23T15:06:14Z</updated><resolved>2015-09-23T15:05:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Met split-brain issue when the azure vm connection was lost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13727</link><project id="" key="" /><description>Hi all,

Recently we met 3 times split-brain issue of my elasticsearch cluster hold on azure VM.

I have three nodes, each node can be data//master node.  I have set the discovery.zen.minimum_master_nodes to 2 and use azure discovery plugin.

Recently azure often maintain their host machines, this cause the network instability between VMs and also caused split-brain issue for my elasticsearch cluster.

We found that the node which has already joined in to one master can be forced to rejoin to another master, and then the split-brain issue happened!

From the log we can see that:
    09-22 16:38:27, node1 lost connection.
    09-22 16:45:15, node2 lost connection, node 3 became master(node1 has recovered)
    09-22 16:45:24, node3 lost connection, node 2 became master.
    09-22 16:45:43, node3 recovered and became master again!.

Below is the error or warns for these three nodes when is split-brain issue happened:
##### Node1(search-prod-wus1):

[2015-09-22 16:45:15,618][INFO ][cluster.service          ] [caps-prod-wus1] master {new [caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]], previous [caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]]}, removed {[caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]],}, reason: zen-disco-receive(from master [[caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]]])
[2015-09-22 16:45:20,566][WARN ][index.store              ] [caps-prod-wus1] [32c3c289eef54e42be5913a63dfd280a][2] Can't open file to read checksums
java.io.FileNotFoundException: No such file [_89i0_es090_0.doc]
[2015-09-22 16:45:21,019][WARN ][index.store              ] [caps-prod-wus1] [32c3c289eef54e42be5913a63dfd280a][2] Can't open file to read checksums
java.io.FileNotFoundException: No such file [_89i0_es090_0.doc]
[2015-09-22 16:45:24,364][INFO ][cluster.service          ] [caps-prod-wus1] master {new [caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]], previous [caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]]}, removed {[caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]],}, added {[caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]],}, reason: zen-disco-receive(from master [[caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]]])
[2015-09-22 16:45:43,059][INFO ][cluster.service          ] [caps-prod-wus1] master {new [caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]], previous [caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]]}, removed {[caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]],}, added {[caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]],}, reason: zen-disco-receive(from master [[caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]]])
#### Nodes(search-prod-wus2):

[2015-09-22 16:38:37,162][WARN ][action.index             ] [caps-prod-wus2] Failed to perform index on remote replica [caps-prod-wus1][vcBwbCeJTQ61Mw2aOqaflg][search-prod-wbp][inet[/10.3.0.5:9300]][32c3c289eef54e42be5913a63dfd280a][3]
org.elasticsearch.transport.NodeDisconnectedException: [caps-prod-wus1][inet[/10.3.0.5:9300]][index/replica] disconnected
[2015-09-22 16:38:37,162][WARN ][cluster.action.shard     ] [caps-prod-wus2] [32c3c289eef54e42be5913a63dfd280a][3] sending failed shard for [32c3c289eef54e42be5913a63dfd280a][3], node[vcBwbCeJTQ61Mw2aOqaflg], [R], s[STARTED], indexUUID [vLXa7OmETyGPGrI82L4SVg], reason [Failed to perform [index] on replica, message [NodeDisconnectedException[[caps-prod-wus1][inet[/10.3.0.5:9300]][index/replica] disconnected]]]
[2015-09-22 16:38:37,162][WARN ][action.index             ] [caps-prod-wus2] Failed to perform index on remote replica [caps-prod-wus1][vcBwbCeJTQ61Mw2aOqaflg][search-prod-wbp][inet[/10.3.0.5:9300]][32c3c289eef54e42be5913a63dfd280a][3]
org.elasticsearch.transport.SendRequestTransportException: [caps-prod-wus1][inet[/10.3.0.5:9300]][index/replica]
[2015-09-22 16:38:37,162][WARN ][cluster.action.shard     ] [caps-prod-wus2] [32c3c289eef54e42be5913a63dfd280a][3] sending failed shard for [32c3c289eef54e42be5913a63dfd280a][3], node[vcBwbCeJTQ61Mw2aOqaflg], [R], s[STARTED], indexUUID [vLXa7OmETyGPGrI82L4SVg], reason [Failed to perform [index] on replica, message [SendRequestTransportException[[caps-prod-wus1][inet[/10.3.0.5:9300]][index/replica]]; nested: NodeNotConnectedException[[caps-prod-wus1][inet[/10.3.0.5:9300]] Node not connected]; ]]

[2015-09-22 16:45:24,005][INFO ][cluster.service          ] [caps-prod-wus2] removed {[caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]],}, reason: zen-disco-node_failed([caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]]), reason transport disconnected (with verified connect)
#### Node3(search-prod-wus3):

[2015-09-22 16:38:38,677][WARN ][action.index             ] [caps-prod-wus3] Failed to perform index on remote replica [caps-prod-wus1][vcBwbCeJTQ61Mw2aOqaflg][search-prod-wbp][inet[/10.3.0.5:9300]][32c3c289eef54e42be5913a63dfd280a][2]
org.elasticsearch.transport.SendRequestTransportException: [caps-prod-wus1][inet[/10.3.0.5:9300]][index/replica]
[2015-09-22 16:45:15,156][INFO ][discovery.azure          ] [caps-prod-wus3] master_left [[caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]]], reason [transport disconnected (with verified connect)]
[2015-09-22 16:45:15,156][INFO ][cluster.service          ] [caps-prod-wus3] master {new [caps-prod-wus3][qQYWMttZScaqbAfBPkV5gw][search-prod-wbu][inet[/10.3.0.6:9300]], previous [caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]]}, removed {[caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]],}, reason: zen-disco-master_failed ([caps-prod-wus2][9v_FW_4AQ7KQ4fA5CLPdTg][search-prod-wus][inet[/10.3.0.4:9300]])

[2015-09-22 16:45:43,126][WARN ][index.store              ] [caps-prod-wus3] [0222d67c4146405497a70df65629e634][0] Can't open file to read checksums
java.io.FileNotFoundException: No such file [_3app.fdt]
</description><key id="107851618">13727</key><summary>Met split-brain issue when the azure vm connection was lost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">OsmondX</reporter><labels /><created>2015-09-23T05:19:14Z</created><updated>2015-09-23T08:45:09Z</updated><resolved>2015-09-23T08:45:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-23T07:12:22Z" id="142517094">Thanks for reporting. Which is ES version are you using?
</comment><comment author="OsmondX" created="2015-09-23T08:41:52Z" id="142530867">My elastic search version is 1.3.2.
</comment><comment author="OsmondX" created="2015-09-23T08:42:25Z" id="142530951">This also happened to my 1.3.9 version of elastic search cluster
</comment><comment author="bleskes" created="2015-09-23T08:44:28Z" id="142531672">I see. So I suspect this is #2488, which was fixed in 1.4 . I suggest you upgrade (to 1.7.2) as soon as possible. Many many things have been fixed since 1.3.2.
</comment><comment author="bleskes" created="2015-09-23T08:45:08Z" id="142531900">I'm closing this now. Please reopen if it happens again after upgrading... 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Factor expressions scripts out to lang-expression plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13726</link><project id="" key="" /><description>This is part of moving script _impls_ out to plugins.

See https://github.com/elastic/elasticsearch/issues/13725 for the motivation.

This one is a pretty easy win, it means removing conflicts for antlr, asm, asm-commons, etc for users, and is a step towards containing classloader security.
</description><key id="107831083">13726</key><summary>Factor expressions scripts out to lang-expression plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T00:41:57Z</created><updated>2015-10-01T04:00:33Z</updated><resolved>2015-09-23T11:32:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-23T03:24:48Z" id="142479939">I left unrelated comments mostly. I think it's good but I think we should have another reviewer.
</comment><comment author="rjernst" created="2015-09-23T07:12:10Z" id="142517070">I fixed some of the remaining tests to use mocks, so further work pulling out script engines should be a little bit easier. LGTM.
</comment><comment author="s1monw" created="2015-09-23T07:51:29Z" id="142522522">LGTM 2
</comment><comment author="rmuir" created="2015-10-01T04:00:33Z" id="144613926">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add the ability to "pre-bundle" plugins with the distribution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13725</link><project id="" key="" /><description>Things like script engines need to be pulled out into plugins, even if we want to ship them by default with our distribution:
- removes dependences from elasticsearch-core (In case its used e.g. as node client or whatever)
- even pre-bundled, such dependencies are then in isolated classloaders (e.g. some other plugin can use antlr 2.0 or asm 7.0 or whatever and that is fine, since plugins are isolated from each other)
- allows us to better secure things like script engines that do special things.
- plugin mechanism is better than tossing jars around: supports classloader isolation, we already can support unit and integration tests, we can support running these from IDEs, we can support them in our security framework, has additional metadata and compatibility checks, ...
- gives us more possibilities, like allowing users to uninstall stuff, or create "minimal" distribution, OEM packaging, or whatever in the future

I have this as a blocker issue for 3.0 after we do some basic refactoring. It is assigned to me and I will not drop it. However, I would like to delay it for now: If we are going to do a big build refactoring, it might be best to wait for gradle. I don't want to spend days coercing maven poms into doing strange things if there is an easier way. 
</description><key id="107831005">13725</key><summary>Add the ability to "pre-bundle" plugins with the distribution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>blocker</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-23T00:40:54Z</created><updated>2015-12-21T15:57:08Z</updated><resolved>2015-12-21T15:57:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2015-09-23T05:05:33Z" id="142492039">+1 on all of the above
</comment><comment author="rmuir" created="2015-10-01T06:57:54Z" id="144638256">I set marked this issue for 2.x as well.
</comment><comment author="rmuir" created="2015-12-04T08:42:18Z" id="161909456">See https://github.com/elastic/elasticsearch/pull/15233 for the 3.0/gradle PR
</comment><comment author="s1monw" created="2015-12-21T14:33:38Z" id="166318384">@rmuir given that https://github.com/elastic/elasticsearch/pull/15303 is closed can we close this one too?
</comment><comment author="rmuir" created="2015-12-21T15:57:08Z" id="166339807">closed by #15303
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and ban ImmutableMap#entrySet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13724</link><project id="" key="" /><description>Banning `ImmutableSet` outright is too much to do all at once - this starts
the process by banning `ImmutableMap#entrySet` - one of the more common ways
that `ImmutableSet`s come up. It then starts to remove calls to
`ImmutableMap#entrySet` by changing declarations from `ImmutableMap` to `Map`.

Unfortunately this process is like pulling on a long, windy string and one
declaration change requires another which requires 5 more which in turn
require another few. So there are lots of scattered changes to do this.

As such, to keep these commits manageable they only remove `ImmutableMap` from
the signatures that are needed for `entrySet` and make no effort to stop using
`ImmutableMap` internally. Removing the usages of `ImmutableMap` complicates
immutability guarantees and should be done separately.
</description><key id="107800621">13724</key><summary>Remove and ban ImmutableMap#entrySet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T20:57:02Z</created><updated>2015-09-25T10:11:44Z</updated><resolved>2015-09-23T15:10:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-22T20:57:27Z" id="142419403">@jasontedor, this is the start of the ImmutableSet work.
</comment><comment author="nik9000" created="2015-09-22T21:00:24Z" id="142420422">Note to any reviewers: I feel like I've played a bit fast a loose with immutability guarantees in places. ImmutableMap ensured returns were immutable all through the call stack while `Collections.unmodifiableMap` is really only enforced at map construction time and this commit feels like it weakens the immutability guarantees somewhat. I suspect that is just the price of dropping guava though.
</comment><comment author="jasontedor" created="2015-09-23T10:03:19Z" id="142551885">@nik9000 Yeah, the change from `ImmutableX` to `unmmodifiableX` that you correctly point out is a known loss. In most cases, `ImmutableX` wasn't being exposed through the API so the immutability guarantees of one over the other should not have been being relied upon. If we do end up needing the stronger immutability guarantees, we will have to revisit.
</comment><comment author="jasontedor" created="2015-09-23T10:05:10Z" id="142552925">@nik9000 Thank you tackling this one, this looks great. Left a few minor comments about comments, but otherwise LGTM. Push at your leisure. :)
</comment><comment author="nik9000" created="2015-09-23T15:09:52Z" id="142633876">Squashed and rebased and caught some more tricksy `ImmutableMap#entrySet` calls that snuck around my first cull.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: Allocation setting to disable new shard allocation on a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13723</link><project id="" key="" /><description>The request is for an allocation setting that can be used to prevent new shards from being allocated on a node while still allowing for rebalancing. 
A use case for this setting is when adding a new node. Should new indexes be created before the cluster has balanced the new node could end up with a disproportionate amount of new shards. In a heavily loaded cluster this could cause stability issues.   It would be preferable to have a setting that would allow the node to participate in rebalancing while not being allocated new shards. 
</description><key id="107786785">13723</key><summary>Feature Request: Allocation setting to disable new shard allocation on a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lb425</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-09-22T19:43:46Z</created><updated>2016-01-28T18:04:23Z</updated><resolved>2016-01-28T18:04:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-23T12:43:20Z" id="142586631">I'm not really sure this is an issue..  For example, I created a whole bunch of indices on 5 nodes, disabled allocation and rebalancing, created a new index (which wasn't allocated), then added a new node.

I reenabled rebalancing. No rebalancing or allocation of the new index occurred until I allowed at least allocation of new primaries, all of which were allocated to the new node.  

Then I enabled allocation all allocation (which allowed the replicas to be allocated).   Shards from the new node were moved to other nodes, but it preferred the smaller shards.  This seems OK to me?

As soon as you start adding special rules and exclusions you run into situations where shards can't be properly allocated, which is a much bigger concern for me.
</comment><comment author="lb425" created="2015-09-28T20:20:24Z" id="143863237">I'm sorry if there was confusion. I'm not trying to report a bug or an issue, because I believe everything is working how it's supposed to, I'm just trying to make a feature request.

On your 5 node cluster were you ingesting data?  The issue we have is that the new shards are all allocated to one, or a small number of nodes, which then are indexing all the incoming data as well as having shards balanced to them. 

What I'm seeing is that when a new indices are created for our time series data they are all allocated to the single newly added node. These indices are immediately indexing documents since they were created in response to time series data which continues until the time that another index is created (usually a day). (If this is the incorrect way to handle time series data then perhaps I could change that instead of ask for this feature)

In this case one node does the work of 8 nodes (since all shards are allocated to it). Should this have been the behavior I wanted I would have created indexes with just one shard and would probably be ok with whats going on. 

Unfortunately in my environment this causes issues, the worst being timeouts that cause nodes to drop out. To handle it now I create a large number of empty indexes that are allocated to the new nodes. I then slowly delete them allowing new shards to be allocated in a more uniform manner.

The feature request is for a cluster setting that prevents new shards from being allocated to a node while still allowing shards to be relocated to it as part of balancing. Something similar can be done be setting allocation rules on all the new indexes, but it would be preferable to have one cluster setting instead of a confusing mess of index level rules.

Some information about my environment:
16 Nodes, split into 2 groups
3600 indices
300GB of time series data per day (sometime being written to just one node in a group)
</comment><comment author="ldimans" created="2015-09-28T20:39:27Z" id="143867276">+1
</comment><comment author="lb425" created="2015-10-03T23:12:25Z" id="145296681">Here is a screen capture from Kopf showing how shards were allocated on an unbalanced cluster. The columns are Indices and the rows are nodes.

&lt;img width="1281" alt="unbalancedclusterallocation" src="https://cloud.githubusercontent.com/assets/2752044/10265554/df214b94-69f1-11e5-89aa-64625eb4feb0.png"&gt;
</comment><comment author="lb425" created="2015-10-03T23:13:02Z" id="145296724">Accidentally closed.
</comment><comment author="wilsonlindelof" created="2015-10-06T17:30:14Z" id="145935931">+1
</comment><comment author="clintongormley" created="2016-01-28T18:04:23Z" id="176308541">I've been thinking about this again.  I'm just not sure this is possible. How do you know when a node is new (or still new), what happens if you can't allocate new shards to any other nodes, etc.

I think a better approach would be to play with the [`total_shards_per_node` setting](https://www.elastic.co/guide/en/elasticsearch/reference/2.1/allocation-total-shards.html) which would prevent all of the shards being allocated to the same node.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Top level inner hits not using filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13722</link><project id="" key="" /><description>When specifying a query for top level inner hits, using a filter does not appear to have any effect on the returned results.  

Script to set up index

```
#!/bin/sh

node="http://localhost:9200"
curl  -XDELETE $node/test1

curl  -XPUT $node/test1

# create mapping for test1
curl  -XPOST $node/test1/_mapping/Parent -d '
{
 Parent: {
  _routing: { required : true },
  properties: {
   display : { "type": "string" }
  }
 }
}'

curl  -XPOST $node/test1/_mapping/Child -d '
{
 Child : {
   _parent : { type : "Parent" },
   _routing : { required : true },
   properties: {
    display : { "type": "string" },
    date: {
      "type": "date",
      "format": "dateOptionalTime || M/d/Y k:m:s a || Y-M-d'"'"'TH:m:s.S || YYYYMMddHHms"
    }
  }
 }
}'

# Create a parent object
curl  -XPUT "$node/test1/Parent/1?routing=1" -d'
{
    "display" : "Parent 1"
}'

# Create 10 child objects, with dates from 1930 to 1939
for i in 0 1 2 3 4 5 6 7 8 9 ; do
curl  -XPUT "$node/test1/Child/1$i?parent=1&amp;routing=1" -d'
{
    "display" : "Child 1'$i'",
    "date" : "193'$i'-01-01"
}'
done

```

Using a query gives the expected result:

```
GET /test1/Parent/_search
{
 "query":{
   "has_child":{
     "type":"Child",
     "query":{ 
       "filtered": {
         "query": {"match_all":{} },
         "filter": {
           "range": {
             "date": {
               "gte": "1932-01-01"
             }
           } 
         }
       }
     }
   }
 }, 
 "inner_hits" : {
  "children" : {
   "type" : { 
    "Child" : { 
     "query" : {       
       "match":{"display": "Child 14"}
     }, 
     "size":10
    }
   }
  }
 }
}
```

However adding a filter to the query returns all the inner hits, not just those that are greater than the date specified in the range.

```
GET /test1/Parent/_search
{
 "query":{
   "has_child":{
     "type":"Child",
     "query":{ 
       "filtered": {
         "query": {"match_all":{} },
         "filter": {
           "range": {
             "date": {
               "gte": "1932-01-01"
             }
           } 
         }
       }
     }
   }
 }, 
 "inner_hits" : {
  "children" : {
   "type" : { 
    "Child" : { 
     "query" : {       
      "filtered": {
       "query": {"match_all":{} },
       "filter": {
        "range": {
         "date": {
          "gte": "1938-01-01"
         }
        } 
       }
      }
     },
     "size":10
    }
   }
  }
 }
}
```

It's hard to tell from the current documentation if this is the expected behavior or not.
</description><key id="107770952">13722</key><summary>Top level inner hits not using filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nemonster</reporter><labels /><created>2015-09-22T18:21:07Z</created><updated>2015-09-23T11:57:34Z</updated><resolved>2015-09-23T11:57:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-23T11:57:34Z" id="142577873">Hi @nemonster 

You didn't specify which version of Elasticsearch you're using, but this works correctly in 2.0.0-beta2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Leaky PatternTokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13721</link><project id="" key="" /><description>Analyzer component reuse does not interact well with `PatternTokenizer` (and possibly other tokenizers, but I haven't looked into them). https://github.com/elastic/elasticsearch/pull/6792 changed the `PatternAnalyzer` from lucene's deprecated version (that packaged its own `PatternTokenizer`) to a standalone `PatternTokenizer`. Under the hood, lucene uses `StringBuilder` to store field values during tokenization but never reclaims storage (e.g. with `trimToSize()`). This causes PatternTokenizer to grow to the largest field the Tokenizer encounters.

The use case we're encountering this issue with is using a custom `PatternTokenizer` for indexing logs, creating indices corresponding to log timestamp. We're running 1.6.2, but so far I don't see anything that this has been fixed in later versions. This really hurts with a large number of indices.

I was looking into closing Analyzers when indices go inactive, but it's been a bit rough trying to find the right place to put it. I'm sure there's probably a better solution as well. Reverting https://github.com/elastic/elasticsearch/pull/6792 is an option if lucene's PatternAnalyzer can be vendored (was removed in 5.x).

In the meantime, are there any short term solutions to explicitly freeing up Analyzers? `index.blocks.read_only` looked promising, but from what I can tell, it only causes requests to be rejected and doesn't release Analyzers (though I might be mistaken). Closing and reopening indices isn't a particularly good option for our use case either.
</description><key id="107770818">13721</key><summary>Leaky PatternTokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">achow</reporter><labels /><created>2015-09-22T18:20:18Z</created><updated>2016-01-05T19:38:17Z</updated><resolved>2015-09-25T10:53:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-25T10:53:40Z" id="143186569">Closing in favour of https://issues.apache.org/jira/browse/LUCENE-6814
</comment><comment author="achow" created="2015-09-25T16:52:30Z" id="143278651">@clintongormley, that fix is only going 5.4 so only available for 2.0. I'd like to get a fix into 1.x as well.
</comment><comment author="achow" created="2015-10-01T21:23:55Z" id="144854491">@clintongormley I've opened https://github.com/elastic/elasticsearch/pull/13804 to try to get a fix into 1.x. Could you please take a look?
</comment><comment author="achow" created="2016-01-05T19:07:38Z" id="169099648">@mikemccand What's the status of forward porting this to 2.x?
</comment><comment author="mikemccand" created="2016-01-05T19:35:02Z" id="169106308">@achow https://github.com/elastic/elasticsearch/pull/14571 was pushed to ES 2.0.x and 2.1.x, and ES 2.2.0 will use Lucene 5.4.x which already has the fix from https://issues.apache.org/jira/browse/LUCENE-6814 so I think all is good?
</comment><comment author="achow" created="2016-01-05T19:38:16Z" id="169107161">@mikemccand Fantastic. Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid ForwardingSet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13720</link><project id="" key="" /><description>Removes CopyOnWriteHashSet, our only usage of ForwardingSet. We weren't
using it.

Related to #13224
</description><key id="107757766">13720</key><summary>Forbid ForwardingSet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Core</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T17:05:40Z</created><updated>2015-09-22T17:46:46Z</updated><resolved>2015-09-22T17:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-22T17:21:15Z" id="142354577">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During shard relocation some requests might fail to be sent to a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13719</link><project id="" key="" /><description>When shards relocate then there might be small window in time where requests fail to reach the relocating shard. This happens when a node that lags one cluster state behind has not realized yet that a node has relocated and the relocation source is already removed.
Below is a graphical representation of an example course of events. This caused us some trouble in test already because results were unexpected, see https://github.com/elastic/elasticsearch/issues/13266. It affects all actions that inherit from `TransportBroadcastAction`, `TransportBroadcastByNodeAction` and might also be problematic for others.  For example: an optimize request might never reach a shard if it is relocating, indices stats may report wrong statistics, see https://github.com/elastic/elasticsearch/issues/13266#issuecomment-138470051, etc.

We should check if we can get away with just sending requests to relocation targets too for the affected actions or if we need to implement these kind of requests as replication action  like we did for refresh and flush.

![recovery-issues](https://cloud.githubusercontent.com/assets/4320215/10023746/c4bbf246-6153-11e5-9fda-0b4cce1eab10.png)

![finally](https://cloud.githubusercontent.com/assets/4320215/10023912/bc99c3d0-6154-11e5-828c-a6ef07e6cdf1.png)

I: shard is relocating from n2 to n3
II: CS2 signals that n3 has started its shard and n2 can remove its own copy. shard on n2 is therefore closed. But n1 lags behind one cluster state and still expects an up and running primary on n2
III:  if now n1 sends an optimize, indices stats request or the likes, it will send the request to n2 (based on CS1) but that does not not have the shard anymore.
</description><key id="107747471">13719</key><summary>During shard relocation some requests might fail to be sent to a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Cluster</label><label>adoptme</label><label>bug</label><label>resiliency</label></labels><created>2015-09-22T16:09:10Z</created><updated>2016-01-28T17:55:23Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>LoggingRunnable.run should catch and log all errors, not just Exception?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13718</link><project id="" key="" /><description>Still digging on https://github.com/elastic/elasticsearch/issues/13487 ... and I think it's unlikely this is the cause ...

`ThreadPool.scheduleWithFixedDelay` wraps the incoming command with a `LoggingRunnable` which runs the command, but catching, suppressing and logging any exceptions.  This is important because the JDK's `ScheduledThreadPoolExecutor.schedulWithFixedDelay` will stop executing the task if an unhandled exception is hit.

But it only catches `Exception` ... I think maybe it should instead catch `Throwable`, so that e.g. `AssertionError` is also logged?

It's remotely possible this test failure is happening because some bad exception (subclassing Error) was hit and the thread is no longer checking for inactive indices ... but then I think the JDK would have sent that exception to stderr, so this is probably not it.
</description><key id="107743657">13718</key><summary>LoggingRunnable.run should catch and log all errors, not just Exception?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T15:49:01Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-23T08:24:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-22T15:50:49Z" id="142330816">Makes sense to me. At the root of a Runnable is the one place I've always excused catching Throwable but maybe someone with a more nuanced opinion than I can comment?
</comment><comment author="dakrone" created="2015-09-22T17:34:18Z" id="142357525">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create concurrent cache with flexible eviction policies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13717</link><project id="" key="" /><description>We need a cache that supports:
1. Concurrency
2. Size-based evictions
3. Time-based evictions
4. Manual invalidation
5. Removal notification
6. Cache statistics

This cache will replace the use of `com.google.common.cache.Cache`.

Relates #13224 
</description><key id="107737534">13717</key><summary>Create concurrent cache with flexible eviction policies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T15:19:30Z</created><updated>2015-10-09T15:56:35Z</updated><resolved>2015-10-09T15:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update phrase-suggest.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13716</link><project id="" key="" /><description>small sentence fix
</description><key id="107715910">13716</key><summary>Update phrase-suggest.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ulkas</reporter><labels><label>docs</label><label>v1.7.3</label></labels><created>2015-09-22T13:33:24Z</created><updated>2015-09-23T13:24:50Z</updated><resolved>2015-09-23T13:15:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-22T13:35:48Z" id="142291224">Much better! Thanks @ulkas. Can you sign the [CLA](https://www.elastic.co/contributor-agreement) so I can merge it? We need the CLA no matter how small the change.
</comment><comment author="ulkas" created="2015-09-22T13:36:08Z" id="142291291">yes, its done already. please merge
</comment><comment author="nik9000" created="2015-09-22T13:37:07Z" id="142291491">&gt; yes, its done already. please merge

Cool. I'll see if I can figure out why the automation is complaining.
</comment><comment author="ulkas" created="2015-09-22T13:42:49Z" id="142293050">dont bother, i didnt sign it before, just right after the automation complained and before you commented
</comment><comment author="nik9000" created="2015-09-22T14:36:09Z" id="142307012">&gt; dont bother, i didnt sign it before, just right after the automation complained and before you commented

Hmm - its not coming through. Is there a chance that you signed it with a different email then you signed the commit?
</comment><comment author="ulkas" created="2015-09-23T07:40:37Z" id="142520593">@nik9000 yes, the license was signed under my company email whereas my github account is personal. but at the signing page there was also a filed for my github username, so it should be able to match my credentials
</comment><comment author="nik9000" created="2015-09-23T12:14:15Z" id="142581430">I found the CLA documentation and manually added you to the checker database and it verified you on that side - this comment should trigger the check again. Regardless of what the check does you are in there and I'll merge this. I just want to see if the check goes green.
</comment><comment author="nik9000" created="2015-09-23T13:24:50Z" id="142598747">OK! Merged to 1.7 and cherry picked to 2.0, 2.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop support for CONF_FILE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13715</link><project id="" key="" /><description>Right now you can configure Elasticsearch to read its configuration from either a `CONF_FILE` or a `CONF_DIR` but the `CONF_FILE` directive isn't consistently implemented because its default (`$CONF_DIR/elasticsearch.yml`) is finicky to get right.

So lets remove it in the spirit of simplification. See discussion about it in #13687 for more.
</description><key id="107712094">13715</key><summary>Drop support for CONF_FILE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>breaking</label><label>v2.0.0</label></labels><created>2015-09-22T13:12:48Z</created><updated>2016-03-10T18:11:03Z</updated><resolved>2015-10-07T08:34:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-22T13:18:14Z" id="142286980">Are we only talking about CONF_FILE (which is only for services) or also about `-Des.conf=/path/to/conf.yml`?
</comment><comment author="nik9000" created="2015-09-22T13:22:52Z" id="142288269">Both seem redundant. `CONF_FILE` is the services parlance for `-Des.conf` I believe.
</comment><comment author="kimchy" created="2015-09-22T13:26:47Z" id="142289227">I am +1 on removing the conf file in ES itself, and just rely on custom conf dir, but then if we can have a good logging message when someone provides it to explain that this is not used, or maybe even exit (in Bootstrap?) if it is specified.
</comment><comment author="brwe" created="2015-09-25T15:06:32Z" id="143247300">Turns out it is not so easy to exit in Bootstrap when people run elasticsearch service. For services   convert a variable `CONF_FILE` to `-Des.default.conf` and [pass this to elasticsearch](https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L99). So we have to check in the init scripts too if `CONF_FILE` was set. Otherwise someone who configured that before will install elasticsearch, start as usual but the new installation will silently pick up the default conf.
For this reason I added a check in all init scripts in #13772  but that is very ugly as @spinscale pointed out.

We thought the following would be a trade-of:
- push the ugly changes in #13772 with all the additional checks to 2.0 and 2.x so that there are no bad surprises for people who use the `CONF_FILE` option and upgrade
- remove `CONF_FILE` and the additional checks completely on master
</comment><comment author="rjernst" created="2015-09-25T23:21:01Z" id="143374475">+1 to your plan @brwe 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: validate GeoShapeQueryBuilder strategy and relation setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13714</link><project id="" key="" /><description>Before the refactoring we didn't check any invalid settings for strategy and relation in the GeoShapeQueryBuilder. However, using SpatialStrategy.TERM and ShapeRelation.INTERSECTS together is invalid and we tried to protect against that in the validate() method.
This PR moves these checks to setter for strategy and relation and adds tests for the new behaviour.

Relates to #10217
</description><key id="107710027">13714</key><summary>Query Refactoring: validate GeoShapeQueryBuilder strategy and relation setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-09-22T13:02:51Z</created><updated>2016-03-11T11:50:42Z</updated><resolved>2015-09-25T08:58:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-22T13:05:25Z" id="142282768">I'm not super happy with this solution since it seems to me that relation() and strategy(), although related by this one illegal pairing, should not simply be combined in one setter like I did for demonstration purposes in this PR. However, I don't see many other options besides delaying the validation to the shard (in `toQuery`) at this point. Happy to get feedback on this.
</comment><comment author="cbuescher" created="2015-09-22T14:24:28Z" id="142304374">Thinking about this a bit longer, I would probably got for separate setters again, since the two parameters are not really tightly related. Many tests just set `relation` without even specifying the optional `stragegy`. So what if we:
- on setting `relation()`, check that either there was no strategy set before, otherwise implicitely set it to INTERSECTS.
- on setting `strategy()`, check that the chosen strategy is compatible with the currently set relation, otherwise throw IAE
  Any opinions about this solution?
</comment><comment author="javanna" created="2015-09-22T17:39:10Z" id="142358663">I left a small comment. Regarding your question, I am on the fence...it is not perfect either way to me but I don't know how to do it better, I wouldn't delay this validation to the data node. I think you can leave what you have now, that's fine.
</comment><comment author="cbuescher" created="2015-09-23T08:36:32Z" id="142530103">@javanna left a comment, I think I should leave that one without a null check for the reasons given above. Also squashed and rebased, should I wait another review round or do you think this is ready to go?
</comment><comment author="javanna" created="2015-09-23T08:38:29Z" id="142530327">@s1monw @colings86 what do you think about the validation in the setter depending on the current state of the object? I am ok with it...
</comment><comment author="colings86" created="2015-09-23T08:47:52Z" id="142532337">@javanna @cbuescher I think the logic needs to be a bit different from what is described above though. I think it should be:
- on setting relation(), check that either there was no strategy set before, if there is then:
  - if the strategy is RECURSIVE set the relation
  - if the strategy is TERM then:
    - if the relation in the parameter is INTERSECTS set the relation
    - if the relation is anything else throw an IllegalArgumentException
- on setting strategy(), check that the chosen strategy is compatible with the currently set relation, otherwise throw an IllegalArgumentException (basically throw the exception if the strategy is TERM and the relation is NOT EQUAL to INTERSECTS)
</comment><comment author="cbuescher" created="2015-09-23T09:26:49Z" id="142543246">@colings86 you're right, the description I added previously was off, but what you described is basically whats done in the code in this PR. Want to recheck and tell me if this looks okay to you?
</comment><comment author="colings86" created="2015-09-23T15:56:22Z" id="142646949">@cbuescher yep you are right, the code follows what I detailed above. I left a minor comment on the exception wording though
</comment><comment author="cbuescher" created="2015-09-24T12:34:06Z" id="142915318">Rebased and addressed the exception wording @colings86 mentioned. Anything else to add here?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update to randomizedtesting 2.1.17</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13713</link><project id="" key="" /><description>See changes: https://github.com/randomizedtesting/randomizedtesting/releases/tag/release%2F2.1.17

We can move off our snapshot copy now.
</description><key id="107700346">13713</key><summary>Update to randomizedtesting 2.1.17</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T12:00:54Z</created><updated>2016-01-22T18:43:07Z</updated><resolved>2015-09-22T13:25:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-22T12:13:19Z" id="142270564">LGTM
</comment><comment author="rmuir" created="2015-09-30T06:26:17Z" id="144294746">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add docs note about cloud.aws.region</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13712</link><project id="" key="" /><description>Closes #13668
</description><key id="107699247">13712</key><summary>Add docs note about cloud.aws.region</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">mnylen</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>docs</label></labels><created>2015-09-22T11:54:12Z</created><updated>2015-09-22T13:26:34Z</updated><resolved>2015-09-22T13:15:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-22T13:25:35Z" id="142289003">Thank you! Also merged in 2.x and 2.0 branches
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>id route value is required</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13711</link><project id="" key="" /><description>id route value is required
</description><key id="107692380">13711</key><summary>id route value is required</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label></labels><created>2015-09-22T11:08:48Z</created><updated>2015-10-06T10:00:48Z</updated><resolved>2015-10-06T09:28:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-22T13:44:57Z" id="142293528">LGTM
</comment><comment author="Mpdreamz" created="2015-10-06T10:00:48Z" id="145807621">merged to master and cherry-picked to `2.0`. `2.1`, `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NGram Tokenizer ignoring search terms smaller than n</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13710</link><project id="" key="" /><description>NGram Tokenizers are a very useful filter to match parts of compound words. Compound words can be commonly found p.e. in the German language, but occur in English as well (p.e. "thunderstorm").
https://www.elastic.co/guide/en/elasticsearch/guide/current/ngrams-compound-words.html

NGram (also called n-gram) Tokenizers split up words into token sequences. Each token has a specified minimum gram length (min_gram) and maximum gram length (max_gram). Tokens consisting of three letters p.e. are called trigrams.
Using trigrams as our starting point, the word "peanut" for example would be divided into the three-letter-tokens "pea", "ean", "anu" and "nut".
Searching for "peanut" would therefore also (partially) match words like "**pea**ce", "S**ean**", "**Anu**bis" and "**nut**rition". They all contain one of these tokens, but might not be words (or the document if they are all in one) we want to see as our results.
Statistically speaking we would then be suffering type I errors (false positives).
https://en.wikipedia.org/wiki/Type_I_and_type_II_errors

Obviously we might then try longer min_gram lengths, p.e. four or five letters long.
With five-grams, "peanut" would be split up in "peanu" and "eanut", giving us far more exact results.
One might come to the conclusion, the longer the n-gram, the better.
Though this is be true to some extent and for some purposes, we will run into another problem.

Elasticsearch's NGram Tokenizers ignore search terms with a length smaller than the specified min_gram!
That means the actual search term will often be significantly shorter and more vague than intended. If there will be a search term left at all.

We might p.e. search for "step by step", trying to find backpacks of this company. Here we have multiple query strings, combined by the AND operator.
If we use our five-gram Tokenizer every word will be ignored. We will have no results resulting in type II errors (false negatives).
Bigrams would respect every word, but lead to a lot of type I errors (see above the "peanut" example).
Trigrams would only exclude the word "by" and respect the rest while being more accurate. But trigrams match the unwanted "Steckdosenleiste" (= multi-outlet power strip). **"Ste"**, "tep", **"Ste"**, "tep" on the one hand and **"ste"**, "tec", "eck", "ckd", "kdo", "dos", "ose", "sen", "enl", "nle", "lei", "eis", "ist", **"ste"** on the other means 50% of the search terms are matched!
So we would might opt for a four-gram Tokenizer, bringing us good results this time.
But then searching for "Jay Smith" would also give us somebody called "John Smith" with the same exact match percentage (type I error).

Even if we wouldn't care for words smaller than four letters, sure enough one can come up with an example where five-grams would be needed.
Searching for "Recht" (= law), a four-gram Tokenizer will match "Echt + rechnung" (= calculation/bill + real):
"Recht" splits up into **"rech"**, **"echt"** while "Echt + rechnung" splits up into **"echt"** and **"rech"**, "echn", "chnu", "hnun", "nung", presenting a perfect match. This might come as a surprise, as those words are in two differents fields (the fields "name" and "description"), making a good example of how unintended and undesired a match can be.
Respecting the token order ("Rech" before "echt") of our search term would help here, but is not supported. Same goes for the "step by step" example.

If it were possible for the Elasticsearch NGram Tokenizer to fall back to smaller sized n-grams, using five-grams for "Smith" and trigrams for "Jay" (instead of ignoring "Jay completely), our problems described above would be gone. We would have relative n-gram precision according to the length of our search terms. But as it stands, we are forced to choose between lots of type I or lots of type II errors.

To sum the examples we have the following situation:
1. A gram size smaller than five leads to problems with our search term "Recht" (type I errors).
2. A gram size larger than three ignores "jay" in "jay smith" (type I error).
3. A gram size larger than four ignores "step by step" (type II error).

This is why a combination of three to five grams (p.e. min_gram=3 and max_gram=5) also fails.

Feel free to play around with the examples in my gist: https://www.found.no/play/gist/6d27486e51bdda88dd84
Edit the min_gram and max_gram sizes to your liking in the "Analysis" section (center left), rerun the queries ("Run" in the top right) and see the altered "Results" section (bottom right) with our Searches # 1, # 2 and # 3.
Searching for "jay smith" corresponds to Search # 1, searching for "step by step" to Search # 2 and finally searching for "recht" to Search # 3. To try out other search terms, edit the queries in the "Searches" section (top right).
Or edit the "Documents" section (top left), to experiment with different words to be searched for.

To summarize: 
If we want to match search terms to compound words ("thunderstorm"), NGram Tokenizers are a fine tool. They split up words into tokens of a predefined size. 
But Elasticsearch's fixed size NGram Tokenizers force us to choose between either lots of type I or type II errors. What works well for short words like "Jay" won't work for longer words like "Recht" (German for law) and vice versa.
Desired would be a flexible NGram Tokenizer. This means being able to switch from longer NGram Tokenizers (p.e. five-grams) to shorter (p.e. trigrams), as the search terms become shorter. Another, maybe more complicated way could be to make the NGram Tokenizer respect the order of the tokens.
</description><key id="107688559">13710</key><summary>NGram Tokenizer ignoring search terms smaller than n</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrdotnic</reporter><labels /><created>2015-09-22T10:46:10Z</created><updated>2016-04-13T11:23:24Z</updated><resolved>2015-09-22T13:44:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-22T13:44:28Z" id="142293424">Hi @mrdotnic 

This is pretty much a duplicate of #9971. In particular see https://github.com/elastic/elasticsearch/issues/9971#issuecomment-89589856. The trick is to use multi-fields with different analysis chains
</comment><comment author="mrdotnic" created="2015-09-23T13:27:46Z" id="142599502">Hi @clintongormley

I just tried the multi fields approach mentioned by @rmuir but without success.

The suggestion works for a search term like "jay" that would be ignored by p.e. a four-gram analyzer but is now considered thanks to the standard analyzer used on top of the four-gram analyzer.
See Search # 2 of this new and smaller gist:
https://www.found.no/play/gist/b593300c94524603f73b

However the problem persists when searching for "jay smith" (Search # 1 of the new gist). 
The four-gram analyzer ignores "jay" but keeps "smith", resulting in the unwanted match "john smith".

Maybe I am doing something wrong with my implementation?
</comment><comment author="clintongormley" created="2015-09-25T10:10:25Z" id="143180422">Hi @mrdotnic 

As soon as you stray into the territory of ngrams, you leave exact matches behind you.  Ngrams help you to expand the search to include more potential matches but, for that same reason, you end up with matches which are false positives.

However, the more ngrams that match the better - so the best results should float to the top.  Usually you'd combine this with e.g. `minimum_should_match: 80%` to trim the long tail of poorly matching results.  Combining a query on an ngram field with a query on a full-word (standard analyzer) field is a good way of bumping up the relevance of words that match exactly.

Here's a potential approach that you can use to satisfy your own requirements: make words shorter than your ngrams required matches on the standard field:

(Note: the syntax used below is for 2.0)

```
PUT t
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "4grams": {
          "type": "ngram",
          "min_gram": 4,
          "max_gram": 4,
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      },
      "analyzer": {
        "4grams": {
          "tokenizer": "4grams",
          "filter": [
            "lowercase"
          ]
        },
        "short_std": {
          "tokenizer": "standard",
          "filter": [
            "short",
            "lowercase"
          ]
        }
      },
      "filter": {
        "short": {
          "type": "length",
          "min": 1,
          "max": 3
        }
      }
    }
  },
  "mappings": {
    "t": {
      "properties": {
        "name": {
          "type": "string",
          "analyzer": "standard",
          "fields": {
            "ngrams": {
              "type": "string",
              "analyzer": "4grams"
            }
          }
        }
      }
    }
  }
}

PUT t/t/1
{"name": "Jay Smith"}

PUT t/t/2
{"name": "John Smith"}

PUT t/t/3
{
  "name": "Jay Smithson"
}

GET /_search
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "name": {
              "query": "Jay Smith",
              "analyzer": "short_std",
              "operator": "and"
            }
          }
        },
        {
          "match": {
            "name.ngrams": {
              "query": "Jay Smith",
              "operator": "or"
            }
          }
        }
      ]
    }
  }
}
```
</comment><comment author="felixbarny" created="2015-11-17T09:56:35Z" id="157324047">Thank you very much for the detailed suggestion. I have some trouble with this approach. If you are searching for `smith` only, you don't get any results. Probably because the must query essentially is empty. This is the relevant gist: https://www.found.no/play/gist/2715cdd064250352fd23

One approach would be to merge the two queries into one like this: https://www.found.no/play/gist/64f63fe1fd99f4580ca7
That way I can find all Smiths and Smithsons when searching for 'smith' but searching for 'jay smith' also returns 'john smith' because jay is removed from the .ngram field.

I really think that an optional parameter like `keep_shorter_terms` would solve a lot and since it's optional it can't harm either. But I'm also open for other suggestions.
</comment><comment author="felixbarny" created="2015-12-03T15:02:41Z" id="161666982">any ideas?
</comment><comment author="felixbarny" created="2016-01-25T13:39:41Z" id="174512170">Am I doing something wrong or is that a use case that can't be handled with Elasticsearch? Any chance you can reopen this issue until it gets resolved or we find a working workaround?

Thanks a lot :)
</comment><comment author="felixbarny" created="2016-04-03T09:08:51Z" id="204922591">Again, this issue is still _not_ fixed for me and I hope you reconsider opening it again as I'm still eagerly waiting for a `keep_shorter_terms` option.
</comment><comment author="felixbarny" created="2016-04-13T06:17:37Z" id="209246975">To summarize this issue: Small terms just disappear in the index as well as in the search string. So more documents as expected match because some criteria in the query is just lost. Combining queries on ngram fields in combination with normal fields does not work either. When using an or, I still get more results as expected from the ngram part of the query. When using an and conjunction, I get less results, because the sub words are not matched by the non ngram part of the query. 

So please, reopen this issue. 
</comment><comment author="clintongormley" created="2016-04-13T11:23:24Z" id="209376356">@felixbarny using `and`|`or` with ngrams doesn't make much sense... ngrams spread the net wide, and you can use `minimum_should_match` to reduce the long tail of poorly matching documents.  Either way, we use the ngram tokenizer and token filter from lucene, which doesn't have support for this.  If you want to convince Lucene to support it, then I'd open a ticket there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Details about the new automatic caching. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13709</link><project id="" key="" /><description>It would be nice to document the decision making of the new automatic caching, what is in it, why it caches what it does, etc.
</description><key id="107687487">13709</key><summary>Details about the new automatic caching. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">jjfalling</reporter><labels><label>docs</label></labels><created>2015-09-22T10:39:23Z</created><updated>2015-12-02T23:07:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Denormalize cluster state output with 'keyed' parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13708</link><project id="" key="" /><description>This pull request adds the `keyed` parameter to the Cluster State API. When set to `false`, this parameter will denormalize some information (like `nodes` or `indices` fields) so that they are no more keyed by node id or index name but encapsulated in an object with a node id or index name sub field.

Here is an example (truncated for brevity) when `keyed=true` (the default behavior):

``` json
{
  "cluster_name" : "Idea",
  "nodes" : {
    "KoOu7TkgSz6oXRwGs21m2Q" : {
      "name" : "Battlestar",
    }
  },
  "metadata" : {
    "indices" : {
      "my_index" : {
        "state" : "open"
      },
      "my_other_index" : {
        "state" : "open"
      }
  }
}
```

and when `keyed=false`:

``` json
{
  "cluster_name" : "Idea",
 "nodes" : [ {
    "id" : "KoOu7TkgSz6oXRwGs21m2Q",
    "name" : "Battlestar"
  } ],
 "metadata" : {
    "indices" : [ {
      "index" : "my_index",
      "state" : "open"
    },{
      "index" : "my_other_index",
      "state" : "open"
  }]
}
```
</description><key id="107680973">13708</key><summary>Denormalize cluster state output with 'keyed' parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:REST</label></labels><created>2015-09-22T10:00:22Z</created><updated>2016-05-23T11:10:09Z</updated><resolved>2016-05-23T08:57:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-22T10:25:31Z" id="142240748">I discussed my concerns with @tlrx that the naming and the implementation is a bit confusing. We came up with a potential alternative that @tlrx has gracefully agree to explore.
</comment><comment author="tlrx" created="2015-10-28T12:35:10Z" id="151830193">@bleskes I gave this another try: I removed the `startKeyedObject()` methods and put the logic down into the `toXContent()` implementations. This way one can clearly see that the `keyed` parameter can have an impact on the final output and it also helps to avoid to override fields... at the price of more complexity.

I'm not fully satisfied with either solution but I can't come with something better. Can you please have a look and tell me what you think? Thanks
</comment><comment author="dakrone" created="2016-04-06T20:50:17Z" id="206561113">@tlrx is this still something you're pursuing?
</comment><comment author="tlrx" created="2016-05-23T08:57:41Z" id="220924196">If the idea is still nice to have, I think we can close this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove TranslogService and fold it into synchronous IndexShard API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13707</link><project id="" key="" /><description>This commit moves the size and ops based flush into a synchronous API into
IndexShard and removes the time-based flush alltogether since it' basically
covered by the inactive async flush API we have today. The functionality doesn't
need to be covered by scheduled task and async APIs while we can actually make all
the decisions in a sync manner which is way easier to control and to test.
</description><key id="107674535">13707</key><summary>Remove TranslogService and fold it into synchronous IndexShard API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Translog</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T09:17:30Z</created><updated>2015-09-23T13:29:10Z</updated><resolved>2015-09-23T10:40:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-22T11:38:25Z" id="142261577">I'm +1 on removing the time component for translog flushing which leaves translog service without any real value. Folding what's left into index shard also makes sense. I'm a bit concerned that flushing is now left to the TransportReplicationAction. It doesn't feel like that's where this logic should be as the replication action doesn't really need to know what the index shard is doing with the write operations. Instead I proposed adding this logic to index shard it self, where all write operations go through as well. 

I share Shay's concern about the flush storm that may happen. In the spirit of the new java 8 lambda functionality it feels like we need a debounce utility :) 
</comment><comment author="s1monw" created="2015-09-22T12:16:15Z" id="142271042">&gt; Instead I proposed adding this logic to index shard it self, where all write operations go through as well.

I should have added this to the issue `this can't go into index shard since it would completely ruin the cleanup and add more crazy state to it` sorry we have to find a different solution. I had the same concerns about the thread pool while I was AFK ....I will work on a better solution
</comment><comment author="s1monw" created="2015-09-22T13:06:11Z" id="142283081">I pushed a new commit that prevent the storms 
</comment><comment author="kimchy" created="2015-09-22T13:38:40Z" id="142291869">left a minor comment, LGTM otherwise
</comment><comment author="bleskes" created="2015-09-22T14:03:28Z" id="142298209">LGTM2. Agreed with Shay's comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Ensure dependency convergence using mvn plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13706</link><project id="" key="" /><description>This adds the dependency convergence part of the already used
enforcer plugin to the build.

If two artifacts depend on different versions of another artifact
this plugin barfs before the build. The phonetic plugin had this problem
with the `commons-codec` dependency.
</description><key id="107666173">13706</key><summary>Build: Ensure dependency convergence using mvn plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>non-issue</label></labels><created>2015-09-22T08:25:18Z</created><updated>2015-09-22T12:37:26Z</updated><resolved>2015-09-22T12:37:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-22T10:24:27Z" id="142240584">Why? this plugin can have its own version of commons-codec. It should not use a random one that you think is a good one. It should use the one that we test with (and the commons-codec guys help us out with that in lucene).

Sorry, this provides no benefits.
</comment><comment author="spinscale" created="2015-09-22T12:37:18Z" id="142275515">you are right, this approach is utterly wrong. The problem I was trying to solve, was in the project I was working on, not ES. Sorry for the noise.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Making searches faster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13705</link><project id="" key="" /><description>Hi,

Kindly help me with some tips on tuning elasticseach for faster searching. Sorry for the long post.
I have two elasticsearch clusters

1)
- Two bare metal nodes, each with 24 cores, 64 GiB RAM, rotational 1TB HDD
- Around 5853319 docs (10GiB) per day with 4 primary shards and replication=1
- 16GiB heap, no other special configuration

In this cluster, if I do a 7 day search in Kibana 4 dashboard with around 30 graphs, it takes more than a minute to return all results, then on applying a filter, it takes another minute to give back results. Can it do it faster than this? 

HQ plugin shows the following stats in warning/error stages for the two nodes:
Search - Query:     308.35ms    355.12ms
Search - Fetch:     104.61ms    84.6ms
Refresh:                     34.63ms    29.37ms 

2) 
- Three EC2 c4.4xlarge data nodes, each with 16 cores, 30 GiB RAM, EBS volumes
- Around 188361074 docs (400GiB) per day with 5 primary shards and replication=1
- 16GiB heap

Was getting error in Kibana, so increased circuitbreaker to 75%. Then did a 7 day search, brought the whole cluster down due to OOM exceptions. Was that overkill? What can I do to make this search successful, and to prevent the cluster from going down?

Hoping for some pointers here. Thanks
</description><key id="107659092">13705</key><summary>Making searches faster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">siddharthasahu</reporter><labels /><created>2015-09-22T07:21:03Z</created><updated>2015-09-22T07:57:04Z</updated><resolved>2015-09-22T07:24:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-22T07:24:55Z" id="142201293">please ask you question on the mailing list https://discuss.elastic.co/ - the issue tracker is only for features and bugs etc.
</comment><comment author="siddharthasahu" created="2015-09-22T07:57:04Z" id="142206470">Oh sorry. Obvious in hindsight. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move a couple more rest-api-spec resource dirs into resources</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13704</link><project id="" key="" /><description>A couple were left behind. This fixes them and cleans up the test
resources for qa tests a little bit.
</description><key id="107646425">13704</key><summary>Move a couple more rest-api-spec resource dirs into resources</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T04:59:28Z</created><updated>2015-09-22T05:11:00Z</updated><resolved>2015-09-22T05:10:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-22T05:00:40Z" id="142182116">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>some files have no license headers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13703</link><project id="" key="" /><description>I noticed this when cleaning up javadocs.

The rules for "core" are too damn lenient. There should be no exclusions.
</description><key id="107646072">13703</key><summary>some files have no license headers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T04:54:03Z</created><updated>2016-10-18T08:52:46Z</updated><resolved>2015-12-18T18:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-10-08T09:35:44Z" id="146472778">Quick grep of all .java files missing "Licensed to Elasticsearch" produces this:
- guice: 
  ./main/java/org/elasticsearch/common/inject/*
  - Copyright (C) 2007 Google Inc.
  - Licensed under the Apache License
- Netty pipelining code
  ./main/java/org/elasticsearch/http/netty/pipelining/HttpPipeliningHandler.java
  ./main/java/org/elasticsearch/http/netty/pipelining/OrderedDownstreamChannelEvent.java
  ./main/java/org/elasticsearch/http/netty/pipelining/OrderedUpstreamMessageEvent.java
  - @author Christopher Hunt
    (Apache license listed here: https://github.com/typesafehub/netty-http-pipelining/blob/master/LICENSE)
- Joda-related
  ./main/java/org/joda/time/base/BaseDateTime.java
  ./main/java/org/joda/time/format/StrictISODateTimeFormat.java
  -  Copyright 2001-2011 Stephen Colebourne
  -  Licensed under the Apache License

So these exclusions all look to be where we have incorporated Apache-licensed code built for non-elasticsearch-related projects and hence not had a CLA.
I didn't manage to run the maven check-license plugin. 
</comment><comment author="jpountz" created="2015-10-08T09:45:12Z" id="146475875">&gt; I didn't manage to run the maven check-license plugin. 

The license check is hooked to the compile phase, so just running "mvn compile" will check licenses. I think one reason why we can't run the license plugin at the moment is that we declare two license plugins (the codehaus and the mycila ones) that use the same namespace.
</comment><comment author="jpountz" created="2015-10-08T09:48:44Z" id="146476578">I was looking at this issue as well and I can't find how to check for multiple licenses within the same source tree.
</comment><comment author="jpountz" created="2015-10-09T09:32:35Z" id="146812245">We made some progress towards this on #14020 by checking all java files in the source tree instead of just those from the org.elasticsearch package. However there doesn't seem to be any way to check for multiple licenses in the same source tree with the license plugin, so I'm bumping this issue to 2.2, maybe the new build would help address this issue?

Another open question is whether we want to check licenses on other files such as ant/maven/gradle files?
</comment><comment author="rmuir" created="2015-12-18T18:37:38Z" id="165866286">&gt; However there doesn't seem to be any way to check for multiple licenses in the same source tree with the license plugin, so I'm bumping this issue to 2.2, maybe the new build would help address this issue?

This is fixed by using rat: https://github.com/elastic/elasticsearch/pull/15545
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix all javadocs issues, re-enable compiler warnings (but disable on java 9 where maven is broken)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13702</link><project id="" key="" /><description>Lets raise the bar for the code here. We need our -Werror back, this enables it except on java 9 (where maven has a bug with it).

I also add full doclint checks (except missing) and at private level. We don't need malformed documentation.

Closes #13699
</description><key id="107639572">13702</key><summary>Fix all javadocs issues, re-enable compiler warnings (but disable on java 9 where maven is broken)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T03:38:04Z</created><updated>2015-09-22T13:14:29Z</updated><resolved>2015-09-22T04:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-22T03:45:51Z" id="142171266">Diff is too large, here is the /pom.xml changes:

```
diff --git a/pom.xml b/pom.xml
index 9350eb0..54014ee 100644
--- a/pom.xml
+++ b/pom.xml
@@ -42,6 +42,7 @@
         &lt;!-- Path warnings must be ignored because maven doesnt create the classes output
              dir when no source files exist (eg for distribution or qa modules) --&gt;
         &lt;xlint.options&gt;-Xlint:-path&lt;/xlint.options&gt;
+        &lt;doclint.options&gt;-Xdoclint:-missing&lt;/doclint.options&gt;

         &lt;!-- libraries --&gt;
         &lt;lucene.version&gt;5.4.0&lt;/lucene.version&gt;
@@ -613,7 +614,9 @@
                             &lt;arg&gt;-XDignore.symbol.file&lt;/arg&gt;
                             &lt;arg&gt;-Xlint:all&lt;/arg&gt;
                             &lt;arg&gt;${xlint.options}&lt;/arg&gt;
-                            &lt;!-- DISABLED: incompatible with java 9 &lt;arg&gt;-Werror&lt;/arg&gt; --&gt;
+                            &lt;arg&gt;-Xdoclint:all/private&lt;/arg&gt;
+                            &lt;arg&gt;${doclint.options}&lt;/arg&gt;
+                            &lt;arg&gt;${javac.werror}&lt;/arg&gt;
                         &lt;/compilerArgs&gt;
                     &lt;/configuration&gt;
                 &lt;/plugin&gt;
@@ -1144,6 +1147,23 @@ org.eclipse.jdt.ui.text.custom_code_templates=&lt;?xml version\="1.0" encoding\="UT
                     &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;
                     &lt;version&gt;1.8&lt;/version&gt;
                     &lt;executions&gt;
+                        &lt;!-- remove this hack when maven works with java 9 properly --&gt;
+                        &lt;execution&gt;
+                            &lt;id&gt;set-werror&lt;/id&gt;
+                            &lt;phase&gt;validate&lt;/phase&gt;
+                            &lt;goals&gt;
+                                &lt;goal&gt;run&lt;/goal&gt;
+                            &lt;/goals&gt;
+                            &lt;configuration&gt;
+                                &lt;target&gt;
+                                    &lt;!-- if we are on java 1.9.* we turn off Werror or maven crushes! --&gt;
+                                    &lt;condition property="javac.werror" value="-Aboguskey=bogusvalue" else="-Werror" &gt;
+                                        &lt;matches pattern="1\.9\..+$" string="${java.runtime.version}" /&gt;
+                                    &lt;/condition&gt;
+                                &lt;/target&gt;
+                                &lt;exportAntProperties&gt;true&lt;/exportAntProperties&gt;
+                            &lt;/configuration&gt;
+                        &lt;/execution&gt;
                         &lt;execution&gt;
                             &lt;id&gt;check-invalid-patterns&lt;/id&gt;
                             &lt;phase&gt;validate&lt;/phase&gt;
```
</comment><comment author="rjernst" created="2015-09-22T04:41:56Z" id="142180442">We need -Werror back! It sucks that so many little things snuck in while it was disabled. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix ping timeout settings inconsistencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13701</link><project id="" key="" /><description>This commit fixes ping timeout settings inconsistencies in
ZenDiscovery. In particular, the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election) refers to the ping
timeout setting as `discovery.zen.ping_timeout` but the code was
ultimately using `discovery.zen.ping.timeout` if this was set.

This commit also changes all instances of the raw string
`&#8220;discovery.zen.ping_timeout"` to the constant
`o.e.d.z.ZenDiscovery.SETTING_PING_TIMEOUT`.

Finally, a constant is added for the setting
`discovery.zen.initial_ping_timeout` and a clarification is added to the
documentation regarding the usage of this setting.

Closes #6579
</description><key id="107633016">13701</key><summary>Fix ping timeout settings inconsistencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>breaking-java</label><label>v5.0.0-alpha1</label></labels><created>2015-09-22T02:12:28Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-09-23T17:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-23T14:18:42Z" id="142615948">Looks good. How about just dropping initial_ping_timeout ? it doesn't really serve any use and isn't documented. I _think_ it's just legacy. Will be good to dig in git archive and see.

Related issues:

https://github.com/elastic/elasticsearch/issues/6579
https://github.com/elastic/elasticsearch/issues/9581
https://github.com/elastic/elasticsearch/issues/9908
</comment><comment author="jasontedor" created="2015-09-23T16:53:33Z" id="142660451">@bleskes Went spelunking through git and found the initial introduction of `initial_ping_timeout` in cb0d7d4735665fa8ca1b59555a06354859c0045a. From there, `ping_timeout` was introduced in 3cda177b9b571b4edc9a48512c57fb830f3ecc19 which came from #723 which appears to have had the intention of renaming the setting but still supporting the old one. I think it's safe to remove `initial_ping_timeout`. I've pushed new commits to this pull request to address, and I've marked that this pull request will close the issues that you found.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to sort and limit aggregations in ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13700</link><project id="" key="" /><description>For example I have the following records with the columns as:(Country,City,Date,Income)

```
USA SF 2015-01 80
USA SF 2015-03 60
USA SF 2015-05 20
USA NY 2015-09 70
USA NY 2015-02 30
U.K LD 2015-05 30
U.K LD 2015-01 20
```

My sql as: `select country,city,max(date) as maxDate,sum(income) as sumIncome from testTable group by country,city order by maxDate desc,sumIncome desc limit 2`.
So the result should be:

```
USA NY 2015-09 100
USA SF 2015-05 160
```

I wrote the ES aggregates as following, but it's wrong: 

```
"aggs":{"sub1": {"terms":{"field":"contry"},
   "aggs":{"sub2":{"terms":{"field":"city",
       "order":[{"submax":"DESC"},{"subsum":"DESC"}]},
     "aggs":{"submax":{"max":{"field":"date"}},"subsum":{"sum":{"field":"income"}}}}}}}
```

First Problem: it's not sorted globally, it seems that just sort in each bucket,
Another problem is that I don't know how to limit the size to 2. Thanks in Advance!
</description><key id="107626696">13700</key><summary>how to sort and limit aggregations in ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JackWang917</reporter><labels /><created>2015-09-22T00:58:09Z</created><updated>2015-09-22T17:46:29Z</updated><resolved>2015-09-22T13:09:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-22T13:09:58Z" id="142284214">Hi @JackWang917 

Please ask questions about usage like this on the forums instead: http://discuss.elastic.co/
</comment><comment author="JackWang917" created="2015-09-22T17:13:45Z" id="142352872">Thank you! where is the forum link please.
</comment><comment author="clintongormley" created="2015-09-22T17:46:29Z" id="142361385">I'm going to leave you to figure this one out by yourself.  (Hint: it's on this page!)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clean up Javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13699</link><project id="" key="" /><description>We're disabling doclint in #13689 so the javadocs that we have will compile. We should get the javadocs to the point where they pass doclint - or at least pass _some_ checks. We're not really sure what standards we should have for Javadoc but right now we don't have any.
</description><key id="107597691">13699</key><summary>Clean up Javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2015-09-21T21:01:46Z</created><updated>2015-09-22T04:46:31Z</updated><resolved>2015-09-22T04:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-21T21:19:08Z" id="142111026">FWIW this is what lucene uses right now:

```
  &lt;property name="javadoc.doclint.args" value="-Xdoclint:all -Xdoclint:-missing"/&gt;
  &lt;property name="javac.doclint.args" value="-Xdoclint:all/protected -Xdoclint:-missing"/&gt;
```

Available options are: `accessibility`, `html`, `missing`, `reference`, or `syntax`

Maybe we should at least start with `html`, `reference`, `syntax` and fix the errors in docs we already have.

`accessibility` and `missing` require adding more (e.g. captions for images/tables, and javadocs for methods that don't currently have any).

And the other trick is to pass doclint options to `javac` too, meaning the compile will just fail (its a fast check) and you don't need to run any additional build target. See the example, for that to work, you have to specify the access level that you normally generate javadocs for (e.g. protected).
</comment><comment author="nik9000" created="2015-09-21T21:21:20Z" id="142111503">&gt; Maybe we should at least start with html, reference, syntax and fix the errors in docs we already have.
&gt; 
&gt; And the other trick is to pass doclint options to javac too, meaning the compile will just fail (its a fast check) and you don't need to run any additional build target. See the example, for that to work, you have to specify the access level that you normally generate javadocs for (e.g. protected).

I think this is the right way to start on this. I suspect there are plenty of Javadoc errors to fix just with those options.
</comment><comment author="rmuir" created="2015-09-22T02:07:33Z" id="142159498">&gt; I suspect there are plenty of Javadoc errors to fix just with those options.

That is the understatement of the year.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggester: Clarify suggestion indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13698</link><project id="" key="" /><description>Clarify how to index multiple suggestion entries per document
</description><key id="107597271">13698</key><summary>Completion Suggester: Clarify suggestion indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>docs</label><label>v2.1.0</label></labels><created>2015-09-21T20:59:10Z</created><updated>2015-10-30T05:43:04Z</updated><resolved>2015-10-30T05:43:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-22T12:33:21Z" id="142274285">Other than the typo, LGTM :)
</comment><comment author="s1monw" created="2015-10-17T19:13:12Z" id="148944932">@areek wanna get it in?
</comment><comment author="areek" created="2015-10-30T05:43:04Z" id="152425371">Thanks for the review. pushed to `feature/completion_suggester_v2` (https://github.com/elastic/elasticsearch/commit/9a9c1e540964c5a887210a685184069c97cbe584)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Netty excption will index data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13697</link><project id="" key="" /><description>Hello guys, hope you guys are fine.
I have a problem when i'll index some data in my elastic server this returns this error

**[2015-09-21 17:38:08,331][WARN ][http.netty               ] [Marvel Girl] Caught exception while handling client http traffic, closing connection [id: 0x92405db8, /127.0.0.1:38750 =&gt; /127.0.0.1:9200]
java.lang.IllegalArgumentException: invalid version format: TOOL/DOCUMENTS/33 HTTP/1.1**

I just downloaded the elastic from website and started, no have plugins or other kind of different things.
I see a lot of issue like that over the web but have no clear solution.
Waiting for you guys.
Thanks for the attention!
</description><key id="107595422">13697</key><summary>Netty excption will index data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">waldemarnt</reporter><labels /><created>2015-09-21T20:48:33Z</created><updated>2015-09-22T12:32:02Z</updated><resolved>2015-09-22T12:32:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-22T12:32:02Z" id="142274063">Hi @waldemarnt 

Something on your network is sending malformed requests to Elasticsearch.  Nothing to do with Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix plugin tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13696</link><project id="" key="" /><description>Fix the vagrant tests after azure was split into 3 plugins. The tests
need to list all the plugins and some dependency so we can make sure the
plugin can be installed and uninstalled.
</description><key id="107585307">13696</key><summary>Fix plugin tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T19:48:51Z</created><updated>2015-09-22T13:01:45Z</updated><resolved>2015-09-22T13:01:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T19:49:00Z" id="142090012">Ping @dadoonet and @brwe for review.
</comment><comment author="dadoonet" created="2015-09-21T22:10:37Z" id="142124345">While reading it at first, I wondered what was the change in `pom.xml` file. Looks like you just change the order of the dependencies to be imported, right?

It looks good to me. Thanks for fixing it! 
</comment><comment author="nik9000" created="2015-09-21T22:49:00Z" id="142130770">&gt; While reading it at first, I wondered what was the change in pom.xml file. Looks like you just change the order of the dependencies to be imported, right?

I think so. I went through them and added one I thought was missing only to find it again. I think all I do was sort them alphabetically.
</comment><comment author="dadoonet" created="2015-09-22T03:59:04Z" id="142173460">++ easier to check !
</comment><comment author="spinscale" created="2015-09-22T12:58:38Z" id="142280512">LGTM, just gave it a testrun. thx for fixing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get lang-javascript, lang-python, securemock ready for script refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13695</link><project id="" key="" /><description>I want to refactor scripting engines so we can contain dangerous "God-like" permissions
like createClassloader/sun.reflect. These are used for dynamic class generation (scripts, mocks).
This will mean some refactoring to ES core.

But first lets get the plugins in order first. I removed those permissions globally, and
fixed grants for lang-javascript, lang-python, securemock so that everything works.

lang-javascript needs no code changes, because rhino is properly written :)
lang-python needs accesscontroller blocks. securemock was already working as of 1.1

This is just a baby step, to try to do some of this incrementally! It doesn't yet provide
us anything.
</description><key id="107579908">13695</key><summary>Get lang-javascript, lang-python, securemock ready for script refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T19:17:13Z</created><updated>2015-09-30T06:25:56Z</updated><resolved>2015-09-21T19:44:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-21T19:22:25Z" id="142084187">LGTM
</comment><comment author="rmuir" created="2015-09-30T06:25:56Z" id="144294617">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removes wheezy from script testing docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13694</link><project id="" key="" /><description>Wheezy was removed from the Vagrantfile and its not used in script testing anymore.
The reason it was removed is commented in the [Vagrantfile](https://github.com/elastic/elasticsearch/blob/master/Vagrantfile#L37).
</description><key id="107578215">13694</key><summary>Removes wheezy from script testing docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrestc</reporter><labels><label>docs</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T19:06:46Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-21T19:08:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T19:08:26Z" id="142080342">Thanks again @andrestc!
</comment><comment author="nik9000" created="2015-09-21T19:11:15Z" id="142081130">Merged to 2.0, 2.1, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure all resoruces are closed on Node#close()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13693</link><project id="" key="" /><description>We are leaking all kinds of resources if something during Node#close() barfs.
This commit cuts over to a list of closeables to release resources that
also closed remaining services if one or more services fail to close.

Closes #13685
</description><key id="107576536">13693</key><summary>Ensure all resoruces are closed on Node#close()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2015-09-21T18:59:10Z</created><updated>2016-01-29T15:19:52Z</updated><resolved>2016-01-29T15:19:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-21T19:01:20Z" id="142077761">+1, I like the list-of-closeables approach.
</comment><comment author="clintongormley" created="2016-01-28T17:43:47Z" id="176299121">@s1monw what happened to this PR?
</comment><comment author="s1monw" created="2016-01-29T14:37:18Z" id="176785485">@clintongormley I guess it got some heavy bikeshedding not visible to this PR? 
</comment><comment author="s1monw" created="2016-01-29T15:19:46Z" id="176805856">I opened a new PR for this https://github.com/elastic/elasticsearch/pull/16316
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More Lucene suggesters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13692</link><project id="" key="" /><description>Lucene has a few available suggesters that would be nice to expose in Elasticsearch:
- [AnalyzingInfixSuggester](https://lucene.apache.org/core/5_3_0/suggest/org/apache/lucene/search/suggest/analyzing/AnalyzingInfixSuggester.html) (see #17029)
- [BlendedInfixSuggester](https://lucene.apache.org/core/5_3_0/suggest/org/apache/lucene/search/suggest/analyzing/BlendedInfixSuggester.html)
- [FreeTextSuggester](https://lucene.apache.org/core/5_3_0/suggest/org/apache/lucene/search/suggest/analyzing/FreeTextSuggester.html)

Any others?
</description><key id="107570278">13692</key><summary>More Lucene suggesters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Suggesters</label><label>discuss</label><label>feature</label><label>Meta</label></labels><created>2015-09-21T18:25:27Z</created><updated>2016-03-10T10:23:40Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index name expressions should not be broken up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13691</link><project id="" key="" /><description>Index name expressions like: `&lt;logstash-{now/D}&gt;` are broken up into: `&lt;logstash-{now` and `D}&gt;`. This shouldn't happen. This PR fixes this by preventing the `PathTrie` to split based on `/` if it is currently in between between `&lt;` and `&gt;` characters.

PR for #13665
</description><key id="107569693">13691</key><summary>Index name expressions should not be broken up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:CRUD</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T18:21:46Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-25T09:49:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-23T07:56:02Z" id="142523668">left a question, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: Move MLT validation to constructor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13690</link><project id="" key="" /><description>The current MoreLikeThisQueryBuilder validation checks for existence of at least one `like` text or item. This is hard to check in setters, so this PR tries to change the construction of the query so that we can do these checks already at construction time.

Relates to #10217
</description><key id="107568573">13690</key><summary>Query Refactoring: Move MLT validation to constructor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label></labels><created>2015-09-21T18:15:14Z</created><updated>2016-03-11T11:50:48Z</updated><resolved>2015-09-23T16:32:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-22T16:26:16Z" id="142340427">left a couple of comments.
</comment><comment author="cbuescher" created="2015-09-23T13:35:34Z" id="142603012">@javanna moved lists used internally to array, which also changes the API quiet a bit, let me know if this was the intention or I should move back on some parts. 
</comment><comment author="javanna" created="2015-09-23T14:16:41Z" id="142615094">left a couple of minor comments, it looks much better to me, LGTM
</comment><comment author="cbuescher" created="2015-09-23T15:40:07Z" id="142642620">Thanks, added two more convenience methods to the QueryBuilders helper, will squash and merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable doclint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13689</link><project id="" key="" /><description>Java 8's javadoc defaults to very strict linting. It is very `-Wall -Werr`
style. And Elasticsearch's Javadocs do not pass and it'd be a huge and not
super useful effort to get them to pass the linting. So this disables it.

Closes #13336
</description><key id="107568267">13689</key><summary>Disable doclint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T18:13:29Z</created><updated>2016-03-10T18:39:17Z</updated><resolved>2015-09-21T21:02:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T18:13:49Z" id="142064901">While I think having nicer Javadoc would be nice lets just get them building again.

Ping @dadoonet for review.
</comment><comment author="rjernst" created="2015-09-21T20:57:18Z" id="142106170">LGTM for now, but can you open a follow up issue to fix these? Eventually I think we should eg have javadocs for plugin apis, and that requires the ability to build non-broken javadocs.
</comment><comment author="nik9000" created="2015-09-21T21:01:58Z" id="142107161">&gt; LGTM for now, but can you open a follow up issue to fix these? Eventually I think we should eg have javadocs for plugin apis, and that requires the ability to build non-broken javadocs.

#13699
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Note that no http settings are dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13688</link><project id="" key="" /><description>Closes #13364
</description><key id="107565959">13688</key><summary>Note that no http settings are dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2015-09-21T17:58:34Z</created><updated>2015-09-22T09:56:40Z</updated><resolved>2015-09-22T09:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T17:59:00Z" id="142061546">Ping @clintongormley for doc review.

Backport to 1.7, 2.0, 2.1 as well?
</comment><comment author="clintongormley" created="2015-09-21T18:19:03Z" id="142066164">LGTM, although I'd probably move it up to just after the "Settings" header.

And backport list is good too

thanks!
</comment><comment author="nik9000" created="2015-09-21T18:32:53Z" id="142069432">@clintongormley - moved. Had to reword a sentence to make the flow work though.
</comment><comment author="clintongormley" created="2015-09-22T09:56:40Z" id="142232130">thanks @nik9000 - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fix inconsitent handling of config dir and path settings and add test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13687</link><project id="" key="" /><description>When running bin/elasticsearch config dir and config file can be passed as a parameter
independently with -Des.config and -Des.path.conf. If the config file is not provided
then it is assumed to be {path.conf}/elasticearch.yml

This was not so for running as a service. For deb the CONF_FILE still pointed
to the default path even if a custom CONF_DIR was defined.
For rpm the CONF_FILE parameter was not passed on at all, see #5329 .

Custom config path and config file now work as follows for all services except systemd:

CONF_DIR and CONF_FILE undefined: CONF_FILE points to default
CONF_DIR defined but CONF_FILE undefined: CONF_FILE points to CONF_DIR/elasticsearch.yml
CONF_FILE defined: CONF_FILE must point to an absolute path

For systemd this commit only fixes that the service could be started if only a CONF_DIR
is defined. However, now a custom CONF_FILE cannot be defined anymore which seemed
the lesser evil to me.

relates to #12712 and #12954
closes #5329
</description><key id="107560867">13687</key><summary>fix inconsitent handling of config dir and path settings and add test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Packaging</label></labels><created>2015-09-21T17:34:28Z</created><updated>2015-09-24T17:09:38Z</updated><resolved>2015-09-24T17:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-21T17:39:07Z" id="142054375">I don't know enough systemd yet to fix this and had a short chat with @nik9000 who wondered anyway how important an option to have a custom config file actually is and if we should remove this. 
</comment><comment author="rjernst" created="2015-09-21T20:54:48Z" id="142105618">A broader question to me is why do we support `CONF_DIR` and `CONF_FILE`? This seems overly complex. I think supporting just `CONF_DIR` would be sufficient (in elasticsearch directly too, not just for rpm/deb).
</comment><comment author="nik9000" created="2015-09-21T20:56:49Z" id="142106066">&gt; A broader question to me is why do we support CONF_DIR and CONF_FILE? This seems overly complex. I think supporting just CONF_DIR would be sufficient (in elasticsearch directly too, not just for rpm/deb).

That is exactly what we were talking about. I suspect the idea behind CONF_FILE is that you could install elasticsearch once and use two jvms running with the same CONF_DIR but a difference CONF_FILE. But I'm not really sure.
</comment><comment author="rjernst" created="2015-09-21T21:01:09Z" id="142106991">&gt;  I suspect the idea behind CONF_FILE is that you could install elasticsearch once and use two jvms running with the same CONF_DIR but a difference CONF_FILE.

That seems like an advanced enough case that the user can simply copy the files/keep them in sync themselves. I'm also suspicious that it works at all with security manager, since it would mean a path outside of the permissions, and I do not see any special case for the actual conf file when setting up permissions (we only grant permission to the config dir).
</comment><comment author="nik9000" created="2015-09-21T21:03:16Z" id="142107463">@clintongormley, do you know more about why we have CONF_FILE? Maybe @spinscale knows?
</comment><comment author="clintongormley" created="2015-09-21T21:12:12Z" id="142109487">@rjernst I don't know the history there.  Looks like it was added way back when: https://github.com/elastic/elasticsearch/commit/d7f7f77d81da9a3d55d31b1bd1716ef82da5894e

I've never understood the reason for it myself. Happy to nuke it
</comment><comment author="clintongormley" created="2015-09-21T21:12:23Z" id="142109531">@electrical you have any thoughts about this?
</comment><comment author="markwalkom" created="2015-09-22T09:30:42Z" id="142226046">Happy for either/or, but we need one of them as its really useful when running multiple instances on my laptop.
</comment><comment author="electrical" created="2015-09-22T11:45:21Z" id="142262664">@clintongormley I'm happy with dropping CONF_FILE. in the puppet modules i only use CONF_DIR anyway.
</comment><comment author="nik9000" created="2015-09-22T13:13:01Z" id="142285453">https://github.com/elastic/elasticsearch/issues/13715
</comment><comment author="brwe" created="2015-09-24T17:09:38Z" id="142991468">I made a new pr here: #13772 to remove the CONF_FILE, closing this one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RepositoryS3RestIT fails when no network is available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13686</link><project id="" key="" /><description>```
Suite: org.elasticsearch.repositories.s3.RepositoryS3RestIT
  2&gt; REPRODUCE WITH: mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch.plugin:repository-s3 -Dtests.seed=1368C600E5FF263C -Dtests.class=org.elasticsearch.repositories.s3.RepositoryS3RestIT -Dtests.method="test {yaml=repository_s3/20_repository/S3 repository can be registereed}" -Des.logger.level=ERROR -Dtests.cluster=localhost:9500 -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=mt_MT -Dtests.timezone=Australia/Eucla -Dtests.rest.suite=repository_s3
FAILURE 11.4s | RepositoryS3RestIT.test {yaml=repository_s3/20_repository/S3 repository can be registereed} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [snapshot.create_repository] returned [500 Internal Server Error] [{"error":{"root_cause":[{"type":"repository_exception","reason":"[test_repo_s3_1] failed to create repository"}],"type":"repository_exception","reason":"[test_repo_s3_1] failed to create repository","caused_by":{"type":"creation_exception","reason":"Guice creation errors:\n\n1) Error injecting constructor, com.amazonaws.AmazonClientException: Unable to execute HTTP request: s3.amazonaws.com\n  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)\n  while locating org.elasticsearch.repositories.s3.S3Repository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error","caused_by":{"type":"amazon_client_exception","reason":"Unable to execute HTTP request: s3.amazonaws.com","caused_by":{"type":"unknown_host_exception","reason":"s3.amazonaws.com"}}}},"status":500}]
```

Caused by `{"type":"unknown_host_exception","reason":"s3.amazonaws.com"}`
</description><key id="107559073">13686</key><summary>RepositoryS3RestIT fails when no network is available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Repository S3</label><label>adoptme</label><label>test</label></labels><created>2015-09-21T17:24:16Z</created><updated>2017-07-26T10:04:04Z</updated><resolved>2017-07-26T10:04:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2016-07-21T13:21:26Z" id="234250974">Was testing that again in master and it still fails.

```
Suite: org.elasticsearch.repositories.s3.RepositoryS3RestIT
  1&gt; [2016-07-21 15:15:18,644][INFO ][org.elasticsearch.test.rest.client] REST client initialized [http://[::1]:53710], elasticsearch version: [5.0.0-alpha5]
  1&gt; [2016-07-21 15:15:18,652][INFO ][org.elasticsearch.test.rest.client] REST client initialized [http://[::1]:53710], elasticsearch version: [5.0.0-alpha5]
  1&gt; [2016-07-21 15:15:28,319][INFO ][org.elasticsearch.repositories.s3] Stash dump on failure [{
  1&gt;   "stash" : { }
  1&gt; }]
  2&gt; REPRODUCE WITH: gradle :plugins:repository-s3:integTest -Dtests.seed=39589EDC630B851 -Dtests.class=org.elasticsearch.repositories.s3.RepositoryS3RestIT -Dtests.method="test {yaml=repository_s3/20_repository/S3 repository can be registered}" -Dtests.security.manager=true -Dtests.locale=ar-KW -Dtests.timezone=Pacific/Yap
FAILURE 10.2s | RepositoryS3RestIT.test {yaml=repository_s3/20_repository/S3 repository can be registered} &lt;&lt;&lt; FAILURES!
   &gt; Throwable #1: java.lang.AssertionError: expected [2xx] status code but api [snapshot.create_repository] returned [500 Internal Server Error] [{"error":{"root_cause":[{"type":"repository_exception","reason":"[test_repo_s3_1] failed to create repository"}],"type":"repository_exception","reason":"[test_repo_s3_1] failed to create repository","caused_by":{"type":"amazon_client_exception","reason":"Unable to execute HTTP request: s3.amazonaws.com","caused_by":{"type":"unknown_host_exception","reason":"s3.amazonaws.com"}}},"status":500}]
   &gt;    at __randomizedtesting.SeedInfo.seed([39589EDC630B851:8BC1B63768CCD5A9]:0)
   &gt;    at org.elasticsearch.test.rest.section.DoSection.execute(DoSection.java:108)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:399)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Not sure yet how we can skip the REST tests for this plugin if no internet connection is available.
</comment><comment author="javanna" created="2016-09-14T14:58:08Z" id="247041252">@dadoonet is this still valid? Maybe we should have a network feature in our rest tests so that this test is run only when we know we have network?
</comment><comment author="dadoonet" created="2017-07-26T10:04:04Z" id="318009484">Well. This one is 2 years old now and no one else seemed to complain about it so I think we can close it.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possibly leak of lock, if plugin hits exception on close()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13685</link><project id="" key="" /><description>here is how to reproduce (i know, kinda crazy):
1. force GroovyScriptEngineService to make NPE on close:

```
+++ b/core/src/main/java/org/elasticsearch/script/groovy/GroovyScriptEngineService.java
@@ -99,17 +99,19 @@ public class GroovyScriptEngineService extends AbstractComponent implements Scri
         }

         // Groovy class loader to isolate Groovy-land code
-        this.loader = new GroovyClassLoader(getClass().getClassLoader(), config);
+        this.loader = null; //new GroovyClassLoader(getClass().getClassLoader(), config);
```
1. mvn install -DskipTests from core/
2. mvn test from plugins/cloud-azure. 

LeakFS detects a file leak:

```
Throwable #1: java.lang.RuntimeException: file handle leaks: [FileChannel(/home/rmuir/workspace/elasticsearch/plugins/cloud-azure/target/J0/temp/org.elasticsearch.index.store.SmbMMapFsTests_2C60FEE3D73A0680-001/tempDir-001/d0/SUITE-CHILD_VM=[0]-CLUSTER_SEED=[-1301520842927563395]-HASH=[11EC1C68B1C5F]-cluster/nodes/2/node.lock), 
```

The issue might just be our test harness stuff, but my concern is it could happen "for real" too. In the case of SimpleFSLock it could be annoying (user has to then remove lock file).
</description><key id="107556387">13685</key><summary>Possibly leak of lock, if plugin hits exception on close()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugins</label><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T17:07:28Z</created><updated>2016-02-01T09:31:53Z</updated><resolved>2016-02-01T09:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>scrolled searches silently ignore 'from' search parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13684</link><project id="" key="" /><description>Our application happily creates scrolled search requests with a 'from' parameter &gt; 0.
It looks like the value of 'from' is silently ignored, as the following test, demonstrates:
- indexes some documents
- request sorted results with from &gt; 0, e.g. 4
- search normally: the first four matches are skipped, as expected
- search with scroll enabled: the first four matches are not skipped, this is not expected

reproduceable testcase (for ES-1.7.1, it seems to fail in ES-2.0.0-beta2 too)

``` java
@Seed("1")
@ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE, numDataNodes = 1, numClientNodes = 1, transportClientRatio = 0, randomDynamicTemplates = false)
public class ESScrollIT extends ElasticsearchIntegrationTest {
    @Override
    protected Settings nodeSettings(final int nodeOrdinal) {
        return ImmutableSettings.settingsBuilder()
            .put("path.home", "target/es")
            .put("path.data", "target/es/data")
            .put("path.logs", "target/es/logs")
            .put(super.nodeSettings(nodeOrdinal))
            .build();
    }

    @Test
    public void testSortFromScroll() throws Exception {
        //index some documents
        createIndex("test");
        final Client client = dataNodeClient();
        client.prepareIndex("test", "test", "0").setSource("{ name: \"0\"}").execute().actionGet();
        client.prepareIndex("test", "test", "1").setSource("{ name: \"1\"}").execute().actionGet();
        client.prepareIndex("test", "test", "2").setSource("{ name: \"2\"}").execute().actionGet();
        client.prepareIndex("test", "test", "3").setSource("{ name: \"3\"}").execute().actionGet();
        client.prepareIndex("test", "test", "4").setSource("{ name: \"4\"}").execute().actionGet();
        client.prepareIndex("test", "test", "5").setSource("{ name: \"5\"}").execute().actionGet();
        refresh();
        ensureGreen("test");

        //sort by name and return the 5th result, once with scrolling, once without
        final SearchResponse normal = client.prepareSearch("test")
            .setTypes("test")
            .addSort("name", ASC)
            .setFrom(4)
            .setSize(1)
            .execute()
            .actionGet();
        final SearchResponse scroll = client.prepareSearch("test")
            .setTypes("test")
            .addSort("name", ASC)
            .setFrom(4)
            .setSize(1)
            .setScroll(timeValueMinutes(1))
            .execute()
            .actionGet();

        //make sure we get the same results from normal &amp; scrolled searches
        assertEquals(6, normal.getHits().getTotalHits());
        assertEquals(6, scroll.getHits().getTotalHits());
        assertEquals(1, normal.getHits().getHits().length);
        assertEquals(1, scroll.getHits().getHits().length);
        assertEquals("4", normal.getHits().getHits()[0].id());

        //the next assertion fails with "expected &lt;[4]&gt; but was:&lt;[0]"
        //setFrom(4) has silently been ignored?
        assertEquals("4", scroll.getHits().getHits()[0].id());
    }
}
```

I would have expected either
1) the same result for scrolled searches vs non scrolled 'normal' searches, or alternatively
2) an error message indicating that 'from' is not supported for scrolled searches
</description><key id="107555621">13684</key><summary>scrolled searches silently ignore 'from' search parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drallax</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-09-21T17:02:35Z</created><updated>2016-02-01T17:23:17Z</updated><resolved>2016-01-28T17:42:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T18:09:35Z" id="142064004">&gt; I would have expected either
&gt; 1) the same result for scrolled searches vs non scrolled 'normal' searches, or alternatively
&gt; 2) an error message indicating that 'from' is not supported for scrolled searches

I agree with choosing from one of your two options.  I'm leaning towards option 2, because I'm not sure that `from` is useful in a scroll request and also, having a high `from` value may make the intial request very heavy.
</comment><comment author="drallax" created="2015-09-23T10:45:08Z" id="142563311">For us, a scrolled search that supports `from` would be a useful thing to have.

It would allow us to return arbitrary slices of large result sets, or at least slices whose beginning is not too far removed (say 10,000 or 100,000 documents) from the first result.
In practice, i expect only moderately sized slices (of say a 1000 documents) when `from`  increases, 
but since our documents can be largish, it would be nice if we can use scrolled searches,
to process e.g. 100 documents at a time to prevent running out of memory.

I suppose we can work around this ourselves by either
- adjusting the ES query to return from+size results and skipping the first `from` results ourselves, or
- performing an initial search that finds the `uid` of the `from`'th result and then perform a scrolled search with an adapted query that only matches documents with larger uid's.

But it would probably be much more efficient if ES could do something like this internally?
</comment><comment author="clintongormley" created="2016-01-28T17:42:51Z" id="176298630">It sounds like the newly added search_after parameter would be a better solution. See https://github.com/elastic/elasticsearch/issues/8192
</comment><comment author="drallax" created="2016-02-01T16:29:40Z" id="178056930">after reading the search_after docs, i am not sure how it would help retrieving documents
starting from the n'th hit, without first consuming the n previous unwanted documents, 
which would be slow?

(i am probably missing something here, but i wonder what search_after is actually 
useful for since ES already has a range query that can be used to search after some term?)
</comment><comment author="jimczi" created="2016-02-01T17:23:17Z" id="178079373">&gt; after reading the search_after docs, i am not sure how it would help retrieving documents
&gt; starting from the n'th hit, without first consuming the n previous unwanted documents, 
&gt; which would be slow?

Unfortunately there is no magic and retrieving documents starting from the n'th hit is heavy in terms of memory and the scroll API is not designed for that (it is designed to scroll a query from the start efficiently). So what @clintongormley is saying is that you could:
- do an initial request with from=N and a sort compatible with `search_after`
- ... and then use `search_after` to scroll the results from the N'th hit.
  `search_after` is much faster than using a range query, it is used to sort the top documents and not to match documents. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix eclipse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13683</link><project id="" key="" /><description>Eclipse was blowing up because it couldn't find javax.annotations.Nonnull
so this turns off nullability annotation checking.
</description><key id="107553878">13683</key><summary>Fix eclipse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T16:52:27Z</created><updated>2015-09-21T17:23:06Z</updated><resolved>2015-09-21T17:23:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T16:52:49Z" id="142041253">@rmuir, I think I remember you saying you use eclipse as well. Have you seen this error?
</comment><comment author="rmuir" created="2015-09-21T17:00:03Z" id="142042992">No, and I am using master branch as of this commit with no problems:

```
commit 859f63be6e28ac038826f8e051aaa301baa3f3c8
Author: David Pilato &lt;david@pilato.fr&gt;
Date:   Mon Sep 21 17:53:09 2015 +0200

    Fix plugins/cloud-azure/licenses/azure-LICENSE.txt

    Closes #13679.
```

I do not know how you configure eclipse, but there is only one way: `mvn eclipse:eclipse`. I know there are other ways but we cannot possibly support all these ways to do it.
</comment><comment author="rmuir" created="2015-09-21T17:01:52Z" id="142043396">Separately, I hear the eclipse null analysis is not really reliable. But we should ask @jpountz about this, I think he knows more.
</comment><comment author="nik9000" created="2015-09-21T17:08:52Z" id="142044919">&gt; I do not know how you configure eclipse, but there is only one way: mvn eclipse:eclipse. I know there are other ways but we cannot possibly support all these ways to do it.

I've been trying:
1. Delete all the elasticsearch projects.
2. `mvn eclipse:clean &amp;&amp; mvn eclipse:eclipse -Pdev`
3. Import them all into eclipse.

That gives me 1232345435143143 errors and when I track them down its all caused by not being able to find `javax.annotations.Nonnull` which we don't use, at least not directly. I tracked that down to a part of Eclipse's null analysis in the workspace configuration (Preferences -&gt; Java -&gt; Compiler Errors/Warnings ----&gt; Null Analysis ----&gt; The configure link in "Use default annotations for null specifications"). When I disable null analysis the errors go away. This commit disables null analysis on project generation.

Its weird that its specific to me though. It happened as soon as I tried to import any Java 8.

If the null analysis isn't good do you think we should merge this defensively or I should try more drastic measures on my eclipse setup?
</comment><comment author="rmuir" created="2015-09-21T17:09:47Z" id="142045129">What version of eclipse are you using?
</comment><comment author="nik9000" created="2015-09-21T17:11:04Z" id="142045403">&gt; What version of eclipse are you using?

Eclipse IDE for Java Developers

Version: Mars Release (4.5.0)
Build id: 20150621-1200

That looks like the current release.
</comment><comment author="rmuir" created="2015-09-21T17:13:29Z" id="142045929">I'm not using that release. In my release (4.4.2) annotation-based null analysis is not enabled by default. Maybe its a defaults change for 4.5?
</comment><comment author="rmuir" created="2015-09-21T17:14:48Z" id="142046203">And yes, i agree with the "defensive" option. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove search-exists API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13682</link><project id="" key="" /><description>The search-exists API tells you if there are any matching results, without performing a count. Besides this being an almost-never-useful operation, it could easily be replicated with:

```
GET _search?size=0&amp;terminate_after=1
```

I propose removing this API.
</description><key id="107547233">13682</key><summary>Remove search-exists API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label></labels><created>2015-09-21T16:15:04Z</created><updated>2015-10-21T15:39:57Z</updated><resolved>2015-10-21T15:39:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T16:20:20Z" id="142032627">&gt; I propose removing this API.

I'm certainly fine with it. I've never used it in production.
</comment><comment author="javanna" created="2015-09-22T08:10:42Z" id="142208336">+1
</comment><comment author="bleskes" created="2015-09-22T08:33:33Z" id="142212078">Will we also wire HEAD _search to do the same and add terminate_after?

&gt; On 22 Sep 2015, at 10:10, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; +1
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2015-09-22T13:20:48Z" id="142287675">@bleskes no - some clients don't support HEAD with body.  I don't see the need for sugar here
</comment><comment author="bleskes" created="2015-09-22T14:38:01Z" id="142307480">I don&#8217;t follow the logic? if you _do_ submit a head a request, why not make sure return as fast as possible with the information needed (this is something there).

&gt; On 22 Sep 2015, at 15:21, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; @bleskes no - some clients don't support HEAD with body. I don't see the need for sugar here
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="HonzaKral" created="2015-09-22T15:10:38Z" id="142319012">Also do we add/leave this functionality in the client as a shortcut? There is a precedent to it with the `create` API that is just `index` with `op_type=create`.
</comment><comment author="clintongormley" created="2015-09-22T17:39:00Z" id="142358621">@bleskes first, i have never heard of anybody using this feature, and can't come up with a real use case for it anyway.  Second, not all clients can support it.  Third, it is easy to replicate the functionality by just setting `?size=0&amp;terminate_after=1`

@HonzaKral I would say we don't bother supporting it in the client as the feature is really not useful.
</comment><comment author="bleskes" created="2015-09-22T18:14:53Z" id="142369388">&gt; first, i have never heard of anybody using this feature, and can't come up with a real use case for it anyway.

I was asked about it a while ago. Not sure if the use case at the time. The one I can think of out of the top of my had is wanting to validate that a search link will actually lead to results - for example a "did you mean" recommendation link (which may come from the completion suggester or another system).

&gt; Second, not all clients can support it

Those that do can, and those don't can't. Just like GET with a body imho. As you say, it's not in your way if you can't.

All I wanted is to make sure this gets some attention. It's super easy to implement on the ES side (and we don't now) and to me is what I would expect from a HEAD _search request. If you rather not do but wait for an, eventual, feature request - it's good with me.
</comment><comment author="clintongormley" created="2015-10-02T15:00:28Z" id="145051718">Another good reason for removing search-exists: https://github.com/elastic/elasticsearch/issues/13094
</comment><comment author="javanna" created="2015-10-02T16:40:26Z" id="145082303">I am looking into this. I think we may want to make `terminate_after` non experimental before using it as a replacement for apis that we are removing. Any reasons why `terminate_after` is still experimental in our docs?
</comment><comment author="jpountz" created="2015-10-02T16:57:47Z" id="145085822">I _think_ it has been made experimental because it was initially used by suggesters. But then we deprecated the limit filter in favour of it and now we are thinking of doing the same for the search/exists API, so it probably makes sense to remove the experimental flag.
</comment><comment author="HonzaKral" created="2015-10-04T17:02:44Z" id="145366693">@clintongormley do you have any data why you are saying it's not useful? People are using it and asking questions around it. Due to it's inconsistencies (#11204) more than about any other API.
</comment><comment author="clintongormley" created="2015-10-06T12:49:53Z" id="145847056">@HonzaKral i don't have data, just instinct (which may well be wrong).  However, this is a really simple function that can be entirely replaced by the search API, which is much more thoroughly tested (and much more widely used).  My goal here is to reduce the surface area of these APIs to make the code base easier to maintain and test.

You're free to add a convenience method to the python API if you feel strongly about it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow aliases to be renamed upon _restore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13681</link><project id="" key="" /><description>Currently, upon restoring an index, it's possible to rename one or more indices using the pattern/replacement properties.

It is often the case where aliases exist alongside indices with similar names. While less common, it's sometimes desirable to rename the _alias_ as well as the index to match (not identically, of course, but following similar results and patterns).

The annoying aspect is that this requires a secondary rename/replacement field and it needs to be distinctly different from the existing one.

I would propose:

``` json
"rename_alias_pattern": "audit_hist_(.+)",
"rename_alias_replacement": "audit_history_$1",
"ignore_global" : false
```
</description><key id="107544653">13681</key><summary>Allow aliases to be renamed upon _restore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>docs</label></labels><created>2015-09-21T16:02:19Z</created><updated>2016-01-29T09:23:06Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T18:06:24Z" id="142063334">Hmmm... I wonder if we should restore aliases at all, or just always strip them on restore?
</comment><comment author="pickypg" created="2015-09-21T18:09:38Z" id="142064010">@clintongormley I think by default, when you don't rename, that it makes a lot of sense to restore them. If you do rename them, then I agree that it makes things a bit murkier.
</comment><comment author="apatrida" created="2015-11-16T16:07:48Z" id="157082600">We run into issues a lot with restoring temporary indexes that have aliases, would be good to have an option saying whether you want aliases restored or not for an index, and renaming them if you want (rename is less necessary if you have one index, but do make sense when you have unknown group of indexes that have the alias you want to rename)
</comment><comment author="clintongormley" created="2016-01-28T17:41:12Z" id="176297949">My vote is simply to remove aliases during restore.  @imotov what do you think?
</comment><comment author="imotov" created="2016-01-28T17:48:31Z" id="176302514">@clintongormley I think we should improve documentation and make `include_aliases` flag more visible. 
</comment><comment author="clintongormley" created="2016-01-29T09:22:51Z" id="176659129">Ah I didn't even know that option existed. Thanks @imotov 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HEAD support for the _search/exists api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13680</link><project id="" key="" /><description>Registers the `_search/exists` api to receive HEAD requests. If it gets one
then it won't output the JSON response but will still do everything else.

Adds magic to the rest tests to ignore `catch: missing` for HEAD requests.
This behavior is checked instead with `- is_false: anystring`. This setup
makes the rest tests for `search/_exists` work for HEAD requests.

Also forces the request body over the source URL parameter for all HEAD
requests.

Closes #11204
</description><key id="107542897">13680</key><summary>HEAD support for the _search/exists api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:REST</label><label>:Search</label><label>enhancement</label><label>feedback_needed</label></labels><created>2015-09-21T15:53:38Z</created><updated>2015-11-08T22:03:03Z</updated><resolved>2015-11-08T22:03:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T15:56:14Z" id="142025728">Ping @HonzaKral and maybe @areek for review.

The linked issue mentions some client API changes but I think what we have now makes sense as `_search/exists` is an action that hits many shards so the extra information returned in the Java api is fine.
</comment><comment author="HonzaKral" created="2015-09-21T16:06:01Z" id="142028719">I cannot speak for the code (LGTM but I am not a java person) but we should also change the yaml tests (https://github.com/nik9000/elasticsearch/tree/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test/search_exists) to not check the body and instead behave as other `exist_` apis (for example see https://github.com/nik9000/elasticsearch/tree/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test/indices.exists).

Thanks!
</comment><comment author="nik9000" created="2015-09-21T16:09:11Z" id="142029948">&gt; I cannot speak for the code (LGTM but I am not a java person) but we should also change the yaml tests (https://github.com/nik9000/elasticsearch/tree/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test/search_exists) to not check the body and instead behave as other exist_ apis (for example see https://github.com/nik9000/elasticsearch/tree/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test/indices.exists).

As I've got them right now the REST tests only look at the body if the request was GET or POST and then they still verify what they used to verify. If the request was HEAD they just check for the 404. I did it in a sneaky way though.
</comment><comment author="HonzaKral" created="2015-09-21T16:14:20Z" id="142031183">https://github.com/nik9000/elasticsearch/blob/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test/search_exists/10_basic.yaml#L26 this line will fail when HEAD is used.
</comment><comment author="nik9000" created="2015-09-21T16:17:43Z" id="142031997">&gt; https://github.com/nik9000/elasticsearch/blob/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test/search_exists/10_basic.yaml#L26 this line will fail when HEAD is used.

Not after this patch it won't. Not in the Java version. That is what I mean by

&gt; This behavior is checked instead with - is_false: anystring.

The operative change is [here](https://github.com/elastic/elasticsearch/pull/13680/files#diff-22041b85e792b81af836d20fe594da4dL89).
</comment><comment author="nik9000" created="2015-09-21T16:19:05Z" id="142032327">I'm fine with doing it in another way but this was the only way I could figure to keep the specs logicless and to test the body when there was one and the 404-ness when there wasn't.
</comment><comment author="HonzaKral" created="2015-09-21T16:29:18Z" id="142035149">According to the specs of the yaml test suite this **should** fail with HEAD: https://github.com/nik9000/elasticsearch/tree/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test#is_true specifies that the key must exist for this assertion to pass.
</comment><comment author="nik9000" created="2015-09-21T16:36:40Z" id="142036835">&gt; According to the specs of the yaml test suite this should fail with HEAD: https://github.com/nik9000/elasticsearch/tree/search_exists_head/rest-api-spec/src/main/resources/rest-api-spec/test#is_true specifies that the key must exist for this assertion to pass.

There are specs for this?! Nice! I had no idea. They are out of date though - `is_true: ''` is currently a special case and checks if the request was a 200. `is_false: ''` does a 404 check.

So what do you want:
1. The tests ignore the GET and POST for this request and only ever do a HEAD.
2. The tests ignore the HEAD and only ever do GET or POST.
3. I change the specs to reflect that HEAD requests get all `true` for `200` and all `false` for `404`. 
4. I just stop working on this entirely and wait for a decision on #13682.
</comment><comment author="HonzaKral" created="2015-09-21T16:41:03Z" id="142037796">I think `4` is the best option for now, if it doesn't work I'd just remove the `is_true` and `is_false` statements altogether - that way only the `do` commands will be checked which means the status code (one plain which means that the request must be asuccess (`2XX`) and the other one has `catch: missing` which means it asserts the status code to be `404`, exactly as needed in both cases.
</comment><comment author="nik9000" created="2015-09-21T16:43:32Z" id="142038384">&gt; I think 4 is the best option for now, if it doesn't work I'd just remove the is_true and is_false statements altogether - that way only the do commands will be checked which means the status code (one plain which means that the request must be asuccess (2XX) and the other one has catch: missing which means it asserts the status code to be 404, exactly as needed in both cases.

Everywhere else HEAD requests get an implicit `ignore: missing`.... Lets just ignore this until #13682 is decided.
</comment><comment author="javanna" created="2015-10-31T08:44:41Z" id="152715752">Search exists has been removed from master. We are still discussing in #11204 whether we should add support for `HEAD` to `_search`, but that seems to complicate things quite a bit. We are also better documenting the search exists and how to utilize the search api to achieve the same (see #https://github.com/elastic/elasticsearch/pull/14393). That said I think this PR could be closed, thanks for your work @nik9000 though!
</comment><comment author="clintongormley" created="2015-11-08T22:03:03Z" id="154879226">@javanna agreed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix plugins/cloud-azure/licenses/azure-LICENSE.txt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13679</link><project id="" key="" /><description>`plugins/cloud-azure/licenses/azure-LICENSE.txt` content is incorrect (HTML content instead of TXT content).
</description><key id="107542421">13679</key><summary>Fix plugins/cloud-azure/licenses/azure-LICENSE.txt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label><label>v2.0.0-rc1</label></labels><created>2015-09-21T15:51:07Z</created><updated>2016-03-10T18:16:24Z</updated><resolved>2015-09-21T15:54:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>In cluster health REST spec, {index} can be one or many indices and should be typed to list.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13678</link><project id="" key="" /><description>{index} can be one or many indices and should be typed to list.
</description><key id="107539293">13678</key><summary>In cluster health REST spec, {index} can be one or many indices and should be typed to list.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels><label>:REST</label><label>bug</label><label>review</label></labels><created>2015-09-21T15:38:21Z</created><updated>2015-10-06T10:00:42Z</updated><resolved>2015-10-06T09:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T15:42:14Z" id="142020235">LGTM
</comment><comment author="Mpdreamz" created="2015-10-06T10:00:42Z" id="145807571">merged to master and cherry-picked to `2.0`. `2.1`, `2.x`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch listening on mutiple specific ips</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13677</link><project id="" key="" /><description>Hi,

Can elasticsearch be configured to listen on multiple IPs, but not on all?
So I want it to listen on all private ips + localhost, but not on the public ips all present on the same node.

Thanks
</description><key id="107539162">13677</key><summary>Elasticsearch listening on mutiple specific ips</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">siddharthasahu</reporter><labels /><created>2015-09-21T15:37:36Z</created><updated>2015-09-21T15:39:18Z</updated><resolved>2015-09-21T15:39:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T15:39:18Z" id="142018419">Duplicate of #13592.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when running through bound ports</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13676</link><project id="" key="" /><description>If there is already an ES running on a port and we try to bind to it, you can get weird errors. In case they're NPEs, I'm assuming because it's 2.x trying to talk to 1.4.4. We should handle this more gracefully.

```
[2015-09-21 10:01:01,575][INFO ][org.elasticsearch.node   ] [Fabian Cortez] version[2.0.0-rc1-SNAPSHOT], pid[438], build[33dbe64/2015-09-21T14:49:58Z]
[2015-09-21 10:01:01,576][INFO ][org.elasticsearch.node   ] [Fabian Cortez] initializing ...
[2015-09-21 10:01:01,628][INFO ][org.elasticsearch.plugins] [Fabian Cortez] loaded [], sites []
[2015-09-21 10:01:01,663][INFO ][org.elasticsearch.env    ] [Fabian Cortez] using [1] data paths, mounts [[/d (/dev/md0)]], net usable_space [76.4gb], net total_space [447.4gb], spins? [possibly], types [xfs]
[2015-09-21 10:01:02,360][INFO ][org.elasticsearch.node   ] [Fabian Cortez] initialized
[2015-09-21 10:01:02,360][INFO ][org.elasticsearch.node   ] [Fabian Cortez] starting ...
[2015-09-21 10:01:02,399][INFO ][org.elasticsearch.transport.netty] [Fabian Cortez] Bound profile [default] to address {127.0.0.1:9305}
[2015-09-21 10:01:02,399][INFO ][org.elasticsearch.transport.netty] [Fabian Cortez] Bound profile [default] to address {[::1]:9300}
[2015-09-21 10:01:02,400][INFO ][org.elasticsearch.transport] [Fabian Cortez] bound_address {127.0.0.1:9305}, publish_address {127.0.0.1:9305}
[2015-09-21 10:01:02,406][INFO ][org.elasticsearch.discovery] [Fabian Cortez] elasticsearch/f_e00ueOTImtSTXhxQC_cA
[2015-09-21 10:01:02,458][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0xcc2174cb, /127.0.0.1:53000 =&gt; /127.0.0.1:9303]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:02,462][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x84cb4c45, /127.0.0.1:38328 =&gt; /127.0.0.1:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:02,459][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0xb6785399, /127.0.0.1:48065 =&gt; /127.0.0.1:9302]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:02,458][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x1347095e, /127.0.0.1:34728 =&gt; /127.0.0.1:9304]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:02,463][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x6230ba89, /127.0.0.1:54629 =&gt; /127.0.0.1:9301]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:03,914][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x8f5f6d28, /127.0.0.1:54635 =&gt; /127.0.0.1:9301]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:03,915][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x07361647, /127.0.0.1:34740 =&gt; /127.0.0.1:9304]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:03,915][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x67a1d1de, /127.0.0.1:53010 =&gt; /127.0.0.1:9303]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:03,916][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0xcf4c2ebd, /127.0.0.1:48070 =&gt; /127.0.0.1:9302]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:03,917][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x16ad7265, /127.0.0.1:38331 =&gt; /127.0.0.1:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:05,420][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x8f6c4c02, /127.0.0.1:38338 =&gt; /127.0.0.1:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:05,425][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0xf632c28f, /127.0.0.1:34751 =&gt; /127.0.0.1:9304]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:05,425][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x56b6d7fc, /127.0.0.1:54643 =&gt; /127.0.0.1:9301]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:05,425][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x07ff7527, /127.0.0.1:53021 =&gt; /127.0.0.1:9303]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:05,425][WARN ][org.elasticsearch.transport.netty] [Fabian Cortez] exception caught on transport layer [[id: 0x76758374, /127.0.0.1:48079 =&gt; /127.0.0.1:9302]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
[2015-09-21 10:01:06,920][INFO ][org.elasticsearch.cluster.service] [Fabian Cortez] new_master {Fabian Cortez}{f_e00ueOTImtSTXhxQC_cA}{127.0.0.1}{127.0.0.1:9305}, reason: zen-disco-join(elected_as_master, [0] joins received)
[2015-09-21 10:01:06,953][INFO ][org.elasticsearch.http.netty] [Fabian Cortez] Bound http to address {127.0.0.1:9205}
[2015-09-21 10:01:06,954][INFO ][org.elasticsearch.http.netty] [Fabian Cortez] Bound http to address {[::1]:9200}
[2015-09-21 10:01:06,954][INFO ][org.elasticsearch.http   ] [Fabian Cortez] bound_address {127.0.0.1:9205}, publish_address {127.0.0.1:9205}
[2015-09-21 10:01:06,955][INFO ][org.elasticsearch.node   ] [Fabian Cortez] started
[2015-09-21 10:01:06,970][INFO ][org.elasticsearch.gateway] [Fabian Cortez] recovered [0] indices into cluster_state
  C-c C-c^C[2015-09-21 10:01:07,486][INFO ][org.elasticsearch.node   ] [Fabian Cortez] stopping ...
[2015-09-21 10:01:07,495][INFO ][org.elasticsearch.node   ] [Fabian Cortez] stopped
[2015-09-21 10:01:07,495][INFO ][org.elasticsearch.node   ] [Fabian Cortez] closing ...
[2015-09-21 10:01:07,498][INFO ][org.elasticsearch.node   ] [Fabian Cortez] closed
```
</description><key id="107532812">13676</key><summary>NullPointerException when running through bound ports</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">drewr</reporter><labels><label>:Network</label><label>bug</label></labels><created>2015-09-21T15:05:52Z</created><updated>2015-11-06T15:54:56Z</updated><resolved>2015-10-07T16:52:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T16:42:25Z" id="146257833">I tried to start to Elasticsearch nodes on the same port and the second one gave me:

```
BindTransportException[Failed to bind to [9501]]; nested: ChannelException[Failed to bind to: /127.0.0.1:9501]; nested: BindException[Address already in use];
```

and then an exception. And then stopped. Which seems to be just about right.
</comment><comment author="nik9000" created="2015-10-07T16:51:09Z" id="146260029">I just tried it with 1.7.2 running on the same port and binding to the same interface as master. Master reported the same failure.

I then tried 1.7.2. and 2.0.0. Then 2.0.0 holding the port with 1.7.2 trying to bind to it. All reported a relatively sane error message.
</comment><comment author="nik9000" created="2015-10-07T16:52:22Z" id="146260321">Given what I've seen so far I'm going to close this. @drewr feel free to reopen if I've reproduced this the wrong way around.
</comment><comment author="krisb78" created="2015-11-06T11:33:04Z" id="154389142">I'm trying to upgrade from 1.7.2 to 2.0.0 and I run into this issue. 
I have 2 client nodes, 3 master nodes and 5 data nodes in the cluster.

I upgraded ES to 2.0.0 on one of the client nodes, installed the new version of the cloud-aws plugin as described here: https://www.elastic.co/guide/en/elasticsearch/plugins/current/cloud-aws.html (naturally, I removed the previous version first).

When I'm restarting the upgraded node, I'm getting:

```
[2015-11-06 10:55:27,553][INFO ][bootstrap                ] max_open_files [4096]
[2015-11-06 10:55:28,578][INFO ][node                     ] [cubitsearch-client-1] version[2.0.0], pid[13014], build[de54438/2015-10-22T08:09:48Z]
[2015-11-06 10:55:28,579][INFO ][node                     ] [cubitsearch-client-1] initializing ...
[2015-11-06 10:55:29,756][INFO ][plugins                  ] [cubitsearch-client-1] loaded [cloud-aws], sites []
[2015-11-06 10:55:37,212][INFO ][node                     ] [cubitsearch-client-1] initialized
[2015-11-06 10:55:37,212][INFO ][node                     ] [cubitsearch-client-1] starting ...
[2015-11-06 10:55:37,384][INFO ][transport                ] [cubitsearch-client-1] publish_address {127.0.0.1:9300}, bound_addresses {127.0.0.1:9300}, {[::1]:9300}
[2015-11-06 10:55:37,402][INFO ][discovery                ] [cubitsearch-client-1] elasticsearch/LCKZRw-mTAW0CavTUudOmw
[2015-11-06 10:55:42,030][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0x59cdb796, /10.0.1.231:33550 =&gt; /10.0.1.235:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-06 10:55:42,057][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0x4679277f, /10.0.1.231:35707 =&gt; /10.0.1.239:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-06 10:55:42,075][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0x5a3b9587, /10.0.1.231:59426 =&gt; /10.0.1.232:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-06 10:55:42,076][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0x61c9ff87, /10.0.1.231:43141 =&gt; /10.0.1.237:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
[2015-11-06 10:55:42,078][WARN ][transport.netty          ] [cubitsearch-client-1] exception caught on transport layer [[id: 0x73098dc9, /10.0.1.231:56611 =&gt; /10.0.1.238:9300]], closing connection
java.lang.NullPointerException
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:206)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:201)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:136)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Am I missing something obvious here?
</comment><comment author="dakrone" created="2015-11-06T15:54:56Z" id="154446700">This is a duplicate of  #14400
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BATS testing: Add SLES-12 to list of tested virtual machines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13675</link><project id="" key="" /><description>This adds SuSe Linux Enterprise Server 12 to the list of tested VMs.
SLES 12 is using systemd, so that the current RPM works
out of the box.

SLES12 however is already quite old and does not ship with java8, so this
required adding an opensuse repo.
</description><key id="107509027">13675</key><summary>BATS testing: Add SLES-12 to list of tested virtual machines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>review</label><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T13:15:58Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-22T13:04:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T13:47:56Z" id="141983485">LGTM
</comment><comment author="andrestc" created="2015-09-21T19:09:11Z" id="142080540">Maybe the list of supported linux flavours should be updated in the [testing docs](https://github.com/elastic/elasticsearch/blob/master/TESTING.asciidoc)?
</comment><comment author="tlrx" created="2015-09-22T18:29:14Z" id="142373477">Nice work!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>exceptions from native search scripts are no longer propagated to client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13674</link><project id="" key="" /><description>In ES-2.0.0-beta2 (and also in ES-2.0.0-beta1), when an exception in thrown from a native 
search script, Java node clients no longer seem to receive any indication that something went wrong.

For example, we have a native sort script that takes a parameter and throws an exception in case it is missing:

``` java
    private static class MyFactory implements NativeScriptFactory {
        @Override
        public ExecutableScript newScript(final Map&lt;String, Object&gt; params) {
            if (params.get("field")== null) {
                throw new ScriptException("missing parameter: field");
            }
            ...
        }

        @Override
        public boolean needsScores() {
            return false;
        }
    }
```

however, the following integration test, that used to work with ES-1.7.1, now fails:

``` java
public abstract class MyScriptIT extends ESIntegTestCase {
    @Rule
    public ExpectedException _exception = ExpectedException.none();

    @Test
    public void testMissingFieldFails() throws Exception {
        createIndex(...);
        ensureGreen(...);

        _exception.expect(SearchPhaseExecutionException.class);
        _exception.expectMessage("missing parameter: field");

        final Map&lt;String, Object&gt; params = new HashMap&lt;&gt;();
        final Script script = new Script("my-script", ScriptType.INLINE, "native", params);
        final ScriptSortBuilder script = SortBuilders.scriptSort(script, "number");
        return _client.prepareSearch(...).addSort(script).execute().actionGet();
    }
}
```

in fact, no exception at all is thrown

```
java.lang.AssertionError: Expected test to throw (an instance of org.elasticsearch.action.search.SearchPhaseExecutionException and exception with message a string containing "missing parameter: field")
...
```

in addition, it appears that 
- scripted fetch fields, scripted metrics also don't propagate any thrown exceptions.
  e.g. AggregationBuilders.sum("sum").script(...)
  e.g. AggregationBuilders.scriptedMetric(...)
- update scripts on the other hand still do return exceptions. 
  e.g. return _client.prepareUpdate("test", "test", id).setScript(...);
  However, the returned message always is "failed to execute script", 
  while it used to contain the message that was generated by our native script. 
</description><key id="107493741">13674</key><summary>exceptions from native search scripts are no longer propagated to client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">drallax</reporter><labels><label>:Exceptions</label><label>docs</label><label>v2.0.0</label></labels><created>2015-09-21T11:40:53Z</created><updated>2015-10-07T14:36:50Z</updated><resolved>2015-10-07T14:36:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T13:08:43Z" id="141968872">&gt; ```
&gt;     createIndex(...);
&gt;     ensureGreen(...);
&gt; ```

In this particular case my guess is that the native script isn't being executed at all because there are no results. If there are results then I'm wrong here.

Is there any chance you could post a gist or even a small reproduction project?

If you happen to still be around and want a faster conversation you can reach out to me in #elasticsearch on freenode. I'm `manybubbles` there.
</comment><comment author="nik9000" created="2015-09-21T13:11:57Z" id="141969520">&gt; Is there any chance you could post a gist or even a small reproduction project?

It'd probably be even better if you could reproduce this with a bash script and curl - but I'll take what I can get.

&gt; scripted fetch fields, scripted metrics also don't propagate any thrown exceptions.
&gt; e.g. AggregationBuilders.sum("sum").script(...)
&gt; e.g. AggregationBuilders.scriptedMetric(...)

Hmmm. These _could_ be consistent with my theory about no hits not triggering it. Or not. 

&gt; however, the following integration test, that used to work with ES-1.7.1, now fails

Hmmm. Curiouser and curiouser.
</comment><comment author="drallax" created="2015-09-21T14:15:41Z" id="141995639">Thanks for getting back on this so quickly.
The source code I posted was a reduced down sketch of what is actually happening, and yes, in reality we do hit documents, so I expect there is a regression somewhere.
I'll see if i can get you some fully working examples (probably as ES integration tests) of the oddities we're seeing here
</comment><comment author="nik9000" created="2015-09-21T14:26:38Z" id="141998334">&gt; I'll see if i can get you some fully working examples (probably as ES integration tests) of the oddities we're seeing here

Thanks!
</comment><comment author="drallax" created="2015-09-21T15:02:13Z" id="142008003">found the problem:
the search response that ES-2.0 server-side returns looks like this (perfectly fine):

```
{
  "took" : 108,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 2,
    "failed" : 1,
    "failures" : [ {
      "shard" : 2,
      "index" : "test",
      "node" : "jsoxbkP9QDGnl5lSIHYBvQ",
      "reason" : {
        "type" : "script_exception",
        "reason" : "missing parameter: field"
      }
    } ]
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  },
  }
}
```

However, our tests fail since

```
_client.prepareSearch().....execute().actionGet()
```

behaves differently from before
in ES-1.7.1 and earlier, it would throw an exception,
in ES-2.0.0-beta1/2, it will NOT throw an exception anymore,
but we end up with a SearchResponse in our hands that we can check for failed shards.
(The same difference exists when adding an Actionlistener to the SearchRequest:
in ES-1.7.1 ActionListener.onFailure() is triggered,
in ES-2.0.0 ActionListener.onResponse() is called instead
)

I am not sure if this qualifies as a bug or intended behaviour?
If the latter, it would be nice if the release notes/migration guide 
would mention that in the Java API (all?) responses now have to be checked for failed shards.
</comment><comment author="nik9000" created="2015-09-21T15:11:45Z" id="142010937">That makes sense to me.

@clintongormley - I think we have our answer to why.

I don't think we can call this a bug - it was probably intentional and being relied on for node to node communication - but it is a pain. It certainly should be in the list of breaking changes.

I've been thinking a lot about how a real Java API for elasticsearch (compared to coopting the internode communications API like we do now) would behave lately and I'm not sure what the right thing to do in this situation is. There was a failure and so the user should know about it so an exception makes sense. But the failure wasn't complete and you can learn _something_ from the response so throwing an exception feels a bit like throwing the baby out with the bathwater. Maybe throwing an exception the contains a reference to the original request....
</comment><comment author="drallax" created="2015-09-21T15:47:55Z" id="142022609">Sounds great :)
</comment><comment author="nik9000" created="2015-10-06T20:10:13Z" id="145983960">I've taken this one. For now I'll see about getting it documented. I'll make sure to bring this up against the Java API when that gets started.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud AWS client side encryption</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13673</link><project id="" key="" /><description>See elasticsearch-cloud-aws PR 118 from @NicolasTr
https://github.com/elastic/elasticsearch-cloud-aws/pull/118
</description><key id="107489102">13673</key><summary>Cloud AWS client side encryption</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GlenRSmith</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>enhancement</label></labels><created>2015-09-21T11:05:48Z</created><updated>2017-04-28T14:56:12Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="NicolasTr" created="2015-10-14T03:36:49Z" id="147922835">Thanks for creating this @GlenRSmith

I'll port the code as soon as I can. Hopefully this month, more realistically somewhere in November.

Did the structure of the project change when the cloud aws plugin was moved to elasticsearch?

@dadoonet 
</comment><comment author="xuzha" created="2015-10-14T05:15:27Z" id="147937133">@NicolasTr  Yes, the`cloud-aws`plugin has been split into two separate plugins : `repository-s3` and `discovery-ec2`. I think this PR is only related to Repository-s3.
</comment><comment author="dadoonet" created="2015-11-23T14:12:58Z" id="158941981">@NicolasTr IIRC you told us that you want to port your changes to this repository. Is it still your plan? Thanks!
</comment><comment author="samcday" created="2016-02-27T11:13:22Z" id="189618978">It's kinda a bummer that by the looks of things @NicolasTr had a totally functional, reviewed, tested PR that was even run through the CLA gauntlet, and it didn't end up making it into the project! :(
</comment><comment author="xuzha" created="2016-03-24T21:18:37Z" id="201024915">The PR has been reverted, reopen this
</comment><comment author="dchang-novotec" created="2017-02-13T16:09:24Z" id="279437516">Any chance that this will be making it into the next release?</comment><comment author="jasontedor" created="2017-02-13T16:17:14Z" id="279439832">&gt; Any chance that this will be making it into the next release?

I can tell you right now that the answer to that is definitively "no".</comment><comment author="dchang-novotec" created="2017-04-28T14:56:12Z" id="298020669">+1  Will definitely like to see this feature soon</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch geo validation to enum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13672</link><project id="" key="" /><description>Relates to #13608:

This switches handling coerce and ignore_malformed parameters to an enum to better reflect their relationship.

@cbuescher / @nknize / @colings86 can either of you have a look please whether those changes make sense and are in line with the issue above before I move on to the other geo queries.
</description><key id="107474193">13672</key><summary>Switch geo validation to enum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Query Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T09:23:10Z</created><updated>2016-03-18T13:24:25Z</updated><resolved>2015-09-24T20:56:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-21T11:48:25Z" id="141950131">@MaineC Looks good, just left a few minor suggestions. Maybe @nknize can comment on whether the enum is clear enough from user perspective to remove the two independent setters from the java api.
</comment><comment author="MaineC" created="2015-09-22T08:03:33Z" id="142207315">@cbuescher addressed your comments. Will move on to other geo queries as well.

@colings86 Saw your comments just when I was about to write that I had addressed all comments. Fixed yours as well.
</comment><comment author="MaineC" created="2015-09-22T09:32:08Z" id="142226292">Also adapted GeoDistanceQueryBuilder, GeoDistanceRangeQueryBuilder, GeoPolygonQueryBuilder.
</comment><comment author="cbuescher" created="2015-09-24T15:28:40Z" id="142963359">@MaineC LGTM, left two minor suggestion but from my side this looks great.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding backoff from retries on GCE errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13671</link><project id="" key="" /><description>In case of any error while trying to get GCE instances list from GCE API, elasticsearch will slow down its API calls.

Closes #13460.

Borrowed from https://github.com/elastic/elasticsearch-cloud-gce/pull/48

I added two more config here. Simply because I think the `DEFAULT_MAX_ELAPSED_TIME_MILLIS` is 15 mins is probably too much.

Please let me know if they are not necessary.
</description><key id="107463023">13671</key><summary>Adding backoff from retries on GCE errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud GCE</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-21T08:13:01Z</created><updated>2015-10-21T17:10:18Z</updated><resolved>2015-10-19T17:41:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-21T08:51:35Z" id="141914006">@tlrx You already reviewed https://github.com/elastic/elasticsearch-cloud-gce/pull/48 but it was missing some tests. @xuzha Added them and other options. Could you review this one please?
</comment><comment author="xuzha" created="2015-10-08T07:01:32Z" id="146438933">Ping @tlrx, could you please review this PR ?
</comment><comment author="tlrx" created="2015-10-08T14:02:29Z" id="146555289">@xuzha Sure, sorry for the delay. Can you please fix the conflicts so that I can run it? Thanks
</comment><comment author="xuzha" created="2015-10-08T17:08:08Z" id="146626013">@tlrx thanks so much, just rebased. Please let me know what you think
</comment><comment author="tlrx" created="2015-10-14T12:23:37Z" id="148032383">@xuzha I left few comments
</comment><comment author="xuzha" created="2015-10-14T17:06:51Z" id="148120078">Thanks @tlrx for the review, I addressed your comments.
</comment><comment author="tlrx" created="2015-10-15T12:43:36Z" id="148373962">@xuzha LGTM (and sorry again for the time it took me to review this)
</comment><comment author="xuzha" created="2015-10-15T17:26:48Z" id="148465674">@tlrx ha, thanks so much for the review :-)
</comment><comment author="clintongormley" created="2015-10-20T12:37:02Z" id="149551810">@xuzha should this be cherry picked to 2.1 and 2.x?
</comment><comment author="xuzha" created="2015-10-20T18:49:29Z" id="149665158">@clintongormley sure thing, this probably need some extra work here, because we renamed the GCE plugin. Will finish it by the end of today. 
</comment><comment author="nik9000" created="2015-10-20T19:01:44Z" id="149668311">Oh! #14215
</comment><comment author="xuzha" created="2015-10-20T19:05:39Z" id="149669672">Oh, thanks @nik9000 I will wait for your PR. 
And sorry I didn't convert the test to ESTestCase.
</comment><comment author="nik9000" created="2015-10-20T21:25:12Z" id="149707941">&gt; Oh, thanks @nik9000 I will wait for your PR. 
&gt; And sorry I didn't convert the test to ESTestCase.

OK! #14215 is merged! Can you drag it along when you backport this?
</comment><comment author="xuzha" created="2015-10-20T21:47:59Z" id="149712861">@nik9000 thanks for fixing this. Will do it tonight.  
</comment><comment author="xuzha" created="2015-10-21T06:41:14Z" id="149796721">@clintongormley done, also included #14215

2.1 : https://github.com/elastic/elasticsearch/commit/5b8d2d5af7df0c1e20db31c140bb31921dd8f95e
2.x : https://github.com/elastic/elasticsearch/commit/b9ee2906dd7f901825f4ce51d96dec5aca89c424

Update: the change is not in 2.1, since 2.1 don't have another change which the fix based on.
</comment><comment author="xuzha" created="2015-10-21T07:05:27Z" id="149800030">@clintongormley, I'm sorry, branch 2.1 don't have the Permission change, this PR is only in 2.x and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added more detail on word_list_path requirements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13670</link><project id="" key="" /><description>Fixes #13595
</description><key id="107445454">13670</key><summary>Added more detail on word_list_path requirements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2015-09-21T05:05:13Z</created><updated>2015-10-06T08:37:07Z</updated><resolved>2015-09-21T09:22:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: moving validation to setters and constructors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13669</link><project id="" key="" /><description>Moving validation from validate() to constructors and setters for the following query builders:
- GeoDistanceQueryBuilder
- GeoDistanceRangeQueryBuilder
- GeoPolygonQueryBuilder
- GeoShapeQueryBuilder
- GeohashCellQuery
- TermsQueryBuilder
</description><key id="107420398">13669</key><summary>Query Refactoring: moving validation to setters and constructors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-20T21:52:22Z</created><updated>2016-03-11T11:50:48Z</updated><resolved>2015-09-22T09:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-21T07:49:28Z" id="141900577">looks good, I left some comments
</comment><comment author="cbuescher" created="2015-09-21T13:31:55Z" id="141978159">Adressed the comments, squashed and rebased.
</comment><comment author="s1monw" created="2015-09-21T18:14:06Z" id="142064944">left a minor comment LGTM - not need for another review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation improvement: cloud.aws.region required for ec2 discovery in non-default regions?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13668</link><project id="" key="" /><description>Just spent four hours figuring out why the ec2 discovery isn't working.

It appears that `cloud.aws.region` must be set, or otherwise the describe instances call won't find anything. After adding `cloud.aws.region: eu-west-1` to elasticsearch configuration, it magically started working.

Could documentation be improved regarding this setting a bit? Maybe highlight that if you are not running in default region it is required for discovery to work?
</description><key id="107411402">13668</key><summary>Documentation improvement: cloud.aws.region required for ec2 discovery in non-default regions?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mnylen</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>docs</label></labels><created>2015-09-20T19:19:18Z</created><updated>2015-09-22T13:25:52Z</updated><resolved>2015-09-22T13:25:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-20T19:37:12Z" id="141825052">Agreed. Do you want to contribute a doc update?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forced awareness fails to balance shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13667</link><project id="" key="" /><description>I have installed ES 1.7.1 on all nodes.  I have three zones:
- A single node in `primary` zone
- A single node in `secondary` zone
- Four nodes in `spot` zone

Indexes with `"number_of_replicas": 2` will not place one shard per zone, rather place a couple of replicas (?even primary?) in the `spot` zone.   The spot zone is very unstable, but very cheap, and since a quorum of the replicas can end up in the spot zone, the cluster is mostly unusable.

Here is the config for the primary node:

```
cluster.name: active-data
node.zone: primary
node.name: primary
node.master: false
node.data: true

cluster.routing.allocation.awareness.force.zone.values: primary,secondary,spot
cluster.routing.allocation.awareness.attributes: zone
```

Here is a sample of my config file for the secondary node:

```
cluster.name: active-data
node.zone: secondary    
node.name: secondary
node.master: false
node.data: true

cluster.routing.allocation.awareness.force.zone.values: primary,secondary,spot
cluster.routing.allocation.awareness.attributes: zone
```

..and here are the configs for nodes in the spot zone:

```
node.zone: spot
node.name: spot_{{id}}
node.master: false
node.data: true

cluster.name: active-data
cluster.routing.allocation.awareness.force.zone.values: primary,secondary,spot
cluster.routing.allocation.awareness.attributes: zone
```

`{{id}}` is replaced with a unique hex UID for each node.

There is one more node, in the `spot` zone, which is master (but has no shards):

```
cluster.name: active-data
node.zone: spot
node.name: coordinator
node.master: true
node.data: false

cluster.routing.allocation.awareness.force.zone.values: primary,secondary,spot
cluster.routing.allocation.awareness.attributes: zone
```

When I have two zones (`primary` and `spot`), with one replica, the cluster is stable; the primary zone always has a copy of every shard, and loss of spot nodes does not cause loss of quorum.
</description><key id="107409708">13667</key><summary>Forced awareness fails to balance shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klahnakoski</reporter><labels><label>:Allocation</label><label>discuss</label></labels><created>2015-09-20T18:35:57Z</created><updated>2015-10-08T12:27:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T08:04:48Z" id="141903363">&gt; Indexes with "number_of_replicas": 2 will not place one shard per zone, rather place a couple of replicas (?even primary?) in the spot zone. 

I've tried this out on 1.7.1 and 2.0.0-beta2 and it behaves correctly.  At least one shard copy is placed in each zone, and a second copy is only placed in a zone if more replicas than zones are specified. A primary could well be placed in the spot zone, but that shouldn't be an issue.

Can you provide some more info about that you're seeing, because it seems to be working correctly to me.

That said, awareness should really only be used with zones of the same size (see https://github.com/elastic/elasticsearch/issues/12431) and having shard copies on mixed box types drags the performance of the expensive boxes down to the level of the cheap boxes.
</comment><comment author="klahnakoski" created="2015-09-21T13:34:36Z" id="141979178">Thank you for looking into this.  

I looked at #12431, I can not say I understand the code completely, but it may be the cause:  I did bring the node in the `secondary` zone down for a while.  During that downtime, the both replicas moved to the `spot` zone.  When the `secondary` was brought back up, it never got back its replicas.  But now I am speculating.

Thank you for noting the degradation issues I may experience when it comes to cheap nodes:  I am aware of this; the 'cheap' nodes actually have more memory, more CPU, and more space than may "expensive" nodes; they are just unreliable.

Last night, I had turned off all machines in the `spot` zone, and reduced to one replica so I would have quorum, and bulk indexing would proceed.  I have turned the `spot` machines back on, and set replicas to 2, but it will be a few hours for the shards to copy over, and then I can get back to you.
</comment><comment author="clintongormley" created="2015-09-21T13:44:08Z" id="141982487">&gt; During that downtime, the both replicas moved to the spot zone. When the secondary was brought back up, it never got back its replicas. 

This sounds like you were using ordinary awareness instead of forced awareness.  With forced awareness, the shards in the secondary zone become unassigned, rather than being assigned to a different zone.
</comment><comment author="klahnakoski" created="2015-09-21T14:46:47Z" id="142004273">I _think_ I am using forced awareness (as per my config file)

```
cluster.routing.allocation.awareness.force.zone.values: primary,secondary,spot
```

I continue to look at #12431, and I do not know how it manages balance properly when it never calculates/uses the quorum size.   Looking at [1], I am more convinced it is the cause:  When `secondary` is missing we have:

```
shardCount==5
averagePerAttribute==1
leftoverPerAttribute==1
currentNodeCount==2
```

Which allocates a shard to the spot zone.  When `secondary` comes back I have 

```
shardCount==6
averagePerAttribute==2
leftoverPerAttribute==0
currentNodeCount==3
```

which prevents allocation to the `secondary` node.

I am still waiting for my replicas to recover.

[1] https://github.com/elastic/elasticsearch/blob/c57951780e0132c50b723d78038ab73e10d176c5/core/src/main/java/org/elasticsearch/cluster/routing/allocation/decider/AwarenessAllocationDecider.java#L222
</comment><comment author="klahnakoski" created="2015-09-21T20:06:16Z" id="142094480">I am having a tough time replicating the issue.  The large index takes too long.  And the small indexes behave just fine.   I am still looking for the sequence of actions that cause the unbalance, and will get back to you when I can, or I have given up.
</comment><comment author="klahnakoski" created="2015-09-22T00:14:53Z" id="142143912">I gave up on `unittest` where I saw the problem, and I set {"index.routing.allocation.exclude.zone" : "spot"} to pevent it from moving shards 

For the `saved_queries`, I set the replicas to 3

![resumed secondary](https://cloud.githubusercontent.com/assets/2334429/10007178/2c869192-6091-11e5-9c8b-d9ac1cc70d8d.png)

Then I set the replicas back down to 2.  I would expect the shards to properly balance, like they usually do, but this time they do not.

![replicas back to 2](https://cloud.githubusercontent.com/assets/2334429/10007191/424d9f3e-6091-11e5-96a0-9f23d5c3c266.png)

and you see the two replicas stick to the `spot` zone.  Now I kill the spot instances

![no spot](https://cloud.githubusercontent.com/assets/2334429/10007293/09bf62e6-6092-11e5-9f48-9e6ccb13690b.png)

The problem now is that there is no quorum, and I can not index more documents.  Also, this state seems to persist; the two replicas never come back, even after a long while.

![after a while](https://cloud.githubusercontent.com/assets/2334429/10008437/8214a4ea-609c-11e5-8de8-d446dd422e36.png)

Assuming a sequence like this does not loose all copies of a shard, I can fix this by setting replicas to zero, and then back to the value I wish.
</comment><comment author="bleskes" created="2015-09-22T06:54:39Z" id="142196234">I think I understand what&#8217;s going on. The forced awareness and the exclude rules are conflicting. The forced awareness tells ES it must spread out evenly across the awareness values, using the spot zone and not assigning shards if the spot zone is not there. The exclude rules prevents the spot zone from being used. The reason why this only kicks in once the shard no is reduce and then increased again is that exclude rules will try to move existing shards but will not unassigned them if that&#8217;s impossible. Once the replica count is decreased and increase the exclude does prevent the allocation of the _new_ shards on the spot zone, and the forced awareness prevents them from being allocated anywhere else. Makes sense?

&gt; On 22 Sep 2015, at 02:15, Kyle Lahnakoski notifications@github.com wrote:
&gt; 
&gt; I gave up on unittest where I saw the problem, and I set {"index.routing.allocation.exclude.zone" : "spot"} to pevent it from moving shards
&gt; 
&gt; For the saved_queries, I set the replicas to 3
&gt; 
&gt; Then I set the replicas back down to 2. I would expect the shards to properly balance, like they usually do, but this time they do not.
&gt; 
&gt; and you see the two replicas stick to the spot zone. Now I kill the spot instances
&gt; 
&gt; The problem now is that there is no quorum, and I can not index more documents. Also, this state seems to persist; the two replicas never come back, even after a long while.
&gt; 
&gt; Assuming a sequence like this does not loose all copies of a shard, I can fix this by setting replicas to zero, and then back to the value I wish.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="klahnakoski" created="2015-09-22T11:55:02Z" id="142265123">Sorry for the confusion:  Only the `unittest` index has an exclude rule, but my comment was about the lifecycle of the `saved_queries` index.   I only mentioned `unittest` to explain why the shards where not moving over time. 
</comment><comment author="clintongormley" created="2015-09-22T13:50:34Z" id="142294833">@klahnakoski try using the cluster reroute api to assign a replica for shard 0 to the secondary node, with the "explain" parameter.  That'll tell us why Elasticsearch doesn't want to assign the shard to that node.
</comment><comment author="klahnakoski" created="2015-09-22T14:38:46Z" id="142307676">Where does the "explain" parameter go?  As a sibling property to "command"?  or sibling of "allocate"? or sibling of "index"?   [1] is vague.

[1]https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html
</comment><comment author="clintongormley" created="2015-09-22T17:34:50Z" id="142357650">This should work:

```
POST _cluster/reroute?explain
{
  "commands": [
    {
      "allocate": {
        "index": "saved_queries20150510_160318",
        "shard": 0,
        "node": "secondary"
      }
    }
  ]
}
```
</comment><comment author="klahnakoski" created="2015-09-23T02:59:47Z" id="142476848">Sorry for the delay, it took a while for me to get back to the mis-allocated state.  When I ran the command, the shard moved back to the secondary.  I have a copy of the _explain_, but it does not have any complaints in it.
</comment><comment author="nikonyrh" created="2015-10-07T20:17:28Z" id="146315877">I have an other scenario (if it is too different maybe a new issue or the mailing list would be more suitable?), help or links to documentation would be greatly appreciated.

I have a few hundred GB of indexes on my "production" node (just one machine in the cluster as it is for my hobby projects). I am trying to run benchmarks for five of the indexes, each of which is about 18 GB in 4 shards. I would like to test how much faster queries and aggregations I would get by joining my gaming laptop to the cluster but I don't want it to receive any master shards.

In my understanding I would lose data if  the only existing shard (at the moment all indexes have zero replicas) is transferred to the laptop, I stop that node and delete the data folder. Am I correct? New documents are constantly being indexed to other indexed which aren't part of this experiment.

I thought I could set "node.master" to false to block primary shards being transferred to the node but clearly this isn't the case, transferring starts immediately when the laptop joins the cluster.

Simply put: how do I prevent laptop node from receiving primary shards? I only want to transfer replica shards to the laptop and have all primary shards staying safely on the current master node.
</comment><comment author="clintongormley" created="2015-10-08T12:27:29Z" id="146522701">@nikonyrh please ask questions like these in the forum: https://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo context suggestion fail using path option with nested object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13666</link><project id="" key="" /><description>I want to create a completion suggester with a geo type context. The geo_type information is stored under a nested object, the configuration seems ok, it is indexing well, however it always put the default values in geo type. Here is the configuration.

```
 suggest: {
               type: "completion",
                 context: {
                     sector: {
                       type: "category",
                       default: "TECH",
                       path: "sector"
                     },
                     location: {
                       type:"geo",
                       precision: ["2km","20km","30km"],
                       path: "shops.location",
                       default:{
                          lat:0.0,
                          lon:0.0
                       }
                    }
                  },
             shops: {
               type: 'nested',
               properties: {
                   shop_aggregation_string: { type: "string", index: "not_analyzed" },
                   id:   { type: 'long' },
                   name:       { type: 'string', index: 'not_analyzed' },
                   address:    { type: 'string', index: 'not_analyzed' },
                   chain_id:   { type: 'long' },
                   chain_name: { type: 'string', index: "not_analyzed" },
                   location:   { type: "geo_point"},
                   stock:      { type: 'integer' },
                   sale:       { type: 'boolean' },
                   price:      { type: 'double' },
               }
             }
```

For each document there is multiple shops. 

Kind regards
</description><key id="107380493">13666</key><summary>Geo context suggestion fail using path option with nested object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">silvestrelosada</reporter><labels /><created>2015-09-20T08:53:38Z</created><updated>2015-09-25T10:59:28Z</updated><resolved>2015-09-21T07:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-21T07:31:41Z" id="141898055">Hi @silvestrelosada 

Nested objects are stored in separate documents from the main object, so the suggester doesn't have access to the values in the nested object (unless you use the include_in_root option to also store the nested values in the top-level object).
</comment><comment author="silvestrelosada" created="2015-09-21T07:40:49Z" id="141899391">Thanks for response an explanation.
</comment><comment author="silvestrelosada" created="2015-09-24T08:03:43Z" id="142846361">Hi again,

I tried your recommendation,

  shops: {
               type: 'nested',
               include_in_root: true,
               properties: {
                   location:   { type: "geo_point"},
               }
             }

The location appears in root object however it cannot be used in path for completion suggester, I guess that the issue is that it does not exist physically on the JSON.

I also tried with include_in_parent and no result.
</comment><comment author="clintongormley" created="2015-09-25T10:50:22Z" id="143185901">then perhaps you want to use `copy_to` instead
</comment><comment author="silvestrelosada" created="2015-09-25T10:59:28Z" id="143187341">I'll try but i guess it wont work, because is not present on JSON document, I mean in _source
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date Math Not Working for GET</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13665</link><project id="" key="" /><description>I try to get a document using date math, but Elasticsearch returns a error.

Version: Elasticsearch 2.0.0-beta2

Put Document

$ curl -XPUT 'localhost:9200/logstash-2015.09.19/animals/1' -d '{"name":"puppy"}'
{"_index":"logstash-2015.09.19","_type":"animals","_id":"1","_version":1,"_shards":{"total":2,"successful":1,"failed":0},"created":true}

Get Document

$ curl -XGET 'localhost:9200/logstash-2015.09.19/animals/1'
{"_index":"logstash-2015.09.19","_type":"animals","_id":"1","_version":1,"found":true,"_source":{"name":"puppy"}}

Get Document with Date Math

$ curl -XGET 'localhost:9200/`&lt;logstash-{now/d}`&gt;/animals/1'
No handler found for uri `[/&lt;logstash-now/d&gt;/animals/1]` and method [GET]
</description><key id="107368558">13665</key><summary>Date Math Not Working for GET</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">sirdavidhuang</reporter><labels><label>:CRUD</label><label>bug</label><label>v2.0.0-rc1</label></labels><created>2015-09-20T02:58:24Z</created><updated>2016-01-26T09:18:55Z</updated><resolved>2015-09-25T09:30:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-20T05:56:04Z" id="141752027">It's not supposed to work AFAIK. Did you see that somewhere in docs?
You should do that on the client side.

May be using an alias would help you?
</comment><comment author="sirdavidhuang" created="2015-09-20T06:27:15Z" id="141752721">This page includes the date math information.
https://www.elastic.co/guide/en/elasticsearch/reference/master/date-math-index-names.html

Example from page.
curl -XGET 'localhost:9200/&lt;logstash-{now/d-2d}&gt;/_search' {
  "query" : {
    ...
  }
}

I try this curl command and get a error.
curl -XGET 'localhost:9200/&lt;logstash-{now/d}&gt;/_search' -d '{
  "query" : {
    "match" : {}
  }
}'

{"error":{"root_cause":[{"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"&lt;logstash-now","index":"&lt;logstash-now"}],"type":"index_not_found_exception","reason":"no such index","resource.type":"index_or_alias","resource.id":"&lt;logstash-now","index":"&lt;logstash-now"},"status":404}

There is also this pull request.

Add date math support in index names #12209
https://github.com/elastic/elasticsearch/pull/12209
</comment><comment author="dadoonet" created="2015-09-20T08:46:40Z" id="141758759">Great. I missed that !
</comment><comment author="martijnvg" created="2015-09-21T07:08:59Z" id="141895700">Thanks for reporting @sirdavidhuang, this is indeed a bug. The issue here is that the index name is cut of at the date rounding (`/)` and only `&lt;logstash-{now` ends up being received as index name.
</comment><comment author="javanna" created="2015-10-20T20:02:13Z" id="149684464">Hi @sirdavidhuang we had initially treated this as a bug and fixed it, but afterwards we found out that the fix introduced a regression (#14177), which made us take a step back. Any slash that should not be used as a path separator in a uri should be properly escaped, and it is wrong to try and make distinctions between the different slashes on the server side depending on what surrounds them, that heuristic is not going to fly (in fact it didn't :) )

We are going to revert the initial fix then, the solution to this problem is to escape the '/' in the url like this: `curl -XGET 'localhost:9200/&lt;logstash-{now%2Fd}&gt;/animals/1'`. That is going to work and didn't require any fix in the first place.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch won't recover after OutOfMemoryError: Java heap space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13664</link><project id="" key="" /><description>I am running an ES instance (4GB RAM, single node) in production. There are not so many concurrent requests usually but occasionally a query could exceed the RAM available and `OutOfMemoryError` occurs. I've never seen the ES node being able to recover from such tragic state, until I do a manual restart. I have to do so once or twice per month.

Is this the behavior by design? When an OutOfMemoryError occurs, ES will enter a undefined state and not be able to recover?

Here is a typical log: http://pastebin.com/3KdP81EC
Here is my `/_nodes` result: http://pastebin.com/kmPzw5KY

Note, it is a single node ES setup so no split-brain issues..
</description><key id="107304338">13664</key><summary>Elasticsearch won't recover after OutOfMemoryError: Java heap space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">foresightyj</reporter><labels /><created>2015-09-19T02:17:21Z</created><updated>2015-09-19T14:45:28Z</updated><resolved>2015-09-19T13:22:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-19T03:27:07Z" id="141616886">You hit heap problems such that oom happens inside lucene indexwriter. When this happens it does not try to be a superhero and just take it to the face and keep on trucking, it closes itself for data safety.
</comment><comment author="clintongormley" created="2015-09-19T13:22:23Z" id="141667824">@foresightyj as @rmuir said, after an OOM you have to restart. No way around it.  What you should do though is to figure out what is causing the OOM and fix that.  The place to get help for that is the forum: http://discuss.elastic.co/.  If you find a problem with some protection mechanism (eg circuit breakers etc) then feel free to open a bug report.
</comment><comment author="foresightyj" created="2015-09-19T14:45:28Z" id="141675696">@clintongormley Thanks for the clear answer. I also realized that discuss.elastic.co is a better place for such questions. I will post future questions there. At the time I came out with this question, discuss.elastic.co was unavailable (maybe it was taken offline for maintenance). I experience that quite frequently in China during day time (which is roughly night time in the western countries)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary copies of license and notice files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13663</link><project id="" key="" /><description>We moved a lot of repositories into elasticsearch, but in their new
location they retained their LICENSE.txt and NOTICE.txt files. These are
all the same, and having the license and notice and the root of the
repository should be sufficient.
</description><key id="107301877">13663</key><summary>Remove unnecessary copies of license and notice files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-19T01:12:02Z</created><updated>2015-09-22T01:05:07Z</updated><resolved>2015-09-22T01:04:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-19T05:56:04Z" id="141625032">Agreed. LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic 2.0 beta returning wrong results from cache?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13662</link><project id="" key="" /><description>I have an array with categories in which i have genres like for example Comedy. I search for documents that contain that and other genres using filtered query in which i have empty query and terms filter. 
Next i use match query on for example title field + terms filter for genres and choose Comedy. On third query i do exact search like first one so only look for Comedy in genres (only using terms filter and empty query) and i get results of last match search (so second query results) which are obviously wrong.
</description><key id="107267034">13662</key><summary>Elastic 2.0 beta returning wrong results from cache?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devlo</reporter><labels><label>blocker</label><label>bug</label></labels><created>2015-09-18T19:54:49Z</created><updated>2016-11-02T19:58:22Z</updated><resolved>2015-10-06T14:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-18T20:06:13Z" id="141553259">@devlo thanks so much for opening this, can you reproduce this in a simple self contained example?
</comment><comment author="devlo" created="2015-09-18T21:25:16Z" id="141572573">As i thought it's a cache problem, i've added _cache: false to terms query (i am using node.js driver) and it's returning good results. I will try to make an example.
Probably has something to do with #5363
</comment><comment author="clintongormley" created="2015-09-19T13:12:10Z" id="141666188">Hi @devlo 

The `_cache` parameter is no longer supported in 2.0 (it is silently ignored) so I doubt it has anything to do with this.  A recreation would be very helpful indeed.
</comment><comment author="clintongormley" created="2015-09-19T13:17:49Z" id="141667639">This is what I've tried doing to recreate this, but it works as expected:

```
POST t/t
{
  "title": "One foo bar",
  "cats": ["one","two","three"]
}

POST t/t
{
  "title": "Two foo bar",
  "cats": ["one","two"]
}

POST t/t
{
  "title": "Three foo bar",
  "cats": ["two"]
}

GET _search
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "*"
        }
      }, 
      "filter": {
        "terms": {
          "cats": [
            "one",
            "three"
          ]
        }
      }
    }
  }
}

GET _search
{
  "query": {
    "filtered": {
      "query": {"match": {
        "title": "One"
      }}, 
      "filter": {
        "terms": {
          "cats": [
            "one",
            "three"
          ]
        }
      }
    }
  }
}
```
</comment><comment author="devlo" created="2015-09-19T21:06:04Z" id="141707343">Hmmm i can't reproduce it now, i've restarted elasticsearch after that, used _cache: false and i thought it was the issue. But i've removed _cache: false and tried now and everything works as intented. Indeed very wierd behaviour. It's wierd because i had the same issue on local test environment and on remote test server. I've restarted elastic on both. I will observe this as i am working on 2.0 beta anyway.
</comment><comment author="clintongormley" created="2015-10-06T14:19:39Z" id="145870946">Looks like a false alarm. Closing.  Feel free to reopen if you can reproduce.
</comment><comment author="JulianRooze" created="2016-08-26T13:31:29Z" id="242735959">Hi,

I believe I currently have this same issue (or at least related) and have a reproducible case (at least on my machine and our production cluster :D). I can't post the data publicly, but poke me on Twitter @queryable (or let me know where I can send it to) and I can provide it for you (it's small enough at 9 MB). 

I've opened a Stackoverflow question on it here:

http://stackoverflow.com/questions/39155676/erratic-search-results-from-elastic-when-sorting-on-a-field
</comment><comment author="clintongormley" created="2016-08-26T13:47:19Z" id="242739976">@JulianRooze could you email it to me at clinton at elastic dot co?
</comment><comment author="JulianRooze" created="2016-08-26T13:58:58Z" id="242742918">@clintongormley I've mailed it to you, thanks! 
</comment><comment author="JulianRooze" created="2016-10-20T18:06:35Z" id="255182821">@clintongormley Hi Clinton, I was curious about this issue and if you guys have made any progress on it. Last I heard you managed to reproduce the faulty behavior. This is both speaking out of curiosity what the cause behind this weird behavior is and whether I can strip the workaround from our code yet :)

Cheers! 
</comment><comment author="clintongormley" created="2016-11-02T17:46:02Z" id="257943335">Sorry @JulianRooze - this was fixed in #20196
</comment><comment author="JulianRooze" created="2016-11-02T19:58:22Z" id="257980873">@clintongormley Great, thanks for letting me know! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: Remove index member from QueryParseContext</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13661</link><project id="" key="" /><description>Given that we are moving to parsing queries on the coordinating node, the index name is not relevant anymore in QueryParseContext, as the parsing phase cannot be related to any specific index. On the contrary, the QueryShardContext is the one that holds mappings etc. and the index name too, as the lucene query creation happens on the data node and can still be related with the index that it happens against.

Changes are mainly around tests that were expecting the index name, moved to using QueryShardException in some of them, removed the index name elsewhere.
</description><key id="107266150">13661</key><summary>Query refactoring: Remove index member from QueryParseContext</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-18T19:48:11Z</created><updated>2015-09-23T08:40:37Z</updated><resolved>2015-09-23T08:40:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-18T19:51:31Z" id="141550550">@colings86 this is something that we discussed, whicih should help moving forward with the search request refactoring, as the `QueryParseContext` won't depend on an index anymore. The way we currently create it still needs to change once we refactored all the queries, it still gets created internally together with the shard context at index creation, so we have one per index per node, but that will change in the future.  You can simply have your own new parse context and use it for now to call `fromXContent`.
</comment><comment author="s1monw" created="2015-09-18T20:04:04Z" id="141552842">LGTM
</comment><comment author="javanna" created="2015-09-21T13:52:53Z" id="141986030">I just rebased this, turns out it looks a bit different after merging #13631 in. Got rid of `TestQueryParsingException` and `TestQueryShardException`. feel free to have another look.
</comment><comment author="colings86" created="2015-09-22T08:00:59Z" id="142206976">LGTM
</comment><comment author="javanna" created="2015-09-22T16:46:30Z" id="142345411">I had to rebase again. After the changes made in #13631 and this PR, I went ahead and removed any dependency of ParsingException from QueryParseContext. Also taking in the whole XContentParser wasn't needed in the constructors, as @colings86  pointed out, `XContentLocation` is enough.
</comment><comment author="colings86" created="2015-09-22T16:48:37Z" id="142345957">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moving system property setting to before it can be used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13660</link><project id="" key="" /><description>Closes #13658
</description><key id="107243080">13660</key><summary>Moving system property setting to before it can be used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Logging</label><label>bug</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-18T17:22:57Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-22T14:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-19T03:03:56Z" id="141613692">+1!
</comment><comment author="nik9000" created="2015-09-22T14:40:02Z" id="142307985">If it fixes the problem, LGTM. Its simple and worth fixing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggester: Add doc-values based payload support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13659</link><project id="" key="" /><description>Note: the PR is against `feature/completion_suggester_v2` branch

Followup of https://github.com/elastic/elasticsearch/pull/13576#issuecomment-140695698. The PR enables returning any doc values enabled fields as suggestion payload.

One can specify fields to be returned as part of the suggestion payload at query time, using the 
`payload` option:

``` bash
POST music/_suggest 
{
  "song-suggest" : {
    "prefix" : "nev",
    "completion" : {
      "field" : "song_suggest"
      "payload" : [ "title" ]
    }
  }
}
```

The specified payload field values will be returned as part of the suggestion `payload`:

``` bash
{
  "song-suggest": [
    {
      "text": "nev",
      "offset": 0,
      "length": 4,
      "options" : [ {
        "text" : "Nirvana",
        "score" : 34.0,
        "payload" : {
          "title" : [ "Nevermind" ]
        }
      } ]
    }
  ]
}
```

Now suggestion `payload` are not part of the completion (FST) index, as was the case before. Payload fields are fetched for the top N completions per shard hence specifying payload fields will incur additional search performance hit. 
</description><key id="107242990">13659</key><summary>Completion Suggester: Add doc-values based payload support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label><label>review</label></labels><created>2015-09-18T17:22:26Z</created><updated>2015-09-21T20:32:51Z</updated><resolved>2015-09-21T20:32:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-18T18:02:26Z" id="141521112">This looks great, thanks @areek, I left a few small comments.

Net/net this is more functionality vs. current suggesters, since you can retrieve more than one field as the payload (vs only one payload per suggestion today), and they can be typed fields (numbers, dates, etc.) too.

The code is also quite simpler than supporting the 2nd fetch phase ...
</comment><comment author="rmuir" created="2015-09-18T18:47:17Z" id="141533476">Wait we are using docvalues as stored fields now? I do not think this is something we should do.
</comment><comment author="s1monw" created="2015-09-18T18:58:56Z" id="141535950">&gt; Wait we are using docvalues as stored fields now? I do not think this is something we should do.

I think for the suggest usecase that's a good alternative to putting it into the FST. We will deprecate this option as well once we have the second roundtrip query_then_fetch for this too but that will go into 3.x
</comment><comment author="areek" created="2015-09-18T23:19:13Z" id="141592378">Thanks @mikemccand for the review, addressed all your feedback. I think the docs can be improved to encourage users to use doc values enabled fields for payload? Maybe @clintongormley has some thoughts?

@rmuir this is added to make the new implementation have feature parity with the current suggester and as @s1monw mentioned will go away in 3.x, where documents will be fetched in the 2nd phase.
</comment><comment author="clintongormley" created="2015-09-19T12:57:47Z" id="141661997">@areek I like it
</comment><comment author="mikemccand" created="2015-09-21T13:55:48Z" id="141987188">Thanks @areek, LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Logger name doesn't correspond to logging config</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13658</link><project id="" key="" /><description>Log messages used to remove the `org.elasticsearch.` prefix from the logger name, eg:

```
 [2015-09-18 18:15:00,996][TRACE][discovery.zen.ping.unicast] [U-Man] [1] disconnecting from...
```

In 2.0.0-beta2 it logs this line as:

```
[2015-09-18 18:15:00,996][TRACE][org.elasticsearch.discovery.zen.ping.unicast] [U-Man] [1] disconnecting from...
```

The `config/logging.yml` still refers to the logger names without the prefix, eg:

```
  # discovery
  discovery: TRACE
```

This needs to be changed to the following to work:

```
  # discovery
  org.elasticsearch.discovery: TRACE
```

Personally I'd prefer to go back to removing the prefix, but not sure if there was a reason for adding the prefix in?  Either way, the config file should be consistent with what we do in practice.
</description><key id="107233368">13658</key><summary>Logger name doesn't correspond to logging config</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Logging</label><label>adoptme</label><label>bug</label><label>v2.0.0-rc1</label></labels><created>2015-09-18T16:20:51Z</created><updated>2015-10-01T11:18:43Z</updated><resolved>2015-09-22T14:41:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2015-09-18T17:14:00Z" id="141510501">I ran out of time to chase down the appropriate fix, but this is the first _static_ use of the `Loggers` class, which is triggering the system property to be read _before_ it's set in `org.elasticsearch.bootstrap.Bootstrap`.

```
    at org.elasticsearch.common.logging.Loggers.&lt;clinit&gt;(Loggers.java:44) // I added a static block to catch it
    at org.elasticsearch.common.MacAddressProvider.&lt;clinit&gt;(MacAddressProvider.java:32)
    at org.elasticsearch.common.TimeBasedUUIDGenerator.&lt;clinit&gt;(TimeBasedUUIDGenerator.java:38)
    at org.elasticsearch.common.Strings.&lt;clinit&gt;(Strings.java:64)
    at org.elasticsearch.common.settings.Settings$Builder.replacePropertyPlaceholders(Settings.java:1178)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.initializeSettings(InternalSettingsPreparer.java:158)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:84)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:107)
    at org.elasticsearch.common.cli.CliTool.&lt;init&gt;(CliTool.java:100)
    at org.elasticsearch.bootstrap.BootstrapCLIParser.&lt;init&gt;(BootstrapCLIParser.java:48)
    at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:222)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
```

Simplest fix is to just put the system property at the top of the `org.elasticsearch.bootstrap.Bootstrap#init` method.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove transform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13657</link><project id="" key="" /><description>Removes the mapping transform feature which when used made debugging very
difficult. Users should transform their documents on the way into
Elasticsearch rather than having Elasticsearch do it.

Closes #12674
</description><key id="107210943">13657</key><summary>Remove transform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>breaking</label><label>v5.0.0-alpha1</label></labels><created>2015-09-18T14:18:03Z</created><updated>2016-10-12T01:20:10Z</updated><resolved>2015-10-30T16:34:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-18T14:18:13Z" id="141463293">Ping @rjernst for review.
</comment><comment author="jpountz" created="2015-09-24T15:24:50Z" id="142962347">The change looks good to me, maybe we should document it (unless it already is?) It's a bit weird to me to drop a feature on a minor release. Is it feasible to either deprecate for now and only remove in 3.0 or backport the change to 2.0 since it's not been released yet? cc @clintongormley 
</comment><comment author="rjernst" created="2015-09-24T18:18:54Z" id="143010055">Also LGTM (just a couple extra things I think can be removed).

I think 3.0 is the correct place to put this, since we missed the window to get this into 2.0. Heaver mappings cleanup will just have to wait a little longer. :)  But let's make sure to deprecate in 2.1.
</comment><comment author="nik9000" created="2015-09-24T18:21:51Z" id="143010774">&gt; I think 3.0 is the correct place to put this, since we missed the window to get this into 2.0. Heaver mappings cleanup will just have to wait a little longer. :) But let's make sure to deprecate in 2.1.

We blasted the documentation in 2.0, IIRC. I'm pretty ambivalent about when it gets removed but @clintongormley wanted to see it gone as soon as we could.
</comment><comment author="clintongormley" created="2015-09-25T13:09:37Z" id="143216833">Question for me is: what features/improvements does mapping transform block?  eg highlighting?  are there things we want to get in for 2.x which are blocked?
</comment><comment author="mattjanssen" created="2015-09-25T15:37:45Z" id="143254360">We just built and deployed a large platform on 2.0 that uses these transforms to extract a geo_point from a geo_shape. We need the geo_point to do distance sorting. We need the geo_shape to do  spatial queries.

We assumed it was a safe feature because it hadn't been deprecated. We didn't even notice it wasn't in the latest docs.

We use mongo-connector to ship the data. We don't want to store the lon/lat twice in our DB just to facilitate Elasticsearch. I guess we should rewrite mongo-connector to do transforms?
</comment><comment author="lukas-vlcek" created="2015-10-05T09:18:21Z" id="145472327">Hmm... I liked mapping transformations.
</comment><comment author="jpountz" created="2015-10-09T14:52:02Z" id="146894071">Maybe node ingest (#14047) removes the need for transform? So we could remove in 3.0 as @rjernst suggested.
</comment><comment author="nik9000" created="2015-10-15T19:21:32Z" id="148495787">&gt; Maybe node ingest (#14047) removes the need for transform? So we could remove in 3.0 as @rjernst suggested.

Right then. Rebasing onto 3.0.
</comment><comment author="nik9000" created="2015-10-15T20:04:44Z" id="148506045">Ok - @rjernst, I think this is ready for another round of review when you are ready for it.
</comment><comment author="rjernst" created="2015-10-26T17:16:21Z" id="151213735">LGTM
</comment><comment author="adichad" created="2015-10-30T06:34:15Z" id="152442527">I have been using mapping transform native (java) scripts all the way up to 1.7.x in production. never saw a deprecation warning in code or documentation online though. In 2.0 documentation the whole section is not present. the migration tool also did not warn about the transform script reference being present in the type mappings. 

Please provide better documentation than throwing a 404 in 2.x documentation at least stating whether this is deprecated or altogether removed in 2.x or not.

In the meanwhile I will look into 2.0 code to see whether I need to incur the effort to move this out now or whether it can wait a month or so.
</comment><comment author="rjernst" created="2015-10-30T06:44:29Z" id="152443480">@adichad Mapping transforms still exist in 2.x. This PR is to remove them from master (3.0), and a follow up will deprecate them in 2.x.
</comment><comment author="adichad" created="2015-10-30T06:57:50Z" id="152444861">Thanks @rjernst !
request to reinstate the related documentation for example at

https://www.elastic.co/guide/en/elasticsearch/reference/1.7/mapping-transform.html
https://www.elastic.co/guide/en/elasticsearch/reference/1.7/_get_transformed.html

to also work at

https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-transform.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/_get_transformed.html

and maintain deprecation warnings there?
</comment><comment author="nik9000" created="2015-10-30T16:34:06Z" id="152579964">Squashed, rebased, and merging to just 3.0.0.
</comment><comment author="niemyjski" created="2016-10-12T00:39:20Z" id="253087197">Removing this feature really made life tough for us... Do you have any work around for this?
</comment><comment author="niemyjski" created="2016-10-12T00:44:30Z" id="253087919">cc @nik9000 @rjernst @clintongormley
</comment><comment author="niemyjski" created="2016-10-12T01:06:06Z" id="253090797">Don't see these changes in here: https://www.elastic.co/guide/en/elasticsearch/reference/5.x/breaking_50_mapping_changes.html We were using this feature like this: 
https://github.com/exceptionless/Exceptionless/blob/master/src/Exceptionless.Core/Repositories/Configuration/Indexes/EventIndex.cs#L191-L212

which now completely breaks us.
</comment><comment author="jasontedor" created="2016-10-12T01:15:47Z" id="253092240">It's on the page that you linked to: https://www.elastic.co/guide/en/elasticsearch/reference/5.0/breaking_50_mapping_changes.html#_source_transform_removed
</comment><comment author="niemyjski" created="2016-10-12T01:18:42Z" id="253092614">From what I understand there are issues with ingest pipelines around bulk operations??
</comment><comment author="nik9000" created="2016-10-12T01:20:10Z" id="253092835">The idea is to replace it with the ingest feature. It isn't the same as the
modifications it makes to the source are saved in the source rather than
just used for indexing.

It was a neat feature but ended up breaking more folks than it helped
because it didn't work well with things like highlighting. It made
debugging indexing problems a nightmare.

On Oct 11, 2016 9:15 PM, "Jason Tedor" notifications@github.com wrote:

It's on the page that you linked to: https://www.elastic.co/guide/
en/elasticsearch/reference/5.0/breaking_50_mapping_changes.
html#_source_transform_removed

&#8212;
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub
https://github.com/elastic/elasticsearch/pull/13657#issuecomment-253092240,
or mute the thread
https://github.com/notifications/unsubscribe-auth/AANLojBAJJ9GLW3QOex07oRQF5yc-usZks5qzDTHgaJpZM4F_5pF
.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for S3 storage class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13656</link><project id="" key="" /><description>This adds support for S3's storage classes: "Standard", "Glacier", "Reduced Redundancy", and "Infrequent Access" (by way of PR #13655). For more information about the different classes, see [AWS Storage Classes Guide](http://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html).
</description><key id="107209185">13656</key><summary>Add support for S3 storage class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">schonfeld</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>feature</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-18T14:07:17Z</created><updated>2015-11-19T13:27:32Z</updated><resolved>2015-11-19T13:13:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-18T14:16:51Z" id="141462978">It looks nice. I was wondering about what I read in http://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html

For snapshot:

&gt; You cannot specify GLACIER as the storage class at the time that you create an object. You create GLACIER objects by first uploading objects using STANDARD, RRS, or STANDARD_IA as the storage class. Then, you transition these objects to the GLACIER storage class using lifecycle management. For more information, see [Object Lifecycle Management](http://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html).

For restore:

&gt; You must first restore the GLACIER objects before you can access them (STANDARD, RRS, and STANDARD_IA objects are available for anytime access). For more information, [GLACIER Storage Class: Additional Lifecycle Configuration Considerations](http://docs.aws.amazon.com/AmazonS3/latest/dev/object-archival.html).

So I was wondering how is it supposed to work here. Could you explain that?
Also did you test it?

Also pinging @tlrx as I'd like another review.
</comment><comment author="schonfeld" created="2015-09-18T14:22:13Z" id="141465149">@dadoonet you're right. To be honest, we should probably exclude Glacier as an option all together, or make that into a separate plugin... Too much extra complexity and logic to live here. The other 2 types - IA, and RR - are still useful by themselves.

Finally, yes - I did test this with both the RR &amp; IA classes. In-fact, I'm using this on production already with a 1.7TB cluster. Just completed a full snapshot set to IA storage class last night, and verified it worked smoothly.
</comment><comment author="dadoonet" created="2015-09-18T14:24:29Z" id="141466459">Cool! Thank you for the tests. 

Can you then reject `GLACIER` and remove it from the documentation?
</comment><comment author="dadoonet" created="2015-09-18T17:55:15Z" id="141519630">I left a comment.
May be we can write a unit test which creates a new `S3Repository` instance and tries "glacier", "Standard", "standard" and other options?

@tlrx thoughts?
</comment><comment author="schonfeld" created="2015-09-18T18:41:58Z" id="141532442">@dadoonet i'd be happy to write a few unit tests... unless they need to be written by your team?
</comment><comment author="dadoonet" created="2015-09-18T19:07:10Z" id="141537961">It's better to have a test within the same PR. So if you can do it, that'd be awesome!
</comment><comment author="dadoonet" created="2015-10-27T14:47:41Z" id="151525722">@schonfeld Do you think you could add a unit test?
</comment><comment author="schonfeld" created="2015-11-03T15:26:54Z" id="153388134">@dadoonet you bet. Give me just a day or two...

One question tho -- I should be targeting ESv2, right?
</comment><comment author="dadoonet" created="2015-11-03T15:34:43Z" id="153390557">You should try to target the master branch. Then I can port your changes on 2.x branches.
</comment><comment author="schonfeld" created="2015-11-03T23:14:54Z" id="153518926">@dadoonet I modified my PR so that it matches the code style of pull request #14297. It made writing tests for this considerably easier. Anyway, should be good now. Rebased on master, included tests, and squashed commits as per the contribution guidelines.
</comment><comment author="dadoonet" created="2015-11-04T17:05:20Z" id="153793466">I left 2 new comments. Please make sure before committing that `gradle build` is working well in `plugins/repository-s3`.
</comment><comment author="schonfeld" created="2015-11-04T17:16:17Z" id="153796539">@dadoonet oops. squashed &amp; committed. I made sure `gradle build` &amp; `gradle test` are working well first.
</comment><comment author="rjernst" created="2015-11-04T20:34:14Z" id="153854455">&gt; I made sure `gradle build` &amp; `gradle test` are working well first.

For future reference, just `gradle build` is necessary, since it runs `assemble` and `check` (which runs `test`. `gradle check` also works, because our integ tests depend on actually assembling.
</comment><comment author="schonfeld" created="2015-11-05T15:17:21Z" id="154089532">@rjernst good to know.. Thanks!
</comment><comment author="schonfeld" created="2015-11-18T17:49:05Z" id="157798122">Is there anything else needed here?
</comment><comment author="dadoonet" created="2015-11-18T18:07:37Z" id="157805020">@schonfeld No. everything is fine. Was planning to merge it tonight or tomorrow.
</comment><comment author="dadoonet" created="2015-11-19T13:13:41Z" id="158053646">Merged in master with d48d8ef863ef5b0bd2717f1a0c4d5de541cb7edd.
Will backport to 2.x branch soonish.
</comment><comment author="dadoonet" created="2015-11-19T13:27:17Z" id="158056656">Merged in 2.x with 470a66bc89de30f56803782d84e623127a9e480b

Thanks @schonfeld!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update AWS SDK version to 1.10.19</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13655</link><project id="" key="" /><description>This updates the AWS S3 Java SDK version for the `repository-s3` plugin, so that new S3 Storage Classes are supported (namely, "Infrequent Access" / STANDARD_IA) (see PR #13656).
</description><key id="107206997">13655</key><summary>Update AWS SDK version to 1.10.19</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">schonfeld</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>review</label><label>upgrade</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-18T13:56:05Z</created><updated>2015-09-19T13:04:53Z</updated><resolved>2015-09-18T19:09:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-18T13:59:10Z" id="141458397">Could you update it for discovery-ec2 as well? 

The more I think about it, the more I think we should put this in parent `pom.xml` or in `plugins/pom.xml` as a property so we update both plugins at the same time.

WDYT?
</comment><comment author="schonfeld" created="2015-09-18T14:07:58Z" id="141460968">@dadoonet you're prob right. Doesn't look like it'll break anything... So, let me go ahead and do that.
</comment><comment author="dadoonet" created="2015-09-18T19:09:50Z" id="141538534">Merged in master and 2.x branches. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Open Questions with MLT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13654</link><project id="" key="" /><description>In this issue I'll list a couple of features that we may or may not want to support for Elasticsearch More Like This (MLT). This is very much subject for discussion.

1) Some queries have a **fuzzy option**. It could be interesting to provide this functionality to More Like This as well. In this case, the generated query would just fuzzify the list of selected terms, according to the fuzziness parameter.

2) Some queries provide **fields boosting**. For More Like This this would boost the fields of the generated query.

3) More Like This could also support **regex patterns** in field names.

4) There is a common use case of **MLT with tags**, that is with unique keywords attached to each document. The problem is that on tags the MLT query does not return any result out of the box because the default min term parameter is set to 2. We could detect such a use case automatically and in this case disregard the min term freq parameter.

5) At the moment MLT only works on string fields. However, it could be useful to also consider other types such as **numerics and/or geo locations**. We have two options, either we treat them as a fixed value, for example for product codes, or treat them as a sort of range. The later case could be handled if the fuzziness parameter is set.

6) MLT generates a boolean query of the "best" terms in the provided document(s). The strategy used on multiple fields is a sort of best field approach. It could be nice to let the user control such a strategy such as asking for **cross fields** instead, in a similar manner to the [Multi Match Query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html).

7) At the moment the document input is a combination of a list of strings, a spec to a document in the index, or a spec to a document not present in the index ([artificial document](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-termvectors.html#docs-termvectors-artificial-doc)). We could additionally provide the ability to specify document inputs as the top n documents returned by a query. This could be useful to build a cheap **recommender system** which would for example return the documents most similar to the last favorited user's documents.

The syntax could look like this:

``` json
{
  "mlt": {
    "like": {
      "query" : {
        "a query that fetches the docs from the user's history"
      },
      "top_n": 3
    }
}
```

This feature could also be useful in a **query expansion** scenario. In this case, the user's original text query could be augmented by the terms of the documents returned after having executed the original query.

This could be used in a bool query in the following manner:

``` json
{
  "bool": {
    "must": {
      "term": { "user": "kimchy" }
    },
    "should": {
      "mlt": {
        "like": {
          "query": {
            "term": { "user": "kimchy" }
          },
          "top_n": 3
        }
      }
    }
  }
}
```

This feature would also let us run **nested MLT queries**, although I'm unsure as to how useful this would be.

8) Do we want to do the same with the terms returned by an aggregation? This would let use the **significant terms** as the important terms instead of more generally use the terms with the highest TF-IDF.

9) The actual Lucene MLT lets us select a **different similarity**. These are limited to TF-IDF similarities though, yet it could still be useful to let the user specify his own custom similarity here.

10) When not using term vectors and on large documents, there could be a performance issue when parsing too many tokens. We could provide an option that sets the **maximum number of tokens** to parse in each field that does not support term vectors.

11) A [highly requested feature](https://github.com/elasticsearch/elasticsearch/issues/1412) is to return the actual **interesting terms** which were selected for the generated query. Although we can now use the [Validate API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-validate.html) for this, it could still be interesting to return these terms as part of the response.

12) Another important feature when using MLT is to be able **highlight with a degree of importance the terms** that match in each document. This is related to the [Intensity Highligthing](https://github.com/elastic/elasticsearch/issues/12326) issue. Perhaps we could provide an option for that as well in the actual MLT query?

13) At some point we may be interested to **replace MLT** with [Item Query](https://github.com/elastic/elasticsearch/issues/12293) which is a cleaner more faithful implementation of nearest neighbor search, and which let's the user exactly determine how the best terms should be selected and how the items should be combined. The original MLT query would then be deprecated. Item Query would then be implemented as a plugin, along side the Term Vectors API.

14) Any other options I may not have covered? Please feel free to expand on this issue.
</description><key id="107185133">13654</key><summary>Open Questions with MLT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>discuss</label><label>feature</label><label>Meta</label></labels><created>2015-09-18T11:35:28Z</created><updated>2017-03-21T15:05:33Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: function_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13653</link><project id="" key="" /><description>Refactor the function_score query so it can be parsed on the coordinating node, split parse into fromXContent and toQuery, make the builder writeable.

Found an issue with script_score function in tests, given that it accesses the shardId through SearchContext, but that's something that we don't have available and I am not sure how to make available. Added a TODO there, left out script_score from testing for now.

Simplified code around enums (boost mode and score mode), we now rely on enums for serialization and naming.

This change is breaking for the java api as it modifies FunctionScoreQueryBuilder, removes all the add method from it. Also breaking for plugins that implement custom score functions, see the migrate guide for more info.
</description><key id="107183090">13653</key><summary>Query refactoring: function_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>breaking</label><label>review</label></labels><created>2015-09-18T11:18:11Z</created><updated>2015-09-23T10:16:23Z</updated><resolved>2015-09-23T10:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-18T12:47:19Z" id="141442144">@javanna I left some comments, but looks really good
</comment><comment author="javanna" created="2015-09-18T15:54:33Z" id="141490945">@colings86 pushed new commits, added coverage and tests for enums (including operator that was missing tests, my bad)
</comment><comment author="javanna" created="2015-09-21T13:42:52Z" id="141982203">I pushed new commits that address the feedback, ready for another round of review.
</comment><comment author="javanna" created="2015-09-22T16:02:06Z" id="142333572">@colings86 @s1monw I went ahead and refactored all the functions too. Apologies for the size of the PR, should be close though.
</comment><comment author="s1monw" created="2015-09-23T09:19:41Z" id="142539262">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing methods for array params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13652</link><project id="" key="" /><description>_From @dsernst on September 16, 2015 21:23_

If I pass in an array like:

``` js
script_file: 'myScript',
lang: 'javascript',
params: {
  myParam: [3, 4, 5]
}
```

Then trying any of these from within `myScript.js` will fail with a `has no method` error:

```
myParam.slice()
myParam.reduce()
myParam.push()
```

`reason: 'EcmaError[TypeError: Cannot find function slice in object [3, 4, 5]. (Script23.js#15)]' },`

_Copied from original issue: elastic/elasticsearch-lang-javascript#42_
</description><key id="107170156">13652</key><summary>Missing methods for array params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Lang JS</label></labels><created>2015-09-18T09:42:46Z</created><updated>2016-01-28T17:35:49Z</updated><resolved>2016-01-28T17:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-18T15:30:01Z" id="141483317">Arrays are passed to the script as objects of type [ArrayList](https://docs.oracle.com/javase/8/docs/api/java/util/ArrayList.html), which doesn't support the slice() method.  Converting it to an array works:

```
  "script": {
    "inline": "foo.toArray().slice()",
    "lang": "js",
    "params": {
      "foo": ["1","2"]
    }
  }
```

So the question is: is an ArrayList the right object to use here?
</comment><comment author="clintongormley" created="2016-01-28T17:35:49Z" id="176296212">Given this workaround, i'm going to close.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding support for customizing the rule file in ICU tokenizer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13651</link><project id="" key="" /><description>Lucene allows to create a ICUTokenizer with a special config argument
enabling the customization of the rule based iterator by providing
custom rules files.

This commit enable this feature. Users could provide a list of RBBI rule
files to ICU tokenizer.

closes #13146 
</description><key id="107150372">13651</key><summary>Adding support for customizing the rule file in ICU tokenizer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Analysis ICU</label><label>feature</label><label>v5.0.0-alpha2</label></labels><created>2015-09-18T07:43:24Z</created><updated>2016-04-22T22:06:56Z</updated><resolved>2016-04-22T19:59:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2016-02-26T12:56:47Z" id="189263782">@xuzha: Sorry that it took so long that somebody reviews your PR. I left a few comments. Just let me know if you have further questions. I think it would make sense to resolve conflicts first given that this branch is now quite out of date.
</comment><comment author="xuzha" created="2016-02-26T18:10:35Z" id="189402868">Thx @danielmitterdorfer for the review. 
The PR is so outdated, I will gave it another look this weekend.
</comment><comment author="dakrone" created="2016-04-06T20:51:01Z" id="206561516">ping @xuzha, I think this needs updating and then another review by @danielmitterdorfer ?
</comment><comment author="xuzha" created="2016-04-06T21:21:18Z" id="206575414">Sorry @dakrone, I was sick last week. I will try to update this ASAP. 
</comment><comment author="xuzha" created="2016-04-11T04:03:15Z" id="208152235">Thanks @danielmitterdorfer for the review. 

I just pushed another commit to address the comments. Sorry that it took a while to update this, it looks so hacky to me :-)
</comment><comment author="danielmitterdorfer" created="2016-04-11T14:00:00Z" id="208357673">@xuzha I answered your questions. I hope that helps now.
</comment><comment author="xuzha" created="2016-04-11T18:23:06Z" id="208485939">Thanks @danielmitterdorfer, I just pushed another commit. 
</comment><comment author="danielmitterdorfer" created="2016-04-12T05:48:34Z" id="208717966">Left a minor comment, otherwise LGTM.
</comment><comment author="danielmitterdorfer" created="2016-04-12T05:49:19Z" id="208718083">@xuzha Can you please add an appropriate version label?
</comment><comment author="xuzha" created="2016-04-12T17:08:33Z" id="209010531">Thanks @danielmitterdorfer so much for the review.  Will add a version when merging the PR.

I will leave this PR open for another few days. Comments are very welcomed.  &lt;3 &lt;3 &lt;3
</comment><comment author="xuzha" created="2016-04-22T22:06:33Z" id="213604704">PR broke the integ tests. ;-(, changed using the new setting back the old version : https://github.com/elastic/elasticsearch/commit/3e4b470f8377636a822f9b9a24d64270a0e60c5f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_shape points_only not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13650</link><project id="" key="" /><description>I downloaded the 2.0-beta2 and decided to try out the `points_only` option for `geo_shapes` that was added in #12893 but I couldn't get it to work.

Here is the mapping that I tried to use: 

``` JSON
{
  "properties": {
    "name": {
      "index": "not_analyzed",
      "type": "string"
    },
    "location": {
      "type": "geo_shape",
      "points_only": true
    }
  }
}
```

When I try it I get the following error:

``` JSON
{
  "error": {
    "root_cause": [
      {
        "type": "mapper_parsing_exception",
        "reason": "Mapping definition for [location] has unsupported parameters:  [points_only : true]"
      }
    ],
    "type": "mapper_parsing_exception",
    "reason": "Mapping definition for [location] has unsupported parameters:  [points_only : true]"
  },
  "status": 400
}
```
</description><key id="107147619">13650</key><summary>geo_shape points_only not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Eschon</reporter><labels /><created>2015-09-18T07:19:47Z</created><updated>2015-09-22T14:05:07Z</updated><resolved>2015-09-18T15:45:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-18T15:15:32Z" id="141480054">First, thanks for trying out beta2!  I tried as well and can confirm this isn't working as expected.  @nknize can you have a look?
</comment><comment author="clintongormley" created="2015-09-18T15:45:15Z" id="141486850">Bah, #12893 was mislabelled... it's actually in 2.1.0

Sorry for the confusion - i've updated the changes list and blog post
</comment><comment author="Eschon" created="2015-09-22T08:02:32Z" id="142207183">Too bad, I was really looking forward to trying it out. Well I guess I'll just have to wait for 2.1.0 then.
You also might want to remove it from the documentation for now https://www.elastic.co/guide/en/elasticsearch/reference/2.0/geo-shape.html
</comment><comment author="clintongormley" created="2015-09-22T14:05:07Z" id="142298638">Bah again :)  So it WAS merged into the 2.0 branch, but after 2.0.0-beta2 was built.  Good news is it will be in the next release of 2.0.0.  I've added a "coming" tag to the docs.

thanks @Eschon 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix GeoPointFieldMapper to index geohash at correct precision.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13649</link><project id="" key="" /><description>Fixes a bug with GeoPointFieldMapper where geohash was only indexed if `geohash_precision` was set to 12. This also adds test coverage for varying geohash precision and `geohash_prefix` indexing.

closes #12467 
</description><key id="107147573">13649</key><summary>Fix GeoPointFieldMapper to index geohash at correct precision.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-18T07:19:24Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-23T14:37:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-18T17:45:30Z" id="141517621">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Setting ES_HEAP_SIZE in windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13648</link><project id="" key="" /><description>I'm using Windows 2012 R2 machine.

I have set my Heap Size in Environment variable as follows
ES_HEAP_SIZE
4g

After setting the heap size, i have installed Elasticsearch as windows service using command

&gt; service.bat install

When i started the service, Elasticsearch services has taken 4GB properly (Checked in Taskmanger.exe)

After some time, the memory used by elasticsearch service is came down to 1 GB.

Is this expected?

Helps much appreciated :)
</description><key id="107134467">13648</key><summary>Setting ES_HEAP_SIZE in windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PandiyanCool</reporter><labels><label>:Core</label><label>discuss</label></labels><created>2015-09-18T05:04:29Z</created><updated>2017-02-13T15:33:11Z</updated><resolved>2017-02-13T15:33:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-18T15:18:57Z" id="141480842">@PandiyanCool Are you setting `bootstrap.mlockall` to true in the config file?  Also, what version of ES are you using?
</comment><comment author="PandiyanCool" created="2015-09-19T03:45:26Z" id="141617555">Yup, I've set it. I'm using version 1.7.1
</comment><comment author="clintongormley" created="2015-09-19T13:22:47Z" id="141667842">@gmarz any ideas about this?
</comment><comment author="gmarz" created="2015-09-21T21:14:20Z" id="142109928">Hey @PandiyanCool how much RAM does the machine have?  What's the % of total memory used after elasticsearch starts up (and the 4gb is allocated)?
</comment><comment author="PandiyanCool" created="2015-09-22T01:55:51Z" id="142158022">@gmarz 
Total RAM memory is 8 GB
After installing &amp; starting the Elasticsearch service, 4 GB is used by Elasticsearch service (checked in task manager - details tab)

After some time, Elasticsearch services is using ~less than 1 GB (as per task manger details).

But when i checked in nodes/stats in API request, the jvm committed heap size is showing 4 GB properly.

Am i missing something?
</comment><comment author="gmarz" created="2015-09-22T21:53:19Z" id="142433229">So I've been looking into this have been able to reproduce the same behavior, and I don't think this is expected.

The 4gb committed heap size that you see in the node stats API is the amount of virtual memory that's _reserved_ by setting ES_HEAP_SIZE (`Xms`), which is expected, even with `bootstrap.mlockall` disabled.

By enabling `bootstrap.mlockall`, we expect the call to `VirtualLock()` to lock the working set into physical memory, which happens initially (this is the memory you see in task manager), but eventually drops off.

I don't have a solid explanation for this yet, but I have observed that the more memory pressure the system is under (i.e. less free space available), the quicker the "drop off" occurs.  It's as though Windows doesn't respect the fact that the pages in the working set are locked, and will release them when resources become low.

There's a lot of info out there that seems to indicate that `VirtualLock` doesn't guarantee pages won't be swapped, only reduces the odds, however the documentation says nothing about this.

I'll continue to look into a possible fix/explanation for this...
</comment><comment author="PandiyanCool" created="2015-09-23T04:06:34Z" id="142485534">Sure @gmarz  :)
</comment><comment author="gmarz" created="2015-09-23T17:02:37Z" id="142663828">From https://msdn.microsoft.com/en-us/library/windows/desktop/ms686237(v=vs.85).aspx:

&gt; When an application is idle, or a low-memory situation causes a demand for memory, the operating system can reduce the application's working set below its minimum working set limit. If memory is abundant, the system might allow an application to exceed its maximum working set limit. The QUOTA_LIMITS_HARDWS_MIN_ENABLE and QUOTA_LIMITS_HARDWS_MAX_ENABLE flags enable you to ensure that limits are enforced.

Sounds like this could be the issue.  Currently, we call [SetProcessWorkingSetSize](https://msdn.microsoft.com/en-us/library/windows/desktop/ms686234%28v=vs.85%29.aspx) which doesn't accept any flags for controlling the working set size limits.

I'm going to try replacing this with `SetProcessWorkingSetSizeEx()` with the QUOTA_LIMITS_HARDWS_MIN_ENABLE flag.
</comment><comment author="PandiyanCool" created="2015-09-24T04:35:04Z" id="142808804">thanks for information @gmarz :) Happy to receive more details about this, Interesting.
</comment><comment author="PandiyanCool" created="2015-12-08T05:50:47Z" id="162776016">hi @gmarz 

Is there any updates on this?
</comment><comment author="gmarz" created="2015-12-08T14:33:23Z" id="162899207">Hey @PandiyanCool, sorry for the delay here.  Had to put this on the back burner as I've been bogged down with other commitments.  That said, haven't forgotten about this and will look into it in the coming days.
</comment><comment author="PandiyanCool" created="2015-12-10T06:32:57Z" id="163515006">thanks @gmarz  :)
</comment><comment author="blkbltjns" created="2017-01-17T20:23:40Z" id="273288528">Any word on this issue?</comment><comment author="PandiyanCool" created="2017-02-13T13:18:34Z" id="279389902">No @blkbltjns </comment><comment author="gmarz" created="2017-02-13T15:33:11Z" id="279425785">Hey @PandiyanCool @blkbltjns I apologize for the really slow reply on this one.

I did try swapping out `SetProcessWorkingSetSize` with `SetProcessWorkingSetSizeEx(QUOTA_LIMITS_HARDWAS_MIN_ENABLE)` a while back as explained above, but unfortunately it didn't have any effect.

&gt; There's a lot of info out there that seems to indicate that VirtualLock doesn't guarantee pages won't be swapped, only reduces the odds, however the documentation says nothing about this.

This seems to be true, and VirtualLock is just a hint, or a best effort- and doesn't guarantee to keep pages in physical memory.  If the process has no running threads, or the system is low on resources, the OS is free to swap out the process.  We will update our docs to mention this.

Still, it's worth enabling it as it's better than nothing.  However, if you want to guarantee that ES is never swapped then you can reduce the size of the page file, or disable it entirely (however this has other consequences like losing memory dumps, and running out of memory can cause the entire OS to crash).

Closing this one for now as there isn't anything actionable.  But please feel free to continue discussion.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Intellij support for plugins with extra permissions.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13647</link><project id="" key="" /><description>graduate this from a hack for insecure plugins to something we can
live with for per-module/plugin permissions, it now works reasonably
in unit tests and with Intellij and Eclipse IDEs.

remove security warnings: we will deal with these issues in a secure
way, if we cannot, then the plugin shouldn't be in our core codebase.

separately, fix test logging when running under intellij.
</description><key id="107130856">13647</key><summary>Add Intellij support for plugins with extra permissions.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>non-issue</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-18T04:18:55Z</created><updated>2015-09-30T06:25:27Z</updated><resolved>2015-09-18T04:35:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-18T04:32:02Z" id="141341140">LGTM
</comment><comment author="rmuir" created="2015-09-18T04:35:07Z" id="141341354">Thanks for help with intellij @rjernst 
</comment><comment author="rmuir" created="2015-09-30T06:25:27Z" id="144294429">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index the query terms from the percolator query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13646</link><project id="" key="" /><description>By adding the query terms to the percolate document, many percolate requests can execute much quicker, because less percolator queries then need to be evaluated by the in-memory index.

Also added `realtime` option that contols whether a percolate request can access the percolator queries in realtime. By default the percolate requests are not executed in realtime. 

The query terms enhancement is only available in **near** realtime mode and on indices created version `3.0.0` and later.

PR for #12664
</description><key id="107077908">13646</key><summary>index the query terms from the percolator query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>feature</label><label>release highlight</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T21:30:04Z</created><updated>2016-02-03T10:11:49Z</updated><resolved>2016-01-06T15:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-18T15:10:48Z" id="141478997">@martijnvg wondering if we should just execute percolation in real time always.  Percolation indices don't generally have high indexing rates, so there won't be many queries sitting unrefreshed.  This of course assumes that we can distinguish recently indexed but unrefreshed queries from those in the index?
</comment><comment author="martijnvg" created="2015-09-18T16:53:30Z" id="141506116">&gt; This of course assumes that we can distinguish recently indexed but unrefreshed queries from those in the index?

Unfortunately we can't distinguish between refreshed and unrefreshed queries. What we can maybe do is keep track of two sets of percolate queries. One set for the refreshed ones and one set of queries that have not been refreshed yet. If a refresh happens we can move the percolator queries from the second set to the first set. At percolate time we then always need to check queries in the second set. 

Right now we can't intercept refresh calls, but we could add that (the same way index/create/delete calls get intercepted right now). Also we need to make sure percolator queries are not evaluated twice, (so not once for the first and second set of queries). This makes this approach a bit more trickier.
</comment><comment author="clintongormley" created="2015-09-18T17:20:02Z" id="141512302">@martijnvg so does setting the `realtime` parameter essentially disable this optimization?
</comment><comment author="martijnvg" created="2015-09-18T18:15:01Z" id="141523679">@clintongormley Yes, the realtime execute doesn't use the Lucene index. When execute in realtime we only iterate over all the in-memory parsed queries. When we execute **not** in realtime we basically run a query (based on percolate query, alias filter and now in this pr also the query terms query) and the percolator queries that are matched are evaluated whether they match with the document being percolated.
</comment><comment author="bleskes" created="2015-09-18T18:19:38Z" id="141524733">I&#8217;m wondering how important is real time percolation in the context of this speed up? Should we keep it simple and go with NRT all the time?

&gt; On 18 Sep 2015, at 20:15, Martijn van Groningen notifications@github.com wrote:
&gt; 
&gt; @clintongormley Yes, the realtime execute doesn't use the Lucene index. When execute in realtime we only iterate over all the in-memory parsed queries. When we execute not in realtime we basically run a query (based on percolate query, alias filter and now in this pr also the query terms query) and the percolator queries that are matched are evaluated whether they match with the document being percolated.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="martijnvg" created="2015-09-18T20:27:06Z" id="141558926">@bleskes You mean remove the realtime percolation?
</comment><comment author="clintongormley" created="2015-09-19T13:07:53Z" id="141664652">Yeah, I'm leaning towards NRT as well.  
</comment><comment author="martijnvg" created="2015-09-21T18:28:57Z" id="142068492">After thinking about it a bit more, I think removing the realtime aspect from the percolator makes sense for the following reasons:
- The realtime aspect only applies on modifications to the percolator queries and there aren't usually that many percolator queries compared to documents in a  regular index.
- The realtime aspect can still be achieved by executing a refresh after a percolate query has been added or modified. Either via the refresh api or the refresh that an index/update optionally can execute.
- Removing the realtime aspect from the percolate api allows to cleanup some code too.
</comment><comment author="martijnvg" created="2015-09-22T14:58:24Z" id="142313730">I updated this PR. The `realtime` option that has been added before in this PR has been removed and the percolator isn't realtime any more. (meaning that modifications to percolator queries aren't visible immediately to the percolator and a refresh needs to take place to make the changes visible to the percolator)
</comment><comment author="jpountz" created="2015-09-24T15:26:57Z" id="142962913">OMG
</comment><comment author="jpountz" created="2015-09-24T16:02:32Z" id="142973475">Open questions/notes:
- if we move the percolator to NRT, then maybe we could have a "PercolatorQuery" that would use the extracted terms as an approximation and the memory index to "confirm" matches? This way we would reuse existing logic in Lucene when intersecting queries that match a documents with filters on the metadata?
- maybe this way we could also use IndexSearcher directly and then things like running aggregations as part of the percolation process would almost come for free?
- should we build a terms query rather than a boolean query to match queries, which will have the benefit of not failing if there are many terms? I'm wondering if we should also disable this optimization for documents that are absurdly large (probably not now but watch for issues once this optimization is released to the wild)
- should we extract terms from the MemoryIndex instead of re-analysing fields since we need to build it anyway to confirm matches?
</comment><comment author="martijnvg" created="2015-09-25T08:40:18Z" id="143161677">@jpountz I like these ideas! I'll try them out.
</comment><comment author="martijnvg" created="2015-09-27T22:34:56Z" id="143599080">I updated this PR and made the suggested changes. It is a bit rough, but the changes look good and cleans things up in the percolator. As a nice side effect of the introduction of the `PercolatorQuery` is that the percolator becomes more unit testable!  I'll cleanup this PR and add the unit tests.
</comment><comment author="martijnvg" created="2015-10-04T20:37:58Z" id="145385598">I cleanup up this PR and added more unit tests. While cleaning up I changed another property of the percolator. The percolate api will no longer return all matches and instead returns 10 matches by default (the can be changed via the `size` parameter).

I also updated the query extract logic to only extract the longest term from a phrase query or bool query with no should clauses.
</comment><comment author="martijnvg" created="2015-10-07T20:30:34Z" id="146318931">@jpountz Thanks for the review! I updated the PR based on your comments and answered your questions.
</comment><comment author="martijnvg" created="2015-10-08T13:22:21Z" id="146543623">@jpountz I updated the PR and fixed the other comments too.
</comment><comment author="martijnvg" created="2015-10-08T20:04:39Z" id="146670909">I was able to remove most of the custom/complex reduce logic with `TopDocs#merge`.

I'm removing the `v2.1.0` label as there are many breaking changes and the for latest commit I intentionally didn't add bwc logic. Once this PR has landed in master, I can open another PR to merge part of this change in 2.x with the bwc logic if needed.
</comment><comment author="jpountz" created="2015-10-16T13:21:48Z" id="148715114">Thanks @martijnvg I think this is getting closer. However I would need help to finish the review:
- PerFieldMappingPostingsFormat ( @rjernst maybe?) Should we have mappings for the query metadata fields or is it ok this way?
- TranslogRecoveryPerformer ( @s1monw or @bleskes ). This code is way out of my comfort zone and I would like help confirming that changes there are ok?
</comment><comment author="martijnvg" created="2015-10-26T10:57:50Z" id="151099487">@jpountz thanks a lot for the great review. I updated the PR and addressed the comments you have made.
</comment><comment author="martijnvg" created="2015-11-02T04:32:20Z" id="152910245">I merged master into this branch and fixed any issues that was caused from merging.

I gave the two outstanding issues (PerFieldMappingPostingsFormat and TranslogRecoveryPerformer) some more thought (based on @jpountz suggestion) and I think if query terms extracting happened from  a custom field mapper or object mapper (that can add new query metadata fields when the query field is parsed into a Lucene query) instead of from a custom `IndexingOperationListener` (in PercolatorQueriesRegistry) then the two outstanding issues would be fixed in a nice way:
- All percolator query meta data fields would be mapped fields (so the if statement in PerFieldMappingPostingsFormat can be removed) 
- The query metadata extracting would rely on `IndexingOperationListener` (so the code that invoked the listener during recovery can be removed).

In order to activate this custom field impl, we can just change the default percolator mapping. Then can drop the `IndexingOperationListener` interface and also do what @bleskes suggested (load the queries per segment upon refresh and maybe only the used queries)
</comment><comment author="martijnvg" created="2015-11-02T11:06:07Z" id="152989329">Moving the query extraction to a custom field mapper is going to be hack, because the extracting requires query parsing to happen and that can't be done in a custom field mapper implementation, since the mapping code is isolated from all the non mapping service (which is good!). What can be done in a custom field mapper impl. is detect what fields query extraction is going add. (it can just traverse the contents of the `query` object field), at least this we can make sure those query meta fields are mapped.

The actual query extraction should happen somewhere else. It should be post document parse, but pre-indexing. 
</comment><comment author="jpountz" created="2015-11-02T18:46:18Z" id="153121417">&gt; What can be done in a custom field mapper impl. is detect what fields query extraction is going add. (it can just traverse the contents of the query object field), at least this we can make sure those query meta fields are mapped.

I'm wondering if we could change query extraction a bit in order to have a fixed mapping. For instance instead of extracting a `{term: { field: value}}` query into `_query_metadata_field: "value"`, we could do `_query_metadata: "field:value"`. This way we would only have two fields `_query_metadata` and `_query_metadata_unknown` that we could add to the default percolator template and then percolation would not have to add dynamic mappings for query metadata? The problem however is that we need to pick as a separator a sequence of characters that can't be used in a field name.
</comment><comment author="martijnvg" created="2015-11-03T04:02:16Z" id="153233925">@jpountz that does make things easier, let me try that out.
</comment><comment author="martijnvg" created="2015-11-03T10:12:34Z" id="153306192">Updated this PR:
- First commit changes the query terms extraction to store all extracted terms from all fields in a single query terms metadata indexed field like @jpountz suggested. (each term value uses its field name it was extracted from as a prefix). This looks solid and allowed me to drop the questionable if statement in `PerFieldMappingPostingsFormat` and allowed the query terms fields to be mapped fields in the mapping, since the fields are no longer dynamic.
- The second commit moves where the query terms extraction takes place from `PercolatorQueriesRegistry.RealTimePercolatorOperationListener#preIndex(...)` to a private method in `IndexShard` named `postPrepareIndex(...)` that gets invoked on each index operation and index recovery operation. I'm not sure if this is the best place, but it does solve the indexing stats issue that @bleskes mentioned when the query terms extraction for recovery took place in `RealTimePercolatorOperationListener`
</comment><comment author="rjernst" created="2015-11-09T00:55:00Z" id="154894499">The mapping changes LGTM. Thanks for removing the special logic!
</comment><comment author="bleskes" created="2015-11-09T08:40:14Z" id="154997545">@martijnvg and I chatted some more about this and we think we can make this work using a standard FieldMapper ala `FieldNamesFieldMapper`. Also, the hope is to only use one lucene field instead of two. Right now we have one field to hold query terms and another to indicate that the query is not parseable. However, since the query terms field's format is very strict, we feel we can use a magic value in it to indicate non-parseable queries. 
</comment><comment author="martijnvg" created="2015-12-30T11:15:42Z" id="167980706">I've updated this PR:
- Merged in master.
- Moved the query extraction to a field mapper implementation.
- Added a bwc test that tests that percolator queries created on a 2.x cluster keep on working.
</comment><comment author="jpountz" created="2015-12-31T15:53:41Z" id="168213893">Things look good to me in general, I mostly left minor comments. Since you refactored metadata extraction to only use a single field, could you check that we actually reject documents that have an empty field name at the root level so that it is impossible that an actual value gets extracted as if extraction was impossible?
</comment><comment author="martijnvg" created="2016-01-04T08:09:08Z" id="168607157">@jpountz Good point, documents that have no or an empty `query` field should be rejected.
</comment><comment author="martijnvg" created="2016-01-04T14:45:01Z" id="168695379">I've updated the PR:
- Renamed `QueryMetaDataService` to `ExtractQueryTermsService`
- Moved the string field holding the extracted terms under the percolator field.
- Brought back the unknown query field, because of an ambiguity issue. If a percolator query holds a query that can't be extracted we just added the nul code point (), but that collides with if we extract terms from an 'empty' field and value, which ES happily accepts (`{ "" : "" }`).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix more issues with plugin unit tests and allow running from IDE.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13645</link><project id="" key="" /><description>This is the more sheisty business along the same lines as
https://github.com/elastic/elasticsearch/pull/13638

1 hour total adding the real functionality, days of wasted time
on simulated fake functionality to satisfy our crazy test framework...

I debugged on the problematic jenkins machine and I think issues are
from parsing the classpath and URL normalization etc (trailing slashes
vs not, etc in URLs). So I simplifed the code, to remove this completely,
inverting the logic so we just use an exclusion list instead of inclusion one.

I also allow tests for these plugins to run from the IDE (works at least for eclipse) too.
At least for eclipse this is even less realistic as it piles all the code (src and test)
into a single codebase, but it means you can _use it_ and you just have to run mvn verify
before pushing as always. And as always... best effort.
</description><key id="107074420">13645</key><summary>Fix more issues with plugin unit tests and allow running from IDE.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>PITA</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T21:07:15Z</created><updated>2015-09-30T06:25:03Z</updated><resolved>2015-09-17T22:06:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-17T21:30:22Z" id="141233288">LGTM
</comment><comment author="rmuir" created="2015-09-17T22:05:54Z" id="141246728">This IDE heuristic does not work for intellij. I will open an issue to improve it and fix that, I am sad the eclipse hack does not work for it. But first i want to get jenkins happy.
</comment><comment author="rmuir" created="2015-09-30T06:25:03Z" id="144294292">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove esoteric apt-get in Vagrantfile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13644</link><project id="" key="" /><description>Removes an esoteric `apt-get update` variant used in Vagrantfile that was
causing only parts of the apt repository to update. That was the point of
the command but when it would leave the repository only half built which
made installing anything but Java difficult. The speed isn't worth the
complexity.
</description><key id="107061220">13644</key><summary>Remove esoteric apt-get in Vagrantfile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T19:52:36Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-22T13:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-17T19:52:46Z" id="141206833">Ping @tlrx or @spinscale for review.
</comment><comment author="tlrx" created="2015-09-22T13:03:19Z" id="142282062">LGTM
</comment><comment author="nik9000" created="2015-09-22T13:08:37Z" id="142283679">Merged to master. I'll backport this to 2.0 and 2.x as well.
</comment><comment author="nik9000" created="2015-09-22T14:25:34Z" id="142304718">And this is in 2.0 and 2.x now as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make bin/elasticsearch wait for pidfile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13643</link><project id="" key="" /><description>If bin/elasticsearch is run with the option to daemonize and the option to
write a pidfile then it will wait for 30 seconds for Elasticsearch to write
the pidfile. If it fails to write the pidfile before the timeout then it
will warn the user to check the logs and further warn them that if nothing
shows up in the logs that they should attempt to run Elasticsearch in the
foreground.

Closes #13392
</description><key id="107059879">13643</key><summary>Make bin/elasticsearch wait for pidfile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label></labels><created>2015-09-17T19:44:06Z</created><updated>2016-02-13T12:28:43Z</updated><resolved>2016-02-10T14:46:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-17T19:45:46Z" id="141205496">Ping @tlrx and @spinscale to look at this. I'm not particularly proud of it but it does get the job done for the init script. systemd is another matter - I think there are more systemd ways to do it and I'm not working on that now.
</comment><comment author="tlrx" created="2015-09-22T14:51:37Z" id="142311912">@nik9000 I left some comments. Actually I'm not sure how it fixes #13392...? This only checks for pidfile but still silently fail when unsupported java version is used (the `&amp;` swallow the stdout explanation message). 
</comment><comment author="nik9000" created="2015-09-22T14:59:24Z" id="142313982">&gt; @nik9000 I left some comments. Actually I'm not sure how it fixes #13392...? This only checks for pidfile but still silently fail when unsupported java version is used (the &amp; swallow the stdout explanation message).

It should fail and give the user the message about the pidfile not being written which is more of a hint than the nothing we gave before. It _should_. At least.
</comment><comment author="electrical" created="2016-01-13T10:06:48Z" id="171240176">Perhaps its just me, but why should bin/elasticsearch care about the pid file? wouldn't that be the job of the service manager that's being used?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>YAML syntax error in elasticsearch.yml causes silent failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13642</link><project id="" key="" /><description>A YAML syntax error in elasticsearch.yml causes Elasticsearch to fail without logging.
Leaving out the space between the colon and the value in an entry causes Elasticsearch to fail to start without logging the failure.
Example: 
index.number_of_shards:6
instead of 
index.number_of_shards: 6

Attempting to start the service shows that it fails.
[FAIL] Starting Elasticsearch Server: failed!
But there is no information in the logs.

Elasticsearch version: 1.7.1
</description><key id="107056837">13642</key><summary>YAML syntax error in elasticsearch.yml causes silent failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lb425</reporter><labels /><created>2015-09-17T19:30:39Z</created><updated>2015-09-18T15:21:44Z</updated><resolved>2015-09-18T15:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dexterama" created="2015-09-17T19:40:26Z" id="141204123">The same issue exists if you leave a space BEFORE the cluster name. By default, it is commented out with a #, then a space, then cluster.name. If you remove the # but not the space when you name your cluster, a silent exception (unlogged) occurs but the instance won't start.
</comment><comment author="PandiyanCool" created="2015-09-18T09:29:40Z" id="141397707">Good catch !!
</comment><comment author="clintongormley" created="2015-09-18T15:21:44Z" id="141481478">This has been fixed in 2.0, which would now throw an exception like the following:

  Exception in thread "main" SettingsException[Failed to load settings from [elasticsearch.yml]]; nested: ElasticsearchParseException[malformed, expected settings to start with 'object', instead was [VALUE_STRING]];
  Likely root cause: ElasticsearchParseException[malformed, expected settings to start with 'object', instead was [VALUE_STRING]]
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: More moving of validation to constructors and setters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13641</link><project id="" key="" /><description>This PR is the second batch in moving the query validation we started
to collect in the validate() method to the corresponding setters
and constructors.
</description><key id="107015754">13641</key><summary>Query Refactoring: More moving of validation to constructors and setters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-17T15:52:55Z</created><updated>2016-03-11T11:50:49Z</updated><resolved>2015-09-20T21:43:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-17T15:57:00Z" id="141131124">left one comment LGTM otherwise - no need for another review
</comment><comment author="cbuescher" created="2015-09-20T21:43:26Z" id="141837696">Merged to query refactoring branch with 37b46fb.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start elasticsearch and verify it does what it should after installing in custom dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13640</link><project id="" key="" /><description>&#8230;stalling in custom dir
</description><key id="107009889">13640</key><summary>Start elasticsearch and verify it does what it should after installing in custom dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-09-17T15:26:02Z</created><updated>2015-09-18T17:04:36Z</updated><resolved>2015-09-17T15:49:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-17T15:47:30Z" id="141128550">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add log4j appender jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13639</link><project id="" key="" /><description>Is there anyway to configure a new appender, add a jar for the logging of elasticsearch?

We would like to use another transport that is not avaible

Thanks
</description><key id="107005445">13639</key><summary>Add log4j appender jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felipegs</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-09-17T15:04:50Z</created><updated>2016-05-20T20:36:20Z</updated><resolved>2016-05-20T20:36:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2016-05-20T20:36:20Z" id="220712243">Closing in favor of #17697.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better simulate problematic plugins permissions in unit tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13638</link><project id="" key="" /><description>We don't have a plugin .zip for unit tests, so we can't do it
correctly. But we can approximate it better, so that if code
is simply missing an AccessController block at least tests will fail.

Cloud-gce unit tests now fail, as they should: they are missing some blocks
around problematic GCE code but we don't yet have the integ tests to catch them.
David only found this out via manual testing on https://github.com/elastic/elasticsearch/pull/13612

I will merge in his changes (to separate out from that unrelated PR) and add anything else missing until unit tests are passing.

Currently the plugins in question are cloud ones, and so we need to 
get the best we can out of the tests we have.
</description><key id="107002372">13638</key><summary>Better simulate problematic plugins permissions in unit tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>review</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T14:50:28Z</created><updated>2015-09-30T06:24:28Z</updated><resolved>2015-09-17T16:06:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-17T15:21:08Z" id="141120173">LGTM
</comment><comment author="rmuir" created="2015-09-30T06:24:28Z" id="144294103">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add documentation for setting network.host with azure discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13637</link><project id="" key="" /><description>With 2.0, we now bind to `localhost` by default instead of binding to the network card and use its IP address.

When the discovery plugin gets from Azure API the list of nodes that should form the cluster, this list is pinged then. But as each node is bound to `localhost`, ping does not get an answer and the node elects itself as the master node.

Closes #13591
</description><key id="106997716">13637</key><summary>Add documentation for setting network.host with azure discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>docs</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T14:30:21Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-18T17:53:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-18T16:30:56Z" id="141499645">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version bump 2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13636</link><project id="" key="" /><description>Add version 2.0.0-rc1 and make it current, and add bwc indices for 2.0.0-beta2. 
</description><key id="106992563">13636</key><summary>Version bump 2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2015-09-17T14:05:58Z</created><updated>2015-09-17T14:39:25Z</updated><resolved>2015-09-17T14:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-17T14:24:15Z" id="141102752">Fixed the version to 2000051
</comment><comment author="s1monw" created="2015-09-17T14:39:05Z" id="141106937">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version bump 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13635</link><project id="" key="" /><description>Add version 2.0.0-beta2 and add bwc indices for 2.0.0-beta1 and 2.0.0-beta2
</description><key id="106992309">13635</key><summary>Version bump 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2015-09-17T14:04:37Z</created><updated>2015-09-17T14:10:49Z</updated><resolved>2015-09-17T14:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Version bump master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13634</link><project id="" key="" /><description>Add version 2.0.0-beta2 and bwc indices for 2.0.0-beta1 and beta2
</description><key id="106991569">13634</key><summary>Version bump master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2015-09-17T14:00:36Z</created><updated>2015-09-17T14:10:32Z</updated><resolved>2015-09-17T14:10:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Adds a validation for plugins script to check if java is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13633</link><project id="" key="" /><description>This does the same validation as in elasticearch bin script.
Closes #13613 
</description><key id="106986753">13633</key><summary>Adds a validation for plugins script to check if java is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">andrestc</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T13:37:07Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-21T18:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-17T14:12:06Z" id="141099704">Looks good to me! I tested it like so:

``` bash
$ export JAVA=$(which java)
$ sudo chmod -x $JAVA
$ bin/plugin
Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME
$ bin/elasticsearch
Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME
$ sudo chmod +x $JAVA
$ bin/plugin
ERROR: command not specified
&lt;snip&gt;
$ bin/elasticsearch
[2015-09-17 14:07:28,822][INFO ][org.elasticsearch.node   ] [Tom Cassidy] version[3.0.0-SNAPSHOT], 
&lt;snip&gt;
^C
```

How would you feel about adding a test like that to the tests in `qa/vagrant`? There are instructions for running them in TESTING.asciidoc. I figure it'd be good to add this to [20_tar_package.bats](https://github.com/elastic/elasticsearch/blob/master/qa/vagrant/src/test/resources/packaging/scripts/20_tar_package.bats).

The test in qa/vagrant are always run in a virtual machine so you can do mean stuff like `chmod -x /usr/bin/java` without worrying about upsetting someone.
</comment><comment author="andrestc" created="2015-09-17T14:20:39Z" id="141101875">Sure @nik9000, i'll definitely add the test! I wanted to add it to the original PR but couldn't find where to place them. I'll update this once im done.
</comment><comment author="andrestc" created="2015-09-18T01:23:52Z" id="141301035">@nik9000 I added 2 separated test cases, one for each script. Took a while to download everything I needed for the tests. Had to also read a bit on bats.
</comment><comment author="nik9000" created="2015-09-18T02:09:30Z" id="141308770">Sorry!  Vagrant is lots of downloading. Did you do just one box or all of
them?  One box is fine. Certainly for this. All of them is a really odyssey
of downloading.
On Sep 17, 2015 9:23 PM, "Andr&#233; Carvalho" notifications@github.com wrote:

&gt; @nik9000 https://github.com/nik9000 I added 2 separated test cases, one
&gt; for each script. Took a while to download everything I needed for the
&gt; tests. Had to also read a bit on bats.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13633#issuecomment-141301035
&gt; .
</comment><comment author="andrestc" created="2015-09-18T11:39:57Z" id="141424751">Its ok! I ended up downloading all of them but ran tests on a single one.
</comment><comment author="andrestc" created="2015-09-18T16:50:06Z" id="141505431">I addressed your comment and updated the branch, @nik9000.
Thanks.
</comment><comment author="nik9000" created="2015-09-18T20:25:46Z" id="141558284">It looks like this fails on centos-7!  Maybe just check that the string is _in_ the output rather than that the entire output is the string?

Other than that its good!

You should probably try something like `mvn -pl qa/vagrant -Dtests.vagrant=all -DtestScripts='20_*' verify` to verify the new test on all the VMs we support. Sorry about that. Its certainly a pain that all the distros are different.
</comment><comment author="andrestc" created="2015-09-20T04:28:14Z" id="141746507">It is ok, I should have tested on all before. My bad.

I'm pretty sure its fixed now. I tested in all boxes except for fedora-22 because I was getting the following output when provisioning the box:

```
==&gt; fedora-22: Transaction Summary
==&gt; fedora-22: ================================================================================
==&gt; fedora-22: Install  38 Packages
==&gt; fedora-22: Total download size: 55 M
==&gt; fedora-22: Installed size: 193 M
==&gt; fedora-22: Downloading Packages:
==&gt; fedora-22: [Errno unknown] Downloading from http://fedora.c3sl.ufpr.br/linux/releases/22/Everything/x86_64/os/Packages/l/libX11-common-1.6.3-1.fc22.noarch.rpm
was successful but error encountered while checksuming: read(19) failed: 
Protocol error: u'Input/Output error'
The SSH command responded with a non-zero exit status. Vagrant
assumes that this means the command failed. The output for this command
should be in the log above. Please read the output to determine what
went wrong.
```

I'm not sure what maybe causing this. I suspect that, if others aren't having this problem, maybe its related to the mirror its using.

Also, I noted Wheezy aka Debian 7 is not on tests anymore. I think we should remove it from docs. If so, i can do it if you want, should I open a new PR?

Thanks for the review!
</comment><comment author="andrestc" created="2015-09-20T15:21:15Z" id="141796448">Actually after retrying a few more times I managed to provision fedora-22. Tests passed fine on it too!
</comment><comment author="nik9000" created="2015-09-21T17:22:01Z" id="142048366">&gt; Also, I noted Wheezy aka Debian 7 is not on tests anymore. I think we should remove it from docs. If so, i can do it if you want, should I open a new PR?

Oh! A new pr would be wonderful.
</comment><comment author="nik9000" created="2015-09-21T18:39:23Z" id="142071109">&gt; Actually after retrying a few more times I managed to provision fedora-22. Tests passed fine on it too!

OK! I got everything passing locally as well! I'm going to merge this then.
</comment><comment author="nik9000" created="2015-09-21T19:06:42Z" id="142079821">Merge to 2.x and master. Thans @andrestc!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add GeoPoint in StreamInput/StreamOutput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13632</link><project id="" key="" /><description>Fixes transport serialization for geo_point type in multi node environment (possibility to use geo_point in script and fielddata_fields).

Closes #13340
</description><key id="106978930">13632</key><summary>Add GeoPoint in StreamInput/StreamOutput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">clement-tourriere</reporter><labels><label>:Fielddata</label><label>bug</label><label>review</label><label>v2.0.0</label></labels><created>2015-09-17T13:02:04Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-08T10:56:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-10-07T14:49:03Z" id="146217930">I'd love to see a test here so it doesn't break in future and so I can be sure exactly what this fixes.
</comment><comment author="jpountz" created="2015-10-08T08:37:19Z" id="146458910">Another thing that is needed would be for GeoPoint to implement ToXContent so that it is properly rendered in the response. I think otherwise we will use toString() and the current GeoPoint.toString() produces an output that looks like geo-json except that it puts the latitude first :s By using ToXContent at least we could use the non-ambiguous representation that writes geo points as an object with "lat" and "lon" properties.
</comment><comment author="javanna" created="2015-10-08T08:41:34Z" id="146459794">&gt; Another thing that is needed would be for GeoPoint to implement ToXContent 

/me wonders why we haven't done it as part of the query-refactoring. sounds like a nice improvement.

One more tiny comment is if we do introduce a public `StreamInput#readGeoPoint` method, I would also add a corresponding `StreamOutput#writeGeoPoint`.
</comment><comment author="jpountz" created="2015-10-08T10:22:50Z" id="146487625">You can ignore my previous comment: I assumed we would use GeoPoint.toString given that GeoPoint does not implement ToXContent but actually we special-case the GeoPoint class in XContentBuilder so everything should be ok.
</comment><comment author="s1monw" created="2015-10-08T10:56:28Z" id="146503852">merged thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename QueryParsingException to a more generic ParsingException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13631</link><project id="" key="" /><description>this allows us to reuse this exception in more places rather than adding
new ones that are basically just subclasses.
</description><key id="106978316">13631</key><summary>Rename QueryParsingException to a more generic ParsingException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T12:58:54Z</created><updated>2015-09-18T15:30:39Z</updated><resolved>2015-09-18T11:30:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-18T11:30:40Z" id="141423554">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>after installing Marvel plugin i get blank screen when browsing to the plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13630</link><project id="" key="" /><description>Hi, 

I've manually installed Marvel plugin on Elastic Search from:
https://download.elasticsearch.org/elasticsearch/marvel/marvel-latest.zip
with this command "bin/plugin -i marvel -u file://PATH_TO_MARVEL_ZIP_FILE"

after the install finished successfully i browsed to http://localhost:9200/_plugin/marvel/  
and got blank screen...
</description><key id="106961006">13630</key><summary>after installing Marvel plugin i get blank screen when browsing to the plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">guybartal</reporter><labels /><created>2015-09-17T11:09:45Z</created><updated>2016-05-19T03:59:14Z</updated><resolved>2015-09-18T10:44:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="diliondani" created="2015-09-18T10:24:33Z" id="141412871">Same here, but the install ends with this messages:
NOTE: Unable to verify checksum for downloaded plugin (unable to find .sha1 or .md5 file to verify)
ERROR: Could not find plugin descriptor 'plugin-descriptor.properties' in plugin zip
Same error with "plugin install elasticsearch/marvel/latest"
running on windows 10 elasticsearch 2.0.0 beta 2.  
</comment><comment author="diliondani" created="2015-09-18T10:30:43Z" id="141413693">OK found the reason here https://discuss.elastic.co/t/marvel-won-t-install-on-elasticsearch-2-0-beta1/29110/2 Patience my friend...
</comment><comment author="dadoonet" created="2015-09-18T10:44:35Z" id="141415927">Yes. Marvel is not ready yet for elasticsearch 2.0.
Stay tuned :)
</comment><comment author="guybartal" created="2015-09-18T15:43:24Z" id="141486343">but i have installed Marvel on Elastic 1.7
</comment><comment author="dadoonet" created="2015-09-18T15:45:33Z" id="141486909">Can you open the developer console?
Anything in browser logs?
</comment><comment author="guybartal" created="2015-09-20T13:45:05Z" id="141786003">![capture](https://cloud.githubusercontent.com/assets/3179107/9980986/ee410122-5fb6-11e5-91f3-28a6e119bada.PNG)
![yml](https://cloud.githubusercontent.com/assets/3179107/9980985/ee3e1354-5fb6-11e5-9451-f4df34b60bdd.jpg)
</comment><comment author="dadoonet" created="2015-09-20T14:11:20Z" id="141789612">The URL here is Kibana and not Marvel.
</comment><comment author="guybartal" created="2015-09-20T17:01:46Z" id="141811102">oops, posted screenshots in wrong issue :)
i have another issue in which i can't browse kibana from remote server, only local browsing is working... i've checked and there are no FWs, but i should open this in a new Issue.

sorry, i'll get the dev console tomorrow for Marvel...
</comment><comment author="dadoonet" created="2015-09-20T17:46:00Z" id="141814218">If you want to open an issue about Kibana, do it in Kibana project.

Better is to ask questions on discuss.elastic.co
</comment><comment author="guybartal" created="2015-09-21T19:07:21Z" id="142080086">I've check again and while browsing http://localhost/_plugin/marvel/
I get blank page with no content. And dev console is empty also. 
Btw, I've restarted my elastic server service after installing the plugin. 
</comment><comment author="dadoonet" created="2015-09-21T21:38:51Z" id="142117003">Any chance you could look at the developper console and see which http calls are blocked if any?
</comment><comment author="flyfy1" created="2016-05-19T03:58:52Z" id="220222158">1. I think Marvel is now supported since I can find it [on the elastic search site](https://www.elastic.co/products/marvel)
2. I'm getting white screen I think it's because I don't have `Kibana` installed.. and I found it quite hard to install
3. It turns out the only thing I want was the `ElasticSearch Sense` from Marvel. So I found [this Chrome Plugin](https://chrome.google.com/webstore/detail/sense-beta/lhjgkmllcaadmopgmanpapmpjgmfcfig?hl=en), and it is helpful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use a dedicated id to serialize EsExceptions instead of it's class name.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13629</link><project id="" key="" /><description>Classnames change quickly due to refactorings etc. If that happens in a minor release
we loose the ability to deserialize the exceptoin coming from another node sicne we today
look it up by classname. This change uses a dedicated static id instead of the classname
to lookup the actual class.
</description><key id="106935796">13629</key><summary>Use a dedicated id to serialize EsExceptions instead of it's class name.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>critical</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-17T08:35:02Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-17T09:09:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-17T08:35:40Z" id="141005766">since the exception mechanism is new in 2.0 I think this should go into 2.0GA since it prevents subtile bugs and is a minor change.
</comment><comment author="s1monw" created="2015-09-17T09:06:28Z" id="141017886">@bleskes applied review comments
</comment><comment author="bleskes" created="2015-09-17T09:07:39Z" id="141018077">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Insecure plugin tests with multiple roots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13628</link><project id="" key="" /><description>Unit tests for plugins are totally unrealistic (no plugin structure, in same classpath/loader, etc).

We tried to support the extra permissions for insecure plugins by using `file:/-` as a url, to match anything. This would also work on windows.

But it would not work on windows if the tests were running from a different drive letter than e.g. .m2/repository.

Here is a different solution (tested on windows). It adds complexity but only to test code. We can't let our real code get complicated because of the crazy way these things are tested...

Closes #13623 
</description><key id="106920871">13628</key><summary>Insecure plugin tests with multiple roots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-09-17T06:38:44Z</created><updated>2015-09-17T06:52:05Z</updated><resolved>2015-09-17T06:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-17T06:47:11Z" id="140984657">Crazy stuff...but LGTM
</comment><comment author="rmuir" created="2015-09-17T06:52:00Z" id="140986099">What is crazy is how much time we waste on the fact we gotta do this N different ways. and forget about IDEs here, they just wont work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TribeNode: batch processing of cluster states</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13627</link><project id="" key="" /><description>The cluster state updates from underlying clusters are now updated in batches, which should improve the cluster change propagation performance. Related to #12814.
</description><key id="106875891">13627</key><summary>TribeNode: batch processing of cluster states</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Tribe Node</label><label>enhancement</label><label>review</label></labels><created>2015-09-16T22:22:48Z</created><updated>2015-12-05T18:00:27Z</updated><resolved>2015-12-05T18:00:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-18T15:55:41Z" id="141491362">@imotov talked this one through I feel the code will be much simpler (with less added components) if we use the recently added `PendingClusterStatesQueue` per incoming cluster. An update would then need to poll all the queues for their next states, process that and that's it. The queue already manages batching.
</comment><comment author="imotov" created="2015-10-08T22:15:26Z" id="146701666">@bleskes I refactored the batch processing. It's still work in progress, more tests are needed and I need to convert other batch tasks into the new mode. But I just want to make sure that I understood you correctly before I jump too deep into it.
</comment><comment author="bleskes" created="2015-10-25T16:51:02Z" id="150941375">@imotov thx! What I had in mind is extract the executor part out of the cluster state update task, so you&#8217;d have a task to do and another executor to apply multiple of those tasks. The semantics would be that all tasks with the same executor can be run in batch. This is very similar to this PR.

To see whether this would work I took your PR and started playing and I ended breaking the update task into 4 roles:
- The info needed for the task (arbitrary object, like your param)
- Configuration (priority, timeout etc.)
- Executor
- Listener

I made all of those interfaces so they can be combined at will (and not having to worry about multiple inheritance) and made the ClusterSTateUpdateTask implement all 3 and have Void as the param of task. This mean that all existing code just compiled :)

Then I took MetaDataMappingService as a use case and removed all local batching and implemented it using the new stuff. It worked pretty well, I think. I even re-introduce batching on the mapping update calls needed for indexing (which removed in the work towards 2.0, see #10720).

It&#8217;s all here: https://github.com/elastic/elasticsearch/compare/master...bleskes:cluster_state_batch?w=1

I made it so it will be very easy to port to 2.1. I think it&#8217;s important because of the mapping update batching. Let me know what you think
</comment><comment author="clintongormley" created="2015-12-05T18:00:27Z" id="162231176">Closing in favour of https://github.com/elastic/elasticsearch/pull/14993
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add 16-bit float data type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13626</link><project id="" key="" /><description>Sometimes you just want fewer bits :)

Would be great if we supported half-precision 16 bit floats.  Robert mentioned that Java doesn't support these newer floats yet, so we'd have to do the conversion in software.  But perhaps someday Java will support it and we can switch over to the intrinsics.

Quoting [wiki](https://en.wikipedia.org/wiki/Half-precision_floating-point_format):

&gt; The minimum strictly positive (subnormal) value is 2&lt;sup&gt;&#8722;24&lt;/sup&gt; &#8776; 5.96 &#215; 10&lt;sup&gt;&#8722;8&lt;/sup&gt;. The minimum positive normal value is 2&lt;sup&gt;&#8722;14&lt;/sup&gt; &#8776; 6.10 &#215; 10&lt;sup&gt;&#8722;5&lt;/sup&gt;. The maximum representable value is (2&#8722;2&lt;sup&gt;&#8722;10&lt;/sup&gt;) &#215; 2&lt;sup&gt;15&lt;/sup&gt; = 65504.

Half-floats would mainly be useful for doc-value metrics that don't need the fully 32bit float, but need a larger dynamic range than an equivalent fixed-point (#13625)

Usual disclaimer about floating point accuracy issues applies, although it is noticed much sooner due to the limited range. :)
</description><key id="106870896">13626</key><summary>Add 16-bit float data type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>discuss</label><label>feature</label></labels><created>2015-09-16T21:48:08Z</created><updated>2016-06-20T19:27:19Z</updated><resolved>2016-06-20T19:27:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-25T14:18:20Z" id="143235976">+1
</comment><comment author="jpountz" created="2016-05-19T15:50:40Z" id="220367754">I have been playing with it at https://issues.apache.org/jira/browse/LUCENE-7289.
</comment><comment author="polyfractal" created="2016-06-20T19:27:19Z" id="227243679">Closed via #18887 (Yay!)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Fixed-Point Numeric data type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13625</link><project id="" key="" /><description>Fixed-point numerics allow the user to specific the number of significant digits they are interested in.  E.g. a user may only care about two sig-figs (`5.01`, `123.19`, etc). instead of the full 6-9 sig-figs in 32bit floats.

Fixed-point numerics can be "layered" on top of our existing support for Longs without too much hassle.  Additional information would need to be saved, such as the number of bits for the fractional component, and the base/sign if it is configurable.  Input/output formatting would also need to know how to convert into the appropriate representation.

But most of the "guts" should work seamlessly.  Ranges are unaffected, aggregations convert everything to Double anyway, etc

Fixed-point has two notable advantages:
- Most important to me, since fixed-points are basically Longs in disguise, they can leverage the compression tricks used in doc values to drastically decrease their on-disk footprint.  In contrast, floats/doubles are stored at their full size since there is no way to encode fp in a random-access format without introducing error
  
  Many "pure metrics" will only care about a few sig-figs (do you really care if your CPU usage is 53.0000001% vs 53.00%) but the space savings will definitely be appreciated
- As a more expert-level feature, if the user knows their dynamic range, they can obtain better precision for their use-case.  E.g. if your range is 0-1 exclusive, a fixed-point can be more precise than floating because you can dedicate all 32 bits to the fractional component instead of only 24bits in the float
</description><key id="106862705">13625</key><summary>Add Fixed-Point Numeric data type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>discuss</label><label>feature</label></labels><created>2015-09-16T20:58:41Z</created><updated>2016-09-27T16:30:19Z</updated><resolved>2016-09-27T16:30:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-16T21:08:00Z" id="140895509">Neat!
</comment><comment author="jpountz" created="2015-09-25T14:21:29Z" id="143236749">+1
</comment><comment author="felixbarny" created="2015-11-04T13:30:43Z" id="153719629">+1
</comment><comment author="kimchy" created="2016-05-13T10:22:09Z" id="219006570">huge +1
</comment><comment author="tsg" created="2016-05-13T10:33:42Z" id="219008610">You might have noticed a few &#128077; s from the Beats team :-). That's because in Topbeat/Metricbeat we have percentage values which we currently represent as a float between 0 and 1 as well as other floats for which we don't need great precision. 

We considered storing them as integers and then use UI logic to transform them in floats, but decided to wait for this (or the half-precision floats) to be implemented, as it would obviously be better for everyone.
</comment><comment author="kimchy" created="2016-05-13T11:37:38Z" id="219019880">btw, this could potentially be completely hidden from the user as an implementation detail? The user can configure precision in the double/float mapping and we do the magic behind the scenes?
</comment><comment author="polyfractal" created="2016-05-13T14:46:39Z" id="219064583">@tsg @kimchy If you and/or the Metricbeat team can skim through #15939 and see what you think of the discussion, that'd be helpful.  The PR was stalled because concern was raised if a software half-float would be better.  I didn't feel comfortable pushing forward with the PR until a comparison could be made, but I'm not sure I can implement a half-float either (and haven't had the time to sit down and learn the needed bit twiddling).

There were questions around the API too: specify number of digits, number of bits, etc.

(You can ignore the code comments, I didn't work on them until the bigger issues were resolved).

&gt; btw, this could potentially be completely hidden from the user as an implementation detail? The user can configure precision in the double/float mapping and we do the magic behind the scenes?

Hiding it as an implementation detail could be doable, need to think about that.  Floating and fixed have very different edge-cases at their extremes which may be confusing to the user.  E.g. fixed points are precise until they overflow at the edge of their range, floats instead become more increasingly more erroneous.

But no one understand floating error anyway, so maybe it's moot :)
</comment><comment author="jpountz" created="2016-05-18T22:02:57Z" id="220171867">&gt; Floating and fixed have very different edge-cases at their extremes which may be confusing to the user.

+1, I don't think we can mix both under the same type without introducing confusion.

&gt; We considered storing them as integers and then use UI logic to transform them in floats, but decided to wait for this (or the half-precision floats) to be implemented, as it would obviously be better for everyone.

I have mixed feelings about this. We could implement it in elasticsearch, but on the other hand I have the feeling it would be more flexible/efficient/easy to do it from client side. For instance imagine we want to store percentages with a precision of 1%: the client would just have to multiply values by 100 at index time and index them as longs (or ints) rather than floats. And then it would also divide values by 100 when reading the _source or the result of histogram, min, max, terms, percentiles, ... aggregations. I think it is great that elasticsearch would be working directly on the "compressed" representation. If we want to do the same thing from within elasticsearch, first we have to figure out the right API to expose such numbers (which proved a bit tricky on the PR), and then either we need to add a multiplication on every doc values read, or we try to add the optimization to not decode every value but this is quite error-prone as we can optimize eg. histogram aggregations to divide post aggregation, but if the field is fed to the scripting API we still need to decode every value. So I'm afraid it would not be easy to expose in a generic way.

(Also, for the record, even if better than regular floats, I think half floats would still be a poor option to index percentages.)
</comment><comment author="tsg" created="2016-05-19T14:53:32Z" id="220349050">The reason I don't like too much doing that in the client is that while we could easily do the conversion to int in Beats, we don't know how the user consumes the data, it might be via the Kibana discover tab or even directly via the Elasticsearch API. So we'd have to somehow communicate that those values should be divided by 100 or whatever.

If it's not possible to do it in a clean way in Elasticsearch, or it doesn't look like it will be done anytime soon, we might reconsider and choose to do that.

&gt; (Also, for the record, even if better than regular floats, I think half floats would still be a poor option to index percentages.)

That's true for half-floats but not for the fixed points, right?

An alternate idea that @polyfractal mentioned was to implement compression for floats, rather than do things like fixed-points/half-floats. If that is possible, it sounds to me like a good idea since it would benefit more use cases.
</comment><comment author="polyfractal" created="2016-05-19T15:08:00Z" id="220353781">&gt; An alternate idea that @polyfractal mentioned was to implement compression for floats, rather than do things like fixed-points/half-floats. If that is possible, it sounds to me like a good idea since it would benefit more use cases.

I actually pinged @jpountz about this particular last night.  He said the compression scheme (separately encode mantissa and exponent as longs, recombine on reads) won't be possible using the current Doc Values setup.  Multi-valued fields are returned in sorted order, so you'd be unable to re-assemble the correct matching pairs of mantissa/exponent.

But, there are apparently thoughts about adding an iterator scheme to DV, which might allow that style of compression in the future.

Other lossless compression schemes that I know about for floats are all based on the previous value(s) which means they can't be used for random access :(
</comment><comment author="dakrone" created="2016-09-27T15:42:55Z" id="249905177">@polyfractal do we have this now with the `scaled_float` type, since it potentially allows you to do something like this?
</comment><comment author="polyfractal" created="2016-09-27T16:30:19Z" id="249919765">Oh, yeah, this was closed by @jpountz in https://github.com/elastic/elasticsearch/pull/19264 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.net.InetAddresses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13624</link><project id="" key="" /><description>This commit removes and now forbids all uses of com.google.common.net.InetAddresses across the codebase. This is one of many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106828735">13624</key><summary>Remove and forbid use of com.google.common.net.InetAddresses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wittyameta</reporter><labels /><created>2015-09-16T18:10:06Z</created><updated>2015-09-17T08:17:21Z</updated><resolved>2015-09-17T08:17:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-16T18:16:49Z" id="140828700">The automatic CLA check is failing here because at least some of these commits are under a different email address than the one that has signed the CLA. That is fine - I checked against the list of CLA signers and @wittyameta is on it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GceDiscoveryTests is flakey</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13623</link><project id="" key="" /><description>These tests seem to always fail on windows:
http://build-us-00.elastic.co/job/es_core_master_window-2008/2236/
http://build-us-00.elastic.co/job/es_core_master_window-2008/2237/
http://build-us-00.elastic.co/job/es_core_master_window-2012/1754/
http://build-us-00.elastic.co/job/es_core_master_window-2012/1753/
</description><key id="106816058">13623</key><summary>GceDiscoveryTests is flakey</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rjernst</reporter><labels /><created>2015-09-16T16:58:46Z</created><updated>2015-09-17T06:52:05Z</updated><resolved>2015-09-17T06:52:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-16T17:50:25Z" id="140818305">Adding the awaitsfix to these tests caused the build to fail because these are the _only_ unit tests for the plugin. I've disabled unit tests altogether in the gce plugin. They need to be re-enabled in the pom once these are fixed.
</comment><comment author="rmuir" created="2015-09-16T18:01:22Z" id="140822832">The issue is, we have to support elasticsearch in too many ways:
- starting up from bin/elasticsearch (integ tests use this way too)
- running unit tests (which are not realistic)
- running from IDEs.

We can't easily work around the problems with GCE and AWS jars for the latter 2, its just too much fucking work. I waste like 2x as much time on that stuff as actually solving real problems.
</comment><comment author="rmuir" created="2015-09-16T18:19:54Z" id="140830612">The issue is, this plugin has security problems and we cannot simulate it in unit tests on windows machines where jar files are spread across multiple drive letters. 

We should not make this stuff more complicated just for these unit tests. We should spend the effort fixing real problems instead, like the problems in the GCE code.

The same issue may happen with AWS plugins, too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query Refactoring: moving validation to constructors and setters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13622</link><project id="" key="" /><description>This PR is the first batch of changes to move the query validation we previously started to collect in the validate() method to the corresponding setters and constructors and adapt the tests that need changing.
</description><key id="106814911">13622</key><summary>Query Refactoring: moving validation to constructors and setters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-16T16:53:17Z</created><updated>2016-03-11T11:50:50Z</updated><resolved>2015-09-17T12:13:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-17T06:14:06Z" id="140978290">left a few small comments, but LGTM already
</comment><comment author="s1monw" created="2015-09-17T07:23:39Z" id="140990842">left some more comments about private ctors... LGTM otherwise
</comment><comment author="cbuescher" created="2015-09-17T10:07:52Z" id="141033761">Removed the private constructors used for prototypes, use bogus values there now. Left an open question for @javanna.
</comment><comment author="s1monw" created="2015-09-17T11:45:56Z" id="141051684">LGTM
</comment><comment author="cbuescher" created="2015-09-17T12:13:23Z" id="141059803">Merged into branch with 887399e, will continue to work on a second batch of queries in the same way now. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add geo_centroid metric aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13621</link><project id="" key="" /><description>Brought up in discussion of PR #13433 this feature moves the weighted centroid result from `geohash_grid_aggregator` a its own `geo_centroid` metric aggregation. This will save space for users that do not care or need the weighted centroid, and decouple the calculation such that it can be used as a subaggregation. This allows the weighted centroid to be calculated based on attributes besides just geolocation (e.g., weights based on population density or median household income)
</description><key id="106807141">13621</key><summary>add geo_centroid metric aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>:Geo</label><label>feature</label><label>v2.1.0</label></labels><created>2015-09-16T16:10:41Z</created><updated>2015-10-14T21:22:01Z</updated><resolved>2015-10-14T21:22:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove some bogus permissions only needed for tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13620</link><project id="" key="" /><description>Especially the worst of the worst with thread permissions: for example,
this prevents some code from starting daemon thread that will outlive
the elasticsearch process and hang around doing evil shit.
</description><key id="106806825">13620</key><summary>Remove some bogus permissions only needed for tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T16:08:42Z</created><updated>2015-09-30T06:24:04Z</updated><resolved>2015-09-16T16:18:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-16T16:14:01Z" id="140791254">LGTM: lock it down
</comment><comment author="rmuir" created="2015-09-30T06:24:04Z" id="144294058">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>VersionConflictEngineException with script update in cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13619</link><project id="" key="" /><description>We&#180;re having problems with VersionConflictEngineExceptions all the time. The update should happen as a script and increment a number value (see sample document below)

We&#180;re running a cluster of two els instances and I can only imagine that the synchronization is causing the conflict version in one node. I would expect the update not to throw this kind of exception in a cluster, as each update is atomically. We are running four application servers that execute this code and the Exceptions are thrown randomly on all instances.

stacktrace:

```
Caused by: org.elasticsearch.index.engine.VersionConflictEngineException: [kpi][4] [opportunity][1442415600000]: version conflict, current [5933], provided [5932]
        at org.elasticsearch.index.engine.internal.InternalEngine.innerIndex(InternalEngine.java:582) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.index.engine.internal.InternalEngine.index(InternalEngine.java:522) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:425) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:193) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:512) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.doStart(TransportShardReplicationOperationAction.java:426) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.start(TransportShardReplicationOperationAction.java:342) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction.doExecute(TransportShardReplicationOperationAction.java:97) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.index.TransportIndexAction.innerExecute(TransportIndexAction.java:134) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:112) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.index.TransportIndexAction.doExecute(TransportIndexAction.java:60) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:75) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:217) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170) [elasticsearch-1.4.4.jar:]
        at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$1.run(TransportInstanceSingleOperationAction.java:187) [elasticsearch-1.4.4.jar:]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [rt.jar:1.8.0_20]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [rt.jar:1.8.0_20]
        at java.lang.Thread.run(Thread.java:745) [rt.jar:1.8.0_20]
```

sample document:

```
{
  "_index": "kpi",
  "_type": "opportunity",
  "_id": "1442412000000",
  "_version": 14742,
  "found": true,
  "_source": {
    "timestamp": "2015-09-16T14:00:00.249+0000",
    "own": 224,
    "shared": 2,
    "network": 3941,
    "unknown": 10575
  }
}
```

each update script looks like this (one of the lines, only one increment per script):

```
ctx._source.own+=1;
ctx._source.shared+=1;
ctx._source.network+=1;
ctx._source.unknown+=1;
```
</description><key id="106804888">13619</key><summary>VersionConflictEngineException with script update in cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">falkorichter</reporter><labels><label>feedback_needed</label></labels><created>2015-09-16T15:59:55Z</created><updated>2016-07-27T18:25:03Z</updated><resolved>2015-09-16T17:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-16T16:11:55Z" id="140790756">Can you confirm that you are not setting the [`retry_on_conflict` parameter](https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html#_updates_and_conflicts)? This parameter is zero by default and is designed exactly for your use case of updates where the ordering of updates (say incrementing a counter) isn't important.

If you do confirm this, this behavior is expected when you have multiple writers attempting to [update](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html) the same document. You can address this issue by using the [`retry_on_conflict` parameter](https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html#_updates_and_conflicts) to retry when a version conflict occurs. You can read more about this issue in the [documentation on partial updates](https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html) including the specific section on [conflicts](https://www.elastic.co/guide/en/elasticsearch/guide/current/partial-updates.html#_updates_and_conflicts).
</comment><comment author="falkorichter" created="2015-09-16T17:22:53Z" id="140811094">I can confirm I&#180;m not setting the retry_on_conflict parameter. But it sounds exactly like the parameter I want to use. I deployed with the parameter and the exceptions seem to be gone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Automatically set project-id and zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13618</link><project id="" key="" /><description>The project id can be retrieved from the running instance by calling  `http://metadata.google.internal/computeMetadata/v1/project/project-id`.

Same for the default zone: `http://metadata.google.internal/computeMetadata/v1/project/attributes/google-compute-default-zone
`

No need then to force them in `elasticsearch.yml`.
</description><key id="106804481">13618</key><summary>Automatically set project-id and zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery GCE</label><label>adoptme</label></labels><created>2015-09-16T15:58:04Z</created><updated>2016-07-21T13:09:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[test] add test for 'plugin list'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13617</link><project id="" key="" /><description>Tests that the plugins that are reported with 'plugin list' are the
same as in the plugins pom.
</description><key id="106796522">13617</key><summary>[test] add test for 'plugin list'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>review</label><label>test</label></labels><created>2015-09-16T15:21:12Z</created><updated>2015-09-17T08:26:35Z</updated><resolved>2015-09-17T08:26:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-16T15:26:22Z" id="140776137">Left minor comments. In general its great!
</comment><comment author="brwe" created="2015-09-16T16:08:26Z" id="140789948">@nik9000 thanks! I pushed a new commit. Is that what you meant?
</comment><comment author="nik9000" created="2015-09-16T16:20:15Z" id="140792720">&gt; @nik9000 thanks! I pushed a new commit. Is that what you meant?

Yup! I left some small comments. I don't like modifying the file you take as an input parameter but otherwise its cool.
</comment><comment author="brwe" created="2015-09-16T16:37:51Z" id="140797072">pushed another commit
</comment><comment author="nik9000" created="2015-09-16T17:07:10Z" id="140806049">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin.bat error parsing Java options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13616</link><project id="" key="" /><description>When running `plugin.bat` on Windows, passing Java options via the command line results in the following error:

```
c:\elasticsearch\bin&gt;plugin "-Des.plugins.staging=true" install analysis-icu
ERROR: unknown command [-Des.plugins.staging=true]. Use [-h] option to list available commands
```

Adding `-Des.plugins.staging=true` directly to the script [here](https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/plugin.bat#L14) works.

This might be because `plugin.bat` is missing the equivalent of [this in the bash script.](https://github.com/elastic/elasticsearch/blob/master/distribution/src/main/resources/bin/plugin#L69-L83).
</description><key id="106790471">13616</key><summary>plugin.bat error parsing Java options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ywelsch/following{/other_user}', u'events_url': u'https://api.github.com/users/ywelsch/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ywelsch/orgs', u'url': u'https://api.github.com/users/ywelsch', u'gists_url': u'https://api.github.com/users/ywelsch/gists{/gist_id}', u'html_url': u'https://github.com/ywelsch', u'subscriptions_url': u'https://api.github.com/users/ywelsch/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/3718355?v=4', u'repos_url': u'https://api.github.com/users/ywelsch/repos', u'received_events_url': u'https://api.github.com/users/ywelsch/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ywelsch/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ywelsch', u'type': u'User', u'id': 3718355, u'followers_url': u'https://api.github.com/users/ywelsch/followers'}</assignee><reporter username="">gmarz</reporter><labels><label>:Plugins</label><label>blocker</label><label>bug</label><label>low hanging fruit</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-16T14:53:07Z</created><updated>2016-03-10T18:11:03Z</updated><resolved>2015-10-13T11:33:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a BaseParser helper for stream parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13615</link><project id="" key="" /><description>This parser prototype allows to decleratively define parsers for XContent
instead of writing messy and error prone while loops. It's a basic prototype without
much javadocs etc. yet. I just wanted to get it out there for initial feedback before I
polish it.
</description><key id="106771677">13615</key><summary>Add a BaseParser helper for stream parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T13:15:55Z</created><updated>2015-09-22T07:59:13Z</updated><resolved>2015-09-22T07:59:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-16T20:12:40Z" id="140872746">@nik9000 I simplified that registration a lot now... /me gets excited about some java 8 features
</comment><comment author="s1monw" created="2015-09-21T19:05:00Z" id="142079206">I updated this with tests and javadocs, I think it's close / ready
</comment><comment author="rjernst" created="2015-09-21T20:26:33Z" id="142099086">This is **awesome**!  LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-gce] warn message is displayed when not using gce discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13614</link><project id="" key="" /><description>Similar to #13581 (in master branch - 3.0):

GCE plugin now tries to start immediately gce discovery even if we don't set `discovery.type: gce`.
Not really an issue because I guess that if someone installs the cloud-gce plugin, it's probably to have discovery set to `gce`.

But we could may be do something better here...

```
[2015-09-16 13:04:11,048][WARN ][org.elasticsearch.discovery.gce] [Kubik] cloud.gce.project_id is not set.
[2015-09-16 13:04:11,048][WARN ][org.elasticsearch.discovery.gce] [Kubik] cloud.gce.zone is not set.
[2015-09-16 13:04:12,230][WARN ][org.elasticsearch.discovery.gce] [Kubik] cloud.gce.project_id is not set.
[2015-09-16 13:04:12,231][WARN ][org.elasticsearch.discovery.gce] [Kubik] cloud.gce.zone is not set.
[2015-09-16 13:04:12,231][WARN ][org.elasticsearch.discovery.gce] [Kubik] cloud.gce.project_id is not set.
[2015-09-16 13:04:12,231][WARN ][org.elasticsearch.discovery.gce] [Kubik] cloud.gce.zone is not set.
[2015-09-16 13:04:12,234][INFO ][org.elasticsearch.node   ] [Kubik] initialized
[2015-09-16 13:04:12,243][INFO ][org.elasticsearch.node   ] [Kubik] starting ...
[2015-09-16 13:04:12,437][INFO ][org.elasticsearch.transport.netty] [Kubik] Bound profile [default] to address {127.0.0.1:9300}
[2015-09-16 13:04:12,438][INFO ][org.elasticsearch.transport] [Kubik] bound_address {127.0.0.1:9300}, publish_address {127.0.0.1:9300}
[2015-09-16 13:04:12,445][INFO ][org.elasticsearch.discovery] [Kubik] elasticsearch/R_PjKk5fTB2zCV-jPcyQ3w
[2015-09-16 13:04:12,449][WARN ][org.elasticsearch.cloud.gce] [Kubik] disabling GCE discovery. Can not get list of nodes
[2015-09-16 13:04:13,955][WARN ][org.elasticsearch.cloud.gce] [Kubik] disabling GCE discovery. Can not get list of nodes
[2015-09-16 13:04:15,466][WARN ][org.elasticsearch.cloud.gce] [Kubik] disabling GCE discovery. Can not get list of nodes
```
</description><key id="106770029">13614</key><summary>[cloud-gce] warn message is displayed when not using gce discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>adoptme</label><label>low hanging fruit</label><label>v2.0.0-rc1</label></labels><created>2015-09-16T13:08:06Z</created><updated>2016-03-10T18:16:24Z</updated><resolved>2015-09-29T17:27:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="xuzha" created="2015-09-29T17:37:53Z" id="144131148">Added 2.0, 2.1 labels, see the conversations in https://github.com/elastic/elasticsearch/pull/13809
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>plugin script doesn't detect when Java is not installed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13613</link><project id="" key="" /><description>When JAVA is not installed, `bin/elasticsearch` outputs:

```
Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME
```

But when you run `bin/plugin`, you get:

```
bin/plugin: 1: eval: -client: not found
```
</description><key id="106761748">13613</key><summary>plugin script doesn't detect when Java is not installed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-09-16T12:17:38Z</created><updated>2015-09-21T18:40:26Z</updated><resolved>2015-09-21T18:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add `_gce_` network host setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13612</link><project id="" key="" /><description>When running in GCE platform, an instance has access to:

http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip

Which gives back the private IP address, for example `10.240.0.2`.

http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/externalIp

Gives back the public Ip address, for example `130.211.108.21`.

As we have for `ec2`, we can support new network host settings:
- `_gce:privateIp:X_`: The private IP address of the machine for a given network interface.
- `_gce:hostname_`: The hostname of the machine.
- `_gce_`: Same as `_gce:privateIp:0_` (recommended).

Closes #13605.
Closes #13590.

BTW resolveIfPossible now throws IOException so code is also updated for ec2 discovery and 
some basic tests have been added.
</description><key id="106749424">13612</key><summary>Add `_gce_` network host setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>enhancement</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-16T10:59:41Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-07T21:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-16T13:38:45Z" id="140745065">For information, I just tested it on GCE platform.

Without `network.host: _gce_`, it gives:

```
[2015-09-16 13:13:13,204][INFO ][org.elasticsearch.transport] [Sleek] bound_address {127.0.0.1:9300}, publish_address {127.0.0.1:9300}
[2015-09-16 13:13:18,969][INFO ][org.elasticsearch.http   ] [Sleek] bound_address {127.0.0.1:9200}, publish_address {127.0.0.1:9200}
```

With `network.host: _gce_`, it gives:

```
[2015-09-16 13:15:30,097][INFO ][org.elasticsearch.transport] [Ooze] bound_address {10.240.0.2:9300}, publish_address {10.240.0.2:9300}
[2015-09-16 13:15:34,509][INFO ][org.elasticsearch.http   ] [Ooze] bound_address {10.240.0.2:9200}, publish_address {10.240.0.2:9200}
```

But it fails with `_gce:publicIp_`. The URL we check in that case should be `http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/access-configs/0/external-ip`.

Will push a fix.

Note that we can also add `_gce:privateDns_` as `http://metadata.google.internal/computeMetadata/v1/instance/hostname` gives internal hostname like `instance-2.c.dadoonet95.internal`

Will update the PR.
</comment><comment author="dadoonet" created="2015-09-16T14:43:11Z" id="140762727">Was doing some more tests. Actually even if a VM is accessible externally using it's public IP address, you can't assign this IP:

```
[2015-09-16 14:34:11,883][DEBUG][org.elasticsearch.cloud.gce] [Aged Genghis] get network information for [access-configs/0/external-ip]
[2015-09-16 14:34:12,202][DEBUG][org.elasticsearch.cloud.gce] [Aged Genghis] ip found [130.211.108.21]
Exception in thread "main" BindTransportException[Failed to bind to [9300-9400]]; nested: ChannelException[Failed to bind to: /130.211.108.21:9400]; nested: BindException[Cannot assign requested address];
Likely root cause: java.net.BindException: Cannot assign requested address
    at sun.nio.ch.Net.bind0(Native Method)
    at sun.nio.ch.Net.bind(Net.java:433)
    at sun.nio.ch.Net.bind(Net.java:425)
    at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)
    at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
    at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:391)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:315)
    at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Refer to the log for complete error details.
```

So it does not really make sense to support `_gce:publicIp_`...
</comment><comment author="dadoonet" created="2015-09-26T08:00:57Z" id="143408766">@bleskes Do you think you can review this? 
</comment><comment author="dadoonet" created="2015-09-26T15:27:17Z" id="143463685">@bleskes Updated to address your comments. Thanks for the review!
</comment><comment author="bleskes" created="2015-09-26T16:38:58Z" id="143468906">Thx David for implementing this. I left some comments. My biggest concern is that we are hard wired to to the first 0 interface. I glimpsed the doc and could find this to be guaranteed by google. Does it? If so, we need to document what we do. If not, we need to retrieve all of the interfaces and chose the first (in a predictable order) and document that.
</comment><comment author="dadoonet" created="2015-09-27T21:53:25Z" id="143597350">Hey @bleskes 

I added a new commit:
- `GceHostnameType` become `GceAddressResolverType`: note that we should may be do the same in ec2 discovery plugin?
- add a new parameter `cloud.gce.network.card` if you want to use _gce_ to get the IP Address but from another network card
- `GceComputeServiceMock` can now read either `.json` or `.txt` files
- Add new tests for `network.host` equal to `_gce_`, `_gce:privateIp_` or `_gce:doesnotexist_` and for `cloud.gce.network.card`
- replace `www.elastic.co` in test by `localhost` so it could better work when no network connection is available (sometimes it's good to code in a plane :) )
</comment><comment author="rjernst" created="2015-09-28T06:02:48Z" id="143647164">&gt; GceComputeServiceMock can now read either .json or .txt files

Do we really need this leniency? Can we just support one?
</comment><comment author="bleskes" created="2015-09-28T06:49:33Z" id="143653967">Thx @dadoonet . left comments.
</comment><comment author="dadoonet" created="2015-09-28T22:11:18Z" id="143888271">@bleskes I double checked and if I throw an IllegalArgumentException in the `GceNameResolver #resolve()` method, then this test fails:

``` java
        Settings nodeSettings = Settings.builder()
                .put("network.host", "_local_")
                .build();

        NetworkService networkService = new NetworkService(nodeSettings);
        GceComputeServiceMock mock = new GceComputeServiceMock(nodeSettings, networkService);
        networkService.addCustomNameResolver(new GceNameResolver(nodeSettings, mock));
        InetAddress[] addresses = networkService.resolveBindHostAddress(null);
```

But I guess this because I implemented the resolve method in a different way than it was previously...

BTW, I'll add a new test like the above one to make sure GCE does not break core network host settings...
</comment><comment author="bleskes" created="2015-09-29T06:13:33Z" id="143956402">@dadoonet you shouldn't throw an exception if you fail to resolve the value to one the gce plugin owns (i.e., https://github.com/elastic/elasticsearch/pull/13612/files#diff-c9f8f0782cd63de93be4eb880bb97576R104 ) 
IMHO it should throw an exception if:
- A value starts with gce but doesn't matching anything in `GceAddressResolverType`
- A value does match a `GceAddressResolverType` and something goes wrong resolving it.

Does that make sense?
</comment><comment author="dadoonet" created="2015-09-29T06:21:37Z" id="143957231">It does. Thanks!
</comment><comment author="dadoonet" created="2015-09-29T07:46:25Z" id="143975690">@bleskes I added a new commit.

I'm unsure about this line https://github.com/dadoonet/elasticsearch/commit/556495925d5a070ea9c1abdd4eb5eca86c9c8311#diff-c9f8f0782cd63de93be4eb880bb97576R115

Is that a good thing to wrap an IOException in a RuntimeException?
</comment><comment author="bleskes" created="2015-09-29T07:59:59Z" id="143977919">@dadoonet it looks like org.elasticsearch.common.network.NetworkService#resolveInetAddress already throws an IOException, so I think it's OK to change NetworkService.CustomNameResolver#resolveDefault to allow throwing it as well. That said, I still think it's good to wrap the underlying exception with a new one that indicates which settings were used (i.e., change your RuntimeException wrapper to IOException).
</comment><comment author="dadoonet" created="2015-09-29T09:34:14Z" id="144006160">@bleskes Updated:
- Updated GCE code
- Updated EC2 code
- Added EC2 tests: for now it's super simple as we don't mock yet EC2 metadata server (so I added some TODOs) but at least we test something!
</comment><comment author="dadoonet" created="2015-10-02T18:19:09Z" id="145113420">@bleskes When you have time, could you please look at it?

Not really a blocker for 2.0 but it would be really nice to have.
</comment><comment author="dadoonet" created="2015-10-06T16:49:29Z" id="145925067">@bleskes I pushed a new commit (and rebased BTW). Let me know.
</comment><comment author="rmuir" created="2015-10-06T17:39:48Z" id="145940851">I dont see anything guaranteeing that the "first interface" is a private address. I don't think we should solve the problem this way, its too arbitrary and risky.

Instead we should add `_site_local_` to ensure that private addresses are really private addresses if that is the intent and use it for these cloud providers.
</comment><comment author="bleskes" created="2015-10-06T18:49:14Z" id="145962565">@rmuir I presume you refer to #13969 , I respond here to make sure it's all in the same place, but if we continue the discussion (and it does refer to the other ticket), lets continue it there.

The `_gce_` is designed to retrieve the instance's [internal ip](https://cloud.google.com/compute/docs/instances-and-network#networkaddresses), meaning:

&gt; Every instance also has a network IP address that is addressable only within the network. Within the network, instances can also be addressed by instance name and the network will resolve an instance name into a network IP address.

From the docs about what a [network](https://cloud.google.com/compute/docs/instances-and-network#networkaddresses) is:

&gt; Every instance is a member of a single network ... Any communication between instances in different networks, even within the same project, must be through external IP addresses. 

So it seems we're good here? 

@dadoonet I failed to find the reference for the exact metadta url you are using. Can you dig it up for future reference?
</comment><comment author="dadoonet" created="2015-10-06T19:31:55Z" id="145973360">@bleskes @rmuir actually when you query metadata:

```
curl "http://metadata.google.internal/computeMetadata/v1/instance/?recursive=true" -H "Metadata-Flavor:Google"
```

You get back something like (I'm hiding non relevant parts):

``` json
{
   "hostname":"blabla.projectname.internal",
   "networkInterfaces":[
      {
         "accessConfigs":[
            {
               "externalIp":"104.155.53.203",
               "type":"ONE_TO_ONE_NAT"
            }
         ],
         "forwardedIps":[

         ],
         "ip":"10.240.0.2",
         "network":"projects/896329523726/networks/default"
      }
   ]
}
```

The `public` address is exposed within `networkInterfaces.accessConfigs.externalIp` if any.
`networkInterfaces.ip` will only give the private IP so I think we are safe here.

Is there any method which can "double-check" that the IP we get is actually non routable on internet - so it's a private one? If so, I could add a safe-guard and either stop the process or put a big **WARN** saying that the `_gce_` address is not private.
</comment><comment author="dadoonet" created="2015-10-06T20:38:04Z" id="145993127">Also, it's confirmed by running this command:

``` sh
$ gcloud compute instances list
NAME ZONE           MACHINE_TYPE  PREEMPTIBLE INTERNAL_IP EXTERNAL_IP    STATUS
cfp  europe-west1-b n1-standard-2             10.240.0.2  104.155.53.203 RUNNING
```
</comment><comment author="rmuir" created="2015-10-06T20:53:30Z" id="145997327">&gt; Is there any method which can "double-check" that the IP we get is actually non routable on internet - so it's a private one? If so, I could add a safe-guard and either stop the process or put a big WARN saying that the _gce_ address is not private.

I don't agree with a warning or any leniency like this. I don't know how we are going from binding to localhost by default, to potentially binding to a public address by default, just with a warning. I don't want to see this PR rushed through, I am very concerned about this.
</comment><comment author="rjernst" created="2015-10-06T21:00:03Z" id="145999181">@dadoonet if you bind to the internal IP for gce, can you confirm external traffic is blocked? IIRC for aws, there was routing magic that made external requests _look_ like they were going to the internal ip (but I might be confused and it was just the ip address was always the internal).
</comment><comment author="dadoonet" created="2015-10-06T21:10:04Z" id="146001905">@rjernst Thanks. So here is what I did:
- start a GCE node using `_local_` as the IP. It bounds to `10.240.0.2`.
- try to access the instance externally with the public IP: `104.155.53.203:9200` -&gt; fails
- add a firewall route to accept connections on 9200. 
- try to access the instance externally with the public IP: `104.155.53.203:9200` -&gt; works

So I guess it's "bad" as we only bound to the private IP so it should not be accessible from a public IP, right?
</comment><comment author="rjernst" created="2015-10-06T21:20:40Z" id="146005563">&gt; So I guess it's "bad" as we only bound to the private IP so it should not be accessible from a public IP, right?

That's what I would expect, yes. Sounds like routing trickery...
</comment><comment author="dadoonet" created="2015-10-06T21:29:37Z" id="146007538">I hear your @rmuir. I can only see two choices:
- We by default bound to `_site_` when we have `discovery.type: gce`. So it bounds to 127.0.0.1 by default and can be accessed only by a process running on the same physical machine. But it's useless in term of building a cluster of nodes. And it means that users will have to explicitly define `network.host: _gce_` which is I believe what they will do in 99.9% of the cases.
- We by default bound to `_local_` or `_gce_` when we have `discovery.type: gce`. So it bounds to a private IP address. Any other machine running within this private network (within this GCE project) will be able to connect to this node. 
  It's safe IMO until someone open a firewall route to this machine on 9200 port. Which is not recommended obviously.
  If someone wants really to refuse any connection within the same project, they will have to set `network.host: _site_`

My comments apply only if your PR #13954 is merged obviously.

Any other solution I could try to implement?
</comment><comment author="rmuir" created="2015-10-06T21:34:37Z" id="146008570">For GCE i do not think `_site_` (which will bind to 10.x, 192.x, 172.16.x) is an option if that cloud provider is configuring static NAT so that they are still in fact "public". 

Basically I don't think we should bind to anything publicly reachable.

I'd really rather us just be consistent and bind to `_local_` always by default, that is so simple and easy to understand and way more secure. I do not think we should be "reverting" this change in the name of ease of use. I am sorry, I just don't agree with that.
</comment><comment author="rmuir" created="2015-10-06T21:48:07Z" id="146011257">Can we separate the "add GCE resolver logic", "add better exception handling for custom resolvers", "add lots of  GCE tests" which are all in this PR, and seem like good changes, from the changing of the default, and just make a separate issue for that?
</comment><comment author="dadoonet" created="2015-10-06T21:56:41Z" id="146013440">Just a note:

&gt; and just make a separate issue for that?

The current PR is not about adding `_gce_` as default. Sorry if there was confusion.
Adding `_gce_` as default will be fixed (if we fix it) with #13969 which is another thing.

So I think we are all on the same page here.
</comment><comment author="bleskes" created="2015-10-07T11:34:16Z" id="146162641">LGTM. Thx @dadoonet 
</comment><comment author="dadoonet" created="2015-10-07T20:51:02Z" id="146324144">Cool! Rebased and tested. Just need now to apply the changes to 2.x, 2.1 and 2.0:

```
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Build Tools and Resources .......................... SUCCESS [  1.773 s]
[INFO] Rest API Specification ............................. SUCCESS [  1.412 s]
[INFO] Elasticsearch: Parent POM .......................... SUCCESS [ 13.108 s]
[INFO] Elasticsearch: Core ................................ SUCCESS [22:08 min]
[INFO] Distribution: Parent POM ........................... SUCCESS [  5.734 s]
[INFO] Distribution: TAR .................................. SUCCESS [01:21 min]
[INFO] Distribution: ZIP .................................. SUCCESS [01:19 min]
[INFO] Distribution: Deb .................................. SUCCESS [  7.956 s]
[INFO] Distribution: RPM .................................. SUCCESS [01:16 min]
[INFO] Plugin: Parent POM ................................. SUCCESS [  3.957 s]
[INFO] Plugin: Analysis: ICU .............................. SUCCESS [ 31.603 s]
[INFO] Plugin: Analysis: Japanese (kuromoji) .............. SUCCESS [ 25.595 s]
[INFO] Plugin: Analysis: Phonetic ......................... SUCCESS [ 24.531 s]
[INFO] Plugin: Analysis: Smart Chinese (smartcn) .......... SUCCESS [ 24.221 s]
[INFO] Plugin: Analysis: Polish (stempel) ................. SUCCESS [ 24.839 s]
[INFO] Plugin: Cloud: Google Compute Engine ............... SUCCESS [ 24.511 s]
[INFO] Plugin: Delete By Query ............................ SUCCESS [ 44.614 s]
[INFO] Plugin: Discovery: Azure ........................... SUCCESS [ 38.441 s]
[INFO] Plugin: Discovery: EC2 ............................. SUCCESS [ 31.232 s]
[INFO] Plugin: Discovery: Multicast ....................... SUCCESS [ 52.638 s]
[INFO] Plugin: Language: Expression ....................... SUCCESS [ 58.571 s]
[INFO] Plugin: Language: Groovy ........................... SUCCESS [05:24 min]
[INFO] Plugin: Language: JavaScript ....................... SUCCESS [ 38.987 s]
[INFO] Plugin: Language: Python ........................... SUCCESS [ 43.542 s]
[INFO] Plugin: Mapper: Murmur3 ............................ SUCCESS [ 27.527 s]
[INFO] Plugin: Mapper: Size ............................... SUCCESS [ 28.513 s]
[INFO] Plugin: Repository: Azure .......................... SUCCESS [ 33.372 s]
[INFO] Plugin: Repository: S3 ............................. SUCCESS [ 30.935 s]
[INFO] Plugin: Store: SMB ................................. SUCCESS [ 34.707 s]
[INFO] Plugin: JVM example ................................ SUCCESS [ 18.298 s]
[INFO] Plugin: Example site ............................... SUCCESS [ 15.260 s]
[INFO] QA: Parent POM ..................................... SUCCESS [  0.847 s]
[INFO] QA: Smoke Test Plugins ............................. SUCCESS [ 38.715 s]
[INFO] QA: Smoke Test Multi-Node IT ....................... SUCCESS [ 19.855 s]
[INFO] QA: Smoke Test Client .............................. SUCCESS [ 13.322 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 44:11 min
[INFO] Finished at: 2015-10-07T22:49:47+02:00
[INFO] Final Memory: 100M/696M
[INFO] ------------------------------------------------------------------------
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move rest-api-spec for plugins into test resources</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13611</link><project id="" key="" /><description>Plugin tests require having rest-api tests, and currently copy that spec
from a directory in the root of the plugin source into the test
resources. This change moves the rest-api-spec dir into test resources
so it is like any other test resources. It also removes unnecessary
configuration for resources from the shared plugin pom.
</description><key id="106741694">13611</key><summary>Move rest-api-spec for plugins into test resources</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Plugins</label><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T10:08:17Z</created><updated>2015-09-16T18:15:47Z</updated><resolved>2015-09-16T18:15:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-16T10:28:08Z" id="140699231">++
</comment><comment author="nik9000" created="2015-09-16T13:48:58Z" id="140747509">&gt; It also removes unnecessary
&gt; configuration for resources from the shared plugin pom.

\o/

LGTM. If it works please merge this. Please.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose pending cluster state queue size in node stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13610</link><project id="" key="" /><description>#13303 and #13062 introduced the notion of pending cluster states and made it easy to know how many cluster states are waiting to be processed by zen discovery. It would be great to expose this information in our node stats for monitoring. Ideally we'd have: total queue size, number of committed cluster states, and number of pending cluster states.
</description><key id="106730938">13610</key><summary>Expose pending cluster state queue size in node stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>:Stats</label><label>adoptme</label><label>low hanging fruit</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T09:06:25Z</created><updated>2015-10-28T18:03:57Z</updated><resolved>2015-10-28T18:03:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix InternalEngineTests.testTranslogReplayWithFailure to expect AssertionError as well</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13609</link><project id="" key="" /><description>This PR fixes this build failure: http://build-us-00.elastic.co/job/es_core_master_small/4162/

The seed repros, which is nice!

The issue is IndexWriter asserts on init that all index files referenced by the commit point its opening do in fact exist, but it checks that by trying to open each one and detecting FNFE/NFSE, which this test randomly throws.

I just fixed the test to allow for AssertionError on init ...
</description><key id="106725217">13609</key><summary>Fix InternalEngineTests.testTranslogReplayWithFailure to expect AssertionError as well</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T08:33:58Z</created><updated>2015-11-22T10:11:23Z</updated><resolved>2015-09-16T08:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-16T08:46:15Z" id="140674137">LGTM
</comment><comment author="s1monw" created="2015-09-16T08:50:34Z" id="140675323">can we assert that the assertion error stacktrace is from IW? ie

``` Java
catch (AssertionError e) {
  assertTrue(ExceptionHelpers.stacktrace(e).contains("org.apache.lucene.index.IndexWriter.filesExist"));
}
```
</comment><comment author="mikemccand" created="2015-09-16T09:58:22Z" id="140692768">&gt; can we assert that the assertion error stacktrace is from IW

Good idea, I pushed that change too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo query builders: Coupling of `coerce` and `ignoreMalformed`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13608</link><project id="" key="" /><description>In the GeoXQueryBuilders, the settings for `coerce' and 'ignoreMalformed' are somewhat related. We should investigate if we can ensure they should be either updated together or make an enum out of it. 
</description><key id="106723675">13608</key><summary>Geo query builders: Coupling of `coerce` and `ignoreMalformed`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T08:24:43Z</created><updated>2015-09-25T09:34:47Z</updated><resolved>2015-09-25T09:34:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-25T09:31:49Z" id="143169977">Closed by #13672
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Range filter with relative date does not use specified time zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13607</link><project id="" key="" /><description>I'm trying to query all documents containing a date which has passed (less than now). My query is:
{
"query": {
   "filtered": {
     "query": {
       "match_all": {}
     }
     "filter": {
       "range": {
         "date_field": {
            "lt": "now"
         }
       }
     }
   }
 }
}

The problem is that I'm in a UTC+3 time zone, and this query returns only documents with a date field of up to 3 hours before the current time (probably because it assumes the time in the document is in UTC).
I tried to add "time_zone" : "+3:00"
to the range query but it didn't work (in fact it didn't matter which value I entered into the time_zone - it didn't affect the result)

I need a way to make the "now" value independent of time zone or simply force it to be in the UTC time zone.
</description><key id="106723467">13607</key><summary>Range filter with relative date does not use specified time zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Lande24</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-09-16T08:23:31Z</created><updated>2015-10-07T10:38:31Z</updated><resolved>2015-09-25T11:55:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T12:42:42Z" id="141660787">This is indeed a bug, easily demonstrated as follows:

```
PUT t/t/1
{
  "date": "2015/01/01"
}

GET _validate/query?explain&amp;rewrite
{
  "query": {
    "bool": {
      "should": [
        {
          "range": {
            "date": {
              "lt": "now",
              "time_zone": "-10:00"
            }
          }
        },
        {
          "range": {
            "date": {
              "lt": "now"
            }
          }
        },
        {
          "range": {
            "date": {
              "lt": "now",
              "time_zone": "+10:00"
            }
          }
        }
      ]
    }
  }
}
```

This returns the following explanation:

```
         "date:[* TO 1442666506311} date:[* TO 1442666506311} date:[* TO 1442666506311}"
```
</comment><comment author="andrestc" created="2015-09-23T01:28:15Z" id="142468199">I'll take a shot at this one!
</comment><comment author="andrestc" created="2015-09-23T01:47:17Z" id="142470129">@clintongormley, I believe i have this fixed, but when I was writing a test for it I came across [this one](https://github.com/elastic/elasticsearch/blob/e713bf215193c48d03c164aaf1143666f30ee1ff/core/src/test/java/org/elasticsearch/common/joda/DateMathParserTests.java#L142):

``` java
// timezone does not affect now
assertDateMathEquals("now/m", "2014-11-18T14:27", now, false, DateTimeZone.forID("+02:00"));
```

This test seems specific for this case. Are we sure this is a bug?

Thanks!
</comment><comment author="clintongormley" created="2015-09-23T12:47:22Z" id="142587556">@andrestc Ah good point!  Dates should be stored as UTC, so `time_zone` shouldn't affect `now()` (which is calculated in UTC).

OK, so we should (a) document it and (b) possibly add an exception when `time_zone` is used with `now`?
</comment><comment author="Lande24" created="2015-09-23T12:50:10Z" id="142588265">In that case, how am I supposed to overcome my problem?
</comment><comment author="clintongormley" created="2015-09-23T12:52:41Z" id="142588810">@Lande24 Make sure your dates are stored in UTC, which you can do by specifying your time zone in the date itself

```
"date": "2015-01-01T13:00:00+03:00"  # indexed in UTC as "2015-01-01T10:00:00"
```

See https://twitter.com/alicemazzy/status/646219411754483712
</comment><comment author="andrestc" created="2015-09-23T13:39:16Z" id="142604053">@clintongormley great, I'll draft this and PR it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent certain index templates from being wiped in between tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13606</link><project id="" key="" /><description /><key id="106719888">13606</key><summary>Prevent certain index templates from being wiped in between tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-16T07:59:38Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-17T11:12:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-09-16T11:23:08Z" id="140711848">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `_gce_` network host setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13605</link><project id="" key="" /><description>When running in GCE platform, an instance has access to:

http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/ip

Which gives back the private IP address, for example `10.240.0.2`.

http://metadata.google.internal/computeMetadata/v1/instance/network-interfaces/0/externalIp

Gives back the public Ip address, for example `130.211.108.21`.

As we have for `ec2`, we can support new network host settings:
- `_gce:privateIp_`: maps to `ip`
- `_gce:publicIp_`: maps to `externalIp`
- `_gce_` shorter version for `_gce:privateIp_`
</description><key id="106718620">13605</key><summary>Add `_gce_` network host setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>enhancement</label></labels><created>2015-09-16T07:51:03Z</created><updated>2015-10-07T21:24:53Z</updated><resolved>2015-10-07T21:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Identical jars are not detected by jar hell</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13604</link><project id="" key="" /><description>When I try to run gradle integration tests to test a plugin by unpacking the zip, the jar hell detection is unable to recognize the same jar on the class path at two (or more) different locations.

I assume this is not intended. It would make gradle classpath setups (maybe other build tools too) to compile and test ES plugins "in-place" very difficult to specify.

My suggestion is to skip jar URLs that point to a jar with a checksum of a jar that has been already checked.

Example:

```
Caused by: java.lang.IllegalStateException: jar hell!
class: org.xbib.elasticsearch.http.HttpModule$_configure_closure1
jar1: /Users/joerg/.gradle/caches/modules-2/files-2.1/org.xbib.elasticsearch.plugin/elasticsearch-webapp/2.0.0-beta1.0/c735bc9b3e05b37a2d553ddda7d76fefc5e817e5/elasticsearch-webapp-2.0.0-beta1.0.jar
jar2: /Users/joerg/Projects/github/jprante/elasticsearch-webapp-example/plugins/webapp/elasticsearch-webapp-2.0.0-beta1.0.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:210)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:128)
    at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:346)
    ... 52 more
```

```
shasum /Users/joerg/.gradle/caches/modules-2/files-2.1/org.xbib.elasticsearch.plugin/elasticsearch-webapp/2.0.0-beta1.0/c735bc9b3e05b37a2d553ddda7d76fefc5e817e5/elasticsearch-webapp-2.0.0-beta1.0.jar
c735bc9b3e05b37a2d553ddda7d76fefc5e817e5  /Users/joerg/.gradle/caches/modules-2/files-2.1/org.xbib.elasticsearch.plugin/elasticsearch-webapp/2.0.0-beta1.0/c735bc9b3e05b37a2d553ddda7d76fefc5e817e5/elasticsearch-webapp-2.0.0-beta1.0.jar
```

```
shasum /Users/joerg/Projects/github/jprante/elasticsearch-webapp-example/plugins/webapp/elasticsearch-webapp-2.0.0-beta1.0.jar
c735bc9b3e05b37a2d553ddda7d76fefc5e817e5  /Users/joerg/Projects/github/jprante/elasticsearch-webapp-example/plugins/webapp/elasticsearch-webapp-2.0.0-beta1.0.jar
```
</description><key id="106715209">13604</key><summary>Identical jars are not detected by jar hell</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>won't fix</label></labels><created>2015-09-16T07:24:13Z</created><updated>2015-11-18T20:11:23Z</updated><resolved>2015-09-16T07:53:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-16T07:26:09Z" id="140654273">The failure is intentional. Why should we be lenient about this? it is still a form of jar hell!
</comment><comment author="jprante" created="2015-09-16T07:28:26Z" id="140654602">If the duplicate class files exist inside the same class loader, and the two duplicate class files are exactly identical then it does not matter which one gets chosen first - this situation is not dangerous.
</comment><comment author="s1monw" created="2015-09-16T07:53:05Z" id="140659232">&gt; If the duplicate class files exist inside the same class loader, and the two duplicate class files are exactly identical then it does not matter which one gets chosen first - this situation is not dangerous.

having the same file twice is a problem we can't work around this and add leniency on that level - we are just enforcing things to be setup properly.
</comment><comment author="vchekan" created="2015-11-18T20:00:01Z" id="157844840">@s1monw correct me if I am wrong, but jar hell conflict can happen not just between a plugin and elasticsearch, but between 2 plugins.

As a plugin author, I can not "setup things properly" because this would require me to know target server's list of plugins at my plugin compile time. In other words, it is impossible for a plugin author to make their plugin compatible with all existing and all future plugins.

To make it even worse, if I exclude a conflicting jar from my plugin, it will break if the other plugin gets uninstalled, because now my plugin could not find needed jar. What should I do as a plugin author?
</comment><comment author="nik9000" created="2015-11-18T20:02:00Z" id="157845338">&gt; As a plugin author, I can not "setup things properly" because this would require me to know target server's list of plugins at my plugin compile time. In other words, it is impossible for a plugin author to make their plugin compatible with all existing and all future plugins.

I believe plugins run in isolated classloaders so two plugins can define the same class without this being an issue.
</comment><comment author="vchekan" created="2015-11-18T20:11:23Z" id="157847664">@nik9000 hmm, I think I understood. I am actually doing bulk insert, and pointing to ES install with some plugins already installed. Perhaps, jar isolation works differently when bulk node is registered. I will solve it by pointing bulk node at "clean" ES install, without any plugins.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove java.lang.reflect.ReflectPermission "suppressAccessChecks"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13603</link><project id="" key="" /><description>This is an extremely dangerous permission, basically as dangerous as Unsafe. In addition to this, by removing this permission we solve the main hurdle for the ability to protect confidential data, e.g. `private` fields are really private and so on. It is the most important step we can make to improve the security of the system right now.

Unfortunately its a doozie to get rid of. This pr includes/builds on:
- cleanups of randomized runner and junit4 (a snapshot for now, while dawid is on vacation)
- cleanups to lucene-test-framework
- improvements to securemock to support this: spy/interception support in particular

All of ES is clean but there were some problems with cloud plugins (AWS and GCE). We should fix the code of these third party libraries so we can help protect credentials to their systems!

In order to make them work, I added an "insecure plugin" mechanism where we (list is managed in es-core) can apply additional permissions as workaround to this kind of thing. It is not an open invitation to bring shitty code into our codebase that needs tons of permissions, but a mechanism we can use for problems like this.

As a followup (this PR is crazy enough), I would like to:
- fix AWS and GCE and remove their hacks (we can keep the mechanism)
- cleanup a bunch of other easier permissions we probably no longer need (due to lucene and testrunner cleanups)
- think about warning or confirmation in pluginmanager when you install such a plugin.
</description><key id="106714425">13603</key><summary>Remove java.lang.reflect.ReflectPermission "suppressAccessChecks"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-16T07:18:28Z</created><updated>2015-10-02T14:21:11Z</updated><resolved>2015-09-16T08:08:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-16T07:28:42Z" id="140654644">LGTM can you put a note / reference to the RandomizedRunner issue that explains why we reference a snapshot I think it's non obvious for others 
</comment><comment author="rmuir" created="2015-09-30T06:23:37Z" id="144293971">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>network.host must be set for discovery-ec2 plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13602</link><project id="" key="" /><description> With 2.0, we now bind to `localhost` by default instead of binding to the network card and use its IP address.

 When the discovery plugin gets from AWS API the list of nodes that should form the cluster, this list is pinged then. But as each node is bound to `localhost`, ping does not get an answer and the node elects itself as the master node.

`network.host` must be set.

 Closes #13589.
</description><key id="106710028">13602</key><summary>network.host must be set for discovery-ec2 plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>docs</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-16T06:40:12Z</created><updated>2016-03-10T18:15:34Z</updated><resolved>2015-10-06T09:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-16T06:40:53Z" id="140645347">@clintongormley Could you review this please?
</comment><comment author="bleskes" created="2015-10-06T08:46:50Z" id="145787013">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get wrong node ip address through host:9200/_cat/nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13601</link><project id="" key="" /><description>I just set up the elasticsearch with one node, and it's ip address is 10.1.5.72
But the ip address returned from  curl 10.1.5.72:9200/_cat/nodes?v is 10.1.5.110.
Here is the result

```
[root@sh-elk-svr01 elasticsearch]# curl 10.1.5.72:9200/_cat/nodes?v
host         ip         heap.percent ram.percent load node.role master name       
sh-elk-svr01 10.1.5.110           10          28 0.00 d         *      Sea Urchin
```

And I noticed that the information in elasticsearch.log was correct

```
[2015-09-16 13:59:24,376][INFO ][http] [Sea Urchin] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.1.5.72:9200]}
```
</description><key id="106706958">13601</key><summary>Get wrong node ip address through host:9200/_cat/nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tuxknight</reporter><labels><label>:CAT API</label><label>feedback_needed</label></labels><created>2015-09-16T06:16:44Z</created><updated>2015-09-23T12:52:52Z</updated><resolved>2015-09-23T12:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T12:32:47Z" id="141660447">Hi @tuxknight 

I believe this is fixed in 2.0, although I can't find the PR.  Could you try out 2.0.0-beta2 and confirm please?
</comment><comment author="tuxknight" created="2015-09-23T05:54:29Z" id="142502439">@clintongormley  Sorry ! Found the reason.
wrong IP address was set in /etc/hosts .

```
$ cat /etc/hosts
10.1.5.110   sh-elk-svr01
```
</comment><comment author="clintongormley" created="2015-09-23T12:52:52Z" id="142588868">thanks @tuxknight 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Server automatically shutdown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13600</link><project id="" key="" /><description>I have three elastic servers all masters having total 32 GB ram each,16 GB allocated to elastic servers.
From last few days any one of three servers randomly automatically goes down and the java.exe is killed automatically.Even no error is logged in elastic search logs.

Am using virtual environment with network file system to store data and logs
Elastic search version is 1.7.0
Indices count:around 300
Total documents in whole elastic search : around 60 million

Below is my config

cluster.name: Cluster1

cluster.routing.allocation.disk.threshold_enabled: false
script.disable_dynamic: false
node.name: "Master1"

node.master: true
node.data: true

index.query.bool.max_clause_count: 50100
indices.fielddata.cache.size: 25%
indices.fielddata.cache.expire: 5m
action.disable_delete_all_indices: true
indices.cluster.send_refresh_mapping: false
index.cache.field.type: soft

path.data: \nas5\Elasticsearch\Data

path.logs: \nas5\Elasticsearch\Logs\Master1

bootstrap.mlockall: true

http.max_content_length: 999mb

indices.recovery.max_bytes_per_sec: 100mb

indices.recovery.concurrent_streams: 5

above config goes same for all three servers Master1,Master2,Master3.
</description><key id="106703707">13600</key><summary>Elastic Server automatically shutdown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dobariya</reporter><labels><label>feedback_needed</label></labels><created>2015-09-16T05:57:50Z</created><updated>2015-10-13T07:32:53Z</updated><resolved>2015-10-13T07:32:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dbblackdiamond" created="2015-09-17T03:23:32Z" id="140958900">Hi,

I am having the same problem. I have a 4 nodes cluster and node seems to randomly crash. The nodes are running on Ubuntu 15.04, kernel version 3.19.0.28. This is a small installation, so each node is configured with 512MB of RAM. I have tried increasing it and it didn't make any difference.
There are no messages in the /var/log/elasticsearch/clustername.log file. 
Below is the content of the /var/log/apport.log file:

```
ERROR: apport (pid 13340) Thu Sep 17 02:25:53 2015: called for pid 28872, signal 6, core limit 0
ERROR: apport (pid 13340) Thu Sep 17 02:25:53 2015: executable: /usr/lib/jvm/java-7-oracle/jre/bin/java (command line "/usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-1.7.1.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.pidfile=/var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch")
ERROR: apport (pid 13340) Thu Sep 17 02:25:53 2015: is_closing_session(): no DBUS_SESSION_BUS_ADDRESS in environment
ERROR: apport (pid 13340) Thu Sep 17 02:25:57 2015: core dump exceeded 49 MiB, dropped from /var/crash/_usr_lib_jvm_java-7-oracle_jre_bin_java.112.crash to avoid memory overflow
ERROR: apport (pid 13340) Thu Sep 17 02:25:57 2015: wrote report /var/crash/_usr_lib_jvm_java-7-oracle_jre_bin_java.112.crash
```

Below is the content of the /var/crash/_usr_lib_jvm_java-7-oracle_jre_bin_java.112.crash:

```
ProblemType: Crash
Architecture: amd64
CrashCounter: 1
Date: Thu Sep 17 02:25:53 2015
DistroRelease: Ubuntu 15.04
ExecutablePath: /usr/lib/jvm/java-7-oracle/jre/bin/java
ExecutableTimestamp: 1442115528
ProcCmdline: /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOut
OfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-1.7.1.jar:/usr/sh
are/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.pidfile=/var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elast
icsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch
ProcCwd: /
ProcEnviron:
 PATH=(custom, no user)
 LANG=en_US.UTF-8
 SHELL=/bin/false
ProcMaps:
 00400000-00401000 r-xp 00000000 08:01 258460                             /usr/lib/jvm/java-7-oracle/jre/bin/java
 00600000-00601000 rw-p 00000000 08:01 258460                             /usr/lib/jvm/java-7-oracle/jre/bin/java
 02232000-0309f000 rw-p 00000000 00:00 0                                  [heap]
 bae00000-cae00000 rw-p 00000000 00:00 0 
 cae00000-fae00000 rw-p 00000000 00:00 0 
 fae00000-ff0ec000 rw-p 00000000 00:00 0 
 ff0ec000-100000000 rw-p 00000000 00:00 0 
 7f3616b70000-7f3617867000 r--s 00000000 08:01 492968                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/3/index/_clw_Lucene41_0.tim
 7f3618258000-7f3618c48000 r--s 00000000 08:01 492605                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_i8x_Lucene41_0.tim
 7f3618c48000-7f3619a4a000 r--s 00000000 08:01 492645                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/0/index/_ipe_Lucene41_0.tim
 7f361a818000-7f361b331000 r--s 00000000 08:01 492865                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_6c5_Lucene41_0.tim
 7f361c707000-7f361c70a000 ---p 00000000 00:00 0 
 7f361c70a000-7f361c808000 rw-p 00000000 00:00 0 
 7f361c8d5000-7f361c8d8000 ---p 00000000 00:00 0 
 7f361c8d8000-7f361c9d6000 rw-p 00000000 00:00 0                          [stack:11126]
 7f361c9d6000-7f361c9d9000 ---p 00000000 00:00 0 
 7f361c9d9000-7f361cad7000 rw-p 00000000 00:00 0                          [stack:28934]
 7f361cad7000-7f361cada000 ---p 00000000 00:00 0 
 7f361cada000-7f361cbd8000 rw-p 00000000 00:00 0                          [stack:28933]
 7f361dd8c000-7f361dd8f000 ---p 00000000 00:00 0 
 7f361dd8f000-7f361de8d000 rw-p 00000000 00:00 0                          [stack:28935]
 7f361de8d000-7f361de90000 ---p 00000000 00:00 0 
 7f361de90000-7f361df8e000 rw-p 00000000 00:00 0                          [stack:28931]
 7f361df8e000-7f361df91000 ---p 00000000 00:00 0 
 7f361df91000-7f361e08f000 rw-p 00000000 00:00 0                          [stack:28930]
 7f361e08f000-7f361e092000 ---p 00000000 00:00 0 
 7f361e092000-7f361e190000 rw-p 00000000 00:00 0                          [stack:10651]
 7f361e760000-7f361eaef000 r--s 00000000 08:01 492585                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_1j0_Lucene41_0.tim
 7f361ee7c000-7f361ee7f000 ---p 00000000 00:00 0 
 7f361ee7f000-7f361ef7d000 rw-p 00000000 00:00 0                          [stack:28928]
 7f361ef7d000-7f361ef80000 ---p 00000000 00:00 0 
 7f361ef80000-7f361f07e000 rw-p 00000000 00:00 0                          [stack:13312]
 7f361f07e000-7f361f081000 ---p 00000000 00:00 0 
 7f361f081000-7f361f17f000 rw-p 00000000 00:00 0                          [stack:28926]
 7f361f17f000-7f361f182000 ---p 00000000 00:00 0 
 7f361f182000-7f361f280000 rw-p 00000000 00:00 0                          [stack:28925]
 7f361f458000-7f361f6a1000 r--s 00000000 08:01 492617                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/3/index/_pkb_Lucene41_0.tim
 7f361f6a7000-7f361f6aa000 ---p 00000000 00:00 0 
 7f361f6aa000-7f361f7a8000 rw-p 00000000 00:00 0 
 7f361f7a8000-7f361fffd000 r--s 00000000 08:01 492672                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_ayb_Lucene41_0.tim
 7f3620000000-7f362015b000 rw-p 00000000 00:00 0 
 7f362015b000-7f3624000000 ---p 00000000 00:00 0 
 7f3624000000-7f3625802000 rw-p 00000000 00:00 0 
 7f3625802000-7f3628000000 ---p 00000000 00:00 0 
 7f3628000000-7f3628036000 rw-p 00000000 00:00 0 
 7f3628036000-7f362c000000 ---p 00000000 00:00 0 
 7f362c000000-7f362c128000 rw-p 00000000 00:00 0 
 7f362c128000-7f3630000000 ---p 00000000 00:00 0 
 7f3630000000-7f363004d000 rw-p 00000000 00:00 0 
 7f363004d000-7f3634000000 ---p 00000000 00:00 0 
 7f3634000000-7f363426c000 rw-p 00000000 00:00 0 
 7f363426c000-7f3638000000 ---p 00000000 00:00 0 
 7f3638008000-7f3638009000 r--s 00000000 08:01 492793                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_6c5_Lucene410_0.dvd
 7f36380bf000-7f36380c3000 r--s 000f7000 08:01 258384                     /usr/lib/jvm/java-7-oracle/jre/lib/ext/localedata.jar
 7f3638157000-7f363815a000 ---p 00000000 00:00 0 
 7f363815a000-7f3638258000 rw-p 00000000 00:00 0                          [stack:28924]
 7f3638278000-7f3638313000 r--s 00000000 08:01 492969                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/3/index/_clw_Lucene410_0.dvd
 7f3638314000-7f3638317000 ---p 00000000 00:00 0 
 7f3638317000-7f3638415000 rw-p 00000000 00:00 0 
 7f3638415000-7f3638418000 ---p 00000000 00:00 0 
 7f3638418000-7f3638516000 rw-p 00000000 00:00 0 
 7f3638516000-7f3638519000 ---p 00000000 00:00 0 
 7f3638519000-7f3638617000 rw-p 00000000 00:00 0 
 7f3638617000-7f363861a000 ---p 00000000 00:00 0 
 7f363861a000-7f3638718000 rw-p 00000000 00:00 0 
 7f3638730000-7f3638731000 r--s 00000000 08:01 492629                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_1j0_Lucene410_0.dvd
 7f3638738000-7f363874e000 r--s 00000000 08:01 492619                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/3/index/_pkb_Lucene410_0.dvd
 7f3638750000-7f36387be000 r--s 00000000 08:01 257053                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.13/0/index/_by_Lucene41_0.tim
 7f36387c4000-7f36387c7000 ---p 00000000 00:00 0 
 7f36387c7000-7f36388c5000 rw-p 00000000 00:00 0 
 7f36388c5000-7f36388c8000 ---p 00000000 00:00 0 
 7f36388c8000-7f36389c6000 rw-p 00000000 00:00 0 
 7f36389c6000-7f36389c9000 ---p 00000000 00:00 0 
 7f36389c9000-7f3638ac7000 rw-p 00000000 00:00 0 
 7f3638ac7000-7f3638aca000 ---p 00000000 00:00 0 
 7f3638aca000-7f3638bc8000 rw-p 00000000 00:00 0 
 7f3638bc8000-7f3638ef7000 r--s 00000000 08:01 492600                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_foo_Lucene41_0.tim
 7f3638ef8000-7f3638fa0000 r--s 00000000 08:01 492657                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/0/index/_ipe_Lucene410_0.dvd
 7f3638fa8000-7f3638fa9000 r--s 00000000 08:01 257050                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.13/0/index/_by_Lucene410_0.dvd
 7f3638fb0000-7f3638fb1000 r--s 00000000 08:01 492675                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_ayb_Lucene410_0.dvd
 7f3638fb8000-7f3638fb9000 r--s 00000000 08:01 492608                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_i8x_Lucene410_0.dvd
 7f3638fc0000-7f3638fc1000 r--s 00000000 08:01 492602                     /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_foo_Lucene410_0.dvd
 7f3638fc6000-7f3638fc9000 ---p 00000000 00:00 0 
 7f3638fc9000-7f36390c7000 rw-p 00000000 00:00 0                          [stack:28923]
 7f36390c7000-7f36390ca000 ---p 00000000 00:00 0 
 7f36390ca000-7f36391c8000 rw-p 00000000 00:00 0                          [stack:28939]
 7f36391c8000-7f36391cb000 ---p 00000000 00:00 0 
 7f36391cb000-7f36392c9000 rw-p 00000000 00:00 0 
 7f36392c9000-7f36392cc000 ---p 00000000 00:00 0 
 7f36392cc000-7f36393ca000 rw-p 00000000 00:00 0 
 7f36393ca000-7f36393cd000 ---p 00000000 00:00 0 
 7f36393cd000-7f36394cb000 rw-p 00000000 00:00 0                          [stack:28940]
 7f36394cb000-7f36394ce000 ---p 00000000 00:00 0 
 7f36394ce000-7f36395cc000 rw-p 00000000 00:00 0                          [stack:28915]
 7f36395cc000-7f36395cf000 ---p 00000000 00:00 0 
 7f36395cf000-7f36396cd000 rw-p 00000000 00:00 0                          [stack:28914]
 7f36396cd000-7f36396d0000 ---p 00000000 00:00 0 
 7f36396d0000-7f36397ce000 rw-p 00000000 00:00 0                          [stack:28913]
 7f36397ce000-7f36397d1000 ---p 00000000 00:00 0 
 7f36397d1000-7f36398cf000 rw-p 00000000 00:00 0                          [stack:28912]
 7f36398cf000-7f36398d2000 ---p 00000000 00:00 0 
 7f36398d2000-7f36399d0000 rw-p 00000000 00:00 0                          [stack:28911]
 7f36399d0000-7f36399d3000 ---p 00000000 00:00 0 
 7f36399d3000-7f3639ad1000 rw-p 00000000 00:00 0 
 7f3639ad1000-7f3639ad4000 ---p 00000000 00:00 0 
 7f3639ad4000-7f3639bd2000 rw-p 00000000 00:00 0                          [stack:28907]
 7f3639bd2000-7f3639bd5000 ---p 00000000 00:00 0 
 7f3639bd5000-7f3639cd3000 rw-p 00000000 00:00 0 
 7f3639cd3000-7f3639cd6000 ---p 00000000 00:00 0 
 7f3639cd6000-7f3639dd4000 rw-p 00000000 00:00 0                          [stack:29136]
 7f3639dd4000-7f3639dd7000 ---p 00000000 00:00 0 
 7f3639dd7000-7f3639ed5000 rw-p 00000000 00:00 0                          [stack:28904]
 7f3639ed5000-7f3639ed8000 ---p 00000000 00:00 0 
 7f3639ed8000-7f3639fd6000 rw-p 00000000 00:00 0                          [stack:28903]
 7f3639fd6000-7f3639fd9000 ---p 00000000 00:00 0 
 7f3639fd9000-7f363a0d7000 rw-p 00000000 00:00 0                          [stack:28902]
 7f363a0d7000-7f363a0da000 ---p 00000000 00:00 0 
 7f363a0da000-7f363a1d8000 rw-p 00000000 00:00 0                          [stack:28901]
 7f363a1d8000-7f363a1db000 ---p 00000000 00:00 0 
 7f363a1db000-7f363a2d9000 rw-p 00000000 00:00 0                          [stack:28900]
 7f363a2d9000-7f363a2dc000 ---p 00000000 00:00 0 
 7f363a2dc000-7f363a3da000 rw-p 00000000 00:00 0                          [stack:28899]
 7f363a3da000-7f363a3dd000 ---p 00000000 00:00 0 
 7f363a3dd000-7f363a4db000 rw-p 00000000 00:00 0                          [stack:28898]
 7f363a4db000-7f363a4de000 ---p 00000000 00:00 0 
 7f363a4de000-7f363a5dc000 rw-p 00000000 00:00 0                          [stack:28897]
 7f363a5dc000-7f363a5df000 ---p 00000000 00:00 0 
 7f363a5df000-7f363a6dd000 rw-p 00000000 00:00 0                          [stack:28896]
 7f363a6dd000-7f363a6e0000 ---p 00000000 00:00 0 
 7f363a6e0000-7f363a7de000 rw-p 00000000 00:00 0                          [stack:28895]
 7f363a7de000-7f363a7e1000 ---p 00000000 00:00 0 
 7f363a7e1000-7f363a8df000 rw-p 00000000 00:00 0                          [stack:28894]
 7f363a8df000-7f363a8e2000 ---p 00000000 00:00 0 
 7f363a8e2000-7f363a9e0000 rw-p 00000000 00:00 0                          [stack:28893]
 7f363a9e0000-7f363aa0f000 r-xp 00000000 08:01 255559                     /usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
 7f363aa0f000-7f363ab0e000 ---p 0002f000 08:01 255559                     /usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
 7f363ab0e000-7f363ab14000 rw-p 0002e000 08:01 255559                     /usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so
 7f363ab14000-7f363ab17000 rw-p 00000000 00:00 0 
 7f363ab17000-7f363ab1a000 ---p 00000000 00:00 0 
 7f363ab1a000-7f363ac18000 rw-p 00000000 00:00 0                          [stack:28891]
 7f363ac18000-7f363ac1a000 r--s 00010000 08:01 255931                     /usr/share/elasticsearch/plugins/marvel/marvel-1.3.1.jar
 7f363ac20000-7f363ac30000 r-xp 00000000 08:01 257747                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnio.so
 7f363ac30000-7f363ae30000 ---p 00010000 08:01 257747                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnio.so
 7f363ae30000-7f363ae31000 rw-p 00010000 08:01 257747                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnio.so
 7f363ae33000-7f363ae37000 r--s 0008b000 08:01 258373                     /usr/lib/jvm/java-7-oracle/jre/lib/jsse.jar
 7f363ae3a000-7f363ae41000 r--s 00062000 08:01 255562                     /usr/share/elasticsearch/lib/sigar/sigar-1.6.4.jar
 7f363ae47000-7f363ae4d000 r--s 0002f000 08:01 255579                     /usr/share/elasticsearch/lib/lucene-queries-4.10.4.jar
 7f363ae52000-7f363ae63000 r--s 000b2000 08:01 255569                     /usr/share/elasticsearch/lib/jts-1.13.jar
 7f363ae68000-7f363ae69000 r--s 00008000 08:01 255577                     /usr/share/elasticsearch/lib/lucene-memory-4.10.4.jar
 7f363ae6f000-7f363ae78000 r--s 00057000 08:01 255580                     /usr/share/elasticsearch/lib/lucene-queryparser-4.10.4.jar
 7f363ae7e000-7f363ae81000 r--s 00026000 08:01 255563                     /usr/share/elasticsearch/lib/antlr-runtime-3.5.jar
 7f363ae88000-7f363ae8a000 r--s 00008000 08:01 255566                     /usr/share/elasticsearch/lib/asm-commons-4.1.jar
 7f363ae90000-7f363aeba000 r--s 00248000 08:01 255572                     /usr/share/elasticsearch/lib/lucene-core-4.10.4.jar
 7f363aebe000-7f363af2d000 r--s 0063e000 08:01 255567                     /usr/share/elasticsearch/lib/groovy-all-2.4.4.jar
 7f363af30000-7f363b230000 rw-p 00000000 00:00 0 
 7f363b230000-7f363b247000 r-xp 00000000 08:01 257781                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnet.so
 7f363b247000-7f363b446000 ---p 00017000 08:01 257781                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnet.so
 7f363b446000-7f363b447000 rw-p 00016000 08:01 257781                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnet.so
 7f363b448000-7f363b450000 r-xp 00000000 08:01 257755                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libmanagement.so
 7f363b450000-7f363b64f000 ---p 00008000 08:01 257755                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libmanagement.so
 7f363b64f000-7f363b650000 rw-p 00007000 08:01 257755                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libmanagement.so
 7f363b654000-7f363b766000 r--s 00c54000 08:01 255557                     /usr/share/elasticsearch/lib/elasticsearch-1.7.1.jar
 7f363b76a000-7f363b76b000 ---p 00000000 00:00 0 
 7f363b76b000-7f363b86b000 rw-p 00000000 00:00 0                          [stack:28890]
 7f363b86b000-7f363b86e000 ---p 00000000 00:00 0 
 7f363b86e000-7f363b96c000 rw-p 00000000 00:00 0                          [stack:28889]
 7f363b96c000-7f363b96f000 ---p 00000000 00:00 0 
 7f363b96f000-7f363ba6d000 rw-p 00000000 00:00 0                          [stack:28888]
 7f363ba6d000-7f363ba70000 ---p 00000000 00:00 0 
 7f363ba70000-7f363bb6e000 rw-p 00000000 00:00 0                          [stack:28887]
 7f363bb6e000-7f363bb71000 ---p 00000000 00:00 0 
 7f363bb71000-7f363bc6f000 rw-p 00000000 00:00 0                          [stack:28886]
 7f363bc6f000-7f363bc72000 ---p 00000000 00:00 0 
 7f363bc72000-7f363bd70000 rw-p 00000000 00:00 0                          [stack:28885]
 7f363bd70000-7f363bef9000 r--p 00000000 08:01 54985                      /usr/lib/locale/locale-archive
 7f363beff000-7f363bf02000 ---p 00000000 00:00 0 
 7f363bf02000-7f363c000000 rw-p 00000000 00:00 0                          [stack:28884]
 7f363c000000-7f363c29e000 rw-p 00000000 00:00 0 
 7f363c29e000-7f3640000000 ---p 00000000 00:00 0 
 7f3640003000-7f3640006000 r--s 0001b000 08:01 255581                     /usr/share/elasticsearch/lib/lucene-sandbox-4.10.4.jar
 7f364000c000-7f3640010000 r--s 000dc000 08:01 255568                     /usr/share/elasticsearch/lib/jna-4.1.0.jar
 7f3640017000-7f364001a000 r--s 0001f000 08:01 255575                     /usr/share/elasticsearch/lib/lucene-highlighter-4.10.4.jar
 7f364001c000-7f364001f000 r--s 0001c000 08:01 255582                     /usr/share/elasticsearch/lib/lucene-spatial-4.10.4.jar
 7f3640026000-7f364004e000 rw-p 00000000 00:00 0 
 7f364004e000-7f3640051000 ---p 00000000 00:00 0 
 7f3640051000-7f364014f000 rw-p 00000000 00:00 0                          [stack:28883]
 7f364014f000-7f3640150000 ---p 00000000 00:00 0 
 7f3640150000-7f364029b000 rw-p 00000000 00:00 0                          [stack:28882]
 7f364029b000-7f364045b000 r--s 039fb000 08:01 258429                     /usr/lib/jvm/java-7-oracle/jre/lib/rt.jar
 7f3640462000-7f3643dae000 rw-p 00000000 00:00 0 
 7f3643dae000-7f3643f2e000 rw-p 00000000 00:00 0 
 7f3643f2e000-7f3643fae000 rw-p 00000000 00:00 0 
 7f3643fae000-7f364412e000 rw-p 00000000 00:00 0 
 7f364412e000-7f3644150000 rw-p 00000000 00:00 0 
 7f3644150000-7f3644157000 rw-p 00000000 00:00 0 
 7f3644157000-7f3644158000 rw-p 00000000 00:00 0 
 7f3644158000-7f36449c8000 rwxp 00000000 00:00 0 
 7f36449c8000-7f3647158000 rw-p 00000000 00:00 0 
 7f3647158000-7f364716f000 r-xp 00000000 08:01 257776                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libzip.so
 7f364716f000-7f364736f000 ---p 00017000 08:01 257776                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libzip.so
 7f364736f000-7f3647370000 rw-p 00017000 08:01 257776                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libzip.so
 7f3647370000-7f364737c000 r-xp 00000000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7f364737c000-7f364757b000 ---p 0000c000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7f364757b000-7f364757c000 r--p 0000b000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7f364757c000-7f364757d000 rw-p 0000c000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7f3647580000-7f364758b000 r-xp 00000000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7f364758b000-7f364778a000 ---p 0000b000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7f364778a000-7f364778b000 r--p 0000a000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7f364778b000-7f364778c000 rw-p 0000b000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7f3647790000-7f36477a7000 r-xp 00000000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7f36477a7000-7f36479a6000 ---p 00017000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7f36479a6000-7f36479a7000 r--p 00016000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7f36479a7000-7f36479a8000 rw-p 00017000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7f36479a8000-7f36479aa000 rw-p 00000000 00:00 0 
 7f36479b0000-7f36479b8000 r-xp 00000000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
 7f36479b8000-7f3647bb7000 ---p 00008000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
 7f3647bb7000-7f3647bb8000 r--p 00007000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
 7f3647bb8000-7f3647bb9000 rw-p 00008000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
 7f3647bc0000-7f3647be9000 r-xp 00000000 08:01 257742                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libjava.so
 7f3647be9000-7f3647de9000 ---p 00029000 08:01 257742                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libjava.so
 7f3647de9000-7f3647deb000 rw-p 00029000 08:01 257742                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libjava.so
 7f3647df0000-7f3647dfd000 r-xp 00000000 08:01 257769                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libverify.so
 7f3647dfd000-7f3647ffc000 ---p 0000d000 08:01 257769                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libverify.so
 7f3647ffc000-7f3647ffe000 rw-p 0000c000 08:01 257769                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libverify.so
 7f3648000000-7f3648cdc000 rw-p 00000000 00:00 0 
 7f3648cdc000-7f364c000000 ---p 00000000 00:00 0 
 7f364c007000-7f364c009000 r--s 00017000 08:01 255584                     /usr/share/elasticsearch/lib/spatial4j-0.4.1.jar
 7f364c009000-7f364c00b000 r--s 00011000 08:01 255573                     /usr/share/elasticsearch/lib/lucene-expressions-4.10.4.jar
 7f364c00b000-7f364c00c000 r--s 0000b000 08:01 255565                     /usr/share/elasticsearch/lib/asm-4.1.jar
 7f364c00e000-7f364c016000 r--s 00066000 08:01 255564                     /usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
 7f364c016000-7f364c018000 r--s 00016000 08:01 255578                     /usr/share/elasticsearch/lib/lucene-misc-4.10.4.jar
 7f364c019000-7f364c01c000 r--s 00029000 08:01 255583                     /usr/share/elasticsearch/lib/lucene-suggest-4.10.4.jar
 7f364c01e000-7f364c020000 r--s 0000e000 08:01 255576                     /usr/share/elasticsearch/lib/lucene-join-4.10.4.jar
 7f364c020000-7f364c023000 r--s 00018000 08:01 255574                     /usr/share/elasticsearch/lib/lucene-grouping-4.10.4.jar
 7f364c025000-7f364c026000 ---p 00000000 00:00 0 
 7f364c026000-7f364c188000 rw-p 00000000 00:00 0                          [stack:28881]
 7f364c188000-7f364c190000 rw-p 00000000 00:00 0 
 7f364c190000-7f364c197000 r-xp 00000000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7f364c197000-7f364c396000 ---p 00007000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7f364c396000-7f364c397000 r--p 00006000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7f364c397000-7f364c398000 rw-p 00007000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7f364c398000-7f364c49f000 r-xp 00000000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7f364c49f000-7f364c69e000 ---p 00107000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7f364c69e000-7f364c69f000 r--p 00106000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7f364c69f000-7f364c6a0000 rw-p 00107000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7f364c6a0000-7f364d216000 r-xp 00000000 08:01 257796                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
 7f364d216000-7f364d415000 ---p 00b76000 08:01 257796                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
 7f364d415000-7f364d4d9000 rw-p 00b75000 08:01 257796                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
 7f364d4d9000-7f364d51a000 rw-p 00000000 00:00 0 
 7f364d520000-7f364d6e0000 r-xp 00000000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7f364d6e0000-7f364d8e0000 ---p 001c0000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7f364d8e0000-7f364d8e4000 r--p 001c0000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7f364d8e4000-7f364d8e6000 rw-p 001c4000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7f364d8e6000-7f364d8ea000 rw-p 00000000 00:00 0 
 7f364d8f0000-7f364d8f3000 r-xp 00000000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7f364d8f3000-7f364daf2000 ---p 00003000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7f364daf2000-7f364daf3000 r--p 00002000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7f364daf3000-7f364daf4000 rw-p 00003000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7f364daf8000-7f364db0d000 r-xp 00000000 08:01 257788                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/jli/libjli.so
 7f364db0d000-7f364dd0c000 ---p 00015000 08:01 257788                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/jli/libjli.so
 7f364dd0c000-7f364dd0d000 rw-p 00014000 08:01 257788                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/jli/libjli.so
 7f364dd10000-7f364dd28000 r-xp 00000000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7f364dd28000-7f364df28000 ---p 00018000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7f364df28000-7f364df29000 r--p 00018000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7f364df29000-7f364df2a000 rw-p 00019000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7f364df2a000-7f364df2e000 rw-p 00000000 00:00 0 
 7f364df30000-7f364df54000 r-xp 00000000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
 7f364df57000-7f364df60000 r--s 0006f000 08:01 255570                     /usr/share/elasticsearch/lib/log4j-1.2.17.jar
 7f364df67000-7f364df78000 r--s 0018f000 08:01 255571                     /usr/share/elasticsearch/lib/lucene-analyzers-common-4.10.4.jar
 7f364df78000-7f364df9a000 rw-p 00000000 00:00 0 
 7f364df9a000-7f364e038000 rw-p 00000000 00:00 0 
 7f364e038000-7f364e040000 rw-s 00000000 08:01 255598                     /tmp/hsperfdata_elasticsearch/28872 (deleted)
 7f364e046000-7f364e049000 ---p 00000000 00:00 0 
 7f364e049000-7f364e148000 rw-p 00000000 00:00 0                          [stack:28880]
 7f364e14c000-7f364e14e000 rw-p 00000000 00:00 0 
 7f364e14e000-7f364e14f000 r--p 00000000 00:00 0 
 7f364e14f000-7f364e153000 rw-p 00000000 00:00 0 
 7f364e153000-7f364e154000 r--p 00023000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
 7f364e154000-7f364e155000 rw-p 00024000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
 7f364e155000-7f364e157000 rw-p 00000000 00:00 0 
 7fff042c1000-7fff042e2000 rw-p 00000000 00:00 0                          [stack]
 7fff04358000-7fff0435a000 r--p 00000000 00:00 0                          [vvar]
 7fff0435a000-7fff0435c000 r-xp 00000000 00:00 0                          [vdso]
 ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
ProcStatus:
 Name:  java
 State: D (disk sleep)
 Tgid:  28872
 Ngid:  0
 Pid:   28872
 PPid:  1
 TracerPid: 0
 Uid:   112 112 112 112
 Gid:   120 120 120 120
 FDSize:    512
 Groups:    120 
 VmPeak:     2038676 kB
 VmSize:     1977020 kB
 VmLck:        0 kB
 VmPin:        0 kB
 VmHWM:   351900 kB
 VmRSS:   322512 kB
 VmData:     1849952 kB
 VmStk:      136 kB
 VmExe:        4 kB
 VmLib:    15844 kB
 VmPTE:     1112 kB
 VmSwap:       42280 kB
 Threads:   47
 SigQ:  0/1924
 SigPnd:    0000000000000000
 ShdPnd:    0000000000000000
 SigBlk:    0000000000000000
 SigIgn:    0000000000000000
 SigCgt:    2000000181005ccf
 CapInh:    0000000000000000
 CapPrm:    0000000000000000
 CapEff:    0000000000000000
 CapBnd:    0000003fffffffff
 Seccomp:   0
 Cpus_allowed:  1
 Cpus_allowed_list: 0
 Mems_allowed:  00000000,00000001
 Mems_allowed_list: 0
 voluntary_ctxt_switches:   31
 nonvoluntary_ctxt_switches:    1
Signal: 6
Uname: Linux 3.19.0-28-generic x86_64
UserGroups: 
```

Below is the content of my /etc/elasticsearch/elasticsearch.yml:

```
##################### Elasticsearch Configuration Example #####################

# This file contains an overview of various configuration settings,
# targeted at operations staff. Application developers should
# consult the guide at &lt;http://elasticsearch.org/guide&gt;.
#
# The installation procedure is covered at
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html&gt;.
#
# Elasticsearch comes with reasonable defaults for most settings,
# so you can try it out without bothering with configuration.
#
# Most of the time, these defaults are just fine for running a production
# cluster. If you're fine-tuning your cluster, or wondering about the
# effect of certain configuration option, please _do ask_ on the
# mailing list or IRC channel [http://elasticsearch.org/community].

# Any element in the configuration can be replaced with environment variables
# by placing them in ${...} notation. For example:
#
#node.rack: ${RACK_ENV_VAR}

# For information on supported formats and syntax for the config file, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html&gt;


################################### Cluster ###################################

# Cluster name identifies your cluster for auto-discovery. If you're running
# multiple clusters on the same network, make sure you're using unique names.
#
cluster.name: "graylog2"


#################################### Node #####################################

# Node names are generated dynamically on startup, so you're relieved
# from configuring them manually. You can tie this node to a specific name:
#
#node.name: "Franz Kafka"

# Every node can be configured to allow or deny being eligible as the master,
# and to allow or deny to store the data.
#
# Allow this node to be eligible as a master node (enabled by default):
#
node.master: true
#
# Allow this node to store data (enabled by default):
#
node.data: true

# You can exploit these settings to design advanced cluster topologies.
#
# 1. You want this node to never become a master node, only to hold data.
#    This will be the "workhorse" of your cluster.
#
#node.master: false
#node.data: true
#
# 2. You want this node to only serve as a master: to not store any data and
#    to have free resources. This will be the "coordinator" of your cluster.
#
#node.master: true
#node.data: false
#
# 3. You want this node to be neither master nor data node, but
#    to act as a "search load balancer" (fetching data from nodes,
#    aggregating results, etc.)
#
#node.master: false
#node.data: false

# Use the Cluster Health API [http://localhost:9200/_cluster/health], the
# Node Info API [http://localhost:9200/_nodes] or GUI tools
# such as &lt;http://www.elasticsearch.org/overview/marvel/&gt;,
# &lt;http://github.com/karmi/elasticsearch-paramedic&gt;,
# &lt;http://github.com/lukas-vlcek/bigdesk&gt; and
# &lt;http://mobz.github.com/elasticsearch-head&gt; to inspect the cluster state.

# A node can have generic attributes associated with it, which can later be used
# for customized shard allocation filtering, or allocation awareness. An attribute
# is a simple key value pair, similar to node.key: value, here is an example:
#
#node.rack: rack314

# By default, multiple nodes are allowed to start from the same installation location
# to disable it, set the following:
#node.max_local_storage_nodes: 1


#################################### Index ####################################

# You can set a number of options (such as shard/replica options, mapping
# or analyzer definitions, translog settings, ...) for indices globally,
# in this file.
#
# Note, that it makes more sense to configure index settings specifically for
# a certain index, either when creating it or by using the index templates API.
#
# See &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules.html&gt; and
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/indices-create-index.html&gt;
# for more information.

# Set the number of shards (splits) of an index (5 by default):
#
#index.number_of_shards: 5

# Set the number of replicas (additional copies) of an index (1 by default):
#
#index.number_of_replicas: 1

# Note, that for development on a local machine, with small indices, it usually
# makes sense to "disable" the distributed features:
#
#index.number_of_shards: 1
#index.number_of_replicas: 0

# These settings directly affect the performance of index and search operations
# in your cluster. Assuming you have enough machines to hold shards and
# replicas, the rule of thumb is:
#
# 1. Having more *shards* enhances the _indexing_ performance and allows to
#    _distribute_ a big index across machines.
# 2. Having more *replicas* enhances the _search_ performance and improves the
#    cluster _availability_.
#
# The "number_of_shards" is a one-time setting for an index.
#
# The "number_of_replicas" can be increased or decreased anytime,
# by using the Index Update Settings API.
#
# Elasticsearch takes care about load balancing, relocating, gathering the
# results from nodes, etc. Experiment with different settings to fine-tune
# your setup.

# Use the Index Status API (&lt;http://localhost:9200/A/_status&gt;) to inspect
# the index status.


#################################### Paths ####################################

# Path to directory containing configuration (this file and logging.yml):
#
#path.conf: /path/to/conf

# Path to directory where to store index data allocated for this node.
#
path.data: /var/opt/elasticsearch/data
#
# Can optionally include more than one location, causing data to be striped across
# the locations (a la RAID 0) on a file level, favouring locations with most free
# space on creation. For example:
#
#path.data: /path/to/data1,/path/to/data2

# Path to temporary files:
#
path.work: /var/opt/elasticsearch/temp

# Path to log files:
#
#path.logs: /path/to/logs

# Path to where plugins are installed:
#
#path.plugins: /path/to/plugins


#################################### Plugin ###################################

# If a plugin listed here is not installed for current node, the node will not start.
#
#plugin.mandatory: mapper-attachments,lang-groovy


################################### Memory ####################################

# Elasticsearch performs poorly when JVM starts swapping: you should ensure that
# it _never_ swaps.
#
# Set this property to true to lock the memory:
#
#bootstrap.mlockall: true

# Make sure that the ES_MIN_MEM and ES_MAX_MEM environment variables are set
# to the same value, and that the machine has enough memory to allocate
# for Elasticsearch, leaving enough memory for the operating system itself.
#
# You should also make sure that the Elasticsearch process is allowed to lock
# the memory, eg. by using `ulimit -l unlimited`.


############################## Network And HTTP ###############################

# Elasticsearch, by default, binds itself to the 0.0.0.0 address, and listens
# on port [9200-9300] for HTTP traffic and on port [9300-9400] for node-to-node
# communication. (the range means that if the port is busy, it will automatically
# try the next port).

# Set the bind address specifically (IPv4 or IPv6):
#
#network.bind_host: 192.168.0.1

# Set the address other nodes will use to communicate with this node. If not
# set, it is automatically derived. It must point to an actual IP address.
#
#network.publish_host: 192.168.0.1

# Set both 'bind_host' and 'publish_host':
#
network.host: 192.168.1.133

# Set a custom port for the node to node communication (9300 by default):
#
#transport.tcp.port: 9300

# Enable compression for all communication between nodes (disabled by default):
#
#transport.tcp.compress: true

# Set a custom port to listen for HTTP traffic:
#
#http.port: 9200

# Set a custom allowed content length:
#
#http.max_content_length: 100mb

# Disable HTTP completely:
#
#http.enabled: false


################################### Gateway ###################################

# The gateway allows for persisting the cluster state between full cluster
# restarts. Every change to the state (such as adding an index) will be stored
# in the gateway, and when the cluster starts up for the first time,
# it will read its state from the gateway.

# There are several types of gateway implementations. For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html&gt;.

# The default gateway type is the "local" gateway (recommended):
#
#gateway.type: local

# Settings below control how and when to start the initial recovery process on
# a full cluster restart (to reuse as much local data as possible when using shared
# gateway).

# Allow recovery process after N nodes in a cluster are up:
#
#gateway.recover_after_nodes: 1

# Set the timeout to initiate the recovery process, once the N nodes
# from previous setting are up (accepts time value):
#
#gateway.recover_after_time: 5m

# Set how many nodes are expected in this cluster. Once these N nodes
# are up (and recover_after_nodes is met), begin recovery process immediately
# (without waiting for recover_after_time to expire):
#
#gateway.expected_nodes: 2


############################# Recovery Throttling #############################

# These settings allow to control the process of shards allocation between
# nodes during initial recovery, replica allocation, rebalancing,
# or when adding and removing nodes.

# Set the number of concurrent recoveries happening on a node:
#
# 1. During the initial recovery
#
#cluster.routing.allocation.node_initial_primaries_recoveries: 4
#
# 2. During adding/removing nodes, rebalancing, etc
#
#cluster.routing.allocation.node_concurrent_recoveries: 2

# Set to throttle throughput when recovering (eg. 100mb, by default 20mb):
#
#indices.recovery.max_bytes_per_sec: 20mb

# Set to limit the number of open concurrent streams when
# recovering a shard from a peer:
#
#indices.recovery.concurrent_streams: 5


################################## Discovery ##################################

# Discovery infrastructure ensures nodes can be found within a cluster
# and master node is elected. Multicast discovery is the default.

# Set to ensure a node sees N other master eligible nodes to be considered
# operational within the cluster. This should be set to a quorum/majority of 
# the master-eligible nodes in the cluster.
#
#discovery.zen.minimum_master_nodes: 1

# Set the time to wait for ping responses from other nodes when discovering.
# Set this option to a higher value on a slow or congested network
# to minimize discovery failures:
#
#discovery.zen.ping.timeout: 3s

# For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html&gt;

# Unicast discovery allows to explicitly control which nodes will be used
# to discover the cluster. It can be used when multicast is not present,
# or to restrict the cluster communication-wise.
#
# 1. Disable multicast discovery (enabled by default):
#
#discovery.zen.ping.multicast.enabled: false
#
# 2. Configure an initial list of master nodes in the cluster
#    to perform discovery when new nodes (master or data) are started:
#
#discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]

# EC2 discovery allows to use AWS EC2 API in order to perform discovery.
#
# You have to install the cloud-aws plugin for enabling the EC2 discovery.
#
# For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-ec2.html&gt;
#
# See &lt;http://elasticsearch.org/tutorials/elasticsearch-on-ec2/&gt;
# for a step-by-step tutorial.

# GCE discovery allows to use Google Compute Engine API in order to perform discovery.
#
# You have to install the cloud-gce plugin for enabling the GCE discovery.
#
# For more information, see &lt;https://github.com/elasticsearch/elasticsearch-cloud-gce&gt;.

# Azure discovery allows to use Azure API in order to perform discovery.
#
# You have to install the cloud-azure plugin for enabling the Azure discovery.
#
# For more information, see &lt;https://github.com/elasticsearch/elasticsearch-cloud-azure&gt;.

################################## Slow Log ##################################

# Shard level query and fetch threshold logging.

#index.search.slowlog.threshold.query.warn: 10s
#index.search.slowlog.threshold.query.info: 5s
#index.search.slowlog.threshold.query.debug: 2s
#index.search.slowlog.threshold.query.trace: 500ms

#index.search.slowlog.threshold.fetch.warn: 1s
#index.search.slowlog.threshold.fetch.info: 800ms
#index.search.slowlog.threshold.fetch.debug: 500ms
#index.search.slowlog.threshold.fetch.trace: 200ms

#index.indexing.slowlog.threshold.index.warn: 10s
#index.indexing.slowlog.threshold.index.info: 5s
#index.indexing.slowlog.threshold.index.debug: 2s
#index.indexing.slowlog.threshold.index.trace: 500ms

################################## GC Logging ################################

#monitor.jvm.gc.young.warn: 1000ms
#monitor.jvm.gc.young.info: 700ms
#monitor.jvm.gc.young.debug: 400ms

#monitor.jvm.gc.old.warn: 10s
#monitor.jvm.gc.old.info: 5s
#monitor.jvm.gc.old.debug: 2s

################################## Security ################################

# Uncomment if you want to enable JSONP as a valid return transport on the
# http server. With this enabled, it may pose a security risk, so disabling
# it unless you need it is recommended (it is disabled by default).
#
#http.jsonp.enable: true
```

Below is the output from "curl -XGET http://192.168.1.131:9200/_cluster/health?pretty

```
{
  "cluster_name" : "graylog2",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 2,
  "active_primary_shards" : 9,
  "active_shards" : 18,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0,
  "delayed_unassigned_shards" : 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch" : 0
}
```

Any idea about what is going on? I am running Elasticsearch 1.7.1, installed from the debian .deb file, pulled directly from the elasticsearch.org website.

Thanks a lot,
Bertrand.
</comment><comment author="imotov" created="2015-09-17T12:47:33Z" id="141070574">If it is always the same server that is crashing and all servers have the same version of JVM and os, it might be faulty memory on this server. Could you try running a memory diagnostics test to rule faulty memory out?
</comment><comment author="dbblackdiamond" created="2015-09-17T18:32:36Z" id="141179738">Unfortunately, it isn't always the same server. Also, all servers are running as a Virtual Machine on the same physical server, so if it was faulty memory, wouldn't that affect all the virtual machines?
</comment><comment author="dobariya" created="2015-09-18T03:43:14Z" id="141335337">yes correct the same situation is with me as well. Just for clarification my nodes are running on windows server 2008 R2  virtual machines
</comment><comment author="clintongormley" created="2015-09-18T15:16:49Z" id="141480365">There is a good chance it is libsigar causing this.  try deleting the `lib/sigar` directory 

2.0 doesn't rely on sigar at all
</comment><comment author="dbblackdiamond" created="2015-09-18T15:37:47Z" id="141485086">OK... I have removed the lib/sigar directory on 2 of my 4 nodes. Let see if it makes any difference.
</comment><comment author="dbblackdiamond" created="2015-09-23T22:35:46Z" id="142749669">Hi, so far, my cluster has been pretty stable. Unfortunately, the host, that runs it, crashed and I had to restart everything, but for the last 48 hours, it has been stable. I'll keep monitoring it and will report back.
</comment><comment author="dbblackdiamond" created="2015-09-24T14:41:17Z" id="142949335">I have looked at my cluster this morning and again, it looks like the java executable crashed again. 
Below is the content of the apport.log file:

```
ERROR: apport (pid 30379) Thu Sep 24 06:57:53 2015: called for pid 2038, signal 6, core limit 0
ERROR: apport (pid 30379) Thu Sep 24 06:57:53 2015: executable: /usr/lib/jvm/java-7-oracle/jre/bin/java (command lin
e "/usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiating
OccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfil
e.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elastic
search/lib/elasticsearch-1.7.1.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.pidfile=
/var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var
/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.config=/etc/elasticsearch/elasticsearc
h.yml -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch")
ERROR: apport (pid 30379) Thu Sep 24 06:57:53 2015: is_closing_session(): no DBUS_SESSION_BUS_ADDRESS in environment
ERROR: apport (pid 30379) Thu Sep 24 06:57:58 2015: core dump exceeded 56 MiB, dropped from /var/crash/_usr_lib_jvm_
java-7-oracle_jre_bin_java.112.crash to avoid memory overflow
ERROR: apport (pid 30379) Thu Sep 24 06:57:58 2015: wrote report /var/crash/_usr_lib_jvm_java-7-oracle_jre_bin_java.
112.crash
```

Below is the content of the .crash file:

```
ProblemType: Crash
Architecture: amd64
CrashCounter: 1
Date: Thu Sep 24 06:57:53 2015
DistroRelease: Ubuntu 15.04
ExecutablePath: /usr/lib/jvm/java-7-oracle/jre/bin/java
ExecutableTimestamp: 1442115528
ProcCmdline: /usr/bin/java -Xms256m -Xmx1g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMS
InitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplic
itGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/share/elasticsearch -cp :/usr/sha
re/elasticsearch/lib/elasticsearch-1.7.1.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -De
s.pidfile=/var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/usr/share/elasticsearch -Des.default.path
.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.config=/etc/elasticsearch/el
asticsearch.yml -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch
ProcCwd: /
ProcEnviron:
 PATH=(custom, no user)
 LANG=en_US.UTF-8
 SHELL=/bin/false
ProcMaps:
 00400000-00401000 r-xp 00000000 08:01 258460                             /usr/lib/jvm/java-7-oracle/jre/bin/java
 00600000-00601000 rw-p 00000000 08:01 258460                             /usr/lib/jvm/java-7-oracle/jre/bin/java
 01818000-02758000 rw-p 00000000 00:00 0                                  [heap]
 bae00000-cae00000 rw-p 00000000 00:00 0 
 cae00000-fae00000 rw-p 00000000 00:00 0 
 fae00000-ff063000 rw-p 00000000 00:00 0 
 ff063000-100000000 rw-p 00000000 00:00 0 
 7ff4cf168000-7ff4cf5be000 r--s 00000000 08:01 493109                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.24/0/index/_2o4_Lucene41_0.tim
 7ff4cfe70000-7ff4d0039000 r--s 00000000 08:01 493077                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.23/0/index/_a5d_Lucene41_0.tim
 7ff4d03d0000-7ff4d09cd000 r--s 00000000 08:01 492675                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.23/0/index/_8z4_Lucene41_0.tim
 7ff4d09d0000-7ff4d1469000 r--s 00000000 08:01 257032                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_3rdp_Lucene41_0.tim
 7ff4d1470000-7ff4d1a56000 r--s 00000000 08:01 493016                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.23/0/index/_596_Lucene41_0.tim
 7ff4d1a57000-7ff4d1a5a000 ---p 00000000 00:00 0 
 7ff4d1a5a000-7ff4d1b58000 rw-p 00000000 00:00 0 
 7ff4d1b58000-7ff4d1ef2000 r--s 00000000 08:01 492653                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.22/0/index/_3t8_Lucene41_0.tim
 7ff4d1ef7000-7ff4d1efa000 ---p 00000000 00:00 0 
 7ff4d1efa000-7ff4d1ff8000 rw-p 00000000 00:00 0 
 7ff4d2037000-7ff4d203a000 ---p 00000000 00:00 0 
 7ff4d203a000-7ff4d2138000 rw-p 00000000 00:00 0 
 7ff4d2138000-7ff4d21b7000 r--s 00000000 08:01 257131                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_3rdp_Lucene410_0.dvd
 7ff4d21b8000-7ff4d225d000 r--s 00000000 08:01 493070                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_37ie_Lucene410_0.dvd
 7ff4d2260000-7ff4d2e42000 r--s 00000000 08:01 493054                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_37ie_Lucene41_0.tim
 7ff4d2e48000-7ff4d2e8e000 r--s 00000000 08:01 257176                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_39f1_Lucene410_0.dvd
 7ff4d2e90000-7ff4d3487000 r--s 00000000 08:01 257174                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_39f1_Lucene41_0.tim
 7ff4d34a8000-7ff4d34ea000 r--s 00000000 08:01 493155                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_3acs_Lucene410_0.dvd
 7ff4d34f0000-7ff4d3a92000 r--s 00000000 08:01 493047                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_3acs_Lucene41_0.tim
 7ff4d3a98000-7ff4d3ad5000 r--s 00000000 08:01 493035                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_37gf_Lucene410_0.dvd
 7ff4d3ad8000-7ff4d401a000 r--s 00000000 08:01 493029                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_37gf_Lucene41_0.tim
 7ff4d4030000-7ff4d4031000 r--s 00000000 08:01 493068                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.24/0/index/_2o4_Lucene410_0.dvd
 7ff4d4040000-7ff4d4041000 r--s 00000000 08:01 492663                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.23/0/index/_8z4_Lucene410_0.dvd
 7ff4d4048000-7ff4d4049000 r--s 00000000 08:01 493081                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.23/0/index/_a5d_Lucene410_0.dvd
 7ff4d4050000-7ff4d4051000 r--s 00000000 08:01 492655                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.22/0/index/_3t8_Lucene410_0.dvd
 7ff4d4058000-7ff4d4059000 r--s 00000000 08:01 493027                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.23/0/index/_596_Lucene410_0.dvd
 7ff4d405b000-7ff4d405e000 ---p 00000000 00:00 0 
 7ff4d405e000-7ff4d415c000 rw-p 00000000 00:00 0                          [stack:2099]
 7ff4d415c000-7ff4d415f000 ---p 00000000 00:00 0 
 7ff4d415f000-7ff4d425d000 rw-p 00000000 00:00 0                          [stack:2098]
 7ff4d425d000-7ff4d4260000 ---p 00000000 00:00 0 
 7ff4d4260000-7ff4d435e000 rw-p 00000000 00:00 0                          [stack:2097]
 7ff4d435e000-7ff4d4361000 ---p 00000000 00:00 0 
 7ff4d4361000-7ff4d445f000 rw-p 00000000 00:00 0                          [stack:2096]
 7ff4d445f000-7ff4d4462000 ---p 00000000 00:00 0 
 7ff4d4462000-7ff4d4560000 rw-p 00000000 00:00 0                          [stack:2095]
 7ff4d4560000-7ff4d4561000 r--s 00000000 08:01 492944                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.17/0/index/_d6y_Lucene410_0.dvd
 7ff4d4568000-7ff4d48db000 r--s 00000000 08:01 492955                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.17/0/index/_d6y_Lucene41_0.tim
 7ff4d48e0000-7ff4d48e1000 r--s 00000000 08:01 492936                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.17/0/index/_brt_Lucene410_0.dvd
 7ff4d48e8000-7ff4d549c000 r--s 00000000 08:01 492919                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.17/0/index/_brt_Lucene41_0.tim
 7ff4d54a0000-7ff4d54a2000 r--s 00000000 08:01 492995                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.17/0/index/_6lc_Lucene410_0.dvd
 7ff4d54a8000-7ff4d63fe000 r--s 00000000 08:01 493006                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.17/0/index/_6lc_Lucene41_0.tim
 7ff4d6400000-7ff4d6401000 r--s 00000000 08:01 492863                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.20/0/index/_54b_Lucene410_0.dvd
 7ff4d6408000-7ff4d65e2000 r--s 00000000 08:01 492857                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.20/0/index/_54b_Lucene41_0.tim
 7ff4d65e8000-7ff4d65ea000 r--s 00000000 08:01 492888                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.20/0/index/_4gn_Lucene410_0.dvd
 7ff4d65f0000-7ff4d701b000 r--s 00000000 08:01 492876                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.20/0/index/_4gn_Lucene41_0.tim
 7ff4d7020000-7ff4d7098000 r--s 00000000 08:01 257042                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_2qip_Lucene410_0.dvd
 7ff4d7098000-7ff4d7acb000 r--s 00000000 08:01 257060                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_2qip_Lucene41_0.tim
 7ff4d7ad0000-7ff4d7b29000 r--s 00000000 08:01 257057                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_8fk_Lucene410_0.dvd
 7ff4d7b30000-7ff4d8301000 r--s 00000000 08:01 257159                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_8fk_Lucene41_0.tim
 7ff4d8308000-7ff4d83ae000 r--s 00000000 08:01 257051                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_1eq3_Lucene410_0.dvd
 7ff4d83b0000-7ff4d90de000 r--s 00000000 08:01 257155                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/2/index/_1eq3_Lucene41_0.tim
 7ff4d90e0000-7ff4d90e1000 r--s 00000000 08:01 492800                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_9sy_Lucene410_0.dvd
 7ff4d90e8000-7ff4d94ae000 r--s 00000000 08:01 492795                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_9sy_Lucene41_0.tim
 7ff4d94b0000-7ff4d94b1000 r--s 00000000 08:01 492809                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_cf6_Lucene410_0.dvd
 7ff4d94b8000-7ff4d9c08000 r--s 00000000 08:01 492841                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_cf6_Lucene41_0.tim
 7ff4d9c08000-7ff4d9c09000 r--s 00000000 08:01 492778                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_825_Lucene410_0.dvd
 7ff4d9c10000-7ff4da13f000 r--s 00000000 08:01 492827                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_825_Lucene41_0.tim
 7ff4da140000-7ff4da141000 r--s 00000000 08:01 492759                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_4o5_Lucene410_0.dvd
 7ff4da148000-7ff4daaa8000 r--s 00000000 08:01 492822                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.19/0/index/_4o5_Lucene41_0.tim
 7ff4daaa8000-7ff4daaa9000 r--s 00000000 08:01 492706                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_bht_Lucene410_0.dvd
 7ff4daab0000-7ff4dad85000 r--s 00000000 08:01 492700                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_bht_Lucene41_0.tim
 7ff4dad88000-7ff4db746000 r--s 00000000 08:01 492821                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_dqy_Lucene41_0.tim
 7ff4db748000-7ff4dbf20000 r--s 00000000 08:01 492685                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_7ud_Lucene41_0.tim
 7ff4dbf20000-7ff4dc9df000 r--s 00000000 08:01 492731                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_4jg_Lucene41_0.tim
 7ff4dc9e4000-7ff4dc9e7000 ---p 00000000 00:00 0 
 7ff4dc9e7000-7ff4dcae5000 rw-p 00000000 00:00 0 
 7ff4dcae5000-7ff4dcae8000 ---p 00000000 00:00 0 
 7ff4dcae8000-7ff4dcbe6000 rw-p 00000000 00:00 0                          [stack:2086]
 7ff4dcbe6000-7ff4dcbe9000 ---p 00000000 00:00 0 
 7ff4dcbe9000-7ff4dcce7000 rw-p 00000000 00:00 0                          [stack:29076]
 7ff4dcce7000-7ff4dccea000 ---p 00000000 00:00 0 
 7ff4dccea000-7ff4dcde8000 rw-p 00000000 00:00 0                          [stack:2084]
 7ff4dcde8000-7ff4dce62000 r--s 00000000 08:01 493270                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_2tgr_Lucene410_0.dvd
 7ff4dce68000-7ff4dd8b6000 r--s 00000000 08:01 493061                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_2tgr_Lucene41_0.tim
 7ff4dd8b8000-7ff4ddfd1000 r--s 00000000 08:01 493368                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_1sj0_Lucene41_0.tim
 7ff4ddfd8000-7ff4ded9a000 r--s 00000000 08:01 493259                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_hy9_Lucene41_0.tim
 7ff4ded9f000-7ff4deda2000 ---p 00000000 00:00 0 
 7ff4deda2000-7ff4deea0000 rw-p 00000000 00:00 0                          [stack:2083]
 7ff4deea0000-7ff4df1f7000 r--s 00000000 08:01 492622                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_2avh_Lucene41_0.tim
 7ff4df1f8000-7ff4dfffa000 r--s 00000000 08:01 492567                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_ipe_Lucene41_0.tim
 7ff4e0000000-7ff4e0143000 rw-p 00000000 00:00 0 
 7ff4e0143000-7ff4e4000000 ---p 00000000 00:00 0 
 7ff4e4000000-7ff4e5530000 rw-p 00000000 00:00 0 
 7ff4e5530000-7ff4e8000000 ---p 00000000 00:00 0 
 7ff4e8000000-7ff4e8030000 rw-p 00000000 00:00 0 
 7ff4e8030000-7ff4ec000000 ---p 00000000 00:00 0 
 7ff4ec000000-7ff4ec136000 rw-p 00000000 00:00 0 
 7ff4ec136000-7ff4f0000000 ---p 00000000 00:00 0 
 7ff4f0000000-7ff4f0272000 rw-p 00000000 00:00 0 
 7ff4f0272000-7ff4f4000000 ---p 00000000 00:00 0 
 7ff4f4000000-7ff4f4036000 rw-p 00000000 00:00 0 
 7ff4f4036000-7ff4f8000000 ---p 00000000 00:00 0 
 7ff4f8000000-7ff4f8098000 rw-p 00000000 00:00 0 
 7ff4f8098000-7ff4fc000000 ---p 00000000 00:00 0 
 7ff4fc000000-7ff4fc001000 r--s 00000000 08:01 492732                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_dqy_Lucene410_0.dvd
 7ff4fc008000-7ff4fc055000 r--s 00000000 08:01 492918                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_1sj0_Lucene410_0.dvd
 7ff4fc058000-7ff4fc0fe000 r--s 00000000 08:01 493291                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/1/index/_hy9_Lucene410_0.dvd
 7ff4fc100000-7ff4fc123000 r--s 00000000 08:01 492610                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_2avh_Lucene410_0.dvd
 7ff4fc128000-7ff4fc1a0000 r--s 00000000 08:01 492565                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_1wf6_Lucene410_0.dvd
 7ff4fc1a0000-7ff4fcbff000 r--s 00000000 08:01 492559                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_1wf6_Lucene41_0.tim
 7ff4fcc00000-7ff4fcca8000 r--s 00000000 08:01 492589                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/graylog_0/0/index/_ipe_Lucene410_0.dvd
 7ff4fccae000-7ff4fccb1000 ---p 00000000 00:00 0 
 7ff4fccb1000-7ff4fcdaf000 rw-p 00000000 00:00 0                          [stack:2248]
 7ff4fcdaf000-7ff4fcdb2000 ---p 00000000 00:00 0 
 7ff4fcdb2000-7ff4fceb0000 rw-p 00000000 00:00 0                          [stack:2106]
 7ff4fceb0000-7ff4fceb3000 ---p 00000000 00:00 0 
 7ff4fceb3000-7ff4fcfb1000 rw-p 00000000 00:00 0                          [stack:6942]
 7ff4fcfb1000-7ff4fcfb4000 ---p 00000000 00:00 0 
 7ff4fcfb4000-7ff4fd0b2000 rw-p 00000000 00:00 0                          [stack:2105]
 7ff4fd0b2000-7ff4fd0b5000 ---p 00000000 00:00 0 
 7ff4fd0b5000-7ff4fd1b3000 rw-p 00000000 00:00 0                          [stack:2078]
 7ff4fd1b3000-7ff4fd1b6000 ---p 00000000 00:00 0 
 7ff4fd1b6000-7ff4fd2b4000 rw-p 00000000 00:00 0                          [stack:2077]
 7ff4fd2b4000-7ff4fd2b7000 ---p 00000000 00:00 0 
 7ff4fd2b7000-7ff4fd3b5000 rw-p 00000000 00:00 0                          [stack:2076]
 7ff4fd3b5000-7ff4fd3b8000 ---p 00000000 00:00 0 
 7ff4fd3b8000-7ff4fd4b6000 rw-p 00000000 00:00 0                          [stack:2075]
 7ff4fd4b6000-7ff4fd4b9000 ---p 00000000 00:00 0 
 7ff4fd4b9000-7ff4fd5b7000 rw-p 00000000 00:00 0                          [stack:2074]
 7ff4fd5b7000-7ff4fd5ba000 ---p 00000000 00:00 0 
 7ff4fd5ba000-7ff4fd6b8000 rw-p 00000000 00:00 0                          [stack:2169]
 7ff4fd6b8000-7ff4fd6bb000 ---p 00000000 00:00 0 
 7ff4fd6bb000-7ff4fd7b9000 rw-p 00000000 00:00 0                          [stack:2072]
 7ff4fd7b9000-7ff4fd7bc000 ---p 00000000 00:00 0 
 7ff4fd7bc000-7ff4fd8ba000 rw-p 00000000 00:00 0                          [stack:2101]
 7ff4fd8ba000-7ff4fd8bd000 ---p 00000000 00:00 0 
 7ff4fd8bd000-7ff4fd9bb000 rw-p 00000000 00:00 0                          [stack:29981]
 7ff4fd9bb000-7ff4fd9be000 ---p 00000000 00:00 0 
 7ff4fd9be000-7ff4fdabc000 rw-p 00000000 00:00 0                          [stack:2069]
 7ff4fdabc000-7ff4fdabf000 ---p 00000000 00:00 0 
 7ff4fdabf000-7ff4fdbbd000 rw-p 00000000 00:00 0                          [stack:2068]
 7ff4fdbbd000-7ff4fdbc0000 ---p 00000000 00:00 0 
 7ff4fdbc0000-7ff4fdcbe000 rw-p 00000000 00:00 0                          [stack:2067]
 7ff4fdcbe000-7ff4fdcc1000 ---p 00000000 00:00 0 
 7ff4fdcc1000-7ff4fddbf000 rw-p 00000000 00:00 0                          [stack:2066]
 7ff4fddbf000-7ff4fddc2000 ---p 00000000 00:00 0 
 7ff4fddc2000-7ff4fdec0000 rw-p 00000000 00:00 0                          [stack:2065]
 7ff4fdec0000-7ff4fdec3000 ---p 00000000 00:00 0 
 7ff4fdec3000-7ff4fdfc1000 rw-p 00000000 00:00 0                          [stack:2064]
 7ff4fdfc1000-7ff4fdfc4000 ---p 00000000 00:00 0 
 7ff4fdfc4000-7ff4fe0c2000 rw-p 00000000 00:00 0                          [stack:2063]
 7ff4fe0c2000-7ff4fe0c5000 ---p 00000000 00:00 0 
 7ff4fe0c5000-7ff4fe1c3000 rw-p 00000000 00:00 0                          [stack:2062]
 7ff4fe1c3000-7ff4fe1c6000 ---p 00000000 00:00 0 
 7ff4fe1c6000-7ff4fe2c4000 rw-p 00000000 00:00 0                          [stack:2061]
 7ff4fe2c4000-7ff4fe2c7000 ---p 00000000 00:00 0 
 7ff4fe2c7000-7ff4fe3c5000 rw-p 00000000 00:00 0                          [stack:2060]
 7ff4fe3c5000-7ff4fe3c8000 ---p 00000000 00:00 0 
 7ff4fe3c8000-7ff4fe4c6000 rw-p 00000000 00:00 0                          [stack:2059]
 7ff4fe4c6000-7ff4fe4c9000 ---p 00000000 00:00 0 
 7ff4fe4c9000-7ff4fe5c7000 rw-p 00000000 00:00 0                          [stack:2058]
 7ff4fe5c7000-7ff4fe5ca000 ---p 00000000 00:00 0 
 7ff4fe5ca000-7ff4fe6c8000 rw-p 00000000 00:00 0                          [stack:2057]
 7ff4fe6c8000-7ff4fe6d8000 r-xp 00000000 08:01 257747                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibnio.so
 7ff4fe6d8000-7ff4fe8d8000 ---p 00010000 08:01 257747                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibnio.so
 7ff4fe8d8000-7ff4fe8d9000 rw-p 00010000 08:01 257747                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibnio.so
 7ff4fe8e0000-7ff4febe0000 rw-p 00000000 00:00 0 
 7ff4febe0000-7ff4febf7000 r-xp 00000000 08:01 257781                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibnet.so
 7ff4febf7000-7ff4fedf6000 ---p 00017000 08:01 257781                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibnet.so
 7ff4fedf6000-7ff4fedf7000 rw-p 00016000 08:01 257781                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibnet.so
 7ff4fedf8000-7ff4fee00000 r-xp 00000000 08:01 257755                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibmanagement.so
 7ff4fee00000-7ff4fefff000 ---p 00008000 08:01 257755                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibmanagement.so
 7ff4fefff000-7ff4ff000000 rw-p 00007000 08:01 257755                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibmanagement.so
 7ff4ff000000-7ff501000000 rw-p 00000000 00:00 0 
 7ff501000000-7ff501900000 rwxp 00000000 00:00 0 
 7ff501900000-7ff504cd6000 rw-p 00000000 00:00 0 
 7ff504cd6000-7ff508000000 ---p 00000000 00:00 0 
 7ff508000000-7ff508001000 r--s 00000000 08:01 492684                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_7ud_Lucene410_0.dvd
 7ff508008000-7ff508009000 r--s 00000000 08:01 492715                     /var/opt/elasticsearch/data/graylog2/nodes
/0/indices/.marvel-2015.09.18/0/index/_4jg_Lucene410_0.dvd
 7ff50800f000-7ff508013000 r--s 000f7000 08:01 258384                     /usr/lib/jvm/java-7-oracle/jre/lib/ext/loc
aledata.jar
 7ff508018000-7ff50801a000 r--s 00010000 08:01 255931                     /usr/share/elasticsearch/plugins/marvel/ma
rvel-1.3.1.jar
 7ff50801b000-7ff50801f000 r--s 0008b000 08:01 258373                     /usr/lib/jvm/java-7-oracle/jre/lib/jsse.ja
r
 7ff50801f000-7ff508025000 r--s 0002f000 08:01 255579                     /usr/share/elasticsearch/lib/lucene-querie
s-4.10.4.jar
 7ff50802a000-7ff50803b000 r--s 000b2000 08:01 255569                     /usr/share/elasticsearch/lib/jts-1.13.jar
 7ff508040000-7ff508041000 r--s 00008000 08:01 255577                     /usr/share/elasticsearch/lib/lucene-memory
-4.10.4.jar
 7ff508047000-7ff508050000 r--s 00057000 08:01 255580                     /usr/share/elasticsearch/lib/lucene-queryp
arser-4.10.4.jar
 7ff508056000-7ff508059000 r--s 00026000 08:01 255563                     /usr/share/elasticsearch/lib/antlr-runtime
-3.5.jar
 7ff508060000-7ff508062000 r--s 00008000 08:01 255566                     /usr/share/elasticsearch/lib/asm-commons-4
.1.jar
 7ff508063000-7ff508066000 r--s 0001b000 08:01 255581                     /usr/share/elasticsearch/lib/lucene-sandbo
x-4.10.4.jar
 7ff50806c000-7ff508070000 r--s 000dc000 08:01 255568                     /usr/share/elasticsearch/lib/jna-4.1.0.jar
 7ff508077000-7ff50807a000 r--s 0001f000 08:01 255575                     /usr/share/elasticsearch/lib/lucene-highli
ghter-4.10.4.jar
 7ff50807c000-7ff50807f000 r--s 0001c000 08:01 255582                     /usr/share/elasticsearch/lib/lucene-spatia
l-4.10.4.jar
 7ff50807f000-7ff508081000 r--s 00017000 08:01 255584                     /usr/share/elasticsearch/lib/spatial4j-0.4
.1.jar
 7ff508081000-7ff508083000 r--s 00011000 08:01 255573                     /usr/share/elasticsearch/lib/lucene-expres
sions-4.10.4.jar
 7ff508088000-7ff5080b2000 r--s 00248000 08:01 255572                     /usr/share/elasticsearch/lib/lucene-core-4
.10.4.jar
 7ff5080b3000-7ff5080b4000 r--s 0000b000 08:01 255565                     /usr/share/elasticsearch/lib/asm-4.1.jar
 7ff5080b6000-7ff5080be000 r--s 00066000 08:01 255564                     /usr/share/elasticsearch/lib/apache-log4j-
extras-1.2.17.jar
 7ff5080be000-7ff5080c0000 r--s 00016000 08:01 255578                     /usr/share/elasticsearch/lib/lucene-misc-4
.10.4.jar
 7ff5080c1000-7ff5080c4000 r--s 00029000 08:01 255583                     /usr/share/elasticsearch/lib/lucene-sugges
t-4.10.4.jar
 7ff5080c6000-7ff508135000 r--s 0063e000 08:01 255567                     /usr/share/elasticsearch/lib/groovy-all-2.
4.4.jar
 7ff508136000-7ff508138000 r--s 0000e000 08:01 255576                     /usr/share/elasticsearch/lib/lucene-join-4
.10.4.jar
 7ff508138000-7ff50813b000 r--s 00018000 08:01 255574                     /usr/share/elasticsearch/lib/lucene-groupi
ng-4.10.4.jar
 7ff50813f000-7ff508148000 r--s 0006f000 08:01 255570                     /usr/share/elasticsearch/lib/log4j-1.2.17.
jar
 7ff50814f000-7ff508160000 r--s 0018f000 08:01 255571                     /usr/share/elasticsearch/lib/lucene-analyz
ers-common-4.10.4.jar
 7ff508164000-7ff50818c000 rw-p 00000000 00:00 0 
 7ff50818c000-7ff50829e000 r--s 00c54000 08:01 255557                     /usr/share/elasticsearch/lib/elasticsearch
-1.7.1.jar
 7ff5082a2000-7ff5082a3000 ---p 00000000 00:00 0 
 7ff5082a3000-7ff5083a3000 rw-p 00000000 00:00 0                          [stack:2056]
 7ff5083a3000-7ff5083a6000 ---p 00000000 00:00 0 
 7ff5083a6000-7ff5084a4000 rw-p 00000000 00:00 0                          [stack:2055]
 7ff5084a4000-7ff5084a7000 ---p 00000000 00:00 0 
 7ff5084a7000-7ff5085a5000 rw-p 00000000 00:00 0                          [stack:2054]
 7ff5085a5000-7ff5085a8000 ---p 00000000 00:00 0 
 7ff5085a8000-7ff5086a6000 rw-p 00000000 00:00 0                          [stack:2053]
 7ff5086a6000-7ff5086a9000 ---p 00000000 00:00 0 
 7ff5086a9000-7ff5087a7000 rw-p 00000000 00:00 0                          [stack:2052]
 7ff5087a7000-7ff5087aa000 ---p 00000000 00:00 0 
 7ff5087aa000-7ff5088a8000 rw-p 00000000 00:00 0                          [stack:2051]
 7ff5088a8000-7ff508a31000 r--p 00000000 08:01 54985                      /usr/lib/locale/locale-archive
 7ff508a35000-7ff508a38000 ---p 00000000 00:00 0 
 7ff508a38000-7ff508b36000 rw-p 00000000 00:00 0                          [stack:2050]
 7ff508b36000-7ff508b39000 ---p 00000000 00:00 0 
 7ff508b39000-7ff508c37000 rw-p 00000000 00:00 0                          [stack:2049]
 7ff508c37000-7ff508c38000 ---p 00000000 00:00 0 
 7ff508c38000-7ff508d83000 rw-p 00000000 00:00 0                          [stack:2048]
 7ff508d83000-7ff508f43000 r--s 039fb000 08:01 258429                     /usr/lib/jvm/java-7-oracle/jre/lib/rt.jar
 7ff508f49000-7ff509042000 rw-p 00000000 00:00 0 
 7ff509042000-7ff509043000 ---p 00000000 00:00 0 
 7ff509043000-7ff50a9d6000 rw-p 00000000 00:00 0                          [stack:2047]
 7ff50a9d6000-7ff50ab56000 rw-p 00000000 00:00 0 
 7ff50ab56000-7ff50abd6000 rw-p 00000000 00:00 0 
 7ff50abd6000-7ff50ad56000 rw-p 00000000 00:00 0 
 7ff50ad56000-7ff50ad78000 rw-p 00000000 00:00 0 
 7ff50ad78000-7ff50ad7f000 rw-p 00000000 00:00 0 
 7ff50ad7f000-7ff50ad80000 rw-p 00000000 00:00 0 
 7ff50ad80000-7ff50ad97000 r-xp 00000000 08:01 257776                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibzip.so
 7ff50ad97000-7ff50af97000 ---p 00017000 08:01 257776                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibzip.so
 7ff50af97000-7ff50af98000 rw-p 00017000 08:01 257776                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibzip.so
 7ff50af98000-7ff50afa4000 r-xp 00000000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7ff50afa4000-7ff50b1a3000 ---p 0000c000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7ff50b1a3000-7ff50b1a4000 r--p 0000b000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7ff50b1a4000-7ff50b1a5000 rw-p 0000c000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
 7ff50b1a8000-7ff50b1b3000 r-xp 00000000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7ff50b1b3000-7ff50b3b2000 ---p 0000b000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7ff50b3b2000-7ff50b3b3000 r--p 0000a000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7ff50b3b3000-7ff50b3b4000 rw-p 0000b000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
 7ff50b3b8000-7ff50b3cf000 r-xp 00000000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7ff50b3cf000-7ff50b5ce000 ---p 00017000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7ff50b5ce000-7ff50b5cf000 r--p 00016000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7ff50b5cf000-7ff50b5d0000 rw-p 00017000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
 7ff50b5d0000-7ff50b5d2000 rw-p 00000000 00:00 0 
 7ff50b5d8000-7ff50b5e0000 r-xp 00000000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.s
o
 7ff50b5e0000-7ff50b7df000 ---p 00008000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.s
o
 7ff50b7df000-7ff50b7e0000 r--p 00007000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.s
o
 7ff50b7e0000-7ff50b7e1000 rw-p 00008000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.s
o
 7ff50b7e8000-7ff50b811000 r-xp 00000000 08:01 257742                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibjava.so
 7ff50b811000-7ff50ba11000 ---p 00029000 08:01 257742                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibjava.so
 7ff50ba11000-7ff50ba13000 rw-p 00029000 08:01 257742                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibjava.so
 7ff50ba18000-7ff50ba25000 r-xp 00000000 08:01 257769                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibverify.so
 7ff50ba25000-7ff50bc24000 ---p 0000d000 08:01 257769                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibverify.so
 7ff50bc24000-7ff50bc26000 rw-p 0000c000 08:01 257769                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/l
ibverify.so
 7ff50bc28000-7ff50bc2f000 r-xp 00000000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7ff50bc2f000-7ff50be2e000 ---p 00007000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7ff50be2e000-7ff50be2f000 r--p 00006000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7ff50be2f000-7ff50be30000 rw-p 00007000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
 7ff50be30000-7ff50bf37000 r-xp 00000000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7ff50bf37000-7ff50c136000 ---p 00107000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7ff50c136000-7ff50c137000 r--p 00106000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7ff50c137000-7ff50c138000 rw-p 00107000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
 7ff50c138000-7ff50ccae000 r-xp 00000000 08:01 257796                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/s
erver/libjvm.so
 7ff50ccae000-7ff50cead000 ---p 00b76000 08:01 257796                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/s
erver/libjvm.so
 7ff50cead000-7ff50cf71000 rw-p 00b75000 08:01 257796                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/s
erver/libjvm.so
 7ff50cf71000-7ff50cfb2000 rw-p 00000000 00:00 0 
 7ff50cfb8000-7ff50d178000 r-xp 00000000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7ff50d178000-7ff50d378000 ---p 001c0000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7ff50d378000-7ff50d37c000 r--p 001c0000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7ff50d37c000-7ff50d37e000 rw-p 001c4000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
 7ff50d37e000-7ff50d382000 rw-p 00000000 00:00 0 
 7ff50d388000-7ff50d38b000 r-xp 00000000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7ff50d38b000-7ff50d58a000 ---p 00003000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7ff50d58a000-7ff50d58b000 r--p 00002000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7ff50d58b000-7ff50d58c000 rw-p 00003000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
 7ff50d590000-7ff50d5a5000 r-xp 00000000 08:01 257788                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/j
li/libjli.so
 7ff50d5a5000-7ff50d7a4000 ---p 00015000 08:01 257788                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/j
li/libjli.so
 7ff50d7a4000-7ff50d7a5000 rw-p 00014000 08:01 257788                     /usr/lib/jvm/java-7-oracle/jre/lib/amd64/j
li/libjli.so
 7ff50d7a8000-7ff50d7c0000 r-xp 00000000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7ff50d7c0000-7ff50d9c0000 ---p 00018000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7ff50d9c0000-7ff50d9c1000 r--p 00018000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7ff50d9c1000-7ff50d9c2000 rw-p 00019000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
 7ff50d9c2000-7ff50d9c6000 rw-p 00000000 00:00 0 
 7ff50d9c8000-7ff50d9ec000 r-xp 00000000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
 7ff50d9ee000-7ff50da10000 rw-p 00000000 00:00 0 
 7ff50da10000-7ff50da18000 rw-p 00000000 00:00 0 
 7ff50da18000-7ff50da3c000 rw-p 00000000 00:00 0 
 7ff50da3c000-7ff50dad8000 rw-p 00000000 00:00 0 
 7ff50dad8000-7ff50dae0000 rw-s 00000000 08:01 257125                     /tmp/hsperfdata_elasticsearch/2038 (delete
d)
 7ff50dae3000-7ff50dae5000 rw-p 00000000 00:00 0 
 7ff50dae5000-7ff50dae6000 r--p 00000000 00:00 0 
 7ff50dae6000-7ff50dae9000 ---p 00000000 00:00 0 
 7ff50dae9000-7ff50dbeb000 rw-p 00000000 00:00 0                          [stack:2046]
 7ff50dbeb000-7ff50dbec000 r--p 00023000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
 7ff50dbec000-7ff50dbed000 rw-p 00024000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
 7ff50dbed000-7ff50dbf0000 rw-p 00000000 00:00 0 
 7fffd07ba000-7fffd07db000 rw-p 00000000 00:00 0                          [stack]
 7fffd07e0000-7fffd07e2000 r--p 00000000 00:00 0                          [vvar]
 7fffd07e2000-7fffd07e4000 r-xp 00000000 00:00 0                          [vdso]
 ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]
ProcStatus:
 Name:  java
 State: D (disk sleep)
 Tgid:  2038
 Ngid:  0
 Pid:   2038
 PPid:  1
 TracerPid: 0
 Uid:   112 112 112 112
 Gid:   120 120 120 120
 FDSize:    512
 Groups:    120 
 VmPeak:     2164628 kB
 VmSize:     2160164 kB
 VmLck:        0 kB
 VmPin:        0 kB
 VmHWM:   383944 kB
 VmRSS:   312008 kB
 VmData:     1840504 kB
 VmStk:      136 kB
 VmExe:        4 kB
 VmLib:    15656 kB
 VmPTE:     1308 kB
 VmSwap:       64412 kB
 Threads:   47
 SigQ:  0/1924
 SigPnd:    0000000000000000
 ShdPnd:    0000000000000000
 SigBlk:    0000000000000000
 SigIgn:    0000000000000000
 SigCgt:    2000000181005ccf
 CapInh:    0000000000000000
 CapPrm:    0000000000000000
 CapEff:    0000000000000000
 CapBnd:    0000003fffffffff
 Seccomp:   0
 Cpus_allowed:  1
 Cpus_allowed_list: 0
 Mems_allowed:  00000000,00000001
 Mems_allowed_list: 0
 voluntary_ctxt_switches:   29
 nonvoluntary_ctxt_switches:    3
Signal: 6
Uname: Linux 3.19.0-28-generic x86_64
UserGroups: 
```

I have been searching for further information and, unfortunately, can't find anything as to why the java process crashed.
Any idea?

Thanks a lot,
Bertrand.
</comment><comment author="clintongormley" created="2015-09-25T12:21:49Z" id="143202173">I'd suggest upgrading your JVM.  Also, giving Elasticsearch 512MB is pretty damn limited, especially with 300 indices.  That is probably unrelated to this JVM crash though.
</comment><comment author="dbblackdiamond" created="2015-09-25T15:05:51Z" id="143247063">I have upgraded the RAM on the VMs to 1GB, although I don't see any out of memory error messages. 
</comment><comment author="clintongormley" created="2015-09-27T09:38:37Z" id="143533270">@dbblackdiamond Have you upgraded the JVM, which was my primary advice?
</comment><comment author="dbblackdiamond" created="2015-09-29T15:38:47Z" id="144097177">@clintongormley: yes I have upgraded my virtual machines from 512MB of RAM to 1GB of RAM. I haven't touched the Java settings though.
</comment><comment author="dobariya" created="2015-10-01T07:55:22Z" id="144647715">@clintongormley: I used debug diagnostic tool by Microsoft which gave me exception report  occurred in java.exe.
I tried uploading the file but it does not allow saying something really went wrong.Please let me know how can i transfer the report which might help you to trance the issue  
</comment><comment author="dbblackdiamond" created="2015-10-02T03:31:12Z" id="144908241">@clintongormley: I have increased the memory even more on one of my servers. I increased it to 4GB of RAM for the server and changed the JVM Heap Settings to use 2GB of RAM. So far, things have been stable for all servers. I'll keep monitoring it.
</comment><comment author="dbblackdiamond" created="2015-10-06T15:56:20Z" id="145908731">@clintongormley: despite increasing the JVM Heap Setting from 512MB to 2GB of RAM, the java process still crashed. Is there anyway to debug what is causing the java process to crash?
</comment><comment author="clintongormley" created="2015-10-06T18:00:04Z" id="145947195">@dbblackdiamond I repeat:  Have you upgraded the JVM, which was my primary advice?
</comment><comment author="dbblackdiamond" created="2015-10-06T21:10:12Z" id="146001970">@clintongormley: My apologies, but I must have misunderstood what you meant by "upgrading the JVM". I thought you were talking about upgrading the resources available to the JVM, ie RAM, which I did, but it seems that you meant upgrading the JDK software version. What version would you recommend? The latest Java 8 version?
</comment><comment author="rmuir" created="2015-10-06T23:56:30Z" id="146037300">You want java 8 update 40 or above.

For the record, when JVM crashes, you want to first find the crash log, otherwise nobody can debug anything :) See http://docs.oracle.com/javase/7/docs/webnotes/tsg/TSG-VM/html/felog.html#gbwcy for some hints at where it might be.
</comment><comment author="dbblackdiamond" created="2015-10-07T03:45:25Z" id="146070432">@rmuir: Thanks a lot for the recommendation. I have updated 2 out of my 3 servers with Java 8 update 60 and I'll monitor the cluster to see if it is more stable.
I have found the latest crash log, and I have included below:

```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f71c155e72e, pid=1583, tid=140126319900416
#
# JRE version: Java(TM) SE Runtime Environment (7.0_80-b15) (build 1.7.0_80-b15)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.80-b11 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x49e72e]  DefNewGeneration::copy_to_survivor_space(oopDesc*)+0x3e
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before
 starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread (0x00007f71bc093000):  VMThread [stack: 0x00007f71b3757000,0x00007f71b3858000] [id=1593]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x0000000004eec390

Registers:
RAX=0x00007f71c1ee6360, RBX=0x000000077f94c56c, RCX=0x0000000000000003, RDX=0x0000000004eec378
RSP=0x00007f71b3856130, RBP=0x00007f71b3856150, RSI=0x0000000000000003, RDI=0x0000000004eec388
R8 =0x0000000000000001, R9 =0x0000000000000015, R10=0x00000000ff9dd86f, R11=0xffffffffffffffff
R12=0x000000077f138338, R13=0x00007f71c1ee6360, R14=0x00007f71bc01c280, R15=0x0000000000000001
RIP=0x00007f71c155e72e, EFLAGS=0x0000000000010206, CSGSFS=0x0000000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e
root@thor:/tmp# cat hs_err_pid1583.log 
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f71c155e72e, pid=1583, tid=140126319900416
#
# JRE version: Java(TM) SE Runtime Environment (7.0_80-b15) (build 1.7.0_80-b15)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (24.80-b11 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x49e72e]  DefNewGeneration::copy_to_survivor_space(oopDesc*)+0x3e
#
# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.java.com/bugreport/crash.jsp
#

---------------  T H R E A D  ---------------

Current thread (0x00007f71bc093000):  VMThread [stack: 0x00007f71b3757000,0x00007f71b3858000] [id=1593]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x0000000004eec390

Registers:
RAX=0x00007f71c1ee6360, RBX=0x000000077f94c56c, RCX=0x0000000000000003, RDX=0x0000000004eec378
RSP=0x00007f71b3856130, RBP=0x00007f71b3856150, RSI=0x0000000000000003, RDI=0x0000000004eec388
R8 =0x0000000000000001, R9 =0x0000000000000015, R10=0x00000000ff9dd86f, R11=0xffffffffffffffff
R12=0x000000077f138338, R13=0x00007f71c1ee6360, R14=0x00007f71bc01c280, R15=0x0000000000000001
RIP=0x00007f71c155e72e, EFLAGS=0x0000000000010206, CSGSFS=0x0000000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f71b3856130)
0x00007f71b3856130:   000000077f94c56c 00007f71b38563c0
0x00007f71b3856140:   00007f71c1ee6360 000000077f94c574
0x00007f71b3856150:   00007f71b3856190 00007f71c18c8b92
0x00007f71b3856160:   0000000200000009 000000077f94c520
0x00007f71b3856170:   000000077f968560 0000000000000240
0x00007f71b3856180:   00007f71b38563c0 00007f71c1f01bf8
0x00007f71b3856190:   00007f71b38561d0 00007f71c1994fe7
0x00007f71b38561a0:   00007f71bc01cfc0 00007f71b38563c0
0x00007f71b38561b0:   00007f71bc01c280 00007f71b3856370
0x00007f71b38561c0:   00007f71bc01c370 00007f71bc0193d0
0x00007f71b38561d0:   00007f71b38561f0 00007f71c155d380
0x00007f71b38561e0:   0000000000000001 00007f71bc0193d0
0x00007f71b38561f0:   00007f71b3856220 00007f71c162a9af
0x00007f71b3856200:   0000000000000000 00007f71b3856280
0x00007f71b3856210:   0000000000000000 00007f71bc01c280
0x00007f71b3856220:   00007f71b3856580 00007f71c15603c0
0x00007f71b3856230:   00007f71b38563c0 00007f7100000001
0x00007f71b3856240:   00007f71b3856370 0000000000000000
0x00007f71b3856250:   00007f71b38564a0 00007f71b3856310
0x00007f71b3856260:   00000000ffffffd9 0000000020ccfc90
0x00007f71b3856270:   00000030f431fd35 0000000000000000
0x00007f71b3856280:   00007f7100004347 0000000000000000
0x00007f71b3856290:   ffffffffffffffe9 0000000000000027
0x00007f71b38562a0:   0000000000000000 0000000000000000
0x00007f71b38562b0:   00007f71c1f8b313 0000000000000000
0x00007f71b38562c0:   ffffffffffffffff 0000000000000017
0x00007f71b38562d0:   00007f71c1ab4e37 0000000500000000
0x00007f71b38562e0:   00007f71c1af085b 0000000000000000
0x00007f71b38562f0:   0000000000000000 0000002f00000000
0x00007f71b3856300:   00007f7100000002 00007f71b3856350
0x00007f71b3856310:   00007f71c1eb3270 00000004000097c4
0x00007f71b3856320:   000000000000000a 00000030f431fd35 

Instructions: (pc=0x00007f71c155e72e)
0x00007f71c155e70e:   c0 0f 84 ab 00 00 00 48 8b 05 5c ae 96 00 8b 56
0x00007f71c155e71e:   08 8b 70 08 89 f1 48 d3 e2 48 03 10 48 8d 7a 10
0x00007f71c155e72e:   8b 4f 08 83 f9 00 0f 8e 9a 00 00 00 f6 c1 01 0f
0x00007f71c155e73e:   85 9d 00 00 00 89 c8 c1 f8 03 4c 63 e8 49 8b 04 

Register to memory mapping:

RAX=0x00007f71c1ee6360: &lt;offset 0xe26360&gt; in /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so at 0x00007f71c10c0000
RBX=0x000000077f94c56c is pointing into object: 0x000000077f94c520
[Ljava.lang.Object; 
 - klass: 'java/lang/Object'[]
 - length: 17
RCX=0x0000000000000003 is an unknown value
RDX=0x0000000004eec378 is an unknown value
RSP=0x00007f71b3856130 is an unknown value
RBP=0x00007f71b3856150 is an unknown value
RSI=0x0000000000000003 is an unknown value
RDI=0x0000000004eec388 is an unknown value
R8 =0x0000000000000001 is an unknown value
R9 =0x0000000000000015 is an unknown value
R10=0x00000000ff9dd86f is an unknown value
R11=0xffffffffffffffff is an unknown value
R12=
[error occurred during error reporting (printing register info), id 0xb]

Stack: [0x00007f71b3757000,0x00007f71b3858000],  sp=0x00007f71b3856130,  free space=1020k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0x49e72e]  DefNewGeneration::copy_to_survivor_space(oopDesc*)+0x3e
V  [libjvm.so+0x808b92]  objArrayKlass::oop_oop_iterate_nv(oopDesc*, FastScanClosure*)+0x152
V  [libjvm.so+0x8d4fe7]  ContiguousSpace::oop_since_save_marks_iterate_nv(FastScanClosure*)+0x77
V  [libjvm.so+0x49d380]  DefNewGeneration::oop_since_save_marks_iterate_nv(FastScanClosure*)+0x40
V  [libjvm.so+0x56a9af]  GenCollectedHeap::oop_since_save_marks_iterate(int, FastScanClosure*, FastScanClosure*)+0x2f
V  [libjvm.so+0x4a03c0]  DefNewGeneration::collect(bool, bool, unsigned long, bool)+0x450
V  [libjvm.so+0x56c843]  GenCollectedHeap::do_collection(bool, bool, unsigned long, bool, int)+0x543
V  [libjvm.so+0x42b0e4]  GenCollectorPolicy::satisfy_failed_allocation(unsigned long, bool)+0x104
V  [libjvm.so+0x9a4284]  VM_GenCollectForAllocation::doit()+0x94
V  [libjvm.so+0x9abf35]  VM_Operation::evaluate()+0x55
V  [libjvm.so+0x9aa2fa]  VMThread::evaluate_operation(VM_Operation*)+0xba
V  [libjvm.so+0x9aa67e]  VMThread::loop()+0x1ce
V  [libjvm.so+0x9aaaf0]  VMThread::run()+0x70
V  [libjvm.so+0x8238c8]  java_start(Thread*)+0x108

VM_Operation (0x00007f71b0b16ba0): GenCollectForAllocation, mode: safepoint, requested by thread 0x00007f719827b800


---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x00007f71a40de000 JavaThread "elasticsearch[Arkady Rossovich][[.marvel-2015.10.03][0]: Lucene Merge Thread #233]" daemon [_thread_blocked, id=14719, stack(0x00007f71b0347000,0x00007f71b0448000)]
  0x00007f7194094000 JavaThread "elasticsearch[Arkady Rossovich][generic][T#16]" daemon [_thread_blocked, id=10712, stack(0x00007f71912f6000,0x00007f71913f7000)]
  0x00007f719408e000 JavaThread "elasticsearch[Arkady Rossovich][fetch_shard_store][T#2]" daemon [_thread_blocked, id=10695, stack(0x00007f71b111f000,0x00007f71b1220000)]
  0x00007f71982a7800 JavaThread "elasticsearch[Arkady Rossovich][search][T#2]" daemon [_thread_blocked, id=1999, stack(0x00007f71b0917000,0x00007f71b0a18000)]
  0x00007f7194094800 JavaThread "elasticsearch[Arkady Rossovich][search][T#1]" daemon [_thread_blocked, id=1998, stack(0x00007f718356f000,0x00007f7183670000)]
  0x00007f71ac0a3800 JavaThread "elasticsearch[Arkady Rossovich][listener][T#1]" daemon [_thread_blocked, id=1825, stack(0x00007f7190ceb000,0x00007f7190dec000)]
  0x00007f7198a6b000 JavaThread "elasticsearch[Arkady Rossovich][management][T#5]" daemon [_thread_blocked, id=1638, stack(0x00007f718524f000,0x00007f7185350000)]
  0x00007f7198431800 JavaThread "elasticsearch[Arkady Rossovich][management][T#4]" daemon [_thread_blocked, id=1637, stack(0x00007f7185357000,0x00007f7185458000)]
  0x00007f7198690800 JavaThread "elasticsearch[Arkady Rossovich][management][T#3]" daemon [_thread_blocked, id=1636, stack(0x00007f718cf27000,0x00007f718d028000)]
  0x00007f719c00e800 JavaThread "elasticsearch[Arkady Rossovich][merge][T#1]" daemon [_thread_blocked, id=1632, stack(0x00007f7190dec000,0x00007f7190eed000)]
  0x00007f71985c5000 JavaThread "elasticsearch[Arkady Rossovich][management][T#2]" daemon [_thread_blocked, id=1631, stack(0x00007f7190eed000,0x00007f7190fee000)]
  0x00007f719c00a000 JavaThread "elasticsearch[Arkady Rossovich][refresh][T#1]" daemon [_thread_blocked, id=1630, stack(0x00007f7190fee000,0x00007f71910ef000)]
  0x00007f719c006800 JavaThread "elasticsearch[Arkady Rossovich][flush][T#1]" daemon [_thread_blocked, id=1629, stack(0x00007f71910ef000,0x00007f71911f0000)]
  0x0000000000c01800 JavaThread "elasticsearch[Arkady Rossovich][warmer][T#1]" daemon [_thread_blocked, id=1628, stack(0x00007f71b05e7000,0x00007f71b06e8000)]
  0x00007f719827b800 JavaThread "elasticsearch[Arkady Rossovich][bulk][T#1]" daemon [_thread_blocked, id=1625, stack(0x00007f71b0a18000,0x00007f71b0b19000)]
  0x00007f71bc00b000 JavaThread "DestroyJavaVM" [_thread_blocked, id=1591, stack(0x00007f71c2a72000,0x00007f71c2b73000)]
  0x00007f71bcbc6000 JavaThread "elasticsearch[keepAlive/1.7.1]" [_thread_blocked, id=1623, stack(0x00007f71b0c1a000,0x00007f71b0d1b000)]
  0x00007f71bcad8000 JavaThread "elasticsearch[Arkady Rossovich][http_server_boss][T#1]{New I/O server boss #9}" daemon [_thread_in_native, id=1622, stack(0x00007f71b0d1b000,0x00007f71b0e1c000)]
  0x00007f71bcad6000 JavaThread "elasticsearch[Arkady Rossovich][http_server_worker][T#2]{New I/O worker #8}" daemon [_thread_in_native, id=1621, stack(0x00007f71b0e1c000,0x00007f71b0f1d000)]
  0x00007f71bcad5000 JavaThread "elasticsearch[Arkady Rossovich][http_server_worker][T#1]{New I/O worker #7}" daemon [_thread_in_native, id=1620, stack(0x00007f71b0f1d000,0x00007f71b101e000)]
  0x00007f7194088800 JavaThread "elasticsearch[Arkady Rossovich][management][T#1]" daemon [_thread_blocked, id=1619, stack(0x00007f71b101e000,0x00007f71b111f000)]
  0x0000000000c13000 JavaThread "elasticsearch[Arkady Rossovich][transport_client_timer][T#1]{Hashed wheel timer #1}" daemon [_thread_blocked, id=1616, stack(0x00007f71b1321000,0x00007f71b1422000)]
  0x00007f71bcad1800 JavaThread "elasticsearch[Arkady Rossovich][clusterService#updateTask][T#1]" daemon [_thread_blocked, id=1613, stack(0x00007f71b1624000,0x00007f71b1725000)]
  0x00007f71bcace000 JavaThread "elasticsearch[Arkady Rossovich][discovery#multicast#receiver][T#1]" daemon [_thread_in_native, id=1612, stack(0x00007f71b1725000,0x00007f71b1826000)]
  0x00007f71bc8f6800 JavaThread "elasticsearch[Arkady Rossovich][[http_server_boss.default]][T#1]{New I/O server boss #6}" daemon [_thread_in_native, id=1611, stack(0x00007f71b1826000,0x00007f71b1927000)]
  0x00007f71bc946000 JavaThread "elasticsearch[Arkady Rossovich][[http_server_worker.default]][T#2]{New I/O worker #5}" daemon [_thread_blocked, id=1610, stack(0x00007f71b1927000,0x00007f71b1a28000)]
  0x00007f71bc942000 JavaThread "elasticsearch[Arkady Rossovich][[http_server_worker.default]][T#1]{New I/O worker #4}" daemon [_thread_blocked, id=1609, stack(0x00007f71b1a28000,0x00007f71b1b29000)]
  0x00007f71bc948800 JavaThread "elasticsearch[Arkady Rossovich][transport_client_boss][T#1]{New I/O boss #3}" daemon [_thread_in_native, id=1608, stack(0x00007f71b1b29000,0x00007f71b1c2a000)]
  0x00007f71bc8ec000 JavaThread "elasticsearch[Arkady Rossovich][transport_client_worker][T#2]{New I/O worker #2}" daemon [_thread_blocked, id=1607, stack(0x00007f71b1c2a000,0x00007f71b1d2b000)]
  0x00007f71bc8e6800 JavaThread "elasticsearch[Arkady Rossovich][transport_client_worker][T#1]{New I/O worker #1}" daemon [_thread_blocked, id=1606, stack(0x00007f71b1d2b000,0x00007f71b1e2c000)]
  0x00007f71bc8e7800 JavaThread "elasticsearch[Arkady Rossovich][[ttl_expire]]" daemon [_thread_blocked, id=1605, stack(0x00007f71b1e2c000,0x00007f71b1f2d000)]
  0x00007f71bc8e3800 JavaThread "elasticsearch[Arkady Rossovich][master_mapping_updater]" [_thread_blocked, id=1604, stack(0x00007f71b1f2d000,0x00007f71b202e000)]
  0x00007f71bc8a7800 JavaThread "elasticsearch[Arkady Rossovich][scheduler][T#1]" daemon [_thread_blocked, id=1603, stack(0x00007f71b202e000,0x00007f71b212f000)]
  0x00007f71bc3b8800 JavaThread "elasticsearch[Arkady Rossovich][[timer]]" daemon [_thread_blocked, id=1602, stack(0x00007f71b212f000,0x00007f71b2230000)]
  0x00007f71bc0c4000 JavaThread "Service Thread" daemon [_thread_blocked, id=1600, stack(0x00007f71b2ec3000,0x00007f71b2fc4000)]
  0x00007f71bc0c1000 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=1599, stack(0x00007f71b2fc4000,0x00007f71b30c5000)]
  0x00007f71bc0bf000 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=1598, stack(0x00007f71b30c5000,0x00007f71b31c6000)]
  0x00007f71bc0bc800 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=1597, stack(0x00007f71b31c6000,0x00007f71b32c7000)]
  0x00007f71bc0ba800 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=1596, stack(0x00007f71b32c7000,0x00007f71b33c8000)]
  0x00007f71bc099800 JavaThread "Finalizer" daemon [_thread_blocked, id=1595, stack(0x00007f71b3555000,0x00007f71b3656000)]
  0x00007f71bc097800 JavaThread "Reference Handler" daemon [_thread_blocked, id=1594, stack(0x00007f71b3656000,0x00007f71b3757000)]

Other Threads:
=&gt;0x00007f71bc093000 VMThread [stack: 0x00007f71b3757000,0x00007f71b3858000] [id=1593]
  0x00007f71bc0cf000 WatcherThread [stack: 0x00007f71b2dc2000,0x00007f71b2ec3000] [id=1601]

VM state:at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread:  ([mutex/lock_event])
[0x00007f71bc007690] Threads_lock - owner thread: 0x00007f71bc093000
[0x00007f71bc007c10] Heap_lock - owner thread: 0x00007f719827b800

Heap
 def new generation   total 76672K, used 72673K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,  99% used [0x000000077ae00000, 0x000000077f08f8b8, 0x000000077f090000)
  from space 8512K,  53% used [0x000000077f090000, 0x000000077f4f8e78, 0x000000077f8e0000)
  to   space 8512K,   8% used [0x000000077f8e0000, 0x000000077f995c60, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 464735K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)

Card table byte_map: [0x00007f71b87ae000,0x00007f71b8bd8000] byte_map_base: 0x00007f71b4bd7000

Polling page: 0x00007f71c2b76000

Code Cache  [0x00007f71b8bd8000, 0x00007f71b9468000, 0x00007f71bbbd8000)
 total_blobs=2951 nmethods=2472 adapters=431 free_code_cache=40625Kb largest_free_block=41368704

Compilation events (10 events):
Event: 202070.366 Thread 0x00007f71bc0bf000 2836 %           org.elasticsearch.index.deletionpolicy.SnapshotIndexCommit::&lt;init&gt; @ 29 (80 bytes)
Event: 202070.378 Thread 0x00007f71bc0bf000 nmethod 2836% 0x00007f71b938a350 code [0x00007f71b938a500, 0x00007f71b938ac38]
Event: 202072.941 Thread 0x00007f71bc0c1000 2837             org.elasticsearch.index.deletionpolicy.SnapshotIndexCommit::&lt;init&gt; (80 bytes)
Event: 202072.956 Thread 0x00007f71bc0c1000 nmethod 2837 0x00007f71b8eef810 code [0x00007f71b8eef9c0, 0x00007f71b8ef00d8]
Event: 204901.650 Thread 0x00007f71bc0bf000 2838 %           org.apache.lucene.index.SegmentInfos::files @ 39 (110 bytes)
Event: 204901.655 Thread 0x00007f71bc0bf000 nmethod 2838% 0x00007f71b9233090 code [0x00007f71b9233240, 0x00007f71b9233678]
Event: 205079.939 Thread 0x00007f71bc0c1000 2839             org.apache.lucene.index.SegmentInfos::files (110 bytes)
Event: 205079.952 Thread 0x00007f71bc0c1000 nmethod 2839 0x00007f71b92b4f50 code [0x00007f71b92b5120, 0x00007f71b92b5610]
Event: 205690.673 Thread 0x00007f71bc0bf000 2840             org.elasticsearch.action.ActionRequest::&lt;init&gt; (10 bytes)
Event: 205690.674 Thread 0x00007f71bc0bf000 nmethod 2840 0x00007f71b8eef610 code [0x00007f71b8eef740, 0x00007f71b8eef798]

GC Heap History (10 events):
Event: 210232.138 GC heap after
Heap after GC invocations=38836 (full 12):
 def new generation   total 76672K, used 3854K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,   0% used [0x000000077ae00000, 0x000000077ae00000, 0x000000077f090000)
  from space 8512K,  45% used [0x000000077f090000, 0x000000077f453b08, 0x000000077f8e0000)
  to   space 8512K,   0% used [0x000000077f8e0000, 0x000000077f8e0000, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 463296K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
}
Event: 210233.808 GC heap before
{Heap before GC invocations=38836 (full 12):
 def new generation   total 76672K, used 72014K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K, 100% used [0x000000077ae00000, 0x000000077f090000, 0x000000077f090000)
  from space 8512K,  45% used [0x000000077f090000, 0x000000077f453b08, 0x000000077f8e0000)
  to   space 8512K,   0% used [0x000000077f8e0000, 0x000000077f8e0000, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 463296K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
Event: 210233.817 GC heap after
Heap after GC invocations=38837 (full 12):
 def new generation   total 76672K, used 3466K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,   0% used [0x000000077ae00000, 0x000000077ae00000, 0x000000077f090000)
  from space 8512K,  40% used [0x000000077f8e0000, 0x000000077fc42af0, 0x0000000780130000)
  to   space 8512K,   0% used [0x000000077f090000, 0x000000077f090000, 0x000000077f8e0000)
 concurrent mark-sweep generation total 2011968K, used 463500K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
}
Event: 210243.834 GC heap before
{Heap before GC invocations=38837 (full 12):
 def new generation   total 76672K, used 71626K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K, 100% used [0x000000077ae00000, 0x000000077f090000, 0x000000077f090000)
  from space 8512K,  40% used [0x000000077f8e0000, 0x000000077fc42af0, 0x0000000780130000)
  to   space 8512K,   0% used [0x000000077f090000, 0x000000077f090000, 0x000000077f8e0000)
 concurrent mark-sweep generation total 2011968K, used 463500K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
Event: 210243.845 GC heap after
Heap after GC invocations=38838 (full 12):
 def new generation   total 76672K, used 5430K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,   0% used [0x000000077ae00000, 0x000000077ae00000, 0x000000077f090000)
  from space 8512K,  63% used [0x000000077f090000, 0x000000077f5ddb00, 0x000000077f8e0000)
  to   space 8512K,   0% used [0x000000077f8e0000, 0x000000077f8e0000, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 463513K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
}
Event: 210247.974 GC heap before
{Heap before GC invocations=38838 (full 12):
 def new generation   total 76672K, used 73590K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K, 100% used [0x000000077ae00000, 0x000000077f090000, 0x000000077f090000)
  from space 8512K,  63% used [0x000000077f090000, 0x000000077f5ddb00, 0x000000077f8e0000)
  to   space 8512K,   0% used [0x000000077f8e0000, 0x000000077f8e0000, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 463513K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
Event: 210247.986 GC heap after
Heap after GC invocations=38839 (full 12):
 def new generation   total 76672K, used 3998K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,   0% used [0x000000077ae00000, 0x000000077ae00000, 0x000000077f090000)
  from space 8512K,  46% used [0x000000077f8e0000, 0x000000077fcc78a0, 0x0000000780130000)
  to   space 8512K,   0% used [0x000000077f090000, 0x000000077f090000, 0x000000077f8e0000)
 concurrent mark-sweep generation total 2011968K, used 464733K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
}
Event: 210254.045 GC heap before
{Heap before GC invocations=38839 (full 12):
 def new generation   total 76672K, used 72158K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K, 100% used [0x000000077ae00000, 0x000000077f090000, 0x000000077f090000)
  from space 8512K,  46% used [0x000000077f8e0000, 0x000000077fcc78a0, 0x0000000780130000)
  to   space 8512K,   0% used [0x000000077f090000, 0x000000077f090000, 0x000000077f8e0000)
 concurrent mark-sweep generation total 2011968K, used 464733K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
Event: 210254.054 GC heap after
Heap after GC invocations=38840 (full 12):
 def new generation   total 76672K, used 4515K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,   0% used [0x000000077ae00000, 0x000000077ae00000, 0x000000077f090000)
  from space 8512K,  53% used [0x000000077f090000, 0x000000077f4f8e78, 0x000000077f8e0000)
  to   space 8512K,   0% used [0x000000077f8e0000, 0x000000077f8e0000, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 464733K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)
}
Event: 210255.347 GC heap before
{Heap before GC invocations=38840 (full 12):
 def new generation   total 76672K, used 72673K [0x000000077ae00000, 0x0000000780130000, 0x0000000780130000)
  eden space 68160K,  99% used [0x000000077ae00000, 0x000000077f08f8b8, 0x000000077f090000)
  from space 8512K,  53% used [0x000000077f090000, 0x000000077f4f8e78, 0x000000077f8e0000)
  to   space 8512K,   0% used [0x000000077f8e0000, 0x000000077f8e0000, 0x0000000780130000)
 concurrent mark-sweep generation total 2011968K, used 464733K [0x0000000780130000, 0x00000007fae00000, 0x00000007fae00000)
 concurrent-mark-sweep perm gen total 65092K, used 39053K [0x00000007fae00000, 0x00000007fed91000, 0x0000000800000000)

Deoptimization events (10 events):
Event: 192760.721 Thread 0x00007f719c016800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b93309d4 method=java.util.concurrent.locks.AbstractQueuedSynchronizer.apparentlyFirstQueuedIsExclusive()Z @ 6
Event: 192761.372 Thread 0x00007f719818c800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b9101134 method=org.elasticsearch.common.jackson.dataformat.smile.SmileParser._decodeLongAscii()V @ 105
Event: 192761.372 Thread 0x00007f719818c800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b8d7dcd8 method=org.elasticsearch.common.jackson.core.util.TextBuffer.emptyAndGetCurrentSegment()[C @ 34
Event: 192772.899 Thread 0x00007f719c016800 Uncommon trap: reason=array_check action=maybe_recompile pc=0x00007f71b932ac6c method=org.apache.lucene.util.ArrayUtil.swap([Ljava/lang/Object;II)V @ 13
Event: 192772.921 Thread 0x00007f719c016800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b926cd28 method=org.apache.lucene.codecs.compressing.CompressingStoredFieldsIndexReader.&lt;init&gt;(Lorg/apache/lucene/store/IndexInput;Lorg/apache/lucene/index/SegmentInfo;)V @ 76
Event: 192775.050 Thread 0x0000000000dcd000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b92df51c method=org.elasticsearch.common.compress.lzf.ChunkEncoder.encodeAndWriteChunk([BIILjava/io/OutputStream;)V @ 71
Event: 192778.234 Thread 0x00007f719c016800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b93645c0 method=org.elasticsearch.common.jackson.core.util.TextBuffer.emptyAndGetCurrentSegment()[C @ 34
Event: 192786.474 Thread 0x00007f71bcad1800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b9035e68 method=org.elasticsearch.index.mapper.core.AbstractFieldMapper.doXContentBody(Lorg/elasticsearch/common/xcontent/XContentBuilder;ZLorg/elasticsearch/common/xcontent/ToXContent$Para
Event: 201720.217 Thread 0x00007f71bc8a7800 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b9117600 method=java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(I)V @ 19
Event: 204739.751 Thread 0x00007f719c00a000 Uncommon trap: reason=unstable_if action=reinterpret pc=0x00007f71b9119ff0 method=org.apache.lucene.index.SegmentInfos.files(Lorg/apache/lucene/store/Directory;Z)Ljava/util/Collection; @ 89

Internal exceptions (10 events):
Event: 209702.553 Thread 0x00007f71bcace000 Threw 0x000000077c4d5018 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 209762.583 Thread 0x00007f71bcace000 Threw 0x000000077df72a50 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 209822.628 Thread 0x00007f71bcace000 Threw 0x000000077d466bf0 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 209882.681 Thread 0x00007f71bcace000 Threw 0x000000077cb61078 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 209942.728 Thread 0x00007f71bcace000 Threw 0x000000077d5d8098 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 210002.779 Thread 0x00007f71bcace000 Threw 0x000000077dcdf858 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 210062.828 Thread 0x00007f71bcace000 Threw 0x000000077d1d0ad8 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 210122.887 Thread 0x00007f71bcace000 Threw 0x000000077e141ab0 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 210182.935 Thread 0x00007f71bcace000 Threw 0x000000077ea581a8 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743
Event: 210242.981 Thread 0x00007f71bcace000 Threw 0x000000077cfa8508 at /HUDSON/workspace/7u-2-build-linux-amd64/jdk7u80/2329/hotspot/src/share/vm/prims/jni.cpp:743

Events (10 events):
Event: 210252.503 Executing VM operation: RevokeBias
Event: 210252.503 Executing VM operation: RevokeBias done
Event: 210252.504 Executing VM operation: RevokeBias
Event: 210252.504 Executing VM operation: RevokeBias done
Event: 210254.045 Executing VM operation: GenCollectForAllocation
Event: 210254.054 Executing VM operation: GenCollectForAllocation done
Event: 210254.930 Thread 0x00007f71a40de000 Thread added: 0x00007f71a40de000
Event: 210254.930 Executing VM operation: RevokeBias
Event: 210254.931 Executing VM operation: RevokeBias done
Event: 210255.345 Executing VM operation: GenCollectForAllocation


Dynamic libraries:
00400000-00401000 r-xp 00000000 08:01 1234574                            /usr/lib/jvm/java-7-oracle/jre/bin/java
00600000-00601000 rw-p 00000000 08:01 1234574                            /usr/lib/jvm/java-7-oracle/jre/bin/java
009a0000-018ec000 rw-p 00000000 00:00 0                                  [heap]
77ae00000-7fed91000 rw-p 00000000 00:00 0 
7fed91000-800000000 rw-p 00000000 00:00 0 
7f7166040000-7f7166486000 r--s 00000000 08:01 1970876                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/3/index/_2ztt_Lucene41_0.tim
7f7166488000-7f7166508000 r--s 00000000 08:01 1970846                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/3/index/_3rsi_Lucene410_0.dvd
7f7166508000-7f7166fab000 r--s 00000000 08:01 1970892                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/3/index/_3rsi_Lucene41_0.tim
7f7166fb0000-7f7167065000 r--s 00000000 08:01 1970863                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/3/index/_2141_Lucene410_0.dvd
7f7167068000-7f7167e9c000 r--s 00000000 08:01 1970899                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/3/index/_2141_Lucene41_0.tim
7f716aea8000-7f716b670000 r--s 00000000 08:01 1970917                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.03/0/index/_39b_Lucene41_0.tim
7f716b8b8000-7f716bc05000 r--s 00000000 08:01 1970810                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/3/index/_5k2g_Lucene410_0.dvd
7f716bc08000-7f716f60e000 r--s 00000000 08:01 1970811                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/3/index/_5k2g_Lucene41_0.tim
7f716f610000-7f716fc0e000 r--s 00000000 08:01 1970716                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.28/0/index/_7cg_Lucene41_0.tim
7f716fff8000-7f7170200000 r--s 00000000 08:01 1970772                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.03/0/index/_4k2_Lucene41_0.tim
7f7170200000-7f717035d000 r--s 00000000 08:01 1970768                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.03/0/index/_3o3_Lucene41_0.tim
7f7170360000-7f7170457000 r--s 00000000 08:01 1970688                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.27/0/index/_9du_Lucene41_0.tim
7f7170458000-7f7170b07000 r--s 00000000 08:01 1970661                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.27/0/index/_7as_Lucene41_0.tim
7f7170b08000-7f71710fe000 r--s 00000000 08:01 1970610                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.26/0/index/_cf0_Lucene41_0.tim
7f7171100000-7f7171748000 r--s 00000000 08:01 1970606                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.26/0/index/_6fy_Lucene41_0.tim
7f7171748000-7f7171c8d000 r--s 00000000 08:01 1970545                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.25/0/index/_azm_Lucene41_0.tim
7f7171c90000-7f71722a6000 r--s 00000000 08:01 1970574                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.25/0/index/_66i_Lucene41_0.tim
7f71722a8000-7f7172860000 r--s 00000000 08:01 1970461                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.23/0/index/_8cc_Lucene41_0.tim
7f7172860000-7f7172e44000 r--s 00000000 08:01 1970330                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.23/0/index/_50u_Lucene41_0.tim
7f7172e78000-7f7173047000 r--s 00000000 08:01 1970573                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.25/0/index/_cu0_Lucene41_0.tim
7f7173050000-7f7173147000 r--s 00000000 08:01 1970666                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.27/0/index/_9e4_Lucene41_0.tim
7f7173148000-7f7173376000 r--s 00000000 08:01 1970605                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.26/0/index/_ei0_Lucene41_0.tim
7f7173378000-7f71739bc000 r--s 00000000 08:01 1970354                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_9zy_Lucene41_0.tim
7f71739c0000-7f7173de8000 r--s 00000000 08:01 1231884                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_4s7y_Lucene41_0.tim
7f7173de8000-7f717440c000 r--s 00000000 08:01 1970412                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_74v_Lucene41_0.tim
7f7174410000-7f7174ba9000 r--s 00000000 08:01 1970325                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_3ho_Lucene41_0.tim
7f7174bb0000-7f71751d5000 r--s 00000000 08:01 1970265                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_a3a_Lucene41_0.tim
7f71751d8000-7f71759af000 r--s 00000000 08:01 1970302                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_3mn_Lucene41_0.tim
7f71759b0000-7f7176471000 r--s 00000000 08:01 1478884                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/2/index/_3rco_Lucene41_0.tim
7f7176478000-7f7176f13000 r--s 00000000 08:01 1231783                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/1/index/_3s0u_Lucene41_0.tim
7f7176f58000-7f7176f86000 r--s 00000000 08:01 1970868                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/3/index/_2ztt_Lucene410_0.dvd
7f7176f86000-7f7176f89000 ---p 00000000 00:00 0 
7f7176f89000-7f7177087000 rw-p 00000000 00:00 0 
7f7177087000-7f717708a000 ---p 00000000 00:00 0 
7f717708a000-7f7177188000 rw-p 00000000 00:00 0 
7f7177188000-7f7177766000 r--s 00000000 08:01 1970288                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_7ay_Lucene41_0.tim
7f7177768000-7f71777e8000 r--s 00000000 08:01 1231792                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/1/index/_3s0u_Lucene410_0.dvd
7f7177800000-7f7177825000 r--s 00000000 08:01 1232094                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_3kxk_Lucene410_0.dvd
7f7177828000-7f7177b8b000 r--s 00000000 08:01 1232091                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_3kxk_Lucene41_0.tim
7f7177b90000-7f7177b91000 r--s 00000000 08:01 1479171                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.30/0/index/_82q_Lucene410_0.dvd
7f7177b98000-7f7177e52000 r--s 00000000 08:01 1479157                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.30/0/index/_82q_Lucene41_0.tim
7f7177e58000-7f7177e59000 r--s 00000000 08:01 1970197                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.24/0/index/_7go_Lucene410_0.dvd
7f7177e60000-7f71781a7000 r--s 00000000 08:01 1970201                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.24/0/index/_7go_Lucene41_0.tim
7f71781a8000-7f71781a9000 r--s 00000000 08:01 1970217                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.24/0/index/_44w_Lucene410_0.dvd
7f71781b0000-7f71787cb000 r--s 00000000 08:01 1970186                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.24/0/index/_44w_Lucene41_0.tim
7f71787d0000-7f71787d1000 r--s 00000000 08:01 1491455                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.15/0/index/_9kb_Lucene410_0.dvd
7f71787d8000-7f7178b38000 r--s 00000000 08:01 1479571                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.15/0/index/_9kb_Lucene41_0.tim
7f7178b38000-7f7178b39000 r--s 00000000 08:01 1491475                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.15/0/index/_76q_Lucene410_0.dvd
7f7178b40000-7f7178d8e000 r--s 00000000 08:01 1491458                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.15/0/index/_76q_Lucene41_0.tim
7f7178d90000-7f7178d91000 r--s 00000000 08:01 1479449                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.15/0/index/_65r_Lucene410_0.dvd
7f7178d98000-7f717999c000 r--s 00000000 08:01 1478984                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.15/0/index/_65r_Lucene41_0.tim
7f71799a0000-7f71799a1000 r--s 00000000 08:01 1479298                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_foo_Lucene410_0.dvd
7f71799a8000-7f7179cd7000 r--s 00000000 08:01 1479295                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_foo_Lucene41_0.tim
7f7179cd8000-7f7179cd9000 r--s 00000000 08:01 1479183                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_i8x_Lucene410_0.dvd
7f7179ce0000-7f717a6d0000 r--s 00000000 08:01 1479307                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_i8x_Lucene41_0.tim
7f717a6d0000-7f717a6d1000 r--s 00000000 08:01 1479275                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_ayb_Lucene410_0.dvd
7f717a6d8000-7f717af2d000 r--s 00000000 08:01 1491459                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_ayb_Lucene41_0.tim
7f717af30000-7f717af31000 r--s 00000000 08:01 1479200                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_6c5_Lucene410_0.dvd
7f717af38000-7f717ba51000 r--s 00000000 08:01 1479455                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.16/0/index/_6c5_Lucene41_0.tim
7f717ba58000-7f717ba59000 r--s 00000000 08:01 1479119                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_d6y_Lucene410_0.dvd
7f717ba60000-7f717bdd3000 r--s 00000000 08:01 1479139                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_d6y_Lucene41_0.tim
7f717bdd8000-7f717bdd9000 r--s 00000000 08:01 1479111                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_brt_Lucene410_0.dvd
7f717bde0000-7f717c994000 r--s 00000000 08:01 1479025                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_brt_Lucene41_0.tim
7f717c998000-7f717c99a000 r--s 00000000 08:01 1479137                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_6lc_Lucene410_0.dvd
7f717c9a0000-7f717d8f6000 r--s 00000000 08:01 1479193                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.17/0/index/_6lc_Lucene41_0.tim
7f717d8f8000-7f717d8f9000 r--s 00000000 08:01 1479062                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.22/0/index/_3ti_Lucene410_0.dvd
7f717d900000-7f717dc9a000 r--s 00000000 08:01 1479074                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.22/0/index/_3ti_Lucene41_0.tim
7f717dca0000-7f717dca1000 r--s 00000000 08:01 1479020                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.20/0/index/_54b_Lucene410_0.dvd
7f717dca8000-7f717de82000 r--s 00000000 08:01 1479016                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.20/0/index/_54b_Lucene41_0.tim
7f717de88000-7f717de8a000 r--s 00000000 08:01 1479001                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.20/0/index/_4gn_Lucene410_0.dvd
7f717de90000-7f717e8bb000 r--s 00000000 08:01 1479014                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.20/0/index/_4gn_Lucene41_0.tim
7f717e8bf000-7f717e8c2000 ---p 00000000 00:00 0 
7f717e8c2000-7f717e9c0000 rw-p 00000000 00:00 0 
7f717e9c0000-7f717ed1a000 r--s 00000000 08:01 1478956                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/2/index/_5jlq_Lucene410_0.dvd
7f717ed20000-7f71827f6000 r--s 00000000 08:01 1478963                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/2/index/_5jlq_Lucene41_0.tim
7f71827f8000-7f71827f9000 r--s 00000000 08:01 1478896                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.30/0/index/_6o5_Lucene410_0.dvd
7f7182800000-7f718299b000 r--s 00000000 08:01 1478890                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.30/0/index/_6o5_Lucene41_0.tim
7f71829a0000-7f71829a3000 r--s 00000000 08:01 1478929                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.30/0/index/_64e_Lucene410_0.dvd
7f71829a8000-7f718356c000 r--s 00000000 08:01 1478961                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.30/0/index/_64e_Lucene41_0.tim
7f718356f000-7f7183572000 ---p 00000000 00:00 0 
7f7183572000-7f7183670000 rw-p 00000000 00:00 0                          [stack:1998]
7f7183688000-7f718388a000 r--s 00000000 08:01 1970512                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.23/0/index/_9gm_Lucene41_0.tim
7f718388f000-7f7183892000 ---p 00000000 00:00 0 
7f7183892000-7f7183990000 rw-p 00000000 00:00 0 
7f7183990000-7f71839be000 r--s 00000000 08:01 1478862                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/2/index/_2z17_Lucene410_0.dvd
7f71839c0000-7f7183e02000 r--s 00000000 08:01 1478863                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/2/index/_2z17_Lucene41_0.tim
7f7183e35000-7f7183e38000 ---p 00000000 00:00 0 
7f7183e38000-7f7183f36000 rw-p 00000000 00:00 0 
7f7183f36000-7f7183f39000 ---p 00000000 00:00 0 
7f7183f39000-7f7184037000 rw-p 00000000 00:00 0 
7f7184037000-7f718403a000 ---p 00000000 00:00 0 
7f718403a000-7f7184138000 rw-p 00000000 00:00 0 
7f7184138000-7f7184354000 r--s 00000000 08:01 1970388                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_5hn_Lucene41_0.tim
7f7184358000-7f718440d000 r--s 00000000 08:01 1478808                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/2/index/_201z_Lucene410_0.dvd
7f7184410000-7f7185248000 r--s 00000000 08:01 1478874                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/2/index/_201z_Lucene41_0.tim
7f718524f000-7f7185252000 ---p 00000000 00:00 0 
7f7185252000-7f7185350000 rw-p 00000000 00:00 0                          [stack:1638]
7f7185350000-7f7185351000 r--s 00000000 08:01 1478733                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_9sy_Lucene410_0.dvd
7f7185357000-7f718535a000 ---p 00000000 00:00 0 
7f718535a000-7f7185458000 rw-p 00000000 00:00 0                          [stack:1637]
7f7185458000-7f718581e000 r--s 00000000 08:01 1478728                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_9sy_Lucene41_0.tim
7f7185820000-7f7185821000 r--s 00000000 08:01 1478741                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_cf6_Lucene410_0.dvd
7f7185828000-7f7185f78000 r--s 00000000 08:01 1478844                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_cf6_Lucene41_0.tim
7f7185f78000-7f7185f79000 r--s 00000000 08:01 1478714                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_825_Lucene410_0.dvd
7f7185f80000-7f71864af000 r--s 00000000 08:01 1478742                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_825_Lucene41_0.tim
7f71864b0000-7f71864b1000 r--s 00000000 08:01 1478694                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_4o5_Lucene410_0.dvd
7f71864b8000-7f7186e18000 r--s 00000000 08:01 1478689                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.19/0/index/_4o5_Lucene41_0.tim
7f7186e18000-7f7186e19000 r--s 00000000 08:01 1478759                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.29/0/index/_4wo_Lucene410_0.dvd
7f7186e20000-7f7187272000 r--s 00000000 08:01 1478761                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.29/0/index/_4wo_Lucene41_0.tim
7f7187278000-7f7187279000 r--s 00000000 08:01 1478618                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_bht_Lucene410_0.dvd
7f7187280000-7f7187555000 r--s 00000000 08:01 1478613                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_bht_Lucene41_0.tim
7f7187558000-7f7187559000 r--s 00000000 08:01 1478641                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_dqy_Lucene410_0.dvd
7f7187560000-7f7187f1e000 r--s 00000000 08:01 1478670                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_dqy_Lucene41_0.tim
7f7187f20000-7f7187f21000 r--s 00000000 08:01 1478597                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_7ud_Lucene410_0.dvd
7f7187f28000-7f7188700000 r--s 00000000 08:01 1478598                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_7ud_Lucene41_0.tim
7f7188700000-7f71891bf000 r--s 00000000 08:01 1478666                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_4jg_Lucene41_0.tim
7f71891c0000-7f718950e000 r--s 00000000 08:01 1232069                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/1/index/_5k5v_Lucene410_0.dvd
7f7189510000-7f718cf22000 r--s 00000000 08:01 1232099                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/1/index/_5k5v_Lucene41_0.tim
7f718cf27000-7f718cf2a000 ---p 00000000 00:00 0 
7f718cf2a000-7f718d028000 rw-p 00000000 00:00 0                          [stack:1636]
7f718d028000-7f718d36b000 r--s 00000000 08:01 1232064                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/0/index/_5fx5_Lucene410_0.dvd
7f718d370000-7f7190ce4000 r--s 00000000 08:01 1232062                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_0/0/index/_5fx5_Lucene41_0.tim
7f7190ceb000-7f7190cee000 ---p 00000000 00:00 0 
7f7190cee000-7f7190dec000 rw-p 00000000 00:00 0                          [stack:1825]
7f7190dec000-7f7190def000 ---p 00000000 00:00 0 
7f7190def000-7f7190eed000 rw-p 00000000 00:00 0                          [stack:1632]
7f7190eed000-7f7190ef0000 ---p 00000000 00:00 0 
7f7190ef0000-7f7190fee000 rw-p 00000000 00:00 0                          [stack:1631]
7f7190fee000-7f7190ff1000 ---p 00000000 00:00 0 
7f7190ff1000-7f71910ef000 rw-p 00000000 00:00 0                          [stack:1630]
7f71910ef000-7f71910f2000 ---p 00000000 00:00 0 
7f71910f2000-7f71911f0000 rw-p 00000000 00:00 0                          [stack:1629]
7f71911f5000-7f71911f8000 ---p 00000000 00:00 0 
7f71911f8000-7f71912f6000 rw-p 00000000 00:00 0 
7f71912f6000-7f71912f9000 ---p 00000000 00:00 0 
7f71912f9000-7f71913f7000 rw-p 00000000 00:00 0                          [stack:10712]
7f71913f7000-7f71913fa000 ---p 00000000 00:00 0 
7f71913fa000-7f71914f8000 rw-p 00000000 00:00 0 
7f71914f8000-7f7191526000 r--s 00000000 08:01 1231847                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/1/index/_309n_Lucene410_0.dvd
7f7191528000-7f719196c000 r--s 00000000 08:01 1231873                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/1/index/_309n_Lucene41_0.tim
7f7191970000-7f7191972000 r--s 00000000 08:01 1970921                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.03/0/index/_39b_Lucene410_0.dvd
7f7191988000-7f71927d0000 r--s 00000000 08:01 1232036                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/1/index/_212o_Lucene41_0.tim
7f71927d0000-7f71931b8000 r--s 00000000 08:01 1231732                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_36q5_Lucene41_0.tim
7f71931b8000-7f7193ffd000 r--s 00000000 08:01 1231877                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_206z_Lucene41_0.tim
7f7194000000-7f71940c1000 rw-p 00000000 00:00 0 
7f71940c1000-7f7198000000 ---p 00000000 00:00 0 
7f7198000000-7f71998c6000 rw-p 00000000 00:00 0 
7f71998c6000-7f719c000000 ---p 00000000 00:00 0 
7f719c000000-7f719c264000 rw-p 00000000 00:00 0 
7f719c264000-7f71a0000000 ---p 00000000 00:00 0 
7f71a0000000-7f71a0023000 rw-p 00000000 00:00 0 
7f71a0023000-7f71a4000000 ---p 00000000 00:00 0 
7f71a4000000-7f71a43e3000 rw-p 00000000 00:00 0 
7f71a43e3000-7f71a8000000 ---p 00000000 00:00 0 
7f71a8000000-7f71a8120000 rw-p 00000000 00:00 0 
7f71a8120000-7f71ac000000 ---p 00000000 00:00 0 
7f71ac000000-7f71ac17f000 rw-p 00000000 00:00 0 
7f71ac17f000-7f71b0000000 ---p 00000000 00:00 0 
7f71b0000000-7f71b0001000 r--s 00000000 08:01 1478628                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.18/0/index/_4jg_Lucene410_0.dvd
7f71b0008000-7f71b0009000 r--s 00000000 08:01 1970761                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.03/0/index/_4k2_Lucene410_0.dvd
7f71b0010000-7f71b0011000 r--s 00000000 08:01 1970774                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.03/0/index/_3o3_Lucene410_0.dvd
7f71b0018000-7f71b0019000 r--s 00000000 08:01 1970720                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.28/0/index/_7cg_Lucene410_0.dvd
7f71b0020000-7f71b0021000 r--s 00000000 08:01 1970690                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.27/0/index/_9du_Lucene410_0.dvd
7f71b0028000-7f71b0029000 r--s 00000000 08:01 1970667                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.27/0/index/_9e4_Lucene410_0.dvd
7f71b0030000-7f71b0031000 r--s 00000000 08:01 1970675                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.27/0/index/_7as_Lucene410_0.dvd
7f71b0038000-7f71b0039000 r--s 00000000 08:01 1970617                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.26/0/index/_ei0_Lucene410_0.dvd
7f71b0040000-7f71b0041000 r--s 00000000 08:01 1970621                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.26/0/index/_cf0_Lucene410_0.dvd
7f71b0048000-7f71b0049000 r--s 00000000 08:01 1970597                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.26/0/index/_6fy_Lucene410_0.dvd
7f71b0050000-7f71b0051000 r--s 00000000 08:01 1970571                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.25/0/index/_cu0_Lucene410_0.dvd
7f71b0058000-7f71b0059000 r--s 00000000 08:01 1970544                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.25/0/index/_azm_Lucene410_0.dvd
7f71b0060000-7f71b00ce000 r--s 00000000 08:01 1970460                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.13/0/index/_by_Lucene41_0.tim
7f71b00d0000-7f71b00d2000 r--s 00000000 08:01 1970290                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_7ay_Lucene410_0.dvd
7f71b00d8000-7f71b00d9000 r--s 00000000 08:01 1970591                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.25/0/index/_66i_Lucene410_0.dvd
7f71b00e0000-7f71b00e1000 r--s 00000000 08:01 1970241                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_5pq_Lucene410_0.dvd
7f71b00e8000-7f71b0326000 r--s 00000000 08:01 1970267                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_5pq_Lucene41_0.tim
7f71b0328000-7f71b0329000 r--s 00000000 08:01 1970526                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.23/0/index/_9gm_Lucene410_0.dvd
7f71b0330000-7f71b0332000 r--s 00000000 08:01 1970305                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_74v_Lucene410_0.dvd
7f71b0338000-7f71b0339000 r--s 00000000 08:01 1970491                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.23/0/index/_8cc_Lucene410_0.dvd
7f71b0340000-7f71b0341000 r--s 00000000 08:01 1970509                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.23/0/index/_50u_Lucene410_0.dvd
7f71b0347000-7f71b034a000 ---p 00000000 00:00 0 
7f71b034a000-7f71b0448000 rw-p 00000000 00:00 0                          [stack:14719]
7f71b0448000-7f71b0472000 r--s 00000000 08:01 1231887                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_4s7y_Lucene410_0.dvd
7f71b0478000-7f71b047a000 r--s 00000000 08:01 1970363                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_3ho_Lucene410_0.dvd
7f71b0480000-7f71b0481000 r--s 00000000 08:01 1970390                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_5hn_Lucene410_0.dvd
7f71b0488000-7f71b0489000 r--s 00000000 08:01 1970455                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.09.13/0/index/_by_Lucene410_0.dvd
7f71b0490000-7f71b0492000 r--s 00000000 08:01 1970356                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.02/0/index/_9zy_Lucene410_0.dvd
7f71b0498000-7f71b049a000 r--s 00000000 08:01 1970304                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_3mn_Lucene410_0.dvd
7f71b04a0000-7f71b04a2000 r--s 00000000 08:01 1970273                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/.marvel-2015.10.01/0/index/_a3a_Lucene410_0.dvd
7f71b04a8000-7f71b052a000 r--s 00000000 08:01 1478913                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/2/index/_3rco_Lucene410_0.dvd
7f71b0530000-7f71b05e6000 r--s 00000000 08:01 1232017                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/1/index/_212o_Lucene410_0.dvd
7f71b05e7000-7f71b05ea000 ---p 00000000 00:00 0 
7f71b05ea000-7f71b06e8000 rw-p 00000000 00:00 0                          [stack:1628]
7f71b06e8000-7f71b075f000 r--s 00000000 08:01 1231758                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_36q5_Lucene410_0.dvd
7f71b0760000-7f71b0815000 r--s 00000000 08:01 1231823                    /var/opt/elasticsearch/data/graylog2/nodes/0/indices/graylog_1/0/index/_206z_Lucene410_0.dvd
7f71b0816000-7f71b0819000 ---p 00000000 00:00 0 
7f71b0819000-7f71b0917000 rw-p 00000000 00:00 0 
7f71b0917000-7f71b091a000 ---p 00000000 00:00 0 
7f71b091a000-7f71b0a18000 rw-p 00000000 00:00 0                          [stack:1999]
7f71b0a18000-7f71b0a1b000 ---p 00000000 00:00 0 
7f71b0a1b000-7f71b0b19000 rw-p 00000000 00:00 0                          [stack:1625]
7f71b0b19000-7f71b0b1c000 ---p 00000000 00:00 0 
7f71b0b1c000-7f71b0c1a000 rw-p 00000000 00:00 0 
7f71b0c1a000-7f71b0c1d000 ---p 00000000 00:00 0 
7f71b0c1d000-7f71b0d1b000 rw-p 00000000 00:00 0                          [stack:1623]
7f71b0d1b000-7f71b0d1e000 ---p 00000000 00:00 0 
7f71b0d1e000-7f71b0e1c000 rw-p 00000000 00:00 0                          [stack:1622]
7f71b0e1c000-7f71b0e1f000 ---p 00000000 00:00 0 
7f71b0e1f000-7f71b0f1d000 rw-p 00000000 00:00 0                          [stack:1621]
7f71b0f1d000-7f71b0f20000 ---p 00000000 00:00 0 
7f71b0f20000-7f71b101e000 rw-p 00000000 00:00 0                          [stack:1620]
7f71b101e000-7f71b1021000 ---p 00000000 00:00 0 
7f71b1021000-7f71b111f000 rw-p 00000000 00:00 0                          [stack:1619]
7f71b111f000-7f71b1122000 ---p 00000000 00:00 0 
7f71b1122000-7f71b1220000 rw-p 00000000 00:00 0                          [stack:10695]
7f71b1220000-7f71b1223000 ---p 00000000 00:00 0 
7f71b1223000-7f71b1321000 rw-p 00000000 00:00 0 
7f71b1321000-7f71b1324000 ---p 00000000 00:00 0 
7f71b1324000-7f71b1422000 rw-p 00000000 00:00 0                          [stack:1616]
7f71b1422000-7f71b1425000 ---p 00000000 00:00 0 
7f71b1425000-7f71b1523000 rw-p 00000000 00:00 0 
7f71b1523000-7f71b1526000 ---p 00000000 00:00 0 
7f71b1526000-7f71b1624000 rw-p 00000000 00:00 0 
7f71b1624000-7f71b1627000 ---p 00000000 00:00 0 
7f71b1627000-7f71b1725000 rw-p 00000000 00:00 0                          [stack:1613]
7f71b1725000-7f71b1728000 ---p 00000000 00:00 0 
7f71b1728000-7f71b1826000 rw-p 00000000 00:00 0                          [stack:1612]
7f71b1826000-7f71b1829000 ---p 00000000 00:00 0 
7f71b1829000-7f71b1927000 rw-p 00000000 00:00 0                          [stack:1611]
7f71b1927000-7f71b192a000 ---p 00000000 00:00 0 
7f71b192a000-7f71b1a28000 rw-p 00000000 00:00 0                          [stack:1610]
7f71b1a28000-7f71b1a2b000 ---p 00000000 00:00 0 
7f71b1a2b000-7f71b1b29000 rw-p 00000000 00:00 0                          [stack:1609]
7f71b1b29000-7f71b1b2c000 ---p 00000000 00:00 0 
7f71b1b2c000-7f71b1c2a000 rw-p 00000000 00:00 0                          [stack:1608]
7f71b1c2a000-7f71b1c2d000 ---p 00000000 00:00 0 
7f71b1c2d000-7f71b1d2b000 rw-p 00000000 00:00 0                          [stack:1607]
7f71b1d2b000-7f71b1d2e000 ---p 00000000 00:00 0 
7f71b1d2e000-7f71b1e2c000 rw-p 00000000 00:00 0                          [stack:1606]
7f71b1e2c000-7f71b1e2f000 ---p 00000000 00:00 0 
7f71b1e2f000-7f71b1f2d000 rw-p 00000000 00:00 0                          [stack:1605]
7f71b1f2d000-7f71b1f30000 ---p 00000000 00:00 0 
7f71b1f30000-7f71b202e000 rw-p 00000000 00:00 0                          [stack:1604]
7f71b202e000-7f71b2031000 ---p 00000000 00:00 0 
7f71b2031000-7f71b212f000 rw-p 00000000 00:00 0                          [stack:1603]
7f71b212f000-7f71b2132000 ---p 00000000 00:00 0 
7f71b2132000-7f71b2230000 rw-p 00000000 00:00 0                          [stack:1602]
7f71b2230000-7f71b2240000 r-xp 00000000 08:01 1233861                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnio.so
7f71b2240000-7f71b2440000 ---p 00010000 08:01 1233861                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnio.so
7f71b2440000-7f71b2441000 rw-p 00010000 08:01 1233861                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnio.so
7f71b2443000-7f71b2447000 r--s 0008b000 08:01 1234487                    /usr/lib/jvm/java-7-oracle/jre/lib/jsse.jar
7f71b2447000-7f71b244d000 r--s 0002f000 08:01 1231651                    /usr/share/elasticsearch/lib/lucene-queries-4.10.4.jar
7f71b2452000-7f71b2463000 r--s 000b2000 08:01 1231641                    /usr/share/elasticsearch/lib/jts-1.13.jar
7f71b2468000-7f71b2469000 r--s 00008000 08:01 1231649                    /usr/share/elasticsearch/lib/lucene-memory-4.10.4.jar
7f71b246f000-7f71b2478000 r--s 00057000 08:01 1231652                    /usr/share/elasticsearch/lib/lucene-queryparser-4.10.4.jar
7f71b247e000-7f71b2481000 r--s 00026000 08:01 1231635                    /usr/share/elasticsearch/lib/antlr-runtime-3.5.jar
7f71b2488000-7f71b248a000 r--s 00008000 08:01 1231638                    /usr/share/elasticsearch/lib/asm-commons-4.1.jar
7f71b248b000-7f71b248e000 r--s 0001b000 08:01 1231653                    /usr/share/elasticsearch/lib/lucene-sandbox-4.10.4.jar
7f71b2494000-7f71b2498000 r--s 000dc000 08:01 1231640                    /usr/share/elasticsearch/lib/jna-4.1.0.jar
7f71b249f000-7f71b24a2000 r--s 0001f000 08:01 1231647                    /usr/share/elasticsearch/lib/lucene-highlighter-4.10.4.jar
7f71b24a4000-7f71b24a7000 r--s 0001c000 08:01 1231654                    /usr/share/elasticsearch/lib/lucene-spatial-4.10.4.jar
7f71b24a7000-7f71b24a9000 r--s 00017000 08:01 1231656                    /usr/share/elasticsearch/lib/spatial4j-0.4.1.jar
7f71b24a9000-7f71b24ab000 r--s 00011000 08:01 1231645                    /usr/share/elasticsearch/lib/lucene-expressions-4.10.4.jar
7f71b24b0000-7f71b24da000 r--s 00248000 08:01 1231644                    /usr/share/elasticsearch/lib/lucene-core-4.10.4.jar
7f71b24db000-7f71b24dc000 r--s 0000b000 08:01 1231637                    /usr/share/elasticsearch/lib/asm-4.1.jar
7f71b24de000-7f71b24e6000 r--s 00066000 08:01 1231636                    /usr/share/elasticsearch/lib/apache-log4j-extras-1.2.17.jar
7f71b24e6000-7f71b24e8000 r--s 00016000 08:01 1231650                    /usr/share/elasticsearch/lib/lucene-misc-4.10.4.jar
7f71b24e9000-7f71b24ec000 r--s 00029000 08:01 1231655                    /usr/share/elasticsearch/lib/lucene-suggest-4.10.4.jar
7f71b24ee000-7f71b255d000 r--s 0063e000 08:01 1231639                    /usr/share/elasticsearch/lib/groovy-all-2.4.4.jar
7f71b255e000-7f71b2560000 r--s 0000e000 08:01 1231648                    /usr/share/elasticsearch/lib/lucene-join-4.10.4.jar
7f71b2567000-7f71b2570000 r--s 0006f000 08:01 1231642                    /usr/share/elasticsearch/lib/log4j-1.2.17.jar
7f71b2577000-7f71b2588000 r--s 0018f000 08:01 1231643                    /usr/share/elasticsearch/lib/lucene-analyzers-common-4.10.4.jar
7f71b2588000-7f71b2888000 rw-p 00000000 00:00 0 
7f71b2888000-7f71b289f000 r-xp 00000000 08:01 1233895                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnet.so
7f71b289f000-7f71b2a9e000 ---p 00017000 08:01 1233895                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnet.so
7f71b2a9e000-7f71b2a9f000 rw-p 00016000 08:01 1233895                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libnet.so
7f71b2aa0000-7f71b2aa8000 r-xp 00000000 08:01 1233869                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libmanagement.so
7f71b2aa8000-7f71b2ca7000 ---p 00008000 08:01 1233869                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libmanagement.so
7f71b2ca7000-7f71b2ca8000 rw-p 00007000 08:01 1233869                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libmanagement.so
7f71b2cac000-7f71b2dbe000 r--s 00c54000 08:01 1231629                    /usr/share/elasticsearch/lib/elasticsearch-1.7.1.jar
7f71b2dc2000-7f71b2dc3000 ---p 00000000 00:00 0 
7f71b2dc3000-7f71b2ec3000 rw-p 00000000 00:00 0                          [stack:1601]
7f71b2ec3000-7f71b2ec6000 ---p 00000000 00:00 0 
7f71b2ec6000-7f71b2fc4000 rw-p 00000000 00:00 0                          [stack:1600]
7f71b2fc4000-7f71b2fc7000 ---p 00000000 00:00 0 
7f71b2fc7000-7f71b30c5000 rw-p 00000000 00:00 0                          [stack:1599]
7f71b30c5000-7f71b30c8000 ---p 00000000 00:00 0 
7f71b30c8000-7f71b31c6000 rw-p 00000000 00:00 0                          [stack:1598]
7f71b31c6000-7f71b31c9000 ---p 00000000 00:00 0 
7f71b31c9000-7f71b32c7000 rw-p 00000000 00:00 0                          [stack:1597]
7f71b32c7000-7f71b32ca000 ---p 00000000 00:00 0 
7f71b32ca000-7f71b33c8000 rw-p 00000000 00:00 0                          [stack:1596]
7f71b33c8000-7f71b3551000 r--p 00000000 08:01 54985                      /usr/lib/locale/locale-archive
7f71b3555000-7f71b3558000 ---p 00000000 00:00 0 
7f71b3558000-7f71b3656000 rw-p 00000000 00:00 0                          [stack:1595]
7f71b3656000-7f71b3659000 ---p 00000000 00:00 0 
7f71b3659000-7f71b3757000 rw-p 00000000 00:00 0                          [stack:1594]
7f71b3757000-7f71b3758000 ---p 00000000 00:00 0 
7f71b3758000-7f71b38a3000 rw-p 00000000 00:00 0                          [stack:1593]
7f71b38a3000-7f71b3a63000 r--s 039fb000 08:01 1234543                    /usr/lib/jvm/java-7-oracle/jre/lib/rt.jar
7f71b3a64000-7f71b3ada000 rw-p 00000000 00:00 0 
7f71b3ada000-7f71b3adb000 ---p 00000000 00:00 0 
7f71b3adb000-7f71b8bce000 rw-p 00000000 00:00 0                          [stack:1592]
7f71b8bce000-7f71b8bd7000 rw-p 00000000 00:00 0 
7f71b8bd7000-7f71b8bd8000 rw-p 00000000 00:00 0 
7f71b8bd8000-7f71b9468000 rwxp 00000000 00:00 0 
7f71b9468000-7f71bbbd8000 rw-p 00000000 00:00 0 
7f71bbbd8000-7f71bbbef000 r-xp 00000000 08:01 1233890                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libzip.so
7f71bbbef000-7f71bbdef000 ---p 00017000 08:01 1233890                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libzip.so
7f71bbdef000-7f71bbdf0000 rw-p 00017000 08:01 1233890                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libzip.so
7f71bbdf0000-7f71bbdfc000 r-xp 00000000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
7f71bbdfc000-7f71bbffb000 ---p 0000c000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
7f71bbffb000-7f71bbffc000 r--p 0000b000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
7f71bbffc000-7f71bbffd000 rw-p 0000c000 08:01 2212                       /lib/x86_64-linux-gnu/libnss_files-2.21.so
7f71bc000000-7f71bcc72000 rw-p 00000000 00:00 0 
7f71bcc72000-7f71c0000000 ---p 00000000 00:00 0 
7f71c0005000-7f71c0130000 rw-p 00000000 00:00 0 
7f71c0130000-7f71c013b000 r-xp 00000000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
7f71c013b000-7f71c033a000 ---p 0000b000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
7f71c033a000-7f71c033b000 r--p 0000a000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
7f71c033b000-7f71c033c000 rw-p 0000b000 08:01 2200                       /lib/x86_64-linux-gnu/libnss_nis-2.21.so
7f71c0340000-7f71c0357000 r-xp 00000000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
7f71c0357000-7f71c0556000 ---p 00017000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
7f71c0556000-7f71c0557000 r--p 00016000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
7f71c0557000-7f71c0558000 rw-p 00017000 08:01 2218                       /lib/x86_64-linux-gnu/libnsl-2.21.so
7f71c0558000-7f71c055a000 rw-p 00000000 00:00 0 
7f71c0560000-7f71c0568000 r-xp 00000000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
7f71c0568000-7f71c0767000 ---p 00008000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
7f71c0767000-7f71c0768000 r--p 00007000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
7f71c0768000-7f71c0769000 rw-p 00008000 08:01 2214                       /lib/x86_64-linux-gnu/libnss_compat-2.21.so
7f71c0770000-7f71c0799000 r-xp 00000000 08:01 1233856                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libjava.so
7f71c0799000-7f71c0999000 ---p 00029000 08:01 1233856                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libjava.so
7f71c0999000-7f71c099b000 rw-p 00029000 08:01 1233856                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libjava.so
7f71c09a0000-7f71c09ad000 r-xp 00000000 08:01 1233883                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libverify.so
7f71c09ad000-7f71c0bac000 ---p 0000d000 08:01 1233883                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libverify.so
7f71c0bac000-7f71c0bae000 rw-p 0000c000 08:01 1233883                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/libverify.so
7f71c0bb0000-7f71c0bb7000 r-xp 00000000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
7f71c0bb7000-7f71c0db6000 ---p 00007000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
7f71c0db6000-7f71c0db7000 r--p 00006000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
7f71c0db7000-7f71c0db8000 rw-p 00007000 08:01 2219                       /lib/x86_64-linux-gnu/librt-2.21.so
7f71c0db8000-7f71c0ebf000 r-xp 00000000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
7f71c0ebf000-7f71c10be000 ---p 00107000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
7f71c10be000-7f71c10bf000 r--p 00106000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
7f71c10bf000-7f71c10c0000 rw-p 00107000 08:01 2217                       /lib/x86_64-linux-gnu/libm-2.21.so
7f71c10c0000-7f71c1c36000 r-xp 00000000 08:01 1233910                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
7f71c1c36000-7f71c1e35000 ---p 00b76000 08:01 1233910                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
7f71c1e35000-7f71c1ef9000 rw-p 00b75000 08:01 1233910                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/server/libjvm.so
7f71c1ef9000-7f71c1f3a000 rw-p 00000000 00:00 0 
7f71c1f40000-7f71c2100000 r-xp 00000000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
7f71c2100000-7f71c2300000 ---p 001c0000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
7f71c2300000-7f71c2304000 r--p 001c0000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
7f71c2304000-7f71c2306000 rw-p 001c4000 08:01 2222                       /lib/x86_64-linux-gnu/libc-2.21.so
7f71c2306000-7f71c230a000 rw-p 00000000 00:00 0 
7f71c2310000-7f71c2313000 r-xp 00000000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
7f71c2313000-7f71c2512000 ---p 00003000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
7f71c2512000-7f71c2513000 r--p 00002000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
7f71c2513000-7f71c2514000 rw-p 00003000 08:01 2205                       /lib/x86_64-linux-gnu/libdl-2.21.so
7f71c2518000-7f71c252d000 r-xp 00000000 08:01 1233902                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/jli/libjli.so
7f71c252d000-7f71c272c000 ---p 00015000 08:01 1233902                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/jli/libjli.so
7f71c272c000-7f71c272d000 rw-p 00014000 08:01 1233902                    /usr/lib/jvm/java-7-oracle/jre/lib/amd64/jli/libjli.so
7f71c2730000-7f71c2748000 r-xp 00000000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
7f71c2748000-7f71c2948000 ---p 00018000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
7f71c2948000-7f71c2949000 r--p 00018000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
7f71c2949000-7f71c294a000 rw-p 00019000 08:01 2209                       /lib/x86_64-linux-gnu/libpthread-2.21.so
7f71c294a000-7f71c294e000 rw-p 00000000 00:00 0 
7f71c2950000-7f71c2974000 r-xp 00000000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
7f71c2978000-7f71c297b000 r--s 00018000 08:01 1231646                    /usr/share/elasticsearch/lib/lucene-grouping-4.10.4.jar
7f71c297e000-7f71c299e000 rw-p 00000000 00:00 0 
7f71c299e000-7f71c29a8000 rw-p 00000000 00:00 0 
7f71c29a8000-7f71c29cb000 rw-p 00000000 00:00 0 
7f71c29cb000-7f71c2a68000 rw-p 00000000 00:00 0 
7f71c2a68000-7f71c2a70000 rw-s 00000000 08:01 1239285                    /tmp/hsperfdata_elasticsearch/1583
7f71c2a70000-7f71c2a72000 rw-p 00000000 00:00 0 
7f71c2a72000-7f71c2a75000 ---p 00000000 00:00 0 
7f71c2a75000-7f71c2b73000 rw-p 00000000 00:00 0                          [stack:1591]
7f71c2b73000-7f71c2b74000 r--p 00023000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
7f71c2b74000-7f71c2b75000 rw-p 00024000 08:01 2208                       /lib/x86_64-linux-gnu/ld-2.21.so
7f71c2b75000-7f71c2b76000 rw-p 00000000 00:00 0 
7f71c2b76000-7f71c2b77000 ---p 00000000 00:00 0 
7f71c2b77000-7f71c2b7d000 rw-p 00000000 00:00 0 
7fff2227d000-7fff2229e000 rw-p 00000000 00:00 0                          [stack]
7fff22360000-7fff22362000 r--p 00000000 00:00 0                          [vvar]
7fff22362000-7fff22364000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Xms2g -Xmx2g -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -XX:+DisableExplicitGC -Dfile.encoding=UTF-8 -Delasticsearch -Des.foreground=yes -Des.path.home=/usr/share/elasticsearch -Des.pidfile=/var/run/elasticsearch/elasticsearch.pid -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.conf=/etc/elasticsearch 
java_command: org.elasticsearch.bootstrap.Elasticsearch
Launcher Type: SUN_STANDARD

Environment Variables:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
SHELL=/bin/false

Signal Handlers:
SIGSEGV: [libjvm.so+0x9a3b20], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGBUS: [libjvm.so+0x9a3b20], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGFPE: [libjvm.so+0x81e740], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGPIPE: [libjvm.so+0x81e740], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGXFSZ: [libjvm.so+0x81e740], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGILL: [libjvm.so+0x81e740], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGUSR1: SIG_DFL, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGUSR2: [libjvm.so+0x81ffb0], sa_mask[0]=0x00000000, sa_flags=0x10000004
SIGHUP: [libjvm.so+0x8210d0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGINT: [libjvm.so+0x8210d0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGTERM: [libjvm.so+0x8210d0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGQUIT: [libjvm.so+0x8210d0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004


---------------  S Y S T E M  ---------------

OS:jessie/sid

uname:Linux 3.19.0-30-generic #33-Ubuntu SMP Mon Sep 21 20:58:04 UTC 2015 x86_64
libc:glibc 2.21 NPTL 2.21 
rlimit: STACK 8192k, CORE 0k, NPROC 15767, NOFILE 65535, AS infinity
load average:0.00 0.04 0.19

/proc/meminfo:
MemTotal:        4047416 kB
MemFree:          139348 kB
MemAvailable:    1878576 kB
Buffers:           75496 kB
Cached:          1674596 kB
SwapCached:         2644 kB
Active:          2128876 kB
Inactive:        1497796 kB
Active(anon):    1276544 kB
Inactive(anon):   602792 kB
Active(file):     852332 kB
Inactive(file):   895004 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:       8388604 kB
SwapFree:        8381132 kB
Dirty:             17696 kB
Writeback:             0 kB
AnonPages:       1873988 kB
Mapped:            98696 kB
Shmem:              2740 kB
Slab:             254492 kB
SReclaimable:     245308 kB
SUnreclaim:         9184 kB
KernelStack:        2000 kB
PageTables:         6264 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    10412312 kB
Committed_AS:    2567120 kB
VmallocTotal:   34359738367 kB
VmallocUsed:       57108 kB
VmallocChunk:   34359677944 kB
HardwareCorrupted:     0 kB
AnonHugePages:   1751040 kB
CmaTotal:              0 kB
CmaFree:               0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:       57280 kB
DirectMap2M:     4136960 kB


CPU:total 1 (1 cores per cpu, 1 threads per core) family 21 model 2 stepping 0, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, tsc

/proc/cpuinfo:
processor   : 0
vendor_id   : AuthenticAMD
cpu family  : 21
model       : 2
model name  : AMD FX(tm)-8350 Eight-Core Processor
stepping    : 0
microcode   : 0x6000626
cpu MHz     : 4024.915
cache size  : 2048 KB
physical id : 0
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 5
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx fxsr_opt rdtscp lm rep_good nopl extd_apicid pni monitor ssse3 lahf_lm cr8_legacy arat vmmcall
bugs        : fxsave_leak
bogomips    : 8049.83
TLB size    : 1536 4K pages
clflush size    : 64
cache_alignment : 64
address sizes   : 48 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 4047416k(139348k free), swap 8388604k(8381132k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (24.80-b11) for linux-amd64 JRE (1.7.0_80-b15), built on Apr 10 2015 19:53:14 by "java_re" with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)

time: Sat Oct  3 10:02:27 2015
elapsed time: 210255 seconds

```

Hopefully that makes some sense to you.
Thanks a lot for the help.
</comment><comment author="rmuir" created="2015-10-07T04:05:05Z" id="146072501">hope your upgrade helps. I can't find any exact bug matching that, except https://bugs.openjdk.java.net/browse/JDK-8129961...
</comment><comment author="dbblackdiamond" created="2015-10-08T22:52:17Z" id="146710157">@clintongormley : I have upgraded java on 2 out of my 3 elasticsearch servers:

```
root@odin:/tmp# java -version
java version "1.8.0_60"
Java(TM) SE Runtime Environment (build 1.8.0_60-b27)
Java HotSpot(TM) 64-Bit Server VM (build 25.60-b23, mixed mode)
```

and found them down this morning, so the upgrade hasn't help, as the only server that stayed up was the non-upgraded one.

Any other idea? What do you think I should do?

Thanks a lot in advance,
Bertrand.
</comment><comment author="rmuir" created="2015-10-09T00:23:26Z" id="146724149">@dbblackdiamond try removing the entire `sigar` directory underneath the elasticsearch `lib` directory. It is a potential suspect here.
</comment><comment author="dbblackdiamond" created="2015-10-09T01:49:35Z" id="146735080">@rmuir: thanks a lot for the suggestion, but I have already done that.
</comment><comment author="rmuir" created="2015-10-09T02:03:33Z" id="146736686">Ok id report the crash to oracle. Not much we can do here i am afraid.
</comment><comment author="rmuir" created="2015-10-09T12:04:55Z" id="146846147">Maybe even try hotspot-gc-use (http://mail.openjdk.java.net/mailman/listinfo/hotspot-gc-use) directly and mention that the crash looks a lot like https://bugs.openjdk.java.net/browse/JDK-8129961. Perhaps it affects more than just java 9...
</comment><comment author="clintongormley" created="2015-10-13T07:32:53Z" id="147629829">Based on what @rmuir said, it doesn't sound like there is much we can do here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test failure testGroovyExceptionSerialization </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13599</link><project id="" key="" /><description>The test fails regularly. Might have been triggered by bfa984e1c28b1b6840fbf46ad1d0d93433f14a78 (the first failure occurred right after this commit).

Some recent failures:
http://build-us-00.elastic.co/job/es_core_master_metal/11142/
http://build-us-00.elastic.co/job/es_core_master_window-2008/2233/

The failure is very easy to reproduce by running the test several times.

```
ava.lang.AssertionError: Failed to execute phase [query], all shards failed; shardFailures {[fnLnw1QCTeuq0x9sMS3WqQ][test][0]: RemoteTransportException[[node_s1][local[2]][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: GroovyScriptExecutionException[failed to run inline script [assert false] using lang [groovy]]; nested: AssertionError[assert false
]; }{[fnLnw1QCTeuq0x9sMS3WqQ][test][1]: RemoteTransportException[[node_s1][local[2]][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: GroovyScriptExecutionException[failed to run inline script [assert false] using lang [groovy]]; nested: AssertionError[assert false
]; }{[fnLnw1QCTeuq0x9sMS3WqQ][test][2]: RemoteTransportException[[node_s1][local[2]][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: GroovyScriptExecutionException[failed to run inline script [assert false] using lang [groovy]]; nested: AssertionError[assert false
]; }{[fnLnw1QCTeuq0x9sMS3WqQ][test][3]: RemoteTransportException[[node_s1][local[2]][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: GroovyScriptExecutionException[failed to run inline script [assert false] using lang [groovy]]; nested: AssertionError[assert false
]; }should have contained an assert error
Expected: &lt;true&gt;
     but: was &lt;false&gt;
    at __randomizedtesting.SeedInfo.seed([25A0FC596FF7CF64:1707A28E84FA491B]:0)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.junit.Assert.assertThat(Assert.java:865)
    at org.elasticsearch.script.GroovyScriptIT.testGroovyExceptionSerialization(GroovyScriptIT.java:94)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1638)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:847)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:897)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:856)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:792)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:803)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:54)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
    at java.lang.Thread.run(Thread.java:745)
```
</description><key id="106687669">13599</key><summary>Test failure testGroovyExceptionSerialization </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>test</label></labels><created>2015-09-16T02:51:57Z</created><updated>2015-09-16T07:22:51Z</updated><resolved>2015-09-16T07:22:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-16T07:22:51Z" id="140653760">thanks @imotov for opening this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cloud GCE documentation update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13598</link><project id="" key="" /><description>When testing ES 2.0.0-beta2 on GCE, I found that the steps for using the Google Cloud SDK have changed a bit. This PR updates the documentation with these changes.
</description><key id="106677897">13598</key><summary>Cloud GCE documentation update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ycombinator</reporter><labels><label>:Plugin Cloud GCE</label><label>docs</label><label>v2.0.0-rc1</label></labels><created>2015-09-16T01:20:16Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-16T14:26:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-16T01:47:37Z" id="140596349">Left a comment. 
BTW I think you should squash all commits.
</comment><comment author="ycombinator" created="2015-09-16T13:38:25Z" id="140744975">Squashing commits now.
</comment><comment author="dadoonet" created="2015-09-16T14:17:19Z" id="140754496">LGTM
</comment><comment author="ycombinator" created="2015-09-16T14:29:27Z" id="140758672">Merged into master, TODO: cherry-pick into 2.0 and 2.x.
</comment><comment author="ycombinator" created="2015-09-16T14:40:52Z" id="140762021">Cherry-picked 516ba6c8f67afc3719520e7a09fb83a48a242d2b into 2.0 and 2.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SELECT state, COUNT(*) for clarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13597</link><project id="" key="" /><description>Adding state to the SQL syntax to make it more clear, given that the buckets returned include the values of state AND count for each.
</description><key id="106672106">13597</key><summary>SELECT state, COUNT(*) for clarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sc0ttkclark</reporter><labels><label>Awaiting CLA</label><label>docs</label></labels><created>2015-09-16T00:08:01Z</created><updated>2015-09-19T13:04:42Z</updated><resolved>2015-09-19T13:04:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-18T17:59:13Z" id="141520437">Hiya @sc0ttkclark 

thanks for the PR. Please could i ask you to sign the CLA so that I can merge it in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="sc0ttkclark" created="2015-09-18T18:11:25Z" id="141522871">Oh that's weird, but I'll bite :)

Signed
</comment><comment author="clintongormley" created="2015-09-19T13:03:53Z" id="141662859">thanks @sc0ttkclark - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.primitives.Ints</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13596</link><project id="" key="" /><description>This commit removes and now forbids all uses of
`com.google.common.primitives.Ints across the codebase`. This is one of
many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106660747">13596</key><summary>Remove and forbid use of com.google.common.primitives.Ints</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-15T22:28:29Z</created><updated>2015-09-18T13:45:40Z</updated><resolved>2015-09-18T13:45:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-16T09:23:53Z" id="140683643">left a bunch of comments
</comment><comment author="jasontedor" created="2015-09-18T01:57:50Z" id="141306880">@s1monw I've addressed your comments as we discussed offline.
</comment><comment author="rmuir" created="2015-09-18T04:19:29Z" id="141339569">This looks great! thanks for cleaning up
</comment><comment author="jasontedor" created="2015-09-18T13:45:37Z" id="141455515">@rmuir Thanks for the thorough and helpful review. This is integrated into master now. Another one bites the dust.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compound word token filter file format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13595</link><project id="" key="" /><description>Via [this](https://discuss.elastic.co/t/compound-word-token-filter-configuration-file/29260) forum post.

We should update [this page](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-compound-word-tokenfilter.html) to be a bit more explicit in what the format of the file you specify needs to be in, just like we do for the stop words file;

&gt; A path (either relative to config location, or absolute) to a stopwords file configuration. Each stop word should be in its own "line" (separated by a line break). The file must be UTF-8 encoded.
</description><key id="106657061">13595</key><summary>Compound word token filter file format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-15T22:02:44Z</created><updated>2015-09-21T09:22:46Z</updated><resolved>2015-09-21T09:22:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix centos-6 tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13594</link><project id="" key="" /><description>Right now we execute some debian-isms in the init.d tests. This switches to
trying both the debian and centos ways to stop services from starting
automatically.
</description><key id="106656172">13594</key><summary>Fix centos-6 tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T21:57:10Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-16T14:00:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T21:57:22Z" id="140558229">Ping @tlrx and @spinscale for more bats review.
</comment><comment author="tlrx" created="2015-09-16T09:02:39Z" id="140677623">LGTM
</comment><comment author="nik9000" created="2015-09-16T14:01:45Z" id="140750831">Pushed to 2.0, 2.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add opensuse-13 to packaging tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13593</link><project id="" key="" /><description>This gets opensuse-13 working with vagrant and the packaging tests. They pass
with some minor tweaks.

Closes #13507
</description><key id="106650933">13593</key><summary>Add opensuse-13 to packaging tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T21:23:47Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-16T14:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T21:24:02Z" id="140550386">Ping @spinscale and @tlrx for more fun fun packaging test reviews!
</comment><comment author="tlrx" created="2015-09-16T08:54:56Z" id="140676071">@nik9000 you rock! Can you please&#160;close #ht13507 in the commit message?

LGTM
</comment><comment author="nik9000" created="2015-09-16T14:10:37Z" id="140752856">Force pushed with the reference to the issue. I'll merge now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow binding to multiple addresses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13592</link><project id="" key="" /><description>Right now you can bind Elasticsearch to only a single address with either:

``` yaml
network.bind_host: "some_ip"
```

or

``` yaml
network.host: "some_ip"
```

It'd be convenient to be able to bind to multiple hosts like so:

``` yaml
network.bind_host: [ "some_ip", "some_other_ip" ]
```

In particular it'd allow the convenient configuration:

``` yaml
network.bind_host: [ "some_data_center_accessible_ip", "localhost" ]
network.publish_host: "some_data_center_accessible_ip"
```

Which would allow elasticsearch to respond to requests to localhost.
</description><key id="106637098">13592</key><summary>Allow binding to multiple addresses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Network</label><label>enhancement</label></labels><created>2015-09-15T20:17:09Z</created><updated>2017-04-24T08:00:49Z</updated><resolved>2015-10-24T04:06:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rayward" created="2015-09-18T03:10:31Z" id="141331974">+1

When testing ES 2.0, I had to bind to `0.0.0.0` so that ES was accessible from both within and outside of my developer VM.

This should also apply to the `http` module.
</comment><comment author="rmuir" created="2015-09-18T03:24:56Z" id="141333428">I plan to look into this, the code should easily support it, we just have to deal with the configuration to accept it.

I really really do not like people being forced to bind to all addresses when they really just want maybe two or three or whatever.
</comment><comment author="vionemc" created="2016-01-29T15:18:27Z" id="176805092">Doesn't work on ElasticSearch 2.x
</comment><comment author="rmuir" created="2016-01-29T15:30:53Z" id="176817111">has not been released.
</comment><comment author="vionemc" created="2016-01-29T15:33:05Z" id="176819657">Ahh, so that's why. Thanks! Really looking forward to it.
</comment><comment author="AseemKumar" created="2016-07-14T15:00:53Z" id="232691422">Has this been fixed? Running 2.3.4 and unable to specify multiple interfaces/addresses.
</comment><comment author="jasontedor" created="2016-07-14T15:41:28Z" id="232703627">@AseemKumar Yes, it works fine in 2.3.4:

``` bash
11:40:45 [jason:~] $ elasticsearch -d --network.bind_host=192.168.1.8,192.168.1.9
11:40:47 [jason:~] $ curl -XGET 192.168.1.8:9200/
{
  "name" : "Mutant X",
  "cluster_name" : "elasticsearch_jason",
  "version" : {
    "number" : "2.3.4",
    "build_hash" : "e455fd0c13dceca8dbbdbb1665d068ae55dabe3f",
    "build_timestamp" : "2016-06-30T11:24:31Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
11:40:55 [jason:~] $ curl -XGET 192.168.1.9:9200/
{
  "name" : "Mutant X",
  "cluster_name" : "elasticsearch_jason",
  "version" : {
    "number" : "2.3.4",
    "build_hash" : "e455fd0c13dceca8dbbdbb1665d068ae55dabe3f",
    "build_timestamp" : "2016-06-30T11:24:31Z",
    "build_snapshot" : false,
    "lucene_version" : "5.5.0"
  },
  "tagline" : "You Know, for Search"
}
```
</comment><comment author="AseemKumar" created="2016-07-14T15:59:22Z" id="232709576">Thanks @jasontedor . I specified the interfaces in my config and it only listens on the localhost

`network.host: [_enp0s25_, _local_]`

Do you know if only IPs can be provided or interface name as well?
</comment><comment author="jasontedor" created="2016-07-14T16:08:11Z" id="232712319">@AseemKumar Do not specify as an array, but comma-delimited just as I showed above: `network.host: _enp0s25_,_local_`.
</comment><comment author="clintongormley" created="2016-07-15T08:38:38Z" id="232894969">@jasontedor if you can't specify as an array, then that's a bug.  we use arrays for a number of other settings.  the comma-separated format is really for command line settings
</comment><comment author="jasontedor" created="2016-07-15T13:02:33Z" id="232945224">@clintongormley I tested this, it does indeed work as an array, and it does work with interface names. I'm not sure why @AseemKumar reported an issue here.

@AseemKumar Can you share exactly what you entered for `network.host`, any other relevant settings from your configuration file, as well as the output of `ifconfig`?
</comment><comment author="rmuir" created="2016-07-15T13:47:25Z" id="232955024">also if you run with DEBUG logging it prints a "java ifconfig". This is `ifconfig` as java sees the world.
</comment><comment author="podollb" created="2016-08-04T18:28:35Z" id="237641190">I wrestled with this too (trying various formats of specifying multiple values), but I got it working using the array notation, but I needed to quote the values. So I think @AseemKumar needs to quote the values in his array: `network.host: ["_enp0s25_", "_local_"]` and then it will work. Otherwise, you could use the more specific configs via something like `network.bind_host: ["_enp0s25_", "_local_"]` and `network.publish_host: _enp0s25_`
</comment><comment author="niemyjski" created="2016-12-29T21:34:35Z" id="269697013">I'm running 5.1.1 and I couldn't get it to bind correctly unless I did `network.host: [ '10.2.0.7', '_local_' ]`. 

```
/sbin/ifconfig
eth0      Link encap:Ethernet  HWaddr 00:0d:3a:10:55:07
          inet addr:10.2.0.7  Bcast:10.2.0.255  Mask:255.255.255.0

lo        Link encap:Local Loopback
          inet addr:127.0.0.1  Mask:255.0.0.0

lxcbr0    Link encap:Ethernet  HWaddr 00:16:3e:00:00:00
          inet addr:10.0.3.1  Bcast:0.0.0.0  Mask:255.255.255.0
```

I've never had to do this before in 1.7.5. Is this normal to bind just so you can access from localhost and `10.2.0.7` (local ip)</comment><comment author="jasontedor" created="2016-12-31T13:53:30Z" id="269865989">Yes. Elasticsearch binds to localhost only by default, and otherwise only what you explicitly configure.</comment><comment author="Sojoy" created="2017-03-15T09:42:43Z" id="286690572">I just tried out rayward's recommendation to ue 0.0.0.0 to bind to all available addresses and it works on ES 5.2.2</comment><comment author="madchap" created="2017-04-02T18:32:16Z" id="291005162">Just a quick note despite the issue being closed, I am not able to do something like (on 2.4.4). I am running under a coreos-kubernetes multi-vagrant setup.

```
network.host: [ '_local_' , '_site:ipv4_' ]
```

Getting an error like, and then the container fails.
```
Exception in thread "main" BindTransportException[Failed to resolve host null]; nested: UnknownHostException[[ '_local_': invalid IPv6 address];
Likely root cause: java.net.UnknownHostException: [ '_local_': invalid IPv6 address
```
</comment><comment author="jasontedor" created="2017-04-02T20:26:35Z" id="291012469">@madchap I tested this and works fine. Are you sure that you input it correctly? I ask because the exception message indicates that it parsed whatever you input as `[ '_local_'`.</comment><comment author="madchap" created="2017-04-02T22:29:11Z" id="291019791">@jasontedor I tried to catch that to verify it, but could not really as the container is getting destroyed before I can actually get to what is being held in my env variable.

But I tend to believe you're right -- there must be something odd with the env variable interpolation I am missing. Thanks for your confirmation too. I've put 0.0.0.0 for now as I need to move on onto other stuff. 
</comment><comment author="pvledoux" created="2017-04-22T11:52:29Z" id="296368133">@madchap Did you could resolved that issue? I have a similar problem, I tried different things with no luck:

-network.host=_eth0_ #works

but

- network.host=[_eth0_,'192.168.100.40','X.X.X.X']

is not working. It tried with single quotes, double quotes around _eth0_.
</comment><comment author="madchap" created="2017-04-22T17:38:13Z" id="296389301">@pvledoux, no I eventually did not after having tried what I think is every possibility. I haven't investigated any further. Maybe something related to the particular docker environment in my case?</comment><comment author="pvledoux" created="2017-04-24T08:00:49Z" id="296565831">@madchap Yes I'm docker too. I will investigate on at side when I have more time.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[discovery-azure] nodes don't see each others</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13591</link><project id="" key="" /><description>With 2.0, we now bind to `localhost` by default instead of binding to the network card and use its IP address.

When the discovery plugin gets from Azure API the list of nodes that should form the cluster, this list is pinged then. But as each node is bound to `localhost`, ping does not get an answer and the node elects itself as the master node.
</description><key id="106633831">13591</key><summary>[discovery-azure] nodes don't see each others</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>docs</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T20:01:03Z</created><updated>2015-10-01T11:19:31Z</updated><resolved>2015-09-18T17:53:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-15T20:01:35Z" id="140520265">See also #13589, #13590, #13591
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[discovery-gce] nodes don't see each others</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13590</link><project id="" key="" /><description>With 2.0, we now bind to `localhost` by default instead of binding to the network card and use its IP address.

When the discovery plugin gets from GCE API the list of nodes that should form the cluster, this list is pinged then. But as each node is bound to `localhost`, ping does not get an answer and the node elects itself as the master node.
</description><key id="106633686">13590</key><summary>[discovery-gce] nodes don't see each others</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>docs</label><label>v2.0.0</label><label>v2.2.0</label></labels><created>2015-09-15T20:00:24Z</created><updated>2016-03-10T18:11:03Z</updated><resolved>2015-10-07T21:23:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-15T20:01:30Z" id="140520231">See also #13589, #13590, #13591
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>nodes don't see each others without setting network.host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13589</link><project id="" key="" /><description>With 2.0, we now bind to `localhost` by default instead of binding to the network card and use its IP address.

When the discovery plugin gets from AWS API the list of nodes that should form the cluster, this list is pinged then. But as each node is bound to `localhost`, ping does not get an answer and the node elects itself as the master node.
</description><key id="106633507">13589</key><summary>nodes don't see each others without setting network.host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>docs</label><label>v2.0.0</label></labels><created>2015-09-15T19:59:36Z</created><updated>2015-10-06T09:22:37Z</updated><resolved>2015-10-06T09:22:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-15T20:01:26Z" id="140520208">See also #13589, #13590, #13591
</comment><comment author="s1monw" created="2015-09-15T20:01:46Z" id="140520332">isn't this a configuration issue? I mean can / do we want to do anything about this?
</comment><comment author="dadoonet" created="2015-09-15T20:04:22Z" id="140521603">It's at least a documentation issue. 

EC2 supports indeed other settings: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/modules-network.html

So you can bind to `_ec2_`. May be this should be a default option when ec2 plugin is loaded?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix example for s3 repository bucket name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13588</link><project id="" key="" /><description>If you try to create a S3 repository with a bucket name such as `my_s3_repository`, you will get `Bucket name should not contain '_'`:

``` json
{
   "error":{
      "root_cause":[
         {
            "type":"repository_exception",
            "reason":"[my_s3_repository] failed to create repository"
         }
      ],
      "type":"repository_exception",
      "reason":"[my_s3_repository] failed to create repository",
      "caused_by":{
         "type":"creation_exception",
         "reason":"Guice creation errors:\n\n1) Error injecting constructor, java.lang.IllegalArgumentException: Bucket name should not contain '_'\n  at org.elasticsearch.repositories.s3.S3Repository.&lt;init&gt;(Unknown Source)\n  while locating org.elasticsearch.repositories.s3.S3Repository\n  while locating org.elasticsearch.repositories.Repository\n\n1 error",
         "caused_by":{
            "type":"illegal_argument_exception",
            "reason":"Bucket name should not contain '_'"
         }
      }
   },
   "status":500
}
```
</description><key id="106627481">13588</key><summary>Fix example for s3 repository bucket name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>docs</label><label>v2.0.2</label><label>v2.1.1</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-15T19:25:23Z</created><updated>2015-11-23T12:17:58Z</updated><resolved>2015-11-23T12:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Prevent losing stacktraces when exceptions occur</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13587</link><project id="" key="" /><description>This commit removes unnecesssary use of ExceptionHelpers where we actually
should serialize / deserialize the actual exception. This commit also
fixes one of the oddest problems where the actual exception was never
rendered / printed if `all shards failed` due to a missing cause.

This commit unfortunately doesn't fix Snapshot/Restore which is almost
unfixable since it has to serialize XContent and read from it which can't
transport exceptions.
</description><key id="106624818">13587</key><summary>Prevent losing stacktraces when exceptions occur</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Exceptions</label><label>bug</label><label>review</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T19:08:53Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-15T21:25:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T19:21:02Z" id="140507260">s/loosing/losing/ ?
</comment><comment author="rjernst" created="2015-09-15T20:29:40Z" id="140528045">LGTM.
</comment><comment author="s1monw" created="2015-09-15T21:25:45Z" id="140551252">I will give this some time before I backport
</comment><comment author="s1monw" created="2015-09-23T10:33:37Z" id="142560291">this has been backported to 2.x in https://github.com/elastic/elasticsearch/commit/7d2cff15b266123e5848e0bcee708985e5211ddd and 2.0 in https://github.com/elastic/elasticsearch/commit/f09f85117989362d16d2467fec3bc5fc3122fa9a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose all addresses that the transports are bound to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13586</link><project id="" key="" /><description>In #12942, the NettyTransport and NettyHttpServerTransport were updated to allow for binding
to multiple addresses. However, the BoundTransportAddress holder only exposed the first address
that the transport was bound to and this object is used to populate the values returned to the user
via our APIs.

This change exposes all of the bound addresses in the BoundTransportAddress holder, which allows
for an accurate representation of all interfaces that elasticsearch is bound to and listening on.
</description><key id="106616284">13586</key><summary>Expose all addresses that the transports are bound to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaymode</reporter><labels><label>:Network</label><label>enhancement</label><label>review</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T18:28:10Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-23T15:08:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-21T17:44:55Z" id="142056238">this looks good to me. I added some minor suggestions. My only other wish, would be to add some heavy comments around the big conditionals with DEFAULT_PROFILE in the transport binding. Some of it is pre-existing conditions, but its hard to wrap your head around, and seeing the null checks etc is not intuitive towards figuring it out. I know i struggled with this part when first looking at this code, then figured it out for about 30 minutes (enough to make my changes) and promptly forgot it.
</comment><comment author="rmuir" created="2015-09-21T18:24:57Z" id="142067534">Looks great, thanks for fixing this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also adds tests for date fields for match query builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13585</link><project id="" key="" /><description>PR is against the query refactoring branch.
</description><key id="106607873">13585</key><summary>Also adds tests for date fields for match query builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-09-15T17:41:20Z</created><updated>2015-09-16T00:20:57Z</updated><resolved>2015-09-15T22:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-15T19:46:18Z" id="140516562">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggester mappings supported parameters documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13584</link><project id="" key="" /><description>In 1.7.1 Im able to create an type with the following mappings:

``` json
 {
  "song" : {
        "properties" : {
            "name" : { "type" : "string" },
            "suggest" : { 
                          "type" : "completion",
                          "analyzer" : "standard",
                          "payloads" : true
            }
        }
    }
}
```

But on the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-suggesters-completion.html) its stated that it supports only "search_analyzer" and "index_analyzer". I think we should state that "analyzer" is supported or fail when its used.
</description><key id="106606533">13584</key><summary>Completion Suggester mappings supported parameters documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrestc</reporter><labels /><created>2015-09-15T17:34:51Z</created><updated>2015-09-15T18:48:11Z</updated><resolved>2015-09-15T17:53:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-15T17:53:21Z" id="140481102">index_analyzer was deprecated in 1.6 or 1.7 (sorry I don't remember which) and removed in 2.0. The settings are now `analyzer` (which is used for both index and search analyzer), and optionally also `search_analyzer` (which overrides `analyzer` for search time).
</comment><comment author="andrestc" created="2015-09-15T18:12:55Z" id="140487375">Thanks @rjernst. Do you want me to PR this into the documentation?
</comment><comment author="rjernst" created="2015-09-15T18:25:33Z" id="140490406">@andrestc If you see a place `index_analyzer` is still mentioned (on master), then yes please.
</comment><comment author="andrestc" created="2015-09-15T18:41:16Z" id="140494434">@rjernst I was thinking about mentioning the deprecation in 1.7, e.g, [here](https://github.com/elastic/elasticsearch/blob/1.7/docs/reference/search/suggesters/completion-suggest.asciidoc#mapping) like it is done [here](https://github.com/elastic/elasticsearch/commit/41772737ebe77277334fd26f4a46ab6d01f81b8c), if you think its needed.
</comment><comment author="rjernst" created="2015-09-15T18:48:11Z" id="140496185">I thought I had done that, but it looks like I missed it for this setting. It seems unlikely (to me) there would be a 1.7.3 (we just released 1.7.2 and the 2.0 beta is getting closer to a final release).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DiskAllocationDecider should take custom data paths for shadow replicas into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13583</link><project id="" key="" /><description>Trying to manually relocate a shard returns the following error. We are using shadow replicas in our setup.

{
  "error": "ElasticsearchIllegalArgumentException[[move_allocation] can't move [accesslogging_hawthorne-2015-09-01][1], from [Sputnik][28AZOAhAQV6ThZyCB5oP4g][halitods4.datacenter.automotive.com][inet[/10.1.68.42:9300]]{site=hawthorne, tag=shadowcopy, enable_custom_paths=true, master=false}, to [Thirty-Three][NWA5D0HIRAe-7XaHtfiq7g][halitods5.datacenter.automotive.com][inet[/10.1.68.43:9300]]{site=hawthorne, tag=shadowcopy, enable_custom_paths=true, master=false}, since its not allowed, reason: [YES(shard is not allocated to same node or host)][YES(node passes include/exclude/require filters)][YES(shard is primary)][YES(below shard recovery limit of [2])][YES(allocation disabling is ignored)][YES(allocation disabling is ignored)][YES(no allocation awareness enabled)][YES(total shard limit disabled: [-1] &lt;= 0)][YES(target node version [1.7.1] is same or newer than source node version [1.7.1])][NO(after allocation more than allowed [85.0%] used disk on node, free: [0.9657585087747922%])][YES(no snapshots are currently running)]]",
  "status": 400
}
</description><key id="106602155">13583</key><summary>DiskAllocationDecider should take custom data paths for shadow replicas into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jinahn</reporter><labels><label>:Shadow Replicas</label><label>adoptme</label><label>enhancement</label></labels><created>2015-09-15T17:09:41Z</created><updated>2015-09-18T15:50:09Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-15T20:31:00Z" id="140528367">@jinahn the relocation actually failed because you are over the [disk watermark](https://www.elastic.co/guide/en/elasticsearch/reference/current/disk.html), as you can see from:

```
NO(after allocation more than allowed [85.0%] used disk on node, free: [0.9657585087747922%])
```
</comment><comment author="jinahn" created="2015-09-15T22:04:21Z" id="140559745">@dakrone The actual disk usage for the drive the indices are is actually only 50% used
</comment><comment author="dakrone" created="2015-09-15T22:51:35Z" id="140572467">@jinahn see #13582, the DiskThresholdDecider does not currently know about custom data paths when calculating the disk usage.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to allocate shards because elasticsearch doesn't free calculate disk space correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13582</link><project id="" key="" /><description>First, let me explain our setup of elasticsearch.
1.  We are 1.7.1, have 1 master node and 3 data nodes.
2.  We are using shadow replicas (all 3 data nodes have the shadowcopy set to true in their configs)
3.  We are also using templates that sets shadow replica to be true, set the number of replicas to 1, and use the shadowcopy tag
4.  Also within the templates we use a index.data_path. All 3 data nodes have enable_custom_paths set to true in the configs.
a.  The path that data_path points to is a nfs share that is mounted on all the nodes (master included)
Currently experiencing an issue when closing and reopening an index. Upon reopening the index es will allocate all the primary shards, but do nothing with the secondary shards (the replicas). Because of this the cluster will stay yellow, no matter how long we wait for it go. However, if es is given another command (ex: closing/opening another index or creating/deleting another index), then the original index that was having trouble allocating the shards would become unstuck and allocate the shards thus making the cluster green. 

The log shows &#8220;[WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [bcw2sQ_7TDanY8ly9F5hdA] would have more than the allowed 10% free disk threshold (4.7% free), preventing allocation&#8221;. However the drive where the indices exists has 400gb free, while the index that is being opened is only about 22gb. 

Can we be sure that es is checking the right drive when calculating the free disk space? Is this a bug where it&#8217;s either do the calculation wrong or isn&#8217;t taking into consideration the custom data path when doing it? Or maybe this is a different issue altogether? The part that makes it strange is how issuing another command on a different index will allow the shards to be allocated. 

[2015-09-14 18:56:21,424][INFO ][cluster.metadata         ] [Assassin] closing indices [[centrallogging_hawthorne-2015-08-23]]
[2015-09-14 18:56:31,547][INFO ][cluster.metadata         ] [Assassin] opening indices [[centrallogging_hawthorne-2015-08-23]]
[2015-09-14 18:56:31,626][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [bcw2sQ_7TDanY8ly9F5hdA] would have more than the allowed 10% free disk threshold (4.7% free), preventing allocation
[2015-09-14 18:56:31,626][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.8% free), preventing allocation
[2015-09-14 18:56:31,961][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.8% free), preventing allocation
[2015-09-14 18:56:32,150][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.8% free), preventing allocation
[2015-09-14 18:56:32,150][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [bcw2sQ_7TDanY8ly9F5hdA] would have more than the allowed 10% free disk threshold (4.5% free), preventing allocation
[2015-09-14 18:56:32,151][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.6% free), preventing allocation
[2015-09-14 18:56:32,254][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.8% free), preventing allocation
[2015-09-14 18:56:32,254][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [bcw2sQ_7TDanY8ly9F5hdA] would have more than the allowed 10% free disk threshold (4.7% free), preventing allocation
[2015-09-14 18:56:32,255][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.7% free), preventing allocation
[2015-09-14 18:56:32,255][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [bcw2sQ_7TDanY8ly9F5hdA] would have more than the allowed 10% free disk threshold (4.5% free), preventing allocation
[2015-09-14 18:56:32,255][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.6% free), preventing allocation
[2015-09-14 18:56:32,279][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [bcw2sQ_7TDanY8ly9F5hdA] would have more than the allowed 10% free disk threshold (4.7% free), preventing allocation
[2015-09-14 18:56:32,279][WARN ][cluster.routing.allocation.decider] [Assassin] After allocating, node [_xDllWgUTIi6f-LLoDY54g] would have more than the allowed 10% free disk threshold (2.7% free), preventing allocation
</description><key id="106600712">13582</key><summary>Unable to allocate shards because elasticsearch doesn't free calculate disk space correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jinahn</reporter><labels /><created>2015-09-15T17:01:47Z</created><updated>2015-09-18T15:50:28Z</updated><resolved>2015-09-18T15:50:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-15T20:36:33Z" id="140530025">&gt; 1. We are also using templates that sets shadow replica to be true,
&gt;    set the number of replicas to 1, and use the shadowcopy tag
&gt; 2. Also within the templates we use a index.data_path. All 3 data
&gt;    nodes have enable_custom_paths set to true in the configs.

The DiskThresholdDecider does not currently look at custom data paths at
all, so it will still be looking for the directory that is configured in
`path.data`.

For this, I would recommend that you disable the disk threshold decider
until it can correctly calculate data for custom data paths.

&gt; a. The path that data_path points to is a nfs share that is mounted on
&gt; all the nodes (master included)

As an aside, I want to caution you about using NFS, as NFS does not correctly handle(0)
a lot of the things Apache Lucene relies on working correctly for a
filesystem.

(0): http://www.time-travellers.org/shane/papers/NFS_considered_harmful.html
</comment><comment author="jinahn" created="2015-09-16T16:47:03Z" id="140799355">@dakrone Thanks for the info!!

Is the DiskThresholdDecider dsiabled by setting cluster.routing.allocation.disk.threshold_enabled to false in the yaml file? Or is it done some otherway? 
</comment><comment author="dakrone" created="2015-09-16T17:45:43Z" id="140816562">&gt; Is the DiskThresholdDecider dsiabled by setting
&gt; `cluster.routing.allocation.disk.threshold_enabled` to false in the
&gt; yaml file? Or is it done some otherway?

That's correct. You can also dynamically change it with the cluster
settings API.
</comment><comment author="clintongormley" created="2015-09-18T15:50:28Z" id="141488764">I'm going to close this in favour of #13583
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[cloud-aws] info message is displayed when not using ec2 discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13581</link><project id="" key="" /><description>In 2.0, aws plugin now tries to start immediately ec2 discovery even if we don't set `discovery.type: ec2`.

For example, if you want to use only S3 feature, this message does not make sense:

```
[2015-09-15 16:44:16,336][INFO ][org.elasticsearch.discovery.ec2] [Ocelot] Exception while retrieving instance list from AWS API: Unable to load AWS credentials from any provider in the chain
```

It should not be an issue anymore in 3.0 as aws is split in 2 plugins, so we can guess that if someone installs the discovery-ec2 plugin, it's probably to have discovery set to ec2.
</description><key id="106598311">13581</key><summary>[cloud-aws] info message is displayed when not using ec2 discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>adoptme</label></labels><created>2015-09-15T16:48:34Z</created><updated>2015-10-02T18:16:43Z</updated><resolved>2015-10-02T18:16:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tests: Possible search context leak in DeleteByQueryTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13580</link><project id="" key="" /><description>See examples here: 
http://build-us-00.elastic.co/job/es_core_master_centos/7546/
http://build-us-00.elastic.co/job/es_g1gc_master_metal/17398/
http://build-us-00.elastic.co/job/es_core_master_window-2012/1735/

The tests fail in [assertSearchContextsClosed](https://github.com/elastic/elasticsearch/blob/master/plugins/delete-by-query/src/test/java/org/elasticsearch/plugin/deletebyquery/DeleteByQueryTests.java#L446) with the error:

```
FAILURE 0.06s J2 | DeleteByQueryTests.testDeleteByQueryWithNoIndices &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: 
   &gt; Expected: &lt;0L&gt;
   &gt;      but: was &lt;1L&gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([1B261257DA113457:444F85E3E99286BE]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.plugin.deletebyquery.DeleteByQueryTests.assertSearchContextsClosed(DeleteByQueryTests.java:446)
   &gt;    at org.elasticsearch.plugin.deletebyquery.DeleteByQueryTests.testDeleteByQueryWithNoIndices(DeleteByQueryTests.java:73)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Once a test fails all tests after it would fail as well. I wasn't able to reproduce the tests with the seed that it was failing with in CI, but I was able to reproduce it once by running the entire suite many times with random seeds. Unfortunately, after it happened it doesn't fail on my machine anymore. 
</description><key id="106592950">13580</key><summary>Tests: Possible search context leak in DeleteByQueryTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">imotov</reporter><labels><label>jenkins</label></labels><created>2015-09-15T16:22:30Z</created><updated>2015-10-19T09:49:17Z</updated><resolved>2015-10-19T09:49:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-16T17:02:24Z" id="140804561">Another example:
http://build-us-00.elastic.co/job/es_g1gc_master_metal/17555/
</comment><comment author="tlrx" created="2015-09-17T10:20:33Z" id="141037063">Test bug: the scroll id (and thus the underlying search context) used in DeleteByQuery is cleared in an async manner at the end of the scrolling in TransportDeleteByQueryAction. So the test should wait a bit for the search context to be released.
</comment><comment author="areek" created="2015-09-24T04:23:36Z" id="142807602">Another instance of this failure: http://build-us-00.elastic.co/job/es_core_2x_regression/118/, seems like it was back ported to 2.x as well
</comment><comment author="javanna" created="2015-10-13T12:02:42Z" id="147695128">I am reopening this, the test still fails at times, I couldn't repro yet though unfortunately: 
http://build-us-00.elastic.co/job/es_core_master_window-2008/2386/
http://build-us-00.elastic.co/job/es_feature_ingest/194/
</comment><comment author="s1monw" created="2015-10-18T20:48:59Z" id="149046408">this is caused by a stats bug fixed in #13801
</comment><comment author="s1monw" created="2015-10-19T09:49:17Z" id="149168141">fixed by #13801
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run bin/plugin as elasticsearch in tar distro</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13579</link><project id="" key="" /><description>Before this commit he tests always run bin/plugin as root which is somewhat
unrealistic and causes trouble (log files owned by root instead of
elasticsearch). After this commit `bin/plugin` runs as root when elasticsearch
is installed via the repository and as elasticsearch otherwise which is much
more realistic.

This also adds extra timeout to starting elasticsearch which is required
when all the plugins are installed. And it fixes up a problem with logging
elasticsearch's log if elasticsearch doesn't start which came up multiple
time while debugging this problem.

Also adds a documentation update noting the right user to run `bin/plugin`.

Closes #13557
</description><key id="106585334">13579</key><summary>Run bin/plugin as elasticsearch in tar distro</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>docs</label><label>review</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T15:46:14Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-22T15:19:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T15:46:46Z" id="140437732">Ping @tlrx and @clintongormley to review the bats and docs.
</comment><comment author="nik9000" created="2015-09-17T13:39:30Z" id="141089856">Rebased to resolve conflicts. There weren't any comments to lose.
</comment><comment author="nik9000" created="2015-09-22T14:00:18Z" id="142297437">@spinscale can you have a look at this?
</comment><comment author="spinscale" created="2015-09-22T14:51:00Z" id="142311737">left two minor comments, tested ran against the two default boxes, failed as expected when I changed the user

thus LGTM
</comment><comment author="nik9000" created="2015-09-22T15:17:06Z" id="142320970">&gt; thus LGTM

I think in that case I'll merge as is because this is sitting in the way of 2.0.0. If we want to clarify it we certainly can make another pull request.
</comment><comment author="nik9000" created="2015-09-22T15:45:53Z" id="142329027">I squashed and merged to master and cherry-picked to 2.x and 2.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Google Cloud Storage repository plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13578</link><project id="" key="" /><description>This pull request adds a new type of repository `gcs` to allow to snapshot/restore indices on Google Cloud Storage service:

```
curl -XPUT 'localhost:9200/_snapshot/my_gcs_repo' -d '                       
{                 
  "type": "gcs", 
  "settings":  {
      "project_id": "my-project-id", 
      "bucket":"my-bucket", 
      "credentials": "/path/to/service-account-file.json"
   }
}
```

For now it only supports authentication using [Service Account](https://cloud.google.com/storage/docs/authentication#service_accounts) files.

Many things still need to be done but I'd be glad to benefit from a first round of review.
- [x] add integration tests
- [x] add license files
- [x] add documentation
- [ ] add HTTP proxy support
- [ ] add HTTP proxy support with custom java key stores

Closes #12880 
</description><key id="106585253">13578</key><summary>Add Google Cloud Storage repository plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Snapshot/Restore</label><label>feature</label><label>v5.0.0-alpha3</label></labels><created>2015-09-15T15:45:47Z</created><updated>2016-05-19T12:09:05Z</updated><resolved>2016-05-19T12:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-09-15T15:46:34Z" id="140437676">Note: it needs #13574 to work
</comment><comment author="imotov" created="2015-10-07T15:13:44Z" id="146226112">Looks like a good start. Left a couple of comments.
</comment><comment author="tlrx" created="2015-11-30T21:35:38Z" id="160768882">@imotov Thanks for your review. I updated the code following your comments and I also rebased the pull request so that it now works with gradle and the security manager.

Once #14050 is in I should be able to add tests.
</comment><comment author="imotov" created="2015-12-09T21:28:57Z" id="163397052">Left some comments. Let's figure out how to add some tests to this thing.
</comment><comment author="ghost" created="2016-01-17T19:25:28Z" id="172369042">Hi, is there any updates on this issue?
</comment><comment author="chrislovecnm" created="2016-01-21T20:25:01Z" id="173697647">:1
</comment><comment author="dakrone" created="2016-04-06T20:51:45Z" id="206561924">ping @tlrx, I think this needs comments to be addressed and then another review?
</comment><comment author="tlrx" created="2016-04-08T16:40:19Z" id="207507278">&gt; ping @tlrx, I think this needs comments to be addressed and then another review?

Sure, thanks for the kindly reminder @dakrone . Let's add an update to this pull request and some explanation why it takes so long to get in.

###### A bit of history

I started to work on this plugin when elasticsearch was around version 1.4. At this time, the Google Client library was almost incompatible with the way Snapshot/Restore worked in ES. We try to find a workaround for some time but the solution was hardly maintainable and had poor performance. Around ES 1.5 or 1.6 the internal snapshot/restore changed a bit and at the same time the Google library improved a lot.  Great! But... Elasticsearch was on its way to 2.x and a lot of things changed and improved too: we moved the plugins from their dedicated repository back in elasticsearch repo, then move from maven to gradle, enabled the security manager, changed the way integration tests worked for plugins. It has been a lot of amazing improvements but keeping this pull request up-to-date required a LOT of attention and time. Knowing that things would stabilize, I put this on a shelve, I strongly apologize for that.

###### Current status

Anyway, I rebased and updated the code today to make it work with elasticsearch 5.0.0-alpha1-SNAPSHOT. It has been tested locally and on Google Compute Engine and it works nicely.

If you want to test it, you can check out this pull request, build it locally using Gradle and deploy it in a elasticsearch 5.0.0-alpha1-SNAPSHOT (download it [here](https://oss.sonatype.org/content/repositories/snapshots/org/elasticsearch/distribution/zip/elasticsearch/5.0.0-alpha1-SNAPSHOT/)). This pull request contains a documentation page.

@imotov Can you please have another look when you have time? That would be great!
</comment><comment author="imotov" created="2016-04-08T23:14:05Z" id="207643333">Left a few comments. Looks pretty good. I think we can simplify thing by getting rid of `GoogleCloudStorageClient` and just move all its functionality directly into `GoogleCloudStorageBlobStore` and `GoogleCloudStorageBlobContainer`. Also, what about `ESBlobStoreRepositoryIntegTestCase`?
</comment><comment author="haizaar" created="2016-04-18T12:04:21Z" id="211350889">&gt; Current status
&gt; 
&gt; Anyway, I rebased and updated the code today to make it work with elasticsearch 5.0.0-alpha1-SNAPSHOT. It has been tested locally and on Google Compute Engine and it works nicely.
&gt; 
&gt; If you want to test it, you can check out this pull request, build it locally using Gradle and deploy it in a elasticsearch 5.0.0-alpha1-SNAPSHOT (download it here). This pull request contains a documentation page.

@tlrx thanks for the update. Do I read you right that the current plan is to release it as part of ES 5.0?
</comment><comment author="haizaar" created="2016-04-19T09:27:12Z" id="211823045">/CC @motinani
</comment><comment author="tlrx" created="2016-04-29T14:51:09Z" id="215742607">@imotov Thanks for your review! I removed the `GoogleCloudStorageClient` class and moved the implementation in the blob store. 

Concerning the tests, I moved the `ESBlobStoreRepositoryIntegTestCase`, `ESBlobStoreContainerTestCase` and `ESBlobStoreTestCase` classes into the test framework so that they can be inherited in plugins (it looks like we already have such abstract test classes like `AbstractNumericTestCase` in the test framework). Then I created a dedicated test for each of theses classes. I had to turn the `GoogleCloudStorageService` into an interface so that it can be mocked and I implemented a `MockHttpTransport` that simulates the Google Cloud Storage service similarly to what is done for the `discovery-gce` plugin. I think we're good on testing side.

Can you please have another look? That would be great.

@haizaar I can't give you any estimated date for this but I'd be happy to see it ready and testable for a beta release of 5.
</comment><comment author="haizaar" created="2016-04-29T19:42:08Z" id="215858516">Tanguy, thanks for the update. Really looking forward to it.
On 29 Apr 2016 17:52, "Tanguy Leroux" notifications@github.com wrote:

&gt; @imotov https://github.com/imotov Thanks for your review! I removed the
&gt; GoogleCloudStorageClient class and moved the implementation in the blob
&gt; store.
&gt; 
&gt; Concerning the tests, I moved the ESBlobStoreRepositoryIntegTestCase,
&gt; ESBlobStoreContainerTestCase and ESBlobStoreTestCase classes into the
&gt; test framework so that they can be inherited in plugins (it looks like we
&gt; already have such abstract test classes like AbstractNumericTestCase in
&gt; the test framework). Then I created a dedicated test for each of theses
&gt; classes. I had to turn the GoogleCloudStorageService into an interface so
&gt; that it can be mocked and I implemented a MockHttpTransport that
&gt; simulates the Google Cloud Storage service similarly to what is done for
&gt; the discovery-gce plugin. I think we're good on testing side.
&gt; 
&gt; Can you please have another look? That would be great.
&gt; 
&gt; @haizaar https://github.com/haizaar I can't give you any estimated date
&gt; for this but I'd be happy to see it ready and testable for a beta release
&gt; of 5.
&gt; 
&gt; &#8212;
&gt; You are receiving this because you were mentioned.
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13578#issuecomment-215742607
</comment><comment author="imotov" created="2016-05-16T13:47:18Z" id="219428772">Left a minor comment. LGTM.
</comment><comment author="tlrx" created="2016-05-19T12:09:05Z" id="220305502">Thanks @imotov !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when deleting an index whose files have the wrong permissions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13577</link><project id="" key="" /><description>[2015-09-06 09:30:01,684][WARN ][cluster.action.index     ] [hostname] [logstash-2015.08.06]failed to ack index store deleted for  index
java.lang.NullPointerException
        at org.elasticsearch.indices.IndicesService$PendingDelete.toString(IndicesService.java:665)
        at java.lang.String.valueOf(String.java:2994)
        at java.lang.StringBuilder.append(StringBuilder.java:131)
        at java.util.AbstractCollection.toString(AbstractCollection.java:462)
        at org.elasticsearch.indices.IndicesService.processPendingDeletes(IndicesService.java:734)
        at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.lockIndexAndAck(NodeIndexDeletedAction.java:125)
        at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction.access$500(NodeIndexDeletedAction.java:49)
        at org.elasticsearch.cluster.action.index.NodeIndexDeletedAction$2.doRun(NodeIndexDeletedAction.java:113)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

The directory tree for the shard for this index became owned by root. When we sent a request to delete the index, ES threw back lots of errors that it couldn't delete the various files on disk, and then hit this NPE.
</description><key id="106573901">13577</key><summary>NPE when deleting an index whose files have the wrong permissions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>feedback_needed</label></labels><created>2015-09-15T14:53:55Z</created><updated>2016-01-28T17:30:45Z</updated><resolved>2016-01-28T17:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-18T17:43:00Z" id="141517083">Hiya @avleen 

Trying to replicate on 1.7.1 but it seems to work OK. I just chowned data/elasticsearch/nodes/0/

What version are you on? And what path exactly had the wrong owner?
</comment><comment author="bleskes" created="2015-09-18T20:16:11Z" id="141555408">@avleen - which ES version is this?

&gt; On 18 Sep 2015, at 19:43, Clinton Gormley notifications@github.com wrote:
&gt; 
&gt; Hiya @avleen
&gt; 
&gt; Trying to replicate on 1.7.1 but it seems to work OK. I just chowned data/elasticsearch/nodes/0/
&gt; 
&gt; What version are you on? And what path exactly had the wrong owner?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="avleen" created="2015-10-09T02:20:57Z" id="146738244">Hey folks,

Terribly sorry for the delay. This should be 1.6.1 I think.

On Fri, Sep 18, 2015 at 4:16 PM Boaz Leskes notifications@github.com
wrote:

&gt; @avleen - which ES version is this?
&gt; 
&gt; &gt; On 18 Sep 2015, at 19:43, Clinton Gormley notifications@github.com
&gt; &gt; wrote:
&gt; &gt; 
&gt; &gt; Hiya @avleen
&gt; &gt; 
&gt; &gt; Trying to replicate on 1.7.1 but it seems to work OK. I just chowned
&gt; &gt; data/elasticsearch/nodes/0/
&gt; &gt; 
&gt; &gt; What version are you on? And what path exactly had the wrong owner?
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13577#issuecomment-141555408
&gt; .
</comment><comment author="clintongormley" created="2015-10-13T07:26:19Z" id="147628342">@avleen any more information on which path had the bad perms?
</comment><comment author="clintongormley" created="2016-01-28T17:30:45Z" id="176293531">Nothing further. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion Suggester: Support returning documents with completions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13576</link><project id="" key="" /><description>Note: the PR is against `feature/completion_suggester_v2` branch

Currently, the new completion suggester (https://github.com/elastic/elasticsearch/pull/11740) can associate a document with a completion. This PR adds support to return documents enriched with their completions.

Example request:

``` bash
POST music/_suggest 
{
  "song-suggest" : {
    "prefix" : "nev",
    "completion" : {
      "field" : "song_suggest"
    }
  }
}
```

Example response:

``` bash
{
  "song-suggest": [
    {
      "text": "nev",
      "offset": 0,
      "length": 4,
      "options": [ {
          "text": "Nevermind",
          "_index": "music",
          "_type": "song",
          "_id": "52",
          "_score": 52,
          "_source": {
            ....
          }
        }, ...
      ]
    }
  ]
}
```

[Source filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-source-filtering.html) options are supported. 

Internally a suggestion request is represented as a search request implementing a two-phased suggest then fetch transport action for multiple shards and a single-phased suggest and fetch phase for single shard.
NOTE: the two-phased suggest transport action has yet to be integrated with the _search API
</description><key id="106566461">13576</key><summary>Completion Suggester: Support returning documents with completions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label></labels><created>2015-09-15T14:20:02Z</created><updated>2016-08-15T15:51:23Z</updated><resolved>2016-08-15T15:51:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-15T19:43:17Z" id="140515871">I looked at this and I like the code but that PR is very delicate and big. I wonder if it would make more sense if we land the branch without this feature on 2.x and move this to 3.0. I think we can generally refactor how we fetch documents and reuse much code if we do it right. I don't know how others feel @mikemccand @clintongormley 
</comment><comment author="mikemccand" created="2015-09-15T19:53:36Z" id="140518141">The change is indeed biggish ...

If we land completion suggester for 2.1 without fetching docs (this PR), we've lost some functionality (payloads) vs the current suggester impls, right?  Is that OK, or are payloads frequently used?

If we do delay this PR, why would we need to delay fetching docs until 3.0?  Couldn't we target e.g. 2.2 or something?
</comment><comment author="s1monw" created="2015-09-15T20:00:10Z" id="140519723">&gt; If we do delay this PR, why would we need to delay fetching docs until 3.0? Couldn't we target e.g. 2.2 or something?

the problem here is I guess that we can't really use this feature until we know all nodes support it otherwise you'd get partial results? The other option would be to return docs eagerly and don't do a second roundtrip which would be ok if fetching stuff from DocValues etc? 
</comment><comment author="mikemccand" created="2015-09-16T09:58:41Z" id="140692895">&gt; the problem here is I guess that we can't really use this feature until we know all nodes support it otherwise you'd get partial results? 

Oh, I see, because a mixed minor version cluster is allowed... hrm.

&gt; The other option would be to return docs eagerly and don't do a second roundtrip which would be ok if fetching stuff from DocValues etc?

This would simplify things greatly (not needing 2nd fetch phase), but then there's some perf hit since we're pulling docs/docvalues for many hits that we then throw away when we merge suggestions across all shards ... and I wonder how much of a perf hit we are talking about even w/ the 2nd fetch phase since the current suggesters pull payloads directly from the FST.
</comment><comment author="s1monw" created="2015-09-16T10:11:52Z" id="140695698">&gt; This would simplify things greatly (not needing 2nd fetch phase), but then there's some perf hit since we're pulling docs/docvalues for many hits that we then throw away when we merge suggestions across all shards ... and I wonder how much of a perf hit we are talking about even w/ the 2nd fetch phase since the current suggesters pull payloads directly from the FST.

I think what we should do is this:
- for 2.1 we only implement a payloads feature to fetch stuff from docvalues eagerly to have the same functionality as the previous one.
- for master we remove the SuggestReqeust/SuggestResponse/TransportSuggestAction completely and convert `_suggest` to be sugar for `_search` without a query and only suggest. 
- once that's in we can implement all the second roundtrip as part of the search and things become much simpler.

WDYT? I discussed this briefly with @areek yesterday night already
</comment><comment author="mikemccand" created="2015-09-16T12:54:11Z" id="140732929">+1 to that plan.
</comment><comment author="areek" created="2015-09-16T20:48:53Z" id="140890356">&gt; I think what we should do is this:
&gt; - for 2.1 we only implement a payloads feature to fetch stuff from docvalues eagerly to have the same functionality as the previous one.
&gt; - for master we remove the SuggestReqeust/SuggestResponse/TransportSuggestAction completely and convert `_suggest` to be sugar for `_search` without a query and only suggest. 
&gt; - once that's in we can implement all the second roundtrip as part of the search and things become much simpler.

This makes sense. I am working on a PR to support fetching docvalues eagerly for 2.1. I will open up followup issues to do the cleanup/enhancement in master.
</comment><comment author="areek" created="2015-09-18T17:37:13Z" id="141515642">I opened https://github.com/elastic/elasticsearch/pull/13659 which implements docvalues based payload support for 2.1, would appreciate a review :)

&gt; but then there's some perf hit since we're pulling docs/docvalues for many hits that we then throw away when we merge suggestions across all shards ... and I wonder how much of a perf hit we are talking about

@mikemccand I plan to benchmark https://github.com/elastic/elasticsearch/pull/13659 against the current suggester payload support to try to shed some light on this
</comment><comment author="clintongormley" created="2016-03-10T11:57:19Z" id="194811744">@areek @s1monw @mikemccand wondering if we should get this in for 5.0?
</comment><comment author="areek" created="2016-03-18T21:50:15Z" id="198557886">@clintongormley @s1monw @mikemccand I opened https://github.com/elastic/elasticsearch/issues/17198, this will simplify adding document fetching to `completion`
</comment><comment author="areek" created="2016-08-15T15:51:23Z" id="239840975">closed as functionality has been implemented in #19536 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.math.LongMath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13575</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.math.LongMath across the codebase. This is one step
of many in the eventual removal of Guava as a dependency.

Relates elastic/elasticsearch#13224
</description><key id="106564137">13575</key><summary>Remove and forbid use of com.google.common.math.LongMath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-15T14:07:59Z</created><updated>2015-09-15T14:28:50Z</updated><resolved>2015-09-15T14:28:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T14:26:25Z" id="140409642">LGTM
</comment><comment author="jasontedor" created="2015-09-15T14:27:02Z" id="140409785">@nik9000 Thanks so much for reviewing again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix blob size in writeBlob() method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13574</link><project id="" key="" /><description>It seems that #13434 computes a wrong blob size when writing snapshot files.

@imotov Can you please have a look?
</description><key id="106563958">13574</key><summary>Fix blob size in writeBlob() method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Snapshot/Restore</label><label>bug</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-15T14:07:00Z</created><updated>2015-09-30T10:42:23Z</updated><resolved>2015-09-30T10:34:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2015-09-16T22:05:41Z" id="140908440">Great catch! I wonder if it might make more sense to move this logic into fileInfo. So `long partBytes();` would become `long partBytes(int part); similar to how`partName` returns the name for each part. What do you think?
</comment><comment author="tlrx" created="2015-09-18T14:40:52Z" id="141471940">@imotov thanks for your review! I agree with your last comment and updated the code. Can you please have another look?

I tested it for #13578.
</comment><comment author="imotov" created="2015-09-22T19:34:35Z" id="142394214">Left one minor comment. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable SSL for Azure blob storage </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13573</link><project id="" key="" /><description>In order to increase Security.

The Microsoft Azure storage services support both HTTP and HTTPS; however, using HTTPS is highly recommended.
</description><key id="106555971">13573</key><summary>Enable SSL for Azure blob storage </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">pandujar</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Repository Azure</label><label>enhancement</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T13:26:06Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-18T14:32:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-15T13:32:51Z" id="140394656">Similar to #13482 but might be more complete :)
</comment><comment author="dadoonet" created="2015-09-18T14:33:26Z" id="141470285">Thank you! Merged in master and in 2.x branch with 2f88e5d75129e2cd5a2889f4a49bb5eaaff0db03
</comment><comment author="dadoonet" created="2015-09-18T15:12:23Z" id="141479333">And ported in 2.0 as well with a3bcc436b62e7f09670a8c8f40d5e819d9d5ab3c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.base.Joiner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13572</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.base.Joiner across the codebase. This is one of many
steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106552037">13572</key><summary>Remove and forbid use of com.google.common.base.Joiner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-15T13:11:57Z</created><updated>2015-09-15T14:30:57Z</updated><resolved>2015-09-15T14:30:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-15T13:30:17Z" id="140394141">Merge conflicts! Oh no!
</comment><comment author="nik9000" created="2015-09-15T13:35:51Z" id="140395288">LGTM. Java 8 has effectively replaced Joiner.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES returns a pair for lat and lon fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13571</link><project id="" key="" /><description>Hi,

I've recently upgraded ES from version 1.4.4 to 1.7.1 and now I have an issue with lat and lon indexed fields for geo_point.
In my index I have a field defined as follows:

```
"geolocation": {
    "lat_lon": true,
    "geohash_prefix": true,
    "type": "geo_point",
    "geohash": true
}
```

When issuing the query

```
{
"fields":["geolocation.lat","geolocation.lon","geolocation"],
  "query": {
    "filtered" : {
        "query" : {
            "match_all" : {}
        },

        "filter" : {
            "geo_distance" : {
                "distance" : "20km",
                "geolocation" : {
                    "lat" : 37.9174,
                    "lon" : -122.3050
                }
            }
        }
    }
  }
}
```

this is the response

```
"hits": {
    "total": 3995,
    "max_score": 1,
    "hits": [
        {
            "_index": "idx",
            "_type": "event",
            "_id": "466201666890120",
            "_score": 1,
            "fields": {
                "geolocation": [
                    "37.768991279975,-122.41938924477"
                ],
                "geolocation.lon": [
                    "37.768991279975,-122.41938924477"
                ],
                "geolocation.lat": [
                    "37.768991279975,-122.41938924477"
                ]
            }
        }
```

In other words geolocation.lat and geolocation.lon contain the pair and not just the single value.
Version 1.4.4 worked properly as expected.
</description><key id="106525895">13571</key><summary>ES returns a pair for lat and lon fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marfago</reporter><labels /><created>2015-09-15T10:18:27Z</created><updated>2015-09-19T12:29:55Z</updated><resolved>2015-09-19T12:29:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T12:29:55Z" id="141660368">Hi @marfago 

I've tried out what you say on 1.4.4 and I think you're incorrect about this having changed.  It depends entirely on how you index your geopoints.  For instance:

```
PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "geolocation": {
          "lat_lon": true,
          "geohash_prefix": true,
          "type": "geo_point",
          "geohash": true
        }
      }
    }
  }
}

PUT t/t/1
{
  "geolocation": "37.768991279975,-122.41938924477"
}

PUT t/t/2
{
  "geolocation": {
    "lat": 37.768991279975,
    "lon": -122.41938924477
  }
}

GET _search
{
  "fields": [
    "geolocation.lat",
    "geolocation.lon",
    "geolocation"
  ]
}
```

returns the following:

```
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "1",
        "_score": 1,
        "fields": {
           "geolocation": [
              "37.768991279975,-122.41938924477"
           ],
           "geolocation.lon": [
              "37.768991279975,-122.41938924477"
           ],
           "geolocation.lat": [
              "37.768991279975,-122.41938924477"
           ]
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "2",
        "_score": 1,
        "fields": {
           "geolocation.lon": [
              -122.41938924477
           ],
           "geolocation.lat": [
              37.768991279975
           ]
        }
     }
  ]
```

The `fields` parameter is trying to extract the value from the `_source` field.  However, if you use the  fielddata_fields` parameter instead, then you get the actual lat/lon values consistently:

```
GET _search
{
  "fielddata_fields": [
    "geolocation.lat",
    "geolocation.lon",
    "geolocation"
  ]
}
```

Returns:

```
  "hits": [
     {
        "_index": "t",
        "_type": "t",
        "_id": "1",
        "_score": 1,
        "_source": {
           "geolocation": "37.768991279975,-122.41938924477"
        },
        "fields": {
           "geolocation": [
              {
                 "lat": 37.768991279975,
                 "lon": -122.41938924477
              }
           ],
           "geolocation.lon": [
              -122.41938924477
           ],
           "geolocation.lat": [
              37.768991279975
           ]
        }
     },
     {
        "_index": "t",
        "_type": "t",
        "_id": "2",
        "_score": 1,
        "_source": {
           "geolocation": {
              "lat": 37.768991279975,
              "lon": -122.41938924477
           }
        },
        "fields": {
           "geolocation": [
              {
                 "lat": 37.768991279975,
                 "lon": -122.41938924477
              }
           ],
           "geolocation.lon": [
              -122.41938924477
           ],
           "geolocation.lat": [
              37.768991279975
           ]
        }
     }
  ]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date math index names should work too when an index doesn't exist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13570</link><project id="" key="" /><description>Date math index names should also work when indexing documents into a non existing index.
</description><key id="106508254">13570</key><summary>Date math index names should work too when an index doesn't exist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Index APIs</label><label>bug</label><label>review</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T08:32:56Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-25T10:07:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-23T07:29:00Z" id="142519056">left a small comment, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES failing with "marking and sending shard failed due to [failed recovery]"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13569</link><project id="" key="" /><description>We have a ES Cluster of 1.5.2 with 2 nodes in it. The cluster has been working smoothly for more than 4months until it went into some kind of Failed Shards errors and has stopped working.

Below is the huge number of errors reported by the cluster on both nodes, and stopping any sending (Logstash) or receiving application (Kibana here) from connecting to it.

Recently it also started causing Memory exhaustion as well on one of the nodes, when I had to shut down our ES completely.

Can anyone advise on the possible reason of the failure and any fix for it?

**************\* Exception in logs **********************
[2015-09-15 00:02:52,692][WARN ][transport                ] [fil_middleware_clus01_node02] Received response for a request that has timed out, sent [30761ms] ago, timed out [760ms] ago, action [in
ternal:discovery/zen/fd/master_ping], node [[fil_middleware_clus01_node01][cYhvLzXwT92_0EsM6IEvDg][ukx06621.uk.fid-intl.com][inet[/10.60.172.71:9301]]{master=true}], id [893623]
[2015-09-15 00:02:52,726][WARN ][indices.cluster          ] [fil_middleware_clus01_node02] [[.mw_kibana02][0]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [.mw_kibana02][0]: Recovery failed from [fil_middleware_clus01_node01][cYhvLzXwT92_0EsM6IEvDg][ukx06621.uk.fid-intl.com][inet[/10.60.172
.71:9301]]{master=true} into [fil_middleware_clus01_node02][DVKSAI0dTyWk3u_nkcaVoA][ukx06622.uk.fid-intl.com][inet[/10.60.172.72:9302]]{master=true}
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:274)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:69)
        at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:550)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.transport.RemoteTransportException: [fil_middleware_clus01_node01][inet[/10.60.172.71:9301]][internal:index/shard/recovery/start_recovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [.mw_kibana02][0] Phase[1] Execution failed
        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:842)
        at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:699)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
        at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [.mw_kibana02][0] Failed to transfer [0] files with total size of [0b]
        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:413)
        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:837)
        ... 10 more
Caused by: java.io.IOException: directory '/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/.mw_kibana02/0/index' exists and is a directory, but cannot be listed: list() returned null
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:226)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:237)
        at org.elasticsearch.index.store.fs.DefaultFsDirectoryService$1.listAll(DefaultFsDirectoryService.java:57)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:532)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:528)
        at org.elasticsearch.index.store.Store.getMetadata(Store.java:219)
        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:159)
        ... 11 more
[2015-09-15 00:02:52,743][WARN ][indices.cluster          ] [fil_middleware_clus01_node02] [[logstash-ctm-stats-2015.04.29][4]] marking and sending shard failed due to [failed recovery]
org.elasticsearch.indices.recovery.RecoveryFailedException: [logstash-ctm-stats-2015.04.29][4]: Recovery failed from [fil_middleware_clus01_node01][cYhvLzXwT92_0EsM6IEvDg][ukx06621.uk.fid-intl.com
][inet[/10.60.172.71:9301]]{master=true} into [fil_middleware_clus01_node02][DVKSAI0dTyWk3u_nkcaVoA][ukx06622.uk.fid-intl.com][inet[/10.60.172.72:9302]]{master=true}
        at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:274)
        at org.elasticsearch.indices.recovery.RecoveryTarget.access$700(RecoveryTarget.java:69)
        at org.elasticsearch.indices.recovery.RecoveryTarget$RecoveryRunner.doRun(RecoveryTarget.java:550)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [logstash-ctm-stats-2015.04.29][4] Phase[1] Execution failed
        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:842)
        at org.elasticsearch.index.shard.IndexShard.recover(IndexShard.java:699)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:125)
        at org.elasticsearch.indices.recovery.RecoverySource.access$200(RecoverySource.java:49)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:146)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:132)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.doRun(MessageChannelHandler.java:277)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:36)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.indices.recovery.RecoverFilesRecoveryException: [logstash-ctm-stats-2015.04.29][4] Failed to transfer [0] files with total size of [0b]
        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:413)
        at org.elasticsearch.index.engine.InternalEngine.recover(InternalEngine.java:837)
        ... 10 more
Caused by: java.io.IOException: directory '/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/logstash-ctm-stats-2015.04.29/4/index' exists and is a directory, but cannot be listed: list()
 returned null
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:226)
        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:237)
        at org.elasticsearch.index.store.fs.DefaultFsDirectoryService$1.listAll(DefaultFsDirectoryService.java:57)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:532)
        at org.elasticsearch.index.store.Store.failIfCorrupted(Store.java:528)
        at org.elasticsearch.index.store.Store.getMetadata(Store.java:219)
        at org.elasticsearch.indices.recovery.RecoverySourceHandler.phase1(RecoverySourceHandler.java:159)
        ... 11 more
</description><key id="106507385">13569</key><summary>ES failing with "marking and sending shard failed due to [failed recovery]"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ashwgupt</reporter><labels><label>feedback_needed</label></labels><created>2015-09-15T08:27:28Z</created><updated>2016-01-28T17:30:27Z</updated><resolved>2016-01-28T17:30:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T12:17:00Z" id="141659933">What's happening with your data store?  eg:

```
Caused by: java.io.IOException: directory '/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/.mw_kibana02/0/index' exists and is a directory, but cannot be listed: list() returned null
Caused by: java.io.IOException: directory '/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/logstash-ctm-stats-2015.04.29/4/index' exists and is a directory, but cannot be listed: list() returned null
```
</comment><comment author="ashwgupt" created="2015-09-21T10:35:43Z" id="141936618">I don't see any issue with it though.

The listing works perfectly fine when done on the server.

/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/logstash-ctm-stats-2015.04.29/4/index$ ls -ltr
total 24
-rw-rw-r--. 1 elkmwpoc elkmwpoc    0 Aug  3 13:51 write.lock
-rw-rw-r--. 1 elkmwpoc elkmwpoc 5539 Aug  3 13:51 _0.cfs
-rw-rw-r--. 1 elkmwpoc elkmwpoc  308 Aug  3 13:51 _0.cfe
-rw-rw-r--. 1 elkmwpoc elkmwpoc  257 Aug  3 13:51 _0.si
-rw-rw-r--. 1 elkmwpoc elkmwpoc  128 Aug  3 14:21 segments_2
-rw-rw-r--. 1 elkmwpoc elkmwpoc   36 Aug  3 14:21 segments.gen

/elkmwpoc/ES/data/fil_middleware_cluster01/nodes/0/indices/.mw_kibana02/0/index$ ls -ltr
total 172
-rw-rw-r--. 1 elkmwpoc elkmwpoc    0 Jun  1 09:58 write.lock
-rw-rw-r--. 1 elkmwpoc elkmwpoc   64 Jul  7 13:21 _n.fdx
-rw-rw-r--. 1 elkmwpoc elkmwpoc 3848 Jul  7 13:21 _n.fdt
-rw-rw-r--. 1 elkmwpoc elkmwpoc  705 Jul  7 13:21 _n_Lucene41_0.tip
-rw-rw-r--. 1 elkmwpoc elkmwpoc 8442 Jul  7 13:21 _n_Lucene41_0.tim
-rw-rw-r--. 1 elkmwpoc elkmwpoc 3623 Jul  7 13:21 _n_Lucene41_0.pos
-rw-rw-r--. 1 elkmwpoc elkmwpoc  961 Jul  7 13:21 _n_Lucene41_0.doc
-rw-rw-r--. 1 elkmwpoc elkmwpoc   90 Jul  7 13:21 _n_Lucene410_0.dvm
-rw-rw-r--. 1 elkmwpoc elkmwpoc   52 Jul  7 13:21 _n_Lucene410_0.dvd
-rw-rw-r--. 1 elkmwpoc elkmwpoc  426 Jul  7 13:21 _n.si
-rw-rw-r--. 1 elkmwpoc elkmwpoc  181 Jul  7 13:21 _n.nvm
-rw-rw-r--. 1 elkmwpoc elkmwpoc  582 Jul  7 ...........
</comment><comment author="ashwgupt" created="2015-09-21T10:36:28Z" id="141936699">To me it looks like ES is failing for some of it's internal fault and unable to deal with it, as the whole of cluster has failed at once.
</comment><comment author="jasontedor" created="2015-09-21T10:48:47Z" id="141939363">A possible cause of the `IOException` that @clintongormley drew attention to is running into system limits on the number of open file descriptors. Could you try checking your [file descriptor configuration](https://www.elastic.co/guide/en/elasticsearch/reference/1.4/setup-configuration.html#file-descriptors) and increase it appropriately based on your specific system (`ulimit`, etc.)?
</comment><comment author="clintongormley" created="2016-01-28T17:30:27Z" id="176293387">No further feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Startup: Limit metaspace max size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13568</link><project id="" key="" /><description>With the introduction of the metaspace replacing the perm gen space in java 8, the way I understand it, that now native memory outside of the heap is used for this area. Also it is configured to be unbounded by default, so if a classloader is leaky (main question is, can this happen, potentially by plugins?), we might not only go OOM with the VM, but with the whole operating system/container.

We might want to check if we can configure `-XX:MaxMetaspaceSize` on startup. Maybe there are other tools like a leak detector (I've seen once on github, not sure how useful, seemed to be for servlets) or this is not a problem at all the way we do things.

Putting it up for discussion thus.
</description><key id="106505179">13568</key><summary>Startup: Limit metaspace max size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>discuss</label><label>enhancement</label></labels><created>2015-09-15T08:13:16Z</created><updated>2016-02-16T15:30:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danielmitterdorfer" created="2015-11-09T21:42:03Z" id="155207146">+1. Unbounded growth for finite resources is a recipe for disaster.
</comment><comment author="robgil" created="2016-02-16T15:30:49Z" id="184729575">+1 for less sleepless nights
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bucket Selector Aggregation does not accept script file as input</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13567</link><project id="" key="" /><description>Using elasticsearch 2.0.0-beta1 with the following search parameters:

```
{ some_filter:
      bucket_selector:
          buckets_path:
              some_variable: "some_index&gt;some_index_value.value"
          script: "bucket-selector"
}
```

The `script` attribute treats `bucket-selector` as an inline/indexed script which causes the ff error:

```
{"type":"script_exception","reason":"scripts of type [inline], operation [aggs] and lang [groovy] are disabled"}
```

And `script_file` may not yet be implemented with the bucket selector aggregation:

```
{"type":"search_parse_exception","reason":"Unknown key for a VALUE_STRING in [some_filter]: [script_file] }
```
</description><key id="106478068">13567</key><summary>Bucket Selector Aggregation does not accept script file as input</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adimasuhid</reporter><labels /><created>2015-09-15T04:51:45Z</created><updated>2015-09-19T12:23:33Z</updated><resolved>2015-09-19T11:32:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T11:32:50Z" id="141653484">Hi @adimasuhid 

The syntax for scripts has changed in 2.0.  See https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_scripting_changes.html

You need to specify a file script as:

```
"script": {
    "file": "name-of-file"
}
```
</comment><comment author="adimasuhid" created="2015-09-19T12:23:33Z" id="141660141">Got it thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increment tribe node version on updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13566</link><project id="" key="" /><description>Currently the tribe node version always stays 0, which can cause issues for the services that rely on cluster state version. For example, ClusterStateObserver doesn't revalidate the cluster state after change, which leads to cluster health check with wait flags to take much longer then actually needed.
</description><key id="106469478">13566</key><summary>Increment tribe node version on updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Tribe Node</label><label>bug</label><label>review</label><label>v1.7.3</label><label>v2.0.0-rc1</label></labels><created>2015-09-15T03:18:49Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-21T18:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-15T07:40:52Z" id="140308153">Good catch. LGTM. We need the 3.0 &amp; 2.1 labels too imho.

IMHO this should be automatic and you shouldn't have to remember to increment it (just like UUIDs are). I'll give it some thought..
</comment><comment author="imotov" created="2015-09-16T22:31:36Z" id="140912554">The version is incremented by ClusterService during update. The reason it is needed here explicitly is somewhat strange nature of a tribe node with its non-master updates that don't increment version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrading 1.7.1 to 1.7.2 with RPM causes postun script failed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13565</link><project id="" key="" /><description>Upgrading 1.7.1 to 1.7.2 with RPM causes postun script failed.

``` sh
$ sudo rpm -Uvh elasticsearch-1.7.2.noarch.rpm 
warning: elasticsearch-1.7.2.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID d88e42b4: NOKEY
Preparing...                          ################################# [100%]
Updating / installing...
   1:elasticsearch-1.7.2-1            ################################# [ 50%]
Cleaning up / removing...
   2:elasticsearch-1.7.1-1            ################################# [100%]
post remove script called with unknown argument `1'
warning: %postun(elasticsearch-1.7.1-1.noarch) scriptlet failed, exit status 1
```

My environment is here.

``` sh
$ lsb_release -a
LSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch
Distributor ID: AmazonAMI
Description:    Amazon Linux AMI release 2015.03
Release:    2015.03
Codename:   n/a
$ rpm --version
RPM version 4.11.2
```
</description><key id="106448107">13565</key><summary>Upgrading 1.7.1 to 1.7.2 with RPM causes postun script failed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">t2y</reporter><labels /><created>2015-09-14T23:34:58Z</created><updated>2015-09-19T11:19:11Z</updated><resolved>2015-09-19T11:19:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="t2y" created="2015-09-14T23:46:50Z" id="140234780">elasticsearch-1.7.1.noarch.rpm has a bug about handling the status variable in postun script. But it seems elasticsearch-1.7.2.noarch.rpm has already fixed.

``` sh
$ rpm -qp --scripts elasticsearch-1.7.1.noarch.rpm
    ...
    # RedHat ####################################################
    0)
        REMOVE_DIRS=true
        REMOVE_SERVICE=true
        REMOVE_USER_AND_GROUP=true
    ;;
    2)
        # If $1=1 this is an upgrade
        IS_UPGRADE=true
    ;;

    *)
        echo "post remove script called with unknown argument \`$1'" &gt;&amp;2
        exit 1
    ;;
```
</comment><comment author="t2y" created="2015-09-14T23:54:43Z" id="140235716">``` sh
$ diff -u &lt;(rpm -qp --scripts elasticsearch-1.7.1.noarch.rpm) &lt;(rpm -qp --scripts elasticsearch-1.7.2.noarch.rpm)
warning: warning: elasticsearch-1.7.1.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID d88e42b4: NOKEY
elasticsearch-1.7.2.noarch.rpm: Header V4 RSA/SHA1 Signature, key ID d88e42b4: NOKEY
--- /dev/fd/63  2015-09-14 23:54:04.798729340 +0000
+++ /dev/fd/62  2015-09-14 23:54:04.802729248 +0000
@@ -81,7 +81,7 @@
     ;;
 esac

-# Built for elasticsearch-1.7.1 (rpm)
+# Built for elasticsearch-1.7.2 (rpm)
 postinstall scriptlet (using /bin/sh):


@@ -183,7 +183,7 @@
     echo " OK"
 fi

-# Built for elasticsearch-1.7.1 (rpm)
+# Built for elasticsearch-1.7.2 (rpm)
 preuninstall scriptlet (using /bin/sh):


@@ -253,7 +253,7 @@
     echo " OK"
 fi

-# Built for elasticsearch-1.7.1 (rpm)
+# Built for elasticsearch-1.7.2 (rpm)
 postuninstall scriptlet (using /bin/sh):


@@ -265,8 +265,8 @@
 #       $1=purge     : indicates an upgrade
 #
 #   On RedHat,
-#       $1=1         : indicates an new install
-#       $1=2         : indicates an upgrade
+#       $1=0         : indicates a removal
+#       $1=1         : indicates an upgrade



@@ -296,7 +296,7 @@
         REMOVE_SERVICE=true
         REMOVE_USER_AND_GROUP=true
     ;;
-    2)
+    1)
         # If $1=1 this is an upgrade
         IS_UPGRADE=true
     ;;
@@ -377,4 +377,4 @@
     fi
 fi

-# Built for elasticsearch-1.7.1 (rpm)
+# Built for elasticsearch-1.7.2 (rpm)
```
</comment><comment author="clintongormley" created="2015-09-19T11:19:11Z" id="141652578">Duplicate of #12606. Fixed by #12298
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove duplicate code in QueryBuilders and Parsers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13564</link><project id="" key="" /><description>Many queries duplicate code when it comes to:
- resolving field to their index names
- handling regex patterns in fields
- handling boost in fields
- setting up the Analyzer from a string
- serializing enums

We should consolidate these functions in a support class instead. This should
also ease adding those functionalities to the other queries.
</description><key id="106426078">13564</key><summary>Remove duplicate code in QueryBuilders and Parsers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label><label>enhancement</label></labels><created>2015-09-14T20:55:46Z</created><updated>2016-08-24T09:52:00Z</updated><resolved>2015-10-19T12:15:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-19T10:31:22Z" id="149177627">I can't think of anything that is left to do here, @cbuescher what do you think?
</comment><comment author="cbuescher" created="2015-10-19T12:09:44Z" id="149196809">@javanna I think can remove code duplications now that we merged with master as we go along and re-open more specific issues for that.
</comment><comment author="ronghuaxiang" created="2016-08-24T09:52:00Z" id="242012485">`{
  "aggs":{
    "dedup" : {
      "terms":{
        "field": "user_id"
       },
       "aggs":{
         "dedup_docs":{
           "top_hits":{
             "size":1
           }
         }
       }    
    }
  }
}`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove index.buffer_size setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13563</link><project id="" key="" /><description>Spinoff from #13548...

This `index.buffer_size` setting seems silly because it's just the starting buffer size ... yet shortly after the node starts, the IndexingMemoryController will take over and set the indexing buffer size itself.
</description><key id="106425819">13563</key><summary>Remove index.buffer_size setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>enhancement</label><label>v2.0.0-rc1</label></labels><created>2015-09-14T20:54:00Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-15T15:34:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-15T08:46:33Z" id="140325034">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace and ban next batch of Guava classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13562</link><project id="" key="" /><description>This commit replaces and bans:
- com.google.common.util.concurrent.UncheckedExecutionException
- com.google.common.util.concurrent.AtomicLongMap
- com.google.common.primitives.Longs
- com.google.common.io.ByteStreams
- com.google.common.collect.UnmodifiableIterator
- com.google.common.collect.ObjectArrays
- com.google.common.collect.Multimap
- com.google.common.collect.MultimapBuilder

Relates to #13224
</description><key id="106424612">13562</key><summary>Replace and ban next batch of Guava classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-14T20:47:38Z</created><updated>2015-09-15T13:17:20Z</updated><resolved>2015-09-15T13:17:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-14T21:01:01Z" id="140204764">Left small comments otherwise it looks good to me.
</comment><comment author="bleskes" created="2015-09-15T08:31:41Z" id="140320014">LGTM. Left a minor comment.
</comment><comment author="jasontedor" created="2015-09-15T12:53:38Z" id="140379072">Have a minor comment, but either way it LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release script: Improvements, run every stage standalone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13561</link><project id="" key="" /><description>In order to be able to clean up broken steps in the release, one should
be able to run each step individually. So now one needs to specifiy each
step to run via cmdline argument
- --deploy-sonatype: runs `mvn deploy` and pushes to the staging mvn repo
- --deploy-s3: copies all artifacts over to the s3bucket
- --deploy-s3-repos: Creates the s3 repositories
- --no-install option, so that an existing mvn repo can be used for s3 operations

Also, several minor changes have been added
- Fixed typo in smoke test reference for email
- Checking for correct AWS environment variables
- Only create releases directory if it does not exist
- RPM sign check is only executed after `mvn install`
- Various path fixes for deb/rpm-s3 uploads
- Added output of deb/rpm-s3 commands for easier debugging
- Add configurable destination s3 bucket
- Removed verbosity, always be verbose
- Added color to the command which is being running right now, to differentiate from console output
</description><key id="106386633">13561</key><summary>Release script: Improvements, run every stage standalone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label></labels><created>2015-09-14T17:04:17Z</created><updated>2015-09-18T17:39:00Z</updated><resolved>2015-09-15T15:11:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[CI] Failure: org.elasticsearch.search.aggregations.bucket.GeoHashGridIT.simple</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13560</link><project id="" key="" /><description>Saw this on the query-refactoring branch today, since we did some changes in Geo recently we need to check if this is only on the feature branch and what changed here:

http://build-us-00.elastic.co/job/es_feature_query_refactoring/8998/

Geohash w has wrong centroid  expected:&lt;[27.677337714038913, 119.06734793896817]&gt; but was:&lt;[27.677337303757668, 119.06734686344862]&gt;

Stacktrace

java.lang.AssertionError: Geohash w has wrong centroid  expected:&lt;[27.677337714038913, 119.06734793896817]&gt; but was:&lt;[27.677337303757668, 119.06734686344862]&gt;
    at __randomizedtesting.SeedInfo.seed([8282B3A257119F6E:1EB05A4306CC780]:0)
    [...]
</description><key id="106379125">13560</key><summary>[CI] Failure: org.elasticsearch.search.aggregations.bucket.GeoHashGridIT.simple</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Geo</label><label>:Query Refactoring</label><label>test</label></labels><created>2015-09-14T16:26:34Z</created><updated>2015-09-14T16:34:46Z</updated><resolved>2015-09-14T16:30:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-14T16:30:51Z" id="140135134">@cbuescher I already opened an issue for this, closing this one as a duplicate of https://github.com/elastic/elasticsearch/issues/13558
</comment><comment author="cbuescher" created="2015-09-14T16:34:46Z" id="140136606">@dakrone thanks, so I guess it fails on master too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.Iterables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13559</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.Iterables across the codebase. This is one of
many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106373256">13559</key><summary>Remove and forbid use of com.google.common.collect.Iterables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-14T16:00:12Z</created><updated>2015-09-15T11:53:28Z</updated><resolved>2015-09-15T11:49:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-14T16:16:04Z" id="140129032">LGTM then.
</comment><comment author="jasontedor" created="2015-09-15T11:53:28Z" id="140366664">@nik9000 Thanks a bunch for reviewing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.elasticsearch.search.aggregations.bucket.GeoHashGridIT.simple failing reproducibly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13558</link><project id="" key="" /><description>I believe this is related to https://github.com/elastic/elasticsearch/pull/13433 and fails reliably with the following:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=29C485CE4EDC48CE -Dtests.class=org.elasticsearch.search.aggregations.bucket.GeoHashGridIT -Dtests.method="simple" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=lv_LV -Dtests.timezone=Pacific/Auckland
```
</description><key id="106356583">13558</key><summary>org.elasticsearch.search.aggregations.bucket.GeoHashGridIT.simple failing reproducibly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Geo</label><label>test</label></labels><created>2015-09-14T14:39:20Z</created><updated>2015-09-15T17:42:33Z</updated><resolved>2015-09-15T17:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-14T14:41:04Z" id="140102105">I've pushed an AwaitsFix for this, @nknize can you take a look at this?
</comment><comment author="dakrone" created="2015-09-14T14:51:37Z" id="140105391">Here is another failure caused by this: http://build-us-00.elastic.co/job/es_core_2x_centos/164/consoleText
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Running bin/plugin creates empty log files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13557</link><project id="" key="" /><description>Recently, running `bin/plugin` started creating empty log files:

```
-rw-r--r--. 1 root root 0 Sep 14 13:50 elasticsearch_deprecation.log
-rw-r--r--. 1 root root 0 Sep 14 13:50 elasticsearch_index_indexing_slowlog.log
-rw-r--r--. 1 root root 0 Sep 14 13:50 elasticsearch_index_search_slowlog.log
-rw-r--r--. 1 root root 0 Sep 14 13:50 elasticsearch.log

drwxr-xr-x. 2 root          root           4096 Sep 14 13:54 logs
```

The trouble is that they are created as the user that ran `bin/plugin` which is usually `root`. Elasticsearch is never run as root so it fails to create and write to to the logs. It warns pretty mightily:

```
   log4j:ERROR setFile(null,true) call failed.
   java.io.FileNotFoundException: /tmp/elasticsearch/logs/elasticsearch.log (Permission denied)
        at java.io.FileOutputStream.open0(Native Method)
        at java.io.FileOutputStream.open(FileOutputStream.java:270)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:213)
        at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:133)
        at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)
        at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)
        at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)
        at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)
        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)
        at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)
        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)
        at org.apache.log4j.PropertyConfigurator.configure(PropertyConfigurator.java:440)
        at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:116)
        at org.elasticsearch.bootstrap.Bootstrap.setupLogging(Bootstrap.java:187)
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:244)
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:35)
   log4j:ERROR Either File or DatePattern options are not set for appender [file].
```

You can recreate it with the tar distribution:

``` bash
mkdir tmp
cd tmp
tar xf ../elasticsearch-2.1.0-SNAPSHOT.tar.gz
cd elasticsearch*
sudo bin/plugin install file://${SOME_PATH_TO_A_PLUG}
ls -l | grep log
```

Note: if you do this inside vagrant do it in `/tmp` or something. `/elasticsearch` is shared using virtualboxfs and will always be owned by the virtualbox user.
</description><key id="106347150">13557</key><summary>Running bin/plugin creates empty log files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>v2.0.0-rc1</label></labels><created>2015-09-14T14:00:12Z</created><updated>2016-03-10T18:16:24Z</updated><resolved>2015-09-22T15:19:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-14T20:04:26Z" id="140191063">I _think_ this is a non-issue/testing methodology problem. Mostly?

The thing is that when you install elasticsearch using the deb `/var/log/elasticsearch` is created properly and `bin/plugin` doesn't go and create log files owned by root even if you run it as root. Which is how you normally run `bin/plugin`. I just checked it on trusty.

The tar package is different. If you run `bin/plugin` as root there you'll get the problem above. But luckily, you don't really have to run `bin/plugin` as root using the tar distribution. So this is only a problem for silly people like me who always run it as root because they are used to doing so.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Modify search request to incorporate query refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13556</link><project id="" key="" /><description /><key id="106331812">13556</key><summary>Modify search request to incorporate query refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>breaking</label><label>WIP</label></labels><created>2015-09-14T12:39:41Z</created><updated>2015-09-23T21:38:59Z</updated><resolved>2015-09-23T21:38:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-14T18:27:29Z" id="140167104">I left a bunch of inline comments but this looks awesome!
</comment><comment author="colings86" created="2015-09-15T08:43:50Z" id="140324445">@s1monw thanks for the review. I have push an update although this is far from finished. I also left a few replies to your comments. Next I'll write a unit test for this class to ensure its serialising and deserialising to/from JSON and the transport layer correctly and then I'll set to work plugging this into the search action
</comment><comment author="s1monw" created="2015-09-15T19:28:40Z" id="140508954">this looks pretty neat dude!
</comment><comment author="colings86" created="2015-09-22T10:36:05Z" id="142242566">@s1monw @javanna I pushed some more commits to this. It should now compile and the tests should pass but there is quite a bit more work to be done:
1. At the moment the SearchSourceBuilder is only used to generate the JSON request bytes to pass into the SearchRequest. We need to embed SearchSourceBuilder in the SearchRequest and then pass it to the ShardLocalSearchRequest and use it there to execute the search (parsing the bits that are currently stored as BytesReferences)
2. There are some muted and commented out tests (all have NORELEASE comments on them) which need to be got working
3. The RestDeleteByQueryAction needs fixing to parse the JSON body of the deleteByQuery request into a QueryBuilder
4. The NewSearchSourceBuilderTests class (to be renamed later) has some NORELEASE comments where we need to add code to randomly generate aggregations, suggests, etc.
</comment><comment author="s1monw" created="2015-09-22T12:49:29Z" id="142278129">@colings86 can you push it to a public branch so we can all work on it?
</comment><comment author="colings86" created="2015-09-22T13:17:47Z" id="142286882">@s1monw pushed a public branch here: https://github.com/elastic/elasticsearch/tree/feature/search-request-refactoring
</comment><comment author="colings86" created="2015-09-23T21:38:56Z" id="142737842">This has been superseded by https://github.com/elastic/elasticsearch/pull/13752
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Both AWS EC2 discovery and S3 plugins do not support credentials file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13555</link><project id="" key="" /><description>Both AWS EC2 discovery and S3 plugins do not support credentials file.

Plugins currently support ...

```
        credentials = new AWSCredentialsProviderChain(
                new EnvironmentVariableCredentialsProvider(),
                new SystemPropertiesCredentialsProvider(),
                new InstanceProfileCredentialsProvider()
        ); 
```

These plugins should use the DefaultAWSCredentialsProviderChain which supports environment, system, instance and credentials file!

http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html

This would simplify code and support more credentials options.

My company (16000+ seats) currently requires this as we use SSO + AWS STS to update credential files.

Thanks!
</description><key id="106323899">13555</key><summary>Both AWS EC2 discovery and S3 plugins do not support credentials file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>discuss</label></labels><created>2015-09-14T11:40:35Z</created><updated>2015-10-08T05:15:30Z</updated><resolved>2015-10-08T05:15:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-14T13:56:38Z" id="140082920">I might be wrong but I think that the security manager won't allow execution of things like access to file `~/.aws/credentials`. @rmuir Am I right?
</comment><comment author="rmuir" created="2015-09-14T13:57:00Z" id="140083037">Thats right.
</comment><comment author="robertsmarty" created="2015-09-15T00:59:06Z" id="140244053">Why would the AWS Java SDK provide a function to utilise the ~/.aws/credentials file if it can't be accessed? Sorry, doesn't make sense to me.
</comment><comment author="rmuir" created="2015-09-15T01:30:52Z" id="140247968">Currently I have several concerns:
- If we allow ourselves access to read the file, then any code can read the file, and if there is a security vulnerability the compromise can escalate to other parts of AWS.
- Current management of the existing options is not yet secure: we are working on tooling, the removal of `setAccessible` so that we have easy intuitive ways to improve the security: e.g. keep it only in a private field.
- The AWS sdk was clearly never used in a secure environment, for example it requires this permission for jackson-databind serialization of its InternalConfig representation, because it has wrong access modifiers to classes and members, and no way to bypass this. This makes the above more challenging.
- The existing mechanism for passing this information around (`Settings`) is not secure: credentials are kept around in memory, passed too to many places, held in `java.lang.String`. We need a new mechanism that is a one-time handoff to the target plugin and then erases itself.

So I think we need to walk before we can run, that is all.
</comment><comment author="robertsmarty" created="2015-10-08T05:15:30Z" id="146421565">Thanks for the detail!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for es-1.7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13554</link><project id="" key="" /><description>It is not obvious which versions of elasticsearch this plugin supports. How should this plugin be installed?
</description><key id="106322835">13554</key><summary>Support for es-1.7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robertsmarty</reporter><labels /><created>2015-09-14T11:31:13Z</created><updated>2015-09-14T12:57:57Z</updated><resolved>2015-09-14T12:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="robertsmarty" created="2015-09-14T11:44:10Z" id="140046827">This is relating to the AWS plugins
</comment><comment author="dadoonet" created="2015-09-14T12:57:57Z" id="140062707">For elasticsearch &lt; 2.0, please refer to https://github.com/elastic/elasticsearch-cloud-aws

The plugin version you should use for 1.7 is 2.7.1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make it easier to retrieve the resolved node name in unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13553</link><project id="" key="" /><description>A plugin class injected with a Settings object fails to retrieve the `node.name` unless this has been explicitly set through the settings object.

```
    @Inject
    public ZooKeeperPaths(Settings settings, ClusterName clusterName) {
        String instanceName = settings.get("node.name");
        if (instanceName == null) {
            throw new IllegalStateException("Instance name cannot be null");
        }
    }
```

Able to reproduce this when running tests for the found-plugin that use ESIntegTestCase. Workaround is to simply set a specific node.name and not rely on auto generated node names.
</description><key id="106312047">13553</key><summary>Make it easier to retrieve the resolved node name in unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">beiske</reporter><labels><label>discuss</label><label>test</label></labels><created>2015-09-14T10:16:13Z</created><updated>2015-09-14T12:23:12Z</updated><resolved>2015-09-14T12:23:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-14T11:47:24Z" id="140047268">This isn't a bug: if the node name is not specified in the settings, then the settings shouldn't be updated to reflect the resolved node name.  Currently you would need to get the ClusterService and retrieve the local node name.

Perhaps there is a way of making this easier/lighter for use in unit tests?  The alternative would be to just specify node names explicitly.
</comment><comment author="beiske" created="2015-09-14T12:16:59Z" id="140052956">The current way to retrieve the autogenerated node name (the one used if no node name is given in the settings) is to depend on `ClusterService` and then do `clusterService.localNode().getName()`. The problem with this is that the ClusterService is relatively high in the dependency hierarchy. This results in unnecessary complexity for tests and can cause challenges with circular dependencies. 

My suggestion is to extract the instance name property into it's own class so that it may be depended on directly, like ClusterName. 

Maybe it is also be possible to do something generic? Are there more properties like this? I guess the distinctive feature in this regard is that it is a property of the running instance that is not necessarily defined in the settings and, unlike most settings, the default value is not static. Had it been a dynamic property that could change at runtime I would consider it justified to depend on the ClusterService.
</comment><comment author="jasontedor" created="2015-09-14T12:17:02Z" id="140052967">The property to get when you are relying on randomly generated names is `name`, not `node.name`. You can see this if you inspect what happens in [`InternalSettingsPreparer#finalizeSettings`](https://github.com/elastic/elasticsearch/blob/bd025080c4feb4b0cd941d007bb5d2113ef0d411/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java#L165), especially [lines 179-185](https://github.com/elastic/elasticsearch/blob/bd025080c4feb4b0cd941d007bb5d2113ef0d411/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java#L179-L185) and [lines 198--205](https://github.com/elastic/elasticsearch/blob/bd025080c4feb4b0cd941d007bb5d2113ef0d411/core/src/main/java/org/elasticsearch/node/internal/InternalSettingsPreparer.java#L198-L205).

This is the relevant property because that's what the [node ultimately uses when it starts up](https://github.com/elastic/elasticsearch/blob/bd025080c4feb4b0cd941d007bb5d2113ef0d411/core/src/main/java/org/elasticsearch/node/Node.java#L239). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace LoadingCache usage with a simple ConcurrentHashMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13552</link><project id="" key="" /><description>This commit replaces the usage of LoadedCache with a simple CHM and calls
to computeIfAbsent and adds LoadingCache and CacheLoader to forbidden APIs

Relates to #13224
</description><key id="106306859">13552</key><summary>Replace LoadingCache usage with a simple ConcurrentHashMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-14T09:45:47Z</created><updated>2015-09-14T12:47:07Z</updated><resolved>2015-09-14T11:26:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-14T09:55:15Z" id="140024412">@jasontedor I simplified it :)
</comment><comment author="jasontedor" created="2015-09-14T09:56:20Z" id="140024650">LGTM.
</comment><comment author="s1monw" created="2015-09-14T11:12:27Z" id="140041735">alright I wil push this now
</comment><comment author="nik9000" created="2015-09-14T12:04:47Z" id="140049816">I haven't reviewed the code but I imagine this is probably worth
benchmarking some.
On Sep 14, 2015 5:56 AM, "Jason Tedor" notifications@github.com wrote:

&gt; LGTM.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13552#issuecomment-140024650
&gt; .
</comment><comment author="jasontedor" created="2015-09-14T12:07:47Z" id="140050235">@nik9000 This isn't the big caching replacement.
</comment><comment author="s1monw" created="2015-09-14T12:11:30Z" id="140050876">@nik9000 I don't think we need to benchmark the hunspell dictionary lazyloading cache
</comment><comment author="nik9000" created="2015-09-14T12:46:31Z" id="140059944">&gt; @nik9000 This isn't the big caching replacement.

Thanks! I really was just scanning through my email this morning I hadn't got to check it yet.
</comment><comment author="nik9000" created="2015-09-14T12:47:07Z" id="140060187">And I just reviewed and agree with pushing it. No benchmarking required. I withdraw that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RandomShapeGenerator sometimes throws an AssertionError</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13551</link><project id="" key="" /><description>The actual assert message is pretty unhelpful (`"geom"`) but on debugging it looks like the random shape generator is sometimes creating a self-intersecting polygon which trips this assert in JTS.

Build failure URL: http://build-us-00.elastic.co/job/es_feature_query_refactoring/8930/testReport/junit/org.elasticsearch.index.query/GeoPolygonQueryBuilderTests/testEqualsAndHashcode/

reproduce command:

```
mvn test -Pdev -pl org.elasticsearch:elasticsearch -Dtests.seed=C20B28F0F4F4711B -Dtests.class=org.elasticsearch.index.query.GeoPolygonQueryBuilderTests -Dtests.method="testEqualsAndHashcode" -Des.logger.level=DEBUG -Dtests.assertion.disabled=false -Dtests.heap.size=1024m -Dtests.locale=sr__#Latn -Dtests.timezone=America/North_Dakota/Center
NOTE: leaving temporary files on disk at: /home/jenkins/workspace/es_feature_query_refactoring/core/target/J3/temp/org.elasticsearch.index.query.GeoPolygonQueryBuilderTests_C20B28F0F4F4711B-001
NOTE: test params are: codec=Asserting(Lucene53), sim=RandomSimilarityProvider(queryNorm=false,coord=crazy): {}, locale=sr__#Latn, timezone=America/North_Dakota/Center
```

stack trace (truncated):

```
java.lang.AssertionError: geom
    at __randomizedtesting.SeedInfo.seed([C20B28F0F4F4711B:B304503D3B133834]:0)
    at com.spatial4j.core.shape.jts.JtsGeometry.cutUnwrappedGeomInto360(JtsGeometry.java:460)
    at com.spatial4j.core.shape.jts.JtsGeometry.&lt;init&gt;(JtsGeometry.java:85)
    at org.elasticsearch.common.geo.builders.ShapeBuilder.jtsGeometry(ShapeBuilder.java:92)
    at org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:170)
    at org.elasticsearch.test.geo.RandomShapeGenerator.createShape(RandomShapeGenerator.java:220)
    at org.elasticsearch.test.geo.RandomShapeGenerator.createShape(RandomShapeGenerator.java:223)
    at org.elasticsearch.test.geo.RandomShapeGenerator.createShape(RandomShapeGenerator.java:142)
    at org.elasticsearch.test.geo.RandomShapeGenerator.createShapeWithin(RandomShapeGenerator.java:87)
    at org.elasticsearch.index.query.GeoPolygonQueryBuilderTests.randomPolygon(GeoPolygonQueryBuilderTests.java:113)
    at org.elasticsearch.index.query.GeoPolygonQueryBuilderTests.doCreateTestQueryBuilder(GeoPolygonQueryBuilderTests.java:49)
    at org.elasticsearch.index.query.GeoPolygonQueryBuilderTests.doCreateTestQueryBuilder(GeoPolygonQueryBuilderTests.java:44)
    at org.elasticsearch.index.query.AbstractQueryTestCase.createTestQueryBuilder(AbstractQueryTestCase.java:251)
    at org.elasticsearch.index.query.AbstractQueryTestCase.testEqualsAndHashcode(AbstractQueryTestCase.java:420)
```
</description><key id="106288322">13551</key><summary>RandomShapeGenerator sometimes throws an AssertionError</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>:Query Refactoring</label><label>jenkins</label><label>test</label><label>v2.1.0</label></labels><created>2015-09-14T07:50:10Z</created><updated>2015-09-18T17:10:37Z</updated><resolved>2015-09-17T18:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaymode" created="2015-09-17T15:19:06Z" id="141119646">Another failure: http://build-us-00.elastic.co/job/es_core_2x_strong/128/testReport/junit/org.elasticsearch.search.geo/GeoShapeIntegrationIT/testPointsOnly/

Reproduces with:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=A4335303BE97F8F7 -Dtests.class=org.elasticsearch.search.geo.GeoShapeIntegrationIT -Dtests.method="testPointsOnly" -Des.logger.level=DEBUG -Des.node.mode=local -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=1003m -Dtests.locale=es_CO -Dtests.timezone=America/Indiana/Tell_City
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NewPathForShardTests is broken every which way from Sunday.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13550</link><project id="" key="" /><description>1. FileSystem wrapping code is broken, thats why you get providermismatch exception!
   Instead of fixing this, it SuppressesForbidden!!!!
2. Because it uses SuppressForbidden on the test, the whole thing is lenient, it uses java.io.File for example!
3. Of course it fails consistently on windows because it can't remove files, because it leaks file handles (locks)
   like a sieve since it does not close node environment. With correct wrapping this is always detected by e.g.
   our leak detection FS. Instead of fixing the leak, it assumesFalse(WINDOWS) !!!!!

I do not know how this snuck past me, but I need this fixed to remove setAccessible.
</description><key id="106273788">13550</key><summary>NewPathForShardTests is broken every which way from Sunday.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-14T05:33:29Z</created><updated>2016-01-22T18:40:34Z</updated><resolved>2015-09-14T08:13:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-14T05:38:59Z" id="139966352">LGTM
</comment><comment author="s1monw" created="2015-09-14T07:11:08Z" id="139981310">&gt; I do not know how this snuck past me, but I need this fixed to remove setAccessible.

LOL - I guess I asked mike to do that while you were on vacation 
</comment><comment author="mikemccand" created="2015-09-14T08:43:51Z" id="140002721">Woops, thanks for fixing @rmuir ;)
</comment><comment author="rmuir" created="2015-09-30T04:38:34Z" id="144279846">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mention we use DEFLATE for best_compression</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13549</link><project id="" key="" /><description>Fixes #13541 
</description><key id="106256732">13549</key><summary>Mention we use DEFLATE for best_compression</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2015-09-14T01:47:25Z</created><updated>2015-09-19T12:03:52Z</updated><resolved>2015-09-19T12:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T12:03:47Z" id="141658904">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve IndexingMemoryController a bit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13548</link><project id="" key="" /><description>I started by trying to explain the test failure in #13487, but failed so far, but then in the process found some other things to fix:
- The `IndexingMemoryController` sets indexing buffer sizes for each shard, but then doesn't always push those changes down to `IndexWriter`
- Instead, we push settings changes down to IW only during flush and refresh, but I think this just hides bugs, and for some configurations could be quite a while ... we should push changes immediately whenever settings are updated
- If you try to specify `indices.memory.index_buffer_size` in the node settings to a bytes (not %) value, you'll hit NPE on starting up the node
- Removed some dead code, added some missing units (time -&gt; timeMS) in variable names
- We currently wait for all merges in a shard to finish before considering it inactive, but I think that's not necessary, i.e. we can safely drop the indexing buffer for an inactive shard while merges are still running.  Other shards can use that heap.

I think we should separately fix these (this PR) ... but I'll keep trying to explain the original test failure.

I did add some more `logger.debug` so maybe next failure reveals something.
</description><key id="106244077">13548</key><summary>Improve IndexingMemoryController a bit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v2.0.0-rc1</label></labels><created>2015-09-13T22:23:03Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-15T14:39:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-14T07:48:40Z" id="139987027">Thx mike. Left comments/questions here and there.
</comment><comment author="mikemccand" created="2015-09-14T19:01:17Z" id="140176737">Thanks @bleskes, I folded in the feedback!
</comment><comment author="bleskes" created="2015-09-15T08:58:45Z" id="140327975">LGTM. Thx @mikemccand 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add max_number_of_fields to Object mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13547</link><project id="" key="" /><description>This is the implementation for #11443. Added "max_number_of_fields" as an option to object mapping. Discard document if the number of direct sub fields reaches the limit. Default behavior is no limit.
</description><key id="106242334">13547</key><summary>add max_number_of_fields to Object mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yanjunh</reporter><labels><label>:Mapping</label><label>discuss</label><label>feedback_needed</label></labels><created>2015-09-13T21:49:12Z</created><updated>2016-03-29T17:55:10Z</updated><resolved>2016-03-29T17:55:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-14T13:17:33Z" id="140071478">I wonder if this should be an index setting instead of an object mapper one. Its neat that you can get the specificity on object mapper but it feels like what matters is the total number of fields on the index not the number of fields in an object.
</comment><comment author="yanjunh" created="2015-09-15T04:50:37Z" id="140279839">Thanks for the feedback. Agree to move the setting to index settings.  Feel it's rarely useful that people needs to set different limit for different object. The total number of fields matters but I think the limit should still be applied at object level. So it's possible to skip indexing objects that reached the limit while still indexing the rest of the document. (same as dynamic:false). Just feel dropping the whole document causes too much unnecessary information loss.  I can see that a developer just grabs an in-memory object and add a few extra fields for logging. The developer may not know some part of the in-memory object is bad for indexing and he may not even be able to change the object content. It will be nice to index the "good" part of the document in this case.
</comment><comment author="nik9000" created="2015-09-15T13:38:10Z" id="140395769">The debate over on #11443 went the other way - be noisy and warn people when they go over the limit and reject the document. Make them fix it.
</comment><comment author="clintongormley" created="2015-09-18T17:29:24Z" id="141514117">I agree with @nik9000 that this should be at the index level. The idea is to warn people who are creating too many fields that they are going to have problems.  If an object is set to enabled:false, then it won't create fields and won't count towards the total.

Also think we should impose a limit by default.  Not sure how many though.  1000?
</comment><comment author="jpountz" created="2015-09-24T14:08:11Z" id="142941057">1000 sounds like a reasonable default to me.
</comment><comment author="kimchy" created="2015-09-24T14:12:45Z" id="142942229">I like the idea of rejecting the document. Just a note regarding the exception, in master we are trying to get away from many dedicated exception, maybe we can use one of the built in ones?
</comment><comment author="yanjunh" created="2015-09-25T16:50:10Z" id="143277366">I will take a look next week. Totally occupied this week. I have some concerns about rejecting the document. It basically stops indexing when the limit has reached until the logging can be fixed. Our use case of ELK is mission critical especially during outage time so we need to keep indexing ongoing, even when part of the document is not searchable.  BTW, if we don't reject document, then we don't need to have the exception. Just need to log a message.
</comment><comment author="yanjunh" created="2015-10-03T17:56:01Z" id="145272624">Add 2 settings (both are runtime adjustable)
  `"index.subfields.limit"`-- the number of fields an object can contain. Positive integer value. Default is no limit.
   `"index.subfields.dynamic_at_limit"` -- The dynamic field setting when the limit is reached. It has 3 values. "strict" is to drop the document and log an exception. This is the default behavior. "false" will keep the document but not index the field. "true" will keep the document and index this field. A warning will be logged for "true" and "false" settings. 

Here is our use case:
Pushed to production yesterday. Here is the setting used in mapping
`"index.subfields.limit":1000,`
 `"index.subfields.dynamic_at_limit": false,`
After a few minutes, we start to see the following in Elasticsearch log files:
`[f812d212-72ee-4099-aae8-6b1503a50d8c_] exceeds the max number of fields configured for [fields.realObj_]`
Now we know that fields.realObj_ contains uuid keys and has reached the configured 1000 limit. Contacted eng team and they turned off emitting uuid keys. We then increased the limit by 100 using the following
`curl -XPUT localhost:9200/index_name/_settings -d '{"index.subfields.limit":1100}'.`

 So new dynamic fields within "fields.realObj_" can still be indexed. 
In the process, the cluster has stayed up, we didn't loss documents and indexed all possible fields.
</comment><comment author="clintongormley" created="2015-10-06T12:42:54Z" id="145844964">Hi @yanjunh 

&gt; I have some concerns about rejecting the document. It basically stops indexing when the limit has reached until the logging can be fixed. Our use case of ELK is mission critical especially during outage time so we need to keep indexing ongoing, even when part of the document is not searchable. BTW, if we don't reject document, then we don't need to have the exception. Just need to log a message.

We know from long and bitter experience that most people seldom look at log messages.  The point of adding this limit is to:
- inform the user early that they are doing something inadvisable
- stop runaway field addition from doing damage
- make the limit dynamically updatable so that the user can pick up quickly from where they left off while making longer term plans to fix the issue

Logging the message is not enough for this.  It needs to be an exception if it is going to have any chance of helping.  I also don't like indexing some fields and not others - it just leads to surprises later on.  (Why is my data wrong?)  If we just reject documents, then it is very clear what is going wrong and which documents have not been indexed.

&gt; `index.subfields.limit`

The important thing here is the number of Lucene fields, not the number of fields within an object.  Don't forget multi-fields, copy_to fields etc.  

I think this should be a single, per-index, dynamic setting: `index.fields.limit`, defaulting to 1000.  If this limit is breached, then we reject any document that tries to add another field by throwing an exception.
</comment><comment author="jpountz" created="2015-10-06T12:48:16Z" id="145846746">&gt; I think this should be a single, per-index, dynamic setting: index.fields.limit, defaulting to 1000. If this limit is breached, then we reject any document that tries to add another field by throwing an exception.

+1 to this proposal
</comment><comment author="nik9000" created="2015-10-06T12:51:41Z" id="145847425">&gt; Logging the message is not enough for this. It needs to be an exception if it is going to have any chance of helping. I also don't like indexing some fields and not others - it just leads to surprises later on. (Why is my data wrong?) If we just reject documents, then it is very clear what is going wrong and which documents have not been indexed.

@clintongormley, what do you say to making the behavior on violation configurable and default to throwing an error. I hate to make more knobs to tune but if it @yanjunh happy I'm ok with it.

&gt; +1 to this proposal

+1
</comment><comment author="jpountz" created="2015-10-06T12:59:57Z" id="145849483">&gt; @clintongormley, what do you say to making the behavior on violation configurable and default to throwing an error. I hate to make more knobs to tune but if it @yanjunh happy I'm ok with it.

This is a bit scary to me: if someone comes to the forums asking why some fields are behaving weirdly, it might take me a lot of time before realizing that it is because elasticsearch was configured to ignore new fields past a certain field number. I would much rather not allow this so that elasticsearch remains more predictable.
</comment><comment author="clintongormley" created="2015-10-06T13:03:54Z" id="145850494">&gt;  it might take me a lot of time before realizing that it is because elasticsearch was configured to ignore new fields past a certain field number

exactly my thoughts.  it is just too easy to forget.
</comment><comment author="rjernst" created="2015-10-06T21:11:24Z" id="146002532">&gt;  If this limit is breached, then we reject any document that tries to add another field by throwing an exception.

And also reject mapping updates with new fields right? Not just dynamic field addition?

+1 to the plan.
</comment><comment author="nik9000" created="2015-10-06T21:17:56Z" id="146004537">&gt; And also reject mapping updates with new fields right? Not just dynamic field addition?

Yes!
</comment><comment author="yanjunh" created="2015-10-07T03:18:54Z" id="146068028">Thanks for all the comments. I think the key argument is the document should be kept or dropped after the limit is reached.

Our system sends out a lot more different kind of logs during outage. It has happened in the past that Elasticsearch stopped working when it is needed the most because of bad data in the outage logs. Now if we just drop the document after the field number limit is reached, it's the same undesirable situation to the engineers. That's why I added an option to keep the document after the limit is reached. People can still see the logs and it helps them to combat outage.

And if there is an option to keep the document, applying the limit index wise makes the document less predictable. We dont know whether a new field will be indexed and it depending on the order it appears in the stream.  If the limit is applied object wise, the bad object will not be indexed properly but the rest of document will be fine and still usable. And most likely people don't need to search or aggregate inside the bad object anyway. They just need to see the source.

Admit that even we drop the document, it's far better than letting the cluster down. I will make another push if everyone likes to drop the document.
</comment><comment author="clintongormley" created="2016-01-18T19:58:58Z" id="172636131">&gt; Admit that even we drop the document, it's far better than letting the cluster down. I will make another push if everyone likes to drop the document.

Sorry for the delay in feedback. Yes, we are in favour of throwing an exception if too many fields are added to an index. The limit should be controlled by an index level setting.
</comment><comment author="yanjunh" created="2016-01-19T05:27:01Z" id="172742390">Sorry I haven't worked on the promised change. I need to find an efficient way to get the total number of fields, not just the number of direct sub fields. Unfortunately our application doesn't allow us to drop documents even after the limit has been reached. So we also need the setting to allow us to keep the document but we are fine to let dropping the document be the default.
</comment><comment author="jpountz" created="2016-01-19T09:33:43Z" id="172791658">You might want to look at #15989 that performs a similar validation.
</comment><comment author="clintongormley" created="2016-03-10T11:55:53Z" id="194811467">Hi @yanjunh 

Are you still interested in working on this?
</comment><comment author="yanjunh" created="2016-03-10T17:41:43Z" id="194971959">@clintongormley Sure. I just started to port this to the master branch. I was dragged by other issues in moving to 2.x. Hopefully I can get another diff sometime next week.
</comment><comment author="jpountz" created="2016-03-29T17:55:10Z" id="203026141">Fixed on master via #17357. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add max_number_of_fields to Object mapping to cap mapping growthing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13546</link><project id="" key="" /><description>This is the implementation for https://github.com/elastic/elasticsearch/issues/11443. Added "max_number_of_fields" as an option to object mapping. Discard document if the number of direct sub fields reaches the limit. Default behavior is no limit.
</description><key id="106241282">13546</key><summary>add max_number_of_fields to Object mapping to cap mapping growthing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yanjunh</reporter><labels /><created>2015-09-13T21:36:34Z</created><updated>2015-09-13T21:44:52Z</updated><resolved>2015-09-13T21:44:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yanjunh" created="2015-09-13T21:43:06Z" id="139921258">just signed CLA
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Supplier instead of Reflection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13545</link><project id="" key="" /><description>Java 8 allows for method references which in-turn will cause
compile errors if a method is not visible while reflection fails late
and maybe too late. We can now register Request instances via FooRequest::new
instead of passing FooRequest.class and call it's ctor via reflection.
</description><key id="106234442">13545</key><summary>Use Supplier instead of Reflection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-13T19:12:35Z</created><updated>2015-09-14T07:21:18Z</updated><resolved>2015-09-14T07:21:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-13T19:18:13Z" id="139909497">awesome. +1.
</comment><comment author="rmuir" created="2015-09-13T19:18:14Z" id="139909502">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add *Exception(Throwable cause) constructors/ call where appropriate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13544</link><project id="" key="" /><description>This is a first step towards getting rid of exceptions lacking context as part of #10021:

In some of our internal exceptions we do not declare a constructor that accepts just the cause-Throwable as argument. As a result on exception creation we sometimes did the following:

throw new Exception(cause.getMessage())

thus loosing all stacktrace information that might have been available.

This PR adds the missing constructors and calls them where appropriate.

@rmuir adding you as assignee as you raised the original issue. Feel free to delegate to whoever you see better fit to review.
</description><key id="106233648">13544</key><summary>Add *Exception(Throwable cause) constructors/ call where appropriate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">MaineC</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-13T18:53:36Z</created><updated>2015-10-20T12:52:59Z</updated><resolved>2015-10-20T08:45:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-13T18:59:55Z" id="139908282">Wonderful! I added one comment
</comment><comment author="MaineC" created="2015-09-13T19:20:02Z" id="139910111">Updated according to your comment.
</comment><comment author="MaineC" created="2015-09-14T19:37:03Z" id="140185259">Thanks for the super-speedy review @rmuir - updated accordingly (also fixed the bug you spotted - though I tried to avoid looking too far left and right while making changes (got burnt last time I did too much in one go) I'm all for fixing obvious bugs quickly).
</comment><comment author="rmuir" created="2015-09-14T21:51:33Z" id="140215504">Looks great! +1, thank you for fixing this stuff
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup InternalClusterInfoService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13543</link><project id="" key="" /><description>This commit allows to refresh the info service in a blocking fashion
which allows tests to prevent installing listeners alltogether and
makes the class easier to test. Installing a listnener is always subject
to concurrent modifications where the listener might be called mulitple times
or with stale information which causes tests to fail.

here is a link to a test that seems flaky for that reason http://build-us-00.elastic.co/job/es_core_master_centos/7505/
</description><key id="106217965">13543</key><summary>Cleanup InternalClusterInfoService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-13T13:43:46Z</created><updated>2015-11-20T14:09:03Z</updated><resolved>2015-09-14T07:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-14T02:29:14Z" id="139944717">LGTM
</comment><comment author="dakrone" created="2015-09-14T16:27:48Z" id="140134281">@s1monw I think this needs to be backported to the 2.x branch still?
</comment><comment author="s1monw" created="2015-09-14T18:05:21Z" id="140162194">@dakrone I backported this to 2.x I really wonder if we should also move it to 2.0 this test fails regularly.
</comment><comment author="s1monw" created="2015-09-14T18:05:56Z" id="140162333">here is the commit https://github.com/elastic/elasticsearch/commit/6c1a348abc8b09096fac6ac1b742e1b897946f7d and bringing back verbosity for java7 https://github.com/elastic/elasticsearch/commit/ef17e633d7de9d58be44d37f1b62c7c33c3eecc9
</comment><comment author="dakrone" created="2015-09-14T20:38:11Z" id="140199062">@s1monw I think it's worth backporting to 2.0 since it fails regularly
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] RelocationIT.testMoveShardsWhileRelocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13542</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_master_centos/7200/

This is a problem with recovery settings being applied on the nodes at different times. 

We throttle recovery in the test and in this case we set it to 1byte/s. At the same time we set the chunk size for sending files to 1byte too. This way we can delay recovery. Later, while recovery is still ongoing, we speed it up again so that the test does not take so long. 
The throttling happens on the target and source node but the chunks are chopped on the source node. 

Unfortunately the settings are applied on each node at different times, depending on when the cluster state arrives. So when we speed up recovery again the source node might send a chunk of 130 bytes (in this case) because it has already processed a new cluster state which allows for greater chunks but the target node is still on the old state and applies an old throttling of 1byte/s. It therefore delays the recovery by 130s and that is why the check that recoveries have finished times out.

@bleskes proposed to fix this by blocking relocation completely with a Tracer instead of relying on the recovery settings. I'll see how far I get. I will mute the test for now.
</description><key id="106216068">13542</key><summary>[test] RelocationIT.testMoveShardsWhileRelocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-09-13T13:05:29Z</created><updated>2015-09-22T09:49:55Z</updated><resolved>2015-09-22T09:49:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-14T14:34:32Z" id="140100188">Discussed with @martijnvg and we think the test can be deleted. It does not actually test the scenario it was written for anyway. I will delete it in a week from now if no one intervenes. ( ping @bleskes just in case).
</comment><comment author="bleskes" created="2015-09-14T14:42:58Z" id="140102934">I'm ok with removing it.

On Mon, Sep 14, 2015 at 4:34 PM, Britta Weber notifications@github.com
wrote:

&gt; ## Discussed with @martijnvg and we think the test can be deleted. It does not actually test the scenario it was written for anyway. I will delete it in a week from now if no one intervenes. ( ping @bleskes just in case).
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/13542#issuecomment-140100188
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expand index.codec docs to mention DEFLATE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13541</link><project id="" key="" /><description>Given we use DEFLATE as this enhanced compression method, as per [here](https://www.elastic.co/blog/store-compression-in-lucene-and-elasticsearch), we should mention it :)
</description><key id="106199822">13541</key><summary>Expand index.codec docs to mention DEFLATE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label><label>v2.0.0-rc1</label></labels><created>2015-09-13T05:31:30Z</created><updated>2015-10-01T11:19:47Z</updated><resolved>2015-09-19T12:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-13T15:13:59Z" id="139886412">Its already documented: https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules.html

```
index.codec
    [experimental] This functionality is experimental and may be changed or removed completely in a
 future release. The default value compresses stored data with LZ4 compression, but this can be set 
to best_compression for a higher compression ratio, at the expense of slower stored fields performance. 
```

I don't see what else needs to be done here?
</comment><comment author="s1monw" created="2015-09-13T18:24:55Z" id="139901354">I don't think we need to document the actual algorithm we use since it's an impl detail?
</comment><comment author="markwalkom" created="2015-09-14T01:47:48Z" id="139938714">It's an easy change. It makes things explicit.
Given it took less words than all the above there is now a PR.
</comment><comment author="rmuir" created="2015-09-14T02:25:17Z" id="139944145">I don't mind the change, its just that its more complicated than that. Actually deflate vs lz4 matters very little: we've had experimental deflate in lucene since... 4.1 i think? But it would not actually compress any better in most cases than lz4, even though it was set to level 9!

Currently we use a hack, a huge block size (60KB vs 16KB) to allow this algorithm to compress better within the confines of the current format.

But what is really needed for it to kick ass is to have shared dictionary support. Its just nontrivial to have both shared dictionary support _and_ bulk merging without recompression _and_ low memory overhead _and_ good performance and all this shit that people want.

Also the java apis around deflate dictionaries are weak: you don't have access to the current compression dictionary like the trick used for http://zlib.net/pigz/ or any of that: so I wouldnt be surprised if the algorithm was changed to something else in the future.

Anyway full details are already in the lucene file formats docs (http://lucene.apache.org/core/5_3_0/core/org/apache/lucene/codecs/lucene50/Lucene50StoredFieldsFormat.html)
</comment><comment author="markwalkom" created="2015-09-14T02:41:48Z" id="139945804">Do the same comments above (roughly) apply for LZ4?
</comment><comment author="clintongormley" created="2015-09-19T12:05:35Z" id="141658976">Closed by #13549
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.base.Preconditions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13540</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.base.Preconditions across the codebase. This is one
of many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106195521">13540</key><summary>Remove and forbid use of com.google.common.base.Preconditions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-13T02:19:24Z</created><updated>2015-09-15T11:53:52Z</updated><resolved>2015-09-15T11:42:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-14T00:59:18Z" id="139933269">A note to the reviewer: these changes require deliberate review since for both `Preconditions.checkArgument` and `Preconditions.checkState`, all the conditionals must be negated (since these methods did the negation behind the scenes).
</comment><comment author="nik9000" created="2015-09-14T13:06:25Z" id="140066627">Now that you've fixed the one thing I found LGTM.
</comment><comment author="jasontedor" created="2015-09-15T11:53:52Z" id="140366730">@nik9000 Thanks again for reviewing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove all setAccessible in tests and forbid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13539</link><project id="" key="" /><description>As a followup, I will upgrade lucene with a new snapshot when @uschindler has committed his patch on https://issues.apache.org/jira/browse/LUCENE-6795, and we can test the hunspell ignoreCase correctly.

I will also be working on junit4 and randomizedrunner so we can make progress on banning this with securitymanager (we will have to grant it to some test framework jars, but not tests themselves).

But lets get our own house in order first.
</description><key id="106190940">13539</key><summary>Remove all setAccessible in tests and forbid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-12T23:49:12Z</created><updated>2015-10-02T14:09:14Z</updated><resolved>2015-09-13T04:39:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-09-13T01:31:40Z" id="139837332">I committed to Lucene.
</comment><comment author="rmuir" created="2015-09-13T01:53:44Z" id="139838336">Thanks for the help here @uschindler !
</comment><comment author="rjernst" created="2015-09-13T02:25:58Z" id="139839997">LGTM
</comment><comment author="rmuir" created="2015-09-30T04:38:03Z" id="144279812">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hack around aws security hole of accessing sun.security.ssl, s3 repository works on java 9 again</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13538</link><project id="" key="" /><description>Today this is really horrible, and we have a PR sent to fix it, but nobody
does anything: aws/aws-sdk-java#432

With java 9, we cannot even grant the permission, this kind of sheistiness is not allowed,
and s3 repository is completely broken.

The problem is their code is still broken, and won't handle neither SecurityException (our PR)
nor the new InaccessibleObjectException they will get from java 9.

We use a really hacky hack to deliver an exception that their code catches (IllegalAccessException) instead.

This means s3 repository is working on java 9, and we close off access to sun.security.ssl completely
</description><key id="106177103">13538</key><summary>Hack around aws security hole of accessing sun.security.ssl, s3 repository works on java 9 again</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Plugin Repository S3</label><label>bug</label><label>PITA</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-12T18:22:34Z</created><updated>2016-01-22T18:40:51Z</updated><resolved>2015-09-12T21:03:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2015-09-12T18:26:44Z" id="139808284">Hahahaha... Lolololol
</comment><comment author="rjernst" created="2015-09-12T19:14:33Z" id="139810409">LGTM
</comment><comment author="s1monw" created="2015-09-13T11:08:54Z" id="139860682">oh boy :) but we have to do something here to resolve this situation! thanks rob
</comment><comment author="rmuir" created="2015-09-30T04:37:19Z" id="144279670">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove easy uses of setAccessible in tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13537</link><project id="" key="" /><description>Don't worry, I will fix the rest. But some of those remaining will need a lucene upgrade,
we need to add a getter or two for tests to do things cleanly.
</description><key id="106168869">13537</key><summary>Remove easy uses of setAccessible in tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-12T15:58:50Z</created><updated>2015-10-02T14:09:01Z</updated><resolved>2015-09-12T21:19:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-12T21:11:46Z" id="139820245">LGTM
</comment><comment author="rmuir" created="2015-09-30T04:37:42Z" id="144279781">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: Re-sort terms aggregation result by key after size truncation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13536</link><project id="" key="" /><description>I'm currently using ElasticSearch aggregations to perform frequency analysis on large groups of cases (10Ks) for variables with many categorical values (100Ks) using a terms aggregation. It's working great!
## Use Case

I'd like to add a feature where I compare the frequency distributions of these variables across two different groups of people. In order to minimize the memory requirements and avoid the use of temporary files, here was my plan:
1. Run a terms query against group A that returns the top N (for N &lt;&lt;100K) hits sorted by key value
2. Run a terms query against group B that returns the top N (for N &lt;&lt;100K) hits sorted by key value
3. Merge the results from (1) and (2) in a streaming fashion, computing a score describing the difference in the two frequencies for the two groups for each pair
4. Iterate until both streams (1) and (2) are exhausted, retaining the top N biggest differences in memory
5. Return these results to the user

This approach would work fine, except that I can't sort the top N by count subsequently by key! I can do the following:
- Sort the entire terms aggregation by document count and take the top N
- Sort the entire terms aggregation by term and take the top N

But I _can't_ do the following:
- Sort the entire terms aggregation by document count, take the top N, and the sort the truncated result by term

This "post truncate sort" would be a very useful feature for me! I imagine that it may be useful for others too.
## Possible Implementations

Here are a couple of additions to the aggregations DSL that would address this use case.
### Post-Truncate Sort in Terms Aggregation

One fairly self-contained fix would be to add a second sort to the terms aggregation that is a NOP by default, but provides a second sort like the `order` feature already in the terms aggregation that is performed after truncation. Purely for illustration, the syntax might look something like this:

```
{
  "aggregations" : {
    "name" : {
      "terms" : {
        "field" : "categorical",
        "size" : 10000,
        "postorder" : { "_term" : "asc" }
      }
    }
  }
}
```
### A New Order Aggregation

A more general fix might be to add a new aggregation type called something like "resort" that simply applies a new sort to its child aggregation(s). Purely for illustration, the syntax might look something like this:

```
{
  "aggregations" : {
    "name" : {
      "resort" : {
        "order" : {
          "name2" : { "_bucket" : "asc" }
        },
        "aggregations" : {
          "name2" : {
            "terms" : {
              "field" : "categorical",
              "size" : 10000,
              "postorder" : { "_term" : "asc" }
            }
          }
        }
      }
    }
  }
}
```

This would allow users to perform arbitrary resorting on their aggregations whenever they needed. This approach may require some thought regarding what options for sorting are available, but would give people a lot more flexibility on their sorts.
## Workarounds

There are some fairly good workarounds for now. Here are a few options I see open to me at the moment:
- Streaming the untruncated terms result for both groups sorted by term
- Pulling the entire truncated result for both groups into memory and working on them directly
- Write sorted results for both groups to temporary files, then stream the results from the files

Regardless, ElasticSearch is great, and a joy to work with! :)
</description><key id="106168857">13536</key><summary>Feature Request: Re-sort terms aggregation result by key after size truncation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sigpwned</reporter><labels><label>:Aggregations</label><label>discuss</label><label>feature</label></labels><created>2015-09-12T15:58:36Z</created><updated>2016-01-28T17:34:08Z</updated><resolved>2016-01-28T17:29:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T11:36:34Z" id="141653586">@colings86 a sorting pipeline agg may be an answer here. What do you think?
</comment><comment author="colings86" created="2015-09-22T08:08:10Z" id="142207967">@clintongormley I agree, we have talked about this before. I think a resorting pipeline aggregation would be useful
</comment><comment author="clintongormley" created="2016-01-28T17:29:37Z" id="176293069">Closing in favour of https://github.com/elastic/elasticsearch/issues/14928
</comment><comment author="sigpwned" created="2016-01-28T17:34:08Z" id="176295251">Thank you for the triage @clintongormley! Always thrilled to see how responsive the ElasticSearch team is to feedback!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to properly ack translog ops during wait on mapping changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13535</link><project id="" key="" /><description>During the second phase of recovery, replayed transaction log entries may need to wait on mapping changes that have not yet propagated to the target node. Currently we correctly replay the operation at a later stage, but we acknowledge the replay request before actually performing the work.

Example failure: http://build-us-00.elastic.co/job/es_feature_two_phase_pub/859/
</description><key id="106168854">13535</key><summary>Failed to properly ack translog ops during wait on mapping changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-12T15:58:34Z</created><updated>2016-03-10T18:14:27Z</updated><resolved>2015-09-13T19:18:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-12T15:59:57Z" id="139790874">@s1monw can you take a look?
</comment><comment author="s1monw" created="2015-09-13T10:51:30Z" id="139859731">LGTM left one comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update index_.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13534</link><project id="" key="" /><description>Specifying a version  using `_version=`
Example:

```
PUT /twitter/tweet/1?_version=2
{
    "message" : "elasticsearch now has versioning support, double cool!"
}
```
</description><key id="106159162">13534</key><summary>Update index_.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aerokite</reporter><labels /><created>2015-09-12T13:15:39Z</created><updated>2015-09-12T13:19:46Z</updated><resolved>2015-09-12T13:19:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of guava Function, Charsets, Collections2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13533</link><project id="" key="" /><description>This commit removes and now forbids all uses of
Function, Charsets, Collections2  across the codebase. This
is one of many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106152508">13533</key><summary>Remove and forbid use of guava Function, Charsets, Collections2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-12T12:26:39Z</created><updated>2015-09-14T08:28:30Z</updated><resolved>2015-09-14T08:28:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-12T12:27:23Z" id="139753287">sorry for the extensive change but I had to play with the new tools a bit :)
</comment><comment author="jasontedor" created="2015-09-12T13:33:33Z" id="139770396">I like it a lot. I left a few thoughts but it LGTM otherwise.
</comment><comment author="s1monw" created="2015-09-13T10:58:47Z" id="139860104">@jasontedor pushed a new commit
</comment><comment author="jasontedor" created="2015-09-13T11:30:11Z" id="139862370">LGTM.
</comment><comment author="s1monw" created="2015-09-13T11:41:36Z" id="139862922">I will wait with pushing until monday to react faster on downstream projects
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MLT: Parser and builder takes same default boost terms value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13532</link><project id="" key="" /><description>The default value for boost terms in MLT is 0 (deactivated). However, the
builder would consider -1 as being the default value. This makes both the
parser and builder take the same default value for boost terms.
</description><key id="106152150">13532</key><summary>MLT: Parser and builder takes same default boost terms value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>review</label></labels><created>2015-09-12T12:19:45Z</created><updated>2016-03-14T13:20:39Z</updated><resolved>2016-03-10T13:21:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-03-10T13:21:20Z" id="194837167">Already handled by search refactoring. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ban setAccessible from core code, restore monitoring stats under java 9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13531</link><project id="" key="" /><description>In addition to being a big security problem, setAccessible is a risk for java 9 migration. We need to clean up our code so we can ban it and eventually enforce this with security manager for third-party code, too, or we may have problems.

Instead of using setAccessible for monitoring stats, we just use the correct interface that is public (but may not be there under some IBM vms). This restores them for java 9, which does not allow what we do today.

Instead of using setAccessible for injection code, use the correct modifier (e.g. public). Injected constructors need to be public. I scanned for problems with `grep` and eclipse call hierarchy and don't think any are missing (I did not trust our tests really would find everything). 

TODO: ban in tests
TODO: ban in security manager at runtime
TODO: make a nice asm-based scan to enforce proper modifiers and similar stuff like this.

Closes #13527
</description><key id="106135869">13531</key><summary>Ban setAccessible from core code, restore monitoring stats under java 9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-12T06:18:53Z</created><updated>2015-09-30T04:36:50Z</updated><resolved>2015-09-12T07:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-12T06:48:54Z" id="139734059">LGTM
</comment><comment author="rmuir" created="2015-09-30T04:36:50Z" id="144279642">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>somtimes data node can not find master.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13530</link><project id="" key="" /><description>My cluster has 9 data nodes / 2 master / 1 search / 112 node client.

Sometimes data node can't find master node.

There was no network, cpu load, memory problem.
No status change log.

I'm using this on AWS EC2. Therefore I am using elasticsearch-cloud-aws plugin(https://github.com/elastic/elasticsearch-cloud-aws#generic-configuration).

I set up the ping_timeout 60s.

Is it normal?
# Master Log

[2015-09-11 06:13:31,348][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101517](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}]) 
[2015-09-11 06:14:17,137][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101518](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~46s)
[2015-09-11 06:14:50,083][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101519](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~33s)
[2015-09-11 06:15:20,093][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101520](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~40s)
[2015-09-11 06:19:32,729][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101521](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~4m 12s)
[2015-09-11 06:20:02,734][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101522](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~30s)
[2015-09-11 06:21:17,182][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101523](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~1m 15s)
[2015-09-11 06:21:47,192][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101524](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~30s)
[2015-09-11 06:22:17,203][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101525](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~30s)
[2015-09-11 06:22:47,213][WARN ][discovery.zen.publish    ] [es-master-1] timed out waiting for all nodes to process published state [101526](timeout [30s], pending nodes: [[es-data-03][CY6HkL
lXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false}])  (~30s)
[2015-09-11 06:28:32,613][INFO ][cluster.service          ] [es-master-1] removed {[es-data-03][CY6HkLlXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_stora
ge_nodes=1, master=false},}, reason: zen-disco-node_failed([es-data-03][CY6HkLlXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, master=false
}), reason transport disconnected 
[2015-09-11 06:29:38,181][INFO ][cluster.service          ] [es-master-1] added {[es-data-03][CY6HkLlXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage
_nodes=1, master=false},}, reason: zen-disco-receive(join from node[[es-data-03][CY6HkLlXSwmqH6BqeTRvcQ][ip-x-x-x-x][inet[/xx.xx.xx.xx:9300]]{rack=rack_tone, max_local_storage_nodes=1, mas
ter=false}]) 
# DATA LOG

[2015-09-11 06:28:30,715][WARN ][discovery.ec2            ] [es-data-03] master left (reason = do not exists on master, act as master failure), current nodes: {[es-xxxxx-xxx-xxx-xxx-xxxxx-3-3
][_oe3CD_LQZuouoTmPk-tCg][xxxxxxx][inet[/xx.xx.xx.xx:9306]]{client=true, data=false},[es-xxxxx-xxx-xxx-xxx-xxxxx-2-2][ENE4uw6zQUedCo-lqrmSkw][xxxxxxxxxx][inet[/xx.xx.xx.xx:9304]]{clie
nt=true, data=false},[es-xxxxx-xxx-xxx-dcl-xxxxx-2-2][wITo5HsPQdWFEnapurGlyQ][xxxxxxxxxx][inet[/xx.xx.xx.xx:9307]]{client=true, data=false},[es-xxxxx-xxx-xxx-xxxx-xxxxx-2-2][g5bwjPizSia

...

[2015-09-11 06:28:30,716][INFO ][cluster.service          ] [es-data-03] removed {[es-master-1][qtVZhNb2T2uuyuEHgIPBmQ][ip-xx-xx-xx-xx][inet[/xx.xx.xx.xx:9300]]{data=false, rack=rack_tone, max
_local_storage_nodes=1, master=true},}, reason: zen-disco-master_failed ([es-master-1][qtVZhNb2T2uuyuEHgIPBmQ][ip-xx-xx-xx-xx][inet[/xx.xx.xx.xx:9300]]{data=false, rack=rack_tone, max_local_st
orage_nodes=1, master=true})
[2015-09-11 06:29:31,881][INFO ][cluster.service          ] [es-data-03] detected_master [es-master-1][qtVZhNb2T2uuyuEHgIPBmQ][ip-xx-xx-xx-xx][inet[/xx.xx.xx.xx:9300]]{data=false, rack=rack_to
ne, max_local_storage_nodes=1, master=true}, added {[es-master-1][qtVZhNb2T2uuyuEHgIPBmQ][ip-xx-xx-xx-xx][inet[/xx.xx.xx.xx:9300]]{data=false, rack=rack_tone, max_local_storage_nodes=1, master
=true},}, reason: zen-disco-receive(from master [[es-master-1][qtVZhNb2T2uuyuEHgIPBmQ][ip-xx-xx-xx-xx][inet[/xx.xx.xx.xx:9300]]{data=false, rack=rack_tone, max_local_storage_nodes=1, master=tr
ue}])
</description><key id="106128129">13530</key><summary>somtimes data node can not find master.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">darjeelings</reporter><labels /><created>2015-09-12T04:19:07Z</created><updated>2015-12-09T23:56:48Z</updated><resolved>2015-09-13T09:15:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-09-13T09:15:20Z" id="139855427">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment><comment author="hmorgado" created="2015-12-09T14:06:23Z" id="163247207">Same problem here...
Try setting  the "discovery.zen.minimum_master_nodes" setting.
From the docs:
"The minimum_master_nodes setting is extremely important to the stability of your cluster. This setting helps prevent split brains, the existence of two masters in a single cluster.
This setting should always be configured to a quorum (majority) of your master-eligible nodes. A quorum is (number of master-eligible nodes / 2) + 1"

https://www.elastic.co/guide/en/elasticsearch/guide/current/_important_configuration_changes.html#_minimum_master_nodes
</comment><comment author="markwalkom" created="2015-12-09T23:56:48Z" id="163440890">@hmorgado please use the forums
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add disk used by ES to _cat/allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13529</link><project id="" key="" /><description>Currently disk.used in cat/allocation shows the full disk usage on the node (including non-ES usage).  It will be helpful to add another column that shows disk usage by ES on the node (eg. adding up disk usage by ES indices, translogs and state files, etc.., or even just disk usage by ES indices on the node will be nice too)
</description><key id="106104040">13529</key><summary>Add disk used by ES to _cat/allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>:CAT API</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-09-11T22:14:37Z</created><updated>2015-10-01T18:26:58Z</updated><resolved>2015-10-01T18:26:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T15:57:34Z" id="141683143">I wonder how heavy this would be to calcuate?
</comment><comment author="bleskes" created="2015-09-19T16:05:26Z" id="141683802">I think pius means the store section of the nodes stats

On za 19 sep. 2015 at 5:57 p.m., Clinton Gormley notifications@github.com
wrote:

&gt; I wonder how heavy this would be to calcuate?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/13529#issuecomment-141683143
&gt; .
</comment><comment author="ppf2" created="2015-09-19T18:13:34Z" id="141694578">Yes, we have users getting confused when looking at disk.used and not realizing that it includes non-ES usage (even though it is documented in the ?help of the api call).  If it is easy to get to the store section of node stats and show that as well it will be helpful.
</comment><comment author="clintongormley" created="2015-09-21T08:05:54Z" id="141903478">ah ok, sure, let's add it
</comment><comment author="inqueue" created="2015-09-24T15:33:07Z" id="142964869">I was confused by disk.used until I found this issue. Can we also document field descriptions? Perhaps that should be a separate enh request.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation does not specify whether "_cache" option is valid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13528</link><project id="" key="" /><description>For the `exists` and `missing` filters, the documentation mentions

&gt; The result of the filter is always cached.

It does not mention that providing a `_cache` option will result in a failure.
The documentation should specify (like `has_child` and `has_parent` do):

&gt;  The _cache and _cache_key options are a no-op in this filter.

For the following filters, I am uncertain as to whether they support `_cache`. The documentation should be updated to specify for them as well:
- `geo_distance_range`
- `ids`
- `indices`
- `limit`
- `match_all`
- `nested`

If someone replies with which of these filters support `_cache` I can update the documentation.

The generic page about filter caches should also contain filters where the option is invalid: https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-cache.html
</description><key id="106100009">13528</key><summary>Documentation does not specify whether "_cache" option is valid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjohnst</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-11T21:44:53Z</created><updated>2015-09-20T20:45:01Z</updated><resolved>2015-09-19T15:56:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T15:56:57Z" id="141683102">Thanks @mjohnst - in fact in 2.0, the `_cache` key is ignored as filters are cached automatically when it makes sense to do so.

You're welcome to send a PR for 1.7 if you like, but I think I'd be happy just moving forward here. I'm going to close this issue (but feel free to send a PR)

thanks
</comment><comment author="mjohnst" created="2015-09-20T20:45:01Z" id="141833003">Thanks for the response :)

I can submit a PR for updated docs.

For the below filters, is `_cache` an acceptable option? Also, are they cached automatically? 
- `geo_distance_range`
- `ids`
- `indices`
- `limit`
- `match_all`
- `nested`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OS/ProcessProbe need to not use setAccessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13527</link><project id="" key="" /><description>I don't know why they do it this way, but it causes grief.

Current code does this which will fail on java 9:

```
OperatingSystemMXBean osMxBean = ManagementFactory.getOperatingSystemMXBean();
Method getTotalPhysicalMemorySize = osMxBean.getClass().getMethod("getTotalPhysicalMemorySize");
getTotalPhysicalMemorySize.setAccessible(true);
System.out.println(getTotalPhysicalMemorySize.invoke(osMxBean));
```

Instead, do it like this:

```
OperatingSystemMXBean osMxBean = ManagementFactory.getOperatingSystemMXBean();
Class c = Class.forName("com.sun.management.OperatingSystemMXBean");
Method getTotalPhysicalMemorySize = c.getMethod("getTotalPhysicalMemorySize");
System.out.println(getTotalPhysicalMemorySize.invoke(osMxBean));
```

I would maybe also add a class.cast for simplicity and better errors/debugging too.

Separately, we need to ban setAccessible completely.
</description><key id="106096844">13527</key><summary>OS/ProcessProbe need to not use setAccessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>bug</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T21:25:07Z</created><updated>2015-09-12T09:09:00Z</updated><resolved>2015-09-12T07:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-11T21:28:37Z" id="139665911">And we also just revert af2df9aef67462adba9146a28e464043ee7ae122 as part of this too. I will take care of this one.
</comment><comment author="uschindler" created="2015-09-11T21:41:52Z" id="139667986">Copypasted from hangouts:

you dont need al that:
https://docs.oracle.com/javase/8/docs/jre/api/management/extension/com/sun/management/OperatingSystemMXBean.html
this one is officially documented in the JDK javadocs
the problem may only be suppressforbidden, because it detects the class name as "sun."
its part of compact3
as you see it also has annotation @Exported
you can get the bean by passing the interface name to the managmentfactory
   public static &lt;T extends PlatformManagedObject&gt; T getPlatformMXBean(Class&lt;T&gt; mxbeanInterface)
you may add @SuppressForbidden
i know this is a bit controversal... &#9786;
IBM J9 may not implement this, because its not required to have the bean
but the interface should be there if they are compatible
you also have that one officially documented: https://docs.oracle.com/javase/8/docs/jre/api/management/extension/com/sun/management/UnixOperatingSystemMXBean.html
</comment><comment author="uschindler" created="2015-09-11T21:42:52Z" id="139668167">In any case, why do you need the Class#forName() in your code. Removing the setAccessible should be fine, or what happens?
</comment><comment author="rmuir" created="2015-09-12T02:06:09Z" id="139702245">&gt; In any case, why do you need the Class#forName() in your code. Removing the setAccessible should be fine, or what happens?

it fails, even with java 8. you have to get the correct method. Sorry Uwe, it may be officially doc'ed as an extension but we should not rely on that, for jvm support everywhere. There is a simple way to do it, and its what i listed above. I will fix the code.
</comment><comment author="rmuir" created="2015-09-12T02:18:38Z" id="139702934">And yes you see it here (https://docs.oracle.com/javase/7/docs/jre/api/management/extension/com/sun/management/package-summary.html) but if you try IBM:

```
Exception in thread "main" java.lang.ClassNotFoundException: com.sun.management.OperatingSystemMXBean
    at java.lang.Class.forName(Class.java:139)
    at test2.main(test2.java:8)
```

This is why we will do it the correct way!
</comment><comment author="uschindler" created="2015-09-12T09:09:00Z" id="139741984">&gt; it fails, even with java 8. you have to get the correct method

it fails also on Java 8, because it is an interface... So you have to reflect the interface and then call the method on the interface.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Edit search scroll docs for syntactic style</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13526</link><project id="" key="" /><description /><key id="106094490">13526</key><summary>Edit search scroll docs for syntactic style</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobbyberg</reporter><labels><label>:Java API</label><label>Awaiting CLA</label><label>docs</label></labels><created>2015-09-11T21:09:14Z</created><updated>2015-09-19T19:55:58Z</updated><resolved>2015-09-19T19:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T15:54:53Z" id="141683022">Correctness, or just style?  I'm not a java developer so i'm not sure which style is preferred.  I'll leave somebody else to decide on this but in the meantime, please could you sign the CLA so that it can be merged? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="bleskes" created="2015-09-19T17:31:14Z" id="141691727">Agreed on that this is a style fix. That said - sure , no problem to pull it in. I left one minor comment. Once the CLA is signed (and the comment is applied), I'll pull it in.
</comment><comment author="bobbyberg" created="2015-09-19T19:43:50Z" id="141701208">Thanks guys!
1. I've replaced the word "correctness" in the commit message with "style"
2. Signed the CLA
3. Added the commend proposed by Boaz
</comment><comment author="bleskes" created="2015-09-19T19:55:58Z" id="141701859">Thx @robertberg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.ImmutableSortedMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13525</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.ImmutableSortedMap across the codebase. This
is one of many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="106078972">13525</key><summary>Remove and forbid use of com.google.common.collect.ImmutableSortedMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T19:33:46Z</created><updated>2015-09-11T21:42:54Z</updated><resolved>2015-09-11T21:42:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-11T19:34:53Z" id="139643204">LGTM
</comment><comment author="jasontedor" created="2015-09-11T21:42:54Z" id="139668175">Thanks for reviewing @nik9000.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of several com.google.common.util. classes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13524</link><project id="" key="" /><description>This commit replaces:
- com.google.common.util.concurrent.ListenableFuture
- com.google.common.util.concurrent.SettableFuture
- com.google.common.util.concurrent.Futures
- com.google.common.util.concurrent.MoreExecutors

And forbids its usage via forbidden APIs. This is one of
many steps in the eventual removal of Guava as a dependency.

Relates to #13224
</description><key id="106072577">13524</key><summary>Remove and forbid use of several com.google.common.util. classes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T19:03:12Z</created><updated>2015-09-19T15:48:27Z</updated><resolved>2015-09-11T19:40:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-11T19:10:31Z" id="139634396">I like it!
</comment><comment author="s1monw" created="2015-09-11T19:20:23Z" id="139637434">rebased with master
</comment><comment author="jasontedor" created="2015-09-11T19:24:25Z" id="139638728">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Forbid Guava in all instead of core</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13523</link><project id="" key="" /><description>Now that Guava is no longer shaded, it can be forbidden in all instead
of core.

Relates #13224
</description><key id="106070828">13523</key><summary>Forbid Guava in all instead of core</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>:Packaging</label><label>non-issue</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T18:55:13Z</created><updated>2015-09-19T15:48:13Z</updated><resolved>2015-09-11T19:13:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-11T18:55:37Z" id="139628600">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Many backwards compatibility tests are broken in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13522</link><project id="" key="" /><description>I'm working on getting the backwards compatibility tests in 2.x but `BasicBackwardsCompatibilityIT` is proving to be very difficult so I'm pulling it out into a separate issue. This issue.
- [x] SnapshotBackwardsCompatibilityIT (#14845)
- [x] SignificantTermsBackwardCompatibilityIT (#14948)
- [x] FunctionScoreBackwardCompatibilityIT (#15332)
- [x] RecoveryBackwardsCompatibilityIT (#14817)
</description><key id="106068827">13522</key><summary>Many backwards compatibility tests are broken in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.2.0</label></labels><created>2015-09-11T18:45:04Z</created><updated>2015-12-10T20:34:56Z</updated><resolved>2015-12-10T20:34:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-14T13:33:53Z" id="140077123">I'm finding more and more and more backwards compatibility tests failing. Each one is a rabbit hole I can lose several hours to. I'm going to spend no more than an hour on each and if I can't fix it then I'll just mark it AwaitsFix with this bug.
</comment><comment author="nik9000" created="2015-10-23T18:52:47Z" id="150662292">There are a few things `@AwaitsFix`ed on this issue. Also we suppress any `index.data_path` set on the indexes during the BWC tests and we _probably_ shouldn't have to do that. We should figure that out as part of this issue as well.
</comment><comment author="nik9000" created="2015-11-18T13:26:26Z" id="157712564">Note for those following along at home - I don't believe this should block 2.1.0. Its fine to kick this to 2.1.1, 2.1.2, etc. The _really_ important BWC tests are passing.
</comment><comment author="nik9000" created="2015-11-18T17:22:24Z" id="157785736">I'm working on SnapshotBackwardsCompatibilityIT on and off now.
</comment><comment author="nik9000" created="2015-11-20T18:59:22Z" id="158494971">misclick
</comment><comment author="nik9000" created="2015-11-20T20:46:31Z" id="158519521">&gt; FunctionScoreBackwardCompatibilityIT

I got a start on this but it'll be a huge pain to do until we bundle the tar plugin in the distribution. The trouble is the security policy. Right now the bwc tests run with our default security policy which means they can't run groovy code and neither can the embedded nodes. If we bundled groovy then it'd be _fairly_ simple to have the test run only external nodes. In fact the bwc tests should always be running only external nodes and this is a good excuse to do that across the board.
</comment><comment author="nik9000" created="2015-12-08T15:49:16Z" id="162923661">As of now all of the scary backwards compatibility tests have been fixed for 2.2. The only remaining one is that `FunctionScoreBackwardCompatibilityIT` which so far as I know really wants to make sure groovy works across a mixed version cluster. I don't think this is a huge, huge issue, but it needs fixing eventually.
</comment><comment author="nik9000" created="2015-12-08T15:49:49Z" id="162923796">This is probably the biggest remaining BWC task:
https://github.com/elastic/elasticsearch/issues/14406
</comment><comment author="brwe" created="2015-12-09T11:08:14Z" id="163190401">&gt; The only remaining one is that FunctionScoreBackwardCompatibilityIT which so far as I know really wants to make sure groovy works across a mixed version cluster.

It was supposed to test that old nodes can parse whatever new nodes send. I added the script then because I thought we might want to check that as well but for now we can simply remove that. I made a pr here: #15332 

&gt;  I don't think this is a huge, huge issue, but it needs fixing eventually.

I too think the problem with testing scripting in mixed clusters remains. We currently cannot test that old nodes can parse requests related to scripting that were sent by new nodes. I do wonder though if there is a more clever way to verify that parsing on old nodes still works without having to spin up a cluster and send requests...
</comment><comment author="brwe" created="2015-12-09T12:46:39Z" id="163216938">Maybe we could make all nodes (old and new) start the same way with the external node service instead of using node builder etc. This would have the advantage that the cluster would be an actual cluster. The test would only start node clients or transport clients that then connect to the cluster. Then we could use whatever modules are available per default and also install whatever plugins we want before we start the test. Would be a larger change though. 
</comment><comment author="nik9000" created="2015-12-09T12:57:22Z" id="163221559">&gt; Maybe we could make all nodes (old and new) start the same way with the external node service instead of using node builder etc.

Yeah! I think it is a good idea too. Maybe another meta issue for improvements in BWC tests?
</comment><comment author="nik9000" created="2015-12-10T20:34:56Z" id="163741975">Britta's fixed the last test! Hurray! Closing this tracking issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Jvm registry string not updated after Java was updated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13521</link><project id="" key="" /><description>Hello,

There is an issue with elasticsearch-service if we update Java and we do not reinstall the elasticsearch-service.

In this key:
HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Apache Software Foundation\Procrun 2.0\elasticsearch-service-x64\Parameters\Java

There is a Jvm string value which points to for example: C:\Program Files\Java\jdk1.8.0_45\jre\bin\server\jvm.dll

If you uninstall java u45 and update to a newer java, elastic search service stops working because it has an outdated reference.
</description><key id="106062669">13521</key><summary>Jvm registry string not updated after Java was updated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/gmarz/following{/other_user}', u'events_url': u'https://api.github.com/users/gmarz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/gmarz/orgs', u'url': u'https://api.github.com/users/gmarz', u'gists_url': u'https://api.github.com/users/gmarz/gists{/gist_id}', u'html_url': u'https://github.com/gmarz', u'subscriptions_url': u'https://api.github.com/users/gmarz/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1594777?v=4', u'repos_url': u'https://api.github.com/users/gmarz/repos', u'received_events_url': u'https://api.github.com/users/gmarz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/gmarz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'gmarz', u'type': u'User', u'id': 1594777, u'followers_url': u'https://api.github.com/users/gmarz/followers'}</assignee><reporter username="">samshteinman</reporter><labels><label>:Packaging</label><label>discuss</label><label>v2.0.3</label><label>v2.1.3</label><label>v2.2.1</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T18:05:40Z</created><updated>2016-06-21T15:45:22Z</updated><resolved>2016-02-11T17:36:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rauanmayemir" created="2015-09-11T20:33:00Z" id="139655377">I've just had similar issue on Linux. init.d service script forces you to set JAVA_HOME in /etc/default/elasticsearch even if you had set it globally. Otherwise it will use one from the predefined list.
</comment><comment author="clintongormley" created="2015-09-19T15:51:19Z" id="141682415">@Mpdreamz is this something we can solve?
</comment><comment author="samshteinman" created="2015-10-07T17:47:05Z" id="146275071">Hey guys just looking for an update for what's going on with this bug?

Cheers!
</comment><comment author="skwasjer" created="2015-12-09T09:37:21Z" id="163163277">I have quickfixed this for myself on Windows (64-bit) with:

```
Windows Registry Editor Version 5.00

[HKEY_LOCAL_MACHINE\SOFTWARE\Wow6432Node\Apache Software Foundation\Procrun 2.0\ES_test\Parameters\Java]
"Jvm"="%JAVA_HOME%\\bin\\server\\jvm.dll"
```

Note that **`ES_test`** in the registry path must be replaced by your service instance name.
</comment><comment author="gmarz" created="2016-02-08T20:06:51Z" id="181545183">I think we should be able to set the Jvm key using %JAVA_HOME% rather than hard coding it (similar to @skwasjer's fix above).  Will open a PR for this soon.
</comment><comment author="AlexKovynev" created="2016-04-25T04:57:16Z" id="214131087">Guys, you have to return everything as it was. Otherwise, the service does not start.

JAVA_HOME returned as "c:\Program Files\Java\jdk1.8.0_91" With quotes!!! And if we add dll path get an error "file not found".
</comment><comment author="gmarz" created="2016-04-25T13:46:59Z" id="214335884">@AlexKovynev I can't reproduce this.  If you edit your environment variables, does the value of JAVA_HOME contain quotes?  If so, they should be removed.
</comment><comment author="AlexKovynev" created="2016-04-26T09:29:37Z" id="214682799">Nope JAVA_HOME does not contains quotes, as i understand Windows return quotes if path contains spaces as "Program Files". Maybe only in .bat. I don't know. For me error disappears if i change path in registry (Windows 10, ES 2.3.1). 
</comment><comment author="gmarz" created="2016-04-26T21:51:04Z" id="214898921">@AlexKovynev would you mind opening a new issue for this and providing the elasticsearch and service log files if you have them? Also, include the value that's stored in the registry when the service is installed.
</comment><comment author="staufour" created="2016-06-21T15:45:21Z" id="227482293">If the JAVA_HOME is not defined in environment variable it's now impossible to run the service.
We have in production Server JRE without global JAVA_HOME.
The JAVA_HOME is defined only on application scope (for Elasticsearch before executing service install and until now working great... pity.)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent log level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13520</link><project id="" key="" /><description>One inconsistent logg level practice is reported by our automatic tool, details are as follows:

In method connectToNode() of class NettyTransport (src\main\java\org\elasticsearch\transport\netty\NettyTransport.java). In the catch block, an error message was logged with a trace level, while in most other catch block, error messages were logged with more higher log level, i.e., debug, warning, error.

try {
connectToChannels(nodeChannels, node);
} catch (Throwable e) {
logger.trace("failed to connect to [{}], cleaning dangling connections", e, node);
nodeChannels.close();
throw e;
}

This might be an inconsistent log level practice and is reported by our automatic tool, I report it, could some of you double check them? Thanks.
</description><key id="106048686">13520</key><summary>Inconsistent log level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">waooog</reporter><labels /><created>2015-09-11T16:42:35Z</created><updated>2015-09-11T17:49:58Z</updated><resolved>2015-09-11T17:48:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-11T17:48:41Z" id="139611498">There are many places where exceptions are only logged at a debug or trace level because not all exceptions are fatal or even problematic; I think that the log level here is appropriate as the exception ends up getting rethrown and places higher up the stack can log at the appropriate level as needed. You can see if this if you backtrace the places that call `NettyTransport#connectToNode(DiscoveryNode, boolean)`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix AwarenessAllocationIT.testAwarenessZones test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13519</link><project id="" key="" /><description>This test sometimes fails because the first node is elected as master and waits 30s for incoming joins but in the meanwhile the 3 other nodes form a cluster on their side. The index will be created and its shards allocated on these 3 nodes, then the test checks for the number of shards on each node (it should be 2 or 3) but because the first node has not fully join the cluster yet one node will have 5 shards.

 closes #13305
</description><key id="106041504">13519</key><summary>Fix AwarenessAllocationIT.testAwarenessZones test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Allocation</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T16:04:31Z</created><updated>2015-09-14T07:02:37Z</updated><resolved>2015-09-14T06:59:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2015-09-11T16:04:45Z" id="139586502">@bleskes Can you have a look please? Thanks
</comment><comment author="bleskes" created="2015-09-11T18:54:18Z" id="139628035">LGTM
</comment><comment author="tlrx" created="2015-09-14T07:02:37Z" id="139980383">@bleskes thanks for your review &amp; help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup SearchRequest &amp; SearchRequestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13518</link><project id="" key="" /><description>We have a gazillion ways to specify the source of the search request.
There is no need to so many we can reduce them dramatically and also remove
some deprecated API.
</description><key id="106035355">13518</key><summary>Cleanup SearchRequest &amp; SearchRequestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T15:37:15Z</created><updated>2015-09-11T17:41:55Z</updated><resolved>2015-09-11T17:41:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-11T15:45:12Z" id="139581103">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop o.e.c.s.Settings from leaking Guava dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13517</link><project id="" key="" /><description>The public `org.elasticsearch.common.settings.Settings#getAsMap` method
leaks the dependency on Guava by returning a
`com.google.common.collect.ImmutableMap`. The leaking of this dependency
should be removed in preparation for the eventual complete removal of
Guava as a dependency.

Relates #13224
</description><key id="106030667">13517</key><summary>Stop o.e.c.s.Settings from leaking Guava dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Settings</label><label>enhancement</label><label>v2.0.0-beta2</label></labels><created>2015-09-11T15:16:02Z</created><updated>2016-03-10T18:14:27Z</updated><resolved>2015-09-11T15:44:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-11T15:18:04Z" id="139572952">LGTM
</comment><comment author="jpountz" created="2015-09-11T15:18:05Z" id="139572967">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to ignore a filed in elastic search mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13516</link><project id="" key="" /><description>Hello,

In my data, i want to ignore a field. How should i create a mapping for this? For example,

{
   "key1":"value1",

   "key2":"value2"
}

i want to ignore key2, so how should i create my mapping?

thanks
</description><key id="106025882">13516</key><summary>How to ignore a filed in elastic search mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2015-09-11T14:52:35Z</created><updated>2015-09-14T15:14:57Z</updated><resolved>2015-09-14T15:14:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-11T15:06:46Z" id="139570246">It depends on what you mean by "ignore," but your question will likely be answered on the [core types](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html) reference docs.  For example:
- You can add `"store":false` to the key2 mapping to not store it in the index
- You can add `"include_in_all":false` to the key2 mapping to make it not included in the `_all` field (which means it won't be included in a text search unless you search explicitly on the key2 field)
- You can set `"index":"no"` to the key2 mapping to make it not searchable at all

FYI, we have a [great discussion forum](https://discuss.elastic.co/c/elasticsearch) where questions like these can be asked!
</comment><comment author="abtpst" created="2015-09-11T15:17:01Z" id="139572698">thanks, i tried to include everything like
"key2":{
                     "type" : "object",
                      "enabled" : false,
                       "store":false,
                       "include_in_all":false,
                       "index":"no"
}

but still, when i try to ingest the data,, it is not ignored. i will post the question on the forum as well.

thanks
</comment><comment author="eskibars" created="2015-09-11T23:18:00Z" id="139684784">Discussion continued [here](https://discuss.elastic.co/t/elastic-search-how-to-ignore-a-field-in-mapping/29145/2) for anybody that wants to track this.  It looks like a relatively simple misconfiguration at this time
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for mappings-&gt;_source example in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13515</link><project id="" key="" /><description>Closes #13417
</description><key id="106023282">13515</key><summary>Fix for mappings-&gt;_source example in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eskibars</reporter><labels><label>docs</label><label>v1.7.2</label><label>v2.0.0-beta2</label></labels><created>2015-09-11T14:37:29Z</created><updated>2016-03-10T18:14:27Z</updated><resolved>2015-09-11T15:02:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-11T14:39:47Z" id="139563506">Would you mind editing the title so its more descriptive and including Closes #13417 in the description? The title is what I use to scan if I need to review the commit or not.
</comment><comment author="nik9000" created="2015-09-11T14:40:32Z" id="139563662">LGTM otherwise.
</comment><comment author="eskibars" created="2015-09-11T14:44:23Z" id="139564453">Done.  Thanks @nik9000 
</comment><comment author="nik9000" created="2015-09-11T15:03:32Z" id="139569521">Cool. I'll merge and backport for you.
</comment><comment author="nik9000" created="2015-09-11T15:06:34Z" id="139570200">Merged to master and cherry-picked into 2.x, 2.0, and 1.7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove JAVA_HOME detection from the debian init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13514</link><project id="" key="" /><description>Only the debian init script did JAVA_HOME detection. Everything else just relied on `bin/elasticsearch`'s `which java` style detection. This strips the detection from the debian init script so its like the rpm init script.

Closes #13403
Closes #9774
</description><key id="106002273">13514</key><summary>Remove JAVA_HOME detection from the debian init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T12:41:00Z</created><updated>2015-11-20T14:10:11Z</updated><resolved>2015-09-22T12:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-11T12:41:50Z" id="139536546">This used to be the description which I'm keeping here for posterity:
This replaces that java 7 detection in the debian init script with java 8.
Tested this in trusty by installing a few Javas and checking if they were
picked up.

Closes #13403

This used to be the first comment:

Poke @tlrx and @spinscale for more to review. I don't know much of the history behind these detection lines and why we only have them in the debian init script and no where else but I suspect this is safe.
</comment><comment author="tlrx" created="2015-09-11T12:52:48Z" id="139538415">To be honest I think we should remove those lines and only checks for `java` bin availability and defaulting to JAVA_HOME if necessary
</comment><comment author="nik9000" created="2015-09-11T13:21:23Z" id="139544216">&gt; To be honest I think we should remove those lines and only checks for java bin availability and defaulting to JAVA_HOME if necessary

Like we do everywhere else? I can do that too. Its simple enough.
</comment><comment author="nik9000" created="2015-09-11T14:35:42Z" id="139562595">&gt; Like we do everywhere else? I can do that too. Its simple enough.

And done!
</comment><comment author="nik9000" created="2015-09-16T15:06:18Z" id="140770034">@tlrx this one is ready for review as well.
</comment><comment author="tlrx" created="2015-09-22T12:54:59Z" id="142279083">@nik9000 Can you please add `closes #9774` to the commit message?

LGTM
</comment><comment author="nik9000" created="2015-09-22T12:59:05Z" id="142280622">&gt; @nik9000 Can you please add closes #9774 to the commit message?

I just squashed the commits and added the change to the commit message.

&gt; LGTM

Merging then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Random free text search from a collection from multiple fields Elastic Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13513</link><project id="" key="" /><description>we are using Elastic Search , MongoDB , mongoosastic

suppose
User:{
username:String,
city : String,
country:String 
 etc 
}
this type of document stores in Elastic Search , now if user search abhay sikandrabad then first it try to find abhay and sikandrabad both. abhay,sikandrabad may be present in any of these thing username , city , country . So basically it search from every fields and if it didnot match then try to match abhay and if data with abhay not found then try to find sikandrabad

Is this type of things already implement in Elastic Search or I have to code for this ?
</description><key id="105995150">13513</key><summary>Random free text search from a collection from multiple fields Elastic Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abhaygarg</reporter><labels /><created>2015-09-11T11:48:59Z</created><updated>2015-09-11T12:11:14Z</updated><resolved>2015-09-11T12:11:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T12:11:14Z" id="139530330">This is mostly what will happen if you search on the `_all` field with a higher boost on `abhay` than `sikandrabad`. Please use [the forums](http://discuss.elastic.co) for questions in the future, github issues are for bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Take relocating shard into consideration during awareness allocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13512</link><project id="" key="" /><description>Previous fix #12551 counted twice for relocating shard (source and target).
Fix it to consider only target node.
</description><key id="105994340">13512</key><summary>Take relocating shard into consideration during awareness allocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>:Allocation</label><label>bug</label><label>v1.7.2</label><label>v2.0.0-beta2</label></labels><created>2015-09-11T11:45:03Z</created><updated>2016-03-10T18:14:27Z</updated><resolved>2015-09-11T12:19:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-11T12:01:56Z" id="139528940">LGTM. thanks @masaruh . Can you label the PR? (and push it it all the same branches)
</comment><comment author="masaruh" created="2015-09-11T13:21:44Z" id="139544275">Thanks @bleskes. Pushed to master, 2.x, 2.0 and 1.7.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document the meanings of the field_value_factor modifiers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13511</link><project id="" key="" /><description>This was brought up on a discuss post: https://discuss.elastic.co/t/need-definition-of-log1p-log2p-ln1p-and-ln2p-of-field-value-factor-modifier/29101

The documentation on the modifier parameter for the field_value_factor says:

```
Modifier to apply to the field value, can be one of: none, log, log1p, log2p, ln, ln1p, ln2p, square, sqrt, or reciprocal. Defaults to none.
```

While this state what the valid values are it is not so helpful since it doesn't define what mathematical operations they map to. Specifically the `log*` options should be explicitly defined
</description><key id="105989156">13511</key><summary>Document the meanings of the field_value_factor modifiers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">colings86</reporter><labels><label>adoptme</label><label>docs</label><label>v2.3.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T11:02:28Z</created><updated>2016-03-10T18:45:24Z</updated><resolved>2016-03-10T18:45:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T12:12:45Z" id="139530705">I'd also be interested to know what benefits it brings compared to using a plain script. I guess the motivation was speed, but now that we support lucene expressions, maybe this is a more flexible option that is equally fast?
</comment><comment author="nik9000" created="2015-09-11T12:15:10Z" id="139531049">&gt; I'd also be interested to know what benefits it brings compared to using a plain script. I guess the motivation was speed, but now that we support lucene expressions, maybe this is a more flexible option that is equally fast?

+1

We should probably test it and see but if they are fast enough we should deprecate them in 2.0 and just consider them a stopgap between groovy and expressions?
</comment><comment author="jpountz" created="2015-09-11T12:21:50Z" id="139532290">+1 sounds like a plan
</comment><comment author="raphi" created="2016-03-09T09:39:55Z" id="194210793">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function score query: remove deprecated support for boost_factor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13510</link><project id="" key="" /><description>boost_factor was deprecated in 1.4.0.beta1. We can remove the support for it in 3.0, its replacement is `weight`.
</description><key id="105983293">13510</key><summary>Function score query: remove deprecated support for boost_factor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query DSL</label><label>breaking</label><label>enhancement</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T10:18:24Z</created><updated>2015-09-11T11:05:37Z</updated><resolved>2015-09-11T11:05:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T10:26:32Z" id="139511466">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch windows service doesn't respect the memory heap max size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13509</link><project id="" key="" /><description>Hello everyone,

We are having some issues with running ElasticSearch on Windows.

So our setup is as follow:

4 workstations with Elastic search started on it.

Specs:
Workstation:
OS: Windows 7 Professional 64 bits
CPU: Core i7 3770
Memory: 8.00 GB

ElasticSearch configuration:
Version: 1.5.2
http.compression: true
Java initial memory pool: 256 MB
Java maximum memory pool: 5358 MB
Thread stack size: 256 KB

Windows configuration:
Virtual memory size: 12058 MB

So everything was fine on each station until today where one of the station have its elasticsearch memory working set at: 6505 MB

So for info, here is the 4 machines ES memory working set:
Station 1: 6505 MB
Station 2: 5772 MB
Station 3: 5855 MB
Station 4: 5841 MB

Kopf plugin give me this informations:
Station 1:
Heap used: 3.89GB
Max: 5.17GB

Station 2:
Heap used: 3.70GB
Max: 5.17GB

Station 3:
Heap used: 2.63GB
Max: 5.17GB

Station 4:
Heap used: 3.53GB
Max: 5.17GB

The station 2 is the master node.

For the documents &amp; index &amp; shards we have:
11 indexes
25.374.590 docs
110 shards

So the question is why the station one take 6505 MB of memory?

And while we are at it, bonus question:

We actually have 11 indexes, one per document type. Is this useful or not? And bonus question 2:
Is 110 shards on 4 stations madness?

Thank you all!

If you need more informations, you can ask me!
</description><key id="105978414">13509</key><summary>ElasticSearch windows service doesn't respect the memory heap max size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheFireCookie</reporter><labels /><created>2015-09-11T09:47:55Z</created><updated>2015-09-11T12:16:58Z</updated><resolved>2015-09-11T12:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T12:16:58Z" id="139531292">I suspect your tool is measuring virtual memory usage, which might account for some files stored in disk given that elasticsearch uses mmap.

&gt; We actually have 11 indexes, one per document type. Is this useful or not? And bonus question 2:
&gt; Is 110 shards on 4 stations madness?

Having indices with one single type instead of different types on the same index makes sense. 110 shards on 4 machines starts to be a lot however, depending on the size of your shards. If they are already large (several GB) then this might be fine, but if they are small you should probably consider reducing the number of shards.

Please use [the forums](http://discuss.elastic.co) for questions in the future. These issues are for bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cat health supports ts=0 option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13508</link><project id="" key="" /><description>If ts=0, cat health disable epoch and timestamp
 Add rest-api test and test

Closes #10109
</description><key id="105967836">13508</key><summary>Cat health supports ts=0 option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>:CAT API</label><label>enhancement</label><label>low hanging fruit</label><label>review</label><label>v5.0.0-alpha2</label></labels><created>2015-09-11T08:39:24Z</created><updated>2016-05-02T11:59:04Z</updated><resolved>2016-04-13T09:56:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2016-02-10T14:57:14Z" id="182407856">Sorry for the super late review. This makes sense to me but it isn't what @drewr asked for in #10109. I'm fine with it because it is better though. `ts=0` always silences timestamps if they are added but `ts=true` doesn't add them if they aren't already there.

If we are going to go with a change like this I'd replace the strings `"timestamp"` and `"epoch"` in the equals check with a constant. Its not like we're likely to rename them but it would still make it more obvious that those fields are _special_.

I don't think it'd be much harder to do with @drewr wanted though - to remove the time from RestHealthAction and add it to RestTable. Then you'd have to construct the rest table with a boolean telling it whether or not timestamp should default to shown (_cat/health) or hidden (everything else).
</comment><comment author="clintongormley" created="2016-03-10T11:50:58Z" id="194810177">@johtani any plans on revisiting this?
</comment><comment author="johtani" created="2016-03-14T11:00:24Z" id="196259465">@nik9000 @clintongormley I will try to generalize the time. I will change ["Table" class](https://github.com/elastic/elasticsearch/blob/9430e17f707bde61418f31d1b080ed087ed4cd4c/core/src/main/java/org/elasticsearch/common/Table.java) 
</comment><comment author="johtani" created="2016-03-16T00:47:26Z" id="197088226">@nik9000 Rebased and fixed some test issue.
And the time move to Table class with ''startHeadersWithTimestamp".
Can you review again?
</comment><comment author="dakrone" created="2016-04-06T20:52:45Z" id="206562399">Pinging @nik9000 again for review
</comment><comment author="nik9000" created="2016-04-06T20:55:44Z" id="206563748">I like it! If we push all the APIs over to returning timestamp we can rename the method back to `startHeaders` but this gets us a migration path. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Run Vagrant tests on OpenSUSE 13.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13507</link><project id="" key="" /><description>Since OpenSUSE 13.2 is officially supported we should run the Vagrant tests on it
</description><key id="105967143">13507</key><summary>Run Vagrant tests on OpenSUSE 13.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tlrx</reporter><labels><label>:Packaging</label><label>test</label></labels><created>2015-09-11T08:34:30Z</created><updated>2015-09-16T14:12:31Z</updated><resolved>2015-09-16T14:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-11T12:34:06Z" id="139534011">We almost certainly should have it in the list but I couldn't find an opensuse box in vagrant atlas that I trusted.
</comment><comment author="tlrx" created="2015-09-11T12:47:36Z" id="139537501">Yes, I saw your comment in TESTING.asciidoc after. I just keep this issue as a reminder
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>very long gc pause caused by ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13506</link><project id="" key="" /><description>[2015-09-11 11:49:22,429][WARN ][monitor.jvm              ] [node-173.65] [gc][ParNew][47112][14482] duration [11.3h], collections [1]/[11.3h], total [11.3h]/[11.4h], memory [4.5gb]-&gt;[3.2gb]/[15.7gb], all_pools {[Code Cache] [12.4mb]-&gt;[12.4mb]/[48mb]}{[Par Eden Space] [1.5gb]-&gt;[157.6mb]/[1.5gb]}{[Par Survivor Space] [67.5mb]-&gt;[79.2mb]/[227.5mb]}{[CMS Old Gen] [2.9gb]-&gt;[2.9gb]/[14gb]}{[CMS Perm Gen] [45.2mb]-&gt;[45.2mb]/[128mb]}

JVM parameters:
-Xss256k
-Xmn2048M
-XX:PermSize=128M
-XX:MaxPermSize=128M
-XX:SurvivorRatio=7
-XX:GCTimeRatio=49
-XX:MaxTenuringThreshold=12
-XX:+UseConcMarkSweepGC
-XX:MaxGCPauseMillis=200
-XX:+UnlockExperimentalVMOptions

JDK version: 1.6.0.45
</description><key id="105964150">13506</key><summary>very long gc pause caused by ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-09-11T08:17:00Z</created><updated>2015-09-11T12:19:28Z</updated><resolved>2015-09-11T12:19:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="makeyang" created="2015-09-11T08:23:15Z" id="139482032">plus parameters:
-Xms:16g
-Xmx:16g
</comment><comment author="jpountz" created="2015-09-11T12:19:27Z" id="139531806">First recommendation is to ugrade to a more recent Java version (latest Java 8 release for instance), which could have lots of improvements in terms of garbage management. The fact that you are using Java 6 also probably means that you are using an old version of elasticsearch, you should upgrade it too.

Please use [the forums](http://discuss.elastic.co) for questions in the future. These issues are for bugs and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fold MoreLikeThisFetchService into MoreLikeThisQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13505</link><project id="" key="" /><description>now that we have a Client on the Shard context we can fold
the doc fetching into the parser / builder.

Relates to #13488
</description><key id="105963626">13505</key><summary>Fold MoreLikeThisFetchService into MoreLikeThisQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-11T08:12:00Z</created><updated>2015-09-11T08:22:31Z</updated><resolved>2015-09-11T08:22:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-11T08:21:08Z" id="139481703">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added documentation for default execution value (range filter)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13504</link><project id="" key="" /><description>Specified which value for execution is default if no value is specified.

Closes #13503
</description><key id="105961194">13504</key><summary>Added documentation for default execution value (range filter)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjohnst</reporter><labels><label>docs</label><label>v1.7.2</label></labels><created>2015-09-11T07:51:21Z</created><updated>2015-09-11T07:53:09Z</updated><resolved>2015-09-11T07:52:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T07:53:09Z" id="139476957">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation should specify default value for execution option in range filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13503</link><project id="" key="" /><description>It's unclear from the documentation that `index` is the default option for the `execution` option in the range filter.
</description><key id="105961047">13503</key><summary>Documentation should specify default value for execution option in range filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mjohnst</reporter><labels /><created>2015-09-11T07:49:54Z</created><updated>2015-09-11T07:53:31Z</updated><resolved>2015-09-11T07:53:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T07:53:31Z" id="139477004">Closed via #13504
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move integration tests to unit tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13502</link><project id="" key="" /><description>Follow up for #12844 but in master branch where cloud-aws has been split in 2 projects. So we need to backport manually changes...
</description><key id="105958157">13502</key><summary>Move integration tests to unit tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery EC2</label><label>review</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T07:27:30Z</created><updated>2015-09-17T21:13:23Z</updated><resolved>2015-09-17T21:12:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-11T07:28:24Z" id="139472112">@bleskes Would you mind looking at this one? It's just a port of #12844 but in master branch.
</comment><comment author="bleskes" created="2015-09-11T08:15:42Z" id="139480511">Left one minor comment. LGTM o.w. 

I'll try to work on some basic integration tests
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>EC2/Azure discovery plugins must declare their UnicastHostsProvider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13501</link><project id="" key="" /><description>Closes #13492
</description><key id="105953895">13501</key><summary>EC2/Azure discovery plugins must declare their UnicastHostsProvider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>blocker</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-11T06:44:54Z</created><updated>2016-03-10T18:14:27Z</updated><resolved>2015-09-11T07:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-11T07:22:00Z" id="139471053">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update 'loading the sample dataset' section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13500</link><project id="" key="" /><description>This command 

``` shell
curl -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary @accounts.json
```

will error out within windows cmd shell (Windows 10 64bit). So on windows @account.json should be double quoted as: 

``` shell
curl -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary "@accounts.json"
```
</description><key id="105942937">13500</key><summary>Update 'loading the sample dataset' section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">amrenbin</reporter><labels /><created>2015-09-11T04:24:44Z</created><updated>2015-09-19T15:30:48Z</updated><resolved>2015-09-19T15:30:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-11T07:58:34Z" id="139477875">@renbinatgit Maybe we should just recommend to use double quotes all the time? This way we wouldn't need OS-specific instructions.
</comment><comment author="clintongormley" created="2015-09-19T15:30:32Z" id="141679674">I've updated all occurrences of --data-binary in the docs to use a quoted argument
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to index heterogeneous JSON Array as value in Elastic Search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13499</link><project id="" key="" /><description>Can Elastic search index fields like

"key": [
            14.0,
            "somestring"
        ]

if i try to ingest this data, i get this error
org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: Found unrecoverable error [Bad Request(400) - [WriteFailureException; nested: MapperParsingException[failed to parse [FIXMessage.key]]; nested: NumberFormatException[For input string: "somestring"]; ]]; Bailing out..

Please also refer to 

http://stackoverflow.com/questions/32513043/elastic-search-json-stringjsonobject-indexing/32514161#32514161
</description><key id="105939754">13499</key><summary>How to index heterogeneous JSON Array as value in Elastic Search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abtpst</reporter><labels /><created>2015-09-11T03:40:14Z</created><updated>2015-09-11T05:03:42Z</updated><resolved>2015-09-11T05:03:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-11T05:03:42Z" id="139453137">No. If a field is mapped as a number, you can not write a String in it.

Please ask questions on discuss.elastic.co.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.Queues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13498</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.Queues across the codebase. This is one of
many steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="105938651">13498</key><summary>Remove and forbid use of com.google.common.collect.Queues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T03:27:27Z</created><updated>2015-09-11T04:00:28Z</updated><resolved>2015-09-11T04:00:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-11T03:57:56Z" id="139446763">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reorganize sharing of constants for mock cluster info service</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13497</link><project id="" key="" /><description>The MockInternalClusterInfoService depends on a constant and helper
method from actual test cases. This moves the constant and helper method
into the mock itself. Without this change, the classes put into the test
jar are not completely useable on their own (since this mock is in the
test jar, but the test cases are not).
</description><key id="105936481">13497</key><summary>Reorganize sharing of constants for mock cluster info service</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-11T03:08:33Z</created><updated>2015-09-11T04:14:01Z</updated><resolved>2015-09-11T04:10:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-11T04:08:11Z" id="139447718">Super minor comment, but otherwise LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactors GeoDistanceQueryBuilder/-Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13496</link><project id="" key="" /><description>Splits parsing and Lucene query generation. Switches from storing lat/lon
separately to using GeoPoint instead.

Relates to #10217
</description><key id="105917038">13496</key><summary>Refactors GeoDistanceQueryBuilder/-Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/MaineC/following{/other_user}', u'events_url': u'https://api.github.com/users/MaineC/events{/privacy}', u'organizations_url': u'https://api.github.com/users/MaineC/orgs', u'url': u'https://api.github.com/users/MaineC', u'gists_url': u'https://api.github.com/users/MaineC/gists{/gist_id}', u'html_url': u'https://github.com/MaineC', u'subscriptions_url': u'https://api.github.com/users/MaineC/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/70953?v=4', u'repos_url': u'https://api.github.com/users/MaineC/repos', u'received_events_url': u'https://api.github.com/users/MaineC/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/MaineC/starred{/owner}{/repo}', u'site_admin': False, u'login': u'MaineC', u'type': u'User', u'id': 70953, u'followers_url': u'https://api.github.com/users/MaineC/followers'}</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-10T23:14:38Z</created><updated>2016-03-11T11:50:51Z</updated><resolved>2015-09-16T09:28:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-10T23:16:08Z" id="139408979">This PR continues #12283, so maybe @MaineC wants to take a look at it?
</comment><comment author="cbuescher" created="2015-09-11T10:10:45Z" id="139506779">@colings86 thanks for the comments, added small changes according to this and also minor changes to the GeoDistanceRange/PolygonQ.B. using new utility methods and writable GeoPoint from this PR. Glad if you want to take another look at that.
</comment><comment author="colings86" created="2015-09-11T10:49:02Z" id="139514454">@cbuescher Left some more comments but I think it's close
</comment><comment author="cbuescher" created="2015-09-11T11:53:09Z" id="139527291">@colings86 adressed your comments, other than the helper method in ESTestCase which I'd prefer to keep there I think I adressed your comments. Would also like for @MaineC to have a final look since this PR contains many of her own changes. 
</comment><comment author="MaineC" created="2015-09-11T19:15:24Z" id="139636497">Left a few comments. Thanks for adopting this PR.
</comment><comment author="cbuescher" created="2015-09-14T10:50:05Z" id="140036334">@MaineC addressed your comments, let me know if you think its okay now.
</comment><comment author="s1monw" created="2015-09-15T19:53:06Z" id="140518026">left minor comments LGTM and doesn't need another review IMO
</comment><comment author="cbuescher" created="2015-09-16T09:28:31Z" id="140684709">Merged with feature branch with 8fb1aa9.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds links to Logstash plugins under the Integrations page.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13495</link><project id="" key="" /><description /><key id="105915789">13495</key><summary>Adds links to Logstash plugins under the Integrations page.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels /><created>2015-09-10T23:06:36Z</created><updated>2015-09-10T23:08:09Z</updated><resolved>2015-09-10T23:08:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Parallel/concurrent reads</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13494</link><project id="" key="" /><description>_this is generalization of https://github.com/elastic/elasticsearch/issues/9483_
## Problem

For libraries that want to read data from Elasticsearch in a parallel/concurrent/multi-threaded way, one needs to split the target (aka data in ES) into _independent_ chunks which then can be consumed individually.
## Current approach

ES doesn't offer a proper API to achieve this; the current approach implies understanding the shards topology (which can be tricky and the incorrect _granularity_) to determine the chunks followed by a hacky attempt of locking down a read to a certain chunk which unfortunately is problematic when trying to read aliases or multiple indexes across the same shards (and what issue https://github.com/elastic/elasticsearch/issues/9483 tried to fix).
## Desired behavior

The user ideally would simply request the number of splits/chunks for a given query and get back the number of possible partitions/readers ids which then the consumer can start using to get back the results.
So for example, the user could ask for 10 chunks for a given index. 

ES in turn would decide whether it can and how to best create these splits: likely the chunks requested will not match the index topology (such as shards) so a split might spread across multiple shards while another might read only a subset of a shard.

This would be similar to the scan/scroll approach where the initial query returns a **scroll_id** which in turn, returns the results.

Going a step further, the user could specify the granularity needed such as: _NODES_ or _SHARDS_ to bound the parallelism based on the actual topology of the target index/indices.

The user should not have to deal with the actual partition or follow-up connection; rather it would have the relevant information already computed by ES.
The chunk should contain locality information - on what node/ip is the data retrieved from.
</description><key id="105908693">13494</key><summary>Parallel/concurrent reads</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">costin</reporter><labels><label>:Search</label><label>feature</label><label>v5.0.0-alpha4</label></labels><created>2015-09-10T22:08:12Z</created><updated>2016-07-05T09:37:20Z</updated><resolved>2016-06-07T14:23:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2015-09-10T22:10:17Z" id="139396908">@boaz @imotov please add your thoughts to the issue
@clintongormley I'm going to close #9483 in favour of this issue; hopefully it won't send the issue in the back of the queue, planning wise :)
</comment><comment author="imotov" created="2015-09-17T15:34:25Z" id="141124174">@costin - just to clarify. I don't really see much sense in creating more slices than we have shards in the index. So, if a client asks for 10 slices and index has only 5 shards, it wouldn't make much sense to return more than 5 slices. On the other side, if the client asks for 2 slices, but we have 5 shards running on 3 different nodes, there is really no good split either. So, maybe client shouldn't really ask for the number of chunks but instead have parallelism mode: `nodes` or `shards` which will cause elasticsearch to give one scroll_id per shard or one scroll_id per node. What do you think?
</comment><comment author="petchema" created="2015-10-01T13:29:26Z" id="144726259">We had this issue with Elasticsearch-hadoop, the map/reduce client is a relatively (compared to our ES cluster) large Hadoop cluster, and since ES-hadoop can only start one mapper per shard we had to create indices with several hundred shards (more than we usually do) so it doesn't limit the mapping parallelism. That requires some planning, and also make those indices less efficient for other operations, like indexing or standard search queries.
</comment><comment author="apatrida" created="2015-11-16T16:15:55Z" id="157084814">@imotov it does make sense to divide a shard into slices if you can read one slice from a primary, one slice from a replica, another from yet another replica, etc.  

but definitely having a plan that allows parallel reading and dividing the load across shards, nodes, machines, the rack or racks.  And then yes, need something that is a per node ID so you can read individually.

I think a lot of us have done this in the same way.  Take an alias, convert to list of indexes, look at the shard plan, solve the plan for evenly dividing work across the highest number of nodes that have data, query from each node using shard/node hints, either locally (if our extraction is on the node or in a plugin) or remotely ensure more parallelism.

This is also something that might cause the desire for I/O, CPU or memory throttling for these queries because you start to have larger batch reads at the same time you don't want to crush the cluster.  And similar circuit breakers (throttles are likely better).
</comment><comment author="imotov" created="2015-11-19T15:49:26Z" id="158097140">@apatrida what I meant is there is no good practical way to "slice" a primary shard and a replica shard into the same slices. You can do things like scanning all records and selecting some records based on hash of id, but it's not very practical and it's something that you can do on the client side already.
</comment><comment author="clintongormley" created="2016-04-14T17:07:38Z" id="210052797">Now that search requests are parsed on the coordinating node, we can add support for breaking a single scroll request down into multiple scrolls, which can be pulled in parallel.  The way it could work is as follows:

This request starts a search against all indices starting with `my_index` and requests 10 scroll IDs in response:

```
GET my_index*/_search?scroll=1m&amp;scroll_ids=10
```

Elasticsearch finds all matching shards and determines that there are 8 shards involved, which means we need two more scroll_ids than we have shards.  This means that it needs to split two shards into two.

It sorts the shards by number of docs in each and chooses to split the two largest shards.  The field stats API returns the min/max UID which we split into two ranges, and apply those ranges as a filter on those shards, eg:

```
{ "range: { "_uid": { "gte": "a000000", "lt": "m000000" }}}
```

The response returns an array of scroll_ids with the total hits, but without returning the first tranche of hits:

```
{
  "scroll_ids": [ {
    "scroll_id": "cXVlcnlUaGVuRmV0Y2g7NTs3NzpWMzZBQTJXb1JEMlFNcGZkZTFMQ3FnOzc2OlYzNkFBMldvUkQyUU1wZmRlMUxDcWc7Nzg6VjM2QUEyV29SRDJRTXBmZGUxTENxZzs3OTpWMzZBQTJXb1JEMlFNcGZkZTFMQ3FnOzgwOlYzNkFBMldvUkQyUU1wZmRlMUxDcWc7MDs=",
   "hits": {
    "total": 10000,
    "hits": []
  },
  ...
]}
```

We assume UIDs are well distributed but that may not be the case. There is no guarantee that each scroll ID will return a similar number of results.  

NOTE: If we have more shards than we requested, we just return one scroll id per shard.
</comment><comment author="bleskes" created="2016-04-15T09:19:27Z" id="210377762">imo, and @costin can confirm, but I believe that reading shards in parallel is good enough and we don't need the complexity of sub-shard split. I think we can start with figuring that part out (and if people ask for 10 readers and we only have 8 shards, we'll give them two empty readers).
</comment><comment author="costin" created="2016-04-15T09:59:04Z" id="210397010">The sub-shard splitting is real and already a problem. This occurs for example in Spark where the reader tries to load the data in memory but it can only access a max of 2 GB (during shuffling, related to `ByteBuffers` and `Integer.MAX_VALUE`).
The solution in Spark is to increase the number of workers so the memory is spread in chunks of 2GBs so many times a reader will have way more partitions than shards.
Unless the shard is actually split, this problem still remains.
</comment><comment author="clintongormley" created="2016-04-19T17:50:23Z" id="212039222">@costin and @imotov just had a chat about this and came up with a better idea:

Return one scroll ID per shard.  Each scroll ID can be used in parallel processes, which means that requests to the same search context on each shard should be executed serially to avoid overlaps. Once no more hits are returned, scrolling is done.

This API could also return info about which node hosts each shard, which can be used by spark/hadoop to choose local workers.
</comment><comment author="s1monw" created="2016-05-02T10:17:51Z" id="216189146">&gt; Return one scroll ID per shard. Each scroll ID can be used in parallel processes, which means that requests to the same search context on each shard should be executed serially to avoid overlaps. Once no more hits are returned, scrolling is done.

I personally think a solution to this problem should have some more properties than serialization. I think we need to make sure we don't depend on the state of a shard to ensure we can recover from failures without starting all over again. For instance if one worker needs to restart we should be able to only resume that one worker not all others. I also think that all state should be on the client except of the information we are already maintaining ie. the point-in-time snapshot (index reader). For this to work, the user should be in-charge of partitioning the document space. For instance a user with N consumers must specify the partition and the number of consumers when the scroll is initialized ie. user smust provide:
- number for workers
- docs per batch 

For every request we allow to provide the worker ID (0 - N) and we return only the slice of the data for the given worker ID. Even further we can allow to provide a document offset of some sort to resume (that would be nice). Implementation wise we can just fork a search context per worker and return all context IDs with the scroll ID once per worker. 

that way we can keep everything with the same concurrency patterns and we can consume documents in parallel if the user wants to.

I hope this makes sense?
</comment><comment author="costin" created="2016-05-04T08:15:30Z" id="216778564">Danke for the in-depth reply!

&gt; we need to make sure we don't depend on the state of a shard to ensure we can recover from failures without starting all over again

Agreed. This is already a problem (if a shard fails, one cannot _resume_ the scroll/read) so to not make this feature heavier, during the discussion we ignored this aspect.
Having this functionality built-in would be of course great since cluster changes would not break clients performing reading.

&gt;  Even further we can allow to provide a document offset of some sort to resume (that would be nice).

That would be indeed nice. There are two scenarios here and it would be great if we could support both:
- the reading task gets restarted but without an offset (it got lost) can simply ask ES for the most recent offset in that _scroll_ and continue. This might imply that the last request might have been lost by the consuming task but that's not ES concern.
- the reading task gets restarted and has an offset. This might point back in time to replay some data that was already sent to the client.

Wdyt?
</comment><comment author="clintongormley" created="2016-05-04T08:25:52Z" id="216782402">Resuming a scroll request could only be done reliably if:
- the index is quiescent (ie no further writes)
- the scroll request is sorted in a deterministic order
</comment><comment author="s1monw" created="2016-05-04T08:27:31Z" id="216782931">&gt; the reading task gets restarted but without an offset (it got lost) can simply ask ES for the most recent offset in that scroll and continue. This might imply that the last request might have been lost by the consuming task but that's not ES concern.

I think for simplicity we can only allow this for the `restart from where you are` which means basically just reusing the scroll ID. For this to work we have to ensure that the search context is not closed until the worker is restarted which is 5 min by default.
</comment><comment author="jimczi" created="2016-05-04T12:02:31Z" id="216839998">&gt; For every request we allow to provide the worker ID (0 - N) and we return only the slice of the data for the given worker ID.

There are some points we need to discuss before going further on this.
First of all if we allow parallel reads on the same shard we'll need to ensure that the searcher is the same among the parallel readers. For instance in the hadoop case I assume that each worker/mapper is independent,  they start a scroll in the beginning of each map and we have no guarantee that the searcher is the same but it's not a problem because each scroll is independent (they'll see different documents from different shards). With parallel readers we need to ensure that the searcher is the same, the only way to achieve this is to create the scroll context at the beginning of the job and we also need to ensure that those contexts won't go away during the timeline of the job. Assuming that the job could take hours I don't see how we could set the ttl on the context beforehand.
Now what can we do if a replica is stopped during the job ? In the current implementation it's not a problem, we can start a new scroll on another replica. With parallel readers it's impossible, we still need to ensure that we read from the same replica/searcher which means that if a replica used in a parallel scroll is not reachable anymore the whole job should fail.
</comment><comment author="costin" created="2016-05-04T12:50:25Z" id="216852866">Not sure whether it helps but in the Hadoop/Spark case the workers do not communicate with each other. At the start of the job, there is some light configuration (in ES-Hadoop that performs a `search_shards` and each task is instructed to connect against one) but that's about it. The configuration is very small and easily transferable. I mention this since `context` sounds heavyweight and not something that can be easily be serialized.
</comment><comment author="jimczi" created="2016-05-05T01:25:34Z" id="217054402">#### New proposal based on Simon&#8217;s idea:

To slice the data we rely on _id or on a field provided by the user 
API wise it&#8217;s really simple and concise: 

```
GET _search?scroll=1m
{
    "slice": {
        "field": "_id",
        "id": 0,
        "max": 2
    },
    "query": {
        "match_all": {}
    }
}
```

If the field is a string we use the hash of the string to perform the slicing, if it&#8217;s a number we use the value. OTB _uid is used from the fielddata. It&#8217;s safe we are sure that the content of the field is constant over time which is the guarantee to make the slicing consistent and it&#8217;s unique per index so each slice should have the same number of documents even if the number of slices is big. Though _id is not the most efficient way to do it especially when the number of slices is small.  To address that we can simply advise in the documentation to use an auto-generated field populated with the hash of the id and docvalues enabled. It can be even more efficient to populate a field with a random number generated when the document is created (and that is never updated). This random number should be in the range [0, max_splits_per_query] to maximize the compression in the docvalues.  

The main benefit I see with this approach is that each slice can be independent, they don&#8217;t need to use the same searcher/replica. 

IMO in the Hadoop/Yarn/Spark case we should slice the data based on the desired number of documents per task/map and default to something like 10,000. With Spark and even with Hadoop it&#8217;s difficult to handle shards with millions of documents that produces long running tasks. So instead of having one task per shard we could have one task per slice_id and each slice_id would hit every shard but get only the slice they are responsible for. So the only states that a split has are the split_id, the number of splits and the query. This approach uses the entire cluster in a nice and fair way as, each slice_id can theoretically use a different replica of the same shard. The drawback is that we&#8217;ll need to handle jobs with thousands of map/task but I don&#8217;t think it&#8217;s a problem because most of the map/reduce systems allow to cap the number of maps/tasks that are allowed to run concurrently. In fact this number should be derived from the number of nodes available in the cluster.

As an high level API it can be used for parallel scrolls of the same query (parallel readers) but also as a way to resume a long running task. For instance the reindex could divide the input data in multiple slices and then perform each slice sequentially. Then to resume a reindex task we would just need to restart after the last successfully processed slice.
</comment><comment author="clintongormley" created="2016-05-05T09:06:08Z" id="217107545">I love the proposal.  Wondering if we should add support for `from` to sorted scroll requests?
</comment><comment author="jimczi" created="2016-05-06T07:20:22Z" id="217368934">Thanks @clintongormley. I think it could be dangerous to use `from`in a sorted scroll request. The sort might be consistent but if the sort field is updatable we might miss documents ? Moreover I think that replaying the whole scroll request with the slice id is enough. For the Hadoop case having the slice feature would mean that we can create splits based on the desired number of documents for each. This number should be set from the time it takes to process all the documents. Tasks that last 30s to 1 minute should be the goal and in that case replaying the failing task should not be a problem (vs replaying from where we fail in the task). What do you think ?
</comment><comment author="clintongormley" created="2016-05-06T08:54:26Z" id="217388721">Sounds good to me
</comment><comment author="s1monw" created="2016-05-06T12:40:57Z" id="217428806">I like it @jimferenczi lets start with this and get it going!!
</comment><comment author="jimczi" created="2016-05-19T20:32:36Z" id="220443666">I did some testing of the `slice` feature. The following table shows the results for a full scroll of an index of 1M random documents on 1 shard (no replica) with different scroll configurations. The configuration contains the number of slices in the scroll, the number of documents per slice, the size of each request within the slice and the number of threads. The "elapsed time" is the total time for the scroll to complete in seconds. For instance the first line is for a scroll with 1 slice (current behavior), 1M documents in that slice, 10,000 documents per scroll request and 1 thread. The elapsed time is 23s. 
The slicing is done on the _uid (only when the number of slices is greater than 1) and the query cache is cleared after each run. I've kept the worst run out of 10 for each configuration:

| Number of slice | Number of documents per slice | Fetch size | Number of threads | Elapsed time |
| :-: | :-: | :-: | :-: | :-: |
| 1 | 1000000 | 10000 | 1 | **23s** |
| 1 | 1000000 | 1000 | 1 | 25s |
| 1 | 1000000 | 100 | 1 | 40s |
| 10 | 100000 | 10000 | 4 | **8s** |
| 10 | 100000 | 10000 | 3 | 10s |
| 10 | 100000 | 10000 | 2 | 12s |
| 10 | 100000 | 10000 | 1 | 25s |
| 10 | 100000 | 1000 | 4 | **8s** |
| 10 | 100000 | 1000 | 3 | 10s |
| 10 | 100000 | 1000 | 2 | 13s |
| 10 | 100000 | 1000 | 1 | 27s |
| 10 | 100000 | 100 | 4 | 11s |
| 10 | 100000 | 100 | 3 | 15s |
| 10 | 100000 | 100 | 2 | 19s |
| 10 | 100000 | 100 | 1 | 38s |
| 100 | 10000 | 10000 | 4 | 11s |
| 100 | 10000 | 10000 | 3 | 14s |
| 100 | 10000 | 10000 | 2 | 21s |
| 100 | 10000 | 10000 | 1 | 43s |
| 100 | 10000 | 1000 | 4 | 12s |
| 100 | 10000 | 1000 | 3 | 15s |
| 100 | 10000 | 1000 | 2 | 23s |
| 100 | 10000 | 1000 | 1 | 46s |
| 100 | 10000 | 100 | 4 | 13s |
| 100 | 10000 | 100 | 3 | 17s |
| 100 | 10000 | 100 | 2 | 26s |
| 100 | 10000 | 100 | 1 | 54s |
| 1000 | 1000 | 1000 | 4 | 44s |
| 1000 | 1000 | 1000 | 3 | 57s |
| 1000 | 1000 | 1000 | 2 | 85s |
| 1000 | 1000 | 1000 | 1 | 175s |
| 1000 | 1000 | 100 | 4 | 54s |
| 1000 | 1000 | 100 | 3 | 71s |
| 1000 | 1000 | 100 | 2 | 106s |
| 1000 | 1000 | 100 | 1 | 212s |
</comment><comment author="garyelephant" created="2016-07-05T09:37:20Z" id="230432410">I'd love this feature for Spark SQL on Elasticsearch case.For now, the bottleneck I see in my environment is that scroll is too slow with one shard per task.This has serious concurrent problem. I'm looking forward for it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.base.Preconditions#checkNotNull</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13493</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.base.Preconditions#checkNotNull across the codebase.
This is one of many steps in the eventual removal of Guava as a
dependency.

Relates #13224
</description><key id="105904965">13493</key><summary>Remove and forbid use of com.google.common.base.Preconditions#checkNotNull</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T21:51:08Z</created><updated>2015-09-10T21:58:33Z</updated><resolved>2015-09-10T21:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-10T21:56:25Z" id="139392236">LGTM. I'm usually partial to that static import instead of the qualified use of functions like `checkNotNull` but I'm not hung up on it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[ec2/azure] discovery does not work anymore from 2.0.0-beta1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13492</link><project id="" key="" /><description>Discovery does not work anymore in azure and ec2.

It's caused by the following commit which does not call anymore `addUnicastHostProvider`.
- AWS: https://github.com/elastic/elasticsearch/commit/40f119d85a4eaa39d0a6e594e46f70de725557f0#diff-767ee396aa86de938ebb72b4c2f359a0L35
- Azure: https://github.com/elastic/elasticsearch/commit/40f119d85a4eaa39d0a6e594e46f70de725557f0#diff-56a7239bf69dfcd365858d333edca510L43

@rjernst Could you have a look at it please?

cc @drewr 
</description><key id="105898077">13492</key><summary>[ec2/azure] discovery does not work anymore from 2.0.0-beta1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Cloud Azure</label><label>:Plugin Cloud GCE</label><label>:Plugin Discovery Azure Classic</label><label>:Plugin Discovery EC2</label><label>blocker</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-10T21:08:43Z</created><updated>2016-03-10T18:16:07Z</updated><resolved>2015-09-11T11:21:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-10T21:18:31Z" id="139382113">This should be simple to fix. The `onModule(DiscoveryModule)` methods in the aws and azure plugins just need to add the provider there (the discovery module has this method now). This is what I intended, but missed it since all tests pass without this! We really need some tests here...
</comment><comment author="rmuir" created="2015-09-10T21:22:10Z" id="139382909">I still think some mocks for integs that listen on a socket is the way to go. means something on a socket in pre-integration-test and the code really treats it like AWS X service or azure Y service. 
</comment><comment author="dadoonet" created="2015-09-10T21:27:23Z" id="139384392">Definitely. I started playing with [this lib](https://github.com/treelogic-swe/aws-mock) (running that in a Jetty container to simulate AWS calls). Was working fine at the beginning but at the end I was missing some important methods.
I started to contribute a bit but then started something else... 
</comment><comment author="dadoonet" created="2015-09-11T08:27:46Z" id="139483962">Reopening as the fix for Azure was wrong. 
Reverted with https://github.com/elastic/elasticsearch/commit/163c34127f877c70a0cb5bf95c64c6efada484eb
Tests are now failing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename start to verify_index in cat test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13491</link><project id="" key="" /><description>This was renamed in other places but not here.

Closes #13489
</description><key id="105896438">13491</key><summary>Rename start to verify_index in cat test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CAT API</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-10T20:59:12Z</created><updated>2016-03-10T18:14:27Z</updated><resolved>2015-09-10T21:03:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-10T21:00:00Z" id="139377304">Ping @s1monw or @rjernst who were involved in #10570 which caused this.
</comment><comment author="s1monw" created="2015-09-10T21:01:14Z" id="139377564">LGTM
</comment><comment author="nik9000" created="2015-09-10T21:04:47Z" id="139378346">Merged to master and cherry picked into 2.x and 2.0.
</comment><comment author="nik9000" created="2015-09-10T21:05:00Z" id="139378390">Thanks for the review, @s1monw.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] plugins simple RestIT tests don't work from IDE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13490</link><project id="" key="" /><description>When running a RestIT test from the IDE, you actually start an internal node which does not automatically load the plugin you would like to test.

We need to add:

``` java
    @Override
    protected Collection&lt;Class&lt;? extends Plugin&gt;&gt; nodePlugins() {
        return pluginList(PLUGIN_HERE.class);
    }
```

Everything works fine when running from maven because each test basically:
- installs elasticsearch
- installs one plugin
- starts elasticsearch with this plugin loaded
- runs the test

Note that this PR only fixes the fact we run an internal cluster with the expected plugin.

Cloud tests will still fail when run from the IDE because is such a case you actually start an internal node with many mock plugins.
And REST test suite for cloud plugins basically checks if the plugin is running by checking the output of NodesInfo API.

And we check:

``` yml
- match:  { nodes.$master.plugins.0.name: cloud-azure  }
- match:  { nodes.$master.plugins.0.jvm: true  }
```

But in that case, this condition is certainly false as we started also `mock-transport-service`, `mock-index-store`, `mock-engine-factory`, `node-mocks`, `asserting-local-transport`, `mock-search-service`.

Closes #13479
</description><key id="105892150">13490</key><summary>[test] plugins simple RestIT tests don't work from IDE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>test</label><label>v2.0.0-rc1</label></labels><created>2015-09-10T20:33:13Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-15T10:00:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-15T07:06:00Z" id="140300211">@bleskes Could you review this change please?
</comment><comment author="bleskes" created="2015-09-15T07:12:22Z" id="140301150">LGTM. I think's good to be able to run things in the IDE.  Can you label the PR? I'm good with 3.0,2.x,2.0

Re 

&gt; - match:  { nodes.$master.plugins.0.name: cloud-azure  }

I wonder if we should have a new type of selector that just does nodes.$master.plugins.*.name: cloud-azure /cc @clintongormley 
</comment><comment author="dadoonet" created="2015-09-15T07:17:52Z" id="140302024">@bleskes Yeah. I looked at it briefly to add it but I don't really know that part. May be @javanna could help on that?
</comment><comment author="s1monw" created="2015-09-15T07:38:44Z" id="140307660">LGTM
</comment><comment author="dadoonet" created="2015-09-15T08:00:19Z" id="140314088">@s1monw @bleskes I actually added a new commit as I found a method which is designed for this: `pluginList()`

Let me know if it's still good for you.
</comment><comment author="s1monw" created="2015-09-15T08:08:19Z" id="140315466">LGTM still
</comment><comment author="dadoonet" created="2015-09-15T09:47:45Z" id="140340101">For information, when cherry picking this to 2.0 branch, I needed to replace:

``` java
@Override
protected Collection&lt;Class&lt;? extends Plugin&gt;&gt; nodePlugins() {
    return pluginList(PLUGIN_HERE.class);
}
```

with

``` java
@Override
protected Settings nodeSettings(int nodeOrdinal) {
    return Settings.builder()
            .put(super.nodeSettings(nodeOrdinal))
            .put("plugin.types", PLUGIN_HERE.class.getName())
            .build();
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cat.recovery/10_basic is missing a recovery stage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13489</link><project id="" key="" /><description>It looks like cat.recovery/10_basic is missing one of the constants from `RecoveryState`.

```
FAILURE 0.29s J3 | Rest2IT.test {yaml=cat.recovery/10_basic/Test cat recovery output} &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: field [$body] was expected to match the provided regex but didn't
   &gt; Expected: ^
   &gt; (
   &gt;   index1      \s+
   &gt;   \d          \s+                                 # shard
   &gt;   \d+         \s+                                 # time
   &gt;   (store|replica|snapshot|relocating)     \s+   # type
   &gt;   (init|index|start|translog|finalize|done) \s+   # stage
   &gt;   [-\w./]+    \s+                                 # source_host
   &gt;   [-\w./]+    \s+                                 # target_host
   &gt;   [-\w./]+    \s+                                 # repository
   &gt;   [-\w./]+    \s+                                 # snapshot
   &gt;   \d+         \s+                                 # files
   &gt;   \d+\.\d+%   \s+                                 # files_percent
   &gt;   \d+         \s+                                 # bytes
   &gt;   \d+\.\d+%   \s+                                 # bytes_percent
   &gt;   \d+         \s+                                 # total_files
   &gt;   \d+         \s+                                 # total_bytes
   &gt;   \d+         \s+                                 # translog
   &gt;   -?\d+\.\d+% \s+                                 # translog_percent
   &gt;   -?\d+       \s+                                 # total_translog
   &gt;   \n
   &gt; )+
   &gt; $
   &gt;      but: was "index1 0 3  store   done         local local n/a n/a 0 0.0%   0   0.0%   0 0   0 100.0% 0 \nindex1 0 48 replica verify_index local local n/a n/a 1 100.0% 130 100.0% 1 130 0 0.0%   1 \n"
   &gt;    at __randomizedtesting.SeedInfo.seed([ED06E3B0DDBF01F2:6552DC6A73436C0A]:0)
   &gt;    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
   &gt;    at org.elasticsearch.test.rest.section.MatchAssertion.doAssert(MatchAssertion.java:55)
   &gt;    at org.elasticsearch.test.rest.section.Assertion.execute(Assertion.java:69)
   &gt;    at org.elasticsearch.test.rest.ESRestTestCase.test(ESRestTestCase.java:373)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; NOTE: leaving temporary files on disk at: /home/manybubbles/Workspaces/Elasticsearch/elasticsearch/core/target/J3/temp/org.elasticsearch.test.rest.Rest2IT_ED06E3B0DDBF01F2-001
  2&gt; NOTE: test params are: codec=Asserting(Lucene53): {}, docValues:{}, sim=RandomSimilarityProvider(queryNorm=true,coord=crazy): {}, locale=it_IT, timezone=Australia/Currie
  2&gt; NOTE: Linux 3.19.0-26-generic amd64/Oracle Corporation 1.8.0_45-internal (64-bit)/cpus=12,threads=1,free=388863912,total=517472256
  2&gt; NOTE: All tests run in this JVM: [UpdateMappingOnClusterIT, Rest2IT]
```

Reproduction command:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=ED06E3B0DDBF01F2 -Dtests.class=org.elasticsearch.test.rest.Rest2IT -Dtests.method="test {yaml=cat.recovery/10_basic/Test cat recovery output}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=it_IT -Dtests.timezone=Australia/Currie
```

I'm not sure which versions this effects.
</description><key id="105886995">13489</key><summary>cat.recovery/10_basic is missing a recovery stage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:CAT API</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-10T20:00:21Z</created><updated>2016-03-10T18:16:07Z</updated><resolved>2015-09-10T21:03:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-10T20:00:38Z" id="139364156">I believe this issue to be pretty rare but it came up for me.
</comment><comment author="nik9000" created="2015-09-10T20:56:25Z" id="139376422">This looks to be caused by #10570. Its simple enough to fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ShapeFetchService and TermsLookupFetchService and use a Client instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13488</link><project id="" key="" /><description>This commit removes all the optional injects etc. for the FetchServices and
provides a Client via IndexQueryParserService / Context. This allows direct
injection instead of optional injection. It also allows to remove all the
unnecessary services and use the fetch code where it belongs.
This commit also adds testing infrastructure for intercepting client calls
to AbstractQueryTestCase to support GET calls in query tests.
</description><key id="105885851">13488</key><summary>Remove ShapeFetchService and TermsLookupFetchService and use a Client instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>enhancement</label><label>review</label></labels><created>2015-09-10T19:53:34Z</created><updated>2015-09-11T07:20:15Z</updated><resolved>2015-09-11T07:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-11T07:01:51Z" id="139468337">LGTM
</comment><comment author="javanna" created="2015-09-11T07:10:11Z" id="139469603">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] IndexingMemoryControllerIT.testIndexBufferSizeUpdateInactiveShard </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13487</link><project id="" key="" /><description>Shard is not marked as inactive although we set the time `shard_inactive_time` to 100ms and then wait 10s for shard to be marked as inactive.
This here seems to be the only failure so far:
http://build-us-00.elastic.co/job/es_core_2x_centos/89/consoleText
I does not reproduce for me.
</description><key id="105855310">13487</key><summary>[test] IndexingMemoryControllerIT.testIndexBufferSizeUpdateInactiveShard </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-09-10T17:13:30Z</created><updated>2016-01-28T17:32:45Z</updated><resolved>2016-01-28T17:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-11T09:00:38Z" id="139492952">The test indexes one doc, confirms the memory controller up'd the indexing buffer, then flushes, then sits idle waiting for indexing buffer to go inactive (500 KB)...

In this failure, it looks like after indexing the one doc but before flushing, the memory controller had kicked in to set to inactive, but then with the flush it became active again ... still not sure why it didn't then become inactive again.  But maybe the timing in this run (becoming inactive before test could flush) is a clue ...
</comment><comment author="jasontedor" created="2015-09-21T12:01:37Z" id="141951961">This failed [again](http://build-us-00.elastic.co/job/es_core_2x_centos/285/testReport/junit/org.elasticsearch.indices.memory/IndexingMemoryControllerIT/testIndexBufferSizeUpdateInactiveShard/), but does not immediately reproduce.
</comment><comment author="mikemccand" created="2015-09-21T12:37:59Z" id="141960181">Thanks for the pointer @jasontedor ... I'll dig.
</comment><comment author="markharwood" created="2015-12-22T14:38:02Z" id="166630999">Another failure http://build-us-00.elastic.co/job/es_core_17_centos/820/
</comment><comment author="mikemccand" created="2015-12-23T11:41:15Z" id="166870364">OK I can explain the frequent 1.7 failures, where the test times out waiting for the index to become active again ... it's a case we already know about from the code:

```
                // since we sync flush once a shard becomes inactive, the translog id can change, however that
                // doesn't mean the an indexing operation has happened. Note that if we're really unlucky and a flush happens
                // immediately after an indexing operation we may not become active immediately. The following
                // indexing operation will mark the shard as active, so it's OK. If that one doesn't come, we might as well stay
                // inactive
```

I.e., a sync'd flush snuck in after the test indexed one document but before IMC noticed the shard was active and so IMC never marks the shard active ... maybe I can fix the check to detect a flush occurred and index another document ... but this (IMC) is all greatly simplified in 2.x (https://github.com/elastic/elasticsearch/pull/15252), maybe we should just remove this test from 1.7.x?

The 2.x failures are different (failing to become inactive) ... not sure about those yet.
</comment><comment author="mikemccand" created="2016-01-28T17:32:40Z" id="176294513">It looks like my fix worked for 1.7, and 2.x stopped failing (not sure why, but there have been plenty of changes) ... so I'm optimistically closing this ... feel free to re-open if we see new test failures.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactors MoreLikeThisQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13486</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="105854935">13486</key><summary>Refactors MoreLikeThisQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label></labels><created>2015-09-10T17:12:07Z</created><updated>2015-09-25T12:31:36Z</updated><resolved>2015-09-16T00:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-11T15:47:18Z" id="139581576">@alexksikes can you rebase this to latest - there are quite some changes here that are now gone
</comment><comment author="alexksikes" created="2015-09-14T15:37:20Z" id="140118171">@s1monw You can take a look now. Thank you.
</comment><comment author="colings86" created="2015-09-15T09:26:09Z" id="140335707">@alexksikes I left some comments. I also think someone else will need to quickly look before its merged as I have no experience of the MLT query
</comment><comment author="alexksikes" created="2015-09-15T09:47:56Z" id="140340124">@colings86 Thanks for the review. I'll address the comments on the next round of review.
</comment><comment author="cbuescher" created="2015-09-15T15:08:41Z" id="140423571">@alexksikes I did a round of reviews, given the amount of complexity in constructing the final MLT query I didn't check the internals of the helper methods moved over from the parsers, maybe those can be simplified later but as far as separating the toQuery/fromXContent I think this is good. Maybe someone else should have final look after you adressed the last comments.
</comment><comment author="alexksikes" created="2015-09-15T16:15:56Z" id="140448133">@cbuescher @colings86 I addressed all the comments. Should I do the package change for the Item class as part of this PR too? Thanks again for the review.
</comment><comment author="s1monw" created="2015-09-15T20:15:45Z" id="140524614">I left some minor comments LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] RoutingServiceTests.testDelayedUnassignedScheduleReroute fails every second day or so</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13485</link><project id="" key="" /><description>Last failure here: http://build-us-00.elastic.co/job/es_core_master_oracle_6/2285/consoleText
Does not reproduce for me.
</description><key id="105853525">13485</key><summary>[test] RoutingServiceTests.testDelayedUnassignedScheduleReroute fails every second day or so</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-09-10T17:05:09Z</created><updated>2015-09-13T15:11:37Z</updated><resolved>2015-09-13T15:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-10T17:44:43Z" id="139322338">I pushed additional logging for this as I was unable to reproduce it, hopefully if it fails again I can determine why.
</comment><comment author="dakrone" created="2015-09-10T21:48:36Z" id="139388473">Another failure with the extra logging here: http://build-us-00.elastic.co/job/es_g1gc_master_metal/17107/consoleText I'll look into it.
</comment><comment author="s1monw" created="2015-09-13T15:11:37Z" id="139886176">fixed by https://github.com/elastic/elasticsearch/commit/5098dcaf96744fbc9f1ae5179e530453e9654de0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backwards compatibility tests should be run with the security manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13484</link><project id="" key="" /><description>Right now the backwards compatibility tests disable the security manager entirely or the external nodes won't run. We should fix that.
</description><key id="105852248">13484</key><summary>Backwards compatibility tests should be run with the security manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T16:57:27Z</created><updated>2015-11-09T13:17:27Z</updated><resolved>2015-11-09T13:17:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-15T23:27:57Z" id="140579856">I know these tests want to execute which is not an option for any code. I suggest a different approach: https://github.com/elastic/elasticsearch/issues/13216
</comment><comment author="nik9000" created="2015-11-02T14:08:42Z" id="153026521">I've resolved this particular issue using a service that runs inside maven to do the forking. I've started using #13216 to collect all issues and ideas for improvement for the backwards tests. I believe that the backwards tests need to be able to fork so they can perform rolling restarts. The tests could really do more but I'm collecting those things in #13216.
</comment><comment author="clintongormley" created="2015-11-08T18:09:24Z" id="154854708">@nik9000 sounds like this one can be closed?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] IndicesSegmentsRequestTests.testVerbose failure </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13483</link><project id="" key="" /><description>It is a very short test so there is not much to say: IndicesSegmentResponse misses a result.
http://build-us-00.elastic.co/job/es_core_master_regression/3241/consoleText
This one actually reproduces every 5th run or so! 
</description><key id="105844924">13483</key><summary>[test] IndicesSegmentsRequestTests.testVerbose failure </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-09-10T16:17:46Z</created><updated>2015-12-18T08:23:31Z</updated><resolved>2015-12-18T08:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-10-22T01:26:23Z" id="150071083">This one is still failing, see: http://build-us-00.elastic.co/job/es_core_2x_metal/343/consoleText

@rjernst I believe you added the verbose segments API? Can you take a look?
</comment><comment author="rjernst" created="2015-12-18T08:23:29Z" id="165712385">This was fixed a while ago:
18ffb63
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use https to talk to blob.core.windows.com</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13482</link><project id="" key="" /><description>... because security and stuff!
</description><key id="105843181">13482</key><summary>Use https to talk to blob.core.windows.com</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">afrazkhan</reporter><labels><label>:Plugin Cloud Azure</label><label>:Plugin Discovery Azure Classic</label><label>enhancement</label></labels><created>2015-09-10T16:10:42Z</created><updated>2015-09-18T15:36:41Z</updated><resolved>2015-09-15T13:34:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-10T21:31:53Z" id="139385342">@afrazkhan Could you sign the CLA please?
</comment><comment author="afrazkhan" created="2015-09-11T07:53:24Z" id="139476989">I have :) Not sure why it's showing that I haven't, but I had an email
confirmation of the signature and everything!

On Thu, 10 Sep 2015 at 22:33, David Pilato notifications@github.com wrote:

&gt; @afrazkhan https://github.com/afrazkhan Could you sign the CLA please?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13482#issuecomment-139385342
&gt; .
</comment><comment author="dadoonet" created="2015-09-15T13:34:16Z" id="140394946">Closing in favor of #13573 which changes also another `http` line.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] make sure ESSingleNodeTestCase waits after starting node until all blocks removed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13481</link><project id="" key="" /><description>When a single node starts up it will first elect itself as master and then tries to
recover the cluster state or, if there is none,  initialize an empty one and publish it.
Until it has done that, the cluster state will contain a global block and
requests might fail with
SERVICE_UNAVAILABLE/1/state not recovered / initialized

We had build failures in `TransportDeleteByQueryActionTests.testExecuteScanFailsOnMissingIndex` because of that: http://build-us-00.elastic.co/job/es_core_master_suse/1646/consoleText
</description><key id="105839964">13481</key><summary>[test] make sure ESSingleNodeTestCase waits after starting node until all blocks removed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>test</label></labels><created>2015-09-10T15:57:42Z</created><updated>2015-09-19T11:08:04Z</updated><resolved>2015-09-14T15:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-10T18:46:34Z" id="139340868">Left a minor suggestion
</comment><comment author="brwe" created="2015-09-11T15:50:09Z" id="139582143">@bleskes thanks for the review! added another commit
</comment><comment author="bleskes" created="2015-09-11T19:01:37Z" id="139631372">LGTM. Left trivial comments.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Release script: Improve automation for package repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13480</link><project id="" key="" /><description>Various release script improvements
- Automatic package repository creation for debian and rpm repositories using deb-s3 and rpm-s3 tools
- Fixing paths in email for repositories
- Add manual verification step for maven staging repo
- Do not create release directory in /tmp, because we might loose it on VMs
- Add signage check for RPM
- Added `--check` option to check for environment and availability of binaries

Closes #13209
</description><key id="105835008">13480</key><summary>Release script: Improve automation for package repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>build</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-10T15:32:48Z</created><updated>2015-09-16T13:39:15Z</updated><resolved>2015-09-14T13:04:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[test] plugins simple RestIT tests don't work from IDE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13479</link><project id="" key="" /><description>When running a RestIT test from the IDE, you actually start an internal node with many mock plugins.

But REST test suite for cloud plugins basically checks if the plugin is running by checking the output of NodesInfo API.

And we check:

``` yml
    - match:  { nodes.$master.plugins.0.name: cloud-azure  }
    - match:  { nodes.$master.plugins.0.jvm: true  }
```

But in that case, this condition is certainly false as we started also `mock-transport-service`, `mock-index-store`, `mock-engine-factory`, `node-mocks`, `asserting-local-transport`, `mock-search-service`.

Also, for all plugin Rest tests we are missing the fact we need to load the plugin when running from the IDE:

``` java
    @Override
    protected Collection&lt;Class&lt;? extends Plugin&gt;&gt; nodePlugins() {
        return Collections.singletonList(PLUGIN_HERE.class);
    }
```

Everything works fine when running from maven because each test basically:
- installs elasticsearch
- installs one plugin
- starts elasticsearch with this plugin loaded
- runs the test
</description><key id="105829172">13479</key><summary>[test] plugins simple RestIT tests don't work from IDE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>test</label></labels><created>2015-09-10T15:04:26Z</created><updated>2015-09-15T10:00:51Z</updated><resolved>2015-09-15T10:00:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-10T16:56:20Z" id="139309097">AFAIK these were never intended to run from the IDE. These are integration tests that run against a real cluster, not like the integ tests that the RestTestCase class is based on. To run these integration tests, you must use an external cluster.
</comment><comment author="dadoonet" created="2015-09-10T16:59:06Z" id="139309706">That's easy to fix though and the PR is almost ready.

But if what you said is true, then we don't need to make RESTtest depends on ESIntegTestCase IMO. 
</comment><comment author="rjernst" created="2015-09-10T17:09:34Z" id="139312330">The integ tests use the rest test framework, as it already existed. If we want to refactor it so that when run within core it still gets an integ cluster, and everywhere else it uses an external cluster, great!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate `score_type` option in favour of the `score_mode` option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13478</link><project id="" key="" /><description /><key id="105826147">13478</key><summary>Deprecate `score_type` option in favour of the `score_mode` option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>deprecation</label><label>review</label><label>v2.1.0</label></labels><created>2015-09-10T14:49:53Z</created><updated>2015-11-20T14:10:20Z</updated><resolved>2015-09-10T15:01:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-10T14:52:24Z" id="139270669">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix backwards compatibility tests in 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13477</link><project id="" key="" /><description>These tests have gone stale because 2.0 hasn't needed to be protocol compatible with anything. But 2.0.1 and 2.1.0 will have to be backwards compatible with 2.0.0. Since we don't have 2.0.0 released but we still want to work on features for 2.1.0 that have to be backwards compatible with 2.0.0 we should use 2.0.0-beta1 as a stand in for the eventual 2.0.0 release.

Closes #13425
Closes #13484
</description><key id="105825809">13477</key><summary>Fix backwards compatibility tests in 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label></labels><created>2015-09-10T14:48:25Z</created><updated>2015-11-02T15:58:08Z</updated><resolved>2015-11-02T13:57:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-10T14:48:46Z" id="139269560">This is very much a work in progress. It'll take a bit more banging around to fix the remaining BWC tests.
</comment><comment author="rjernst" created="2015-09-10T16:49:59Z" id="139307565">Thanks @nik9000! I left a couple comments. The changes so far look fine, I think.
</comment><comment author="nik9000" created="2015-09-15T22:14:25Z" id="140561498">I'm dropping 2.0.0 from this because its not strictly required for 2.0.0 - its needed for 2.0.1 and 2.1.0. We'll get this soon but I don't want it to be a blocker.
</comment><comment author="nik9000" created="2015-09-17T13:30:08Z" id="141085772">Ok - I've finally got this ready for review. It doesn't fix all the tests but I was sinking hour and hours and hours per busted test and I didn't want each one to block getting the rest fixed.

I did squash and rebase this during the review and for that I'm sorry - it was getting pretty sprawling and had some twists and turns that ended up not being good ways to fix the tests. But its really really ready for review now.
</comment><comment author="nik9000" created="2015-09-17T14:14:24Z" id="141100292">Added 3.0 label. This should be merged into 2.0, 2.x, and master branches even though we don't expect master to be backwards compatible with anything.
</comment><comment author="nik9000" created="2015-09-17T19:40:02Z" id="141204037">Ping to @rjernst and probably @dakrone to review.

I've disabled custom data paths for these tests for now because they don't seem to be working properly - probably because the data paths are relative to the path.home and the upgraded nodes have a different path.home than the backwards nodes.
</comment><comment author="nik9000" created="2015-10-22T17:54:40Z" id="150305527">Quick update for anyone following along at home: as of the last patch the BWC tests that are not `@AwaitsFix` are now passing reasonably consistently. All but `UnicastBackwardsCompatibilityIT` pass all the times I've tried them. I'm working on that last one.

After that I think we have two options:
1. Just merge this.
2. I keep working on un `@AwaitsFix`ing things until 2.0 is released and then I switch this to use 2.0 and roll back the `Version` hack I added way back at the beginning of this process and _then_ we merge it.
</comment><comment author="javanna" created="2015-10-23T15:52:24Z" id="150614447">I had a quick look and left some comments. Also ran tests on my machine, UpgradeIT failed with `java.nio.file.NoSuchFileException: /Users/lucacavanna/dev/elasticsearch/core/core/target/J0/./temp/bwc/external_1/data`, maybe something is wrong on my machine? Also added 2.0.1 label as this will need to go to 2.0 branch too.
</comment><comment author="nik9000" created="2015-10-23T17:17:11Z" id="150637247">&gt; Also added 2.0.1 label as this will need to go to 2.0 branch too.

Awesome.

&gt; UpgradeIT failed

Booo! It didn't fail for me but let me try again.
</comment><comment author="rmuir" created="2015-10-23T19:17:15Z" id="150667606">I'm really really concerned about the execution here. I do not think it should be allowed, even in tests.

If these tests must really run this way (which i dont believe), we should start up some other background test listening on a socket or similar responsible for launching them, instead of allowing execution from the ES environment. (i already see this "setting a precendent" for more leniency).
</comment><comment author="s1monw" created="2015-10-23T19:51:03Z" id="150673976">&gt; I'm really really concerned about the execution here. I do not think it should be allowed, even in tests.

the reason why we run this way is a historic one. This is how I added it back in 1.2 when we had no BWC tests at all. From the todays perspective I think we should move away from how it's done. The question is if it makes sense to make the test work again and then work from there to cut over to a new system that is controlled from the outside ie. basic tests that replace the once that we have? 
To me what is most important are 2 kinds of tests:
- upgrade tests where we upgrade a cluster from one version to another either as _rolling_ or _full cluster_ restart
- running simple tests against a mixed version cluster to ensure our wire protocols are working and we can operate in such a scenario

both can relatively simple be done without starting / stopping nodes from within ES and maybe thats a good start? I still wonder if we should make the ones we have work first and then go from there?
</comment><comment author="nik9000" created="2015-10-23T20:04:22Z" id="150676589">&gt; I still wonder if we should make the ones we have work first and then go from there?

This patch as it stands gets them working in the same way they worked before.

I'm going to work on a little daemon we can start pre-integration-test that can shim the start/stop of `ExternalNode`s. We could cut over to using that and keep the tests as they stand and reenable seccomp. I'd be much happier with the PR if we can do that.

There are lots of great things we could do here - move to the qa module and get proper maven dependency resolution so they run on every run - invoke current version of elasticsearch as an `ExternalNode` so its less mock-ed.
</comment><comment author="nik9000" created="2015-10-28T18:44:25Z" id="151948459">OK - I've build a service that can be forked pre-integration-test to run these the external nodes. Right now it forks in core's build only if `tests.bwc.path` is set. I'd love to move all of these tests to qa and turn on proper dependency resolution so we don't need `tests.bwc.path`.

I'm in no way proud of the service's implementation - it gets the job done but I'm sure I've done tons of non normal stuff so feel free to complain about it.
</comment><comment author="nik9000" created="2015-10-28T19:44:33Z" id="151968903">Another update: the tests now pass with the security manager enabled so I've removed the recommendation to disable it. The external nodes and the internal nodes run under the security manager. Only ExternalNodeService runs without it and it runs in the maven JVM, at least it does the way I've got it rigged right now.
</comment><comment author="nik9000" created="2015-10-29T20:19:35Z" id="152306827">Yet another update: the tests now no longer need the external setup and download step, they resolve their dependencies using maven.
</comment><comment author="rjernst" created="2015-10-30T17:41:26Z" id="152597782">LGTM, let's get this into 2.x/2.1. It won't work on master until we port to gradle, but maybe by then we can re-evaluate how to do bwc tests to simplify this (and use all real nodes instead of some mock and some external).
</comment><comment author="nik9000" created="2015-10-30T21:24:03Z" id="152654744">Squashed and rebased.
</comment><comment author="nik9000" created="2015-11-02T14:03:44Z" id="153024656">Merged to 2.x. Cherry-picking to 2.1 now.
</comment><comment author="nik9000" created="2015-11-02T15:58:08Z" id="153061439">And merged into 2.1: 10315d8167de96fc0b2b992b5407a8518b379a94
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>hack jython tests to work on windows.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13476</link><project id="" key="" /><description>We already have a jython hack, but windows needs access to the "root" holding the jar file (see https://github.com/int3/jython/blob/master/src/org/python/core/PySystemState.java#L571-L616)

Its different on windows, because when canonicalizing the file (case sensitivity), it needs access to the file.

I reproduced the problem in a VM and that `mvn verify` passes with the fix.

relates to #13467 but does not explain why tests apparently were not running with security manager in jenkins for windows, there is something up with maven/jenkins there.
</description><key id="105825719">13476</key><summary>hack jython tests to work on windows.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label></labels><created>2015-09-10T14:48:04Z</created><updated>2015-09-19T15:11:43Z</updated><resolved>2015-09-10T19:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-10T15:31:04Z" id="139282240">lgtm
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pipeline Aggregations at the root of the agg tree are now validated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13475</link><project id="" key="" /><description>Previously PipelineAggregatorFactory's at the root to the agg tree (top-level aggs) were not validated. This commit adds a call to PipelineAggregatoFactory.validate() for that case.

Closes #13179
</description><key id="105825248">13475</key><summary>Pipeline Aggregations at the root of the agg tree are now validated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T14:46:08Z</created><updated>2015-09-11T06:56:49Z</updated><resolved>2015-09-10T15:00:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-10T14:58:01Z" id="139272146">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add backwards compatibility test for index failed stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13474</link><project id="" key="" /><description>We added index failed stats in #13130 but didn't add any backwards compatibility tests because the backwards compatibility tests were broken (#13425). Once they are fixed we should add one for index failed stats.
</description><key id="105824435">13474</key><summary>Add backwards compatibility test for index failed stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Stats</label><label>adoptme</label><label>test</label></labels><created>2015-09-10T14:42:19Z</created><updated>2017-03-21T15:16:30Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="andrestc" created="2015-09-15T02:23:25Z" id="140257979">I have something already started for this, waiting on #13425 so I can try and submit the PR
</comment><comment author="nik9000" created="2015-09-15T13:39:34Z" id="140396080">&gt; I have something already started for this, waiting on #13425 so I can try and submit the PR

Cool! I'll get back to the backwards compatibility tests ASAP. They are mind numbing because they take sooooo long to fail. I'm to the point where I'm just going to make the failing ones as `@AwaitsFix` and move on. At least that way you'll be able to start on this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to start elastic search in RHEL 7</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13473</link><project id="" key="" /><description>I have installed JDK and also elastic search 1.4.5.

when I tried to start it using 'sudo systemctl start elasticsearch.service' I faced the below error.

&gt; elasticsearch.service - Starts and stops a single elasticsearch instance on this system
&gt;   Loaded: loaded (/usr/lib/systemd/system/elasticsearch.service; enabled)
&gt;   Active: failed (Result: exit-code) since Thu 2015-09-10 09:51:13 EDT; 8s ago
&gt;     Docs: http://www.elasticsearch.org
&gt;  Process: 2456 ExecStart=/usr/share/elasticsearch/bin/elasticsearch -d -p &gt;/var/run/elasticsearch/elasticsearch.pid -Des.default.config=$CONF_FILE -&gt;Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -&gt;Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -&gt;Des.default.path.conf=$CONF_DIR (code=exited, status=1/FAILURE)

Helps much appreciated :)
</description><key id="105823254">13473</key><summary>Unable to start elastic search in RHEL 7</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PandiyanCool</reporter><labels /><created>2015-09-10T14:36:30Z</created><updated>2015-09-18T04:57:00Z</updated><resolved>2015-09-14T15:35:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-10T15:44:37Z" id="139286339">You're going to want to have a look at the LOG_DIR ([defaults to /var/log/elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-service.html)) for information.  You may need to bump up your [logging level](https://www.elastic.co/guide/en/elasticsearch/guide/current/logging.html) temporarily
</comment><comment author="eskibars" created="2015-09-10T16:02:29Z" id="139292648">Also, I'm going to insert a plug for https://discuss.elastic.co/ here :smile: It's where general questions/help are provided (we largely use github for specific bugs and feature requests)
</comment><comment author="eskibars" created="2015-09-14T15:35:43Z" id="140117755">I'm going to go ahead and close this issue at this time.  If you still think there's a bug after talking through the forums, please reopen it!
</comment><comment author="PandiyanCool" created="2015-09-18T04:57:00Z" id="141344272">sure thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PUT Mapping enforces _timestamp to be set every time if index has it stored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13472</link><project id="" key="" /><description>This was found on ES 1.7.1.

Creating an index with _timestamp enabled:

```
curl -XPOST localhost:9200/test -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_timestamp": {"enabled": true, "store": true},
            "properties" : {
                "field1" : { "type" : "string", "index" : "not_analyzed" }
            }
        }
    }
}'
```

If we try creating a new field, with PUT Mapping API:

```
curl -XPUT 'http://localhost:9200/test/_mapping/type1' -d '
{
  "type1": {
    "properties": {
      "field2": {"type": "string"}
    }
  }
}
'
```

It fails saying _timestamp has different store values:

```
[2015-09-10 11:27:45,636][DEBUG][action.admin.indices.mapping.put] [Black Tom Cassidy] failed to put mappings on indices [[test]], type [type1]
org.elasticsearch.index.mapper.MergeMappingException: Merge failed with failures {[mapper [_timestamp] has different store values]}
    at org.elasticsearch.cluster.metadata.MetaDataMappingService$4.execute(MetaDataMappingService.java:511)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:374)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:188)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:158)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

To be able to create the new mapping, we need to specify the _timestamp configuration again:

```
curl -XPUT 'http://localhost:9200/test/_mapping/type1' -d '
{
  "type1": {
    "_timestamp" : { "enabled" : true, "store": "true" },
    "properties": {
      "field2": {"type": "string"}
    }
  }
}
'
```

Unless I'm missing something, I believe this is not de desired behavior. I think since I'm not stating anything about the _timestamp field on the Put Mapping, it should succeed keeping the configuration stated on index creation time.

If this is indeed a bug, I can try submiting a PR for it.
</description><key id="105822949">13472</key><summary>PUT Mapping enforces _timestamp to be set every time if index has it stored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andrestc</reporter><labels><label>bug</label><label>discuss</label></labels><created>2015-09-10T14:34:51Z</created><updated>2015-09-19T15:04:36Z</updated><resolved>2015-09-19T15:04:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-10T17:21:03Z" id="139316444">I'm able to reproduce.  If not defined, it appears the _timestamp type is implied (and implied to have the default value of "false").  So, adding ?ignore_conflicts=true also allows the update mapping to go through, but can have other (undesired) consequences if you had a "real" conflict.  So yes, looks like a bug.  _timestamp is deprecated in 2.0 though, in favor of using a normal date field, so it may be best to treat this as a limitation in 1.x, since there is a (relatively simple) workaround.  Thoughts?
</comment><comment author="clintongormley" created="2015-09-19T15:04:36Z" id="141678371">This has been fixed in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>clean up crazy filtering of ES test.jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13471</link><project id="" key="" /><description>Today this has a lot of includes and excludes (is this some attempt to optimize something?). I think its the wrong tradeoff, can't we just put everything in there? Otherwise its very flaky and can cause very difficult to debug build failures.

```
                            &lt;includes&gt;
                                &lt;include&gt;rest-api-spec/**/*&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/test/**/*&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/bootstrap/BootstrapForTesting.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/bootstrap/XTestSecurityManager*.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/common/cli/CliToolTestCase.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/common/cli/CliToolTestCase$*.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/cluster/MockInternalClusterInfoService.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/cluster/MockInternalClusterInfoService$*.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/index/shard/MockEngineFactoryPlugin.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/search/MockSearchService.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/search/MockSearchService$*.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/cache/recycler/MockPageCacheRecycler.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/cache/recycler/MockPageCacheRecycler$*.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/common/util/MockBigArrays.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/common/util/MockBigArrays$*.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/node/NodeMocksPlugin.class&lt;/include&gt;
                                &lt;include&gt;org/elasticsearch/node/MockNode.class&lt;/include&gt;
                            &lt;/includes&gt;
                            &lt;excludes&gt;
                                &lt;!-- unit tests for yaml suite parser &amp; rest spec parser need to be excluded --&gt;
                                &lt;exclude&gt;org/elasticsearch/test/rest/test/**/*&lt;/exclude&gt;
                                &lt;!-- unit tests for test framework classes--&gt;
                                &lt;exclude&gt;org/elasticsearch/test/test/**/*&lt;/exclude&gt;
                            &lt;/excludes&gt;
```
</description><key id="105817964">13471</key><summary>clean up crazy filtering of ES test.jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>discuss</label><label>test</label></labels><created>2015-09-10T14:24:08Z</created><updated>2016-01-28T17:23:16Z</updated><resolved>2016-01-28T17:23:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T17:23:16Z" id="176290847">This has been sorted out in master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Several other parent/child cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13470</link><project id="" key="" /><description>- Dropped ScoreType in favour of Lucene's ScoreMode
- Removed `score_type` option from `has_child` and `has_parent` queries in favour for the already existing `score_mode` option.
- Removed the score mode `sum` in favour for the already existing `total` score mode. (`sum` doesn't exist in Lucene's ScoreMode class)
- If `max_children` is set to `0` it now really means that zero children are allowed to match.
</description><key id="105815860">13470</key><summary>Several other parent/child cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking-java</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T14:18:21Z</created><updated>2016-07-29T12:08:57Z</updated><resolved>2015-09-10T15:16:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-10T14:30:17Z" id="139261988">This looks good to me, let's just make sure we push the score_type deprecation to 2.x so that users are warned
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get function score query results as fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13469</link><project id="" key="" /><description>[I asked about this a couple of weeks ago in the forum](https://discuss.elastic.co/t/get-function-score-value-as-fields/28129) and didn't get any answers, so it seems that this is not implemented at the moment, therefore this is a feature request.

Basically, I need to return several function score query results as fields. Parsing the explanation tree is not really an option as it's cumbersome, error-prone, and slow (explanations bloat up the response quite a bit).

There is `script_fields` but it doesn't seem like it's possible to call arbitrary function score queries (e.g. a decay function).

To give an example, I'd like to run something like this:

``` json
{
  "script_fields": {
    "pepe": {
      "random_score": {
        "seed": 123
      }
    }
  }
}
```

where [`random_score`](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-random) could be any function score query.

or alternatively being able to run any arbitrary function score as part of a script:

``` json
{
  "script_fields": {
    "pepe": {
      "script": "function_queries.random_score(123)"
    }
  }
}
```
</description><key id="105812759">13469</key><summary>Get function score query results as fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mausch</reporter><labels><label>:Scripting</label><label>discuss</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2015-09-10T14:03:14Z</created><updated>2016-01-20T11:48:52Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mickdelaney" created="2015-09-10T17:45:59Z" id="139322636">+1
</comment><comment author="nik9000" created="2015-09-10T17:48:23Z" id="139323162">@brwe, do you have a sense of how hard this would be?
</comment><comment author="brwe" created="2015-09-10T18:01:47Z" id="139328906">I did not take a close look yet but don't think it would be super hard. However, I wonder what your use case is? What do you need the function results for? 
</comment><comment author="mickdelaney" created="2015-09-10T18:21:52Z" id="139334488">Use the scores in the application individually, rather than relying on a single score. Or parsing the explanation (which is huge obviously). 

Does that make sense ??
</comment><comment author="nik9000" created="2015-09-10T18:32:35Z" id="139337135">&gt; Use the scores in the application individually, rather than relying on a single score. Or parsing the explanation (which is huge obviously).
&gt; 
&gt; Does that make sense ??

Some sense, yeah. The trouble with using the scores individually is that the result ordering and cutoff will still be based on the overall score. But I see how you could flavor the individual results. Like put a little "closest!" marker or the one with the highest geographic score or something. The thing is that you might not get the actual "closest" result back because it might score lower for other reasons. Then again that could be fine for your use case.

You can work around not having this by implementing the function score as a script. Everything but random score should be pretty doable.
</comment><comment author="mausch" created="2015-09-11T08:54:58Z" id="139491672">&gt; You can work around not having this by implementing the function score as a script. Everything but random score should be pretty doable.

Yep, but that wouldn't give us access to built-in functions (e.g. a [decay function](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-decay)). We'd have to avoid all these nice built-in functions and reimplement them as scripts instead (which is slower, clunkier, etc). That's why I also suggested being able to access built-in functions from scripts as an alternative.

The main use case for this is to expose individual score components to the end user. At the moment if you have a complex scoring mechanism with lots of functions, the end user doesn't understand why results rank the way the rank. Sometimes this is ok or the user shouldn't have to care (e.g. Google) but otherwise it's useful to aid the user in making a decision on each result. Combining scores from different functions is sometimes highly personal and you can't have a one-size-fits-all overall score so the user has to be able to see into the otherwise black box.

Another use case we're considering is automating the relevance tuning to some degree.

Lastly it would be really useful for business analytics, to understand how users behave, then mine that data, analyse it, etc.
</comment><comment author="nik9000" created="2015-09-11T12:25:22Z" id="139532833">&gt; Yep, but that wouldn't give us access to built-in functions (e.g. a decay function). We'd have to avoid all these nice built-in functions and reimplement them as scripts instead (which is slower, clunkier, etc). That's why I also suggested being able to access built-in functions from scripts as an alternative.

Actually what I meant was that you could absolutely reimplement the decay functions as scripts. If you used expression scripts they should perform quite well. On par with the native functions. But implementing them is a pain so this is really just a workaround. [Here](https://en.wikipedia.org/wiki/Special:Search/prefer-recent:.5,10_stuff?cirrusDumpQuery) is an example reimplementing one of the functions to add more features, in this case it does exponential decay but only to 50% of the score.

I think I understand your use case. Would it be fair to sum it up as an easier to parse and smaller explain output?
</comment><comment author="mausch" created="2015-09-11T12:45:40Z" id="139537186">&gt; I think I understand your use case. Would it be fair to sum it up as an easier to parse and smaller explain output?

Technically, yes. The simplest way really is to be able to expose this as pseudo-fields, just like [Solr has been doing for quite some time now](http://wiki.apache.org/solr/CommonQueryParameters#functions).
</comment><comment author="nik9000" created="2015-09-11T13:39:24Z" id="139548516">I think I'm sold. I had a voice conversation with @brwe and @colings86 about this and we came to the conclusion that:
1. This is reasonably easy to do.
2. It would have been useful for @brwe to have had in the past - in particular when building presentations about how the scores are built.
3. Its totally possible to build this and misinterpret the results.
4. In many many cases you don't want to return these results to the user. For example if you want to show a user the result that is closest to them you would want the distance from their location without any exponential decay applied. In those cases this would be trappy.
5. Its possible that users may think that these values are cached between the search and fetch phase. They wouldn't be. It'd be more trouble than its worth.

So I think this is worth doing but we'll have to make sure there is a big warning. I hate those but its wroth it I think.

I'm marking this as low hanging fruit because it should be reasonably simple but I don't know how its ranked relative to other things. So its probably a good thing for a new contributor to pick up.

Ping @clintongormley to check all this - he'll have opinions that might differ from mine and he gets to make the final call.
</comment><comment author="nik9000" created="2015-09-11T13:46:28Z" id="139551250">I should point out that this might be useful to build [radar charts](https://en.wikipedia.org/wiki/Radar_chart) describing search scores. Which are pretty.
</comment><comment author="clintongormley" created="2015-09-19T15:44:20Z" id="141682141">Is perhaps a simpler alternative just to provide convenience functions that implement gauss/exp/linear decay for use in scripts? Then this could be done easily with script fields.

I'd be interested to see how the above suggestions would be exposed at the API level
</comment><comment author="clintongormley" created="2016-01-20T10:21:24Z" id="173161198">Thinking about this issue some more...

The original request (to store the results of scripts in the function score to return them with each hit) won't work.  We'd have to cache these results on each shard during the search phase to later be returned with fetch phase, or pass back all of these results to the coordinating node during the search phase.  Horribly complicated, lots of overhead.

The alternative is to rerun these scripts during the fetch phase for just the documents being fetched.  This wouldn't be possible without rerunning the query itself, as these scripts can have access to the `_score`, which depends on the sub-clause of the query where the script is executed. Plus, think that these scripts may be executed within a highly nested set of bool clauses, or within a nested or has_child query - what would be the structure for returning these values?  The overhead of rerunning the query and returning this big structure would be high, and it is already implemented as the explain API.

The last alternative is to calculate values as script fields, which you can do today (although you don't have access to the `_score`, or the context of nested queries etc).  While less flexible, this is more efficient because you don't rerun the query, you don't cache results on the shard, and you just have a simple list of keys and values, rather than the massive structure required to support something like explain.

All of the convenience functions in function score are documented and can be reimplemented pretty easily in raw scripts.  I am not in favour of adding the full JSON support that we have in function score for script_fields. The only thing I would consider doing is providing predefined functions (eg gauss_decay) in a script language which package up the calculations.

Perhaps that is something we can look at doing with the new scripting language that we are adding (https://github.com/elastic/elasticsearch/issues/13084).  @jdconrad what's your take on doing this?
</comment><comment author="mausch" created="2016-01-20T11:48:52Z" id="173181386">Thanks for the analysis @clintongormley . 

I agree that caching the results from the function query would be complicated to implement (though from the point of view of the client it would be the best option). It would also probably require adding a name to each function query that the client wants to fetch as a pseudo-field.

I don't mind the performance hit of re-running the function query as a pseudo-field (this is what Solr does). In terms of Elasticsearch this means making the function query available in a `script_field`. As you say it would be quite easy to make the built-in functions available in scripts.

However from the point of view of the client code, this would mean having to generate both the JSON for a function query, e.g.:

``` json
"linear": {
  "some_field": {
    "scale": "10",
    "offset": "11",
    "origin": "12"
  }
}
```

and for the target scripting language, e.g. in Groovy this could be something like:

``` groovy
linear('some_field', [scale: 10, offset: 11, origin: 12])
```

That's why I suggested to support JSON in `script_fields`, because it would be the exact same JSON for `function_score` and for `script_fields`. Plus, JSON is much easier to generate than Groovy.

I think it would be perfect if we could access any function query (including any custom function registered from a plugin) like this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reproduction printer should add rest-spec-api to the project list if a rest test fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13468</link><project id="" key="" /><description>Right now when a rest test fails the reproduction info looks like:

```
mvn install -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=3D7BB5990DBFD568 -Dtests.class=org.elasticsearch.test.rest.Rest6IT -Dtests.method="test {yaml=cat.shards/10_basic/Help}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=sq_AL -Dtests.timezone=Africa/Algiers
```

If you want to go fix the yaml file and rerun the test you'll actually need:

```
mvn install -Pdev -Dskip.unit.tests -pl rest-api-spec,org.elasticsearch:elasticsearch -Dtests.seed=3D7BB5990DBFD568 -Dtests.class=org.elasticsearch.test.rest.Rest6IT -Dtests.method="test {yaml=cat.shards/10_basic/Help}" -Des.logger.level=ERROR -Dtests.assertion.disabled=false -Dtests.security.manager=true -Dtests.heap.size=512m -Dtests.locale=sq_AL -Dtests.timezone=Africa/Algiers
```

Note the rest-api-spec in the project list.

I think we should add the `rest-api-spec` to the project list for rest tests to make that common use case easier. Rebuilding the rest-api-spec project is quick so even if you aren't using that use case you aren't out much time.
</description><key id="105807211">13468</key><summary>Reproduction printer should add rest-spec-api to the project list if a rest test fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>test</label></labels><created>2015-09-10T13:34:33Z</created><updated>2016-01-28T17:04:12Z</updated><resolved>2016-01-28T17:04:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-10T13:41:45Z" id="139237523">++ it makes sense to me
</comment><comment author="clintongormley" created="2016-01-28T17:04:12Z" id="176284509">apparently this has been fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[test] security manager problem with python plugin on windows</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13467</link><project id="" key="" /><description>`PythonScriptEngineTests` fail with below error on Windows since we switched security manager on per default.

```
 2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.plugin:lang-python -Dtests.seed=1D11CE3B5E4534A6 -Dtests.class=org.elasticsearch.script.python.PythonScriptEngineTests -Dtests.method="testSimpleEquation" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=mk -Dtests.timezone=Asia/Dhaka
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.plugin:lang-python -Dtests.seed=1D11CE3B5E4534A6 -Dtests.class=org.elasticsearch.script.python.PythonScriptEngineTests -Dtests.method="testSimpleEquation" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=mk -Dtests.timezone=Asia/Dhaka
ERROR   1.47s | PythonScriptEngineTests.testSimpleEquation &lt;&lt;&lt;
   &gt; Throwable #1: java.security.AccessControlException: access denied ("java.io.FilePermission" "C:\Users\Administrator\.m2\repository\org\python\jython-standalone\2.7.0" "read")
   &gt;    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:457)
   &gt;    at java.security.AccessController.checkPermission(AccessController.java:884)
   &gt;    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
   &gt;    at java.lang.SecurityManager.checkRead(SecurityManager.java:888)
   &gt;    at java.io.File.exists(File.java:814)
   &gt;    at java.io.WinNTFileSystem.canonicalize(WinNTFileSystem.java:434)
   &gt;    at java.io.File.getCanonicalPath(File.java:618)
   &gt;    at org.python.core.PySystemState.findRoot(PySystemState.java:750)
   &gt;    at org.python.core.PySystemState.initRegistry(PySystemState.java:782)
   &gt;    at org.python.core.PySystemState.doInitialize(PySystemState.java:1045)
   &gt;    at org.python.core.PySystemState.initialize(PySystemState.java:974)
   &gt;    at org.python.core.PySystemState.initialize(PySystemState.java:930)
   &gt;    at org.python.core.PySystemState.initialize(PySystemState.java:925)
   &gt;    at org.python.core.PySystemState.initialize(PySystemState.java:920)
   &gt;    at org.python.core.PySystemState.initialize(PySystemState.java:916)
   &gt;    at org.python.core.ThreadStateMapping.getThreadState(ThreadStateMapping.java:32)
   &gt;    at org.python.core.Py.getThreadState(Py.java:1440)
   &gt;    at org.python.core.Py.getThreadState(Py.java:1436)
   &gt;    at org.python.core.Py.getSystemState(Py.java:1456)
   &gt;    at org.python.util.PythonInterpreter.&lt;init&gt;(PythonInterpreter.java:105)
   &gt;    at org.python.util.PythonInterpreter.threadLocalStateInterpreter(PythonInterpreter.java:81)
   &gt;    at org.elasticsearch.script.python.PythonScriptEngineService.&lt;init&gt;(PythonScriptEngineService.java:57)
   &gt;    at org.elasticsearch.script.python.PythonScriptEngineTests.setup(PythonScriptEngineTests.java:48)
   &gt;    at java.lang.Thread.run(Thread.java:745)Throwable #2: java.lang.NullPointerException
   &gt;    at org.elasticsearch.script.python.PythonScriptEngineTests.close(PythonScriptEngineTests.java:56)
   &gt;    at java.lang.Thread.run(Thread.java:745)
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.plugin:lang-python -Dtests.seed=1D11CE3B5E4534A6 -Dtests.class=org.elasticsearch.script.python.PythonScriptEngineTests -Dtests.method="testObjectMapInter" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=mk -Dtests.timezone=Asia/Dhaka
  2&gt; REPRODUCE WITH: mvn test -Pdev -pl org.elasticsearch.plugin:lang-python -Dtests.seed=1D11CE3B5E4534A6 -Dtests.class=org.elasticsearch.script.python.PythonScriptEngineTests -Dtests.method="testObjectMapInter" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.client.ratio=0.0 -Dtests.heap.size=512m -Dtests.jvm.argline="-server -XX:+UseG1GC -XX:+UseCompressedOops -XX:+AggressiveOpts -Djava.net.preferIPv4Stack=true" -Dtests.locale=mk -Dtests.timezone=Asia/Dhaka
ERROR   0.06s | PythonScriptEngineTests.testObjectMapInter &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.NullPointerException
   &gt;    at org.python.core.imp.load(imp.java:713)
   &gt;    at org.python.core.Py.importSiteIfSelected(Py.java:1558)
   &gt;    at org.python.util.PythonInterpreter.&lt;init&gt;(PythonInterpreter.java:116)
   &gt;    at org.python.util.PythonInterpreter.threadLocalStateInterpreter( don't work wellPythonInterpreter.java:81)
   &gt;    at org.elasticsearch.script.python.PythonScriptEngineService.&lt;init&gt;(PythonScriptEngineService.java:57)
   &gt;    at org.elasticsearch.script.python.PythonScriptEngineTests.setup(PythonScriptEngineTests.java:48)
   &gt;    at java.lang.Thread.run(Thread.java:745)Throwable #2: java.lang.NullPointerException
   &gt;    at org.elasticsearch.script.python.PythonScriptEngineTests.close(PythonScriptEngineTests.java:56)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```

Build url:

http://build-us-00.elastic.co/job/es_core_master_window-2008/2201/consoleText
</description><key id="105805307">13467</key><summary>[test] security manager problem with python plugin on windows</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Plugin Lang Python</label><label>test</label></labels><created>2015-09-10T13:24:02Z</created><updated>2016-01-28T17:02:17Z</updated><resolved>2016-01-28T17:02:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-10T13:46:10Z" id="139238499">Why was this module not running with security manager before on windows? Is this some bug in the maven build? Some bug in jenkins configuration?

The security manager default change was intended to only impact IDEs.
</comment><comment author="rmuir" created="2015-09-10T13:50:14Z" id="139239996">Separate from that, I knew that python did this, but it only impacts tests for us. We already have a hack for it here (looks like this needs additional permissions to work on windows):

https://github.com/elastic/elasticsearch/blob/master/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java#L96-L101

But I really want to know why jenkins on windows wasn't running python with security manager before.
</comment><comment author="rmuir" created="2015-09-10T14:11:57Z" id="139246463">I don't see anything in this python build.xml that would cause this. Looks like tests were never running with securitymanager at all under windows for some maven/jenkins reason. There might be a long tail of things to fix now.
</comment><comment author="clintongormley" created="2016-01-28T17:02:17Z" id="176283912">Windows build are now using the security manager. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor of GeoShapeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13466</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217

PR goes against the query-refactoring branch
</description><key id="105801013">13466</key><summary>Refactor of GeoShapeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-10T12:57:57Z</created><updated>2015-09-11T18:38:33Z</updated><resolved>2015-09-11T08:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-10T13:29:50Z" id="139234642">I did a first round of review... looks awesome already
</comment><comment author="colings86" created="2015-09-10T14:16:04Z" id="139248008">@s1monw I pushed a commit which should address most of your comments and replied to the others
</comment><comment author="colings86" created="2015-09-10T14:18:35Z" id="139248713">@s1monw actually hold off reviewing again for now, looks like there are test failures I need to sort out
</comment><comment author="colings86" created="2015-09-10T14:23:35Z" id="139252826">@s1monw ok good to go now :)
</comment><comment author="colings86" created="2015-09-11T08:13:32Z" id="139480213">@s1monw @javanna pushed more commits to address your comments including one to serialize ShapeRelation and SpatialStrategy using the ordinal value, with a Unit test class for each
</comment><comment author="s1monw" created="2015-09-11T08:19:21Z" id="139481276">LGTM thanks man!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't be lenient if JarHell is found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13465</link><project id="" key="" /><description>Closes #13404
</description><key id="105795594">13465</key><summary>Don't be lenient if JarHell is found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T12:26:21Z</created><updated>2015-09-30T03:11:12Z</updated><resolved>2015-09-21T07:53:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-10T12:26:54Z" id="139219835">running this patch with IntelliJ 14 passes without a jarhell. I think 13 is still broken but hey, we have to move on...
</comment><comment author="jpountz" created="2015-09-10T14:30:58Z" id="139262771">LGTM. Great to remove exceptions.
</comment><comment author="dadoonet" created="2015-09-21T15:28:16Z" id="142015342">@s1monw @rmuir When I run RestIT tests for plugins from IntelliJ I'm getting JarHell issue.
Is it expected?
Should I change something in the IDE?

Running `jvm-example` plugin tests for example gives me:

```
/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -ea -Didea.launcher.port=7532 "-Didea.launcher.bin.path=/Applications/IntelliJ IDEA 14.app/Contents/bin" -Dfile.encoding=UTF-8 -classpath "/Applications/IntelliJ IDEA 14.app/Contents/lib/idea_rt.jar:/Applications/IntelliJ IDEA 14.app/Contents/plugins/junit/lib/junit-rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/plugins/jvm-example/target/test-classes:/Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/plugins/jvm-example/target/classes:/Users/dpilato/.m2/repository/org/hamcrest/hamcrest-all/1.3/hamcrest-all-1.3.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-test-framework/5.4.0-snapshot-1702855/lucene-test-framework-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-codecs/5.4.0-snapshot-1702855/lucene-codecs-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/com/carrotsearch/randomizedtesting/randomizedtesting-runner/2.2.0-snapshot-pr202/randomizedtesting-runner-2.2.0-snapshot-pr202.jar:/Users/dpilato/.m2/repository/junit/junit/4.11/junit-4.11.jar:/Users/dpilato/.m2/repository/org/apache/ant/ant/1.8.2/ant-1.8.2.jar:/Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/core/target/test-classes:/Users/dpilato/.m2/repository/org/hdrhistogram/HdrHistogram/2.1.6/HdrHistogram-2.1.6.jar:/Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/core/target/classes:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-core/5.4.0-snapshot-1702855/lucene-core-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-backward-codecs/5.4.0-snapshot-1702855/lucene-backward-codecs-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-analyzers-common/5.4.0-snapshot-1702855/lucene-analyzers-common-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-queries/5.4.0-snapshot-1702855/lucene-queries-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-memory/5.4.0-snapshot-1702855/lucene-memory-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-highlighter/5.4.0-snapshot-1702855/lucene-highlighter-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-queryparser/5.4.0-snapshot-1702855/lucene-queryparser-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-sandbox/5.4.0-snapshot-1702855/lucene-sandbox-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-suggest/5.4.0-snapshot-1702855/lucene-suggest-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-misc/5.4.0-snapshot-1702855/lucene-misc-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-join/5.4.0-snapshot-1702855/lucene-join-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-grouping/5.4.0-snapshot-1702855/lucene-grouping-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-spatial/5.4.0-snapshot-1702855/lucene-spatial-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-spatial3d/5.4.0-snapshot-1702855/lucene-spatial3d-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/apache/lucene/lucene-expressions/5.4.0-snapshot-1702855/lucene-expressions-5.4.0-snapshot-1702855.jar:/Users/dpilato/.m2/repository/org/antlr/antlr4-runtime/4.5/antlr4-runtime-4.5.jar:/Users/dpilato/.m2/repository/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar:/Users/dpilato/.m2/repository/org/ow2/asm/asm-commons/5.0.4/asm-commons-5.0.4.jar:/Users/dpilato/.m2/repository/com/spatial4j/spatial4j/0.4.1/spatial4j-0.4.1.jar:/Users/dpilato/.m2/repository/com/vividsolutions/jts/1.13/jts-1.13.jar:/Users/dpilato/.m2/repository/com/github/spullara/mustache/java/compiler/0.8.13/compiler-0.8.13.jar:/Users/dpilato/.m2/repository/com/google/guava/guava/18.0/guava-18.0.jar:/Users/dpilato/.m2/repository/com/carrotsearch/hppc/0.7.1/hppc-0.7.1.jar:/Users/dpilato/.m2/repository/joda-time/joda-time/2.8.2/joda-time-2.8.2.jar:/Users/dpilato/.m2/repository/org/joda/joda-convert/1.2/joda-convert-1.2.jar:/Users/dpilato/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.5.3/jackson-core-2.5.3.jar:/Users/dpilato/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-smile/2.5.3/jackson-dataformat-smile-2.5.3.jar:/Users/dpilato/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-yaml/2.5.3/jackson-dataformat-yaml-2.5.3.jar:/Users/dpilato/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.5.3/jackson-databind-2.5.3.jar:/Users/dpilato/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.5.0/jackson-annotations-2.5.0.jar:/Users/dpilato/.m2/repository/org/yaml/snakeyaml/1.12/snakeyaml-1.12.jar:/Users/dpilato/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.5.3/jackson-dataformat-cbor-2.5.3.jar:/Users/dpilato/.m2/repository/io/netty/netty/3.10.3.Final/netty-3.10.3.Final.jar:/Users/dpilato/.m2/repository/com/ning/compress-lzf/1.0.2/compress-lzf-1.0.2.jar:/Users/dpilato/.m2/repository/com/tdunning/t-digest/3.0/t-digest-3.0.jar:/Users/dpilato/.m2/repository/commons-cli/commons-cli/1.3.1/commons-cli-1.3.1.jar:/Users/dpilato/.m2/repository/org/codehaus/groovy/groovy-all/2.4.4/groovy-all-2.4.4-indy.jar:/Users/dpilato/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/dpilato/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/Users/dpilato/.m2/repository/org/slf4j/slf4j-api/1.6.2/slf4j-api-1.6.2.jar:/Users/dpilato/.m2/repository/net/java/dev/jna/jna/4.1.0/jna-4.1.0.jar:/Users/dpilato/.m2/repository/org/apache/httpcomponents/httpclient/4.3.6/httpclient-4.3.6.jar:/Users/dpilato/.m2/repository/org/apache/httpcomponents/httpcore/4.3.3/httpcore-4.3.3.jar:/Users/dpilato/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/dpilato/.m2/repository/commons-codec/commons-codec/1.6/commons-codec-1.6.jar" com.intellij.rt.execution.application.AppMain com.intellij.rt.execution.junit.JUnitStarter -ideVersion5 org.elasticsearch.plugin.example.JvmExampleRestIT
[2015-09-21 17:24:49,039][WARN ][org.elasticsearch.bootstrap] Unable to lock JVM Memory: error=78,reason=Function not implemented
[2015-09-21 17:24:49,041][WARN ][org.elasticsearch.bootstrap] This can result in part of the JVM being swapped out.

java.lang.RuntimeException: Error collecting parameters from: public static java.lang.Iterable org.elasticsearch.plugin.example.JvmExampleRestIT.parameters() throws java.io.IOException,org.elasticsearch.test.rest.parser.RestTestParseException
    at com.carrotsearch.randomizedtesting.RandomizedRunner.collectFactoryParameters(RandomizedRunner.java:1514)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.collectTestCandidates(RandomizedRunner.java:1341)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.&lt;init&gt;(RandomizedRunner.java:394)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
    at org.junit.internal.builders.AnnotatedBuilder.buildRunner(AnnotatedBuilder.java:29)
    at org.junit.internal.builders.AnnotatedBuilder.runnerForClass(AnnotatedBuilder.java:21)
    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
    at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:26)
    at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:59)
    at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:26)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:41)
    at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:212)
    at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)
Caused by: java.lang.ExceptionInInitializerError
    at org.elasticsearch.test.ESTestCase.&lt;clinit&gt;(ESTestCase.java:103)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.collectFactoryParameters(RandomizedRunner.java:1508)
    ... 20 more
Caused by: java.lang.RuntimeException: found jar hell in test classpath
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:64)
    ... 26 more
Caused by: java.lang.IllegalStateException: jar hell!
class: jdk.packager.services.UserJvmOptionsService
jar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/ant-javafx.jar
jar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/lib/packager.jar
    at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:248)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:152)
    at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:87)
    at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:62)
    ... 26 more


Process finished with exit code 255
```
</comment><comment author="s1monw" created="2015-09-21T21:28:33Z" id="142113589">I'd remove `ant-javafx.jar` from the CP which version of intelliJ are you using?
</comment><comment author="rmuir" created="2015-09-21T21:37:44Z" id="142116753">The intellij bug is more, the bug looks like it adds the whole jdk to the ordinary classpath... 
</comment><comment author="jasontedor" created="2015-09-21T22:11:11Z" id="142124441">I stripped the SDK classpath in IntelliJ down to the default `sun.boot.class.path` and I'm not seeing jar hell failures anymore. Specifically:

```
jre/lib/charsets.jar
jre/lib/jce.jar
jre/lib/jfr.jar
jre/lib/jsse.jar
jre/lib/resources.jar
jre/lib/rt.jar
```
</comment><comment author="dadoonet" created="2015-09-21T22:17:46Z" id="142125611">So I probably misconfigured my JDK8 on IntelliJ.

![project structure intellij idea aujourd hui at 00 16 49](https://cloud.githubusercontent.com/assets/274222/10006652/3e5c8222-60bf-11e5-9ede-6a3fd713c823.png)

Thanks guys. I'll fix that on my end.
</comment><comment author="rmuir" created="2015-09-30T03:11:12Z" id="144269788">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to change search_quote_analyzer value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13464</link><project id="" key="" /><description>For existing index it is not possible to change value of **search_quote_analyzer**.
**Elasticsearch 1.7.1** constantly keeps original value.
## Steps to reproduce
### 1) Create new index (with _whitespace_ analyzer)

``` javascript
PUT test
{
  "mappings": {
    "item": {
      "properties": {
        "Text": {
          "type": "string",
          "analyzer": "whitespace"
        }
      }
    }
  }
}
```
### 2) Try to change **search_analyzer** to _simple_

``` javascript
PUT test/_mapping/item
{
  "properties": {
    "Text": {
      "type": "string",
      "index_analyzer": "whitespace",
      "search_analyzer": "simple"
    }
  }
}
```
### :exclamation:Now we have the first problem, our index mapping is following

``` javascript
"mappings": {
  "item": {
      "properties": {
         "Text": {
            "type": "string",
            "index_analyzer": "whitespace",
            "search_analyzer": "simple",
            "search_quote_analyzer": "whitespace"
         }
      }
   }
}
```
### 3) Try to change **search_quote_analyzer** explicitly

``` javascript
PUT test/_mapping/item
{
  "properties": {
    "Text": {
      "type": "string",
      "index_analyzer": "whitespace",
      "search_analyzer": "simple",
      "search_quote_analyzer": "simple"
    }
  }
}
```
### :exclamation:Result is still the same

``` javascript
"mappings": {
  "item": {
      "properties": {
         "Text": {
            "type": "string",
            "index_analyzer": "whitespace",
            "search_analyzer": "simple",
            "search_quote_analyzer": "whitespace"
         }
      }
   }
}
```
</description><key id="105793313">13464</key><summary>Unable to change search_quote_analyzer value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zdeseb</reporter><labels /><created>2015-09-10T12:11:20Z</created><updated>2015-09-10T16:30:38Z</updated><resolved>2015-09-10T16:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-10T16:30:38Z" id="139302274">This should be fixed with #8871. Please try the ES 2.0 beta.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.Sets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13463</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.Sets across the codebase. This is one of many
steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="105784137">13463</key><summary>Remove and forbid use of com.google.common.collect.Sets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T11:03:17Z</created><updated>2015-09-10T15:24:53Z</updated><resolved>2015-09-10T15:22:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-10T11:36:18Z" id="139210074">LGTM. 
</comment><comment author="jasontedor" created="2015-09-10T15:24:53Z" id="139279343">Thanks for reviewing @bleskes.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index last update date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13462</link><project id="" key="" /><description>I was looking for a way to get last index modification approximate date and created a topic on discussion list: https://discuss.elastic.co/t/index-last-update-date/28838.

Can ES expose last modification date of segment files?
</description><key id="105775517">13462</key><summary>Index last update date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">prog8</reporter><labels /><created>2015-09-10T10:05:22Z</created><updated>2017-02-25T01:00:50Z</updated><resolved>2015-09-10T11:58:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-10T11:28:13Z" id="139209008">What is your use-case exactly? For instance a segment file could be very recent even if no documents were indexed if a shard was relocated.
</comment><comment author="prog8" created="2015-09-10T11:33:50Z" id="139209748">Yeah I know that this may be an issue when relocation happens. The use case is that I'd like to know if I can force optimize if the index is not active for long enough time. This is doable when I add update timestamp to each document which brings significant overhead and requires reindexing. This is also doable when I have an approximate time of last modification of index files. The other option is to expose all segments hash tags so I can compare if index changed or not (keeping old hash tags outside of ES).

Any other thought?
</comment><comment author="bleskes" created="2015-09-10T11:43:15Z" id="139211184">I'm not sure I follow exactly what you want, but we have a section on the shard stats, which exposes information about the latest lucene commit point - which will indicate if something changed as well (i.e., we ES flushed or a merge finished) . See https://github.com/elastic/elasticsearch/pull/10687
</comment><comment author="prog8" created="2015-09-10T11:45:24Z" id="139211477">Hmmm... sounds great. Didn't know about this. Thanks for information.
</comment><comment author="bleskes" created="2015-09-10T11:58:38Z" id="139213765">great. Closing then.... 
</comment><comment author="PhaedrusTheGreek" created="2017-02-17T22:48:29Z" id="280787840">I'm wondering if this should be reconsidered.  It would be very useful to have last updated in index metadata.  Especially with rollover indices in play now, curator could make use of this stat optionally as an alternative to field_data when it's not available or not reliable due to backfilling.

An additional use for this would be for folks that want to only apply some action if the index has changed in a certain time range (like copy a backup to another cluster).</comment><comment author="jasontedor" created="2017-02-18T12:18:49Z" id="280842082">&gt; It would be very useful to have last updated in index metadata.

This requires a master operation which would be entirely too expensive on every indexing request that mutates the index.

&gt; An additional use for this would be for folks that want to only apply some action if the index has changed in a certain time range (like copy a backup to another cluster).

Snapshots are incremental which means they play nicely with taking periodic snapshots. If an index has not changed since the last snapshot, no new data will be transferred.</comment><comment author="clintongormley" created="2017-02-20T13:57:32Z" id="281085690">&gt; This requires a master operation which would be entirely too expensive on every indexing request that mutates the index.

This could be a value that is held per shard, and only reduced to a global max when the request is made, no?</comment><comment author="bleskes" created="2017-02-20T16:17:01Z" id="281120646">We already track whether a shard is active or not (shards become inactive after 5m of no writes). For that we track the nano time of the last write in the engine. I think we can expose those in the stats in the form of last time since write? Note that this is not 100% the same as it is reset during relocation of shards, but it might be good enough and simple to implement.


&gt; On 20 Feb 2017, at 14:57, Clinton Gormley &lt;notifications@github.com&gt; wrote:
&gt; 
&gt; This requires a master operation which would be entirely too expensive on every indexing request that mutates the index.
&gt; 
&gt; This could be a value that is held per shard, and only reduced to a global max when the request is made, no?
&gt; 
&gt; &#8212;
&gt; You are receiving this because you modified the open/close state.
&gt; Reply to this email directly, view it on GitHub, or mute the thread.
&gt; 

</comment><comment author="jasontedor" created="2017-02-20T17:08:18Z" id="281133694">&gt; For that we track the nano time of the last write in the engine.

For this we do not track absolute time, only relative time.</comment><comment author="bleskes" created="2017-02-20T18:18:38Z" id="281149025">&gt; For this we do not track absolute time, only relative time.

Yes that's what I mean with "I think we can expose those in the stats in the form of time since last write" (although I garbled that sentence with an edit).
</comment><comment author="PhaedrusTheGreek" created="2017-02-21T13:32:47Z" id="281345074">The use case that brought this on is this - "Has the index been updated in the last x minutes?, if so copy it".   </comment><comment author="jasontedor" created="2017-02-21T13:37:05Z" id="281346091">&gt; The use case that brought this on is this - "Has the index been updated in the last x minutes?, if so copy it".

Do you mean run a snapshot?

A cron trigger of running a snapshot every x minutes achieves the same.</comment><comment author="clintongormley" created="2017-02-21T13:59:10Z" id="281351426">@jasontedor there are other use cases, eg if index A, B, or C have changed since $last_built, then rebuild the composite index X</comment><comment author="jasontedor" created="2017-02-21T14:10:15Z" id="281354224">&gt; there are other use cases

@clintongormley Yes, I completely agree. &#128516;

It's important that we understand what those are so that we know whether or not they can be solved already today (like snapshotting), whether or not we need to provide a solution that is robust in the face of relocations, and whether or not we need to provide a solution that uses an absolute clock versus a relative clock.</comment><comment author="PhaedrusTheGreek" created="2017-02-21T14:46:50Z" id="281364409">The use case is for a "near-realtime sync" between 2 indices in different clusters.  I guess this will be solved eventually with changelog, but for now, we could run snapshot/restore every single minute, but that seems inefficient.  It would be better if we could say "Something has changed, run the snapshot/restore process"</comment><comment author="jasontedor" created="2017-02-22T19:51:38Z" id="281782868">@PhaedrusTheGreek For that use case, a solution that is not robust in the face of relocations will not work. If an indexing request hits, then the shard is relocated, and no future indexing requests arrive, the shard will not be seen as changed.</comment><comment author="animageofmine" created="2017-02-24T23:38:56Z" id="282435013">@jasontedor 

Another use case: 

"If the index has update since last time, run the client query again otherwise return the results back from cache"</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow connection to GCE instances from outside GCE platform</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13461</link><project id="" key="" /><description>_From @dadoonet on January 28, 2014 21:17_

Until now, we can only connect and discover nodes when running inside GCE platform.
It means no authentification or credentials are required.

We want to be able to support as well node detection from outside GCE platform. Which means probably that users will need to open 9300 port in GCE firewall.

_Copied from original issue: elastic/elasticsearch-cloud-gce#10_
</description><key id="105771384">13461</key><summary>Allow connection to GCE instances from outside GCE platform</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Discovery GCE</label><label>adoptme</label><label>feature</label></labels><created>2015-09-10T09:44:00Z</created><updated>2016-07-21T14:41:01Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[gce] Back-Off From Retries On GCE Error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13460</link><project id="" key="" /><description>_From @npilon on November 1, 2013 18:28_

Is it possible for the GCE discovery plugin to apply some kind of back-off in the case of a fundamental error, IE, having the wrong zone configured?

_Copied from original issue: elastic/elasticsearch-cloud-gce#6_
</description><key id="105771168">13460</key><summary>[gce] Back-Off From Retries On GCE Error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>adoptme</label><label>enhancement</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-10T09:43:20Z</created><updated>2015-10-21T07:58:46Z</updated><resolved>2015-10-19T17:41:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-10T09:43:20Z" id="139185558">_From @tlrx on October 24, 2014 13:0_

Wow, please apologize for the delayed reply.

Do you still need such feature? Have you tried to configure multiple zones?
</comment><comment author="dadoonet" created="2015-09-10T09:43:20Z" id="139185559">_From @npilon on October 24, 2014 18:41_

&gt; Do you still need such feature?

We haven't run into this problem in a while, but it would certainly be _nice_ if a misconfiguration didn't run a project out of API quota, yes.

&gt; Have you tried to configure multiple zones?

No; all our servers are in one GCE zone.
</comment><comment author="dadoonet" created="2015-09-10T09:46:43Z" id="139188139">Note that there is a pending PR which might be useful to port here: https://github.com/elastic/elasticsearch-cloud-gce/pull/48

A test is missing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[gce] instances don't see each other</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13459</link><project id="" key="" /><description>_From @yaraju on July 8, 2015 12:31_

I'm trying out the GCE plugin on ES 1.6.0 with plugin version 2.6.0.

I'm unable to get multicast autodiscovery to work at all.

Here is my [startup script for the instances](https://storage.googleapis.com/prefab-pheonix-365.appspot.com/setup_es_1_6_noauth.sh). (Includes elasticsearch.yml config)

And here is the log info when I switch discovery logging to "TRACE":
(NOTE: Logs are of 2nd run where I made a fresh GCloud project with ID: **es-cloud1000**)

```
[2015-07-08 12:13:45,017][INFO ][node                     ] [Jonathan "John" Garrett] version[1.6.0], pid[8724], bu
ild[cdd3ac4/2015-06-09T13:36:34Z]
[2015-07-08 12:13:45,018][INFO ][node                     ] [Jonathan "John" Garrett] initializing ...
[2015-07-08 12:13:45,055][INFO ][plugins                  ] [Jonathan "John" Garrett] loaded [marvel, cloud-gce], sites [marvel, head]
[2015-07-08 12:13:45,113][INFO ][env                      ] [Jonathan "John" Garrett] using [1] data paths, mounts [[/ (/dev/sda1)]], net usable_space [7.8gb], net total_space [9.8gb], types [ext4]
[2015-07-08 12:13:48,138][DEBUG][discovery.zen.elect      ] [Jonathan "John" Garrett] using minimum_master_nodes [-1]
[2015-07-08 12:13:48,144][DEBUG][discovery.zen.ping.multicast] [Jonathan "John" Garrett] using group [224.2.2.4], with port [54328], ttl [3], and address [null]
[2015-07-08 12:13:48,148][DEBUG][discovery.zen.ping.unicast] [Jonathan "John" Garrett] using initial hosts [], with concurrent_connects [10]
[2015-07-08 12:13:48,149][DEBUG][discovery.gce            ] [Jonathan "John" Garrett] using ping.timeout [3s], join.timeout [1m], master_election.filter_client [true], master_election.filter_data [false]
[2015-07-08 12:13:48,151][DEBUG][discovery.zen.fd         ] [Jonathan "John" Garrett] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2015-07-08 12:13:48,153][DEBUG][discovery.zen.fd         ] [Jonathan "John" Garrett] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2015-07-08 12:13:49,196][INFO ][node                     ] [Jonathan "John" Garrett] initialized
[2015-07-08 12:13:49,196][INFO ][node                     ] [Jonathan "John" Garrett] starting ...
[2015-07-08 12:13:49,276][INFO ][transport                ] [Jonathan "John" Garrett] bound_address {inet[/0.0.0.0:
9300]}, publish_address {inet[/10.240.180.235:9300]}
[2015-07-08 12:13:49,295][INFO ][discovery                ] [Jonathan "John" Garrett] dummy/jF69Ngu7Tyu4nYCcsec81g
[2015-07-08 12:13:49,299][TRACE][discovery.gce            ] [Jonathan "John" Garrett] starting to ping
[2015-07-08 12:13:49,308][TRACE][discovery.zen.ping.multicast] [Jonathan "John" Garrett] [1] sending ping request
[2015-07-08 12:13:49,312][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] connecting to [Jonathan 
"John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]
[2015-07-08 12:13:49,344][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] connected to [Jonathan "
John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]
[2015-07-08 12:13:49,345][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] sending to [Jonathan "Jo
hn" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]
[2015-07-08 12:13:49,372][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] received response from [
Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]: [pi
ng_response{node [[Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.
180.235:9300]]], id[1], master [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[Jonathan "
John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]], id[2], maste
r [null], hasJoinedOnce [false], cluster_name[dummy]}]
[2015-07-08 12:13:50,810][TRACE][discovery.zen.ping.multicast] [Jonathan "John" Garrett] [1] sending ping request
[2015-07-08 12:13:50,812][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] sending to [Jonathan "Jo
hn" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]
[2015-07-08 12:13:50,819][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] received response from [
Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]: [pi
ng_response{node [[Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.
180.235:9300]]], id[1], master [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[Jonathan "
John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]], id[3], maste
r [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[Jonathan "John" Garrett][jF69Ngu7Tyu4nY
Ccsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]], id[4], master [null], hasJoinedOnce [fals
e], cluster_name[dummy]}]
[2015-07-08 12:13:52,313][TRACE][discovery.zen.ping.multicast] [Jonathan "John" Garrett] [1] sending last pings
[2015-07-08 12:13:52,314][TRACE][discovery.zen.ping.multicast] [Jonathan "John" Garrett] [1] sending ping request
[2015-07-08 12:13:52,321][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] sending to [Jonathan "Jo
hn" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]
[2015-07-08 12:13:52,328][TRACE][discovery.zen.ping.unicast] [Jonathan "John" Garrett] [1] received response from [
Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]: [pi
ng_response{node [[Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.
180.235:9300]]], id[1], master [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[Jonathan "
John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]], id[3], maste
r [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[Jonathan "John" Garrett][jF69Ngu7Tyu4nY
Ccsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]]], id[5], master [null], hasJoinedOnce [fals
e], cluster_name[dummy]}, ping_response{node [[Jonathan "John" Garrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-
1000.internal][inet[/10.240.180.235:9300]]], id[6], master [null], hasJoinedOnce [false], cluster_name[dummy]}]
[2015-07-08 12:13:53,065][TRACE][discovery.gce            ] [Jonathan "John" Garrett] full ping responses: {none}
[2015-07-08 12:13:53,066][DEBUG][discovery.gce            ] [Jonathan "John" Garrett] filtered ping responses: (fil
ter_client[true], filter_data[false]) {none}
[2015-07-08 12:13:53,076][INFO ][cluster.service          ] [Jonathan "John" Garrett] new_master [Jonathan "John" G
arrett][jF69Ngu7Tyu4nYCcsec81g][es-node1.c.escloud-1000.internal][inet[/10.240.180.235:9300]], reason: zen-disco-jo
in (elected_as_master)
[2015-07-08 12:13:53,090][TRACE][discovery.gce            ] [Jonathan "John" Garrett] cluster joins counter set to 
[1] (elected as master)
[2015-07-08 12:13:53,225][INFO ][http                     ] [Jonathan "John" Garrett] bound_address {inet[/0.0.0.0:
9200]}, publish_address {inet[/10.240.180.235:9200]}
[2015-07-08 12:13:53,229][INFO ][node                     ] [Jonathan "John" Garrett] started
[2015-07-08 12:13:53,367][INFO ][gateway                  ] [Jonathan "John" Garrett] recovered [1] indices into cl
uster_state
```

Please let me know if I can provide any additional info.

_Copied from original issue: elastic/elasticsearch-cloud-gce#54_
</description><key id="105770455">13459</key><summary>[gce] instances don't see each other</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Cloud GCE</label><label>adoptme</label><label>bug</label></labels><created>2015-09-10T09:38:12Z</created><updated>2016-11-16T15:44:00Z</updated><resolved>2016-08-09T15:42:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-10T09:38:13Z" id="139184519">_From @yaraju on July 8, 2015 12:34_

Additional info:
I was running with two nodes:
es-node1
es-node2
Each of the Marvel indices expect to have 1 shard, 1 replica - but despite the 2nd machine showing up - each of them did not notice the other.
</comment><comment author="dadoonet" created="2015-09-10T09:38:13Z" id="139184521">_From @yaraju on July 8, 2015 12:44_

Also, logs from 2nd node:

```
[2015-07-08 12:14:14,507][INFO ][node                     ] [White Tiger] version[1.6.0], pid[9627], build[cdd3ac4/
2015-06-09T13:36:34Z]
[2015-07-08 12:14:14,508][INFO ][node                     ] [White Tiger] initializing ...
[2015-07-08 12:14:14,535][INFO ][plugins                  ] [White Tiger] loaded [marvel, cloud-gce], sites [marvel
, head]
[2015-07-08 12:14:14,585][INFO ][env                      ] [White Tiger] using [1] data paths, mounts [[/ (/dev/sd
a1)]], net usable_space [7.8gb], net total_space [9.8gb], types [ext4]
[2015-07-08 12:14:16,990][DEBUG][discovery.zen.elect      ] [White Tiger] using minimum_master_nodes [-1]
[2015-07-08 12:14:16,992][DEBUG][discovery.zen.ping.multicast] [White Tiger] using group [224.2.2.4], with port [54
328], ttl [3], and address [null]
[2015-07-08 12:14:16,995][DEBUG][discovery.zen.ping.unicast] [White Tiger] using initial hosts [], with concurrent_
connects [10]
[2015-07-08 12:14:16,996][DEBUG][discovery.gce            ] [White Tiger] using ping.timeout [3s], join.timeout [1m
], master_election.filter_client [true], master_election.filter_data [false]
[2015-07-08 12:14:16,998][DEBUG][discovery.zen.fd         ] [White Tiger] [master] uses ping_interval [1s], ping_ti
meout [30s], ping_retries [3]
[2015-07-08 12:14:17,000][DEBUG][discovery.zen.fd         ] [White Tiger] [node  ] uses ping_interval [1s], ping_ti
meout [30s], ping_retries [3]
[2015-07-08 12:14:17,832][INFO ][node                     ] [White Tiger] initialized
[2015-07-08 12:14:17,834][INFO ][node                     ] [White Tiger] starting ...
[2015-07-08 12:14:17,895][INFO ][transport                ] [White Tiger] bound_address {inet[/0.0.0.0:9300]}, publ
ish_address {inet[/10.240.170.154:9300]}
[2015-07-08 12:14:17,909][INFO ][discovery                ] [White Tiger] dummy/kJxYRSn_RtyEaWxBMZkvXQ
[2015-07-08 12:14:17,912][TRACE][discovery.gce            ] [White Tiger] starting to ping
[2015-07-08 12:14:17,921][TRACE][discovery.zen.ping.multicast] [White Tiger] [1] sending ping request
[2015-07-08 12:14:17,923][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] connecting to [White Tiger][kJxYRSn_
RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]
[2015-07-08 12:14:17,949][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] connected to [White Tiger][kJxYRSn_R
tyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]
[2015-07-08 12:14:17,949][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] sending to [White Tiger][kJxYRSn_Rty
EaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]
[2015-07-08 12:14:17,973][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] received response from [White Tiger]
[kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]: [ping_response{node [[White
 Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[1], master [null
], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.
c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[2], master [null], hasJoinedOnce [false], cluster_name[du
mmy]}]
[2015-07-08 12:14:19,422][TRACE][discovery.zen.ping.multicast] [White Tiger] [1] sending ping request
[2015-07-08 12:14:19,424][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] sending to [White Tiger][kJxYRSn_Rty
EaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]
[2015-07-08 12:14:19,426][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] received response from [White Tiger]
[kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]: [ping_response{node [[White
 Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[1], master [null
], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.
c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[3], master [null], hasJoinedOnce [false], cluster_name[du
mmy]}, ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170
.154:9300]]], id[4], master [null], hasJoinedOnce [false], cluster_name[dummy]}]
[2015-07-08 12:14:20,925][TRACE][discovery.zen.ping.multicast] [White Tiger] [1] sending last pings
[2015-07-08 12:14:20,925][TRACE][discovery.zen.ping.multicast] [White Tiger] [1] sending ping request
[2015-07-08 12:14:20,927][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] sending to [White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]
[2015-07-08 12:14:20,929][TRACE][discovery.zen.ping.unicast] [White Tiger] [1] received response from [White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]: [ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[1], master [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[3], master [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[5], master [null], hasJoinedOnce [false], cluster_name[dummy]}, ping_response{node [[White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]]], id[6], master [null], hasJoinedOnce [false], cluster_name[dummy]}]
[2015-07-08 12:14:21,676][TRACE][discovery.gce            ] [White Tiger] full ping responses: {none}
[2015-07-08 12:14:21,677][DEBUG][discovery.gce            ] [White Tiger] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2015-07-08 12:14:21,685][INFO ][cluster.service          ] [White Tiger] new_master [White Tiger][kJxYRSn_RtyEaWxBMZkvXQ][es-node2.c.escloud-1000.internal][inet[/10.240.170.154:9300]], reason: zen-disco-join (elected_as_master)
[2015-07-08 12:14:21,695][TRACE][discovery.gce            ] [White Tiger] cluster joins counter set to [1] (elected as master)
[2015-07-08 12:14:21,805][INFO ][http                     ] [White Tiger] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.240.170.154:9200]}
[2015-07-08 12:14:21,805][INFO ][node                     ] [White Tiger] started
[2015-07-08 12:14:21,894][INFO ][gateway                  ] [White Tiger] recovered [1] indices into cluster_state
```
</comment><comment author="dadoonet" created="2015-09-10T09:38:13Z" id="139184522">_From @yaraju on July 8, 2015 18:34_

I [rewrote my script](https://storage.googleapis.com/prefab-pheonix-365.appspot.com/setup_es_1_5_noauth.sh) to use ES 1.5.0 and plugin version 2.5.0, and that works fine.

So my script is fine, but fail with the new plugin.

I can share my Gcloud project with you if you'd like to take a closer look.
</comment><comment author="dadoonet" created="2015-09-10T09:38:14Z" id="139184524">_From @akleiman on July 16, 2015 17:3_

Seems like the same problem I had in #53
</comment><comment author="dadoonet" created="2015-09-10T09:38:14Z" id="139184526">_From @schonfeld on July 28, 2015 5:27_

This, too (see #53), is probably due to someone replacing some important code with a "TODO" comment...

https://github.com/elastic/elasticsearch-cloud-gce/blob/master/src/main/java/org/elasticsearch/discovery/gce/GceDiscovery.java#L49
</comment><comment author="dadoonet" created="2015-09-10T09:38:14Z" id="139184528">_From @danielschonfeld on July 28, 2015 5:28_

cc @schonfeld @dadoonet :+1: 
</comment><comment author="dadoonet" created="2016-08-09T15:42:24Z" id="238594828">I just ran today some tests on 5.0.0-alpha5 and discovery is working well. I think we could close this.
I'm also supposing that everything works also well in 2.x series. AFAIR the tests I did at least.
</comment><comment author="beachmang" created="2016-11-14T21:00:06Z" id="260460902">@dadoonet I just upgraded some nodes from 1.5 -&gt; 1.7.5 yesterday and I'm having the problem outlined in this thread. My (2) masters no longer see each other. 
</comment><comment author="dadoonet" created="2016-11-16T15:26:18Z" id="260974329">@beachmang Can you share your logs? Can you run that with DEBUG level so we have more clues?
</comment><comment author="beachmang" created="2016-11-16T15:30:54Z" id="260975688">@dadoonet My apologies, I read yesterday that the 2.5 version of the plugin was functioning with es 1.6 so I tried that. Happy to report it's also working with 1.7.5.
</comment><comment author="dadoonet" created="2016-11-16T15:44:00Z" id="260979570">Great. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_search_shards api spec has wrong types for its parts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13458</link><project id="" key="" /><description /><key id="105768884">13458</key><summary>_search_shards api spec has wrong types for its parts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-09-10T09:28:01Z</created><updated>2015-09-10T10:07:40Z</updated><resolved>2015-09-10T10:07:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-10T09:40:57Z" id="139184985">LGTM
</comment><comment author="Mpdreamz" created="2015-09-10T10:07:39Z" id="139193885">merged to master/2.0/2.x https://github.com/elastic/elasticsearch/commit/8e7be8b5522e11b5ea0aecc09fc5918dcd09af6d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FailedToResolveConfigException[Failed to resolve config path</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13457</link><project id="" key="" /><description>I am using official elasticsearch docker image to run my elasticsearch instance.
I am trying to create an index with some filters which require to load some files (synonym filter, stopword filter...) none of such filters works because I get errors such as this one:

``` javascript
{
   "error": "RemoteTransportException[[Hood][inet[/172.17.0.6:9300]][indices:admin/create]]; nested: IndexCreationException[[admin] failed to create index]; nested: FailedToResolveConfigException[Failed to resolve config path [/usr/share/elasticsearch/config/synonyms/sk_SK.txt], tried file path [/usr/share/elasticsearch/config/synonyms/sk_SK.txt], path file [/opt/logstash/config/usr/share/elasticsearch/config/synonyms/sk_SK.txt], and classpath]; ",
   "status": 500
}
```

this is my code to create the index:

``` javascript
POST /admin
{
   "settings": {
      "analysis": {
         "filter": {
           "synonym_filter": {
              "type": "synonym",
              "synonyms_path": "/usr/share/elasticsearch/config/synonyms/sk_SK.txt",
              "ignore_case": true
           },
           "sk_SK" : {
              "type" : "hunspell",
              "locale" : "sk_SK",
              "dedup" : true,
              "recursion_level" : 0
            },
            "nGram_filter": {
               "type": "nGram",
               "min_gram": 2,
               "max_gram": 20,
               "token_chars": [
                  "letter",
                  "digit",
                  "punctuation",
                  "symbol"
               ]
            }
         },
         "analyzer": {
            "slovencina_synonym": {
              "type": "custom",
              "tokenizer": "standard",
              "filter": [
                "lowercase",
                "synonym_filter",
                "asciifolding"
                ]
            },
            "slovencina": {
              "type": "custom",
              "tokenizer": "standard",
              "filter": [
                "lowercase",
                "asciifolding"
                ]
            },
            "nGram_analyzer": {
               "type": "custom",
               "tokenizer": "whitespace",
               "filter": [
                  "lowercase",
                  "asciifolding",
                  "nGram_filter"
               ]
            },
            "whitespace_analyzer": {
               "type": "custom",
               "tokenizer": "whitespace",
               "filter": [
                  "lowercase",
                  "asciifolding"
               ]
            }
         }
      }
   }
}
```

I was googling the error and I have read some posts that the problem is in file/folder permissions, so I tried the following:

``` shell
sudo chown -R elasticsearch:elasticsearch /usr/share/elasticsearch/config/*
chmod o+x /usr
chmod o+x /usr/share
chmod o+x /usr/share/elasticsearch
chmod o+x /usr/share/elasticsearch/config/
```

still did not work after this.
</description><key id="105767734">13457</key><summary>FailedToResolveConfigException[Failed to resolve config path</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Alino</reporter><labels /><created>2015-09-10T09:23:42Z</created><updated>2015-09-19T14:35:30Z</updated><resolved>2015-09-10T09:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-10T09:32:10Z" id="139183356">Please ask questions on discuss.elastic.co.

Check that all your nodes have access to this dir.
May be your docker instance does not have access to this file?

Closing as I don't think it's a bug here but more a mis configuration with docker. Feel free to reopen if you think it's a bug, otherwise let's talk about it on discuss.
</comment><comment author="Alino" created="2015-09-10T09:36:23Z" id="139184201">I have only single docker instance - single elasticsearch instance, and I believe this should work out of the box within official docker image, therefore I consider it as a bug.
</comment><comment author="Alino" created="2015-09-10T10:42:50Z" id="139201503">I created a thread on discuss.elastic.co
https://discuss.elastic.co/t/official-docker-image-bug-cannot-resolve-config-path/29015
</comment><comment author="clintongormley" created="2015-09-19T14:35:30Z" id="141675207">@Alino We don't provide an official docker image, at least not yet. We may do so in the future
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove ordinal member from Operator enum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13456</link><project id="" key="" /><description>Use the already available enum ordinal instead.
</description><key id="105766795">13456</key><summary>Remove ordinal member from Operator enum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Query Refactoring</label><label>enhancement</label><label>review</label></labels><created>2015-09-10T09:18:58Z</created><updated>2015-09-10T12:25:31Z</updated><resolved>2015-09-10T12:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="cbuescher" created="2015-09-10T10:30:42Z" id="139199891">LGTM, one question though. I thought the explicit ordinal was there to prevent accidental mixups in the enum order, also to make clearer that we rely on this ordering in serialization. Just want to make sure I understand this so we can apply it to other enums where necessary. 
</comment><comment author="javanna" created="2015-09-10T10:41:57Z" id="139201401">I see what you mean, I think it is safe to use, of course we shouldn't go and change the order in the enum though... I saw Simon do the same in another PR as well ;) @s1monw thoughts?
</comment><comment author="cbuescher" created="2015-09-10T10:50:18Z" id="139202463">I'm more asking for best practices for other enums where order/naming might change, e.g. if we decide to remove a member of the enum at some point, serialization breaks. That doesn't happen when we explicitly choose the value. This is not an issue here, mostly asking for future reference.
</comment><comment author="alexksikes" created="2015-09-10T11:19:02Z" id="139207767">That is true it will be harder to maintain bwc in this case, but I do much prefer the simplicity of this. The other way is to directly serialize the name of the enum if we don't care about the extra bits.
</comment><comment author="javanna" created="2015-09-10T12:00:13Z" id="139214004">Once we do run into bw comp issues with this, not likely to happen, they can be handled adding back some layer very similar to what we are removing here, taking into account the previous ordering. We don'd need it now and we most likely won't need it in the future either. That's my take. The reason why I didn't propose this from the beginning is that I didn't know we could do this, purely ignorance on my end :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Document meaning of "FST" and "FSTs".</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13455</link><project id="" key="" /><description>Explain, that `FST` and `FSTs` stand for finite state transducer. Googling for `FST` will not bring you far.
</description><key id="105766776">13455</key><summary>[docs] Document meaning of "FST" and "FSTs".</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apepper</reporter><labels><label>docs</label><label>v1.7.2</label><label>v2.0.0-beta2</label></labels><created>2015-09-10T09:18:49Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-11T09:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2015-09-10T10:03:34Z" id="139192632">Thank you @apepper, this looks great!  Could you please sign the CLA https://www.elastic.co/contributor-agreement/ so I can merge this?
</comment><comment author="apepper" created="2015-09-10T10:06:40Z" id="139193706">&gt; Could you please sign the CLA https://www.elastic.co/contributor-agreement/ so I can merge this?

Actually my company already did so about an hour ago. I'm not sure, how fast this will show up here.
</comment><comment author="mikemccand" created="2015-09-11T09:23:29Z" id="139497963">Thanks @apepper, I see the CLA ... I'll merge this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>synced flush incorrectly documented url parameters as path parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13454</link><project id="" key="" /><description /><key id="105766298">13454</key><summary>synced flush incorrectly documented url parameters as path parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2015-09-10T09:15:19Z</created><updated>2015-09-10T10:08:07Z</updated><resolved>2015-09-10T10:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-10T09:38:18Z" id="139184541">LGTM
</comment><comment author="Mpdreamz" created="2015-09-10T10:08:07Z" id="139193981">merged to master/2.0/2.x
https://github.com/elastic/elasticsearch/commit/aaf7116bfb03c74bb591c19da1c129987ab66375
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ask a node to move all its shards away</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13453</link><project id="" key="" /><description>In planned maintenance, I have to remove a node from my cluster. If I have replica at least 1 for all my indices, no problem. But without replica, it could be interesting to instruct a node that it should move away all the shards it stores. With this functionality, I just have to wait that the shards move on another node, and then I can safely remove the node.
</description><key id="105747903">13453</key><summary>ask a node to move all its shards away</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpparis-orange</reporter><labels /><created>2015-09-10T07:33:08Z</created><updated>2015-09-10T07:51:38Z</updated><resolved>2015-09-10T07:44:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-10T07:44:19Z" id="139149927">Heya,

You can use the exclude option of the [shard allocation filtering](https://www.elastic.co/guide/en/elasticsearch/reference/current/index-modules-allocation.html#shard-allocation-filtering) to move the shards away (using the node name or an IP).

I'm closing the issue as I think this covers your request. Feel free to open if it doesn't.
</comment><comment author="jpparis-orange" created="2015-09-10T07:51:38Z" id="139150975">Yeah! Will take a look at it in the next week!
I'll also have to RTFM a bit more ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update query_dsl.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13452</link><project id="" key="" /><description>Minor orthographic fix in documentation
</description><key id="105745667">13452</key><summary>Update query_dsl.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tcucchietti</reporter><labels><label>docs</label></labels><created>2015-09-10T07:19:45Z</created><updated>2015-09-19T14:10:19Z</updated><resolved>2015-09-19T14:10:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-19T14:10:11Z" id="141671809">thanks @tcucchietti - merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch node unresponsive, high active_opens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13451</link><project id="" key="" /><description>Received an alert saying that one of my nodes was down, when I tried to curl `/` it just hung.

Checked on the health of the cluster and the node and noted something very strange:

```
            "network": {
                "tcp": {
                    "active_opens": 102188,
                    "passive_opens": 7133683,
                    "curr_estab": 205,
                    "in_segs": 1483621255,
                    "out_segs": 2405602124,
                    "retrans_segs": 569006,
                    "estab_resets": 9251,
                    "attempt_fails": 3252,
                    "in_errs": 11,
                    "out_rsts": 23640
                }
            },
```

```
# sudo netstat -tupn |grep CLOSE_WAIT |  wc -l
11711
# sudo netstat -tupn  |  wc -l
11940
```

shows a ton of CLOSE_WAIT

The active opens were about 10x more on this node than any other one.  What are active opens vs passive opens, and what is the expected number of active/passive opens, and how can I make elasticsearch close these connections more aggressively?

I'm running `1.7.1` on Java 1.8

```
{
  "status" : 200,
  "name" : &lt;redacted&gt;,
  "cluster_name" : &lt;redacted&gt;,
  "version" : {
    "number" : "1.7.1",
    "build_hash" : "b88f43fc40b0bcd7f173a1f9ee2e97816de80b19",
    "build_timestamp" : "2015-07-29T09:54:16Z",
    "build_snapshot" : false,
    "lucene_version" : "4.10.4"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="105744600">13451</key><summary>Elasticsearch node unresponsive, high active_opens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sinneduy</reporter><labels /><created>2015-09-10T07:09:40Z</created><updated>2015-09-10T08:28:16Z</updated><resolved>2015-09-10T08:28:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sinneduy" created="2015-09-10T07:25:06Z" id="139137454">http://elasticsearch-users.115913.n3.nabble.com/Increasing-CLOSE-WAIT-connections-and-HTTP-current-open-metric-td4019752.html

seems to be related, but it doesn't to have a conclusive solution or understanding of what is happening
</comment><comment author="markwalkom" created="2015-09-10T08:28:16Z" id="139166002">Please join us in #elasticsearch on Freenode or at https://discuss.elastic.co/ for troubleshooting help or general questions. 

We reserve Github for confirmed bugs and feature requests :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update query_dsl.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13450</link><project id="" key="" /><description>Corrected vocabulary.
</description><key id="105740612">13450</key><summary>Update query_dsl.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ramky1982</reporter><labels><label>docs</label></labels><created>2015-09-10T06:28:28Z</created><updated>2015-12-09T19:44:14Z</updated><resolved>2015-12-09T19:43:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-10T13:09:04Z" id="139229801">Looks good to me. @ramky1982, can you sign the [CLA](https://www.elastic.co/contributor-agreement)? Its required for all pull requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Top level parent child inner_hits doesn't use correct mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13449</link><project id="" key="" /><description>If type is specified, it uses parent's mapping for inner hits:

```
DELETE test

PUT test
{
  "mappings": {
    "parent": {},
    "child": {
      "_parent": {
        "type": "parent"
      }
    }
  }
}

POST test/parent/1
{"name": "parent1"}

POST test/child/1?parent=1
{"name": "child1", "num": 1}

# Without type
# This works.
GET test/_search
{
  "query": {
    "has_child": {
      "type": "child",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "sort":"num"
      }
    }
  }
}

# With type
# This gets parse failure saying "No mapping found for [num] in order to sort on"
GET test/parent/_search
{
  "query": {
    "has_child": {
      "type": "child",
      "query": {
        "match_all": {}
      },
      "inner_hits": {
        "sort":"num"
      }
    }
  }
}
```

Looks like it's because `SubSearchContext` uses `type` for main query while parsing sub search.

Note, this happens in 1.7.1 but not 2.x thanks to great mapping refactoring.
</description><key id="105735972">13449</key><summary>Top level parent child inner_hits doesn't use correct mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">masaruh</reporter><labels><label>bug</label></labels><created>2015-09-10T05:42:08Z</created><updated>2015-09-19T12:11:18Z</updated><resolved>2015-09-14T08:36:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-14T08:36:09Z" id="140001010">@masaruh correct, this has fixed by the great mapping refactoring, because it is a mapping issue that manifest here because has_child and has_parent queries operate between types, but the search api scoped the mapping view down to a single type. This makes this query fail. Without the mapping refactoring this issue can't fixed properly, so two things can be done. Upgrade to 2.0 once it is released or don't use types in the url when using the has_child query.  Filtering by type can also be done in the query dsl instead.

There was another similar issue open for this too, but I can't find it at the moment, but also that one has been fixed by the mapping refactoring. I'm closing this issue and because it has been fixed in 2.0 and it can't really be fixed in 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>-Werror is not working with java 9 / jigsaw</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13448</link><project id="" key="" /><description>Does it work with existing java 9 builds at all? 

I get vague compilation failures from maven, with no mention of any warnings or anything, even when running with debugging enabled.

We might need to conditionally disable Werror for java 9, but it would be good to investigate why its not working and get it fixed (seems like a maven bug).
</description><key id="105733722">13448</key><summary>-Werror is not working with java 9 / jigsaw</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>build</label></labels><created>2015-09-10T05:22:59Z</created><updated>2016-05-18T13:39:49Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-10T05:24:59Z" id="139120284">Here is the useless fail you get even with mvn -X

```
[INFO] Compiling 2901 source files to /home/rmuir/workspace/elasticsearch/core/target/classes
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 4.709 s
[INFO] Finished at: 2015-09-10T01:24:16-04:00
[INFO] Final Memory: 17M/57M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project elasticsearch: Compilation failure -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.3:compile (default-compile) on project elasticsearch: Compilation failure
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:120)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:347)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:154)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:213)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:157)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(java.base@9.0/Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(java.base@9.0/NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(java.base@9.0/DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(java.base@9.0/Method.java:517)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)
Caused by: org.apache.maven.plugin.compiler.CompilationFailureException: Compilation failure
    at org.apache.maven.plugin.compiler.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:915)
    at org.apache.maven.plugin.compiler.CompilerMojo.execute(CompilerMojo.java:129)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:132)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:208)
    ... 19 more
[ERROR] 
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException

```
</comment><comment author="rjernst" created="2015-09-10T16:23:05Z" id="139300670">I tried on a java 9 ea build and got the same problem. I will conditionalize -Werror in maven based on java version until we can figure it out.
</comment><comment author="rmuir" created="2015-09-10T18:01:46Z" id="139328905">+1 for that approach. We can get java 9 ea working again and just open a bug with maven or whatever and enable it once we have a version that works.
</comment><comment author="uschindler" created="2016-05-18T13:38:12Z" id="220028294">Hi,
while doing the invokedynamic stuff for string concats, I hit this issue, too. I was finally removing the `-Werror` from the gradle build plugin.

With build 118 the situation gets worse: `Class.newInstance()` gots deprecated in Java 9, so it produces a deprecation warning, too. BUT: as we are targetting Java 8 currently the build, the code developer cannot know if a method is deprecated in Java 9. Because of that we have forbiddenapis, which checks on the deprecated list of Java 8, not the one of 9! Because forbiddenapis already does this, the @deprecated checks by javac could be disabled (at least for JDK signatures).

Personally, I don't like -Werror because of that, because you can never know if future java versions or different JVM vendors introduce as new warnings. So -Xlint:all and then exclude some is bad idea! If you want to fail on warning, be explicit on _which_ warning you want to fail, not "all" minus some, because "all" is undefined.

Alternatively only enable -Werror if the compile JVM is the one you expect.

So +1 to add `-Werror` only if the compiler Java version is 1.8 and is coming from Oracle or OpenJDK. Thats the right thing to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JarHell needs support for non-URLClassLoader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13447</link><project id="" key="" /><description>In java 9 with jigsaw, classloader is not a URLClassLoader, nor any of its parent/grandparent (aren't we glad we don't try to add plugin jars directly to it anymore), so we need to adjust JarHell's check and Security's codesource assignment. 

Basically it should just take URLs, and as far as getting URLs from the classpath, we can fallback on parsing `java.class.path` depending on the loader type.

So maybe some helper method like:

```
static URL[] getClasspathURLs(ClassLoader loader)
```

We can expose the two approaches via static methods and heavily test in jenkins this way. We can even compare them against each other and so on when both methods will work. 
</description><key id="105733626">13447</key><summary>JarHell needs support for non-URLClassLoader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>non-issue</label></labels><created>2015-09-10T05:21:09Z</created><updated>2016-04-05T13:58:35Z</updated><resolved>2015-09-14T08:22:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-14T08:22:23Z" id="139996298">Fixed via #13439
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds links to Logstash plugins under the Integrations page.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13446</link><project id="" key="" /><description /><key id="105711816">13446</key><summary>Adds links to Logstash plugins under the Integrations page.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">palecur</reporter><labels><label>docs</label></labels><created>2015-09-10T01:01:00Z</created><updated>2015-09-10T21:36:23Z</updated><resolved>2015-09-10T21:36:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markwalkom" created="2015-09-10T01:31:14Z" id="139089571">LGTM :D
</comment><comment author="dadoonet" created="2015-09-10T21:29:31Z" id="139384893">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better logging for when license plugin is missing from a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13445</link><project id="" key="" /><description>Because the license plugin updates the cluster state, any node without the license plugin fails to join the cluster. However because the default logging is INFO the messages about failing to join the cluster state don't show up on either the master node or the node without the license. On the client node which was trying to join all I saw was after setting discovery to TRACE:

[2015-09-09 16:43:36,224][DEBUG][discovery.zen ] [phl-head-03] no master node is set, despite of join request completing. retrying pings.

There should be more information logged at INFO without having to enable DEBUG logging at both the master and the node which fails to join the cluster due to the missing license plugin. This was on Elasticsearch 1.7.1.
</description><key id="105709673">13445</key><summary>Better logging for when license plugin is missing from a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">msimos</reporter><labels><label>:Cluster</label><label>discuss</label></labels><created>2015-09-10T00:33:29Z</created><updated>2016-05-12T13:42:50Z</updated><resolved>2015-12-18T16:34:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="craigbro" created="2015-09-10T20:14:06Z" id="139366910">If it was going to NOT sync and join the cluster because of the missing plugin, I would expect it to error outright and die, or log it at as big frigging ERROR level -- "hey, I can't sync state because I don't have a plugin the rest of the cluster has." This is not an INFO level fail, this is a "stop the world I want to get off" level ERROR.
</comment><comment author="clintongormley" created="2015-09-19T15:13:33Z" id="141678822">@areek can you take a look at this please?
</comment><comment author="ppf2" created="2015-10-08T05:25:08Z" id="146423642">+1, this affects all clusters that are running plugins that require the license module and if the connection is made via a NodeClient (which includes both Java NodeClient clients as well as the _tribe node_).

On the tribe node scenario, we are currently logging the important details only at the DEBUG level on the cluster the tribe node is attempting to join to.  Such exceptions (when the tribe node does not have the license plugin installed properly) should be logged at the ERROR or WARN level :)

```
[2015-10-07 22:17:00,133][DEBUG][cluster.service          ] ack received from node [[tribe_node_1/t1][3jIcB_yJSFGxI_w_NDahMg][Iceman][inet[/192.168.1.127:9302]]{client=true, data=false}], cluster_state update (version: 21)
org.elasticsearch.transport.RemoteTransportException: [tribe_node_1/t1][inet[/192.168.1.127:9302]][internal:discovery/zen/publish]
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No custom index metadata factory registered for type [licenses]
    at org.elasticsearch.cluster.metadata.MetaData.lookupFactorySafe(MetaData.java:130)
    at org.elasticsearch.cluster.metadata.MetaData$Builder.readFrom(MetaData.java:1467)
    at org.elasticsearch.cluster.ClusterState$Builder.readFrom(ClusterState.java:681)
    at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$PublishClusterStateRequestHandler.messageReceived(PublishClusterStateAction.java:192)
    at org.elasticsearch.discovery.zen.publish.PublishClusterStateAction$PublishClusterStateRequestHandler.messageReceived(PublishClusterStateAction.java:175)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:222)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:114)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:337)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="tarass" created="2015-11-08T23:10:20Z" id="154885449">This somehow wasn't affecting us prior to 2.0, but now Java clients can't join cluster as a full client Node because they don't have a license plugin. The worst part is that this was a "silent" behavior.
</comment><comment author="PhaedrusTheGreek" created="2015-12-02T15:19:36Z" id="161329906">+1

Java Node client joins the cluster, and the cluster even reports that the client joined and all seems well:

```
host      ip        heap.percent ram.percent load node.role master name                 
127.0.0.1 127.0.0.1            2         100 2.87 c         -      Some Java Client     
127.0.0.1 127.0.0.1            3         100 2.87 d         *      My ES Instance 
```

If you then execute a search, you get:

```
ClusterBlockException[blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];]
... stack trace ...
```

Or attempt to Index data, you get:

```
MasterNotDiscoveredException[waited for [1m]]
... stack trace ...
```
</comment><comment author="PhaedrusTheGreek" created="2015-12-02T15:20:52Z" id="161330288">Related discussion on this issue from Logstash:
https://discuss.elastic.co/t/receiving-logstash-error-after-installing-watcher-service-unavailable-1-state-not-recovered-initialized-service-unavailable-2-no-master/32721/6
</comment><comment author="PhaedrusTheGreek" created="2015-12-02T17:12:05Z" id="161367993">To enable the License Plugin in the Java Client for version 2.0.0:

```
&lt;dependency&gt;
         &lt;groupId&gt;org.elasticsearch.plugin&lt;/groupId&gt;
         &lt;artifactId&gt;license&lt;/artifactId&gt;
         &lt;version&gt;2.0.0&lt;/version&gt;
&lt;/dependency&gt; 

```

and

```
Node node = nodeBuilder()
    .settings(Settings.settingsBuilder()
         .put("node.name", "Some Java Client")
         .put("path.home", "/tmp/")
         .put("plugin.types", LicensePlugin.class.getName())
        )
    .client(true)
    .node();
```
</comment><comment author="curu" created="2015-12-11T08:25:31Z" id="163873150">the document of license plugin(or marvel install doc) should explicitly point out the possible problem may arise.and give user a solution.

and the java api should also point this out.
</comment><comment author="gmoskovicz" created="2015-12-21T19:10:05Z" id="166393123">+1 better logging will help
</comment><comment author="shavo007" created="2016-05-08T09:16:02Z" id="217705099">I tried the approach above and still fails locally!! How do i resolve this?
</comment><comment author="PhaedrusTheGreek" created="2016-05-12T13:42:50Z" id="218760855">@shavo007 , please post your question with configuration examples at https://discuss.elastic.co/, as that will be a better place to expand the troubleshooting discussion.   Please post a link to the discussion back on this issue for reference.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove all accessClassInPackage permissions for java 9 compatibility</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13444</link><project id="" key="" /><description>With jigsaw this stuff is not really an option anymore of security manager or not (try a build: http://jdk9.java.net/jigsaw)

We have quite a few of these in master, they all need to be removed, so things work on java 9:

---

```
  // needed by aws core sdk (TODO: look into this)
  permission java.lang.RuntimePermission "accessClassInPackage.sun.security.ssl";
```

~~I opened a PR for this (https://github.com/aws/aws-sdk-java/pull/432) but they have gone silent. Maybe they do not know, aws-sdk will not work on java 9 if it tries to do this (and I don't see why they should be doing it at all).~~

Removed.

---

```
  // needed by groovy engine
  permission java.lang.RuntimePermission "accessClassInPackage.sun.reflect";
```

This one needs looking into, I don't know why groovy needs this.

---

```
  // needed for mock filesystems in tests (to capture implCloseChannel)
  permission java.lang.RuntimePermission "accessClassInPackage.sun.nio.ch";
```

~~This one becomes more fine-grained to just the lucene test-framework jar in https://github.com/elastic/elasticsearch/pull/13439. Additionally I raised a thread on nio-dev about it: http://mail.openjdk.java.net/pipermail/nio-dev/2015-September/003321.html~~

Removed.

---

```
grant codeBase "${es.security.jar.lucene.core}" {
  // needed to allow MMapDirectory's "unmap hack"
  permission java.lang.RuntimePermission "accessClassInPackage.sun.misc";
};
```

This one is currently being discussed on the jdk lists: http://mail.openjdk.java.net/pipermail/core-libs-dev/2015-September/035055.html

However if its not available, lucene will fall back to not unmapping, so things will still "work", you just have disk usage, address space, etc tied to the garbage collector.
</description><key id="105706384">13444</key><summary>remove all accessClassInPackage permissions for java 9 compatibility</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>bug</label><label>v6.0.0</label></labels><created>2015-09-09T23:57:13Z</created><updated>2017-05-03T06:55:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-10T01:19:53Z" id="139088065">sun.nio.ch can be removed completely after we upgrade lucene again.
</comment><comment author="rmuir" created="2015-09-10T03:35:09Z" id="139106749">lucene tests are passing with the jigsaw EA now: https://issues.apache.org/jira/browse/LUCENE-6795

i will bump the lucene version in https://github.com/elastic/elasticsearch/pull/13439 and see how ES looks.
</comment><comment author="rmuir" created="2015-09-10T03:59:17Z" id="139110240">The answer is that nothing compiles, all tests fail, and ES won't start, of course :)
</comment><comment author="rmuir" created="2015-09-10T16:30:18Z" id="139302204">@uschindler opened a bug at groovy that might be related to our issues: https://issues.apache.org/jira/browse/GROOVY-7587
</comment><comment author="uschindler" created="2015-09-10T16:30:48Z" id="139302309">&gt; However if its not available, lucene will fall back to not unmapping, so things will still "work", you just have disk usage, address space, etc tied to the garbage collector.

The Lucene MMapDirectory unmapping is part of the whitelisted APIs in JIGSAW, so we can still call it: http://openjdk.java.net/jeps/260

Thanks to our work in arguing about sun.misc.Cleaner :-)
</comment><comment author="jprante" created="2015-09-13T20:07:22Z" id="139913779">`sun.nio.ch` and `sun.nio.fs` (at least) are used by Groovy when using NIO, e.g. when looking for `sun.nio.fs.UnixPath` methods dynamically.

Example:

```
Exception in thread "main" java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.nio.fs")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkPackageAccess(SecurityManager.java:1564)
    at java.lang.Class.checkPackageAccess(Class.java:2372)
    at java.lang.Class.checkMemberAccess(Class.java:2351)
    at java.lang.Class.getMethod(Class.java:1783)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass.hasUsableImplementation(CachedSAMClass.java:130)
    at org.codehaus.groovy.reflection.stdclasses.CachedSAMClass.getSAMMethod(CachedSAMClass.java:191)
    at org.codehaus.groovy.reflection.ClassInfo.isSAM(ClassInfo.java:359)
    at org.codehaus.groovy.reflection.ClassInfo.createCachedClass(ClassInfo.java:349)
    at org.codehaus.groovy.reflection.ClassInfo.access$700(ClassInfo.java:41)
    at org.codehaus.groovy.reflection.ClassInfo$LazyCachedClassRef.initValue(ClassInfo.java:497)
    at org.codehaus.groovy.reflection.ClassInfo$LazyCachedClassRef.initValue(ClassInfo.java:488)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:49)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:36)
    at org.codehaus.groovy.reflection.ClassInfo.getCachedClass(ClassInfo.java:111)
    at org.codehaus.groovy.reflection.ReflectionCache.getCachedClass(ReflectionCache.java:110)
    at org.codehaus.groovy.reflection.CachedClass$4.initValue(CachedClass.java:141)
    at org.codehaus.groovy.reflection.CachedClass$4.initValue(CachedClass.java:138)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:49)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:36)
    at org.codehaus.groovy.reflection.CachedClass.getCachedSuperClass(CachedClass.java:248)
    at org.codehaus.groovy.reflection.CachedClass$8.initValue(CachedClass.java:214)
    at org.codehaus.groovy.reflection.CachedClass$8.initValue(CachedClass.java:200)
    at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:49)
    at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:36)
    at org.codehaus.groovy.reflection.CachedClass.getInterfaces(CachedClass.java:252)
    at org.codehaus.groovy.reflection.CachedClass.&lt;init&gt;(CachedClass.java:238)
    at org.codehaus.groovy.reflection.ClassInfo.createCachedClass(ClassInfo.java:352)
    &lt;&lt;&lt;truncated&gt;&gt;&gt;
```
</comment><comment author="rmuir" created="2015-09-13T20:09:30Z" id="139913882">That is not allowed. See https://issues.apache.org/jira/browse/GROOVY-7587
</comment><comment author="uschindler" created="2015-09-13T21:45:03Z" id="139921369">The whole thing is already investigated by both Oracle and also Groovy people. This is no 1 issue on the jigsaw-dev mailing list :-)

The issue has nothing to do with this it accessing stuff inside nio. The problem is: Theoretically Groovy can handle that, it is just "surprised" by a new Exception type it cannot handle. Oracle now thinks about making this new Exception a subclass of SecurityException to work around issues like that (setAccessible was only documented to throw SecurityException and now it suddenly throws another runtime exception. Because of that Groovy cannot handle that (and hide the non-accessible method, it just fails). By default it simply ignores fields it cannot make accessible.
</comment><comment author="jprante" created="2015-09-14T07:19:15Z" id="139982631">I'm confused, because I get the security exception in both JDK 8 and JDK 9.
</comment><comment author="rmuir" created="2015-09-14T07:27:57Z" id="139983950">You get the SecurityException because we don't allow it by policy in elasticsearch. 

On java 9 even if there is no securitymanager, you will still get InaccessibleObjectException.
</comment><comment author="jprante" created="2015-09-14T07:34:22Z" id="139984796">FWIW, using 

```
java version "1.9.0-ea"
Java(TM) SE Runtime Environment (build 1.9.0-ea-b81)
Java HotSpot(TM) 64-Bit Server VM (build 1.9.0-ea-b81, mixed mode)
```

and a command line like

```
./bin/elasticsearch -Des.cluster.name=mycluster -Dsecurity.manager.enabled=false
```

then ES 2.0.0-beta1 with my groovy plugin which uses `Paths.get(...)` does not abort with an exception.
</comment><comment author="rmuir" created="2015-09-14T07:36:39Z" id="139985119">You need to use a jigsaw build.
</comment><comment author="jprante" created="2015-09-14T07:38:04Z" id="139985267">Ok, I missed that.
</comment><comment author="rmuir" created="2015-09-14T07:47:19Z" id="139986805">You can download one from here: http://jdk9.java.net/jigsaw

Master is fully functional on that build but only after some battles this weekend :)

The remaining internal packages that we use (for mmap unmapping and reflection for scripting and mocks) are "ok for now" because they are "critical internal apis" (http://openjdk.java.net/jeps/260). But these are deprecated and going away eventually.
</comment><comment author="uschindler" created="2015-09-14T07:47:24Z" id="139986821">Hi,
actually Robert's and your problem are 2 different ones: https://issues.apache.org/jira/browse/GROOVY-7587 talks about a new exception which solely happens on setAccessible. This will only affect Java 9 Jigsaw builds. This is a real bug.

The problem here is a different one and has to do with reflection in general. The problem is that Java returns package internal classes which implement a public interface. Because groovy needs to call methods on it, it must reflect on them. So we must allow that otherwise you cannot even find out which methods are. Reflecting internal packages is not necessarily bad, that is something every scripting language must be able to do. Java 8's internal Javascript can do this, the problem is external scripting engines. You must just disallow to call seAccessible()!

In fact Groovy does not want to make them accessible, it just needs to find out what methods are on the object - it does not want to make them accessible or call them. These are 2 different issues. The latter will also work with Java 9!
</comment><comment author="jprante" created="2015-09-14T08:04:34Z" id="139990554">Yes, I got confused because I thought JDK 9 early access include jigsaw but there are separate builds. 

The issues are indeed different. Just worrying about how Groovy will work in the JDK 9 future, and with or without security manager. It's a bit too early to see the light at the end of the tunnel.

Robert is doing amazing work!
</comment><comment author="jprante" created="2015-09-14T08:04:42Z" id="139990572">Yes, I got confused because I thought JDK 9 early access include jigsaw but there are separate builds. 

The issues are indeed different. Just worrying about how Groovy will work in the JDK 9 future, and with or without security manager. It's a bit too early to see the light at the end of the tunnel.

Robert is doing amazing work!
</comment><comment author="uschindler" created="2015-09-14T08:11:10Z" id="139992401">&gt; Just worrying about how Groovy will work in the JDK 9 future, and with or without security manager

It does not work at all. The most funny fact is: They cannot fix (and more important test the bugfix) because of a chicken-egg problem: "For Groovy we have a chicken and egg problem for testing, because this change breaks Groovy, and Groovy uses Gradle to build. Since Gradle itself uses Groovy, we have no compatible build tool to test the fix... So it's very problematic." (http://mail.openjdk.java.net/pipermail/jigsaw-dev/2015-September/004517.html)

So they cannot even build Groovy with Java 9 - LOL! Personally, I would nuke Groovy in ES and use Nashorn only.
</comment><comment author="jprante" created="2015-09-14T08:35:24Z" id="140000908">It's more complex and not only Groovy - all platforms with dynamic class loading and inspection with `setAccessible` tweaks are affected by the new jigsaw puzzle, where Guava is one prominent example (I just worked through the openjdk thread). And Guava is also an ES dependency. In the end it's the question whether only JDK code will be able to control the setup of a JVM module structure at JVM startup time or non-JDK code too. Let's hope Oracle will not ignore these projects.
</comment><comment author="uschindler" created="2015-09-14T08:46:01Z" id="140003493">bq. And Guava is also an ES dependency.

That is not a problem, because ES does not use the reflection stuff in Guava. This is like commons-lang + commons-collections + commons-io, a large library.
</comment><comment author="uschindler" created="2015-09-14T08:48:23Z" id="140003872">&gt; Let's hope Oracle will not ignore these projects

It is also vice versa. Like Lucene and ES, projects should look into the issues and fix their code. As seen from my work yesterday, most stuff can be done without making anything accesible by just refectoring your code. I did a short conculsion to the lucene-dev mailing list: http://mail-archives.apache.org/mod_mbox/lucene-dev/201509.mbox/%3C014801d0ee23%245c8f5df0%2415ae19d0%24%40thetaphi.de%3E
</comment><comment author="s1monw" created="2015-09-14T09:42:29Z" id="140019057">&gt; And Guava is also an ES dependency.

@jprante we are getting rid of guava for 3.x here https://github.com/elastic/elasticsearch/issues/13224
</comment><comment author="rmuir" created="2015-09-14T12:37:53Z" id="140056440">&gt; t's more complex and not only Groovy - all platforms with dynamic class loading and inspection with setAccessible tweaks

And setAccessible is not a "tweak", its not ok to use at all. I am working to ban it completely with the security policy as soon as possible.

this thread says it nicely: http://mail.openjdk.java.net/pipermail/hotspot-dev/2015-June/019277.html

```
On 06/29/2015 10:47 PM, John Rose wrote:
&gt; I assume you are thinking about setAccessible, but if that's allowed there's nothing to prevent people from taking whole system down in a hundred different ways.  

Ok, thanks for the clarification John. I&#8217;ll admit to occasionally forgetting that setAccessible is as unsafe as, well, Unsafe.
```
</comment><comment author="uschindler" created="2015-09-14T13:14:29Z" id="140070878">Nice post!

I think we have 2 issues here: setAccessible must be banned anyways. The problem with groovy is also another problem: Dynamic scripting languages have to figure out which methods they can call on, so they have to "inspect" the class. As you see from stack trace above, Groovy did not want to call setAccessible. The issue was it was not even able to do Class#getMethod() which retrieves only public methods. This is needed to call the function dynamically. The problem is that we forbid refelction at all!

So forbidding setAccesible is fine, but for dynamic scripting languages you should allow "inspection" of those classes, otherwise they will never be able to call any method.
</comment><comment author="rmuir" created="2015-09-14T13:22:05Z" id="140073086">&gt; So forbidding setAccesible is fine, but for dynamic scripting languages you should allow "inspection" of those classes, otherwise they will never be able to call any method.

That is not going to happen. it is too much of a security risk. For scripts in ES, this is not needed (and by the way, groovy scripts run with _zero_ permissions so there is no sense in them getting a Path or File or anything like that).
</comment><comment author="rmuir" created="2015-09-14T13:23:09Z" id="140073847">And its not allowed in java 9 anyway, so good time to stop dreaming.
</comment><comment author="uschindler" created="2015-09-14T13:38:38Z" id="140078179">```
C:\Users\Uwe Schindler\Desktop\robert&gt;cat Test.java
import java.nio.file.*;
import java.lang.reflect.Method;
public class Test {
 public static void main(String... args) throws Exception {
  Path path = Paths.get(".");
  Class&lt;?&gt; clazz = path.getClass();
  System.out.println(clazz.getName());
  Method m = clazz.getMethod("resolve", String.class);
  System.out.println(m);
 }
}
C:\Users\Uwe Schindler\Desktop\robert&gt;javac Test.java

C:\Users\Uwe Schindler\Desktop\robert&gt;java -version
java version "1.9.0-ea"
Java(TM) SE Runtime Environment (build 1.9.0-ea-jigsaw-nightly-h3337-20150908-b80)
Java HotSpot(TM) 64-Bit Server VM (build 1.9.0-ea-jigsaw-nightly-h3337-20150908-b80, mixed mode)

C:\Users\Uwe Schindler\Desktop\robert&gt;java Test
sun.nio.fs.WindowsPath
public default java.nio.file.Path java.nio.file.Path.resolve(java.lang.String)

C:\Users\Uwe Schindler\Desktop\robert&gt;
```

I was just talking about "inspection" not calling or making stuff accessible! :-)
</comment><comment author="rmuir" created="2015-09-14T13:48:26Z" id="140080876">Public methods are no problem, we don't do anything to prevent that.

As far as `accessDeclaredMembers` we don't prevent that either: that is not low hanging fruit at the moment.
</comment><comment author="uschindler" created="2015-09-14T14:07:11Z" id="140086354">You don't understand the issue here, full stack trace analysis:

@jprante posted the folowing stack trace:

```
Exception in thread "main" java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessClassInPackage.sun.nio.fs")
    at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)
    at java.security.AccessController.checkPermission(AccessController.java:884)
    at java.lang.SecurityManager.checkPermission(SecurityManager.java:549)
    at java.lang.SecurityManager.checkPackageAccess(SecurityManager.java:1564)
    at java.lang.Class.checkPackageAccess(Class.java:2372)
    at java.lang.Class.checkMemberAccess(Class.java:2351)
    at java.lang.Class.getMethod(Class.java:1783)
```

When you look at what this code does: It calls getMethod() on some sun.nio class, this is basically what my above example code does. It does not call the method using invoke. Nor does it call setAccessibe on it. It just refelect on the method using Class#getMethod().

And this code works perfectly with Java 9 Jigsaw, I just did what the groovy code does as example. It reflects on this NIO internal class to figure out what methods this class has, so it can create the runtime proxy class:

``` java
import java.nio.file.*;
import java.lang.reflect.Method;
public class Test {
 public static void main(String... args) throws Exception {
  Path path = Paths.get(".");
  Class&lt;?&gt; clazz = path.getClass();
  System.out.println(clazz.getName());
  Method m = clazz.getMethod("resolve", String.class);
  System.out.println(m);
 }
}
```

Thats nothing more which my code an statement contains. There is nothing security relevant in that. It just wants to find out if a class it gets as a instance back by some method contains a method. And that still works in Java 9.

I dont want to argue here, I just want to ensure we all know that we are talking about the same. So we should figure out why his code example fails. At least it would not fail under Java 9: to figure out if a public method is there!
</comment><comment author="rmuir" created="2015-09-14T14:09:53Z" id="140087942">&gt; There is nothing security relevant in that. It just wants to find out if a class it gets as a instance back by some method contains a method

There absolutely is. How many remote executions does it take from groovy to learn the lesson? Sorry, i worked too hard to remove these permissions granting access to internal packages. There is no chance of adding more `accessClassInPackage` permissions, that is just wrong to do.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move static bwc indexes to a shared location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13443</link><project id="" key="" /><description>There are a few tests that currently use the statically generated
backcompat indexes. This change moves them to a shared location, so they
no longer have to build a path based on the package name of the old
index tests.
</description><key id="105698587">13443</key><summary>Move static bwc indexes to a shared location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>test</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T22:44:03Z</created><updated>2015-09-10T20:33:34Z</updated><resolved>2015-09-10T17:31:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-10T17:19:02Z" id="139315984">LGTM
</comment><comment author="nik9000" created="2015-09-10T17:22:42Z" id="139316796">LGTM too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove noise when IDE test runners try to System.exit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13442</link><project id="" key="" /><description>See https://issues.apache.org/jira/browse/LUCENE-6794
</description><key id="105693266">13442</key><summary>Remove noise when IDE test runners try to System.exit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T22:01:35Z</created><updated>2015-09-30T03:10:33Z</updated><resolved>2015-09-09T22:22:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-09T22:05:19Z" id="139058902">LGTM
</comment><comment author="rmuir" created="2015-09-30T03:10:33Z" id="144269741">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup QueryTest framework and be more strict</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13441</link><project id="" key="" /><description>This commit contains:
- renaming BaseQueryTestCase to AbstractQueryTestCase
- uses always STRICT parsing when parsing from builders XContent
- ensures that SearchContext is only but always available during toQuery but never during parse phase
- adds a way to override the default ParseFieldMatcher to allow queries to set deprecated API by default

this also found a bug
</description><key id="105690649">13441</key><summary>Cleanup QueryTest framework and be more strict</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-09T21:43:49Z</created><updated>2015-09-10T11:12:28Z</updated><resolved>2015-09-10T11:12:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-10T08:26:46Z" id="139165733">this looks great, thanks a lot @s1monw . left one comment.
</comment><comment author="s1monw" created="2015-09-10T10:10:16Z" id="139194624">@javanna I cleaned it up even further and use strict parsing everywhere. can you take another look?
</comment><comment author="javanna" created="2015-09-10T10:16:21Z" id="139196772">looks perfect, thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not dump stack traces of threads on test failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13440</link><project id="" key="" /><description>Reverts 878301c795e6ad2c47c620c893b3324a564e0d91, relates #12425
</description><key id="105685328">13440</key><summary>Do not dump stack traces of threads on test failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T21:10:17Z</created><updated>2015-09-09T21:23:43Z</updated><resolved>2015-09-09T21:23:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2015-09-09T21:22:00Z" id="139049611">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>upgrade lucene to r1702265</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13439</link><project id="" key="" /><description>Mainly motivated to remove sun.nio.ch permission. But it has some boostings changes too.
</description><key id="105679490">13439</key><summary>upgrade lucene to r1702265</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T20:37:17Z</created><updated>2015-09-30T04:36:23Z</updated><resolved>2015-09-11T07:48:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-09T20:52:28Z" id="139042160">Maybe we could mark these super super slow tests with `@Nightly` instead of disabling them entirely? Regarding the thread dumps I think @jasontedor added this feature in order to have more information when a hard-to-reproduce test fails, but I tend to agree it makes the output very verbose given the number of threads elasticsearch starts. I don't know how useful it proved, @jasontedor what do you think?
</comment><comment author="rmuir" created="2015-09-09T20:53:03Z" id="139042335">Feel free to push changes. I am done here. 

I just want to add the security permission. I do not care about the crappy tests
</comment><comment author="jpountz" created="2015-09-09T21:02:45Z" id="139044814">OK, I'll take care of addressing potential comments and merging this PR.
</comment><comment author="jasontedor" created="2015-09-09T21:10:46Z" id="139047084">@jpountz I think it can be removed. I filed #13440 to remove it.
</comment><comment author="jpountz" created="2015-09-09T22:00:39Z" id="139058126">thanks @jasontedor !
</comment><comment author="jpountz" created="2015-09-10T21:39:16Z" id="139386726">I rebased to include @jasontedor 's changes and fixed the remaining bugs. It should be good now.
</comment><comment author="rmuir" created="2015-09-10T21:50:31Z" id="139389348">looks great
</comment><comment author="rmuir" created="2015-09-11T05:10:04Z" id="139453599">This branch now works with jigsaw (`mvn verify` passes). I wrote up a summary here: http://mail.openjdk.java.net/pipermail/jigsaw-dev/2015-September/004511.html
</comment><comment author="rmuir" created="2015-09-30T04:36:23Z" id="144279607">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.collect.Maps</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13438</link><project id="" key="" /><description>This commit removes and now forbids all uses of
com.google.common.collect.Maps across the codebase. This is one of many
steps in the eventual removal of Guava as a dependency.

Relates #13224
</description><key id="105676560">13438</key><summary>Remove and forbid use of com.google.common.collect.Maps</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T20:19:16Z</created><updated>2015-09-09T21:58:46Z</updated><resolved>2015-09-09T21:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-09T21:10:34Z" id="139047042">left minor comments, otherwise LGTM. If you are good with the proposed changes, please feel free to push without further review!
</comment><comment author="jasontedor" created="2015-09-09T21:44:59Z" id="139055001">Thank you for reviewing @jpountz!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Listener functionality in SearchService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13437</link><project id="" key="" /><description>org.elasticsearch.index.IndexService Has support Listeners (addListener etc)
Why org.elasticsearch.search.SearchService does not have same functionality?

If we have no reason about why Search service does not have support Listeners. I try to implement.
</description><key id="105674188">13437</key><summary>Listener functionality in SearchService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kiryam</reporter><labels><label>:Internal</label><label>discuss</label></labels><created>2015-09-09T20:05:29Z</created><updated>2016-01-28T16:18:46Z</updated><resolved>2016-01-28T16:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T16:18:46Z" id="176259169">No further info on this ticket. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If search query failed slow log will be ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13436</link><project id="" key="" /><description>in org.elasticsearch.index.search.stats.ShardSearchStats

``` java
    public void onFailedQueryPhase(SearchContext searchContext) {
        totalStats.queryCurrent.dec();
        if (searchContext.groupStats() != null) {
            for (int i = 0; i &lt; searchContext.groupStats().size(); i++) {
                groupStats(searchContext.groupStats().get(i)).queryCurrent.dec();
            }
        }
    }

    public void onQueryPhase(SearchContext searchContext, long tookInNanos) {
        totalStats.queryMetric.inc(tookInNanos);
        totalStats.queryCurrent.dec();
        if (searchContext.groupStats() != null) {
            for (int i = 0; i &lt; searchContext.groupStats().size(); i++) {
                StatsHolder statsHolder = groupStats(searchContext.groupStats().get(i));
                statsHolder.queryMetric.inc(tookInNanos);
                statsHolder.queryCurrent.dec();
            }
        }
        slowLogSearchService.onQueryPhase(searchContext, tookInNanos);
    }
```

Method slowLogSearchService.onQueryPhase(searchContext, tookInNanos); called If search query completed. But if query will fail - nothing happens.
</description><key id="105673060">13436</key><summary>If search query failed slow log will be ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kiryam</reporter><labels><label>:Logging</label><label>discuss</label></labels><created>2015-09-09T19:59:27Z</created><updated>2015-09-19T13:59:54Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>add more notes about Eclipse setup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13435</link><project id="" key="" /><description>Added notes about "search for nested projects..." and the amount of memory Eclipse uses.
</description><key id="105671533">13435</key><summary>add more notes about Eclipse setup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels><label>docs</label><label>v2.0.0-rc1</label></labels><created>2015-09-09T19:50:47Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-22T14:12:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-09T21:31:43Z" id="139051795">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplify the BlobContainer blob writing interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13434</link><project id="" key="" /><description>Instead of asking blob store to create output for posting blob content, this change provides that content of the blob to the blob store for writing. This will significantly simplify the  interface for S3 and Azure plugins.

I will open separate PRs for changes in S3 and Azure plugins to keep things simple.
</description><key id="105670624">13434</key><summary>Simplify the BlobContainer blob writing interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T19:45:14Z</created><updated>2015-09-11T20:43:14Z</updated><resolved>2015-09-11T20:43:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-09T19:52:43Z" id="139027162">I think you should do this in a single PR but you can add different commits in it.
I think it would be better to review the whole change and understand better the benefits.

I did not read in details for now but for sure it will simplify azure/s3 code!
</comment><comment author="imotov" created="2015-09-10T02:59:17Z" id="139102251">@dadoonet I just want to mention that this PR is not going to break S3 and Azure plugin. I have added backward compatibility layer for them. My main concern is that if I will dump all the changes into a single PR I will have hard time finding someone, who would be able to review all this changes. So, I am just trying to make things simpler.
</comment><comment author="dadoonet" created="2015-09-10T05:03:35Z" id="139118440">Ok. I missed that it's backward compatible. 
</comment><comment author="tlrx" created="2015-09-10T07:24:33Z" id="139137358">@imotov I looked at the code, it's LGTM

I'm so so glad that you make this change, this will help a lot! Thank you so much
</comment><comment author="dadoonet" created="2015-09-10T07:26:16Z" id="139137720">I looked at the simplifications you mentioned for Azure and S3. I can easily find the added value for Azure as we don't need anymore to have `AzureOutputStream` but I think we will have to keep around almost all what we have for S3.

That being said, I'm +1 for the change as it simplifies the interface and what a plugin author has basically to provide.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Weighted centroid for geohash_grid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13433</link><project id="" key="" /><description>A widely requested feature (specifically for Kibana) this PR adds a `centroid` to the GeoHashGridAggregator that represents the weighted centroid for all geo_points that fall within the corresponding grid bucket.  This feature provides the ability to position visualization markers at a location on the map that is more indicative of the actual point locations rather than the center of the geohash cell (e.g., a marker correctly located in the bay area for a level 3 geohash instead of in the middle of the ocean). The following is an example of the output: 

``` javascript
  "aggregations" : {
    "geohashgrid" : {
      "buckets" : [ {
        "key" : "g",
        "doc_count" : 8,
        "centroid" : [72.37982577458024, -18.86329485476017]
      }, {
        "key" : "v",
        "doc_count" : 7,
        "centroid" : [64.23251533415169, 68.72181602753699]
      }, {
        "key" : "5",
        "doc_count" : 6,
        "centroid" : [-63.65684769116342, -24.55287636257708]
      }, {
        "key" : "3",
        "doc_count" : 6,
        "centroid" : [-19.161130998283625, -103.60427623987198]
      }, {
        "key" : "j",
        "doc_count" : 5,
        "centroid" : [-68.90129841165617, 71.04619401041418]
      } ]
    }
  }
```

This feature can also be backported to 2.0 for availability in the upcoming 2.0.RC.

closes #10004 
</description><key id="105669084">13433</key><summary>Weighted centroid for geohash_grid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Aggregations</label><label>feature</label><label>review</label></labels><created>2015-09-09T19:34:24Z</created><updated>2015-11-20T14:17:27Z</updated><resolved>2015-09-11T15:00:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2015-09-11T09:12:35Z" id="139494821">@nknize I left a comment but it looks good. I have assumed that the files that have been renamed form foo to Xfoo have not changed do I didn't review them but let me know if they have and which bits changed and i'll review them.
</comment><comment author="nknize" created="2015-09-11T13:27:54Z" id="139545781">@colings86 That's right XFoo just means they'll be removed once the latest version of Lucene is integrated.
</comment><comment author="colings86" created="2015-09-11T13:29:29Z" id="139546096">LGTM, love this change :)
</comment><comment author="nknize" created="2015-09-11T14:46:17Z" id="139564860">@clintongormley do we want this in 2.0rc? Or save for 2.1? /cc @s1monw 
</comment><comment author="colings86" created="2015-09-11T14:49:10Z" id="139565499">Personally I would say this should wait for 2.1. I don't see it as critical for 2.0 so I think we shouldn't add it
</comment><comment author="nknize" created="2015-09-11T14:52:36Z" id="139566287">++ agree. Can always backport if there's demand.  Thx @colings86!
</comment><comment author="jpountz" created="2015-09-11T14:59:07Z" id="139568457">+1 to 2.1
</comment><comment author="spalger" created="2015-09-14T14:21:50Z" id="140096164">:dancer: :dancer: :dancer: 
</comment><comment author="spalger" created="2015-09-14T14:25:49Z" id="140098080">@nknize Any reason the centroid is a JSON string inside of the JSON response? Maybe that changed since the description was written?
</comment><comment author="jpountz" created="2015-09-15T10:04:48Z" id="140343230">Now that I look at this feature again, I'm wondering whether this should be a different (metric) geo_centroid aggregation instead of an additional feature of the geohashgrid aggregation? This way we wouldn't require 8 more bytes of memory per bucket for users who don't need this feature, and it could be used with other aggregations. For instance if you use a `filters` aggregation to create buckets based on geo filters, you could still compute the centroid of documents matching each bucket by using this aggregation as a sub aggregation.
</comment><comment author="jpountz" created="2015-09-15T10:05:44Z" id="140343521">Also is the labeling right? It is tagged 2.0.0-beta2 but in the discussion we seemed to agree it should be in 2.1?
</comment><comment author="colings86" created="2015-09-15T10:08:18Z" id="140344012">++ That makes a lot of sense. Especially as you could actually use it in more novel ways to compute centroids in buckets of a terms aggregation which is aggregating on say town names, to get the same functionality but on more semantically meaningful 'cells'
</comment><comment author="nknize" created="2015-09-16T16:11:42Z" id="140790697">+++ This is a wonderful idea. Just opened a new issue to refactor this logic.
</comment><comment author="nknize" created="2015-09-16T16:13:37Z" id="140791148">@spalger re: geojson string. I think I had corrected this from a string to a double array and the above example was from an early test, but I'll check and make sure.
</comment><comment author="nknize" created="2015-09-16T18:53:01Z" id="140841744">@spalger confirmed. The centroid is a double array...

``` javascript
  "aggregations" : {
    "geohashgrid" : {
      "buckets" : [ {
        "key" : "zej",
        "doc_count" : 10,
        "centroid" : [ 165.87957870215178, 61.95794915780425 ]
      }, {
        "key" : "zdj",
        "doc_count" : 8,
        "centroid" : [ 165.7405973598361, 56.84159707278013 ]
      } ]
    }
  }
}
```
</comment><comment author="bleskes" created="2015-09-17T08:35:14Z" id="141005692">Given all the confusion around lat lon syntax, I wonder if it makes sense to make it an explicit json object, i.e., centroid: { lat: XX, lon: YY } 

+100 to a metric.

&gt; On 16 Sep 2015, at 20:53, Nick Knize notifications@github.com wrote:
&gt; 
&gt; @spalger confirmed. The centroid is a double array...
&gt; 
&gt;   "aggregations" :
&gt;  {
&gt; 
&gt; "geohashgrid" :
&gt;  {
&gt; 
&gt; "buckets" :
&gt;  [ {
&gt; 
&gt; "key" : "zej"
&gt; ,
&gt; 
&gt; "doc_count" : 10
&gt; ,
&gt; 
&gt; "centroid" : [ 165.87957870215178, 61.95794915780425
&gt;  ]
&gt;       }, {
&gt; 
&gt; "key" : "zdj"
&gt; ,
&gt; 
&gt; "doc_count" : 8
&gt; ,
&gt; 
&gt; "centroid" : [ 165.7405973598361, 56.84159707278013
&gt;  ]
&gt;       } ]
&gt;     }
&gt;   }
&gt; }
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="clintongormley" created="2015-11-20T14:17:18Z" id="158412880">Removed in favour of https://github.com/elastic/elasticsearch/pull/13846
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child has_parent query parsers shouldn't require search context</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13432</link><project id="" key="" /><description /><key id="105667509">13432</key><summary>has_child has_parent query parsers shouldn't require search context</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>non-issue</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T19:25:41Z</created><updated>2015-09-19T14:36:13Z</updated><resolved>2015-09-10T11:45:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-09T21:00:57Z" id="139044438">LGTM
</comment><comment author="jpountz" created="2015-09-09T21:33:02Z" id="139052067">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term Vectors: splits the actual results from the response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13431</link><project id="" key="" /><description>Today term vectors are always written into a bytes stream regardless as to
whether they have been asked on the shard or by the client. This WIP consists
of splitting the Term Vectors response from the actual returned fields. This
leads, for example, to a speed improvement in fetch sub phase in which the
returned fields could be used as is, and are not turned into a bytes stream,
only to be read again immediately after.
</description><key id="105659282">13431</key><summary>Term Vectors: splits the actual results from the response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>WIP</label></labels><created>2015-09-09T18:51:08Z</created><updated>2016-03-10T11:45:45Z</updated><resolved>2016-03-10T11:45:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-10T08:52:53Z" id="139173358">I looked at it but I think it goes int the wrong direction. Instead of adding yet another class with all kinds of code duplication we should just pass in Fields and also expose that interface and delay the serialization. That way we can keep everything as is and just fix the TermVectorReponse impl details?
</comment><comment author="alexksikes" created="2015-09-10T09:08:02Z" id="139177133">That sounds great. This was just a very quick prototype with minimal code change to illustrate the problem and the solution. Yes I think implementation wise it would be nicer to just delay serialization in TermVectorsReponse. Thanks for the review.
</comment><comment author="clintongormley" created="2016-03-10T11:45:45Z" id="194807206">PR is out of date. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent field mapper should always store doc values join field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13430</link><project id="" key="" /><description>Now that the pre 2.0 parent child implementation has been removed there is no need to have logic that decides not to store the join doc values field.
</description><key id="105637795">13430</key><summary>Parent field mapper should always store doc values join field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>non-issue</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T16:48:06Z</created><updated>2016-03-10T18:54:43Z</updated><resolved>2015-09-10T10:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-09T20:53:56Z" id="139042671">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If sorting by nested field then the `nested_path` should always be specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13429</link><project id="" key="" /><description>PR for #13420
</description><key id="105636145">13429</key><summary>If sorting by nested field then the `nested_path` should always be specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>breaking</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T16:41:08Z</created><updated>2015-09-10T10:38:11Z</updated><resolved>2015-09-10T10:38:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-09T20:52:55Z" id="139042265">LGTM
</comment><comment author="jpountz" created="2015-09-09T21:34:41Z" id="139052376">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added lifecycle-mapping plugin for Eclipse IDE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13428</link><project id="" key="" /><description>The addition of the lifecycle-mapping plugin reduces the amount of
manual steps one needs to do after checking out the elasticsearch
project and importing it into an Eclipse IDE. The plugins that don't
have mappings are now either ignored or executed.

The ones that were not included here are resolved by installing the
appropriate m2e-connector which is one of the very first "quick fix"
options presented.

This should have no negative affect on the regular build or on other IDEs.
</description><key id="105630589">13428</key><summary>Added lifecycle-mapping plugin for Eclipse IDE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djschny</reporter><labels /><created>2015-09-09T16:14:44Z</created><updated>2015-09-09T19:52:57Z</updated><resolved>2015-09-09T19:52:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-09T16:20:46Z" id="138961630">Last I checked CONTRIBUTING.md recommends people use `mvn eclipse:eclipse` instead of m2e-connector. I always liked m2e-connector but `mvn eclipse:eclipse` is useful here because it sets up the license header and all that jazz.
</comment><comment author="djschny" created="2015-09-09T16:40:01Z" id="138967508">@nik9000 unless I'm missing something, even after I had performed the `mvn eclipse:eclipse`, the lifecycle configuration errors were still present.
</comment><comment author="rmuir" created="2015-09-09T16:41:04Z" id="138968426">mvn eclipse:eclipse works fine, I use it every day.
</comment><comment author="nik9000" created="2015-09-09T16:48:40Z" id="138971953">&gt; @nik9000 unless I'm missing something, even after I had performed the mvn eclipse:eclipse, the lifecycle configuration errors were still present.

Its because m2e-connect gets in the way of eclipse:eclipse. You have to uninstall it or remove the maven nature from the project. I just leave it uninstalled because eclipse:eclipse does the job for Elasticsearch.
</comment><comment author="djschny" created="2015-09-09T19:52:56Z" id="139027211">@nik9000 you're right, the m2e-connect was causing the problem. I uninstalled, re-imported and everything was fine.

@rmuir - after removing m2e-connect then everything worked fine

I've created https://github.com/elastic/elasticsearch/pull/13435 to add more detail to the CONTRIBUTING.md to make note of these and other items. This PR is no longer relevant.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide the ability to search an index in the context of another</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13427</link><project id="" key="" /><description>Proposing providing the ability to specify a list of indexes to search, but use the term weights from only one - similar to how cross fields blends weights.

This may have applications in multi language search, where the requirement is to return mixed language results (each doc is only 1 language) i.e. the user doesn't specify a language.  This typically results in poor quality results, as TF-IDF favours the where the language terms are rare.

This feature may have other applications beyond languages.  I appreciate this has challenges aligning token streams due to some terms not appearing in all indexes and being stemmed differently.  Possibly provide different blending mechanisms.

@markharwood @polyfractal may have additional thoughts.
</description><key id="105607751">13427</key><summary>Provide the ability to search an index in the context of another</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gingerwizard</reporter><labels><label>:Search</label><label>discuss</label></labels><created>2015-09-09T14:25:19Z</created><updated>2016-01-28T16:17:48Z</updated><resolved>2016-01-28T16:17:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2015-09-10T08:24:24Z" id="139165335">Would [DFS Query then fetch](https://www.elastic.co/blog/understanding-query-then-fetch-vs-dfs-query-then-fetch) search mode provide what is required?
</comment><comment author="clintongormley" created="2016-01-28T16:17:48Z" id="176258760">Given the esoteric nature of this request, and the fact that it hasn't garnered any interest so far, I'm going to close this issue for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor of GeoPolygonQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13426</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217

PR goes against the query-refactoring branch
</description><key id="105604905">13426</key><summary>Refactor of GeoPolygonQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-09T14:12:23Z</created><updated>2015-09-11T18:36:06Z</updated><resolved>2015-09-11T06:54:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-10T10:35:24Z" id="139200593">left a few minor comments, looks good already though
</comment><comment author="colings86" created="2015-09-10T12:48:07Z" id="139226036">@javanna I pushed a commit to address your comments
</comment><comment author="s1monw" created="2015-09-10T12:53:40Z" id="139227106">left some minors - looks great
</comment><comment author="colings86" created="2015-09-10T13:44:48Z" id="139238214">@s1monw thanks for the review. I've push a new commit that should address all your comments
</comment><comment author="s1monw" created="2015-09-10T18:47:03Z" id="139341128">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Backwards compatibility tests broken for 2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13425</link><project id="" key="" /><description>#13130 uncovered that the backwards compatibility tests don't really work in 2.x. There are a few issues:
1. We don't have a release of 2.x so the best we can do to test 2.x is to use 2.0.0-beta1. But `Version#minimumCompatibilityVersion` seems to reject that. Its simple enough to just make it accept the beta for now.
2. `CompositeTestCluster` seems to be a bit broken.
3. The backwards section in TESTING.asciidoc is super out of date.
</description><key id="105603441">13425</key><summary>Backwards compatibility tests broken for 2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>blocker</label><label>test</label><label>v2.4.0</label></labels><created>2015-09-09T14:05:38Z</created><updated>2016-04-22T13:46:18Z</updated><resolved>2016-04-22T13:46:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-21T18:34:55Z" id="142069900">Removing v2.0.0 label. This is a blocked for 2.1.0 and 2.0.1 but not 2.0.0.
</comment><comment author="nik9000" created="2016-04-22T13:46:17Z" id="213434644">This is fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split NestedQueryParser into toQuery and formXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13424</link><project id="" key="" /><description>This commit splits NestedQueryParser into toQuery and fromXContent.

Relates to #10217
</description><key id="105593647">13424</key><summary>Split NestedQueryParser into toQuery and formXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-09T13:12:24Z</created><updated>2015-09-10T12:43:27Z</updated><resolved>2015-09-10T12:43:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-10T09:50:50Z" id="139189279">left a few comments, looks good though
</comment><comment author="s1monw" created="2015-09-10T12:19:10Z" id="139217734">@javanna I addressed all comments
</comment><comment author="javanna" created="2015-09-10T12:26:13Z" id="139219527">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove GeoDistanceQuery in preference to GeoDistanceRangeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13423</link><project id="" key="" /><description>Today we have GeoDistanceRangeQuery which allows you to define a from and to distance away from a point to match documents. We also have GeoDistanceQuery which does exactly the same thing except the from is fixed to 0. This is confusing as we have two query type which do almost the same thing and are easily confused.

I think we should remove GeoDistanceQuery since it is the only range query we have (correct me if I am wrong) where we have a special veriant for fixing one end of the range. I think over time we should then rename GeoDistanceRangeQuery to GeoDistanceQuery.
</description><key id="105593230">13423</key><summary>Remove GeoDistanceQuery in preference to GeoDistanceRangeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>:Query DSL</label><label>breaking</label><label>discuss</label></labels><created>2015-09-09T13:09:58Z</created><updated>2017-03-20T21:15:21Z</updated><resolved>2017-03-20T21:15:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2017-03-20T21:15:21Z" id="287900365">We went the other way and removed `geo_distance_range` query in https://github.com/elastic/elasticsearch/pull/22835.  Closing</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging tests use Java 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13422</link><project id="" key="" /><description>To do this we:
1. All the rpm based distros we test support Java 8. We just ask to install
it.
2. There is a ppa that works for the Ubuntus. We just add that for them.
3. Debian Jessie has Java 8 in its backports. We just add that repository.
4. Debian Wheezy doesn't have Java 8 easily accessible so we drop it. We
could add it back with Orache Java 8 at a later date but that will take a
few more backflips and won't support things like vagrant-cachier.

This required a ton of rebuilding of vagrant boxes so it also fixes:
1. apt-get update is run too frequently
2. Lots of weird warning messages are spit out of apt-get
3. Switch from the chef provided based images to those provided by boxcutter.
The chef images has left vagrant atlas!

Closes #13366
</description><key id="105582878">13422</key><summary>Packaging tests use Java 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>review</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-09T12:05:11Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-14T12:26:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-09T12:06:01Z" id="138888821">Because this contains a few cleanup operations I'll likely backport this to 2.x and 2.0 without the Java 8 portions once this is merge to master.
</comment><comment author="spinscale" created="2015-09-11T08:29:51Z" id="139484860">Am I doing it wrong? Running `mvn clean verify -Dtests.vagrant` I get a fair share of these failures in the vagrant tests

```
[exec] trusty: # Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/plugins/PluginManagerCliParser : Unsupported major.minor version 52.0
```

Is the default java on those VMs somewhat old? Should I delete my VMs? Anything potentially cached?
</comment><comment author="tlrx" created="2015-09-11T11:55:39Z" id="139527873">I hit the same error as @spinscale even if I destroy the VMs first.
</comment><comment author="nik9000" created="2015-09-11T12:04:03Z" id="139529334">&gt; Is the default java on those VMs somewhat old? Should I delete my VMs? Anything potentially cached?

Weird. I certainly wasn't seeing that error but I've been destroying and rebuilding those VMs a ton of times. So you _probably_ have to destroy the VMs first, honestly.
</comment><comment author="nik9000" created="2015-09-11T12:04:41Z" id="139529413">&gt; I hit the same error as @spinscale even if I destroy the VMs first.

Oh! Now I don't know.
</comment><comment author="spinscale" created="2015-09-11T13:45:52Z" id="139550987">running `vagrant destroy` in the 'root' directory and rerunning the tests solved this for me, everything passing now, LGTM
</comment><comment author="nik9000" created="2015-09-11T13:47:25Z" id="139551524">&gt; running vagrant destroy in the 'root' directory and rerunning the tests solved this for me, everything passing now, LGTM

Hurray! I'm playing with it again and coming to the same conclusion. You _should_ be able to run `vagrant destroy` from any directory you want inside of elasticsearch.
</comment><comment author="nik9000" created="2015-09-11T14:07:07Z" id="139556466">Destroying works for me.

@tlrx, does destroying it work for you?

If it does we can either:
1. Live with that or
2. File an issue about making sure we select the right java version and allow both to be installed.
</comment><comment author="nik9000" created="2015-09-13T21:57:29Z" id="139921933">&gt; @tlrx, does destroying it work for you?

Just to keep things moving I'll merge this on Monday morning if I haven't heard from you. Its an improvement if the vagrant tests work in master for _anyone_ because right now they work for no one.
</comment><comment author="tlrx" created="2015-09-14T08:53:10Z" id="140005300">Unfortunately it still does not work for me:

```
$ uname -a
Linux portable 3.19.0-28-generic #30~14.04.1-Ubuntu
$ vagrant version
Installed Version: 1.7.4
```

I destroyed the VMS using `vagrant destroy` then set up the boxes using `vagrant up --provision $box --provider virtualbox` and run `mvn clean install -DskipTests &amp;&amp;  mvn -Dtests.vagrant -pl qa/vagrant verify`. Same thing if a remove all boxes with `rm -rf ~/.vagrant.d`.

It looks like the VMs are still using java 7.

If it's just on my computer I think you can merge, but that would be nice to check that it works on more than 2 (yours &amp; @spinscale's) computers.
</comment><comment author="tlrx" created="2015-09-14T10:18:23Z" id="140028869">I manually remove the `.vagrant/` directory in the root directory and execute the tests again: it works.

LGTM then
</comment><comment author="nik9000" created="2015-09-14T12:03:20Z" id="140049632">That is rough !  Thanks for tricking that down!
On Sep 14, 2015 6:18 AM, "Tanguy Leroux" notifications@github.com wrote:

&gt; I manually remove the .vagrant/ directory in the root directory and
&gt; execute the tests again: it works.
&gt; 
&gt; LGTM then
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/13422#issuecomment-140028869
&gt; .
</comment><comment author="nik9000" created="2015-09-14T12:27:40Z" id="140054722">Ok - now that I think about this - I can just backport this 100% for 2.0 and 2.x. It won't _hurt_ anything to use Java 8 everywhere and, eventually, we can be a bit more picky. Maybe when working on #13392. Any objections to me just backporting this cleanly and getting those branches onto Java 8 for now?
</comment><comment author="tlrx" created="2015-09-14T12:32:20Z" id="140055508">@nik9000 I don't have any objections
</comment><comment author="nik9000" created="2015-09-14T13:19:27Z" id="140071814">Will do then.
</comment><comment author="nik9000" created="2015-09-14T14:27:30Z" id="140098488">Backported to 2.x. Starting on 2.0.
</comment><comment author="nik9000" created="2015-09-14T14:51:45Z" id="140105419">Backported to 2.0. Found #13557 in the process. Fun.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>isSubClass QB compilation error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13421</link><project id="" key="" /><description>When I try to compile the query-refactoring branch with Java 8u31, I get the following compilation error:

```
Error:java: java.lang.AssertionError: isSubClass QB
```

This bug seems to be related: http://bugs.java.com/view_bug.do?bug_id=8067111
</description><key id="105582190">13421</key><summary>isSubClass QB compilation error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>jvm bug</label></labels><created>2015-09-09T12:00:32Z</created><updated>2015-10-19T10:32:11Z</updated><resolved>2015-10-19T10:32:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Nested sort inside a top hits inside a reverse nested doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13420</link><project id="" key="" /><description>Hello,
I'm facing something that looks like a bug. I have a simple document model : my documents are items, and inside these items there are categories.
I want to get a list of categories with a certain type, and for each of these categories, a list of top items belonging to this category, sorted on the category rank. So I'm doing a top hits inside reverse nested 

Mapping example :

```
POST /bugth
{
    "mappings": 
    {        
        "item" : {
            "properties" : {
                "id" : { "type" : "integer" },
                "categories" : { 
                    "type" : "nested",
                    "properties" : {
                        "id" : { "type" : "integer" },
                        "type" : { "type" : "integer" },
                        "rank" : { "type" : "integer" }
                    }
                }
            }
        }        
    }
}
```

Some data to test :

```
PUT /bugth/item/3
{
    "id": 3,
    "categories": [{
        "id": 1,
        "type": 1,
        "rank": 15
    }, {
        "id": 2,
        "type": 1,
        "rank": 1
    }]
}

PUT /bugth/item/2
{
    "id": 2,
    "categories": [{
        "id": 1,
        "type": 1,
        "rank": 10
    }, {
        "id": 2,
        "type": 1,
        "rank": 5
    },
    {
        "id": 3,
        "type": 2,
        "rank": 20
    }]
}

PUT /bugth/item/1
{
    "id": 1,
    "categories": {
        "id": 1,
        "type": 1,
        "rank": 0
    }
}
```

Finally the search query :

```
POST /bugth/item/_search
{
    "size": 0,
    "query": {
    "filtered": {
      "filter": {
        "match_all": {}
      }
    }
  },
  "aggs": {
      "categories": {                        
          "nested": {
              "path": "categories"
          },
          "aggs": {
              "filteredcategories": {
                  "filter": {
                      "bool": {
                          "must": {
                              "term": { "categories.type": 1}
                          }                          
                      }
                  },
                  "aggs": {
                      "top_categories": {
                          "terms": {
                              "field": "categories.id"
                          },
                          "aggs": {
                              "top_items": {
                                  "reverse_nested": {                                
                                  },
                                  "aggs": {                          
                                      "top_items_per_categories": {                              
                                          "top_hits": {                   
                                              "sort": [
                                                {
                                                  "categories.rank": {
                                                    "order": "asc",
                                                    "mode": "max"
                                                  }
                                                }                                                
                                              ]
                                              }
                                          }
                                      }
                                  }
                              }
                          }
                      }
                  }
                }
          }
      }    
    }
}
```

Ultimately, I want to sort by the category rank of the current bucket but I don't know how to refer to the bucket id inside the sort clause.
Anyways, sorting by any nested property doesn't work (sort doesn't apply). However, sorting by a base property of item do work correctly.

Is this a bug or just me doing my query the wrong way ?

Thanks
</description><key id="105581535">13420</key><summary>Nested sort inside a top hits inside a reverse nested doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ericrenard</reporter><labels /><created>2015-09-09T11:54:45Z</created><updated>2015-09-10T10:38:12Z</updated><resolved>2015-09-10T10:38:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-09T16:16:19Z" id="138960574">@ericrenard if you define the `nested_path` option and set it to `categories` then it work as expected:

``` json
{
  "size": 0,
  "query": {
    "filtered": {
      "filter": {
        "match_all": {}
      }
    }
  },
  "aggs": {
    "categories": {
      "nested": {
        "path": "categories"
      },
      "aggs": {
        "filteredcategories": {
          "filter": {
            "bool": {
              "must": {
                "term": {
                  "categories.type": 1
                }
              }
            }
          },
          "aggs": {
            "top_categories": {
              "terms": {
                "field": "categories.id"
              },
              "aggs": {
                "top_items": {
                  "reverse_nested": {},
                  "aggs": {
                    "top_items_per_categories": {
                      "top_hits": {
                        "sort": [
                          {
                            "categories.rank": {
                              "order": "asc",
                              "mode": "max",
                              "nested_path": "categories"
                            }
                          }
                        ]
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

The problem is that nested sorting inside `top_hits` can't detect automatically with the nested path should be. Also there are cases where the `nested_path` is selected incorrectly in a regular search request.

I think the automatic detection of the nested path should be removed, because nested sorting is clearer when you tell it what to do. There is a TODO in the code, but I lost track of it. 
</comment><comment author="ericrenard" created="2015-09-09T20:07:23Z" id="139031615">Yeah, you're right, it works with nested_path, thanks !
Only thing left for me to figure out is if I can reference the current bucket key in the top hits sort, ideally I want to sort by the rank of the category of the current bucket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get count of values of field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13419</link><project id="" key="" /><description>I want to get the count of the values for a given field. Assuming I have logs index, and  field "response_code" will have different values 200,404,502,503. Is there a way to get the number of values for response_code, i.e. a query which will say 4 different values for response_code.

Thank you
</description><key id="105579666">13419</key><summary>Get count of values of field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rakesh91</reporter><labels /><created>2015-09-09T11:40:07Z</created><updated>2015-09-09T20:32:03Z</updated><resolved>2015-09-09T20:32:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-09T20:27:20Z" id="139036460">I believe what you're looking for is the [cardinality aggregation](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html).  Does that get you what you're looking for?
</comment><comment author="rakesh91" created="2015-09-09T20:32:02Z" id="139037458">@eskibars Thanks a lot. I was looking for the same.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove support for deprecated queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13418</link><project id="" key="" /><description>This removes support for the `and`, `or`, `fquery`, `limit` and `filtered`
queries. `query` is still supported until #13326 is fixed.
</description><key id="105575530">13418</key><summary>Remove support for deprecated queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Query DSL</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T11:11:23Z</created><updated>2016-03-10T18:30:16Z</updated><resolved>2015-09-10T08:39:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-09T12:18:00Z" id="138890868">left one comment, LGTM besides that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_source false ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13417</link><project id="" key="" /><description>Hi.
elasticsearch 1.7.1
According to documentation https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html
**ES config**
node.name: "m1"
node.local: false
script.inline: off
script.indexed: off
node.master: true
node.data: true
node.max_local_storage_nodes: 1
index.number_of_shards: 3
index.number_of_replicas: 1
bootstrap.mlockall: true
network.host: 0.0.0.0
transport.tcp.port: 9300
http.port: 9200
http.enabled: true
http.cors.enabled: true
gateway.type: local
gateway.recover_after_nodes: 1
gateway.recover_after_time: 5m
gateway.expected_nodes: 2
discovery.zen.minimum_master_nodes: 1
discovery.zen.ping.timeout: 3s
discovery.zen.ping.multicast.enabled: false
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.fetch.warn: 1s
index.indexing.slowlog.threshold.index.warn: 10s
index.indexing.slowlog.threshold.index.info: 5s

**I tried simple test:**

```
PUT /tweets
{
  "mappings": {},
  "employee": {
    "_source": {
      "enabled": false
    }
  }
}
```

**Success:**

```
{
  "acknowledged": true
}
```

**Then add document**

```
PUT /tweets/employee/1
{
  "first_name" :  "Douglas",
  "last_name" :   "Fir",
  "age" :         35,
  "about":        "I like to build cabinets",
  "interests":  [ "forestry" ]
}
```

**As it's specified in docs, disabling _source will disable UPDATE API**

Ok, I ran:

```
POST /tweets/employee/1/_update
{
  "doc" : {
    "age": 58
  }
}
```

and this query was successfull
Checking:

```
GET /tweets/employee/1
```

**Result**

```
{
  "_index": "tweets",
  "_type": "employee",
  "_id": "1",
  "_version": 3,
  "found": true,
  "_source": {
    "first_name": "Douglas",
    "last_name": "Fir",
    "age": 58,
    "about": "I like to build cabinets",
    "interests": [
      "forestry"
    ]
  }
}
```

**"age": 58** - it was changed.

Also, when I do simple GET of this doc, I see _source field:

```
{
  "_index": "tweets",
  "_type": "employee",
  "_id": "1",
  "_version": 2,
  "found": true,
  "_source": {
    "first_name": "Douglas",
    "last_name": "Fir",
    "age": 58,
    "about": "I like to build cabinets",
    "interests": [
      "forestry"
    ]
  }
}
```

It seems disabling of _source field now working
</description><key id="105573379">13417</key><summary>_source false ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">izenk</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-09T10:56:07Z</created><updated>2015-09-11T15:02:55Z</updated><resolved>2015-09-11T15:02:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eskibars" created="2015-09-09T20:21:38Z" id="139035236">You've identified an issue with the documentation.  The example provided there is wrong.  It should not be

```
PUT tweets
{
  "mappings": {},
  "tweet": {
    "_source": {
      "enabled": false
    }
  }
}
```

It should be

```
PUT tweets
{
  "mappings": {
    "tweet": {
      "_source": {
        "enabled": false
      }
    }
  }
}
```
</comment><comment author="eskibars" created="2015-09-09T20:22:36Z" id="139035447">Sorry... did not mean to accidentally close this.  Reopening for a fix to docs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Protected against large size option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13416</link><project id="" key="" /><description>PR for #13394
</description><key id="105565767">13416</key><summary>Protected against large size option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Inner Hits</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-09T10:07:57Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-09T11:14:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-09T10:09:32Z" id="138863196">LGTM
</comment><comment author="martijnvg" created="2015-09-09T11:14:39Z" id="138878399">pushed via: https://github.com/elastic/elasticsearch/commit/65e7aba780c058649064627163a45f1c90065f1f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>org.apache.lucene.util missing from elasticsearch-2.0.0.beta1 jar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13415</link><project id="" key="" /><description>Hi,
I'm not sure what the intention is regarding wrapping Lucene packages, but the util package (at least?), which is used by Environment is missing from the jar.
Best, Dan.
</description><key id="105552068">13415</key><summary>org.apache.lucene.util missing from elasticsearch-2.0.0.beta1 jar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fakeh</reporter><labels><label>discuss</label><label>feedback_needed</label></labels><created>2015-09-09T09:01:37Z</created><updated>2015-09-09T14:45:04Z</updated><resolved>2015-09-09T13:58:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-09T09:20:56Z" id="138851701">I don't understand. 

Environment class uses `org.apache.lucene.util.Constants` which is part of Lucene core.

In which context are you seeing an error? Can you describe what you are doing?
</comment><comment author="fakeh" created="2015-09-09T09:26:47Z" id="138852733">Hi dadoonet, 
elasticsearch-2.0.0-beta1.jar shades most of the packages from org.apache.lucence except util. Since Environment references Constants, the ES jar is insufficient on its own to deploy (so what's the point in shading the Lucene dependencies?).

Specifically, I'm trying to load the ES Java client in Karaf 4.0.1 (an OSGi container) and am encountering:

```
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.env.Environment
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:97)
    at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:70)
    at org.elasticsearch.client.transport.TransportClient$Builder.build(TransportClient.java:114)
```

I hoped that it was because Environment wasn't able to load the Constants class, which it references statically, but even after providing the util package the class still does not load.

Best, Dan.
</comment><comment author="dadoonet" created="2015-09-09T09:33:49Z" id="138854967">Are you using the shaded version of elasticsearch 2.0.0-beta1? What is exactly your dependency?

Note that we won't provide a shaded version anymore in 2.0.0: https://github.com/elastic/elasticsearch/pull/13252
</comment><comment author="fakeh" created="2015-09-09T09:46:52Z" id="138858826">I wasn't aware there was an unshaded version. From Maven Central, as per the blog about the beta:

```
    &lt;dependency&gt;
        &lt;groupId&gt;org.elasticsearch&lt;/groupId&gt;
        &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
        &lt;version&gt;2.0.0-beta1&lt;/version&gt;
    &lt;/dependency&gt;
```

If you're not shading future releases then this issue shouldn't continue to be a problem :) If I can't figure out this NoClassDefFoundError then I'll make a separate issue.

Do you have some documentation of the dependencies? There's rather a lot of them and when constructing an OSGi environment it can be painful to figure out which are required and which are optional.

PS hopefully the ServiceMix wizards will pick up ES 2.0.0 ASAP!

Best, Dan.
</comment><comment author="dadoonet" created="2015-09-09T09:49:54Z" id="138859461">The version you are using is not the shaded version. So it should work fine. I think you have another issue here.
</comment><comment author="fakeh" created="2015-09-09T09:55:33Z" id="138860518">Here's the jar in central: http://search.maven.org/remotecontent?filepath=org/elasticsearch/elasticsearch/2.0.0-beta1/elasticsearch-2.0.0-beta1.jar 

It's got org.apache.lucene (except util) and joda.time packages in it.

Dan
</comment><comment author="dadoonet" created="2015-09-09T10:01:34Z" id="138861572">I understand now what you are saying. This won't change in 2.0.0.

Yes we have specific versions of some Lucene classes in our repository.
But you still need to add Lucene and other dependencies by yourself.

If you are using maven, depending on elasticsearch-2.0.0-beta1 should pull lucene-core where the class you are looking for is located.

I think your classpath is incorrect here.

I don't see that as an issue on our side.
</comment><comment author="fakeh" created="2015-09-09T10:13:35Z" id="138864768">As I said initially, I'm not sure of your intentions regarding bundling these packages. I wonder why it's beneficial to have specific packages included if the resultant jar is not then independent from requiring the source jars in addition?

`I think your classpath is incorrect here.`

Absolutely. But I do think that Elastic are in the best position to help issues when moving from Maven dependency resolution to other systems (production, CI, etc).

PS Is there some other meaning of shading packages, hence our confusion?
</comment><comment author="dadoonet" created="2015-09-09T10:29:39Z" id="138867415">&gt; PS Is there some other meaning of shading packages, hence our confusion?

We used to shade some dependencies such as JODA, guava, jackson... and relocate all packages.
That's what we call "shading".

&gt; But I do think that Elastic are in the best position to help issues when moving from Maven dependency resolution to other systems (production, CI, etc).

I'd like to understand what the fix would look like for you. Is there anything wrong with OSGi if a project has classes located either in org.apache.lucene and ore.elasticsearch?
Do we need to add any META information? 
</comment><comment author="fakeh" created="2015-09-09T10:55:03Z" id="138875107">&gt; I'd like to understand what the fix would look like for you.

Ideally, ES would be built with OSGi manifest headers from source! This isn't terribly hard as part of the bundle's release cycle but does require some understanding of tooling and some testing, but can be burdensome for us third parties to keep up with releases, reorganising the jars and make some assumptions and guesses about the original intention of the library's organisation. ServiceMix has been doing this for you for [some time](http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.servicemix.bundles%22%20AND%20a%3A%22org.apache.servicemix.bundles.elasticsearch%22) and looking at their POMs would be a great place to start if you were to take this route.

In lieu of OSGi support at source, I would recommend not including third party libraries within your jars at all giving us the flexibility to supply our own versions via OSGi resolution.

&gt; Is there anything wrong with OSGi if a project has classes located either in org.apache.lucene and ore.elasticsearch?

Whilst it is not an insurmountable problem, having resolution of some org.apache.lucene packages within the jar and some without just seems conceptually weird given that the one(s?) without are still required.

More generally, keeping the dependencies minimal would be appreciated, perhaps by splitting ES into sub-functionalities?

I'm in a rush to be elsewhere so this isn't a super-informative post, but if you're interested in more I can try to be of help again later.

Best, Dan.
</comment><comment author="rmuir" created="2015-09-09T12:32:21Z" id="138895176">-1 to OSGI. I don't even need to explain why.
</comment><comment author="fakeh" created="2015-09-09T13:26:38Z" id="138908408">&gt; -1 to OSGI. I don't even need to explain why.

I think this issue has become a bit side-tracked with OSGi's involvement. The more general question, I think, is whether to include lucene's .util or not.
I would, as above, love to see ES distribute OSGi compatible libraries, but it wasn't the original intent of the issue.

Dan.
</comment><comment author="dadoonet" created="2015-09-09T13:58:04Z" id="138917313">lucene .util package is available in lucene core lib as far as I know. You should add it as a dependency in your project.

If it's not possible, then I think you have no other choice to shade yourself elasticsearch and its dependencies within the same final jar as it was done by ServiceMix if I read correctly their pom file.

I documented in the issue (https://github.com/elastic/elasticsearch/pull/13252) how to do that.

Back to the original question: we are not going to include lucene .util in elasticsearch jar.

Closing. But feel free to add other comments here or on discuss.elastic.co where other members of the community could also share other solutions/ideas around that.
</comment><comment author="fakeh" created="2015-09-09T14:45:04Z" id="138933523">Having explored more, I see that you're not selecting packages to include, but using the class path to allow you to override individual classes in Lucene and in Joda.

At least that's that question answered.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Engine: refresh before translog commit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13414</link><project id="" key="" /><description>When we commit the translog, documents that were in it before cannot be retrieved from
it anymore via get and have to be retrieved from the index instead. But they will only
be visible if between index and get a refresh is called. Therfore we have to call
first refresh and then translog.commit() because otherwise there is a small gap
in which we cannot read from the translog anymore but also not from the index.

closes #13379
</description><key id="105543587">13414</key><summary>Engine: refresh before translog commit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Core</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-09T08:30:01Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-09T10:06:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-09T08:35:06Z" id="138828049">left minor commetns LGTM 
</comment><comment author="brwe" created="2015-09-09T08:42:23Z" id="138830403">@s1monw thanks! addressed all comments
</comment><comment author="mikemccand" created="2015-09-09T08:48:10Z" id="138832498">Is there still a race here?  I.e. after `refresh` but before `translog.commit`, another thread indexes some documents, but these documents are not visible via realtime get until the next refresh?
</comment><comment author="brwe" created="2015-09-09T09:31:34Z" id="138853837">@mikemccand good point. but if I understand it correctly we actually create a new translog before we refresh in `translog.prepareCommit()` and then in `translog.commit()` just delete the old one. Documents that came in between `refresh()` and `translog.commit()` will be in the new translog and therefore are still available from there after commit. Only docs that were in translog before `prepareCommit()` are not available from translog anymore but they can be retrieved from index. Let me know if this makes sense, I might just miss something.
</comment><comment author="mikemccand" created="2015-09-09T09:34:18Z" id="138855197">&gt; but if I understand it correctly we actually create a new translog before we refresh in `translog.prepareCommit()`

Aha!  Sorry, I missed the `prepareCommit` call ... so you are correct!  Thanks :)
</comment><comment author="mikemccand" created="2015-09-09T09:49:35Z" id="138859418">LGTM
</comment><comment author="s1monw" created="2015-09-09T10:05:43Z" id="138862564">LGTM 2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term Filter: `now` operator - documentation missing?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13413</link><project id="" key="" /><description>It seems up from version 1.7, the `now` operator is no longer accepted. I haven't been able to find any documentation for this though. Did I overlook something? And is that a correct assumption?

Thanks!
</description><key id="105537858">13413</key><summary>Term Filter: `now` operator - documentation missing?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">whythecode</reporter><labels /><created>2015-09-09T07:48:50Z</created><updated>2015-09-09T07:54:01Z</updated><resolved>2015-09-09T07:54:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="whythecode" created="2015-09-09T07:54:01Z" id="138816329">Hmmm - actually, I think I went crazy for a second there... it probably never actually worked.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Gateway: allow overriding the Gateway implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13412</link><project id="" key="" /><description>Allows mocking the gateway and overriding it from plugins.
</description><key id="105534763">13412</key><summary>Gateway: allow overriding the Gateway implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>non-issue</label><label>v2.0.0-beta2</label></labels><created>2015-09-09T07:19:45Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-09T11:21:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2015-09-09T08:11:39Z" id="138819521">minor things, LGTM apart from that. Maybe you can add a `why` to your commit in addition to the `what`, so later generations will enjoy reading why we needed this...
</comment><comment author="bleskes" created="2015-09-09T10:14:11Z" id="138865005">Thx @rjernst @spinscale . The new utilities simplified the tests quite a bit. I pushed an update.
</comment><comment author="rjernst" created="2015-09-09T10:36:02Z" id="138868356">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>can not get document id in Scripted Metric Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13411</link><project id="" key="" /><description>I have same error data in ES. 
The error field is word_count. Its value is calculated by field(sentence).
so I want to get these document id ,then use bulk to update.
use  Scripted Metric Aggregation  I can get _index and _type   ,but can not get _id.
- for example I can get _index and _type

``` json
 {
    "size":0,
    "aggs":{
      "err_ids": {
        "scripted_metric": {
           "init_script" : "_agg[\"count\"]=0;_agg[\"err_ids\"]=[];",
           "map_script" : "if(_agg[\"count\"]==0){_agg[\"err_ids\"].add([doc._index,doc._type]);_agg[\"count\"]=1}",
           "combine_script" : "return _agg[\"err_ids\"]",
           "reduce_script" : "return _aggs"
                          }
                 }
    }
 }
{
  "took" : 38,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 32448,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "err_ids" : {
      "value" : [ [ [ [ "als-552b873f6d6f6e176d040000-2015.09.07" ], [ "als_production" ] ] ] ]
    }
  }
}
```
- but the _id is null

``` json
{
    "size":0,
    "aggs":{
      "err_ids": {
        "scripted_metric": {
           "init_script" : "_agg[\"count\"]=0;_agg[\"err_ids\"]=[];",
           "map_script" : "if(_agg[\"count\"]==0){_agg[\"err_ids\"].add([doc._id,doc._id.value]);_agg[\"count\"]=1}",
           "combine_script" : "return _agg[\"err_ids\"]",
           "reduce_script" : "return _aggs"
                          }
                 }
    }
 }
{
  "took" : 43,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 32448,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "err_ids" : {
      "value" : [ [ [ [ ], null ] ] ]
    }
  }
}
```

now I use script filter get these ids ,but it is not convenient to use

``` json
 {
    "size":10,
    "fields":[],
    "filter":{
      "script": {
           "script" : "(doc[\"word_count\"].value != doc[\"sentence\"].value.toString().split(/\\s+/).length)"
                 }
    }
 }
{
  "took" : 108,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 32346,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "als-xxxxxxxxxxxxx-2015.09.07",
      "_type" : "als_production",
      "_id" : "AU-oW1n_q-7mSo1II5ZH",
      "_score" : 1.0
    },
   ......&#65341;
 }
}
```
</description><key id="105528447">13411</key><summary>can not get document id in Scripted Metric Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">759803573</reporter><labels /><created>2015-09-09T06:22:15Z</created><updated>2015-09-11T05:28:58Z</updated><resolved>2015-09-11T05:28:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="759803573" created="2015-09-11T05:28:55Z" id="139456018">I use dynamie templates,don't save id.so I can not get id.
it can use _uid to get type and id
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix compiler warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13410</link><project id="" key="" /><description>We have a handful of compiler warnings, mostly because of passing an
array to varargs methods. This change fixes these warnings and adds
-Werror so we don't get anymore of these warnings.

Note this does _not_ enable deprecation or unchecked type warnings, so
these remain "hidden". We should work towards removing those as well,
but this is a first step.
</description><key id="105521130">13410</key><summary>Fix compiler warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T05:18:47Z</created><updated>2015-09-09T21:05:12Z</updated><resolved>2015-09-09T21:04:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-09T05:23:11Z" id="138788531">Looks great!

&gt; Note this does not enable deprecation or unchecked type warnings, so
&gt; these remain "hidden". We should work towards removing those as well,
&gt; but this is a first step.

Good, it at least prevents warnings of other types from stacking up. How bad is the unchecked situation? We should really open a followup for that one.

Deprecation is more complicated, especially if we want to deprecate our own apis. That one is less important I think.
</comment><comment author="rjernst" created="2015-09-09T05:24:56Z" id="138789205">&gt; How bad is the unchecked situation? 

Hundreds of warnings...well, at least 100
</comment><comment author="rmuir" created="2015-09-09T05:38:29Z" id="138790597">Can we also turn on -Xlint ? I don't care if we then have to add e.g. -Xlint:-serial to disable some of the stupid ones, or ones that need fixes, but it would be nice to know where we stand.
</comment><comment author="rjernst" created="2015-09-09T07:22:28Z" id="138811037">I turned on -Xlint and disabled the frequent ones I found, and fixed a handful of easy ones. There are some tricky ones we should really look into...

However, while core main/test now passes compile, I get a failure compiling tests for distribution/tar. But there is no warning, or any error at all... @dadoonet Could you try my branch and see if you understand why? I've run with -X but I don't see anything that explains why the failure occurred.
</comment><comment author="s1monw" created="2015-09-09T07:24:44Z" id="138811353">&gt; However, while core main/test now passes compile, I get a failure compiling tests for distribution/tar. But there is no warning, or any error at all... @dadoonet Could you try my branch and see if you understand why? I've run with -X but I don't see anything that explains why the failure occurred.

this is likely your JVM / Compiler we had problems her with `java version "1.8.0_31"` so if you run that I'd upgrade
</comment><comment author="s1monw" created="2015-09-09T07:48:32Z" id="138814851">this is really weird if you run `mvn clean test-compile -X` you get the classpath it uses etc. to pass to `javac`. If I do this manually I get the following error:

```
warning: [path] bad path element "/Users/simon/projects/elasticsearch/distribution/tar/target/classes": no such file or directory
error: warnings found and -Werror specified
1 error
1 warning
```

creating `target/classes` fixes the issue
</comment><comment author="s1monw" created="2015-09-09T07:54:17Z" id="138816374">hmm without `-Xlint:all` it passes
</comment><comment author="dadoonet" created="2015-09-09T08:26:27Z" id="138822834">Funny. The same is happening in plugins but not for all plugins...
Analysis are working fine. Cloud-azure is failing... 

```
warning: [path] bad path element "/Users/dpilato/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-api.jar": no such file or directory
warning: [path] bad path element "/Users/dpilato/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/activation.jar": no such file or directory
warning: [path] bad path element "/Users/dpilato/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jsr173_1.0_api.jar": no such file or directory
warning: [path] bad path element "/Users/dpilato/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb1-impl.jar": no such file or directory
error: warnings found and -Werror specified
1 error
4 warnings
```

Looking at this ATM.
</comment><comment author="dadoonet" created="2015-09-09T08:40:58Z" id="138829912">@rjernst Adding `&lt;arg&gt;-Xlint:-path&lt;/arg&gt;` fixes the issue.

And BTW this is cool stuff because now we can see some cloud-azure code issues! :)

```
[INFO] -------------------------------------------------------------
[WARNING] COMPILATION WARNING : 
[INFO] -------------------------------------------------------------
[WARNING] /Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceSimpleMock.java:[50,23] [static] static variable should be qualified by type name, AzureModule, instead of by an expression
[WARNING] /Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AbstractAzureRepositoryServiceTestCase.java:[50,23] [static] static variable should be qualified by type name, AzureModule, instead of by an expression
[WARNING] /Users/dpilato/Documents/Elasticsearch/dev/es-master/elasticsearch/plugins/cloud-azure/src/test/java/org/elasticsearch/cloud/azure/AzureComputeServiceTwoNodesMock.java:[54,23] [static] static variable should be qualified by type name, AzureModule, instead of by an expression
[INFO] 3 warnings 
```
</comment><comment author="dadoonet" created="2015-09-09T08:50:07Z" id="138834158">Just wondering if we should add a maven property like 

``` xml
&lt;lint.level&gt;-Werror&lt;/lint.level&gt;
```

And then define instead of `&lt;arg&gt;-Werror&lt;/arg&gt;`:

``` xml
&lt;arg&gt;${xlint.level}&lt;/arg&gt;
```

So we could potentially run: `mvn clean install -Dlint.level=` which will not stop the build but WARN errors
</comment><comment author="dadoonet" created="2015-09-09T08:58:33Z" id="138839758">@rjernst If you don't want to bypass path checks for each module, then add in parent pom:

```
&lt;properties&gt;
    &lt;xlint.path&gt;&lt;/xlint.path&gt;
&lt;/properties&gt;
```

And:

```
                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                    &lt;version&gt;3.3&lt;/version&gt;
                    &lt;configuration&gt;
                        &lt;fork&gt;true&lt;/fork&gt;
                        &lt;maxmem&gt;512m&lt;/maxmem&gt;
                        &lt;!-- REMOVE WHEN UPGRADE:
                             see https://jira.codehaus.org/browse/MCOMPILER-209 it's a bug where
                             incremental compilation doesn't work unless it's set to false causeing
                             recompilation of the entire codebase each time without any changes. Should
                             be fixed in version &gt; 3.1
                         --&gt;
                        &lt;useIncrementalCompilation&gt;false&lt;/useIncrementalCompilation&gt;
                        &lt;showWarnings&gt;true&lt;/showWarnings&gt;
                        &lt;compilerArgs&gt;
                            &lt;arg&gt;-XDignore.symbol.file&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:all&lt;/arg&gt;
                            &lt;arg&gt;${xlint.path}&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-cast&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-deprecation&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-fallthrough&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-overrides&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-rawtypes&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-serial&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-try&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-unchecked&lt;/arg&gt;
                            &lt;arg&gt;-Werror&lt;/arg&gt;
                        &lt;/compilerArgs&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
```

In distribution/pom.xml:

```
&lt;properties&gt;
    &lt;xlint.path&gt;-Xlint:-path&lt;/xlint.path&gt;
&lt;/properties&gt;
```

Same in cloud-azure (and may be some others)...
</comment><comment author="rjernst" created="2015-09-09T08:59:34Z" id="138840646">&gt; Just wondering if we should add a maven property

No, I don't think we should do that, because the next step is "let's set the default back to warn". This PR fixes issues, we don't want to go backwards from there. And there are some we really need to investigate (see some of the TODOs I added in this).
</comment><comment author="rjernst" created="2015-09-09T09:03:04Z" id="138842926">@dadoonet thanks for the suggestion on parameterizing -Xling:-path. I actually want to just be able to add additional compiler args within a child pom, eg core. This way the warning removals can be isolated to the modules that need them, instead of allowing leniency in all modules.
</comment><comment author="dadoonet" created="2015-09-09T09:14:37Z" id="138850172">@rjernst then add in parent pom:

``` xml
&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
    &lt;version&gt;3.3&lt;/version&gt;
    &lt;configuration&gt;
        &lt;fork&gt;true&lt;/fork&gt;
        &lt;maxmem&gt;512m&lt;/maxmem&gt;
        &lt;!-- REMOVE WHEN UPGRADE:
             see https://jira.codehaus.org/browse/MCOMPILER-209 it's a bug where
             incremental compilation doesn't work unless it's set to false causeing
             recompilation of the entire codebase each time without any changes. Should
             be fixed in version &gt; 3.1
         --&gt;
        &lt;useIncrementalCompilation&gt;false&lt;/useIncrementalCompilation&gt;
        &lt;showWarnings&gt;true&lt;/showWarnings&gt;
        &lt;compilerArgs&gt;
            &lt;arg&gt;-XDignore.symbol.file&lt;/arg&gt;
            &lt;arg&gt;-Xlint:all&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-path&lt;/arg&gt;
            &lt;arg&gt;-Werror&lt;/arg&gt;
        &lt;/compilerArgs&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
```

In core/pom.xml:

``` xml
&lt;plugin&gt;
    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;compilerArgs&gt;
            &lt;arg&gt;-XDignore.symbol.file&lt;/arg&gt;
            &lt;arg&gt;-Xlint:all&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-path&lt;/arg&gt;
            &lt;arg&gt;-Werror&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-cast&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-deprecation&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-fallthrough&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-overrides&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-rawtypes&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-serial&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-try&lt;/arg&gt;
            &lt;arg&gt;-Xlint:-unchecked&lt;/arg&gt;
        &lt;/compilerArgs&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
```

But you don't really inherit from parent pom. You have to declare again all compilerArgs.
</comment><comment author="dadoonet" created="2015-09-09T09:53:36Z" id="138860150">Ok so here is a better solution.

In `/pom.xml`:

``` xml
    &lt;properties&gt;
        &lt;xlint.options/&gt;
    &lt;/properties&gt;

...

                &lt;plugin&gt;
                    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                    &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;
                    &lt;version&gt;3.3&lt;/version&gt;
                    &lt;configuration&gt;
                        &lt;fork&gt;true&lt;/fork&gt;
                        &lt;maxmem&gt;512m&lt;/maxmem&gt;
                        &lt;!-- REMOVE WHEN UPGRADE:
                             see https://jira.codehaus.org/browse/MCOMPILER-209 it's a bug where
                             incremental compilation doesn't work unless it's set to false causeing
                             recompilation of the entire codebase each time without any changes. Should
                             be fixed in version &gt; 3.1
                         --&gt;
                        &lt;useIncrementalCompilation&gt;false&lt;/useIncrementalCompilation&gt;
                        &lt;showWarnings&gt;true&lt;/showWarnings&gt;
                        &lt;compilerArgs&gt;
                            &lt;arg&gt;-XDignore.symbol.file&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:all&lt;/arg&gt;
                            &lt;arg&gt;-Xlint:-path&lt;/arg&gt;
                            &lt;arg&gt;${xlint.options}&lt;/arg&gt;
                            &lt;arg&gt;-Werror&lt;/arg&gt;
                        &lt;/compilerArgs&gt;
                    &lt;/configuration&gt;
                &lt;/plugin&gt;
```

In `core/pom.xml`:

```
    &lt;properties&gt;
        &lt;xlint.options&gt;-Xlint:-cast,-deprecation,-fallthrough,-overrides,-rawtypes,-serial,-try,-unchecked&lt;/xlint.options&gt;
    &lt;/properties&gt;
```

That said, there are also some deprecated classes in plugins, so I would also add in parent pom `-deprecation` option.
</comment><comment author="rjernst" created="2015-09-09T19:48:49Z" id="139026325">@dadoonet @s1monw  Thanks for maven help. I've added suppressions and ignores specific to each module where needed. I think this is ready.
</comment><comment author="rjernst" created="2015-09-09T21:04:48Z" id="139045212">Pushing as this really hasn't changed other than limiting which xlint options are used.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove and forbid use of com.google.common.base.Throwables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13409</link><project id="" key="" /><description>This commit removes and now forbids all uses of
`com.google.common.base.Throwables` across the codebase.

For uses of `com.google.common.base.Throwables#getStackTraceAsString`,
use `org.elasticsearch.ExceptionsHelper#stackTrace`.

Relates #13224
</description><key id="105502534">13409</key><summary>Remove and forbid use of com.google.common.base.Throwables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-09T01:57:43Z</created><updated>2015-09-09T02:19:58Z</updated><resolved>2015-09-09T01:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-09T01:59:07Z" id="138752979">LGTM
</comment><comment author="jasontedor" created="2015-09-09T01:59:56Z" id="138753081">Thanks for the expedient review @nik9000.
</comment><comment author="nik9000" created="2015-09-09T02:04:40Z" id="138753633">&gt; Thanks for the expedient review @nik9000.

Sure! It was super small and I trust you tried it and made sure the output was good enough so I didn't have to download it and do the same.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split HasParentQueryParser into toQuery and formXContent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13408</link><project id="" key="" /><description>This commit splits HasParentQueryParser into toQuery and fromXContent.
This change also deprecates several keys in favor of simplified settings
and adds basic unittests for HasParentQueryParser.
</description><key id="105461198">13408</key><summary>Split HasParentQueryParser into toQuery and formXContent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-08T20:19:25Z</created><updated>2015-09-09T10:18:58Z</updated><resolved>2015-09-09T10:18:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-09T09:55:24Z" id="138860489">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable security manager by default in tests (e.g. IDEs)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13407</link><project id="" key="" /><description>Otherwise people will be confused when they use maven.
</description><key id="105457528">13407</key><summary>Enable security manager by default in tests (e.g. IDEs)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>test</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-08T19:56:31Z</created><updated>2015-09-30T03:10:11Z</updated><resolved>2015-09-09T13:12:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-08T19:58:22Z" id="138683112">LGTM
</comment><comment author="rmuir" created="2015-09-30T03:10:11Z" id="144269706">I just backported this to 2.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature Request] Add NoMatchQuery to support better templating experience</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13406</link><project id="" key="" /><description>Consider the following query, which is constructed using Mustache on the client side:

```
                            {
                                "bool": {
                                    "minimum_should_match":1,
                                    "should":[
                                        {{#someCondition}}
                                        {"terms": {
                                            "field1": {{vals1}}
                                        }},
                                        {{/someCondition}}
                                        {{#hasIp}}
                                        { "terms": {
                                            "ips": {{ips}}
                                        }}
                                        {{/hasIp}}
                                    ]
                                }
                            }
```

When hasIp &amp;&amp; someCondition or !hasIp &amp;&amp; !someCondition everything works as expected (because `minimum_number_should_match` causes empty should clause to effectively be a NoMatchQuery). For the case where !hasIp &amp;&amp; someCondition, this Mustache template is going to render invalid JSON (trailing comma right before a closing bracket).

A possible solution works something like this:

```
                            {
                                "bool": {
                                    "minimum_should_match":1,
                                    "should":[
                                        {{#someCondition}}
                                        {"terms": {
                                            "field1": {{vals1}}
                                        }},
                                        {{/someCondition}}
                                        {{#hasIp}}
                                        { "terms": {
                                            "ips": {{ips}}
                                        }},
                                        {{/hasIp}}
                                        {"term": {
                                            "emails": "no-such-email:a-match-none-query"
                                        }}
                                    ]
                                }
                            }
```

Adding a query that translates to definitive no-match. It can be domain specific (e.g. looking for inexistent email) or an empty boolean clause that translates to no results.

It will be really useful to have a NoMatchQuery type to complement MatchAllQuery, this will allow writing templates like the one above much easier, and without resorting to query types that actually cost in index lookups.

As always, I'll be happy to provide a PR once all required details have been agreed upon.
</description><key id="105456869">13406</key><summary>[Feature Request] Add NoMatchQuery to support better templating experience</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>enhancement</label></labels><created>2015-09-08T19:52:19Z</created><updated>2015-09-19T12:55:03Z</updated><resolved>2015-09-19T12:55:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2015-09-15T18:05:16Z" id="140485529">@eskibars this is not just a search templates requirement  - it's a general one related to the query DSL, although our use case is as described
</comment><comment author="eskibars" created="2015-09-15T22:08:39Z" id="140560531">@synhershko fair enough although I can't think of any real use case outside of templating
</comment><comment author="synhershko" created="2015-09-15T22:10:56Z" id="140560914">A no-op query for example - it will be more syntactically correct to issue one with NoMatchQuery than with a hack build around boolean should's.

A no-op query is sometimes needed when you have no way in your system to avoid the query but need no results back.
</comment><comment author="javanna" created="2015-09-16T08:50:34Z" id="140675321">heads up, while working on refactoring queries so that we can parse them on the coordinating node and stream them to the data nodes, the same requirement came up internally. We went ahead and added the `match_none` query then. It is currently in the query-refactoring branch which will be merged to master soon. That means that elasticsearch 3.0 will include this query.
</comment><comment author="synhershko" created="2015-09-16T08:51:42Z" id="140675520">@javanna hurray! any chance for backporting this to ES 2.0 GA ? :)
</comment><comment author="s1monw" created="2015-09-16T08:52:48Z" id="140675704">@synhershko 2.0 GA is frozen
</comment><comment author="synhershko" created="2015-09-16T08:53:48Z" id="140675865">Ok, back-porting this to 2.x will be highly appreciated
</comment><comment author="clintongormley" created="2015-09-19T12:55:02Z" id="141661914">@synhershko why not just replace:

```
{{#hasIp}}
{ "terms": { "ips": {{ips}}}},
{{/hasIp}}
```

with:

```
{ "terms": { "ips": {{ips}}}},
```

Any empty terms list acts as a no-match query already

I don't think there is any more to do here, so closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactors MultiMatchQueryBuilder and Parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13405</link><project id="" key="" /><description>Relates to #10217

This PR is against the query-refactoring branch.
</description><key id="105455063">13405</key><summary>Refactors MultiMatchQueryBuilder and Parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/cbuescher/following{/other_user}', u'events_url': u'https://api.github.com/users/cbuescher/events{/privacy}', u'organizations_url': u'https://api.github.com/users/cbuescher/orgs', u'url': u'https://api.github.com/users/cbuescher', u'gists_url': u'https://api.github.com/users/cbuescher/gists{/gist_id}', u'html_url': u'https://github.com/cbuescher', u'subscriptions_url': u'https://api.github.com/users/cbuescher/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/10398885?v=4', u'repos_url': u'https://api.github.com/users/cbuescher/repos', u'received_events_url': u'https://api.github.com/users/cbuescher/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/cbuescher/starred{/owner}{/repo}', u'site_admin': False, u'login': u'cbuescher', u'type': u'User', u'id': 10398885, u'followers_url': u'https://api.github.com/users/cbuescher/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-08T19:40:57Z</created><updated>2015-09-25T12:31:36Z</updated><resolved>2015-09-16T12:03:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-09T12:08:08Z" id="138889152">@cbuescher can you review this once you are done with the match query please?
</comment><comment author="cbuescher" created="2015-09-09T15:42:51Z" id="138951144">@alexksikes did a first round of reviews. Since this query relies much in MatchQueryBuilder which is currently WIP in #13402, there is a bit of movement here. Also, in working on the match query I first went with having many fields as optional objects, but after discussion today changed many to using primitive types plus defaults. Maybe this applies in other places for this PR as well.
</comment><comment author="alexksikes" created="2015-09-12T11:37:37Z" id="139750910">@cbuescher I rebased and addressed all the comments. Thanks for the review.
</comment><comment author="cbuescher" created="2015-09-14T18:01:05Z" id="140161014">@alexksikes left a few comments, very minor and mostly open questions on my side. Not sure if someone else should take a final look, otherwise looks good to me.
</comment><comment author="alexksikes" created="2015-09-15T13:34:55Z" id="140395078">@cbuescher I addressed all the comments. In some specific cases MatchQueryBuilder (and so does MultiMatch) fails on date fields, which is untested yet on MatchQueryBuilder. I'll open a PR for this. Thanks for the review.
</comment><comment author="cbuescher" created="2015-09-16T10:23:24Z" id="140697788">Had a look at the last changes, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove intellij jar hell leniency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13404</link><project id="" key="" /><description>Its confusing to people that we are lenient to jar hell when running under intellij: thats totally an intellij bug, but by allowing it, people think that jar hell check is "optional" or that we don't execute it in tests, which we totally do in maven and jenkins.

This leniency needs to be removed.
</description><key id="105454065">13404</key><summary>Remove intellij jar hell leniency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>blocker</label></labels><created>2015-09-08T19:34:15Z</created><updated>2015-11-30T12:51:11Z</updated><resolved>2015-09-21T07:53:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-08T19:38:47Z" id="138678879">+1 I think it makes sense to run tests in IDEs the same way as we run in maven. I will try to figure out how to fix intellij and post a guide here.
</comment><comment author="mikemccand" created="2015-09-08T19:49:10Z" id="138681141">+1
</comment><comment author="rjernst" created="2015-09-08T19:54:59Z" id="138682400">+1
</comment><comment author="s1monw" created="2015-09-09T08:28:36Z" id="138823214">running this patch with IntelliJ 14 passes without a jarhell. 

``` patch
diff --git a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
index a7b9043..d942cc2 100644
--- a/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
+++ b/core/src/test/java/org/elasticsearch/bootstrap/BootstrapForTesting.java
@@ -61,13 +61,7 @@ public class BootstrapForTesting {
         try {
             JarHell.checkJarHell();
         } catch (Exception e) {
-            if (Boolean.parseBoolean(System.getProperty("tests.maven"))) {
-                throw new RuntimeException("found jar hell in test classpath", e);
-            } else {
-                Loggers.getLogger(BootstrapForTesting.class)
-                    .warn("Your ide or custom test runner has jar hell issues, " +
-                          "you might want to look into that", e);
-            }
+            throw new RuntimeException("found jar hell in test classpath", e);
         }
```

I think we are good to go
</comment><comment author="davireis" created="2015-11-29T17:22:44Z" id="160433109">Is there a way to disable the jar hell check? I am using elasticsearch with several other libraries, and there are no simple solutions to completely get rid of jar hell in all my dependencies.  It seems that the way things are implemented, I can't even swallow the exception and move along.
</comment><comment author="rmuir" created="2015-11-29T17:23:52Z" id="160433174">No.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master branch shouldn't attempt to detect java 7 directories for JAVA_HOME</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13403</link><project id="" key="" /><description>Elasticsearch's master branch need Java 8 and so it shouldn't be looking for Java 7 directories for its Java home. See https://github.com/elastic/elasticsearch/blob/master/distribution/deb/src/main/packaging/init.d/elasticsearch#L47 for an example of a place where it does that.
</description><key id="105444503">13403</key><summary>Master branch shouldn't attempt to detect java 7 directories for JAVA_HOME</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>v5.0.0-alpha1</label></labels><created>2015-09-08T18:36:40Z</created><updated>2015-09-22T12:59:10Z</updated><resolved>2015-09-22T12:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query refactoring: MatchQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13402</link><project id="" key="" /><description>This PR separates toQuery and JSON parsing for MatchQueryBuilder and adds equals, hashcode, read/write methods. Also moving MatchQueryBuilder.Type to MatchQuery to MatchQuery, adding serialization and hashcode, equals there.

Relates to #10217
</description><key id="105425159">13402</key><summary>Query refactoring: MatchQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cbuescher</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-08T16:56:20Z</created><updated>2016-03-11T11:50:55Z</updated><resolved>2015-09-10T10:17:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-08T18:09:28Z" id="138653334">this looks great, I put some general comments where I thought it makes sense to be more strict and have less null vaules to signal not set.
</comment><comment author="javanna" created="2015-09-09T10:07:02Z" id="138862787">I reviewed this too, looks good, only thing is as Simon said I would move to primitive types whenever possible on the builder, remove null checks and set proper defaults.
</comment><comment author="cbuescher" created="2015-09-09T13:36:09Z" id="138910874">@s1monw @javanna I changed the optional values in the builder to defaults where possible, could you have another look?
</comment><comment author="javanna" created="2015-09-09T13:54:15Z" id="138916203">left couple of tiny comments, LGTM though
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows service doesn't stop properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13401</link><project id="" key="" /><description>After addressing the issues in #13247 and getting the service to start.  Stopping the service in 2.x/master no longer works either and fails with the following error:

```
2015-09-08 09:38:56 Commons Daemon procrun stderr initialized
java.lang.NoSuchMethodError: close
```

[Apache Commons Daemon procrun](http://commons.apache.org/proper/commons-daemon/procrun.html) requires a stop method (`--StopMethod`), [which use to exist](https://github.com/elastic/elasticsearch/blob/1.7/src/main/java/org/elasticsearch/bootstrap/Elasticsearch.java#L27) but has been removed.

Can this be added back?
</description><key id="105406556">13401</key><summary>Windows service doesn't stop properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Packaging</label><label>bug</label><label>v2.0.0-beta2</label></labels><created>2015-09-08T15:30:44Z</created><updated>2015-09-14T17:16:38Z</updated><resolved>2015-09-09T18:41:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-08T15:51:33Z" id="138606619">I think its fine, as long as we add comments about how its used? There were tons of methods like this in bootstrap: I probably removed a bunch of them. Technically, this one just needs to call Bootstrap.INSTANCE.stop() I think.
</comment><comment author="gmarz" created="2015-09-08T20:06:06Z" id="138684754">Thanks @rmuir, I've updated #13398 to include this as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow field data to be stored on disk for analyzed string fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13400</link><project id="" key="" /><description>Hello, sorry if there are already issues related to this topic. 

I am wondering if it would 1) be possible 2) be a good idea to allow the fielddata data structure to be stored on disk and loaded into the file system cache when needed, similar to doc_values. I know that allowing doc_values storage for analyzed string fields would not work for performance reasons (way too many values per document, for one). How I imagine this would work is: fielddata is build as it currently is and then serialized and written to disk; then when it is needed it is loaded into the file system cache. The benefit here is decreased memory use while (hopefully) still avoiding the cold query latency spike that occurs when you aggregate over an index without eager loaded field data for the first time (I assume that is primarily the cost of un-inverting a large index and building the fielddata structure, and that paging in that structure from file is faster). 

I need this feature because I use Elasticsearch for several aggregations on analyzed text (including a terms aggregation) for over a billion records across hundreds of daily indices. Unfortunately, this means that I need to eager load fielddata for all of these indices to prevent 20-200 second query latency on historical queries (which would also likely evict more recent field data and add GC latency). Currently we need around 90gb of RAM just for field data on one replica of our data, which is fairly expensive, and we think it would reduce costs of our Elasticsearch deployment significantly if we could move some of that storage requirement to disk.

Thanks!
</description><key id="105405814">13400</key><summary>Allow field data to be stored on disk for analyzed string fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jmacdonald92</reporter><labels><label>:Fielddata</label><label>discuss</label><label>high hanging fruit</label></labels><created>2015-09-08T15:26:59Z</created><updated>2016-01-28T15:58:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-08T21:31:30Z" id="138708220">What kind of text do you analyze? If this is simple text that you just happen to normalize (lowercasing, removal of accents, etc.), this other issue should allow you to enable doc values on it: #12394
</comment><comment author="jmacdonald92" created="2015-09-08T21:55:08Z" id="138715302">Sorry I was not clear about that. The primary use of the field is terms aggregation to find the top tokens in a slice of the data. We analyze the data using the standard analyzer. So basically, the only type of data that will not have doc_values as the default post #12394. 

I know that allowing doc_values for tokenized string data would have terrible performance. I was wondering if it would be possible to store the current fielddata structure for these string fields on disk and keep it updated/page it in to the FS cache like doc_values. From what I understand, the problems with enabling doc values stem from the data structures used, but it appears that whichever data structure is backing the fielddata implementations doesn't have this issue. 
</comment><comment author="jpountz" created="2015-09-08T22:04:49Z" id="138717264">OK, so this is actual full text content and wouldn't be covered by #12394. 

Indeed doc values are good to store string fields when documents have at most a couple values. Full text makes it harder because there can be a lot of variance in the number of terms that each document has, which is hard to store efficiently.
</comment><comment author="rmuir" created="2015-09-08T22:08:06Z" id="138717904">Fielddata is just as bad, it just blows up memory faster. Again, frequencies are lost so its just a bad idea for tokenized text.
</comment><comment author="jmacdonald92" created="2015-09-08T22:46:54Z" id="138724605">I am either fundamentally miss-understanding how fielddata works in the context of aggregations or @rmuir and I are not talking about the same thing/use case. 

As far as I understand, when Elasticsearch performs an aggregation it has to un-invert the stored index first. By default it does this with field data, but this process causes a latency spike if the field data isn't in the cache, since the process of constructing it (and potentially garbage collecting whatever was in the cache previously) is computationally expensive. The alternative is to index the field with doc_values = true, which constructs an inverted index using a different data structure and saves it to disk. Then when that doc_values data is needed the file on disk is page in to the file system cache outside the JVM heap. A cache miss here is much less problematic, since there is no need to garbage collect and the data structure is pre-built, since the only cost is reading from disk (usually an SSD). 

Given that understanding (which is likely wrong), I am confused about the statement that fielddata is bad for tokenized text. As far as I understand, fielddata is currently the only un-inverted form Elasticsearch supports for tokenized text. I absolutely agree that is takes a lot of memory, however, I have never encountered an issue with performance. Also, I don't know why frequencies being lost is problematic, since as far as I know the terms aggregation never supported terms frequency anyway (and if I am wrong and it does, I am not sure how).

Anyway, what I am really looking for is a way to replicate the job the fielddata currently performs for tokenized text without the requirement of having it constantly loaded into memory or face terrible performance. 
</comment><comment author="rmuir" created="2015-09-08T22:56:40Z" id="138725983">Thats the issue though, frequencies are important for tokenized text. I think somehow for whatever reason, your use case is ok with this, but generally speaking it is the wrong datastructure. I dont think we should try to provide enhancements around fielddata or docvalues support for that reason alone.
</comment><comment author="jmacdonald92" created="2015-09-09T00:42:39Z" id="138741027">I agree 100% that frequencies are important for tokenized text and that for that reason (among others) fielddata and docvalues are not a great format for storing and using an un-inverted index of tokenized text. What has me confused is that, as far as I know, fielddata is the only format Elasticsearch currently uses for that purpose. 

At the end of the day, the thing I really care about is reducing the memory requirement to run Elasticsearch. As far as I know, that means I need some way to store the data needed for aggregations in a way that does not take a long time to retrieve, or have to be held in memory constantly. As far as I know, we currently have this option for every other field type in dovvalues. So what I am really looking for is not specific to either fielddata or docvalues. 

That said, (I think) Elasticsearch currently uses fielddata (only) to fulfill this purpose for tokenized string fields. So my thought was saving the fielddata structure to disk and treating it like a cache backed by files on disk. I have no idea if that is the best solution, or if it is even possible but I wanted to start a discussion about moving (or having the option to move) tokenized string field storage to disk in some way if possible. 
</comment><comment author="jmacdonald92" created="2015-09-10T14:37:46Z" id="139266169">I wanted to update that I have been doing some digging into the way fielddata is generated, stored and used in Elasticsearch, and I am starting to realize that adding an FS backed cache not be possible.

I may be wrong about this, but as far as I could tell fielddata is comprised of some sort of tree of lucene data structures containing that allows you to map global ordinals to segment specific ordinals (which is obviously a gross oversimplification). The potential problem I am seeing is that these data structures are likely not serializable. I am not sure if there is a better or more possible way to store fielddata, but as @rmuir  said, fielddata is probably not the best way to accomplish the goal anyway. 

Anyway, I am still interested in some solution that would allow us stop keeping fielddata for tokenized strings in memory constantly without negatively impacting performance. 

Thanks
</comment><comment author="clintongormley" created="2015-09-19T14:53:49Z" id="141677231">@jmacdonald92 There appear to be two different use cases for doc values (or some other structure) with analyzed string fields. One is just the normalization case (eg lower-casing all terms), which is covered by https://github.com/elastic/elasticsearch/issues/10061.

The other use case is harder to pin down.  Suggested purposes include tag clouds, aggregated term statistics for naive bayes training, feature selection, and query expansion.

The problem is coming up with an efficient data structure which supports these cases.  Some require term statistics, and some don't. But all of them look pretty heavy. For the significant terms agg, we provide the sampler aggregation to reduce the load (see https://www.elastic.co/guide/en/elasticsearch/reference/2.0/search-aggregations-bucket-sampler-aggregation.html), but more details about real use cases would be useful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Split the _parent field mapping's field type into two field types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13399</link><project id="" key="" /><description>Split the _parent field mapping's field type into two feld types:
1) A shared immutable fieldtype for the _parent field (used for direct access to that field in the dsl). This field type is stored and indexed.
2) A per type field type for the join field. The field type has doc values enabled and field data type is allowed to be changed.

This resolves the issue that a mapping is not compatible if parent and child types have different field data loading settings.

PR for #13169
</description><key id="105400681">13399</key><summary>Split the _parent field mapping's field type into two field types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Mapping</label><label>:Parent/Child</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-08T15:00:43Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-09T09:30:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-09-08T15:22:19Z" id="138598976">I left some comments. Overall the idea looks good.
</comment><comment author="martijnvg" created="2015-09-08T21:08:59Z" id="138700318">@rjernst Thanks for looking at this! I updated the PR and applied your comments and I left a comment on the public parent field constructor comment.
</comment><comment author="rjernst" created="2015-09-08T21:25:30Z" id="138706999">I left a few more comments. It would also be great to have some unit tests, so we know the field types are what we expect given some basic settings (pre/post 2.0, field data loading variants).
</comment><comment author="martijnvg" created="2015-09-08T22:22:12Z" id="138720719">Added unit tests and extra compatibility upon merging.
</comment><comment author="rjernst" created="2015-09-08T23:28:34Z" id="138730413">Fantastic tests, thank you! LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix service.bat start/stop issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13398</link><project id="" key="" /><description>This PR addresses a few issues with the service.bat script:

1) Fixes a bug in the concatenation of java options in elasticsearch.in.bat which tripped up Apache Commons Daemon, and caused ES to startup without any params, eventually leading to the "path.home is not configured" exception.

2) service.bat was not passing the `start` argument to ES

3) The service could not be stopped gracefully via the `stop` command because there wasn't a method for procrun to call (`--StopMethod`): http://commons.apache.org/proper/commons-daemon/procrun.html

Closes #13247
Closes #13401
</description><key id="105398683">13398</key><summary>Fix service.bat start/stop issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gmarz</reporter><labels><label>:Packaging</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-08T14:50:37Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-09T18:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2015-09-08T21:12:26Z" id="138701079">looks good, thanks for digging into this one!
</comment><comment author="Mpdreamz" created="2015-09-09T15:32:29Z" id="138948418">LGTM :+1:
</comment><comment author="gmarz" created="2015-09-09T18:42:48Z" id="139002308">Merged to 2.0, 2.x, and master.  Thanks for the review @rmuir  and @Mpdreamz.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Migrate from Spatial4J/JTS to Apache SIS</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13397</link><project id="" key="" /><description>JTS is LGPL which causes licensing issues for `geo_shape` field types. This issue migrates all spatial4j and jts dependencies to the lucene sister project, Apache SIS. Doing so will provide coordinate unwrapping, reference system support, WKT, and ISO geo standards out of the box, completely removing superfluous spatial code from Elasticsearch.  There are a few gaps in SIS' spatial relation logic that will be tracked separately on Apache SIS jira.
</description><key id="105393274">13397</key><summary>Migrate from Spatial4J/JTS to Apache SIS</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nknize</reporter><labels><label>:Geo</label><label>enhancement</label><label>FAKENEWS</label><label>high hanging fruit</label></labels><created>2015-09-08T14:27:43Z</created><updated>2017-03-23T17:23:48Z</updated><resolved>2017-03-20T21:24:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nknize" created="2017-03-20T21:24:41Z" id="287902830">Looks like we may not even need SIS. :tada: Closing as FAKE_NEWS</comment><comment author="desruisseaux" created="2017-03-20T21:53:42Z" id="287910079">Just as a side note, it is true that Apache SIS is not needed for geometric objects. If there is a wish to handle Coordinate Reference Systems however (WKT, GML, map projections, etc.), then it may still worth consideration.</comment><comment author="nknize" created="2017-03-20T22:00:18Z" id="287911642">Thanks @desruisseaux! I am still considering SIS for handling CRS in the ES geo parse phase; but I'm also considering Proj4J. Any (objective) thoughts :smile: for one over the other?

</comment><comment author="desruisseaux" created="2017-03-21T00:46:10Z" id="287942967">Proj4J is a port in Java of Proj.4, a library in C. Proj.4 has a solid community; Proj4J a little bit less, but it may not be a risk factor since that project is a port. Apache SIS in comparison currently depends on a very small committer base, but we are trying to improve that with the help of the Apache Software Foundation and Google Summer of Code. Apache SIS is in very active development.

Proj4J has more map projections than SIS while being much more lightweight (307 kb compared to 2 or 3 Mb for SIS), which may seem paradoxical. But the map projections currently supported by SIS cover 96% of EPSG geodetic dataset, and the missing projections are progressively added. The difference in the size of the two libraries are explained by the amount of features that are not directly map projections but are still needed in referencing applications. More on it below.

Both Proj4J and SIS have some form of EPSG geodetic dataset. But the EPSG data included in Proj.4/Proj4J are a subset of the real EPSG database with all metadata (precision, domain of validity, transformation paths, etc.) trimmed. Absence of those metadata can cause errors like accidentally using for USA a coordinate operation designed for Cuba (this error happened because there is about 80 coordinate operations from NAD27 to WGS84; just specifying the source and target CRS are not sufficient). Absence of information about transformation paths put Proj4 in the category of "early-binding implementations", while Apache SIS is a "late-binding implementations" (EPSG guidance notes have a short discussion about "early-binding" versus "late-binding" implementations). In short, "early-binding" means that Proj.4 uses WGS84 has a pivot system for almost every datum changes, as seen from their "+towgs84" parameter. While EPSG recognizes that this is a common practice because easier to implement, EPSG does not recommend it. Not all coordinate transformations are defined relative to WGS84. Furthermore there is today 6 different versions of WGS84 with an offset up to 1.5 meters between them. For those reasons, the "TOWGS84" keyword that existed in WKT 1 does not exist anymore in the WKT 2 standard (ISO 19162) - more on WKT later. Instead we are supposed to check for transformation paths in the EPSG database for every pair of CRS ("late-binding" approach), which is what SIS does. We may object that a few meters error does not matter for most practical applications. This is true, but the problem is that Proj.4 is a black box; it does not tell which operation method it selected for a pair of CRS, where the operation is valid and what is its accuracy. So the user is not informed about the fact that the coordinate operations (s)he is doing may not be as accurate as (s)he think. Apache SIS by contrast provides those EPSG metadata. It also logs a warning if the user asks for a deprecated EPSG code, with a message telling what the replacement is.

An other issue is the Proj.4 EPSG file ignores axis order, which causes interoperability problems. I question whether this handling is compliant with EPSG terms of use. EPSG data are not open source; one of their constraint is that any change that modify the numerical results (as changing axis order does) shall not be attributed to EPSG. There is discussion at OGC right now about making this statement clearer ("if it is not compliant with EPSG definition, make it very obvious for example with a ';axisOrder=lon,lat' suffix in the code"), specifically because of this axis order issue. Apache SIS is compliant with EPSG definitions, but also provides some way to get (lon,lat) axis order if this is really what the user wants. But SIS does that in a way that can not be confused with EPSG definition.

Apache SIS supports multi-dimensional CRS with elevation, time and parametric axes. Transformations between different elevations is currently limited to ellipsoidal heights, but this will be fixed with the ISO 19111 revision in progress right now (by the introduction of "interpolation CRS").

Apache SIS has a Well Known Text (WKT) parser and formatter, supporting both version 1 and 2 (ISO 19162) of WKT. Except for an ESRI prototype, SIS is currently the only open-source project to support WKT 2 as far as I know. WKT 2 fixes some interoperability problems caused by different softwares interpreting WKT 1 in different ways. It also brings new features like temporal CRS, polar projections support (there is no standard way to represent a projection over a pole in WKT 1), the above-cited interpolation CRS, and more.

Apache SIS can read and write Coordinate Reference System in GML. What we get after reading GML is a fully operational object ready to perform map projections.

In addition of transforming points, Apache SIS provides a method for transforming multi-dimensional bounding box. This is a difficult task because of the need to locate e.g. where is the lowest or highest point of the curve (this is not necessarily a box corner). Iterating on 40 points on each side of the box is not sufficient. Instead, Apache SIS provides a quite accurate answer by leveraging its capability to compute Jacobian matrices in addition of projected points for each map projection. Again, things become more complicated at poles.

There is other features (JSR-363 - Unit of Measurement API - implementation, derived CRS, performance and more). An other aspect is that Apache SIS follows more closely the OGC and ISO standards.

In case of doubt, it is possible to write code in a way that allow to change implementation. Apache SIS in an implementation of GeoAPI 3.0 interfaces. There is also an implementation for Proj.4 (the C version; not yet for Proj4J but we could add it). By coding against the GeoAPI 3.0 interfaces, it is possible to keep ElasticSearch relatively independent of the underlying referencing library.
</comment><comment author="desruisseaux" created="2017-03-23T17:23:48Z" id="288796075">To elaborate on the last paragraph of my above comment, I can help a little bit in making Proj4J a GeoAPI 3.0.1 implementation if there is an interest for that. If ElasticSearch is coded against only those interfaces, or keep implementation-specific API calls in a small area of the code only, it would be easier for the users to choose their implementation. Those who want a lightweight (early-binding) implementation and are okay with the absence of envelope transformations, metadata, GML / WKT parser, etc. may use Proj4J (however the axis order problem is more serious and may need a hack) without penalizing the users who need a more elaborated implementation.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add changes for nested queries to migration doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13396</link><project id="" key="" /><description /><key id="105378256">13396</key><summary>add changes for nested queries to migration doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2015-09-08T13:15:58Z</created><updated>2015-09-08T13:27:33Z</updated><resolved>2015-09-08T13:27:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-08T13:27:33Z" id="138558701">Actually, this only worked by chance sometimes but most of the time not and we always warned that the full path is needed for nested (https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-nested-query.html#query-dsl-nested-query) so that is actually not a change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch pings other hosts on its own port</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13395</link><project id="" key="" /><description>I have two elasticsearch nodes, one of which is a client node (called `nodeA`, running on `serverA`), and one of which is a master (called `nodeB`, running on `serverB`). NodeA has tcp port `9317` and nodeB uses port `9310`.

I have noticed that when the client node (nodeA) starts up, it tries to contact nodeB, but it uses nodeA's port. This seems very strange to me, just because one node is running on a particular port, why does it assume all other nodes will be? I have checked that the firewall is not blocking any of the pings.

I wouldn't care about this except that the port it's trying to contact is blocked by the firewall, which results in a `NoRouteToHostException`, which appears as a WARN in the log file - having an exception which is 'normal behaviour' makes it harder to spot actual issues when they occur.

---

nodeA's elasticsearch.yml:

```
discovery.zen.ping.unicast.hosts: [serverA, serverB]
node.master: false
node.client: true
transport.tcp.port: 9317
```

nodeB's elasticsearch.yml:

```
discovery.zen.ping.unicast.hosts: [serverA, serverB]
transport.tcp.port: 9310
```

extract from my firewall log on serverB demonstrating that it was iptables that blocked the connection from serverA to serverB on port 9317:

```
Sep  8 11:34:02 serverB kernel: IPTABLES_REJECT: IN=eth7 OUT= SRC=serverA DST=serverB LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=49165 DF PROTO=TCP SPT=52202 DPT=9317 WINDOW=14600 RES=0x00 SYN URGP=0
```

This is elasticsearch 1.7.1 running on sun java 1.7.0_67 on CentOS 6.4.
</description><key id="105377865">13395</key><summary>elasticsearch pings other hosts on its own port</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels><label>:Network</label><label>feedback_needed</label></labels><created>2015-09-08T13:13:51Z</created><updated>2015-09-08T14:28:19Z</updated><resolved>2015-09-08T14:28:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-08T13:37:47Z" id="138560940">Hi @tstibbs 

Given that serverA knows nothing about serverB (other than its IP address), why do you think it would try to ping on any port other than the locally configured `transport.tcp.port`?

I think it finally succeeds in connecting thanks to multicast (which you appear not to have disabled).

If you want it to use unicast to connect to a node which is running on a different port, then you should specify the port in the hosts list:

```
 discovery.zen.ping.unicast.hosts: [serverA:9317, serverB:9310]
```
</comment><comment author="tstibbs" created="2015-09-08T14:11:58Z" id="138571501">Thanks @clintongormley for the quick response. For some reason I thought that missing out the port in the hosts list caused it to do something like a multicast ping but directed to a specific host. It sounds from your response like missing out the port simply causes it to use the locally-configured port number, is that correct?
</comment><comment author="clintongormley" created="2015-09-08T14:28:19Z" id="138581520">Correct.  I'll close this issue as it seems resolved
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>inner hits: huge number in "size" makes node go OOM even with very few docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13394</link><project id="" key="" /><description>spin off from https://discuss.elastic.co/t/difficulty-in-formulating-query-for-my-usecase/28773/4

The below example will make elasticsearch 1.7.1 OOM. I assume this is similar to https://github.com/elastic/elasticsearch/issues/12510 ?

```

DELETE testidx

PUT testidx
{
  "mappings": {
    "doc": {
      "properties": {
        "inner_hits": {
          "type": "nested"
        }
      }
    }
  }
}

POST testidx/doc
{
  "Language": [
    "English"
  ],
  "Tags": [
    "MT",
    "MUMBAI",
    "CHEN"
  ],
  "_boost": 3,
  "inner_hits": [
    {
      "Code": "ET00009709",
      "IsDefault": "",
      "Language": "English",
      "Format": "3D",
      "Region": "MUMBAI"
    },
    {
      "Code": "ET00009710",
      "IsDefault": "Y",
      "Language": "English",
      "Format": "2D",
      "Region": "CHEN"
    },
    {
      "Code": "ET00009713",
      "IsDefault": "",
      "Language": "Hindi",
      "Format": "2D",
      "Region": "MUMBAI"
    },
    {
      "Code": "ET00009714",
      "IsDefault": "",
      "Language": "Tamil",
      "Format": "3D",
      "Region": "MUMBAI"
    },
    {
      "Code": "ET00009715",
      "IsDefault": "",
      "Language": "Hindi",
      "Format": "3D",
      "Region": "MUMBAI"
    },
    {
      "Code": "ET00009716",
      "IsDefault": "",
      "Language": "Bengali",
      "Format": "2D",
      "Region": "MUMBAI"
    }
  ]
}

POST testidx/_search
{
  "fields": [], 
  "query": {
    "nested": {
      "path": "inner_hits",
      "query": {
        "match": {
          "Region": "MUMBAI"
        }
      },
      "inner_hits": {
        "size": 100000000
      }
    }
  }
}

```
</description><key id="105372658">13394</key><summary>inner hits: huge number in "size" makes node go OOM even with very few docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Inner Hits</label><label>adoptme</label><label>enhancement</label></labels><created>2015-09-08T12:43:53Z</created><updated>2015-09-09T11:14:11Z</updated><resolved>2015-09-09T11:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2015-09-08T12:49:19Z" id="138549951">@brwe Yes, this is similar to #12510 and should be solved in the same as was done in `top_hits` aggregator.
</comment><comment author="clintongormley" created="2015-09-08T13:31:12Z" id="138559441">will also be helped by https://github.com/elastic/elasticsearch/pull/13188
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor of GeohashCellQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13393</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

Relates to #10217

PR goes against the query-refactoring branch
</description><key id="105364274">13393</key><summary>Refactor of GeohashCellQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-08T12:01:26Z</created><updated>2015-09-09T12:34:43Z</updated><resolved>2015-09-09T12:34:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-09-08T13:57:08Z" id="138567745">left a few minor comments, looks good
</comment><comment author="colings86" created="2015-09-09T10:10:49Z" id="138863577">@javanna I pushed a commit which addresses your comments
</comment><comment author="javanna" created="2015-09-09T12:22:56Z" id="138892571">left one tiny comment around default values in the builder, LGTM besides that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>2.0.0-beta1 Init script startup fails silently when hitting java.lang.UnsupportedClassVersionError (unsupported JVM)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13392</link><project id="" key="" /><description>OS info

```
elk@master-clone:/opt$ uname -a
Linux master-clone 3.2.0-4-amd64 #1 SMP Debian 3.2.68-1+deb7u3 x86_64 GNU/Linux
```

Java info

```
elk@master-clone:/opt$ java -version
java version "1.6.0_36"
OpenJDK Runtime Environment (IcedTea6 1.13.8) (6b36-1.13.8-1~deb7u1)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)
```

Download and Install deb (probably affects other packages)

```
elk@master-clone:/opt$ wget https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.deb
--2015-09-08 13:31:03--  https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.7.1.deb
Resolving download.elastic.co (download.elastic.co)... 107.22.213.126, 107.22.237.122, 107.20.222.112, ...
Connecting to download.elastic.co (download.elastic.co)|107.22.213.126|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 27321880 (26M) [application/x-debian-package]
Saving to: `elasticsearch-1.7.1.deb'

100%[=================================================================================================================================================================================================================================================================================&gt;] 27,321,880  4.43M/s   in 6.6s    

2015-09-08 13:31:11 (3.94 MB/s) - `elasticsearch-1.7.1.deb' saved [27321880/27321880]

elk@master-clone:/opt$ sudo dpkg -i elasticsearch-1.7.1.deb 
Selecting previously unselected package elasticsearch.
(Reading database ... 143837 files and directories currently installed.)
Unpacking elasticsearch (from elasticsearch-1.7.1.deb) ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Setting up elasticsearch (1.7.1) ...
```

Launch service

```
elk@master-clone:/opt$ sudo /etc/init.d/elasticsearch start
[FAIL] Starting Elasticsearch Server: failed!
```

removing -b flag , from /etc/init.d/elasticsearch

```
start-stop-daemon --start -b --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
```

to

```
start-stop-daemon --start --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
        start-stop-daemon --stop --pidfile "$PID_FILE" \
```

to make this surface

```
elk@master-clone:/opt$ sudo /etc/init.d/elasticsearch start
[....] Starting Elasticsearch Server:Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
 failed!
```

This is really bad user experience and it would be very desirable at least to catch this output and present it to the user.
</description><key id="105363171">13392</key><summary>2.0.0-beta1 Init script startup fails silently when hitting java.lang.UnsupportedClassVersionError (unsupported JVM)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nellicus</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.7.1</label></labels><created>2015-09-08T11:54:11Z</created><updated>2016-03-16T19:39:39Z</updated><resolved>2016-01-28T15:51:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nellicus" created="2015-09-08T13:40:09Z" id="138561491">will try this on 2.x 
</comment><comment author="nellicus" created="2015-09-09T15:44:50Z" id="138951682">still reproducible on 2.x I'm afraid

```
Last login: Tue Sep  8 11:12:34 2015 from antonios-macbook-air.local
elk@master-clone:~$ wget https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/2.0.0-beta1/elasticsearch-2.0.0-beta1.deb
--2015-09-09 17:31:08--  https://download.elasticsearch.org/elasticsearch/release/org/elasticsearch/distribution/deb/elasticsearch/2.0.0-beta1/elasticsearch-2.0.0-beta1.deb
Resolving download.elasticsearch.org (download.elasticsearch.org)... 107.21.118.106, 107.20.198.195, 184.72.232.248, ...
Connecting to download.elasticsearch.org (download.elasticsearch.org)|107.21.118.106|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 28431018 (27M) [application/x-debian-package]
Saving to: `elasticsearch-2.0.0-beta1.deb'

100%[=================================================================================================================================================================================================================================================================================&gt;] 28,431,018  4.48M/s   in 9.1s    

2015-09-09 17:31:19 (2.99 MB/s) - `elasticsearch-2.0.0-beta1.deb' saved [28431018/28431018]

elk@master-clone:~$ sudo dpkg -i elasticsearch-2.0.0-beta1.deb 
[sudo] password for elk: 

Selecting previously unselected package elasticsearch.
(Reading database ... 143837 files and directories currently installed.)
Unpacking elasticsearch (from elasticsearch-2.0.0-beta1.deb) ...
Creating elasticsearch group... OK
Creating elasticsearch user... OK
Setting up elasticsearch (2.0.0~beta1) ...

elk@master-clone:~$ uname -a
Linux master-clone 3.2.0-4-amd64 #1 SMP Debian 3.2.68-1+deb7u3 x86_64 GNU/Linux
elk@master-clone:~$ cat /etc/debian_version 
7.9
elk@master-clone:~$ java -version
java version "1.6.0_36"
OpenJDK Runtime Environment (IcedTea6 1.13.8) (6b36-1.13.8-1~deb7u1)
OpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)
elk@master-clone:~$ time sudo /etc/init.d/elasticsearch start
[FAIL] Starting Elasticsearch Server: failed!

real    0m11.111s
user    0m0.004s
sys 0m0.012s
elk@master-clone:~$ sudo cp /etc/init.d/elasticsearch ~/
elk@master-clone:~$ egrep start-stop-daemon /etc/init.d/elasticsearch 
# Description:       Starts elasticsearch using start-stop-daemon
    start-stop-daemon -d $ES_HOME --start -b --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
        start-stop-daemon --stop --pidfile "$PID_FILE" \
```

remove background flag

```
elk@master-clone:~$ sudo sed -i 's/\-b//g' /etc/init.d/elasticsearch 
```

```
elk@master-clone:~$ egrep start-stop-daemon /etc/init.d/elasticsearch 
# Description:       Starts elasticsearch using start-stop-daemon
    start-stop-daemon -d $ES_HOME --start  --user "$ES_USER" -c "$ES_USER" --pidfile "$PID_FILE" --exec $DAEMON -- $DAEMON_OPTS
        start-stop-daemon --stop --pidfile "$PID_FILE" \
elk@master-clone:~$ time sudo /etc/init.d/elasticsearch start
[....] Starting Elasticsearch Server:Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.
 failed!

real    0m11.101s
user    0m0.004s
sys 0m0.008s
```
</comment><comment author="nik9000" created="2015-09-09T15:50:27Z" id="138953868">I've dropped the discuss label because I'm pretty sure this is worth fixing. I'll have a go at it sometime in the next few days if no one else wants it. I'm claiming it now so I don't forget about it but if anyone wants it chime in. I don't enjoy working on packaging issues but I enjoy having them solved....
</comment><comment author="rmuir" created="2015-09-09T17:43:18Z" id="138987408">Its too bad `java` launcher is so broken, since it has a `-version:xxx` with fancy support for lists, wildcards, ranges, etc, to specify what is needed.

But it does not work at all.
</comment><comment author="nik9000" created="2015-09-17T14:41:46Z" id="141107918">OK! I'm going to have a look at this.

I noticed that in systemd we've got elasticsearch setup as a `simple` service (the default) but if we switched to a `notify` service then we'd be able to send notifications back to systemd over a socket that tells it how Elasticsearch is doing. Neat stuff. Minimally I'd hope that we could send it a notification once the node is started. We could even send it information about where we are in the startup process.... Neat! Anyway, [man sd_notify](http://www.dsm.fordham.edu/cgi-bin/man-cgi.pl?topic=sd_notify&amp;ampsect=3) has good docs for this. sd_notify doesn't have a Java library so that'd be trouble there.

Now I'll investigate the init script side....
</comment><comment author="clintongormley" created="2016-01-28T15:51:49Z" id="176244670">I think this can be fixed simply by not double-daemonizing as we do today.  Closing in favour of https://github.com/elastic/elasticsearch/issues/8796

(If I've got this wrong, please reopen)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extract gateway required allocation calculations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13391</link><project id="" key="" /><description>and add a basic test
</description><key id="105348243">13391</key><summary>Extract gateway required allocation calculations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>non-issue</label><label>v2.0.0-beta2</label></labels><created>2015-09-08T10:23:25Z</created><updated>2015-09-14T17:13:50Z</updated><resolved>2015-09-09T11:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasontedor" created="2015-09-08T10:36:05Z" id="138510508">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor of GeoDistanceRangeQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13390</link><project id="" key="" /><description>Moving the query building functionality from the parser to the builders
new toQuery() method analogous to other recent query refactorings.

This PR contains and AwaitsFix annotation on the toQuery() method because it requires https://github.com/elastic/elasticsearch/pull/13381 to be merged before this test can pass

Also this PR removes the check that the index is created before 2.0 for the normalize parameter. The parameter is now always parsed but as a deprecated parameter. We cannot and should not access the index version during parsing.

Relates to #10217

PR goes against the query-refactoring branch
</description><key id="105326086">13390</key><summary>Refactor of GeoDistanceRangeQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Query Refactoring</label><label>review</label></labels><created>2015-09-08T08:19:33Z</created><updated>2015-09-25T12:31:36Z</updated><resolved>2015-09-09T12:42:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-08T11:08:13Z" id="138519476">@colings86 I pushed  #10217 to master
</comment><comment author="colings86" created="2015-09-08T11:13:05Z" id="138520196">@s1monw did you mean https://github.com/elastic/elasticsearch/pull/13381 ? #10217 is the meta issue
</comment><comment author="s1monw" created="2015-09-08T13:46:43Z" id="138564405">&gt; @s1monw did you mean #13381 

yes  :) sorry
</comment><comment author="javanna" created="2015-09-08T13:49:13Z" id="138565320">left a few comments, looks good though
</comment><comment author="colings86" created="2015-09-09T10:02:13Z" id="138861692">@javanna I pushed a commit which addresses your comments
</comment><comment author="javanna" created="2015-09-09T10:10:49Z" id="138863578">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>code return of bin/elasticsearch script should be compatible with init.d deamon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13389</link><project id="" key="" /><description>The script under "/bin/elasticsearch" is not compliant with the init.d deamon.

Version tested: 1.5.2
https://github.com/elastic/elasticsearch/blob/1.5/bin/elasticsearch#L162

For example if your JVM crash at startup (for example you want to allocate much HEAP than the JVM can handle with ES_HEAP_SIZE) this script will exit with EXIT_SUCCESS (0).

This will lead to incorrect behaviour of init.d.

For exemple with systemd, the daemon status will look like this : 

``` bash
# systemctl status elasticsearch
&#9679; elasticsearch.service - LSB: Starts elasticsearch
   Loaded: loaded (/etc/init.d/elasticsearch)
   Active: active (exited) since Tue 2015-09-08 09:46:55 CEST; 7s ago
  Process: 7931 ExecStop=/etc/init.d/elasticsearch stop (code=exited, status=0/SUCCESS)
  Process: 7946 ExecStart=/etc/init.d/elasticsearch start (code=exited, status=0/SUCCESS)
```

Please, Fix the return status of /bin/elasticsearch in case of a JVM crash.
</description><key id="105321718">13389</key><summary>code return of bin/elasticsearch script should be compatible with init.d deamon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">durandx</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2015-09-08T07:51:15Z</created><updated>2015-09-08T10:42:49Z</updated><resolved>2015-09-08T10:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-08T10:40:28Z" id="138511154">Given that we have to background the task, any ideas what changes we can make to achieve what you are after?
</comment><comment author="rmuir" created="2015-09-08T10:42:49Z" id="138511661">This is already fixed for 2.0

```
rmuir@beast:~/workspace/elasticsearch/distribution/zip/target/releases/elasticsearch-3.0.0-SNAPSHOT$ ES_HEAP_SIZE=100000 bin/elasticsearch
Error occurred during initialization of VM
Too small initial heap
rmuir@beast:~/workspace/elasticsearch/distribution/zip/target/releases/elasticsearch-3.0.0-SNAPSHOT$ echo $?
1
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] Java add missing breaking changes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13388</link><project id="" key="" /><description>@clintongormley Wanna review?

Closes #13151
</description><key id="105318645">13388</key><summary>[doc] Java add missing breaking changes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>docs</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-08T07:32:19Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-08T19:26:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[maven] assembly: elasticsearch is already excluded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13387</link><project id="" key="" /><description>As elasticsearch is marked as provided we don't need to explicitly exclude it from the assembly descriptor.

We get a warning today for all plugins, the following:

```
[INFO] --- maven-assembly-plugin:2.5.5:single (default) @ repository-s3 ---
[INFO] Reading assembly descriptor: /path/to/plugin-assembly.xml
[WARNING] The following patterns were never triggered in this artifact exclusion filter:
o  'org.elasticsearch:elasticsearch'
[INFO] Building zip: /path/to/target/releases/repository-s3-3.0.0-SNAPSHOT.zip
[INFO] 
```

It now gives:

```
[INFO] --- maven-assembly-plugin:2.5.5:single (default) @ repository-s3 ---
[INFO] Reading assembly descriptor: /path/to/plugin-assembly.xml
[INFO] Building zip: /path/to/target/releases/repository-s3-3.0.0-SNAPSHOT.zip
[INFO] 
```

I marked it as 2.0 as well as I think it does not hurt. But we can port it only in 2.1.
</description><key id="105314012">13387</key><summary>[maven] assembly: elasticsearch is already excluded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>build</label><label>v2.0.0-rc1</label></labels><created>2015-09-08T06:56:18Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-17T20:40:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-08T19:32:40Z" id="138677642">@nik9000 Do you think you can review this change?
</comment><comment author="nik9000" created="2015-09-15T22:13:03Z" id="140561263">LGTM

Sorry it took me so long to review this. I missed it the first time it streamed by my email.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some stats for deleted indices shouldn't be accumulated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13386</link><project id="" key="" /><description>For example, `open_contexts`. When there are open search contexts when an index is being deleted, the count is accumulated in `OldShardsStats` and it keeps growing.
(It looks it's because [freeing context](https://github.com/elastic/elasticsearch/blob/4173b00241cf9522427eeb81e9451619fe334c76/core/src/main/java/org/elasticsearch/search/SearchService.java#L175-L189) is called after [it collects old shard stats](https://github.com/elastic/elasticsearch/blob/8a06fef019a608153cc22b3ba6d29b5943348ead/core/src/main/java/org/elasticsearch/indices/IndicesService.java#L458-L469))

Another example is `current*` in merge stats. If an index is deleted while it's merging segment(s), those stats are accumulated.

I think those point-in-time stats shouldn't be accumulated.
</description><key id="105295600">13386</key><summary>Some stats for deleted indices shouldn't be accumulated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">masaruh</reporter><labels><label>:Stats</label><label>bug</label><label>v2.0.1</label><label>v2.1.0</label><label>v2.2.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-08T03:56:03Z</created><updated>2015-10-25T10:21:44Z</updated><resolved>2015-10-19T09:48:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ppf2" created="2015-09-28T10:49:40Z" id="143707610">Also, applies to closing indices.  eg. start with a node restarted with no contexts outstanding, create a scroll, close index, reopen index, _stats api will show that contexts are cleared, but node stats api will show that there are still open contexts.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move resources to their proper location</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13385</link><project id="" key="" /><description>We have a a number of resources in the java source tree, mostly for
tests. This change moves them to their proper location.
</description><key id="105290234">13385</key><summary>Move resources to their proper location</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>build</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-08T02:46:50Z</created><updated>2015-09-08T21:43:59Z</updated><resolved>2015-09-08T20:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-08T05:44:55Z" id="138440894">It looks good to me. 

For completeness, IIRC the same idea I submitted 2 years ago, you should probably remove [that](https://github.com/elastic/elasticsearch/blob/master/core/pom.xml#L199-L205).
</comment><comment author="rjernst" created="2015-09-08T05:59:52Z" id="138444830">@dadoonet I removed the now unnecessary maven parts.
</comment><comment author="dadoonet" created="2015-09-08T06:10:20Z" id="138447060">Left a comment. Just run the full test again because I remember hitting issues when filtering/non filtering some resources. Filtering does not work well with zip or gz files :)
</comment><comment author="rjernst" created="2015-09-08T06:27:06Z" id="138448648">I ran verify in core and it passes. To which dir are you referring?
</comment><comment author="rjernst" created="2015-09-08T06:46:11Z" id="138451197">Ok, I see now what is using this. `es-build.properties` is injected with build properties by "filtering". Maybe the thing using this could grab this information from the jar instead. I will take a look as a follow up issue. I'll add back the resources block, but specifically only filtering that file.
</comment><comment author="rjernst" created="2015-09-08T08:16:57Z" id="138472286">@dadoonet I pushed a new commit.
</comment><comment author="rjernst" created="2015-09-08T20:25:27Z" id="138689451">All tests are passing. I'll push now based on the looks good earlier.
</comment><comment author="rjernst" created="2015-09-08T21:43:59Z" id="138712967">2.x commit: db7d797
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexNameExpressionResolver should not ignore any wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13384</link><project id="" key="" /><description>Suffix wildcard case should work for the expression which only has one wildcard in the end.

Fix for #13334.
</description><key id="105285310">13384</key><summary>IndexNameExpressionResolver should not ignore any wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Index APIs</label><label>bug</label><label>review</label><label>v2.0.0-rc1</label></labels><created>2015-09-08T01:52:14Z</created><updated>2016-03-10T18:13:20Z</updated><resolved>2015-09-15T18:05:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-14T19:26:07Z" id="140182889">Left a small comment otherwise it LGTM.
</comment><comment author="xuzha" created="2015-09-14T19:26:32Z" id="140183000">Thx @nik9000  :-)
</comment><comment author="xuzha" created="2015-09-14T23:23:57Z" id="140231846">Do we need this for 2.0 and 2.1 @clintongormley ?
</comment><comment author="nik9000" created="2015-09-14T23:36:13Z" id="140233480">Almost certainly 2.1 (2.x branch) and probably not 2.0. Its a bug and doesn't have much chance of breaking anything so I'm for putting it in 2.0 as well.
</comment><comment author="martijnvg" created="2015-09-15T07:29:03Z" id="140304450">LGTM

@xuzha @nik9000 @clintongormley  I think this bugfix should be backported to 2.x and 2.0
</comment><comment author="xuzha" created="2015-09-15T18:23:59Z" id="140490074">Thanks @nik9000 and @martijnvg for the review.

Changes are in master https://github.com/elastic/elasticsearch/commit/c6da8d5e133f5804c3387b954d1f2445039396ab, 2.x https://github.com/elastic/elasticsearch/commit/defe1de9e9db52c80d2f581b8f7d358631eec5cb, 2.0 https://github.com/elastic/elasticsearch/commit/0e0c89af547e7768cb89f450cf3afe7bb6f0a725
</comment><comment author="martijnvg" created="2015-09-15T18:27:36Z" id="140490907">thank you @xuzha!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove environment from transport client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13383</link><project id="" key="" /><description>Transport clients run embedded within external applications, so
elasticsearch should not be doing anything with the filesystem, as there
is not elasticsearch home.

This change makes a number of cleanups to the internal API for loading
settings and creating an environment. The loadFromConfig option was
removed, since it was always true except for tests. We now always
attempt to load settings from config a file when an environment is
created. The prepare methods were also simplified so there is now
prepareSettingsAndEnvironment which nodes use, and prepareSettings which
the transport client uses. I also attempted to improve the tests, but
there is a still a lot of follow up work to do there.

closes #13155

NOTE: I left a handful of TODOs for things that looked fishy in prepareSettingsAndEnvironment. In general this should really be cleaned up to be much simpler. There are far to many ways to configure the settings for a developer to understand the code, so I don't know how a user would understand whether to put a setting in properties, env vars, or one of many possible files, or forcing a property, or setting a default, etc.
</description><key id="105279991">13383</key><summary>Remove environment from transport client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Settings</label><label>blocker</label><label>breaking</label><label>v2.0.0-beta2</label></labels><created>2015-09-08T00:33:09Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-08T20:28:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-08T07:30:06Z" id="138460753">left some minors LGTM otherwise
</comment><comment author="clintongormley" created="2015-09-08T08:21:12Z" id="138473254">@rjernst could you also add a note to the breaking changes docs please: https://www.elastic.co/guide/en/elasticsearch/reference/2.0/_java_api_changes.html
</comment><comment author="s1monw" created="2015-09-08T13:21:16Z" id="138557469">@clintongormley what is the breaking change here?
</comment><comment author="clintongormley" created="2015-09-08T13:38:27Z" id="138561094">@s1monw the fact that the node client now requires a `path.home` setting, which it didn't before.
</comment><comment author="rjernst" created="2015-09-08T17:23:36Z" id="138640314">It's also breaking because the `loadConfigSettings` argument is gone.

@clintongormley I added a note to the migration guide.
</comment><comment author="rjernst" created="2015-09-08T20:23:31Z" id="138688822">@s1monw I updated to remove the extra path.logs setter at the end of prepare. It was only there for logging, so I moved it directly to the logging configurator.
</comment><comment author="s1monw" created="2015-09-08T20:27:53Z" id="138689964">LGTM
</comment><comment author="rjernst" created="2015-09-08T21:42:54Z" id="138712763">2.x commit: 2d698d6
2.0 commit: 5cd65d0
</comment><comment author="mtraynham" created="2016-02-17T18:39:31Z" id="185341750">Just ran into this with an upgrade from 1.7 to 2.2, since we were using `elasticsearch.yml` to config the node name for our TransportClient.

I want to point out that the documentation ([here](https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/transport-client.html)) is wrong to suggest you can still do this.

&gt; Or using elasticsearch.yml file as shown in Node Client
</comment><comment author="clintongormley" created="2016-02-28T21:47:53Z" id="189951067">thanks @mtraynham - fixed in https://github.com/elastic/elasticsearch/commit/1769e9b78a5f8c17aa5ca5d8f00916a6ac96cfb8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapped Boolean field saving string values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13382</link><project id="" key="" /><description>```
PUT /test_boolean
{   "mappings": {
        "shops": {
            "properties": {
                "shop_type": {
                    "type": "object",
                    "dynamic": "strict",
                    "properties": {
                        "dealer": {"type": "boolean"},
                        "seller": {"type": "boolean"}
                    }
                }
            }
        }
    }
}

# THIS DOC SHOULD GET INDEXED
PUT test_boolean/shops/1
{
    "shop_type": [  
        {   "dealer": true, 
            "seller": false
        }
    ]
}


#THIS DOC SHOULDNOT GET INDEXED. MAPPER EXCEPTION EXPECTED
PUT test_boolean/shops/2
{
    "shop_type": [  
        {   "dealer": true, 
            "seller": "No this not a seller"
        }
    ]
}
```

Tried on latest: 1.7.1

According to my usecase the second document should fail by Mapper Exception telling cannot match string "No this not a seller" to boolean type.

I read though this https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html#boolean 
The doc says any string is finally treated as True. But i can see the final persisted doc contains the string as well. 

GET test_boolean/shops/2

```
{
   "_index": "test_boolean",
   "_type": "shops",
   "_id": "2",
   "_version": 1,
   "found": true,
   "_source": {
      "shop_type": [
         {
            "dealer": true,
            "seller": "No this not a seller"
         }
      ]
   }
}
```

Is this a bug or a feature. If a feature, how can i circumvent this problem to enforce my usecase. 
</description><key id="105278218">13382</key><summary>Mapped Boolean field saving string values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">saurajeet</reporter><labels /><created>2015-09-08T00:05:07Z</created><updated>2015-09-08T21:33:10Z</updated><resolved>2015-09-08T21:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-08T01:16:49Z" id="138401173">Your source document contains the original text but true is indexed.

For example if you search "seller" with a TermQuery on seller field, you won't get it.

Do the same with a String type and you'll get the result.

I think there is an opened issue...
</comment><comment author="jpountz" created="2015-09-08T09:57:02Z" id="138500268">See #11503. We use the same logic for parsing booleans in documents and in settings.
</comment><comment author="jpountz" created="2015-09-08T21:33:10Z" id="138708672">Closing this issue as a duplicate of #11503.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove cyclic dependencies between IndexService and FieldData / BitSet caches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13381</link><project id="" key="" /><description>Adds a listeners to each of the caches that allows us to remove the dependency on IndexService which is cyclic since
the IndexService depends on both of these caches. This cyclic dependency makes
testing the individual parts very hard and is only added for the sake of
incrementing some stats.
</description><key id="105266231">13381</key><summary>Remove cyclic dependencies between IndexService and FieldData / BitSet caches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-07T20:48:42Z</created><updated>2015-09-08T13:46:57Z</updated><resolved>2015-09-08T11:07:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2015-09-07T21:05:12Z" id="138377122">this is related to #13333
</comment><comment author="jpountz" created="2015-09-08T08:11:40Z" id="138470047">I just left one comment related to how listeners are set, otherwise LGTM
</comment><comment author="javanna" created="2015-09-08T08:34:04Z" id="138478145">LGTM too
</comment><comment author="s1monw" created="2015-09-08T10:43:00Z" id="138511776">pushed a new commit @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Engine: Let AlreadyClosedException and EngineClosedExceptionBubble up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13380</link><project id="" key="" /><description>Whe we call optimize we ignore Exceptions that indicate a closed shard.
However, when a shard is closed while an optimize request is in flight it
might also trigger an AlreadyClosedException from the IndexWriter when we
get the config or ForceMergeFailedEngineException with the EngineClosedException
wrapped inside. Because these are not identified as exceptions that indicate
a closed shard (TransportActions.isShardNotAvailableException(..)) optimize
would sometimes report failures when shards were relocating while optimize was called
and sometimes not. This caused weird test failures, see #13266 .
Instead, we should let EngineClosedException bubble up and also recognize
AlreadyClosedException as an indicator for a closed shard.
</description><key id="105249821">13380</key><summary>Engine: Let AlreadyClosedException and EngineClosedExceptionBubble up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Engine</label><label>bug</label><label>review</label><label>v2.0.0-beta2</label></labels><created>2015-09-07T17:22:10Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-08T15:01:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-08T07:16:50Z" id="138458665">Thx @brwe . Left some comments
</comment><comment author="brwe" created="2015-09-08T12:26:35Z" id="138542223">@bleskes @s1monw thanks for the review! addressed all comments.
</comment><comment author="bleskes" created="2015-09-08T12:47:09Z" id="138549318">Cool. LGTM. left some minor comments - no need for another cycle as far as I'm concerned. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CI: BulkIT.testBulkUpdate_largerVolume fails one of its GetRequests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13379</link><project id="" key="" /><description>http://build-us-00.elastic.co/job/es_core_2x_strong/28/

The issue is reproducible most of the time running this seed (on osx), further parameters having no influence already removed:

```
mvn verify -Pdev -Dskip.unit.tests -pl org.elasticsearch:elasticsearch -Dtests.seed=872FF07FDDFBCB -Dtests.class=org.elasticsearch.document.BulkIT -Dtests.method="testBulkUpdate_largerVolume" -Des.logger.level=DEBUG -Des.node.mode=local
```

When not running in `local` mode, this does not occur, also adding `waitForRelocations` makes it vanish, but I am not sure, if this is the underlying issue here.
</description><key id="105246925">13379</key><summary>CI: BulkIT.testBulkUpdate_largerVolume fails one of its GetRequests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>test</label></labels><created>2015-09-07T16:55:38Z</created><updated>2015-09-09T10:06:26Z</updated><resolved>2015-09-09T10:06:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-09-08T22:13:48Z" id="138718973">I think this is a bug in `InternalEngine.flush()`. When we flush we call `refresh()` and `translog.commit()` in the wrong order here: https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/index/engine/InternalEngine.java#L759
This leaves a short gap in which we can't get a document from translog and index. 
The test fails reliable for me but when I switch the order it passes.
I will work on a unit test and make a pr.
</comment><comment author="s1monw" created="2015-09-09T07:34:13Z" id="138812717">&gt; I think this is a bug in InternalEngine.flush(). When we flush we call refresh() and translog.commit() in the wrong order here:

whoa!!! good catch we need a get and commit stress test... that reproduces this problem in isolation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Trying to remove filtering on cluster but isn't working...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13378</link><project id="" key="" /><description>Hi,

I'm seeking assistance with this question.

I'm trying to force shards onto a node but first performed a cluster api settings call with the following response:-

```
{"persistent":{},"transient":{"cluster":{"routing":{"allocation":{"require":{"*":" "," *":" "},"exclude":{"_ip":""}, "include":{"_ip":""}}}}}}        
```

This filtering is preventing the force allocation of shards occurring.

How do I remove filtering from the cluster so that I can now proceed with this operation?

Thanks
</description><key id="105232017">13378</key><summary>Trying to remove filtering on cluster but isn't working...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">trekr5</reporter><labels /><created>2015-09-07T15:05:35Z</created><updated>2015-09-08T08:14:58Z</updated><resolved>2015-09-08T08:14:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-08T08:14:58Z" id="138471385">Hi @trekr5 

I'm afraid you're stuck...  Currently there is no way to unset these settings, you have to override them with new settings.  Fortunately, you made them transient settings instead of persistent settings, so a full cluster restart will remove them.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use only one way of specifying routing, version and version_type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13377</link><project id="" key="" /><description>Today we have a couple of ways of specifying the routing, version, and version
type of a given document request.

Beyond camelCase, we use:

`_routing` or `routing`
`_version` or `version`
`_version_type` or `version_type`

This makes the use of ParseField difficult for these parameters, as the camelCase of say `_routing` isn't `routing`. We should probably settle on using only one, or make ParseField handle this case.

Relates to #8988
</description><key id="105231187">13377</key><summary>Use only one way of specifying routing, version and version_type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:REST</label><label>adoptme</label><label>deprecation</label><label>low hanging fruit</label><label>v6.0.0</label></labels><created>2015-09-07T15:00:05Z</created><updated>2017-05-03T06:55:22Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-08T21:17:30Z" id="138703442">&gt; or make ParseField handle this case

Please no. :-)

+1 on settling on just one
</comment><comment author="clintongormley" created="2016-04-20T12:08:19Z" id="212399690">Let's settle on a single form (I don't really care if it is `_foo` or `foo`) and deprecate the other form (with depr logging), but only remote it in 6.0. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed pre 2.x parent child implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13376</link><project id="" key="" /><description>I may have forgotten something, but this seem to be all the code that can be removed.

The breaking part is that parent/child will not work on indices created on 1.x and before.
</description><key id="105226715">13376</key><summary>Removed pre 2.x parent child implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking</label><label>review</label><label>v5.0.0-alpha1</label></labels><created>2015-09-07T14:35:49Z</created><updated>2015-09-09T13:20:11Z</updated><resolved>2015-09-09T13:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-08T21:27:07Z" id="138707310">LGTM. Nice stats :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should scripts have context?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13375</link><project id="" key="" /><description>It would be nice if scripts could have some kind of context in which all the necessary imports would be performed as well as some global variables initialization. This is necessary, for example, when we want to evaluate a machine learning model in say an aggregation. The context would be loading the model together with its vectorizer only once, so that it could be evaluated repeatedly while performing the aggregation. Currently, it seems that the only way to achieve this would be to pass in as parameters to the script the serialized model, yet the model would still need to be deserialized each time. Otherwise the only other possibility would be to move to native scripts.
</description><key id="105225712">13375</key><summary>Should scripts have context?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2015-09-07T14:29:14Z</created><updated>2016-01-28T14:27:23Z</updated><resolved>2016-01-28T14:27:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-08T09:54:09Z" id="138499611">I am a bit worried that we have the temptation to go in two opposite directions with scripts:
1. making them simple, safe and fast, potentially at the cost of flexibility (#13084)
2. making them more flexible (eg. this issue)

If you ask me, I think 1 is more important. 2 might not be completely incompatible with 1 but I think we should do 1 before 2 if we don't want to end up in a situation where this new scripting language becomes the go-to solution for scripting but it can not support new options that we would introduce in this issue.
</comment><comment author="clintongormley" created="2016-01-28T14:27:23Z" id="176207534">Closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging: Add .noarch suffix to RPM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13374</link><project id="" key="" /><description>This adds back the `.noarch` suffix to the RPM
</description><key id="105216919">13374</key><summary>Packaging: Add .noarch suffix to RPM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>review</label></labels><created>2015-09-07T13:32:55Z</created><updated>2015-09-10T14:38:34Z</updated><resolved>2015-09-10T14:38:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-07T13:36:36Z" id="138301782">LGTM.
</comment><comment author="dadoonet" created="2015-09-07T13:37:00Z" id="138301967">And I guess this should go also in 2.x and master, right?
</comment><comment author="spinscale" created="2015-09-07T14:19:33Z" id="138310277">can you test this against 2.0 and run the vagrant tests locally?
</comment><comment author="spinscale" created="2015-09-07T14:59:32Z" id="138319050">this PR is broken. running `mvn package` produces the right name in the `target/` dir, but running `mvn install` produces `~/.m2/repository/org/elasticsearch/distribution/rpm/elasticsearch/2.0.0-beta2-SNAPSHOT/elasticsearch-2.0.0-beta2-SNAPSHOT.rpm`, because maven should totally rename the artifact another time... time to search for another option deep in the XML
</comment><comment author="dadoonet" created="2015-09-07T15:13:10Z" id="138322620">WTF! So before the PR, in .m2, the artifact name was correct, right?
</comment><comment author="spinscale" created="2015-09-07T15:17:41Z" id="138323442">@dadoonet not sure I understand you. The problem is that `mvn install` renames again, so where is the connection to "before that PR"? See this log

```
[INFO] Installing /~/devel/elasticsearch/distribution/rpm/target/rpm/elasticsearch/RPMS/noarch/elasticsearch-2.0.0-beta2_SNAPSHOT20150907151351.noarch.rpm to /~/.m2/repository/org/elasticsearch/distribution/rpm/elasticsearch/2.0.0-beta2-SNAPSHOT/elasticsearch-2.0.0-beta2-SNAPSHOT.rpm
```
</comment><comment author="dadoonet" created="2015-09-07T15:19:16Z" id="138323675">Ah I thought this renaming was happening only because of your change within this PR...

I'm looking for another solution then.
</comment><comment author="spinscale" created="2015-09-10T14:38:34Z" id="139266370">Closing this. Adding lines of XML (license checkers needs to check different artifacts, the vagrant tests need to be fixed, pom.xml) for a simple file name change is not worth the hassle.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add test for package reinstall after remove</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13373</link><project id="" key="" /><description>Closes #13286
</description><key id="105212881">13373</key><summary>Add test for package reinstall after remove</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v2.0.0-beta2</label></labels><created>2015-09-07T13:08:48Z</created><updated>2016-03-10T18:14:26Z</updated><resolved>2015-09-08T17:47:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2015-09-07T13:09:32Z" id="138293795">I believe this will need a rebase before it can really be merged and pass. And It'll need a solution to #13366.
</comment><comment author="dakrone" created="2015-09-08T14:37:55Z" id="138585596">LGTM
</comment><comment author="nik9000" created="2015-09-08T17:50:55Z" id="138647285">Thanks @dakrone! Merged to 2.0, 2.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MLT: builder takes a new Item object like its parser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13372</link><project id="" key="" /><description>Previously the parser could take any Term Vectors request, but this would be
not the case of the builder which would still use MultiGetRequest.Item. This
introduces a new Item class which is used by both the builder and parser.

Beyond that the rest is mostly cleanups such as:

1) Deprecating the ignoreLike methods, in favor to using unlike.

2) Deprecating and renaming MoreLikeThisBuilder#addItem to addLikeItem.

3) Ordering the methods of MoreLikeThisBuilder more logically.

This change is needed for the upcoming query refactoring of MLT.
</description><key id="105205432">13372</key><summary>MLT: builder takes a new Item object like its parser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>non-issue</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-07T12:21:06Z</created><updated>2015-09-14T12:19:39Z</updated><resolved>2015-09-09T09:42:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-07T13:30:17Z" id="138299218">I left some comments but in general it looks good.
</comment><comment author="alexksikes" created="2015-09-07T15:54:48Z" id="138329246">@jpountz I addressed all the comments. I'll do the package change in another commit for 3.0 only. Thank you for the review.
</comment><comment author="jpountz" created="2015-09-08T21:22:52Z" id="138706459">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failed action with response of 400, dropping action</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13371</link><project id="" key="" /><description>Hi,

For the following log:

Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None

When I trim ':' with logstash kv filter, the log can be passed to ES, but then the walltime becomes '160000'. 

kv {
          trim =&gt; "&lt;&gt;:"
          field_split =&gt; " "
          prefix =&gt; "job_"
}

If I don't trim ':' I get the following error and the log is dropped. Any idea why this happens? I'm running logstash 1.5.4 and ES 1.4.5 on a CentOS 6.6 server.

{:timestamp=&gt;"2015-09-07T17:33:30.017000+1000", :message=&gt;"failed action with response of 400, dropping action: [\"index\", {:_id=&gt;nil, :_index=&gt;\"logstash-2015.09.07\", :_type=&gt;\"program\", :_routing=&gt;nil}, #&lt;LogStash::Event:0x6af14eac @metadata={\"retry_count\"=&gt;0}, @accessors=#&lt;LogStash::Util::Accessors:0x6ad72e6f @store={\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, @lut={\"[pbs_object_type]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"pbs_object_type\"], \"pbs_object\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"pbs_object\"], \"tags\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"tags\"], \"[pbs_program]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"pbs_program\"], \"[message]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"message\"], \"message\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"message\"], \"job_user\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"job_user\"], \"pbs_user\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"pbs_user\"], \"rnt651@raijin1\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"rnt651@raijin1\"], \"job_cpu_hrs\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"job_cpu_hrs\"], \"job_ncpus\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"job_ncpus\"], \"[type]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"type\"], \"[program]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"program\"], \"[tags]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"tags\"], \"type\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"type\"], \"[syslog_severity]\"=&gt;[{\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, \"syslog_severity\"]}&gt;, @data={\"host\"=&gt;\"r-man2\", \"message\"=&gt;\"Job Start Summary: jobid=2291646.r-man2 project=p08 user=rnt651@raijin1 queue=express-node ncpus=16 mem_requested=8gb walltime=16:00:00 cpu_hrs=768.0 jobfs_local=8gb jobfs_global=None\", \"offset\"=&gt;253322067, \"path\"=&gt;\"/var/spool/PBS/server_logs/20150907\", \"program\"=&gt;\"pbs_server\", \"type\"=&gt;\"program\", \"@version\"=&gt;\"1\", \"@timestamp\"=&gt;\"2015-09-07T07:33:08.000Z\", \"pbs_timestamp\"=&gt;\"09/07/2015 17:33:08\", \"pbs_event_class\"=&gt;\"job\", \"pbs_node\"=&gt;\"Server@r-man2\", \"pbs_object_type\"=&gt;\"job\", \"job_id\"=&gt;\"2291646.r-man2\", \"received_from\"=&gt;\"r-man2\", \"pbs_host\"=&gt;\"r-man2\", \"pbs_program\"=&gt;\"pbs_server\", \"received_at\"=&gt;\"09/07/2015 17:33:08\", \"tags\"=&gt;[\"pbs_production\", \"pbs_job\"], \"job_jobid\"=&gt;\"2291646.r-man2\", \"job_project\"=&gt;\"p08\", \"job_user\"=&gt;\"rnt651@raijin1\", \"job_queue\"=&gt;\"express-node\", \"job_ncpus\"=&gt;16, \"job_mem_requested\"=&gt;\"8gb\", \"job_walltime\"=&gt;\"16:00:00\", \"job_cpu_hrs\"=&gt;768.0, \"job_jobfs_local\"=&gt;\"8gb\", \"job_jobfs_global\"=&gt;\"None\", \"pbs_user\"=&gt;\"rnt651@raijin1\"}, @metadata_accessors=#&lt;LogStash::Util::Accessors:0x2b8f1a3f @store={\"retry_count\"=&gt;0}, @lut={}&gt;, @cancelled=false&gt;]", :level=&gt;:warn}

Any hint is much appreciated.
</description><key id="105202535">13371</key><summary>failed action with response of 400, dropping action</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">angel-smile</reporter><labels /><created>2015-09-07T11:57:41Z</created><updated>2015-09-07T14:27:21Z</updated><resolved>2015-09-07T14:27:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-07T14:27:20Z" id="138312745">Hi @angel-smile 

It sounds like you have a mapping issue somewhere, which should be visible in the elasticsearch log.  That said this doesn't sound like a bug.  You should ask this (with more details, eg the log message) in the forums instead: http://discuss.elastic.co/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Referencing previous initialised variables raises a NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13370</link><project id="" key="" /><description>_From @pedrocunha on March 26, 2015 15:5_

Hello,

The following search request raises a NullPointerException complaining foo is not defined:

```
curl -XPOST "http://localhost:9200/index/model/_search" -d'
{
  "query": {
    "function_score": {
      "script_score": {
        "script": "foo = 2; foo + 2",
        "lang": "python"
      }
    }
  }
}'
```

This is similar to https://github.com/elastic/elasticsearch-lang-python/issues/19 but in this case I'm trying to reference the variable just initialised. I'm using ES 1.4.3 and python plugin 2.4.1. I've tried both 1.5 ES version and the plugin snapshot versions, and the behaviour is the same.

Note that the above is totally valid if we change "lang" to "groovy".

Am I missing out something ?

Please help,
Thank you

_Copied from original issue: elastic/elasticsearch-lang-python#29_
</description><key id="105160497">13370</key><summary>Referencing previous initialised variables raises a NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Lang Python</label><label>adoptme</label><label>bug</label></labels><created>2015-09-07T07:17:24Z</created><updated>2017-04-19T07:20:58Z</updated><resolved>2017-04-19T07:20:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-07T07:17:25Z" id="138213322">Actually your python script does not return anything. When function_score API tries to access the value, it generates a NPE.
I think we need to catch it properly in elasticsearch core code itself.

What are you trying to achieve here? Could you illustrate your example with a more concrete use case?
Also, could you put here the full stack trace?

cc @brwe Do you think we should detect that in elasticsearch core to avoid NPE if the language returns null. WDYT?
</comment><comment author="dadoonet" created="2015-09-07T07:17:25Z" id="138213323">_From @pedrocunha on March 30, 2015 8:41_

Ok, some context. My cluster has currently around 1M docs which are used to search. We apply a bunch of filtering and in the end we use the script score to compute some business logic. The script is multi-line and written in groovy. Irrelevant whitespace and comments are then stripped, cached and then sent to ES as a dynamic script (which then ES caches it too). 

This works quite well for us: response time is consistently 30-50ms~ despite the high search query demand.

Groovy is not considered a sandboxed language anymore. So, in theory using Python could make our scripts bit more readable and shorter (if we don't take a performance hit) and that's why I was interested in giving it a try. 

Does that give some real context in this case ?

&gt; Actually your python script does not return anything. When function_score API tries to access the value, it generates a NPE.

```
curl -XPOST "http://localhost:9200/index/model/_search" -d'
{
  "query": {
    "function_score": {
      "script_score": {
        "script": "return 2",
        "lang": "python"
      }
    }
  }
}'
```

Specifying `return` raises a python error:

```
[2015-03-30 09:32:42,076][DEBUG][action.search.type       ] [Belathauzer] [index][2], node[0AjBuVPwSWyjcJJAePpBOw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@65c6eb62] lastShard [true]
org.elasticsearch.search.SearchParseException: [index][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
  "query": {
    "function_score": {
      "script_score": {
        "script":"return 2",
        "lang": "python"
      }
    }
  }
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:721)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:557)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:529)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:291)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.query.QueryParsingException: [search_development] script_score the script could not be loaded
    at org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionParser.parse(ScriptScoreFunctionParser.java:93)
    at org.elasticsearch.index.query.functionscore.FunctionScoreQueryParser.parse(FunctionScoreQueryParser.java:133)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:302)
    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:705)
    ... 9 more
Caused by: SyntaxError: ("'return' outside function", ('&lt;script&gt;', 1, 0, ''))

    at org.python.core.ParserFacade.fixParseError(ParserFacade.java:95)
    at org.python.core.CompilerFacade.compile(CompilerFacade.java:36)
    at org.python.core.Py.compile_flags(Py.java:1823)
    at org.python.core.Py.compile_flags(Py.java:1828)
    at org.python.util.PythonInterpreter.compile(PythonInterpreter.java:277)
    at org.python.util.PythonInterpreter.compile(PythonInterpreter.java:272)
    at org.python.util.PythonInterpreter.compile(PythonInterpreter.java:266)
    at org.elasticsearch.script.python.PythonScriptEngineService.compile(PythonScriptEngineService.java:70)
    at org.elasticsearch.script.ScriptService.compile(ScriptService.java:297)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:429)
    at org.elasticsearch.index.query.functionscore.script.ScriptScoreFunctionParser.parse(ScriptScoreFunctionParser.java:90)
    ... 16 more
```

Full stack of the NullPointerException with the original code:

```
[2015-03-30 09:31:19,067][DEBUG][action.search.type       ] [Belathauzer] All shards failed for phase: [query]
org.elasticsearch.search.query.QueryPhaseExecutionException: [index][4]: query[filtered(function score (ConstantScore(*:*),function=script[foo = 2; foo +2], params [null]))-&gt;cache(+_type:property +org.elasticsearch.index.search.nested.NonNestedDocsFilter@55c9b8e4)],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)
    at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:286)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:297)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.script.python.PythonScriptEngineService$PythonSearchScript.runAsDouble(PythonScriptEngineService.java:215)
    at org.elasticsearch.common.lucene.search.function.ScriptScoreFunction.score(ScriptScoreFunction.java:100)
    at org.elasticsearch.common.lucene.search.function.FunctionScoreQuery$FunctionFactorScorer.innerScore(FunctionScoreQuery.java:160)
    at org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer$AnyNextDoc.score(CustomBoostFactorScorer.java:133)
    at org.elasticsearch.common.lucene.search.function.CustomBoostFactorScorer.score(CustomBoostFactorScorer.java:71)
    at org.apache.lucene.search.TopScoreDocCollector$OutOfOrderTopScoreDocCollector.collect(TopScoreDocCollector.java:140)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:157)
    ... 8 more
```

So a few things:
- It would be great if we could have multi-line python scripts. Maybe specified as an array of strings which then have the right indentation in order to be parsed/eval correctly.
- While it failed above with the return - note that removing the return (and making `script: "2"`) does make the function score work and apply 2 to every document - it would be great if we can make sure early `return`s work as expected.
</comment><comment author="dadoonet" created="2015-09-07T07:17:25Z" id="138213326">_From @pedrocunha on April 7, 2015 10:32_

Any new feedback on this? Thanks.
</comment><comment author="cwarny" created="2016-06-14T13:49:12Z" id="225886401">@dadoonet did you end up finding a workaround? Facing same problem
</comment><comment author="dadoonet" created="2016-06-14T13:55:46Z" id="225888356">May be @pedrocunha did?
</comment><comment author="pedrocunha" created="2016-06-14T14:11:36Z" id="225893512">Workaround for me was staying in Groovy land unfortunately.
</comment><comment author="cwarny" created="2016-06-14T14:14:18Z" id="225894305">Ugh, OK, thanks. Seems like not being able to register multi-line Python scripts defeats the purpose of being able to use Python for ES scripting
</comment><comment author="rjernst" created="2017-04-19T07:20:58Z" id="295135572">Python scripting has been removed in master (6.0).</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Python Module Access</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13369</link><project id="" key="" /><description>_From @theNewFlesh on February 18, 2015 22:23_

It would great if you could implement either:
1) The ability to specify a Python file (that may import other modules) which is executed with the 
     relevant Elasticsearch variables (doc, _doc, _source, etc) instead of a Python expression.  This will
     involve invoking the exec function instead of the eval function.
OR 
2) Python expressions to have access to modules/objects specified outside of the default Python
    environment.  This will likely mean a configuration option specifying a python function for setting up
    the globals dictionary passed to the eval statement.

_Copied from original issue: elastic/elasticsearch-lang-python#28_
</description><key id="105160406">13369</key><summary>Python Module Access</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugin Lang Python</label><label>discuss</label></labels><created>2015-09-07T07:16:31Z</created><updated>2016-01-28T14:17:51Z</updated><resolved>2016-01-28T14:17:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2016-01-28T14:17:51Z" id="176203533">Given all of the changes that have gone into locking elasticsearch down, this one sounds like a nonstarter.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es 0.90.2 plus jdk6.0_25-b06 crashed on production</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13368</link><project id="" key="" /><description>hs_error_pid.log as below:
# 
# A fatal error has been detected by the Java Runtime Environment:
# 
# SIGSEGV (0xb) at pc=0x00007fd05cb3a628, pid=117184, tid=140505275176704
# 
# JRE version: 6.0_25-b06
# Java VM: Java HotSpot(TM) 64-Bit Server VM (20.0-b11 mixed mode linux-amd64 )
# Problematic frame:
# J  org.elasticsearch.common.io.stream.HandlesStreamOutput.writeString(Ljava/lang/String;)V
# 
# If you would like to submit a bug report, please visit:
# http://java.sun.com/webapps/bugreport/crash.jsp
# 

---------------  T H R E A D  ---------------

Current thread (0x00007fcb01d6b800):  JavaThread "elasticsearch[node-173.68][generic][T#1]" daemon [_thread_in_Java, id=117388, stack(0x00007fc9ef001000,0x00007fc9ef042000)]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x00007fd4cfc9d560

Registers:
RAX=0x00000000005baa4e, RBX=0x00007fcccfc9d808, RCX=0x00007fcf8c012cc8, RDX=0x0000000000000010
RSP=0x00007fc9ef0403f0, RBP=0x00007fd4cfc9d560, RSI=0x00007fcf8df23b90, RDI=0x00007fcf8df23a00
R8 =0x00007fcf8c0ab1f8, R9 =0x0000000000000054, R10=0x00007fcf8c0ab1f8, R11=0x00007fcccfdc9f68
R12=0x00007fc9ef040650, R13=0x00007fcba5d67718, R14=0x00007fcf8df23bb8, R15=0x00007fcb01d6b800
RIP=0x00007fd05cb3a628, EFLAGS=0x0000000000010293, CSGSFS=0x0000000000000033, ERR=0x0000000000000004
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007fc9ef0403f0)
0x00007fc9ef0403f0:   00000065ffffffff 00007fcbad7a2768
0x00007fc9ef040400:   00007fcf8df23b90 0000001100000015
0x00007fc9ef040410:   00007fcccfc9d550 00000008000000c5
0x00007fc9ef040420:   00007fcf8df23bd0 00007fcbad7a27a8
0x00007fc9ef040430:   00007fcccfc9db90 00007fcbad7a2328
0x00007fc9ef040440:   00007fcccfdc9f68 ffffffff00000054
0x00007fc9ef040450:   00007fc9ef040450 00000001ffffffff
0x00007fc9ef040460:   00007f8fb435f000 00007fcba5d67708
0x00007fc9ef040470:   0000000000000000 00007fcf8c949d90
0x00007fc9ef040480:   00007fc9ef0404f0 00007fd05c880a82
0x00007fc9ef040490:   00007fc9ef0404f0 00007fd05c880a82
0x00007fc9ef0404a0:   00007fcf8df23b90 00007fcbad7a2768
0x00007fc9ef0404b0:   00007fc9ef0404b0 00007fcf8ca09915
0x00007fc9ef0404c0:   00007fc9ef040518 00007fcf8ca0a2e8
0x00007fc9ef0404d0:   00007fcf8e9af038 00007fcf8ca099c0
0x00007fc9ef0404e0:   00007fc9ef0404a0 00007fc9ef040510
0x00007fc9ef0404f0:   00007fc9ef040560 00007fd05c880a82
0x00007fc9ef040500:   0000000000000000 0000000000000000
0x00007fc9ef040510:   00007fcbad7a2768 00007fcccffa3658
0x00007fc9ef040520:   00007fc9ef040520 00007fcf8d637036
0x00007fc9ef040530:   00007fc9ef040588 00007fcf8d6376e8
0x00007fc9ef040540:   0000000000000000 00007fcf8d637080
0x00007fc9ef040550:   00007fc9ef040510 00007fc9ef040580
0x00007fc9ef040560:   00007fc9ef0405d0 00007fd05c880a82
0x00007fc9ef040570:   00007fcccffa3658 00007fcccfca2658
0x00007fc9ef040580:   00007fcbad7a2768 00007fcccffadcd0
0x00007fc9ef040590:   00007fc9ef040590 00007fcf8d652185
0x00007fc9ef0405a0:   00007fc9ef0405f8 00007fcf8d652980
0x00007fc9ef0405b0:   0000000000000000 00007fcf8d652238
0x00007fc9ef0405c0:   00007fc9ef040580 00007fc9ef0405f0
0x00007fc9ef0405d0:   00007fc9ef040640 00007fd05c880a82
0x00007fc9ef0405e0:   0000000000000000 0000000000000000 

Instructions: (pc=0x00007fd05cb3a628)
0x00007fd05cb3a608:   83 d0 0a 00 00 4c 8b 5c 24 20 4b 8d 5c cb 18 8b
0x00007fd05cb3a618:   6c 24 5c 49 8d 6c eb 18 4c 8b 1b 4c 89 5c 24 50
0x00007fd05cb3a628:   4c 8b 5d 00 4c 89 5c 24 60 4c 8b 5c 24 50 4d 3b
0x00007fd05cb3a638:   dd 0f 84 82 01 00 00 4c 3b 5c 24 10 0f 84 0f 01 

Register to memory mapping:

RAX=0x00000000005baa4e is an unknown value
RBX=0x00007fcccfc9d808 is an oop

[error occurred during error reporting (printing register info), id 0xb]

Stack: [0x00007fc9ef001000,0x00007fc9ef042000],  sp=0x00007fc9ef0403f0,  free space=252k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
J  org.elasticsearch.common.io.stream.HandlesStreamOutput.writeString(Ljava/lang/String;)V

[error occurred during error reporting (printing native stack), id 0xb]

---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
  0x00007fc950085800 JavaThread "elasticsearch[node-173.68][bulk][T#3]" daemon [_thread_blocked, id=117672, stack(0x00007fc9ed549000,0x00007fc9ed58a000)]
  0x00007fcb14004800 JavaThread "elasticsearch[node-173.68][refresh][T#1]" daemon [_thread_blocked, id=117670, stack(0x00007fc9ed5cb000,0x00007fc9ed60c000)]
  0x00007fc94c082000 JavaThread "elasticsearch[node-173.68][bulk][T#2]" daemon [_thread_blocked, id=117625, stack(0x00007fc9eedf9000,0x00007fc9eee3a000)]
  0x00007fcb14002800 JavaThread "elasticsearch[node-173.68][snapshot][T#1]" daemon [_thread_blocked, id=117599, stack(0x00007fc9eee3a000,0x00007fc9eee7b000)]
  0x00007fc950082000 JavaThread "elasticsearch[node-173.68][bulk][T#1]" daemon [_thread_blocked, id=117598, stack(0x00007fc9eed77000,0x00007fc9eedb8000)]
  0x00007fcb14002000 JavaThread "elasticsearch[node-173.68][merge][T#1]" daemon [_thread_blocked, id=117597, stack(0x00007fc9eedb8000,0x00007fc9eedf9000)]
  0x00007fc934082000 JavaThread "elasticsearch[node-173.68][management][T#2]" daemon [_thread_blocked, id=117595, stack(0x00007fc9ed60c000,0x00007fc9ed64d000)]
  0x00007fc8bc00c000 JavaThread "elasticsearch[node-173.68][warmer][T#1]" daemon [_thread_blocked, id=117594, stack(0x00007fc9ed64d000,0x00007fc9ed68e000)]
  0x00007fc8d40b8800 JavaThread "Timer-0" daemon [_thread_blocked, id=117593, stack(0x00007fc9edd37000,0x00007fc9edd78000)]
  0x00007fc924082800 JavaThread "elasticsearch[node-173.68][management][T#1]" daemon [_thread_blocked, id=117536, stack(0x00007fcb6006f000,0x00007fcb600b0000)]
  0x00007fcb02686000 JavaThread "elasticsearch[keepAlive/0.90.2]" [_thread_blocked, id=117535, stack(0x00007fc9edd78000,0x00007fc9eddb9000)]
  0x00007fcb02684000 JavaThread "elasticsearch[node-173.68][http_server_boss][T#1]{New I/O server boss #195}" daemon [_thread_in_native, id=117534, stack(0x00007fc9eddb9000,0x00007fc9eddfa000)]
  0x00007fcb02657800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#64]{New I/O worker #194}" daemon [_thread_in_native, id=117533, stack(0x00007fc9eddfa000,0x00007fc9ede3b000)]
  0x00007fcb0262b800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#63]{New I/O worker #193}" daemon [_thread_in_native, id=117532, stack(0x00007fc9ede3b000,0x00007fc9ede7c000)]
  0x00007fcb025ff800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#62]{New I/O worker #192}" daemon [_thread_in_native, id=117531, stack(0x00007fc9ede7c000,0x00007fc9edebd000)]
  0x00007fcb025d3800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#61]{New I/O worker #191}" daemon [_thread_in_native, id=117530, stack(0x00007fc9edebd000,0x00007fc9edefe000)]
  0x00007fcb025a7800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#60]{New I/O worker #190}" daemon [_thread_in_native, id=117529, stack(0x00007fc9edefe000,0x00007fc9edf3f000)]
  0x00007fcb0257b800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#59]{New I/O worker #189}" daemon [_thread_in_native, id=117528, stack(0x00007fc9edf3f000,0x00007fc9edf80000)]
  0x00007fcb0254f800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#58]{New I/O worker #188}" daemon [_thread_in_native, id=117527, stack(0x00007fc9edf80000,0x00007fc9edfc1000)]
  0x00007fcb02523800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#57]{New I/O worker #187}" daemon [_thread_in_native, id=117526, stack(0x00007fc9edfc1000,0x00007fc9ee002000)]
  0x00007fcb024f7800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#56]{New I/O worker #186}" daemon [_thread_in_native, id=117525, stack(0x00007fc9ee002000,0x00007fc9ee043000)]
  0x00007fcb024cb800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#55]{New I/O worker #185}" daemon [_thread_in_native, id=117524, stack(0x00007fc9ee043000,0x00007fc9ee084000)]
  0x00007fcb0249f800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#54]{New I/O worker #184}" daemon [_thread_in_native, id=117523, stack(0x00007fc9ee084000,0x00007fc9ee0c5000)]
  0x00007fcb02473800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#53]{New I/O worker #183}" daemon [_thread_in_native, id=117522, stack(0x00007fc9ee0c5000,0x00007fc9ee106000)]
  0x00007fcb02447800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#52]{New I/O worker #182}" daemon [_thread_in_native, id=117521, stack(0x00007fc9ee106000,0x00007fc9ee147000)]
  0x00007fcb0241b800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#51]{New I/O worker #181}" daemon [_thread_in_native, id=117520, stack(0x00007fc9ee147000,0x00007fc9ee188000)]
  0x00007fcb023ef800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#50]{New I/O worker #180}" daemon [_thread_in_native, id=117519, stack(0x00007fc9ee188000,0x00007fc9ee1c9000)]
  0x00007fcb023c3800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#49]{New I/O worker #179}" daemon [_thread_in_native, id=117518, stack(0x00007fc9ee1c9000,0x00007fc9ee20a000)]
  0x00007fcb02397800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#48]{New I/O worker #178}" daemon [_thread_in_native, id=117517, stack(0x00007fc9ee20a000,0x00007fc9ee24b000)]
  0x00007fcb0236b800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#47]{New I/O worker #177}" daemon [_thread_in_native, id=117516, stack(0x00007fc9ee24b000,0x00007fc9ee28c000)]
  0x00007fcb0233f800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#46]{New I/O worker #176}" daemon [_thread_in_native, id=117515, stack(0x00007fc9ee28c000,0x00007fc9ee2cd000)]
  0x00007fcb02313800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#45]{New I/O worker #175}" daemon [_thread_in_native, id=117514, stack(0x00007fc9ee2cd000,0x00007fc9ee30e000)]
  0x00007fcb022e7800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#44]{New I/O worker #174}" daemon [_thread_in_native, id=117513, stack(0x00007fc9ee30e000,0x00007fc9ee34f000)]
  0x00007fcb022bb800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#43]{New I/O worker #173}" daemon [_thread_in_native, id=117512, stack(0x00007fc9ee34f000,0x00007fc9ee390000)]
  0x00007fcb0228f800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#42]{New I/O worker #172}" daemon [_thread_in_native, id=117511, stack(0x00007fc9ee390000,0x00007fc9ee3d1000)]
  0x00007fcb02263800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#41]{New I/O worker #171}" daemon [_thread_in_native, id=117510, stack(0x00007fc9ee3d1000,0x00007fc9ee412000)]
  0x00007fcb02237800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#40]{New I/O worker #170}" daemon [_thread_in_native, id=117509, stack(0x00007fc9ee412000,0x00007fc9ee453000)]
  0x00007fcb0220b800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#39]{New I/O worker #169}" daemon [_thread_in_native, id=117508, stack(0x00007fc9ee453000,0x00007fc9ee494000)]
  0x00007fcb021df000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#38]{New I/O worker #168}" daemon [_thread_in_native, id=117507, stack(0x00007fc9ee494000,0x00007fc9ee4d5000)]
  0x00007fcb021b3000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#37]{New I/O worker #167}" daemon [_thread_in_native, id=117506, stack(0x00007fc9ee4d5000,0x00007fc9ee516000)]
  0x00007fcb02187000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#36]{New I/O worker #166}" daemon [_thread_in_native, id=117505, stack(0x00007fc9ee516000,0x00007fc9ee557000)]
  0x00007fcb0215b000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#35]{New I/O worker #165}" daemon [_thread_in_native, id=117504, stack(0x00007fc9ee557000,0x00007fc9ee598000)]
  0x00007fcb0212f000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#34]{New I/O worker #164}" daemon [_thread_in_native, id=117503, stack(0x00007fc9ee598000,0x00007fc9ee5d9000)]
  0x00007fcb02103000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#33]{New I/O worker #163}" daemon [_thread_in_native, id=117502, stack(0x00007fc9ee5d9000,0x00007fc9ee61a000)]
  0x00007fcb020d7000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#32]{New I/O worker #162}" daemon [_thread_in_native, id=117501, stack(0x00007fc9ee61a000,0x00007fc9ee65b000)]
  0x00007fcb020ab000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#31]{New I/O worker #161}" daemon [_thread_in_native, id=117500, stack(0x00007fc9ee65b000,0x00007fc9ee69c000)]
  0x00007fcb0207f000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#30]{New I/O worker #160}" daemon [_thread_in_native, id=117499, stack(0x00007fc9ee69c000,0x00007fc9ee6dd000)]
  0x00007fcb02053000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#29]{New I/O worker #159}" daemon [_thread_in_native, id=117498, stack(0x00007fc9ee6dd000,0x00007fc9ee71e000)]
  0x00007fcb02027000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#28]{New I/O worker #158}" daemon [_thread_in_native, id=117497, stack(0x00007fc9ee71e000,0x00007fc9ee75f000)]
  0x00007fcb01ffb800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#27]{New I/O worker #157}" daemon [_thread_in_native, id=117496, stack(0x00007fc9ee75f000,0x00007fc9ee7a0000)]
  0x00007fcb01fd0000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#26]{New I/O worker #156}" daemon [_thread_in_native, id=117495, stack(0x00007fc9ee7a0000,0x00007fc9ee7e1000)]
  0x00007fcb01fa4000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#25]{New I/O worker #155}" daemon [_thread_in_native, id=117494, stack(0x00007fc9ee7e1000,0x00007fc9ee822000)]
  0x00007fcb01f78000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#24]{New I/O worker #154}" daemon [_thread_in_native, id=117493, stack(0x00007fc9ee822000,0x00007fc9ee863000)]
  0x00007fcb01f4c000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#23]{New I/O worker #153}" daemon [_thread_in_native, id=117492, stack(0x00007fc9ee863000,0x00007fc9ee8a4000)]
  0x00007fcb01f20000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#22]{New I/O worker #152}" daemon [_thread_in_native, id=117491, stack(0x00007fc9ee8a4000,0x00007fc9ee8e5000)]
  0x00007fcb01ef4000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#21]{New I/O worker #151}" daemon [_thread_in_native, id=117490, stack(0x00007fc9ee8e5000,0x00007fc9ee926000)]
  0x00007fcb01ec8800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#20]{New I/O worker #150}" daemon [_thread_in_native, id=117489, stack(0x00007fc9ee926000,0x00007fc9ee967000)]
  0x00007fcb006de000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#19]{New I/O worker #149}" daemon [_thread_in_native, id=117488, stack(0x00007fc9ee967000,0x00007fc9ee9a8000)]
  0x00007fcb006dc000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#18]{New I/O worker #148}" daemon [_thread_in_native, id=117487, stack(0x00007fc9ee9a8000,0x00007fc9ee9e9000)]
  0x00007fcb006da000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#17]{New I/O worker #147}" daemon [_thread_in_native, id=117486, stack(0x00007fc9ee9e9000,0x00007fc9eea2a000)]
  0x00007fcb006d8000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#16]{New I/O worker #146}" daemon [_thread_in_native, id=117485, stack(0x00007fc9eea2a000,0x00007fc9eea6b000)]
  0x00007fcb006d6000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#15]{New I/O worker #145}" daemon [_thread_in_native, id=117484, stack(0x00007fc9eea6b000,0x00007fc9eeaac000)]
  0x00007fcb006d4000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#14]{New I/O worker #144}" daemon [_thread_in_native, id=117483, stack(0x00007fc9eeaac000,0x00007fc9eeaed000)]
  0x00007fcb006d2000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#13]{New I/O worker #143}" daemon [_thread_in_native, id=117482, stack(0x00007fc9eeaed000,0x00007fc9eeb2e000)]
  0x00007fcb006bf800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#12]{New I/O worker #142}" daemon [_thread_in_native, id=117481, stack(0x00007fc9eeb2e000,0x00007fc9eeb6f000)]
  0x00007fcb00693800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#11]{New I/O worker #141}" daemon [_thread_in_native, id=117480, stack(0x00007fc9eeb6f000,0x00007fc9eebb0000)]
  0x00007fcb00667800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#10]{New I/O worker #140}" daemon [_thread_in_native, id=117479, stack(0x00007fc9eebb0000,0x00007fc9eebf1000)]
  0x00007fcb0053d800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#9]{New I/O worker #139}" daemon [_thread_in_native, id=117478, stack(0x00007fc9eebf1000,0x00007fc9eec32000)]
  0x00007fcb0053b800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#8]{New I/O worker #138}" daemon [_thread_in_native, id=117477, stack(0x00007fc9eec32000,0x00007fc9eec73000)]
  0x00007fcb0053a000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#7]{New I/O worker #137}" daemon [_thread_in_native, id=117476, stack(0x00007fc9eec73000,0x00007fc9eecb4000)]
  0x00007fcb00538800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#6]{New I/O worker #136}" daemon [_thread_in_native, id=117475, stack(0x00007fc9eecb4000,0x00007fc9eecf5000)]
  0x00007fcb00537000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#5]{New I/O worker #135}" daemon [_thread_in_native, id=117474, stack(0x00007fc9eecf5000,0x00007fc9eed36000)]
  0x00007fcb00536000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#4]{New I/O worker #134}" daemon [_thread_in_native, id=117473, stack(0x00007fc9eefc0000,0x00007fc9ef001000)]
  0x00007fcb00535800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#3]{New I/O worker #133}" daemon [_thread_in_native, id=117472, stack(0x00007fc9eeefd000,0x00007fc9eef3e000)]
  0x00007fcb0050a800 JavaThread "elasticsearch[node-173.68][http_server_worker][T#2]{New I/O worker #132}" daemon [_thread_in_native, id=117471, stack(0x00007fc9eef7f000,0x00007fc9eefc0000)]
  0x00007fcb004e0000 JavaThread "elasticsearch[node-173.68][http_server_worker][T#1]{New I/O worker #131}" daemon [_thread_in_native, id=117470, stack(0x00007fc9eef3e000,0x00007fc9eef7f000)]
  0x00007fc9a4086800 JavaThread "elasticsearch[node-173.68][clusterService#updateTask][T#1]" daemon [_thread_blocked, id=117469, stack(0x00007fc9eeebc000,0x00007fc9eeefd000)]
  0x00007fc8e8001800 JavaThread "elasticsearch[node-173.68][generic][T#6]" daemon [_thread_blocked, id=117399, stack(0x00007fc9eed36000,0x00007fc9eed77000)]
  0x00007fc9fc001000 JavaThread "elasticsearch[node-173.68][transport_client_timer][T#1]{Hashed wheel timer #1}" daemon [_thread_blocked, id=117394, stack(0x00007fc9eee7b000,0x00007fc9eeebc000)]
=&gt;0x00007fcb01d6b800 JavaThread "elasticsearch[node-173.68][generic][T#1]" daemon [_thread_in_Java, id=117388, stack(0x00007fc9ef001000,0x00007fc9ef042000)]
  0x00007fcb01d65800 JavaThread "elasticsearch[node-173.68][transport_server_boss][T#1]{New I/O server boss #130}" daemon [_thread_in_native, id=117387, stack(0x00007fc9ef042000,0x00007fc9ef083000)]
  0x00007fcb01d52800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#64]{New I/O worker #129}" daemon [_thread_in_native, id=117386, stack(0x00007fc9ef083000,0x00007fc9ef0c4000)]
  0x00007fcb01d26800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#63]{New I/O worker #128}" daemon [_thread_in_native, id=117385, stack(0x00007fc9ef0c4000,0x00007fc9ef105000)]
  0x00007fcb01cfa800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#62]{New I/O worker #127}" daemon [_thread_in_native, id=117384, stack(0x00007fc9ef105000,0x00007fc9ef146000)]
  0x00007fcb01cce800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#61]{New I/O worker #126}" daemon [_thread_in_native, id=117383, stack(0x00007fc9ef146000,0x00007fc9ef187000)]
  0x00007fcb01ca2800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#60]{New I/O worker #125}" daemon [_thread_in_native, id=117382, stack(0x00007fc9ef187000,0x00007fc9ef1c8000)]
  0x00007fcb01c76800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#59]{New I/O worker #124}" daemon [_thread_in_native, id=117381, stack(0x00007fc9ef1c8000,0x00007fc9ef209000)]
  0x00007fcb01c4a800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#58]{New I/O worker #123}" daemon [_thread_in_native, id=117380, stack(0x00007fc9ef209000,0x00007fc9ef24a000)]
  0x00007fcb01c1e800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#57]{New I/O worker #122}" daemon [_thread_in_native, id=117379, stack(0x00007fc9ef24a000,0x00007fc9ef28b000)]
  0x00007fcb01bf2800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#56]{New I/O worker #121}" daemon [_thread_in_native, id=117378, stack(0x00007fc9ef28b000,0x00007fc9ef2cc000)]
  0x00007fcb01bc6800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#55]{New I/O worker #120}" daemon [_thread_in_native, id=117377, stack(0x00007fc9ef2cc000,0x00007fc9ef30d000)]
  0x00007fcb01b9a800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#54]{New I/O worker #119}" daemon [_thread_in_native, id=117376, stack(0x00007fc9ef30d000,0x00007fc9ef34e000)]
  0x00007fcb01b6e000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#53]{New I/O worker #118}" daemon [_thread_in_native, id=117375, stack(0x00007fc9ef34e000,0x00007fc9ef38f000)]
  0x00007fcb01b42000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#52]{New I/O worker #117}" daemon [_thread_in_native, id=117374, stack(0x00007fc9ef38f000,0x00007fc9ef3d0000)]
  0x00007fcb01b16000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#51]{New I/O worker #116}" daemon [_thread_in_native, id=117373, stack(0x00007fc9ef3d0000,0x00007fc9ef411000)]
  0x00007fcb01aea000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#50]{New I/O worker #115}" daemon [_thread_in_native, id=117372, stack(0x00007fc9ef411000,0x00007fc9ef452000)]
  0x00007fcb01abe000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#49]{New I/O worker #114}" daemon [_thread_in_native, id=117371, stack(0x00007fc9ef452000,0x00007fc9ef493000)]
  0x00007fcb01a92000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#48]{New I/O worker #113}" daemon [_thread_in_native, id=117370, stack(0x00007fc9ef493000,0x00007fc9ef4d4000)]
  0x00007fcb01a66000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#47]{New I/O worker #112}" daemon [_thread_in_native, id=117369, stack(0x00007fc9ef4d4000,0x00007fc9ef515000)]
  0x00007fcb01a3a000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#46]{New I/O worker #111}" daemon [_thread_in_native, id=117368, stack(0x00007fc9ef515000,0x00007fc9ef556000)]
  0x00007fcb01a0e000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#45]{New I/O worker #110}" daemon [_thread_in_native, id=117367, stack(0x00007fc9ef556000,0x00007fc9ef597000)]
  0x00007fcb019e2000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#44]{New I/O worker #109}" daemon [_thread_in_native, id=117366, stack(0x00007fc9ef597000,0x00007fc9ef5d8000)]
  0x00007fcb019b6000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#43]{New I/O worker #108}" daemon [_thread_in_native, id=117365, stack(0x00007fc9ef5d8000,0x00007fc9ef619000)]
  0x00007fcb0198a000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#42]{New I/O worker #107}" daemon [_thread_in_native, id=117364, stack(0x00007fc9ef619000,0x00007fc9ef65a000)]
  0x00007fcb0195e000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#41]{New I/O worker #106}" daemon [_thread_in_native, id=117363, stack(0x00007fc9ef65a000,0x00007fc9ef69b000)]
  0x00007fcb01932000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#40]{New I/O worker #105}" daemon [_thread_in_native, id=117362, stack(0x00007fc9ef69b000,0x00007fc9ef6dc000)]
  0x00007fcb01906000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#39]{New I/O worker #104}" daemon [_thread_in_native, id=117361, stack(0x00007fc9ef6dc000,0x00007fc9ef71d000)]
  0x00007fcb018da000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#38]{New I/O worker #103}" daemon [_thread_in_native, id=117360, stack(0x00007fc9ef71d000,0x00007fc9ef75e000)]
  0x00007fcb018ae000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#37]{New I/O worker #102}" daemon [_thread_in_native, id=117359, stack(0x00007fc9ef75e000,0x00007fc9ef79f000)]
  0x00007fcb01882000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#36]{New I/O worker #101}" daemon [_thread_in_native, id=117358, stack(0x00007fc9ef79f000,0x00007fc9ef7e0000)]
  0x00007fcb01856000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#35]{New I/O worker #100}" daemon [_thread_in_native, id=117357, stack(0x00007fc9ef7e0000,0x00007fc9ef821000)]
  0x00007fcb0182a000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#34]{New I/O worker #99}" daemon [_thread_in_native, id=117356, stack(0x00007fc9ef821000,0x00007fc9ef862000)]
  0x00007fcb017fe000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#33]{New I/O worker #98}" daemon [_thread_in_native, id=117355, stack(0x00007fc9ef862000,0x00007fc9ef8a3000)]
  0x00007fcb017d2000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#32]{New I/O worker #97}" daemon [_thread_in_native, id=117354, stack(0x00007fc9ef8a3000,0x00007fc9ef8e4000)]
  0x00007fcb017a6000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#31]{New I/O worker #96}" daemon [_thread_in_native, id=117353, stack(0x00007fc9ef8e4000,0x00007fc9ef925000)]
  0x00007fcb0177a000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#30]{New I/O worker #95}" daemon [_thread_in_native, id=117352, stack(0x00007fc9ef925000,0x00007fc9ef966000)]
  0x00007fcb0174e000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#29]{New I/O worker #94}" daemon [_thread_in_native, id=117351, stack(0x00007fc9ef966000,0x00007fc9ef9a7000)]
  0x00007fcb01721800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#28]{New I/O worker #93}" daemon [_thread_in_native, id=117350, stack(0x00007fc9ef9a7000,0x00007fc9ef9e8000)]
  0x00007fcb016f5800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#27]{New I/O worker #92}" daemon [_thread_in_native, id=117349, stack(0x00007fc9ef9e8000,0x00007fc9efa29000)]
  0x00007fcb016c9800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#26]{New I/O worker #91}" daemon [_thread_in_native, id=117348, stack(0x00007fc9efa29000,0x00007fc9efa6a000)]
  0x00007fcb0169d800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#25]{New I/O worker #90}" daemon [_thread_in_native, id=117347, stack(0x00007fc9efa6a000,0x00007fc9efaab000)]
  0x00007fcb01671800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#24]{New I/O worker #89}" daemon [_thread_in_native, id=117346, stack(0x00007fc9efaab000,0x00007fc9efaec000)]
  0x00007fcb01645800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#23]{New I/O worker #88}" daemon [_thread_in_native, id=117345, stack(0x00007fc9efaec000,0x00007fc9efb2d000)]
  0x00007fcb01619800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#22]{New I/O worker #87}" daemon [_thread_in_native, id=117344, stack(0x00007fc9efb2d000,0x00007fc9efb6e000)]
  0x00007fcb015ed800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#21]{New I/O worker #86}" daemon [_thread_in_native, id=117343, stack(0x00007fc9efb6e000,0x00007fc9efbaf000)]
  0x00007fcb015c1800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#20]{New I/O worker #85}" daemon [_thread_in_native, id=117342, stack(0x00007fc9efbaf000,0x00007fc9efbf0000)]
  0x00007fcb01595800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#19]{New I/O worker #84}" daemon [_thread_in_native, id=117341, stack(0x00007fc9efbf0000,0x00007fc9efc31000)]
  0x00007fcb01569800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#18]{New I/O worker #83}" daemon [_thread_in_native, id=117340, stack(0x00007fc9efc31000,0x00007fc9efc72000)]
  0x00007fcb0153d800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#17]{New I/O worker #82}" daemon [_thread_in_native, id=117339, stack(0x00007fc9efc72000,0x00007fc9efcb3000)]
  0x00007fcb01511800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#16]{New I/O worker #81}" daemon [_thread_in_native, id=117338, stack(0x00007fc9efcb3000,0x00007fc9efcf4000)]
  0x00007fcb014e5800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#15]{New I/O worker #80}" daemon [_thread_in_native, id=117337, stack(0x00007fc9efcf4000,0x00007fc9efd35000)]
  0x00007fcb014b9800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#14]{New I/O worker #79}" daemon [_thread_in_native, id=117336, stack(0x00007fc9efd35000,0x00007fc9efd76000)]
  0x00007fcb0148d800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#13]{New I/O worker #78}" daemon [_thread_in_native, id=117335, stack(0x00007fc9efd76000,0x00007fc9efdb7000)]
  0x00007fcb01461800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#12]{New I/O worker #77}" daemon [_thread_in_native, id=117334, stack(0x00007fc9efdb7000,0x00007fc9efdf8000)]
  0x00007fcb01436000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#11]{New I/O worker #76}" daemon [_thread_in_native, id=117333, stack(0x00007fc9efdf8000,0x00007fc9efe39000)]
  0x00007fcb0140a800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#10]{New I/O worker #75}" daemon [_thread_in_native, id=117332, stack(0x00007fc9efe39000,0x00007fc9efe7a000)]
  0x00007fcb013de800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#9]{New I/O worker #74}" daemon [_thread_in_native, id=117331, stack(0x00007fc9efe7a000,0x00007fc9efebb000)]
  0x00007fcb013b2800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#8]{New I/O worker #73}" daemon [_thread_in_native, id=117330, stack(0x00007fc9efebb000,0x00007fc9efefc000)]
  0x00007fcb01386800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#7]{New I/O worker #72}" daemon [_thread_in_native, id=117329, stack(0x00007fc9efefc000,0x00007fc9eff3d000)]
  0x00007fcb0135b000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#6]{New I/O worker #71}" daemon [_thread_in_native, id=117328, stack(0x00007fc9eff3d000,0x00007fc9eff7e000)]
  0x00007fcb0132f000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#5]{New I/O worker #70}" daemon [_thread_in_native, id=117327, stack(0x00007fc9eff7e000,0x00007fc9effbf000)]
  0x00007fcb01253000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#4]{New I/O worker #69}" daemon [_thread_in_native, id=117326, stack(0x00007fc9effbf000,0x00007fc9f0000000)]
  0x00007fcb01251000 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#3]{New I/O worker #68}" daemon [_thread_in_native, id=117325, stack(0x00007fcb2c020000,0x00007fcb2c061000)]
  0x00007fcb0124e800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#2]{New I/O worker #67}" daemon [_thread_in_native, id=117324, stack(0x00007fcb2c061000,0x00007fcb2c0a2000)]
  0x00007fcb0124d800 JavaThread "elasticsearch[node-173.68][transport_server_worker][T#1]{New I/O worker #66}" daemon [_thread_in_native, id=117323, stack(0x00007fcb2c0a2000,0x00007fcb2c0e3000)]
  0x00007fcb0124a800 JavaThread "elasticsearch[node-173.68][transport_client_boss][T#1]{New I/O boss #65}" daemon [_thread_in_native, id=117322, stack(0x00007fcb2c0e3000,0x00007fcb2c124000)]
  0x00007fcb01234000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#64]{New I/O worker #64}" daemon [_thread_in_native, id=117321, stack(0x00007fcb2c124000,0x00007fcb2c165000)]
  0x00007fcb01208000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#63]{New I/O worker #63}" daemon [_thread_in_native, id=117320, stack(0x00007fcb2c165000,0x00007fcb2c1a6000)]
  0x00007fcb011dc800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#62]{New I/O worker #62}" daemon [_thread_in_native, id=117319, stack(0x00007fcb2c1a6000,0x00007fcb2c1e7000)]
  0x00007fcb011b1000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#61]{New I/O worker #61}" daemon [_thread_in_native, id=117318, stack(0x00007fcb2c1e7000,0x00007fcb2c228000)]
  0x00007fcb01185800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#60]{New I/O worker #60}" daemon [_thread_in_native, id=117317, stack(0x00007fcb2c228000,0x00007fcb2c269000)]
  0x00007fcb01159800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#59]{New I/O worker #59}" daemon [_thread_in_native, id=117316, stack(0x00007fcb2c269000,0x00007fcb2c2aa000)]
  0x00007fcb0112d800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#58]{New I/O worker #58}" daemon [_thread_in_native, id=117315, stack(0x00007fcb2c2aa000,0x00007fcb2c2eb000)]
  0x00007fcb01102000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#57]{New I/O worker #57}" daemon [_thread_in_native, id=117314, stack(0x00007fcb2c2eb000,0x00007fcb2c32c000)]
  0x00007fcb010d6000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#56]{New I/O worker #56}" daemon [_thread_in_native, id=117313, stack(0x00007fcb2c32c000,0x00007fcb2c36d000)]
  0x00007fcb010aa000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#55]{New I/O worker #55}" daemon [_thread_in_native, id=117312, stack(0x00007fcb2c36d000,0x00007fcb2c3ae000)]
  0x00007fcb0107e000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#54]{New I/O worker #54}" daemon [_thread_in_native, id=117311, stack(0x00007fcb2c3ae000,0x00007fcb2c3ef000)]
  0x00007fcb01052000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#53]{New I/O worker #53}" daemon [_thread_in_native, id=117310, stack(0x00007fcb2c3ef000,0x00007fcb2c430000)]
  0x00007fcb01026000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#52]{New I/O worker #52}" daemon [_thread_in_native, id=117309, stack(0x00007fcb2c430000,0x00007fcb2c471000)]
  0x00007fcb00ffa000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#51]{New I/O worker #51}" daemon [_thread_in_native, id=117308, stack(0x00007fcb2c471000,0x00007fcb2c4b2000)]
  0x00007fcb00fce000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#50]{New I/O worker #50}" daemon [_thread_in_native, id=117307, stack(0x00007fcb2c4b2000,0x00007fcb2c4f3000)]
  0x00007fcb00fa2000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#49]{New I/O worker #49}" daemon [_thread_in_native, id=117306, stack(0x00007fcb2c4f3000,0x00007fcb2c534000)]
  0x00007fcb00f76000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#48]{New I/O worker #48}" daemon [_thread_in_native, id=117305, stack(0x00007fcb2c534000,0x00007fcb2c575000)]
  0x00007fcb00f4a000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#47]{New I/O worker #47}" daemon [_thread_in_native, id=117304, stack(0x00007fcb2c575000,0x00007fcb2c5b6000)]
  0x00007fcb00f1e000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#46]{New I/O worker #46}" daemon [_thread_in_native, id=117303, stack(0x00007fcb2c5b6000,0x00007fcb2c5f7000)]
  0x00007fcb00ef2000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#45]{New I/O worker #45}" daemon [_thread_in_native, id=117302, stack(0x00007fcb2c5f7000,0x00007fcb2c638000)]
  0x00007fcb00ec6000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#44]{New I/O worker #44}" daemon [_thread_in_native, id=117301, stack(0x00007fcb2c638000,0x00007fcb2c679000)]
  0x00007fcb00e9a000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#43]{New I/O worker #43}" daemon [_thread_in_native, id=117300, stack(0x00007fcb2c679000,0x00007fcb2c6ba000)]
  0x00007fcb00e6e000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#42]{New I/O worker #42}" daemon [_thread_in_native, id=117299, stack(0x00007fcb2c6ba000,0x00007fcb2c6fb000)]
  0x00007fcb00e42000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#41]{New I/O worker #41}" daemon [_thread_in_native, id=117298, stack(0x00007fcb2c6fb000,0x00007fcb2c73c000)]
  0x00007fcb00e16000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#40]{New I/O worker #40}" daemon [_thread_in_native, id=117297, stack(0x00007fcb2c73c000,0x00007fcb2c77d000)]
  0x00007fcb00dea800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#39]{New I/O worker #39}" daemon [_thread_in_native, id=117296, stack(0x00007fcb2c77d000,0x00007fcb2c7be000)]
  0x00007fcb00dbe800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#38]{New I/O worker #38}" daemon [_thread_in_native, id=117295, stack(0x00007fcb2c7be000,0x00007fcb2c7ff000)]
  0x00007fcb00d92800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#37]{New I/O worker #37}" daemon [_thread_in_native, id=117294, stack(0x00007fcb2c7ff000,0x00007fcb2c840000)]
  0x00007fcb00d66800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#36]{New I/O worker #36}" daemon [_thread_in_native, id=117293, stack(0x00007fcb2c840000,0x00007fcb2c881000)]
  0x00007fcb00d3a800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#35]{New I/O worker #35}" daemon [_thread_in_native, id=117292, stack(0x00007fcb2c881000,0x00007fcb2c8c2000)]
  0x00007fcb00d0e000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#34]{New I/O worker #34}" daemon [_thread_in_native, id=117291, stack(0x00007fcb2c8c2000,0x00007fcb2c903000)]
  0x00007fcb00ce2000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#33]{New I/O worker #33}" daemon [_thread_in_native, id=117290, stack(0x00007fcb2c903000,0x00007fcb2c944000)]
  0x00007fcb00cb6000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#32]{New I/O worker #32}" daemon [_thread_in_native, id=117289, stack(0x00007fcb2c944000,0x00007fcb2c985000)]
  0x00007fcb00c8a000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#31]{New I/O worker #31}" daemon [_thread_in_native, id=117288, stack(0x00007fcb2c985000,0x00007fcb2c9c6000)]
  0x00007fcb00c5e000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#30]{New I/O worker #30}" daemon [_thread_in_native, id=117287, stack(0x00007fcb2c9c6000,0x00007fcb2ca07000)]
  0x00007fcb00c32000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#29]{New I/O worker #29}" daemon [_thread_in_native, id=117286, stack(0x00007fcb2ca07000,0x00007fcb2ca48000)]
  0x00007fcb00c06000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#28]{New I/O worker #28}" daemon [_thread_in_native, id=117285, stack(0x00007fcb2ca48000,0x00007fcb2ca89000)]
  0x00007fcb00bda000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#27]{New I/O worker #27}" daemon [_thread_in_native, id=117284, stack(0x00007fcb2ca89000,0x00007fcb2caca000)]
  0x00007fcb00bae000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#26]{New I/O worker #26}" daemon [_thread_in_native, id=117283, stack(0x00007fcb2caca000,0x00007fcb2cb0b000)]
  0x00007fcb00b82000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#25]{New I/O worker #25}" daemon [_thread_in_native, id=117282, stack(0x00007fcb2cb0b000,0x00007fcb2cb4c000)]
  0x00007fcb00b56000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#24]{New I/O worker #24}" daemon [_thread_in_native, id=117281, stack(0x00007fcb2cb4c000,0x00007fcb2cb8d000)]
  0x00007fcb00b2a000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#23]{New I/O worker #23}" daemon [_thread_in_native, id=117280, stack(0x00007fcb2cb8d000,0x00007fcb2cbce000)]
  0x00007fcb00afe000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#22]{New I/O worker #22}" daemon [_thread_in_native, id=117279, stack(0x00007fcb2cbce000,0x00007fcb2cc0f000)]
  0x00007fcb00ad2000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#21]{New I/O worker #21}" daemon [_thread_in_native, id=117278, stack(0x00007fcb2cc0f000,0x00007fcb2cc50000)]
  0x00007fcb00aa6000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#20]{New I/O worker #20}" daemon [_thread_in_native, id=117277, stack(0x00007fcb2cc50000,0x00007fcb2cc91000)]
  0x00007fcb00a7a000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#19]{New I/O worker #19}" daemon [_thread_in_native, id=117276, stack(0x00007fcb2cc91000,0x00007fcb2ccd2000)]
  0x00007fcb00a4e000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#18]{New I/O worker #18}" daemon [_thread_in_native, id=117275, stack(0x00007fcb2ccd2000,0x00007fcb2cd13000)]
  0x00007fcb00a22800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#17]{New I/O worker #17}" daemon [_thread_in_native, id=117274, stack(0x00007fcb2cd13000,0x00007fcb2cd54000)]
  0x00007fcb009f6800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#16]{New I/O worker #16}" daemon [_thread_in_native, id=117273, stack(0x00007fcb2cd54000,0x00007fcb2cd95000)]
  0x00007fcb009ca800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#15]{New I/O worker #15}" daemon [_thread_in_native, id=117272, stack(0x00007fcb2cd95000,0x00007fcb2cdd6000)]
  0x00007fcb0099f000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#14]{New I/O worker #14}" daemon [_thread_in_native, id=117271, stack(0x00007fcb2cdd6000,0x00007fcb2ce17000)]
  0x00007fcb0075f800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#13]{New I/O worker #13}" daemon [_thread_in_native, id=117270, stack(0x00007fcb2ce17000,0x00007fcb2ce58000)]
  0x00007fcb0075d800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#12]{New I/O worker #12}" daemon [_thread_in_native, id=117269, stack(0x00007fcb2ce58000,0x00007fcb2ce99000)]
  0x00007fcb006e4800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#11]{New I/O worker #11}" daemon [_thread_in_native, id=117268, stack(0x00007fcb2ce99000,0x00007fcb2ceda000)]
  0x00007fcb0047a000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#10]{New I/O worker #10}" daemon [_thread_in_native, id=117267, stack(0x00007fcb2ceda000,0x00007fcb2cf1b000)]
  0x00007fcb00438800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#9]{New I/O worker #9}" daemon [_thread_in_native, id=117266, stack(0x00007fcb2cf1b000,0x00007fcb2cf5c000)]
  0x00007fcb00425800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#8]{New I/O worker #8}" daemon [_thread_in_native, id=117265, stack(0x00007fcb2cf5c000,0x00007fcb2cf9d000)]
  0x00007fcb0046f000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#7]{New I/O worker #7}" daemon [_thread_in_native, id=117264, stack(0x00007fcb2cf9d000,0x00007fcb2cfde000)]
  0x00007fcb00725800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#6]{New I/O worker #6}" daemon [_thread_in_native, id=117263, stack(0x00007fcb2cfde000,0x00007fcb2d01f000)]
  0x00007fcb00452800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#5]{New I/O worker #5}" daemon [_thread_in_native, id=117262, stack(0x00007fcb2d01f000,0x00007fcb2d060000)]
  0x00007fcb00411000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#4]{New I/O worker #4}" daemon [_thread_in_native, id=117261, stack(0x00007fcb2d060000,0x00007fcb2d0a1000)]
  0x00007fcb00401800 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#3]{New I/O worker #3}" daemon [_thread_in_native, id=117260, stack(0x00007fcb2d0a1000,0x00007fcb2d0e2000)]
  0x00007fcb0072d000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#2]{New I/O worker #2}" daemon [_thread_in_native, id=117259, stack(0x00007fcb2d0e2000,0x00007fcb2d123000)]
  0x00007fcb00447000 JavaThread "elasticsearch[node-173.68][transport_client_worker][T#1]{New I/O worker #1}" daemon [_thread_in_native, id=117258, stack(0x00007fcb2d123000,0x00007fcb2d164000)]
  0x00007fcb00486000 JavaThread "elasticsearch[node-173.68][[ttl_expire]]" daemon [_thread_blocked, id=117257, stack(0x00007fcb2d164000,0x00007fcb2d1a5000)]
  0x00007fcb003f4000 JavaThread "elasticsearch[node-173.68][thrift_server][T#1]" daemon [_thread_in_native, id=117256, stack(0x00007fcf98039000,0x00007fcf9807a000)]
  0x00007fcb0071a800 JavaThread "elasticsearch[node-173.68][scheduler][T#1]" daemon [_thread_blocked, id=117255, stack(0x00007fcf980bb000,0x00007fcf980fc000)]
  0x00007fcb00468000 JavaThread "elasticsearch[node-173.68][[timer]]" daemon [_thread_blocked, id=117254, stack(0x00007fcb2d1a5000,0x00007fcb2d1e6000)]
  0x00007fd058009000 JavaThread "DestroyJavaVM" [_thread_blocked, id=117185, stack(0x00007fd05fde4000,0x00007fd05fe25000)]
  0x00007fd05968d800 JavaThread "Wrapper-Connection" daemon [_thread_in_native, id=117251, stack(0x00007fcf9807a000,0x00007fcf980bb000)]
  0x00007fd0596b1000 JavaThread "Wrapper-Control-Event-Monitor" daemon [_thread_blocked, id=117249, stack(0x00007fcf980fc000,0x00007fcf9813d000)]
  0x00007fd05962e800 JavaThread "Low Memory Detector" daemon [_thread_blocked, id=117247, stack(0x00007fcf98345000,0x00007fcf98386000)]
  0x00007fd05962c000 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=117246, stack(0x00007fcf98386000,0x00007fcf98487000)]
  0x00007fd05962a000 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=117245, stack(0x00007fcf98487000,0x00007fcf98588000)]
  0x00007fd059627800 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=117244, stack(0x00007fcf98588000,0x00007fcf985c9000)]
  0x00007fd059625800 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=117243, stack(0x00007fcf985c9000,0x00007fcf9860a000)]
  0x00007fd059607800 JavaThread "Finalizer" daemon [_thread_blocked, id=117242, stack(0x00007fd03c00b000,0x00007fd03c04c000)]
  0x00007fd059605800 JavaThread "Reference Handler" daemon [_thread_blocked, id=117241, stack(0x00007fd03c04c000,0x00007fd03c08d000)]

Other Threads:
  0x00007fd0595fe000 VMThread [stack: 0x00007fcf9860a000,0x00007fcf9870b000] [id=117240]
  0x00007fd059641000 WatcherThread [stack: 0x00007fcf98244000,0x00007fcf98345000] [id=117248]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap
 garbage-first heap   total 16777216K, used 4594228K [0x00007fcb8c000000, 0x00007fcf8c000000, 0x00007fcf8c000000)
  region size 8192K, 524 young (4292608K), 1 survivors (8192K)
 compacting perm gen  total 131072K, used 42788K [0x00007fcf8c000000, 0x00007fcf94000000, 0x00007fcf94000000)
   the space 131072K,  32% used [0x00007fcf8c000000, 0x00007fcf8e9c9280, 0x00007fcf8e9c9400, 0x00007fcf94000000)
No shared spaces configured.

Code Cache  [0x00007fd05c87b000, 0x00007fd05cdfb000, 0x00007fd05f87b000)
 total_blobs=2014 nmethods=1616 adapters=351 free_code_cache=44948928 largest_free_block=123392

Dynamic libraries:
40000000-40009000 r-xp 00000000 08:03 32769411                           /export/servers/jdk1.6.0_25/bin/java
40108000-4010a000 rwxp 00008000 08:03 32769411                           /export/servers/jdk1.6.0_25/bin/java
403fb000-4041c000 rwxp 00000000 00:00 0                                  [heap]
3d72c00000-3d72c20000 r-xp 00000000 08:01 266790                         /lib64/ld-2.12.so
3d72e1f000-3d72e20000 r-xp 0001f000 08:01 266790                         /lib64/ld-2.12.so
3d72e20000-3d72e21000 rwxp 00020000 08:01 266790                         /lib64/ld-2.12.so
3d72e21000-3d72e22000 rwxp 00000000 00:00 0 
3d73000000-3d73002000 r-xp 00000000 08:01 266793                         /lib64/libdl-2.12.so
3d73002000-3d73202000 ---p 00002000 08:01 266793                         /lib64/libdl-2.12.so
3d73202000-3d73203000 r-xp 00002000 08:01 266793                         /lib64/libdl-2.12.so
3d73203000-3d73204000 rwxp 00003000 08:01 266793                         /lib64/libdl-2.12.so
3d73400000-3d7358a000 r-xp 00000000 08:01 266791                         /lib64/libc-2.12.so
3d7358a000-3d7378a000 ---p 0018a000 08:01 266791                         /lib64/libc-2.12.so
3d7378a000-3d7378e000 r-xp 0018a000 08:01 266791                         /lib64/libc-2.12.so
3d7378e000-3d7378f000 rwxp 0018e000 08:01 266791                         /lib64/libc-2.12.so
3d7378f000-3d73794000 rwxp 00000000 00:00 0 
3d73800000-3d73817000 r-xp 00000000 08:01 261272                         /lib64/libpthread-2.12.so
3d73817000-3d73a17000 ---p 00017000 08:01 261272                         /lib64/libpthread-2.12.so
3d73a17000-3d73a18000 r-xp 00017000 08:01 261272                         /lib64/libpthread-2.12.so
3d73a18000-3d73a19000 rwxp 00018000 08:01 261272                         /lib64/libpthread-2.12.so
3d73a19000-3d73a1d000 rwxp 00000000 00:00 0 
3d73c00000-3d73c07000 r-xp 00000000 08:01 266798                         /lib64/librt-2.12.so
3d73c07000-3d73e06000 ---p 00007000 08:01 266798                         /lib64/librt-2.12.so
3d73e06000-3d73e07000 r-xp 00006000 08:01 266798                         /lib64/librt-2.12.so
3d73e07000-3d73e08000 rwxp 00007000 08:01 266798                         /lib64/librt-2.12.so
3d74000000-3d74083000 r-xp 00000000 08:01 261235                         /lib64/libm-2.12.so
3d74083000-3d74282000 ---p 00083000 08:01 261235                         /lib64/libm-2.12.so
3d74282000-3d74283000 r-xp 00082000 08:01 261235                         /lib64/libm-2.12.so
3d74283000-3d74284000 rwxp 00083000 08:01 261235                         /lib64/libm-2.12.so
3d75800000-3d75816000 r-xp 00000000 08:01 261477                         /lib64/libnsl-2.12.so
3d75816000-3d75a15000 ---p 00016000 08:01 261477                         /lib64/libnsl-2.12.so
3d75a15000-3d75a16000 r-xp 00015000 08:01 261477                         /lib64/libnsl-2.12.so
3d75a16000-3d75a17000 rwxp 00016000 08:01 261477                         /lib64/libnsl-2.12.so
3d75a17000-3d75a19000 rwxp 00000000 00:00 0 
7fc80c000000-7fc80dd1b000 rwxp 00000000 00:00 0 
7fc80dd1b000-7fc810000000 ---p 00000000 00:00 0 
7fc814000000-7fc814021000 rwxp 00000000 00:00 0 
7fc814021000-7fc818000000 ---p 00000000 00:00 0 
7fc81c000000-7fc81c021000 rwxp 00000000 00:00 0 
7fc81c021000-7fc820000000 ---p 00000000 00:00 0 
7fc820000000-7fc820021000 rwxp 00000000 00:00 0 
7fc820021000-7fc824000000 ---p 00000000 00:00 0 
7fc824000000-7fc824021000 rwxp 00000000 00:00 0 
7fc824021000-7fc828000000 ---p 00000000 00:00 0 
7fc828000000-7fc828021000 rwxp 00000000 00:00 0 
7fc828021000-7fc82c000000 ---p 00000000 00:00 0 
7fc82c000000-7fc82c021000 rwxp 00000000 00:00 0 
7fc82c021000-7fc830000000 ---p 00000000 00:00 0 
7fc830000000-7fc830021000 rwxp 00000000 00:00 0 
7fc830021000-7fc834000000 ---p 00000000 00:00 0 
7fc834000000-7fc834021000 rwxp 00000000 00:00 0 
7fc834021000-7fc838000000 ---p 00000000 00:00 0 
7fc838000000-7fc838021000 rwxp 00000000 00:00 0 
7fc838021000-7fc83c000000 ---p 00000000 00:00 0 
7fc83c000000-7fc83c021000 rwxp 00000000 00:00 0 
7fc83c021000-7fc840000000 ---p 00000000 00:00 0 
7fc840000000-7fc840021000 rwxp 00000000 00:00 0 
7fc840021000-7fc844000000 ---p 00000000 00:00 0 
7fc844000000-7fc844021000 rwxp 00000000 00:00 0 
7fc844021000-7fc848000000 ---p 00000000 00:00 0 
7fc848000000-7fc848021000 rwxp 00000000 00:00 0 
7fc848021000-7fc84c000000 ---p 00000000 00:00 0 
7fc84c000000-7fc84c021000 rwxp 00000000 00:00 0 
7fc84c021000-7fc850000000 ---p 00000000 00:00 0 
7fc850000000-7fc850021000 rwxp 00000000 00:00 0 
7fc850021000-7fc854000000 ---p 00000000 00:00 0 
7fc854000000-7fc854021000 rwxp 00000000 00:00 0 
7fc854021000-7fc858000000 ---p 00000000 00:00 0 
7fc858000000-7fc858021000 rwxp 00000000 00:00 0 
7fc858021000-7fc85c000000 ---p 00000000 00:00 0 
7fc85c000000-7fc85c021000 rwxp 00000000 00:00 0 
7fc85c021000-7fc860000000 ---p 00000000 00:00 0 
7fc860000000-7fc860021000 rwxp 00000000 00:00 0 
7fc860021000-7fc864000000 ---p 00000000 00:00 0 
7fc864000000-7fc864021000 rwxp 00000000 00:00 0 
7fc864021000-7fc868000000 ---p 00000000 00:00 0 
7fc868000000-7fc868021000 rwxp 00000000 00:00 0 
7fc868021000-7fc86c000000 ---p 00000000 00:00 0 
7fc86c000000-7fc86c021000 rwxp 00000000 00:00 0 
7fc86c021000-7fc870000000 ---p 00000000 00:00 0 
7fc870000000-7fc870021000 rwxp 00000000 00:00 0 
7fc870021000-7fc874000000 ---p 00000000 00:00 0 
7fc874000000-7fc874021000 rwxp 00000000 00:00 0 
7fc874021000-7fc878000000 ---p 00000000 00:00 0 
7fc878000000-7fc878021000 rwxp 00000000 00:00 0 
7fc878021000-7fc87c000000 ---p 00000000 00:00 0 
7fc87c000000-7fc87c021000 rwxp 00000000 00:00 0 
7fc87c021000-7fc880000000 ---p 00000000 00:00 0 
7fc880000000-7fc880021000 rwxp 00000000 00:00 0 
7fc880021000-7fc884000000 ---p 00000000 00:00 0 
7fc884000000-7fc884021000 rwxp 00000000 00:00 0 
7fc884021000-7fc888000000 ---p 00000000 00:00 0 
7fc888000000-7fc888021000 rwxp 00000000 00:00 0 
7fc888021000-7fc88c000000 ---p 00000000 00:00 0 
7fc88c000000-7fc88c021000 rwxp 00000000 00:00 0 
7fc88c021000-7fc890000000 ---p 00000000 00:00 0 
7fc890000000-7fc890021000 rwxp 00000000 00:00 0 
7fc890021000-7fc894000000 ---p 00000000 00:00 0 
7fc894000000-7fc894021000 rwxp 00000000 00:00 0 
7fc894021000-7fc898000000 ---p 00000000 00:00 0 
7fc898000000-7fc898021000 rwxp 00000000 00:00 0 
7fc898021000-7fc89c000000 ---p 00000000 00:00 0 
7fc89c000000-7fc89c021000 rwxp 00000000 00:00 0 
7fc89c021000-7fc8a0000000 ---p 00000000 00:00 0 
7fc8a0000000-7fc8a0021000 rwxp 00000000 00:00 0 
7fc8a0021000-7fc8a4000000 ---p 00000000 00:00 0 
7fc8a4000000-7fc8a4021000 rwxp 00000000 00:00 0 
7fc8a4021000-7fc8a8000000 ---p 00000000 00:00 0 
7fc8a8000000-7fc8a8021000 rwxp 00000000 00:00 0 
7fc8a8021000-7fc8ac000000 ---p 00000000 00:00 0 
7fc8ac000000-7fc8ac021000 rwxp 00000000 00:00 0 
7fc8ac021000-7fc8b0000000 ---p 00000000 00:00 0 
7fc8b0000000-7fc8b0021000 rwxp 00000000 00:00 0 
7fc8b0021000-7fc8b4000000 ---p 00000000 00:00 0 
7fc8b4000000-7fc8b4021000 rwxp 00000000 00:00 0 
7fc8b4021000-7fc8b8000000 ---p 00000000 00:00 0 
7fc8b8000000-7fc8b8021000 rwxp 00000000 00:00 0 
7fc8b8021000-7fc8bc000000 ---p 00000000 00:00 0 
7fc8bc000000-7fc8bc03f000 rwxp 00000000 00:00 0 
7fc8bc03f000-7fc8c0000000 ---p 00000000 00:00 0 
7fc8c0000000-7fc8c0021000 rwxp 00000000 00:00 0 
7fc8c0021000-7fc8c4000000 ---p 00000000 00:00 0 
7fc8c4000000-7fc8c4021000 rwxp 00000000 00:00 0 
7fc8c4021000-7fc8c8000000 ---p 00000000 00:00 0 
7fc8c8000000-7fc8c8021000 rwxp 00000000 00:00 0 
7fc8c8021000-7fc8cc000000 ---p 00000000 00:00 0 
7fc8cc000000-7fc8cc021000 rwxp 00000000 00:00 0 
7fc8cc021000-7fc8d0000000 ---p 00000000 00:00 0 
7fc8d0000000-7fc8d0021000 rwxp 00000000 00:00 0 
7fc8d0021000-7fc8d4000000 ---p 00000000 00:00 0 
7fc8d4000000-7fc8d464e000 rwxp 00000000 00:00 0 
7fc8d464e000-7fc8d8000000 ---p 00000000 00:00 0 
7fc8d8000000-7fc8d8021000 rwxp 00000000 00:00 0 
7fc8d8021000-7fc8dc000000 ---p 00000000 00:00 0 
7fc8dc000000-7fc8dc021000 rwxp 00000000 00:00 0 
7fc8dc021000-7fc8e0000000 ---p 00000000 00:00 0 
7fc8e0000000-7fc8e0029000 rwxp 00000000 00:00 0 
7fc8e0029000-7fc8e4000000 ---p 00000000 00:00 0 
7fc8e4000000-7fc8e4021000 rwxp 00000000 00:00 0 
7fc8e4021000-7fc8e8000000 ---p 00000000 00:00 0 
7fc8e8000000-7fc8e8021000 rwxp 00000000 00:00 0 
7fc8e8021000-7fc8ec000000 ---p 00000000 00:00 0 
7fc8ec000000-7fc8ec021000 rwxp 00000000 00:00 0 
7fc8ec021000-7fc8f0000000 ---p 00000000 00:00 0 
7fc8f0000000-7fc8f04e1000 rwxp 00000000 00:00 0 
7fc8f04e1000-7fc8f4000000 ---p 00000000 00:00 0 
7fc8f4000000-7fc8f4082000 rwxp 00000000 00:00 0 
7fc8f4082000-7fc8f8000000 ---p 00000000 00:00 0 
7fc8f8000000-7fc8f8082000 rwxp 00000000 00:00 0 
7fc8f8082000-7fc8fc000000 ---p 00000000 00:00 0 
7fc8fc000000-7fc8fc082000 rwxp 00000000 00:00 0 
7fc8fc082000-7fc900000000 ---p 00000000 00:00 0 
7fc900000000-7fc900082000 rwxp 00000000 00:00 0 
7fc900082000-7fc904000000 ---p 00000000 00:00 0 
7fc904000000-7fc904082000 rwxp 00000000 00:00 0 
7fc904082000-7fc908000000 ---p 00000000 00:00 0 
7fc908000000-7fc908082000 rwxp 00000000 00:00 0 
7fc908082000-7fc90c000000 ---p 00000000 00:00 0 
7fc90c000000-7fc90c082000 rwxp 00000000 00:00 0 
7fc90c082000-7fc910000000 ---p 00000000 00:00 0 
7fc910000000-7fc910083000 rwxp 00000000 00:00 0 
7fc910083000-7fc914000000 ---p 00000000 00:00 0 
7fc914000000-7fc914082000 rwxp 00000000 00:00 0 
7fc914082000-7fc918000000 ---p 00000000 00:00 0 
7fc918000000-7fc918082000 rwxp 00000000 00:00 0 
7fc918082000-7fc91c000000 ---p 00000000 00:00 0 
7fc91c000000-7fc91c082000 rwxp 00000000 00:00 0 
7fc91c082000-7fc920000000 ---p 00000000 00:00 0 
7fc920000000-7fc920082000 rwxp 00000000 00:00 0 
7fc920082000-7fc924000000 ---p 00000000 00:00 0 
7fc924000000-7fc924084000 rwxp 00000000 00:00 0 
7fc924084000-7fc928000000 ---p 00000000 00:00 0 
7fc928000000-7fc928082000 rwxp 00000000 00:00 0 
7fc928082000-7fc92c000000 ---p 00000000 00:00 0 
7fc92c000000-7fc92c082000 rwxp 00000000 00:00 0 
7fc92c082000-7fc930000000 ---p 00000000 00:00 0 
7fc930000000-7fc930082000 rwxp 00000000 00:00 0 
7fc930082000-7fc934000000 ---p 00000000 00:00 0 
7fc934000000-7fc934084000 rwxp 00000000 00:00 0 
7fc934084000-7fc938000000 ---p 00000000 00:00 0 
7fc938000000-7fc938082000 rwxp 00000000 00:00 0 
7fc938082000-7fc93c000000 ---p 00000000 00:00 0 
7fc93c000000-7fc93c082000 rwxp 00000000 00:00 0 
7fc93c082000-7fc940000000 ---p 00000000 00:00 0 
7fc940000000-7fc940082000 rwxp 00000000 00:00 0 
7fc940082000-7fc944000000 ---p 00000000 00:00 0 
7fc944000000-7fc944082000 rwxp 00000000 00:00 0 
7fc944082000-7fc948000000 ---p 00000000 00:00 0 
7fc948000000-7fc948082000 rwxp 00000000 00:00 0 
7fc948082000-7fc94c000000 ---p 00000000 00:00 0 
7fc94c000000-7fc94c083000 rwxp 00000000 00:00 0 
7fc94c083000-7fc950000000 ---p 00000000 00:00 0 
7fc950000000-7fc950087000 rwxp 00000000 00:00 0 
7fc950087000-7fc954000000 ---p 00000000 00:00 0 
7fc954000000-7fc954082000 rwxp 00000000 00:00 0 
7fc954082000-7fc958000000 ---p 00000000 00:00 0 
7fc958000000-7fc958082000 rwxp 00000000 00:00 0 
7fc958082000-7fc95c000000 ---p 00000000 00:00 0 
7fc95c000000-7fc95c082000 rwxp 00000000 00:00 0 
7fc95c082000-7fc960000000 ---p 00000000 00:00 0 
7fc960000000-7fc960082000 rwxp 00000000 00:00 0 
7fc960082000-7fc964000000 ---p 00000000 00:00 0 
7fc964000000-7fc964082000 rwxp 00000000 00:00 0 
7fc964082000-7fc968000000 ---p 00000000 00:00 0 
7fc968000000-7fc968082000 rwxp 00000000 00:00 0 
7fc968082000-7fc96c000000 ---p 00000000 00:00 0 
7fc96c000000-7fc96c082000 rwxp 00000000 00:00 0 
7fc96c082000-7fc970000000 ---p 00000000 00:00 0 
7fc970000000-7fc970082000 rwxp 00000000 00:00 0 
7fc970082000-7fc974000000 ---p 00000000 00:00 0 
7fc974000000-7fc974082000 rwxp 00000000 00:00 0 
7fc974082000-7fc978000000 ---p 00000000 00:00 0 
7fc978000000-7fc978082000 rwxp 00000000 00:00 0 
7fc978082000-7fc97c000000 ---p 00000000 00:00 0 
7fc97c000000-7fc97c082000 rwxp 00000000 00:00 0 
7fc97c082000-7fc980000000 ---p 00000000 00:00 0 
7fc980000000-7fc980082000 rwxp 00000000 00:00 0 
7fc980082000-7fc984000000 ---p 00000000 00:00 0 
7fc984000000-7fc984082000 rwxp 00000000 00:00 0 
7fc984082000-7fc988000000 ---p 00000000 00:00 0 
7fc988000000-7fc988082000 rwxp 00000000 00:00 0 
7fc988082000-7fc98c000000 ---p 00000000 00:00 0 
7fc98c000000-7fc98c082000 rwxp 00000000 00:00 0 
7fc98c082000-7fc990000000 ---p 00000000 00:00 0 
7fc990000000-7fc990082000 rwxp 00000000 00:00 0 
7fc990082000-7fc994000000 ---p 00000000 00:00 0 
7fc994000000-7fc994082000 rwxp 00000000 00:00 0 
7fc994082000-7fc998000000 ---p 00000000 00:00 0 
7fc998000000-7fc998082000 rwxp 00000000 00:00 0 
7fc998082000-7fc99c000000 ---p 00000000 00:00 0 
7fc99c000000-7fc99c082000 rwxp 00000000 00:00 0 
7fc99c082000-7fc9a0000000 ---p 00000000 00:00 0 
7fc9a0000000-7fc9a0082000 rwxp 00000000 00:00 0 
7fc9a0082000-7fc9a4000000 ---p 00000000 00:00 0 
7fc9a4000000-7fc9a40b8000 rwxp 00000000 00:00 0 
7fc9a40b8000-7fc9a8000000 ---p 00000000 00:00 0 
7fc9a8000000-7fc9a8082000 rwxp 00000000 00:00 0 
7fc9a8082000-7fc9ac000000 ---p 00000000 00:00 0 
7fc9ac000000-7fc9ac082000 rwxp 00000000 00:00 0 
7fc9ac082000-7fc9b0000000 ---p 00000000 00:00 0 
7fc9b0000000-7fc9b0083000 rwxp 00000000 00:00 0 
7fc9b0083000-7fc9b4000000 ---p 00000000 00:00 0 
7fc9b4000000-7fc9b4083000 rwxp 00000000 00:00 0 
7fc9b4083000-7fc9b8000000 ---p 00000000 00:00 0 
7fc9b8000000-7fc9b8088000 rwxp 00000000 00:00 0 
7fc9b8088000-7fc9bc000000 ---p 00000000 00:00 0 
7fc9bc000000-7fc9bc082000 rwxp 00000000 00:00 0 
7fc9bc082000-7fc9c0000000 ---p 00000000 00:00 0 
7fc9c0000000-7fc9c00a5000 rwxp 00000000 00:00 0 
7fc9c00a5000-7fc9c4000000 ---p 00000000 00:00 0 
7fc9c4000000-7fc9c4021000 rwxp 00000000 00:00 0 
7fc9c4021000-7fc9c8000000 ---p 00000000 00:00 0 
7fc9c8000000-7fc9c8082000 rwxp 00000000 00:00 0 
7fc9c8082000-7fc9cc000000 ---p 00000000 00:00 0 
7fc9cc000000-7fc9cc082000 rwxp 00000000 00:00 0 
7fc9cc082000-7fc9d0000000 ---p 00000000 00:00 0 
7fc9d0000000-7fc9d0086000 rwxp 00000000 00:00 0 
7fc9d0086000-7fc9d4000000 ---p 00000000 00:00 0 
7fc9d4000000-7fc9d4082000 rwxp 00000000 00:00 0 
7fc9d4082000-7fc9d8000000 ---p 00000000 00:00 0 
7fc9d8000000-7fc9d8082000 rwxp 00000000 00:00 0 
7fc9d8082000-7fc9dc000000 ---p 00000000 00:00 0 
7fc9dc000000-7fc9dc082000 rwxp 00000000 00:00 0 
7fc9dc082000-7fc9e0000000 ---p 00000000 00:00 0 
7fc9e0000000-7fc9e0082000 rwxp 00000000 00:00 0 
7fc9e0082000-7fc9e4000000 ---p 00000000 00:00 0 
7fc9e4000000-7fc9e4082000 rwxp 00000000 00:00 0 
7fc9e4082000-7fc9e8000000 ---p 00000000 00:00 0 
7fc9e8000000-7fc9e8082000 rwxp 00000000 00:00 0 
7fc9e8082000-7fc9ec000000 ---p 00000000 00:00 0 
7fc9ed549000-7fc9ed54c000 ---p 00000000 00:00 0 
7fc9ed54c000-7fc9ed58a000 rwxp 00000000 00:00 0 
7fc9ed58a000-7fc9ed58d000 ---p 00000000 00:00 0 
7fc9ed58d000-7fc9ed5cb000 rwxp 00000000 00:00 0 
7fc9ed5cb000-7fc9ed5ce000 ---p 00000000 00:00 0 
7fc9ed5ce000-7fc9ed60c000 rwxp 00000000 00:00 0 
7fc9ed60c000-7fc9ed60f000 ---p 00000000 00:00 0 
7fc9ed60f000-7fc9ed64d000 rwxp 00000000 00:00 0 
7fc9ed64d000-7fc9ed650000 ---p 00000000 00:00 0 
7fc9ed650000-7fc9edd37000 rwxp 00000000 00:00 0 
7fc9edd37000-7fc9edd3a000 ---p 00000000 00:00 0 
7fc9edd3a000-7fc9edd78000 rwxp 00000000 00:00 0 
7fc9edd78000-7fc9edd7b000 ---p 00000000 00:00 0 
7fc9edd7b000-7fc9eddb9000 rwxp 00000000 00:00 0 
7fc9eddb9000-7fc9eddbc000 ---p 00000000 00:00 0 
7fc9eddbc000-7fc9eddfa000 rwxp 00000000 00:00 0 
7fc9eddfa000-7fc9eddfd000 ---p 00000000 00:00 0 
7fc9eddfd000-7fc9ede3b000 rwxp 00000000 00:00 0 
7fc9ede3b000-7fc9ede3e000 ---p 00000000 00:00 0 
7fc9ede3e000-7fc9ede7c000 rwxp 00000000 00:00 0 
7fc9ede7c000-7fc9ede7f000 ---p 00000000 00:00 0 
7fc9ede7f000-7fc9edebd000 rwxp 00000000 00:00 0 
7fc9edebd000-7fc9edec0000 ---p 00000000 00:00 0 
7fc9edec0000-7fc9edefe000 rwxp 00000000 00:00 0 
7fc9edefe000-7fc9edf01000 ---p 00000000 00:00 0 
7fc9edf01000-7fc9edf3f000 rwxp 00000000 00:00 0 
7fc9edf3f000-7fc9edf42000 ---p 00000000 00:00 0 
7fc9edf42000-7fc9edf80000 rwxp 00000000 00:00 0 
7fc9edf80000-7fc9edf83000 ---p 00000000 00:00 0 
7fc9edf83000-7fc9edfc1000 rwxp 00000000 00:00 0 
7fc9edfc1000-7fc9edfc4000 ---p 00000000 00:00 0 
7fc9edfc4000-7fc9ee002000 rwxp 00000000 00:00 0 
7fc9ee002000-7fc9ee005000 ---p 00000000 00:00 0 
7fc9ee005000-7fc9ee043000 rwxp 00000000 00:00 0 
7fc9ee043000-7fc9ee046000 ---p 00000000 00:00 0 
7fc9ee046000-7fc9ee084000 rwxp 00000000 00:00 0 
7fc9ee084000-7fc9ee087000 ---p 00000000 00:00 0 
7fc9ee087000-7fc9ee0c5000 rwxp 00000000 00:00 0 
7fc9ee0c5000-7fc9ee0c8000 ---p 00000000 00:00 0 
7fc9ee0c8000-7fc9ee106000 rwxp 00000000 00:00 0 
7fc9ee106000-7fc9ee109000 ---p 00000000 00:00 0 
7fc9ee109000-7fc9ee147000 rwxp 00000000 00:00 0 
7fc9ee147000-7fc9ee14a000 ---p 00000000 00:00 0 
7fc9ee14a000-7fc9ee188000 rwxp 00000000 00:00 0 
7fc9ee188000-7fc9ee18b000 ---p 00000000 00:00 0 
7fc9ee18b000-7fc9ee1c9000 rwxp 00000000 00:00 0 
7fc9ee1c9000-7fc9ee1cc000 ---p 00000000 00:00 0 
7fc9ee1cc000-7fc9ee20a000 rwxp 00000000 00:00 0 
7fc9ee20a000-7fc9ee20d000 ---p 00000000 00:00 0 
7fc9ee20d000-7fc9ee24b000 rwxp 00000000 00:00 0 
7fc9ee24b000-7fc9ee24e000 ---p 00000000 00:00 0 
7fc9ee24e000-7fc9ee28c000 rwxp 00000000 00:00 0 
7fc9ee28c000-7fc9ee28f000 ---p 00000000 00:00 0 
7fc9ee28f000-7fc9ee2cd000 rwxp 00000000 00:00 0 
7fc9ee2cd000-7fc9ee2d0000 ---p 00000000 00:00 0 
7fc9ee2d0000-7fc9ee30e000 rwxp 00000000 00:00 0 
7fc9ee30e000-7fc9ee311000 ---p 00000000 00:00 0 
7fc9ee311000-7fc9ee34f000 rwxp 00000000 00:00 0 
7fc9ee34f000-7fc9ee352000 ---p 00000000 00:00 0 
7fc9ee352000-7fc9ee390000 rwxp 00000000 00:00 0 
7fc9ee390000-7fc9ee393000 ---p 00000000 00:00 0 
7fc9ee393000-7fc9ee3d1000 rwxp 00000000 00:00 0 
7fc9ee3d1000-7fc9ee3d4000 ---p 00000000 00:00 0 
7fc9ee3d4000-7fc9ee412000 rwxp 00000000 00:00 0 
7fc9ee412000-7fc9ee415000 ---p 00000000 00:00 0 
7fc9ee415000-7fc9ee453000 rwxp 00000000 00:00 0 
7fc9ee453000-7fc9ee456000 ---p 00000000 00:00 0 
7fc9ee456000-7fc9ee494000 rwxp 00000000 00:00 0 
7fc9ee494000-7fc9ee497000 ---p 00000000 00:00 0 
7fc9ee497000-7fc9ee4d5000 rwxp 00000000 00:00 0 
7fc9ee4d5000-7fc9ee4d8000 ---p 00000000 00:00 0 
7fc9ee4d8000-7fc9ee516000 rwxp 00000000 00:00 0 
7fc9ee516000-7fc9ee519000 ---p 00000000 00:00 0 
7fc9ee519000-7fc9ee557000 rwxp 00000000 00:00 0 
7fc9ee557000-7fc9ee55a000 ---p 00000000 00:00 0 
7fc9ee55a000-7fc9ee598000 rwxp 00000000 00:00 0 
7fc9ee598000-7fc9ee59b000 ---p 00000000 00:00 0 
7fc9ee59b000-7fc9ee5d9000 rwxp 00000000 00:00 0 
7fc9ee5d9000-7fc9ee5dc000 ---p 00000000 00:00 0 
7fc9ee5dc000-7fc9ee61a000 rwxp 00000000 00:00 0 
7fc9ee61a000-7fc9ee61d000 ---p 00000000 00:00 0 
7fc9ee61d000-7fc9ee65b000 rwxp 00000000 00:00 0 
7fc9ee65b000-7fc9ee65e000 ---p 00000000 00:00 0 
7fc9ee65e000-7fc9ee69c000 rwxp 00000000 00:00 0 
7fc9ee69c000-7fc9ee69f000 ---p 00000000 00:00 0 
7fc9ee69f000-7fc9ee6dd000 rwxp 00000000 00:00 0 
7fc9ee6dd000-7fc9ee6e0000 ---p 00000000 00:00 0 
7fc9ee6e0000-7fc9ee71e000 rwxp 00000000 00:00 0 
7fc9ee71e000-7fc9ee721000 ---p 00000000 00:00 0 
7fc9ee721000-7fc9ee75f000 rwxp 00000000 00:00 0 
7fc9ee75f000-7fc9ee762000 ---p 00000000 00:00 0 
7fc9ee762000-7fc9ee7a0000 rwxp 00000000 00:00 0 
7fc9ee7a0000-7fc9ee7a3000 ---p 00000000 00:00 0 
7fc9ee7a3000-7fc9ee7e1000 rwxp 00000000 00:00 0 
7fc9ee7e1000-7fc9ee7e4000 ---p 00000000 00:00 0 
7fc9ee7e4000-7fc9ee822000 rwxp 00000000 00:00 0 
7fc9ee822000-7fc9ee825000 ---p 00000000 00:00 0 
7fc9ee825000-7fc9ee863000 rwxp 00000000 00:00 0 
7fc9ee863000-7fc9ee866000 ---p 00000000 00:00 0 
7fc9ee866000-7fc9ee8a4000 rwxp 00000000 00:00 0 
7fc9ee8a4000-7fc9ee8a7000 ---p 00000000 00:00 0 
7fc9ee8a7000-7fc9ee8e5000 rwxp 00000000 00:00 0 
7fc9ee8e5000-7fc9ee8e8000 ---p 00000000 00:00 0 
7fc9ee8e8000-7fc9ee926000 rwxp 00000000 00:00 0 
7fc9ee926000-7fc9ee929000 ---p 00000000 00:00 0 
7fc9ee929000-7fc9ee967000 rwxp 00000000 00:00 0 
7fc9ee967000-7fc9ee96a000 ---p 00000000 00:00 0 
7fc9ee96a000-7fc9ee9a8000 rwxp 00000000 00:00 0 
7fc9ee9a8000-7fc9ee9ab000 ---p 00000000 00:00 0 
7fc9ee9ab000-7fc9ee9e9000 rwxp 00000000 00:00 0 
7fc9ee9e9000-7fc9ee9ec000 ---p 00000000 00:00 0 
7fc9ee9ec000-7fc9eea2a000 rwxp 00000000 00:00 0 
7fc9eea2a000-7fc9eea2d000 ---p 00000000 00:00 0 
7fc9eea2d000-7fc9eea6b000 rwxp 00000000 00:00 0 
7fc9eea6b000-7fc9eea6e000 ---p 00000000 00:00 0 
7fc9eea6e000-7fc9eeaac000 rwxp 00000000 00:00 0 
7fc9eeaac000-7fc9eeaaf000 ---p 00000000 00:00 0 
7fc9eeaaf000-7fc9eeaed000 rwxp 00000000 00:00 0 
7fc9eeaed000-7fc9eeaf0000 ---p 00000000 00:00 0 
7fc9eeaf0000-7fc9eeb2e000 rwxp 00000000 00:00 0 
7fc9eeb2e000-7fc9eeb31000 ---p 00000000 00:00 0 
7fc9eeb31000-7fc9eeb6f000 rwxp 00000000 00:00 0 
7fc9eeb6f000-7fc9eeb72000 ---p 00000000 00:00 0 
7fc9eeb72000-7fc9eebb0000 rwxp 00000000 00:00 0 
7fc9eebb0000-7fc9eebb3000 ---p 00000000 00:00 0 
7fc9eebb3000-7fc9eebf1000 rwxp 00000000 00:00 0 
7fc9eebf1000-7fc9eebf4000 ---p 00000000 00:00 0 
7fc9eebf4000-7fc9eec32000 rwxp 00000000 00:00 0 
7fc9eec32000-7fc9eec35000 ---p 00000000 00:00 0 
7fc9eec35000-7fc9eec73000 rwxp 00000000 00:00 0 
7fc9eec73000-7fc9eec76000 ---p 00000000 00:00 0 
7fc9eec76000-7fc9eecb4000 rwxp 00000000 00:00 0 
7fc9eecb4000-7fc9eecb7000 ---p 00000000 00:00 0 
7fc9eecb7000-7fc9eecf5000 rwxp 00000000 00:00 0 
7fc9eecf5000-7fc9eecf8000 ---p 00000000 00:00 0 
7fc9eecf8000-7fc9eed36000 rwxp 00000000 00:00 0 
7fc9eed36000-7fc9eed39000 ---p 00000000 00:00 0 
7fc9eed39000-7fc9eed77000 rwxp 00000000 00:00 0 
7fc9eed77000-7fc9eed7a000 ---p 00000000 00:00 0 
7fc9eed7a000-7fc9eedb8000 rwxp 00000000 00:00 0 
7fc9eedb8000-7fc9eedbb000 ---p 00000000 00:00 0 
7fc9eedbb000-7fc9eedf9000 rwxp 00000000 00:00 0 
7fc9eedf9000-7fc9eedfc000 ---p 00000000 00:00 0 
7fc9eedfc000-7fc9eee3a000 rwxp 00000000 00:00 0 
7fc9eee3a000-7fc9eee3d000 ---p 00000000 00:00 0 
7fc9eee3d000-7fc9eee7b000 rwxp 00000000 00:00 0 
7fc9eee7b000-7fc9eee7e000 ---p 00000000 00:00 0 
7fc9eee7e000-7fc9eeebc000 rwxp 00000000 00:00 0 
7fc9eeebc000-7fc9eeebf000 ---p 00000000 00:00 0 
7fc9eeebf000-7fc9eeefd000 rwxp 00000000 00:00 0 
7fc9eeefd000-7fc9eef00000 ---p 00000000 00:00 0 
7fc9eef00000-7fc9eef3e000 rwxp 00000000 00:00 0 
7fc9eef3e000-7fc9eef41000 ---p 00000000 00:00 0 
7fc9eef41000-7fc9eef7f000 rwxp 00000000 00:00 0 
7fc9eef7f000-7fc9eef82000 ---p 00000000 00:00 0 
7fc9eef82000-7fc9eefc0000 rwxp 00000000 00:00 0 
7fc9eefc0000-7fc9eefc3000 ---p 00000000 00:00 0 
7fc9eefc3000-7fc9ef001000 rwxp 00000000 00:00 0 
7fc9ef001000-7fc9ef004000 ---p 00000000 00:00 0 
7fc9ef004000-7fc9ef042000 rwxp 00000000 00:00 0 
7fc9ef042000-7fc9ef045000 ---p 00000000 00:00 0 
7fc9ef045000-7fc9ef083000 rwxp 00000000 00:00 0 
7fc9ef083000-7fc9ef086000 ---p 00000000 00:00 0 
7fc9ef086000-7fc9ef0c4000 rwxp 00000000 00:00 0 
7fc9ef0c4000-7fc9ef0c7000 ---p 00000000 00:00 0 
7fc9ef0c7000-7fc9ef105000 rwxp 00000000 00:00 0 
7fc9ef105000-7fc9ef108000 ---p 00000000 00:00 0 
7fc9ef108000-7fc9ef146000 rwxp 00000000 00:00 0 
7fc9ef146000-7fc9ef149000 ---p 00000000 00:00 0 
7fc9ef149000-7fc9ef187000 rwxp 00000000 00:00 0 
7fc9ef187000-7fc9ef18a000 ---p 00000000 00:00 0 
7fc9ef18a000-7fc9ef1c8000 rwxp 00000000 00:00 0 
7fc9ef1c8000-7fc9ef1cb000 ---p 00000000 00:00 0 
7fc9ef1cb000-7fc9ef209000 rwxp 00000000 00:00 0 
7fc9ef209000-7fc9ef20c000 ---p 00000000 00:00 0 
7fc9ef20c000-7fc9ef24a000 rwxp 00000000 00:00 0 
7fc9ef24a000-7fc9ef24d000 ---p 00000000 00:00 0 
7fc9ef24d000-7fc9ef28b000 rwxp 00000000 00:00 0 
7fc9ef28b000-7fc9ef28e000 ---p 00000000 00:00 0 
7fc9ef28e000-7fc9ef2cc000 rwxp 00000000 00:00 0 
7fc9ef2cc000-7fc9ef2cf000 ---p 00000000 00:00 0 
7fc9ef2cf000-7fc9ef30d000 rwxp 00000000 00:00 0 
7fc9ef30d000-7fc9ef310000 ---p 00000000 00:00 0 
7fc9ef310000-7fc9ef34e000 rwxp 00000000 00:00 0 
7fc9ef34e000-7fc9ef351000 ---p 00000000 00:00 0 
7fc9ef351000-7fc9ef38f000 rwxp 00000000 00:00 0 
7fc9ef38f000-7fc9ef392000 ---p 00000000 00:00 0 
7fc9ef392000-7fc9ef3d0000 rwxp 00000000 00:00 0 
7fc9ef3d0000-7fc9ef3d3000 ---p 00000000 00:00 0 
7fc9ef3d3000-7fc9ef411000 rwxp 00000000 00:00 0 
7fc9ef411000-7fc9ef414000 ---p 00000000 00:00 0 
7fc9ef414000-7fc9ef452000 rwxp 00000000 00:00 0 
7fc9ef452000-7fc9ef455000 ---p 00000000 00:00 0 
7fc9ef455000-7fc9ef493000 rwxp 00000000 00:00 0 
7fc9ef493000-7fc9ef496000 ---p 00000000 00:00 0 
7fc9ef496000-7fc9ef4d4000 rwxp 00000000 00:00 0 
7fc9ef4d4000-7fc9ef4d7000 ---p 00000000 00:00 0 
7fc9ef4d7000-7fc9ef515000 rwxp 00000000 00:00 0 
7fc9ef515000-7fc9ef518000 ---p 00000000 00:00 0 
7fc9ef518000-7fc9ef556000 rwxp 00000000 00:00 0 
7fc9ef556000-7fc9ef559000 ---p 00000000 00:00 0 
7fc9ef559000-7fc9ef597000 rwxp 00000000 00:00 0 
7fc9ef597000-7fc9ef59a000 ---p 00000000 00:00 0 
7fc9ef59a000-7fc9ef5d8000 rwxp 00000000 00:00 0 
7fc9ef5d8000-7fc9ef5db000 ---p 00000000 00:00 0 
7fc9ef5db000-7fc9ef619000 rwxp 00000000 00:00 0 
7fc9ef619000-7fc9ef61c000 ---p 00000000 00:00 0 
7fc9ef61c000-7fc9ef65a000 rwxp 00000000 00:00 0 
7fc9ef65a000-7fc9ef65d000 ---p 00000000 00:00 0 
7fc9ef65d000-7fc9ef69b000 rwxp 00000000 00:00 0 
7fc9ef69b000-7fc9ef69e000 ---p 00000000 00:00 0 
7fc9ef69e000-7fc9ef6dc000 rwxp 00000000 00:00 0 
7fc9ef6dc000-7fc9ef6df000 ---p 00000000 00:00 0 
7fc9ef6df000-7fc9ef71d000 rwxp 00000000 00:00 0 
7fc9ef71d000-7fc9ef720000 ---p 00000000 00:00 0 
7fc9ef720000-7fc9ef75e000 rwxp 00000000 00:00 0 
7fc9ef75e000-7fc9ef761000 ---p 00000000 00:00 0 
7fc9ef761000-7fc9ef79f000 rwxp 00000000 00:00 0 
7fc9ef79f000-7fc9ef7a2000 ---p 00000000 00:00 0 
7fc9ef7a2000-7fc9ef7e0000 rwxp 00000000 00:00 0 
7fc9ef7e0000-7fc9ef7e3000 ---p 00000000 00:00 0 
7fc9ef7e3000-7fc9ef821000 rwxp 00000000 00:00 0 
7fc9ef821000-7fc9ef824000 ---p 00000000 00:00 0 
7fc9ef824000-7fc9ef862000 rwxp 00000000 00:00 0 
7fc9ef862000-7fc9ef865000 ---p 00000000 00:00 0 
7fc9ef865000-7fc9ef8a3000 rwxp 00000000 00:00 0 
7fc9ef8a3000-7fc9ef8a6000 ---p 00000000 00:00 0 
7fc9ef8a6000-7fc9ef8e4000 rwxp 00000000 00:00 0 
7fc9ef8e4000-7fc9ef8e7000 ---p 00000000 00:00 0 
7fc9ef8e7000-7fc9ef925000 rwxp 00000000 00:00 0 
7fc9ef925000-7fc9ef928000 ---p 00000000 00:00 0 
7fc9ef928000-7fc9ef966000 rwxp 00000000 00:00 0 
7fc9ef966000-7fc9ef969000 ---p 00000000 00:00 0 
7fc9ef969000-7fc9ef9a7000 rwxp 00000000 00:00 0 
7fc9ef9a7000-7fc9ef9aa000 ---p 00000000 00:00 0 
7fc9ef9aa000-7fc9ef9e8000 rwxp 00000000 00:00 0 
7fc9ef9e8000-7fc9ef9eb000 ---p 00000000 00:00 0 
7fc9ef9eb000-7fc9efa29000 rwxp 00000000 00:00 0 
7fc9efa29000-7fc9efa2c000 ---p 00000000 00:00 0 
7fc9efa2c000-7fc9efa6a000 rwxp 00000000 00:00 0 
7fc9efa6a000-7fc9efa6d000 ---p 00000000 00:00 0 
7fc9efa6d000-7fc9efaab000 rwxp 00000000 00:00 0 
7fc9efaab000-7fc9efaae000 ---p 00000000 00:00 0 
7fc9efaae000-7fc9efaec000 rwxp 00000000 00:00 0 
7fc9efaec000-7fc9efaef000 ---p 00000000 00:00 0 
7fc9efaef000-7fc9efb2d000 rwxp 00000000 00:00 0 
7fc9efb2d000-7fc9efb30000 ---p 00000000 00:00 0 
7fc9efb30000-7fc9efb6e000 rwxp 00000000 00:00 0 
7fc9efb6e000-7fc9efb71000 ---p 00000000 00:00 0 
7fc9efb71000-7fc9efbaf000 rwxp 00000000 00:00 0 
7fc9efbaf000-7fc9efbb2000 ---p 00000000 00:00 0 
7fc9efbb2000-7fc9efbf0000 rwxp 00000000 00:00 0 
7fc9efbf0000-7fc9efbf3000 ---p 00000000 00:00 0 
7fc9efbf3000-7fc9efc31000 rwxp 00000000 00:00 0 
7fc9efc31000-7fc9efc34000 ---p 00000000 00:00 0 
7fc9efc34000-7fc9efc72000 rwxp 00000000 00:00 0 
7fc9efc72000-7fc9efc75000 ---p 00000000 00:00 0 
7fc9efc75000-7fc9efcb3000 rwxp 00000000 00:00 0 
7fc9efcb3000-7fc9efcb6000 ---p 00000000 00:00 0 
7fc9efcb6000-7fc9efcf4000 rwxp 00000000 00:00 0 
7fc9efcf4000-7fc9efcf7000 ---p 00000000 00:00 0 
7fc9efcf7000-7fc9efd35000 rwxp 00000000 00:00 0 
7fc9efd35000-7fc9efd38000 ---p 00000000 00:00 0 
7fc9efd38000-7fc9efd76000 rwxp 00000000 00:00 0 
7fc9efd76000-7fc9efd79000 ---p 00000000 00:00 0 
7fc9efd79000-7fc9efdb7000 rwxp 00000000 00:00 0 
7fc9efdb7000-7fc9efdba000 ---p 00000000 00:00 0 
7fc9efdba000-7fc9efdf8000 rwxp 00000000 00:00 0 
7fc9efdf8000-7fc9efdfb000 ---p 00000000 00:00 0 
7fc9efdfb000-7fc9efe39000 rwxp 00000000 00:00 0 
7fc9efe39000-7fc9efe3c000 ---p 00000000 00:00 0 
7fc9efe3c000-7fc9efe7a000 rwxp 00000000 00:00 0 
7fc9efe7a000-7fc9efe7d000 ---p 00000000 00:00 0 
7fc9efe7d000-7fc9efebb000 rwxp 00000000 00:00 0 
7fc9efebb000-7fc9efebe000 ---p 00000000 00:00 0 
7fc9efebe000-7fc9efefc000 rwxp 00000000 00:00 0 
7fc9efefc000-7fc9efeff000 ---p 00000000 00:00 0 
7fc9efeff000-7fc9eff3d000 rwxp 00000000 00:00 0 
7fc9eff3d000-7fc9eff40000 ---p 00000000 00:00 0 
7fc9eff40000-7fc9eff7e000 rwxp 00000000 00:00 0 
7fc9eff7e000-7fc9eff81000 ---p 00000000 00:00 0 
7fc9eff81000-7fc9effbf000 rwxp 00000000 00:00 0 
7fc9effbf000-7fc9effc2000 ---p 00000000 00:00 0 
7fc9effc2000-7fc9f0000000 rwxp 00000000 00:00 0 
7fc9f0000000-7fc9f0082000 rwxp 00000000 00:00 0 
7fc9f0082000-7fc9f4000000 ---p 00000000 00:00 0 
7fc9f4000000-7fc9f4082000 rwxp 00000000 00:00 0 
7fc9f4082000-7fc9f8000000 ---p 00000000 00:00 0 
7fc9f8000000-7fc9f8021000 rwxp 00000000 00:00 0 
7fc9f8021000-7fc9fc000000 ---p 00000000 00:00 0 
7fc9fc000000-7fc9fc021000 rwxp 00000000 00:00 0 
7fc9fc021000-7fca00000000 ---p 00000000 00:00 0 
7fca00000000-7fca00021000 rwxp 00000000 00:00 0 
7fca00021000-7fca04000000 ---p 00000000 00:00 0 
7fca04000000-7fca04021000 rwxp 00000000 00:00 0 
7fca04021000-7fca08000000 ---p 00000000 00:00 0 
7fca08000000-7fca08021000 rwxp 00000000 00:00 0 
7fca08021000-7fca0c000000 ---p 00000000 00:00 0 
7fca0c000000-7fca0c021000 rwxp 00000000 00:00 0 
7fca0c021000-7fca10000000 ---p 00000000 00:00 0 
7fca10000000-7fca10021000 rwxp 00000000 00:00 0 
7fca10021000-7fca14000000 ---p 00000000 00:00 0 
7fca14000000-7fca14021000 rwxp 00000000 00:00 0 
7fca14021000-7fca18000000 ---p 00000000 00:00 0 
7fca18000000-7fca18021000 rwxp 00000000 00:00 0 
7fca18021000-7fca1c000000 ---p 00000000 00:00 0 
7fca1c000000-7fca1c021000 rwxp 00000000 00:00 0 
7fca1c021000-7fca20000000 ---p 00000000 00:00 0 
7fca20000000-7fca20021000 rwxp 00000000 00:00 0 
7fca20021000-7fca24000000 ---p 00000000 00:00 0 
7fca24000000-7fca24021000 rwxp 00000000 00:00 0 
7fca24021000-7fca28000000 ---p 00000000 00:00 0 
7fca28000000-7fca28021000 rwxp 00000000 00:00 0 
7fca28021000-7fca2c000000 ---p 00000000 00:00 0 
7fca2c000000-7fca2c021000 rwxp 00000000 00:00 0 
7fca2c021000-7fca30000000 ---p 00000000 00:00 0 
7fca30000000-7fca30021000 rwxp 00000000 00:00 0 
7fca30021000-7fca34000000 ---p 00000000 00:00 0 
7fca34000000-7fca34021000 rwxp 00000000 00:00 0 
7fca34021000-7fca38000000 ---p 00000000 00:00 0 
7fca38000000-7fca38021000 rwxp 00000000 00:00 0 
7fca38021000-7fca3c000000 ---p 00000000 00:00 0 
7fca3c000000-7fca3c021000 rwxp 00000000 00:00 0 
7fca3c021000-7fca40000000 ---p 00000000 00:00 0 
7fca40000000-7fca40021000 rwxp 00000000 00:00 0 
7fca40021000-7fca44000000 ---p 00000000 00:00 0 
7fca44000000-7fca44021000 rwxp 00000000 00:00 0 
7fca44021000-7fca48000000 ---p 00000000 00:00 0 
7fca48000000-7fca48021000 rwxp 00000000 00:00 0 
7fca48021000-7fca4c000000 ---p 00000000 00:00 0 
7fca4c000000-7fca4c021000 rwxp 00000000 00:00 0 
7fca4c021000-7fca50000000 ---p 00000000 00:00 0 
7fca50000000-7fca50021000 rwxp 00000000 00:00 0 
7fca50021000-7fca54000000 ---p 00000000 00:00 0 
7fca54000000-7fca54021000 rwxp 00000000 00:00 0 
7fca54021000-7fca58000000 ---p 00000000 00:00 0 
7fca58000000-7fca58021000 rwxp 00000000 00:00 0 
7fca58021000-7fca5c000000 ---p 00000000 00:00 0 
7fca5c000000-7fca5c021000 rwxp 00000000 00:00 0 
7fca5c021000-7fca60000000 ---p 00000000 00:00 0 
7fca60000000-7fca60021000 rwxp 00000000 00:00 0 
7fca60021000-7fca64000000 ---p 00000000 00:00 0 
7fca64000000-7fca64021000 rwxp 00000000 00:00 0 
7fca64021000-7fca68000000 ---p 00000000 00:00 0 
7fca68000000-7fca68021000 rwxp 00000000 00:00 0 
7fca68021000-7fca6c000000 ---p 00000000 00:00 0 
7fca6c000000-7fca6c021000 rwxp 00000000 00:00 0 
7fca6c021000-7fca70000000 ---p 00000000 00:00 0 
7fca70000000-7fca70021000 rwxp 00000000 00:00 0 
7fca70021000-7fca74000000 ---p 00000000 00:00 0 
7fca74000000-7fca74021000 rwxp 00000000 00:00 0 
7fca74021000-7fca78000000 ---p 00000000 00:00 0 
7fca78000000-7fca78021000 rwxp 00000000 00:00 0 
7fca78021000-7fca7c000000 ---p 00000000 00:00 0 
7fca7c000000-7fca7c021000 rwxp 00000000 00:00 0 
7fca7c021000-7fca80000000 ---p 00000000 00:00 0 
7fca80000000-7fca80021000 rwxp 00000000 00:00 0 
7fca80021000-7fca84000000 ---p 00000000 00:00 0 
7fca84000000-7fca84021000 rwxp 00000000 00:00 0 
7fca84021000-7fca88000000 ---p 00000000 00:00 0 
7fca88000000-7fca88021000 rwxp 00000000 00:00 0 
7fca88021000-7fca8c000000 ---p 00000000 00:00 0 
7fca8c000000-7fca8c021000 rwxp 00000000 00:00 0 
7fca8c021000-7fca90000000 ---p 00000000 00:00 0 
7fca90000000-7fca90021000 rwxp 00000000 00:00 0 
7fca90021000-7fca94000000 ---p 00000000 00:00 0 
7fca94000000-7fca94021000 rwxp 00000000 00:00 0 
7fca94021000-7fca98000000 ---p 00000000 00:00 0 
7fca98000000-7fca98021000 rwxp 00000000 00:00 0 
7fca98021000-7fca9c000000 ---p 00000000 00:00 0 
7fca9c000000-7fca9c021000 rwxp 00000000 00:00 0 
7fca9c021000-7fcaa0000000 ---p 00000000 00:00 0 
7fcaa0000000-7fcaa0021000 rwxp 00000000 00:00 0 
7fcaa0021000-7fcaa4000000 ---p 00000000 00:00 0 
7fcaa4000000-7fcaa4021000 rwxp 00000000 00:00 0 
7fcaa4021000-7fcaa8000000 ---p 00000000 00:00 0 
7fcaa8000000-7fcaa8021000 rwxp 00000000 00:00 0 
7fcaa8021000-7fcaac000000 ---p 00000000 00:00 0 
7fcaac000000-7fcaac021000 rwxp 00000000 00:00 0 
7fcaac021000-7fcab0000000 ---p 00000000 00:00 0 
7fcab0000000-7fcab0021000 rwxp 00000000 00:00 0 
7fcab0021000-7fcab4000000 ---p 00000000 00:00 0 
7fcab4000000-7fcab4021000 rwxp 00000000 00:00 0 
7fcab4021000-7fcab8000000 ---p 00000000 00:00 0 
7fcab8000000-7fcab8021000 rwxp 00000000 00:00 0 
7fcab8021000-7fcabc000000 ---p 00000000 00:00 0 
7fcabc000000-7fcabc021000 rwxp 00000000 00:00 0 
7fcabc021000-7fcac0000000 ---p 00000000 00:00 0 
7fcac0000000-7fcac0084000 rwxp 00000000 00:00 0 
7fcac0084000-7fcac4000000 ---p 00000000 00:00 0 
7fcac4000000-7fcac4021000 rwxp 00000000 00:00 0 
7fcac4021000-7fcac8000000 ---p 00000000 00:00 0 
7fcac8000000-7fcac8082000 rwxp 00000000 00:00 0 
7fcac8082000-7fcacc000000 ---p 00000000 00:00 0 
7fcacc000000-7fcacc087000 rwxp 00000000 00:00 0 
7fcacc087000-7fcad0000000 ---p 00000000 00:00 0 
7fcad0000000-7fcad0083000 rwxp 00000000 00:00 0 
7fcad0083000-7fcad4000000 ---p 00000000 00:00 0 
7fcad4000000-7fcad4083000 rwxp 00000000 00:00 0 
7fcad4083000-7fcad8000000 ---p 00000000 00:00 0 
7fcad8000000-7fcad8083000 rwxp 00000000 00:00 0 
7fcad8083000-7fcadc000000 ---p 00000000 00:00 0 
7fcadc000000-7fcadc08b000 rwxp 00000000 00:00 0 
7fcadc08b000-7fcae0000000 ---p 00000000 00:00 0 
7fcae0000000-7fcae0021000 rwxp 00000000 00:00 0 
7fcae0021000-7fcae4000000 ---p 00000000 00:00 0 
7fcae4000000-7fcae4021000 rwxp 00000000 00:00 0 
7fcae4021000-7fcae8000000 ---p 00000000 00:00 0 
7fcae8000000-7fcae8088000 rwxp 00000000 00:00 0 
7fcae8088000-7fcaec000000 ---p 00000000 00:00 0 
7fcaec000000-7fcaec09b000 rwxp 00000000 00:00 0 
7fcaec09b000-7fcaf0000000 ---p 00000000 00:00 0 
7fcaf0000000-7fcaf0083000 rwxp 00000000 00:00 0 
7fcaf0083000-7fcaf4000000 ---p 00000000 00:00 0 
7fcaf4000000-7fcaf4089000 rwxp 00000000 00:00 0 
7fcaf4089000-7fcaf8000000 ---p 00000000 00:00 0 
7fcaf8000000-7fcaf8021000 rwxp 00000000 00:00 0 
7fcaf8021000-7fcafc000000 ---p 00000000 00:00 0 
7fcafc000000-7fcafc089000 rwxp 00000000 00:00 0 
7fcafc089000-7fcb00000000 ---p 00000000 00:00 0 
7fcb00000000-7fcb02691000 rwxp 00000000 00:00 0 
7fcb02691000-7fcb04000000 ---p 00000000 00:00 0 
7fcb04000000-7fcb04021000 rwxp 00000000 00:00 0 
7fcb04021000-7fcb08000000 ---p 00000000 00:00 0 
7fcb08000000-7fcb08021000 rwxp 00000000 00:00 0 
7fcb08021000-7fcb0c000000 ---p 00000000 00:00 0 
7fcb0c000000-7fcb0c021000 rwxp 00000000 00:00 0 
7fcb0c021000-7fcb10000000 ---p 00000000 00:00 0 
7fcb10000000-7fcb10021000 rwxp 00000000 00:00 0 
7fcb10021000-7fcb14000000 ---p 00000000 00:00 0 
7fcb14000000-7fcb14021000 rwxp 00000000 00:00 0 
7fcb14021000-7fcb18000000 ---p 00000000 00:00 0 
7fcb18000000-7fcb18021000 rwxp 00000000 00:00 0 
7fcb18021000-7fcb1c000000 ---p 00000000 00:00 0 
7fcb1c000000-7fcb1c021000 rwxp 00000000 00:00 0 
7fcb1c021000-7fcb20000000 ---p 00000000 00:00 0 
7fcb20000000-7fcb23fdd000 rwxp 00000000 00:00 0 
7fcb23fdd000-7fcb24000000 ---p 00000000 00:00 0 
7fcb24000000-7fcb27ff7000 rwxp 00000000 00:00 0 
7fcb27ff7000-7fcb28000000 ---p 00000000 00:00 0 
7fcb28000000-7fcb28021000 rwxp 00000000 00:00 0 
7fcb28021000-7fcb2c000000 ---p 00000000 00:00 0 
7fcb2c020000-7fcb2c023000 ---p 00000000 00:00 0 
7fcb2c023000-7fcb2c061000 rwxp 00000000 00:00 0 
7fcb2c061000-7fcb2c064000 ---p 00000000 00:00 0 
7fcb2c064000-7fcb2c0a2000 rwxp 00000000 00:00 0 
7fcb2c0a2000-7fcb2c0a5000 ---p 00000000 00:00 0 
7fcb2c0a5000-7fcb2c0e3000 rwxp 00000000 00:00 0 
7fcb2c0e3000-7fcb2c0e6000 ---p 00000000 00:00 0 
7fcb2c0e6000-7fcb2c124000 rwxp 00000000 00:00 0 
7fcb2c124000-7fcb2c127000 ---p 00000000 00:00 0 
7fcb2c127000-7fcb2c165000 rwxp 00000000 00:00 0 
7fcb2c165000-7fcb2c168000 ---p 00000000 00:00 0 
7fcb2c168000-7fcb2c1a6000 rwxp 00000000 00:00 0 
7fcb2c1a6000-7fcb2c1a9000 ---p 00000000 00:00 0 
7fcb2c1a9000-7fcb2c1e7000 rwxp 00000000 00:00 0 
7fcb2c1e7000-7fcb2c1ea000 ---p 00000000 00:00 0 
7fcb2c1ea000-7fcb2c228000 rwxp 00000000 00:00 0 
7fcb2c228000-7fcb2c22b000 ---p 00000000 00:00 0 
7fcb2c22b000-7fcb2c269000 rwxp 00000000 00:00 0 
7fcb2c269000-7fcb2c26c000 ---p 00000000 00:00 0 
7fcb2c26c000-7fcb2c2aa000 rwxp 00000000 00:00 0 
7fcb2c2aa000-7fcb2c2ad000 ---p 00000000 00:00 0 
7fcb2c2ad000-7fcb2c2eb000 rwxp 00000000 00:00 0 
7fcb2c2eb000-7fcb2c2ee000 ---p 00000000 00:00 0 
7fcb2c2ee000-7fcb2c32c000 rwxp 00000000 00:00 0 
7fcb2c32c000-7fcb2c32f000 ---p 00000000 00:00 0 
7fcb2c32f000-7fcb2c36d000 rwxp 00000000 00:00 0 
7fcb2c36d000-7fcb2c370000 ---p 00000000 00:00 0 
7fcb2c370000-7fcb2c3ae000 rwxp 00000000 00:00 0 
7fcb2c3ae000-7fcb2c3b1000 ---p 00000000 00:00 0 
7fcb2c3b1000-7fcb2c3ef000 rwxp 00000000 00:00 0 
7fcb2c3ef000-7fcb2c3f2000 ---p 00000000 00:00 0 
7fcb2c3f2000-7fcb2c430000 rwxp 00000000 00:00 0 
7fcb2c430000-7fcb2c433000 ---p 00000000 00:00 0 
7fcb2c433000-7fcb2c471000 rwxp 00000000 00:00 0 
7fcb2c471000-7fcb2c474000 ---p 00000000 00:00 0 
7fcb2c474000-7fcb2c4b2000 rwxp 00000000 00:00 0 
7fcb2c4b2000-7fcb2c4b5000 ---p 00000000 00:00 0 
7fcb2c4b5000-7fcb2c4f3000 rwxp 00000000 00:00 0 
7fcb2c4f3000-7fcb2c4f6000 ---p 00000000 00:00 0 
7fcb2c4f6000-7fcb2c534000 rwxp 00000000 00:00 0 
7fcb2c534000-7fcb2c537000 ---p 00000000 00:00 0 
7fcb2c537000-7fcb2c575000 rwxp 00000000 00:00 0 
7fcb2c575000-7fcb2c578000 ---p 00000000 00:00 0 
7fcb2c578000-7fcb2c5b6000 rwxp 00000000 00:00 0 
7fcb2c5b6000-7fcb2c5b9000 ---p 00000000 00:00 0 
7fcb2c5b9000-7fcb2c5f7000 rwxp 00000000 00:00 0 
7fcb2c5f7000-7fcb2c5fa000 ---p 00000000 00:00 0 
7fcb2c5fa000-7fcb2c638000 rwxp 00000000 00:00 0 
7fcb2c638000-7fcb2c63b000 ---p 00000000 00:00 0 
7fcb2c63b000-7fcb2c679000 rwxp 00000000 00:00 0 
7fcb2c679000-7fcb2c67c000 ---p 00000000 00:00 0 
7fcb2c67c000-7fcb2c6ba000 rwxp 00000000 00:00 0 
7fcb2c6ba000-7fcb2c6bd000 ---p 00000000 00:00 0 
7fcb2c6bd000-7fcb2c6fb000 rwxp 00000000 00:00 0 
7fcb2c6fb000-7fcb2c6fe000 ---p 00000000 00:00 0 
7fcb2c6fe000-7fcb2c73c000 rwxp 00000000 00:00 0 
7fcb2c73c000-7fcb2c73f000 ---p 00000000 00:00 0 
7fcb2c73f000-7fcb2c77d000 rwxp 00000000 00:00 0 
7fcb2c77d000-7fcb2c780000 ---p 00000000 00:00 0 
7fcb2c780000-7fcb2c7be000 rwxp 00000000 00:00 0 
7fcb2c7be000-7fcb2c7c1000 ---p 00000000 00:00 0 
7fcb2c7c1000-7fcb2c7ff000 rwxp 00000000 00:00 0 
7fcb2c7ff000-7fcb2c802000 ---p 00000000 00:00 0 
7fcb2c802000-7fcb2c840000 rwxp 00000000 00:00 0 
7fcb2c840000-7fcb2c843000 ---p 00000000 00:00 0 
7fcb2c843000-7fcb2c881000 rwxp 00000000 00:00 0 
7fcb2c881000-7fcb2c884000 ---p 00000000 00:00 0 
7fcb2c884000-7fcb2c8c2000 rwxp 00000000 00:00 0 
7fcb2c8c2000-7fcb2c8c5000 ---p 00000000 00:00 0 
7fcb2c8c5000-7fcb2c903000 rwxp 00000000 00:00 0 
7fcb2c903000-7fcb2c906000 ---p 00000000 00:00 0 
7fcb2c906000-7fcb2c944000 rwxp 00000000 00:00 0 
7fcb2c944000-7fcb2c947000 ---p 00000000 00:00 0 
7fcb2c947000-7fcb2c985000 rwxp 00000000 00:00 0 
7fcb2c985000-7fcb2c988000 ---p 00000000 00:00 0 
7fcb2c988000-7fcb2c9c6000 rwxp 00000000 00:00 0 
7fcb2c9c6000-7fcb2c9c9000 ---p 00000000 00:00 0 
7fcb2c9c9000-7fcb2ca07000 rwxp 00000000 00:00 0 
7fcb2ca07000-7fcb2ca0a000 ---p 00000000 00:00 0 
7fcb2ca0a000-7fcb2ca48000 rwxp 00000000 00:00 0 
7fcb2ca48000-7fcb2ca4b000 ---p 00000000 00:00 0 
7fcb2ca4b000-7fcb2ca89000 rwxp 00000000 00:00 0 
7fcb2ca89000-7fcb2ca8c000 ---p 00000000 00:00 0 
7fcb2ca8c000-7fcb2caca000 rwxp 00000000 00:00 0 
7fcb2caca000-7fcb2cacd000 ---p 00000000 00:00 0 
7fcb2cacd000-7fcb2cb0b000 rwxp 00000000 00:00 0 
7fcb2cb0b000-7fcb2cb0e000 ---p 00000000 00:00 0 
7fcb2cb0e000-7fcb2cb4c000 rwxp 00000000 00:00 0 
7fcb2cb4c000-7fcb2cb4f000 ---p 00000000 00:00 0 
7fcb2cb4f000-7fcb2cb8d000 rwxp 00000000 00:00 0 
7fcb2cb8d000-7fcb2cb90000 ---p 00000000 00:00 0 
7fcb2cb90000-7fcb2cbce000 rwxp 00000000 00:00 0 
7fcb2cbce000-7fcb2cbd1000 ---p 00000000 00:00 0 
7fcb2cbd1000-7fcb2cc0f000 rwxp 00000000 00:00 0 
7fcb2cc0f000-7fcb2cc12000 ---p 00000000 00:00 0 
7fcb2cc12000-7fcb2cc50000 rwxp 00000000 00:00 0 
7fcb2cc50000-7fcb2cc53000 ---p 00000000 00:00 0 
7fcb2cc53000-7fcb2cc91000 rwxp 00000000 00:00 0 
7fcb2cc91000-7fcb2cc94000 ---p 00000000 00:00 0 
7fcb2cc94000-7fcb2ccd2000 rwxp 00000000 00:00 0 
7fcb2ccd2000-7fcb2ccd5000 ---p 00000000 00:00 0 
7fcb2ccd5000-7fcb2cd13000 rwxp 00000000 00:00 0 
7fcb2cd13000-7fcb2cd16000 ---p 00000000 00:00 0 
7fcb2cd16000-7fcb2cd54000 rwxp 00000000 00:00 0 
7fcb2cd54000-7fcb2cd57000 ---p 00000000 00:00 0 
7fcb2cd57000-7fcb2cd95000 rwxp 00000000 00:00 0 
7fcb2cd95000-7fcb2cd98000 ---p 00000000 00:00 0 
7fcb2cd98000-7fcb2cdd6000 rwxp 00000000 00:00 0 
7fcb2cdd6000-7fcb2cdd9000 ---p 00000000 00:00 0 
7fcb2cdd9000-7fcb2ce17000 rwxp 00000000 00:00 0 
7fcb2ce17000-7fcb2ce1a000 ---p 00000000 00:00 0 
7fcb2ce1a000-7fcb2ce58000 rwxp 00000000 00:00 0 
7fcb2ce58000-7fcb2ce5b000 ---p 00000000 00:00 0 
7fcb2ce5b000-7fcb2ce99000 rwxp 00000000 00:00 0 
7fcb2ce99000-7fcb2ce9c000 ---p 00000000 00:00 0 
7fcb2ce9c000-7fcb2ceda000 rwxp 00000000 00:00 0 
7fcb2ceda000-7fcb2cedd000 ---p 00000000 00:00 0 
7fcb2cedd000-7fcb2cf1b000 rwxp 00000000 00:00 0 
7fcb2cf1b000-7fcb2cf1e000 ---p 00000000 00:00 0 
7fcb2cf1e000-7fcb2cf5c000 rwxp 00000000 00:00 0 
7fcb2cf5c000-7fcb2cf5f000 ---p 00000000 00:00 0 
7fcb2cf5f000-7fcb2cf9d000 rwxp 00000000 00:00 0 
7fcb2cf9d000-7fcb2cfa0000 ---p 00000000 00:00 0 
7fcb2cfa0000-7fcb2cfde000 rwxp 00000000 00:00 0 
7fcb2cfde000-7fcb2cfe1000 ---p 00000000 00:00 0 
7fcb2cfe1000-7fcb2d01f000 rwxp 00000000 00:00 0 
7fcb2d01f000-7fcb2d022000 ---p 00000000 00:00 0 
7fcb2d022000-7fcb2d060000 rwxp 00000000 00:00 0 
7fcb2d060000-7fcb2d063000 ---p 00000000 00:00 0 
7fcb2d063000-7fcb2d0a1000 rwxp 00000000 00:00 0 
7fcb2d0a1000-7fcb2d0a4000 ---p 00000000 00:00 0 
7fcb2d0a4000-7fcb2d0e2000 rwxp 00000000 00:00 0 
7fcb2d0e2000-7fcb2d0e5000 ---p 00000000 00:00 0 
7fcb2d0e5000-7fcb2d123000 rwxp 00000000 00:00 0 
7fcb2d123000-7fcb2d126000 ---p 00000000 00:00 0 
7fcb2d126000-7fcb2d164000 rwxp 00000000 00:00 0 
7fcb2d164000-7fcb2d167000 ---p 00000000 00:00 0 
7fcb2d167000-7fcb2d1a5000 rwxp 00000000 00:00 0 
7fcb2d1a5000-7fcb2d1a8000 ---p 00000000 00:00 0 
7fcb2d1a8000-7fcb2d1e6000 rwxp 00000000 00:00 0 
7fcb2d1e6000-7fcb2d215000 r-xp 00000000 08:03 41681153                   /export/data/workRoot/es-search/lib/sigar/libsigar-amd64-linux.so
7fcb2d215000-7fcb2d314000 ---p 0002f000 08:03 41681153                   /export/data/workRoot/es-search/lib/sigar/libsigar-amd64-linux.so
7fcb2d314000-7fcb2d31a000 rwxp 0002e000 08:03 41681153                   /export/data/workRoot/es-search/lib/sigar/libsigar-amd64-linux.so
7fcb2d31a000-7fcb2d31d000 rwxp 00000000 00:00 0 
7fcb2d31d000-7fcb2d324000 r-xp 00000000 08:03 32769262                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libnio.so
7fcb2d324000-7fcb2d423000 ---p 00007000 08:03 32769262                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libnio.so
7fcb2d423000-7fcb2d425000 rwxp 00006000 08:03 32769262                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libnio.so
7fcb2d425000-7fcb2d426000 rwxp 00000000 00:00 0 
7fcb2d426000-7fcb2d428000 r-xs 00012000 08:03 41681335                   /export/data/workRoot/es-search/plugins/transport-thrift/elasticsearch-transport-thrift-1.5.0.jar
7fcb2d428000-7fcb2d42a000 r-xs 00005000 08:03 41681337                   /export/data/workRoot/es-search/plugins/transport-thrift/slf4j-api-1.6.2.jar
7fcb2d42a000-7fcb2d42f000 r-xs 00040000 08:03 41681334                   /export/data/workRoot/es-search/plugins/transport-thrift/commons-lang-2.5.jar
7fcb2d42f000-7fcb2d430000 r-xs 00002000 08:03 41681338                   /export/data/workRoot/es-search/plugins/transport-thrift/slf4j-log4j12-1.6.2.jar
7fcb2d430000-7fcb2d437000 r-xs 0004e000 08:03 41681336                   /export/data/workRoot/es-search/plugins/transport-thrift/libthrift-0.9.0.jar
7fcb2d437000-7fcb2d439000 r-xs 00019000 08:03 41681174                   /export/data/workRoot/es-search/plugins/analysis-stconvert/elasticsearch-analysis-stconvert-1.2.1.jar
7fcb2d439000-7fcb2d43a000 r-xs 00001000 08:03 41681172                   /export/data/workRoot/es-search/plugins/analysis-smartcn/elasticsearch-analysis-smartcn-1.4.0.jar
7fcb2d43a000-7fcb2d43c000 r-xs 0036d000 08:03 41681173                   /export/data/workRoot/es-search/plugins/analysis-smartcn/lucene-analyzers-smartcn-4.2.1.jar
7fcb2d43c000-7fcb2d43e000 r-xs 0000c000 08:03 41681168                   /export/data/workRoot/es-search/plugins/analysis-ik/elasticsearch-analysis-ik-1.2.2.jar
7fcb2d43e000-7fcb2d45e000 r-xs 0019d000 08:03 41681299                   /export/data/workRoot/es-search/plugins/mapper-attachments/poi-3.8.jar
7fcb2d45e000-7fcb2d49b000 r-xs 0024f000 08:03 41681310                   /export/data/workRoot/es-search/plugins/mapper-attachments/xmlbeans-2.3.0.jar
7fcb2d49b000-7fcb2d49e000 r-xs 00015000 08:03 41681278                   /export/data/workRoot/es-search/plugins/mapper-attachments/apache-mime4j-core-0.7.2.jar
7fcb2d49e000-7fcb2d4a0000 r-xs 0000a000 08:03 41681308                   /export/data/workRoot/es-search/plugins/mapper-attachments/vorbis-java-core-0.1.jar
7fcb2d4a0000-7fcb2d4a4000 r-xs 0002f000 08:03 41681303                   /export/data/workRoot/es-search/plugins/mapper-attachments/rome-0.9.jar
7fcb2d4a4000-7fcb2d4a6000 r-xs 00015000 08:03 41681304                   /export/data/workRoot/es-search/plugins/mapper-attachments/tagsoup-1.2.1.jar
7fcb2d4a6000-7fcb2d4b6000 r-xs 00112000 08:03 41681302                   /export/data/workRoot/es-search/plugins/mapper-attachments/poi-scratchpad-3.8.jar
7fcb2d4b6000-7fcb2d4c2000 r-xs 000d8000 08:03 41681300                   /export/data/workRoot/es-search/plugins/mapper-attachments/poi-ooxml-3.8.jar
7fcb2d4c2000-7fcb2d4c4000 r-xs 00010000 08:03 41681285                   /export/data/workRoot/es-search/plugins/mapper-attachments/commons-codec-1.5.jar
7fcb2d4c4000-7fcb2d4c8000 r-xs 00019000 08:03 41681281                   /export/data/workRoot/es-search/plugins/mapper-attachments/aspectjrt-1.6.11.jar
7fcb2d4c8000-7fcb2d4cf000 r-xs 00044000 08:03 41681279                   /export/data/workRoot/es-search/plugins/mapper-attachments/apache-mime4j-dom-0.7.2.jar
7fcb2d4cf000-7fcb2d4d6000 r-xs 0006b000 08:03 41681305                   /export/data/workRoot/es-search/plugins/mapper-attachments/tika-core-1.2.jar
7fcb2d4d6000-7fcb2d568000 r-xs 003ec000 08:03 41681301                   /export/data/workRoot/es-search/plugins/mapper-attachments/poi-ooxml-schemas-3.8.jar
7fcb2d568000-7fcb2d571000 r-xs 00077000 08:03 41681292                   /export/data/workRoot/es-search/plugins/mapper-attachments/isoparser-1.0-RC-1.jar
7fcb2d571000-7fcb2d574000 r-xs 00014000 08:03 41681284                   /export/data/workRoot/es-search/plugins/mapper-attachments/boilerpipe-1.1.0.jar
7fcb2d574000-7fcb2d5a5000 r-xs 003f0000 08:03 41681297                   /export/data/workRoot/es-search/plugins/mapper-attachments/netcdf-4.2-min.jar
7fcb2d5a5000-7fcb2d5a9000 r-xs 00014000 08:03 41681311                   /export/data/workRoot/es-search/plugins/mapper-attachments/xz-1.0.jar
7fcb2d5a9000-7fcb2d5ab000 r-xs 0000b000 08:03 41681294                   /export/data/workRoot/es-search/plugins/mapper-attachments/jempbox-1.7.0.jar
7fcb2d5ab000-7fcb2d5ad000 r-xs 00006000 08:03 41681307                   /export/data/workRoot/es-search/plugins/mapper-attachments/vorbis-java-core-0.1-tests.jar
7fcb2d5ad000-7fcb2d5ae000 r-xs 00003000 08:03 41681289                   /export/data/workRoot/es-search/plugins/mapper-attachments/elasticsearch-mapper-attachments-1.7.0.jar
7fcb2d5ae000-7fcb2d5b1000 r-xs 00023000 08:03 41681293                   /export/data/workRoot/es-search/plugins/mapper-attachments/jdom-1.0.jar
7fcb2d5b1000-7fcb2d5b4000 r-xs 00033000 08:03 41681295                   /export/data/workRoot/es-search/plugins/mapper-attachments/juniversalchardet-1.0.3.jar
7fcb2d5b4000-7fcb2d5b6000 r-xs 00006000 08:03 41681291                   /export/data/workRoot/es-search/plugins/mapper-attachments/geronimo-stax-api_1.0_spec-1.0.1.jar
7fcb2d5b6000-7fcb2d5be000 r-xs 0006e000 08:03 41681306                   /export/data/workRoot/es-search/plugins/mapper-attachments/tika-parsers-1.2.jar
7fcb2d5be000-7fcb2d5c3000 r-xs 00033000 08:03 41681282                   /export/data/workRoot/es-search/plugins/mapper-attachments/bcmail-jdk15-1.45.jar
7fcb2d5c3000-7fcb2d5e8000 r-xs 00172000 08:03 41681283                   /export/data/workRoot/es-search/plugins/mapper-attachments/bcprov-jdk15-1.45.jar
7fcb2d5e8000-7fcb2d5ed000 r-xs 00048000 08:03 41681288                   /export/data/workRoot/es-search/plugins/mapper-attachments/dom4j-1.6.1.jar
7fcb2d5ed000-7fcb2d5f1000 r-xs 0002a000 08:03 41681290                   /export/data/workRoot/es-search/plugins/mapper-attachments/fontbox-1.7.0.jar
7fcb2d5f1000-7fcb2d607000 r-xs 003a5000 08:03 41681298                   /export/data/workRoot/es-search/plugins/mapper-attachments/pdfbox-1.7.0.jar
7fcb2d607000-7fcb2d60c000 r-xs 00036000 08:03 41681286                   /export/data/workRoot/es-search/plugins/mapper-attachments/commons-compress-1.4.1.jar
7fcb2d60c000-7fcb2d60d000 r-xs 0000a000 08:03 41681280                   /export/data/workRoot/es-search/plugins/mapper-attachments/asm-3.1.jar
7fcb2d60d000-7fcb2d60e000 r-xs 00003000 08:03 41681309                   /export/data/workRoot/es-search/plugins/mapper-attachments/vorbis-java-tika-0.1.jar
7fcb2d60e000-7fcb2d610000 r-xs 0000d000 08:03 41681287                   /export/data/workRoot/es-search/plugins/mapper-attachments/commons-logging-1.1.1.jar
7fcb2d610000-7fcb2d613000 r-xs 00014000 08:03 41681296                   /export/data/workRoot/es-search/plugins/mapper-attachments/metadata-extractor-2.4.0-beta-1.jar
7fcb2d613000-7fcb2d616000 r-xs 00016000 08:03 41681175                   /export/data/workRoot/es-search/plugins/analysis-string2int/commons-pool-1.5.5.jar
7fcb2d616000-7fcb2d618000 r-xs 00002000 08:03 41681179                   /export/data/workRoot/es-search/plugins/analysis-string2int/simplelrucache-0.1.jar
7fcb2d618000-7fcb2d61b000 r-xs 00020000 08:03 41681178                   /export/data/workRoot/es-search/plugins/analysis-string2int/jedis-2.1.0.jar
7fcb2d61b000-7fcb2d61d000 r-xs 0000b000 08:03 41681176                   /export/data/workRoot/es-search/plugins/analysis-string2int/concurrentlinkedhashmap-lru-1.2.jar
7fcb2d61d000-7fcb2d621000 r-xs 0002b000 08:03 41681171                   /export/data/workRoot/es-search/plugins/analysis-pinyin/pinyin4j-2.5.0.jar
7fcb2d621000-7fcb2df82000 rwxp 00000000 00:00 0 
7fcb2df82000-7fcb2df95000 r-xp 00000000 08:03 32769289                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libnet.so
7fcb2df95000-7fcb2e096000 ---p 00013000 08:03 32769289                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libnet.so
7fcb2e096000-7fcb2e099000 rwxp 00014000 08:03 32769289                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libnet.so
7fcb2e099000-7fcb2e16f000 r-xs 009a8000 08:03 41681135                   /export/data/workRoot/es-search/lib/elasticsearch-0.90.2.jar
7fcb2e16f000-7fcb34000000 r-xp 00000000 08:01 653941                     /usr/lib/locale/locale-archive
7fcb34000000-7fcb34021000 rwxp 00000000 00:00 0 
7fcb34021000-7fcb38000000 ---p 00000000 00:00 0 
7fcb38000000-7fcb38021000 rwxp 00000000 00:00 0 
7fcb38021000-7fcb3c000000 ---p 00000000 00:00 0 
7fcb3c000000-7fcb3c2fb000 rwxp 00000000 00:00 0 
7fcb3c2fb000-7fcb40000000 ---p 00000000 00:00 0 
7fcb40000000-7fcb40021000 rwxp 00000000 00:00 0 
7fcb40021000-7fcb44000000 ---p 00000000 00:00 0 
7fcb44000000-7fcb44021000 rwxp 00000000 00:00 0 
7fcb44021000-7fcb48000000 ---p 00000000 00:00 0 
7fcb48000000-7fcb48021000 rwxp 00000000 00:00 0 
7fcb48021000-7fcb4c000000 ---p 00000000 00:00 0 
7fcb4c000000-7fcb4c021000 rwxp 00000000 00:00 0 
7fcb4c021000-7fcb50000000 ---p 00000000 00:00 0 
7fcb50000000-7fcb50021000 rwxp 00000000 00:00 0 
7fcb50021000-7fcb54000000 ---p 00000000 00:00 0 
7fcb54000000-7fcb54021000 rwxp 00000000 00:00 0 
7fcb54021000-7fcb58000000 ---p 00000000 00:00 0 
7fcb58000000-7fcb58021000 rwxp 00000000 00:00 0 
7fcb58021000-7fcb5c000000 ---p 00000000 00:00 0 
7fcb5c000000-7fcb5c021000 rwxp 00000000 00:00 0 
7fcb5c021000-7fcb60000000 ---p 00000000 00:00 0 
7fcb60001000-7fcb60003000 r-xs 00003000 08:03 41681177                   /export/data/workRoot/es-search/plugins/analysis-string2int/elasticsearch-analysis-string2int-1.2.1.jar
7fcb60003000-7fcb60005000 r-xs 0000b000 08:03 41681169                   /export/data/workRoot/es-search/plugins/analysis-mmseg/elasticsearch-analysis-mmseg-1.2.0.jar
7fcb60005000-7fcb6003a000 r-xs 00000000 08:01 393052                     /var/run/nscd/dbGdWA5z (deleted)
7fcb6003a000-7fcb60041000 r-xs 00062000 08:03 41681164                   /export/data/workRoot/es-search/lib/sigar/sigar-1.6.4.jar
7fcb60041000-7fcb6004a000 r-xs 00056000 08:03 41681147                   /export/data/workRoot/es-search/lib/lucene-queryparser-4.3.1.jar
7fcb6004a000-7fcb6006f000 r-xs 001f8000 08:03 41681141                   /export/data/workRoot/es-search/lib/lucene-core-4.3.1.jar
7fcb6006f000-7fcb60072000 ---p 00000000 00:00 0 
7fcb60072000-7fcb60aba000 rwxp 00000000 00:00 0 
7fcb60aba000-7fcb60abb000 ---p 00000000 00:00 0 
7fcb60abb000-7fcb8bfc0000 rwxp 00000000 00:00 0 
7fcb8bfc0000-7fcb8c000000 rwxp 00000000 00:00 0 
7fcb8c000000-7fcf94000000 rwxp 00000000 00:00 0 
7fcf94000000-7fcf94021000 rwxp 00000000 00:00 0 
7fcf94021000-7fcf98000000 ---p 00000000 00:00 0 
7fcf98000000-7fcf98002000 r-xs 0000f000 08:03 41681151                   /export/data/workRoot/es-search/lib/spatial4j-0.3.jar
7fcf98002000-7fcf98005000 r-xs 00020000 08:03 41681150                   /export/data/workRoot/es-search/lib/lucene-suggest-4.3.1.jar
7fcf98005000-7fcf98008000 r-xs 00014000 08:03 41681149                   /export/data/workRoot/es-search/lib/lucene-spatial-4.3.1.jar
7fcf98008000-7fcf9800e000 r-xs 0002c000 08:03 41681146                   /export/data/workRoot/es-search/lib/lucene-queries-4.3.1.jar
7fcf9800e000-7fcf9800f000 r-xs 00008000 08:03 41681145                   /export/data/workRoot/es-search/lib/lucene-memory-4.3.1.jar
7fcf9800f000-7fcf98011000 r-xs 0000e000 08:03 41681144                   /export/data/workRoot/es-search/lib/lucene-join-4.3.1.jar
7fcf98011000-7fcf98015000 r-xs 0001b000 08:03 41681143                   /export/data/workRoot/es-search/lib/lucene-highlighter-4.3.1.jar
7fcf98015000-7fcf98018000 r-xs 00018000 08:03 41681142                   /export/data/workRoot/es-search/lib/lucene-grouping-4.3.1.jar
7fcf98018000-7fcf98028000 r-xs 0016e000 08:03 41681139                   /export/data/workRoot/es-search/lib/lucene-analyzers-common-4.3.1.jar
7fcf98028000-7fcf98039000 r-xs 000ac000 08:03 41681137                   /export/data/workRoot/es-search/lib/jts-1.12.jar
7fcf98039000-7fcf9803c000 ---p 00000000 00:00 0 
7fcf9803c000-7fcf9807a000 rwxp 00000000 00:00 0 
7fcf9807a000-7fcf9807d000 ---p 00000000 00:00 0 
7fcf9807d000-7fcf980bb000 rwxp 00000000 00:00 0 
7fcf980bb000-7fcf980be000 ---p 00000000 00:00 0 
7fcf980be000-7fcf980fc000 rwxp 00000000 00:00 0 
7fcf980fc000-7fcf980ff000 ---p 00000000 00:00 0 
7fcf980ff000-7fcf9813d000 rwxp 00000000 00:00 0 
7fcf9813d000-7fcf98143000 r-xp 00000000 08:03 32769309                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libmanagement.so
7fcf98143000-7fcf98242000 ---p 00006000 08:03 32769309                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libmanagement.so
7fcf98242000-7fcf98244000 rwxp 00005000 08:03 32769309                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libmanagement.so
7fcf98244000-7fcf98245000 ---p 00000000 00:00 0 
7fcf98245000-7fcf98345000 rwxp 00000000 00:00 0 
7fcf98345000-7fcf98348000 ---p 00000000 00:00 0 
7fcf98348000-7fcf98386000 rwxp 00000000 00:00 0 
7fcf98386000-7fcf98389000 ---p 00000000 00:00 0 
7fcf98389000-7fcf98487000 rwxp 00000000 00:00 0 
7fcf98487000-7fcf9848a000 ---p 00000000 00:00 0 
7fcf9848a000-7fcf98588000 rwxp 00000000 00:00 0 
7fcf98588000-7fcf9858b000 ---p 00000000 00:00 0 
7fcf9858b000-7fcf985c9000 rwxp 00000000 00:00 0 
7fcf985c9000-7fcf985cc000 ---p 00000000 00:00 0 
7fcf985cc000-7fcf9860a000 rwxp 00000000 00:00 0 
7fcf9860a000-7fcf9860b000 ---p 00000000 00:00 0 
7fcf9860b000-7fcf9870b000 rwxp 00000000 00:00 0 
7fcf9870b000-7fcf988a3000 r-xs 0302a000 08:03 32769255                   /export/servers/jdk1.6.0_25/jre/lib/rt.jar
7fcf988a3000-7fcf988a4000 ---p 00000000 00:00 0 
7fcf988a4000-7fcf989a4000 rwxp 00000000 00:00 0 
7fcf989a4000-7fcf989a5000 ---p 00000000 00:00 0 
7fcf989a5000-7fcf98aa5000 rwxp 00000000 00:00 0 
7fcf98aa5000-7fcf98aa6000 ---p 00000000 00:00 0 
7fcf98aa6000-7fcf98ba6000 rwxp 00000000 00:00 0 
7fcf98ba6000-7fcf98ba7000 ---p 00000000 00:00 0 
7fcf98ba7000-7fcf98ca7000 rwxp 00000000 00:00 0 
7fcf98ca7000-7fcf98ca8000 ---p 00000000 00:00 0 
7fcf98ca8000-7fcf98da8000 rwxp 00000000 00:00 0 
7fcf98da8000-7fcf98da9000 ---p 00000000 00:00 0 
7fcf98da9000-7fcf9bfbf000 rwxp 00000000 00:00 0 
7fcf9bfbf000-7fcf9c000000 rwxp 00000000 00:00 0 
7fcf9c000000-7fcf9c021000 rwxp 00000000 00:00 0 
7fcf9c021000-7fcfa0000000 ---p 00000000 00:00 0 
7fcfa0000000-7fcfa0021000 rwxp 00000000 00:00 0 
7fcfa0021000-7fcfa4000000 ---p 00000000 00:00 0 
7fcfa4000000-7fcfa4021000 rwxp 00000000 00:00 0 
7fcfa4021000-7fcfa8000000 ---p 00000000 00:00 0 
7fcfa8000000-7fcfa8021000 rwxp 00000000 00:00 0 
7fcfa8021000-7fcfac000000 ---p 00000000 00:00 0 
7fcfac000000-7fcfac021000 rwxp 00000000 00:00 0 
7fcfac021000-7fcfb0000000 ---p 00000000 00:00 0 
7fcfb0000000-7fcfb0021000 rwxp 00000000 00:00 0 
7fcfb0021000-7fcfb4000000 ---p 00000000 00:00 0 
7fcfb4000000-7fcfb4021000 rwxp 00000000 00:00 0 
7fcfb4021000-7fcfb8000000 ---p 00000000 00:00 0 
7fcfb8000000-7fcfb8021000 rwxp 00000000 00:00 0 
7fcfb8021000-7fcfbc000000 ---p 00000000 00:00 0 
7fcfbc000000-7fcfbc021000 rwxp 00000000 00:00 0 
7fcfbc021000-7fcfc0000000 ---p 00000000 00:00 0 
7fcfc0000000-7fcfc0021000 rwxp 00000000 00:00 0 
7fcfc0021000-7fcfc4000000 ---p 00000000 00:00 0 
7fcfc4000000-7fcfc4021000 rwxp 00000000 00:00 0 
7fcfc4021000-7fcfc8000000 ---p 00000000 00:00 0 
7fcfc8000000-7fcfc8021000 rwxp 00000000 00:00 0 
7fcfc8021000-7fcfcc000000 ---p 00000000 00:00 0 
7fcfcc000000-7fcfcc021000 rwxp 00000000 00:00 0 
7fcfcc021000-7fcfd0000000 ---p 00000000 00:00 0 
7fcfd0000000-7fcfd0021000 rwxp 00000000 00:00 0 
7fcfd0021000-7fcfd4000000 ---p 00000000 00:00 0 
7fcfd4000000-7fcfd4021000 rwxp 00000000 00:00 0 
7fcfd4021000-7fcfd8000000 ---p 00000000 00:00 0 
7fcfd8000000-7fcfd8021000 rwxp 00000000 00:00 0 
7fcfd8021000-7fcfdc000000 ---p 00000000 00:00 0 
7fcfdc000000-7fcfdc021000 rwxp 00000000 00:00 0 
7fcfdc021000-7fcfe0000000 ---p 00000000 00:00 0 
7fcfe0000000-7fcfe0021000 rwxp 00000000 00:00 0 
7fcfe0021000-7fcfe4000000 ---p 00000000 00:00 0 
7fcfe4000000-7fcfe4021000 rwxp 00000000 00:00 0 
7fcfe4021000-7fcfe8000000 ---p 00000000 00:00 0 
7fcfe8000000-7fcfe8021000 rwxp 00000000 00:00 0 
7fcfe8021000-7fcfec000000 ---p 00000000 00:00 0 
7fcfec000000-7fcfec021000 rwxp 00000000 00:00 0 
7fcfec021000-7fcff0000000 ---p 00000000 00:00 0 
7fcff0000000-7fcff0021000 rwxp 00000000 00:00 0 
7fcff0021000-7fcff4000000 ---p 00000000 00:00 0 
7fcff4000000-7fcff402b000 rwxp 00000000 00:00 0 
7fcff402b000-7fcff8000000 ---p 00000000 00:00 0 
7fcff8000000-7fcff8021000 rwxp 00000000 00:00 0 
7fcff8021000-7fcffc000000 ---p 00000000 00:00 0 
7fcffc000000-7fcffc03d000 rwxp 00000000 00:00 0 
7fcffc03d000-7fd000000000 ---p 00000000 00:00 0 
7fd000000000-7fd00002f000 rwxp 00000000 00:00 0 
7fd00002f000-7fd004000000 ---p 00000000 00:00 0 
7fd004000000-7fd004036000 rwxp 00000000 00:00 0 
7fd004036000-7fd008000000 ---p 00000000 00:00 0 
7fd008000000-7fd00803d000 rwxp 00000000 00:00 0 
7fd00803d000-7fd00c000000 ---p 00000000 00:00 0 
7fd00c000000-7fd00c021000 rwxp 00000000 00:00 0 
7fd00c021000-7fd010000000 ---p 00000000 00:00 0 
7fd010000000-7fd010021000 rwxp 00000000 00:00 0 
7fd010021000-7fd014000000 ---p 00000000 00:00 0 
7fd014000000-7fd014037000 rwxp 00000000 00:00 0 
7fd014037000-7fd018000000 ---p 00000000 00:00 0 
7fd018000000-7fd01803b000 rwxp 00000000 00:00 0 
7fd01803b000-7fd01c000000 ---p 00000000 00:00 0 
7fd01c000000-7fd01c038000 rwxp 00000000 00:00 0 
7fd01c038000-7fd020000000 ---p 00000000 00:00 0 
7fd020000000-7fd020038000 rwxp 00000000 00:00 0 
7fd020038000-7fd024000000 ---p 00000000 00:00 0 
7fd024000000-7fd024023000 rwxp 00000000 00:00 0 
7fd024023000-7fd028000000 ---p 00000000 00:00 0 
7fd028000000-7fd02803a000 rwxp 00000000 00:00 0 
7fd02803a000-7fd02c000000 ---p 00000000 00:00 0 
7fd02c000000-7fd02c044000 rwxp 00000000 00:00 0 
7fd02c044000-7fd030000000 ---p 00000000 00:00 0 
7fd030000000-7fd030029000 rwxp 00000000 00:00 0 
7fd030029000-7fd034000000 ---p 00000000 00:00 0 
7fd034000000-7fd034021000 rwxp 00000000 00:00 0 
7fd034021000-7fd038000000 ---p 00000000 00:00 0 
7fd038000000-7fd03802c000 rwxp 00000000 00:00 0 
7fd03802c000-7fd03c000000 ---p 00000000 00:00 0 
7fd03c000000-7fd03c002000 r-xs 0000a000 08:03 41681148                   /export/data/workRoot/es-search/lib/lucene-sandbox-4.3.1.jar
7fd03c002000-7fd03c00b000 r-xs 0006f000 08:03 41681138                   /export/data/workRoot/es-search/lib/log4j-1.2.17.jar
7fd03c00b000-7fd03c00e000 ---p 00000000 00:00 0 
7fd03c00e000-7fd03c04c000 rwxp 00000000 00:00 0 
7fd03c04c000-7fd03c04f000 ---p 00000000 00:00 0 
7fd03c04f000-7fd03c0c1000 rwxp 00000000 00:00 0 
7fd03c0c1000-7fd03c0c2000 ---p 00000000 00:00 0 
7fd03c0c2000-7fd03c1c2000 rwxp 00000000 00:00 0 
7fd03c1c2000-7fd03c1c3000 ---p 00000000 00:00 0 
7fd03c1c3000-7fd03c2c3000 rwxp 00000000 00:00 0 
7fd03c2c3000-7fd03c2c4000 ---p 00000000 00:00 0 
7fd03c2c4000-7fd03c3c4000 rwxp 00000000 00:00 0 
7fd03c3c4000-7fd03c3c5000 ---p 00000000 00:00 0 
7fd03c3c5000-7fd03c4c5000 rwxp 00000000 00:00 0 
7fd03c4c5000-7fd03c4c6000 ---p 00000000 00:00 0 
7fd03c4c6000-7fd03c5c6000 rwxp 00000000 00:00 0 
7fd03c5c6000-7fd03c5c7000 ---p 00000000 00:00 0 
7fd03c5c7000-7fd03c6c7000 rwxp 00000000 00:00 0 
7fd03c6c7000-7fd03c6c8000 ---p 00000000 00:00 0 
7fd03c6c8000-7fd03c7c8000 rwxp 00000000 00:00 0 
7fd03c7c8000-7fd03c7c9000 ---p 00000000 00:00 0 
7fd03c7c9000-7fd03c8c9000 rwxp 00000000 00:00 0 
7fd03c8c9000-7fd03c8ca000 ---p 00000000 00:00 0 
7fd03c8ca000-7fd03c9ca000 rwxp 00000000 00:00 0 
7fd03c9ca000-7fd03c9cb000 ---p 00000000 00:00 0 
7fd03c9cb000-7fd03cacb000 rwxp 00000000 00:00 0 
7fd03cacb000-7fd03cacc000 ---p 00000000 00:00 0 
7fd03cacc000-7fd03cbcc000 rwxp 00000000 00:00 0 
7fd03cbcc000-7fd03cbcd000 ---p 00000000 00:00 0 
7fd03cbcd000-7fd03cccd000 rwxp 00000000 00:00 0 
7fd03cccd000-7fd03ccce000 ---p 00000000 00:00 0 
7fd03ccce000-7fd03cdce000 rwxp 00000000 00:00 0 
7fd03cdce000-7fd03cdcf000 ---p 00000000 00:00 0 
7fd03cdcf000-7fd03cecf000 rwxp 00000000 00:00 0 
7fd03cecf000-7fd03ced0000 ---p 00000000 00:00 0 
7fd03ced0000-7fd03cfd0000 rwxp 00000000 00:00 0 
7fd03cfd0000-7fd03cfd1000 ---p 00000000 00:00 0 
7fd03cfd1000-7fd03d0d1000 rwxp 00000000 00:00 0 
7fd03d0d1000-7fd03d0d2000 ---p 00000000 00:00 0 
7fd03d0d2000-7fd03d1d2000 rwxp 00000000 00:00 0 
7fd03d1d2000-7fd03d1d3000 ---p 00000000 00:00 0 
7fd03d1d3000-7fd03d2d3000 rwxp 00000000 00:00 0 
7fd03d2d3000-7fd03d2d4000 ---p 00000000 00:00 0 
7fd03d2d4000-7fd03d3d4000 rwxp 00000000 00:00 0 
7fd03d3d4000-7fd03d3d5000 ---p 00000000 00:00 0 
7fd03d3d5000-7fd03d4d5000 rwxp 00000000 00:00 0 
7fd03d4d5000-7fd03d4d6000 ---p 00000000 00:00 0 
7fd03d4d6000-7fd03d5d6000 rwxp 00000000 00:00 0 
7fd03d5d6000-7fd03d5d7000 ---p 00000000 00:00 0 
7fd03d5d7000-7fd03d6d7000 rwxp 00000000 00:00 0 
7fd03d6d7000-7fd03d6d8000 ---p 00000000 00:00 0 
7fd03d6d8000-7fd03d7d8000 rwxp 00000000 00:00 0 
7fd03d7d8000-7fd03d7d9000 ---p 00000000 00:00 0 
7fd03d7d9000-7fd03eff0000 rwxp 00000000 00:00 0 
7fd03eff0000-7fd03eff1000 ---p 00000000 00:00 0 
7fd03eff1000-7fd03f0f1000 rwxp 00000000 00:00 0 
7fd03f0f1000-7fd03f0f2000 ---p 00000000 00:00 0 
7fd03f0f2000-7fd03f1f2000 rwxp 00000000 00:00 0 
7fd03f1f2000-7fd03f1f3000 ---p 00000000 00:00 0 
7fd03f1f3000-7fd03f2f3000 rwxp 00000000 00:00 0 
7fd03f2f3000-7fd03f2f4000 ---p 00000000 00:00 0 
7fd03f2f4000-7fd03f3f4000 rwxp 00000000 00:00 0 
7fd03f3f4000-7fd03f3f5000 ---p 00000000 00:00 0 
7fd03f3f5000-7fd03f4f5000 rwxp 00000000 00:00 0 
7fd03f4f5000-7fd03f4f6000 ---p 00000000 00:00 0 
7fd03f4f6000-7fd03f5f6000 rwxp 00000000 00:00 0 
7fd03f5f6000-7fd03f5f7000 ---p 00000000 00:00 0 
7fd03f5f7000-7fd03f6f7000 rwxp 00000000 00:00 0 
7fd03f6f7000-7fd03f6f8000 ---p 00000000 00:00 0 
7fd03f6f8000-7fd03f7f8000 rwxp 00000000 00:00 0 
7fd03f7f8000-7fd03f7f9000 ---p 00000000 00:00 0 
7fd03f7f9000-7fd03f8f9000 rwxp 00000000 00:00 0 
7fd03f8f9000-7fd03f8fa000 ---p 00000000 00:00 0 
7fd03f8fa000-7fd03f9fa000 rwxp 00000000 00:00 0 
7fd03f9fa000-7fd03f9fb000 ---p 00000000 00:00 0 
7fd03f9fb000-7fd03fafb000 rwxp 00000000 00:00 0 
7fd03fafb000-7fd03fafc000 ---p 00000000 00:00 0 
7fd03fafc000-7fd03fbfc000 rwxp 00000000 00:00 0 
7fd03fbfc000-7fd03fbfd000 ---p 00000000 00:00 0 
7fd03fbfd000-7fd03fcfd000 rwxp 00000000 00:00 0 
7fd03fcfd000-7fd03fcfe000 ---p 00000000 00:00 0 
7fd03fcfe000-7fd03fdfe000 rwxp 00000000 00:00 0 
7fd03fdfe000-7fd03fdff000 ---p 00000000 00:00 0 
7fd03fdff000-7fd03feff000 rwxp 00000000 00:00 0 
7fd03feff000-7fd03ff00000 ---p 00000000 00:00 0 
7fd03ff00000-7fd040000000 rwxp 00000000 00:00 0 
7fd040000000-7fd04003a000 rwxp 00000000 00:00 0 
7fd04003a000-7fd044000000 ---p 00000000 00:00 0 
7fd044000000-7fd044046000 rwxp 00000000 00:00 0 
7fd044046000-7fd048000000 ---p 00000000 00:00 0 
7fd048000000-7fd048021000 rwxp 00000000 00:00 0 
7fd048021000-7fd04c000000 ---p 00000000 00:00 0 
7fd04c000000-7fd04c02c000 rwxp 00000000 00:00 0 
7fd04c02c000-7fd050000000 ---p 00000000 00:00 0 
7fd050000000-7fd05002a000 rwxp 00000000 00:00 0 
7fd05002a000-7fd054000000 ---p 00000000 00:00 0 
7fd054000000-7fd054046000 rwxp 00000000 00:00 0 
7fd054046000-7fd058000000 ---p 00000000 00:00 0 
7fd058000000-7fd059720000 rwxp 00000000 00:00 0 
7fd059720000-7fd05c000000 ---p 00000000 00:00 0 
7fd05c000000-7fd05c006000 r-xs 00043000 08:03 41681140                   /export/data/workRoot/es-search/lib/lucene-codecs-4.3.1.jar
7fd05c006000-7fd05c00a000 r-xs 000d0000 08:03 41681136                   /export/data/workRoot/es-search/lib/jna-3.3.0.jar
7fd05c00a000-7fd05c0b4000 rwxp 00000000 00:00 0 
7fd05c0b4000-7fd05c0b5000 ---p 00000000 00:00 0 
7fd05c0b5000-7fd05c1b5000 rwxp 00000000 00:00 0 
7fd05c1b5000-7fd05c1b6000 ---p 00000000 00:00 0 
7fd05c1b6000-7fd05c2b6000 rwxp 00000000 00:00 0 
7fd05c2b6000-7fd05c2b7000 ---p 00000000 00:00 0 
7fd05c2b7000-7fd05c3b7000 rwxp 00000000 00:00 0 
7fd05c3b7000-7fd05c3b8000 ---p 00000000 00:00 0 
7fd05c3b8000-7fd05c4b8000 rwxp 00000000 00:00 0 
7fd05c4b8000-7fd05c4b9000 ---p 00000000 00:00 0 
7fd05c4b9000-7fd05c5b9000 rwxp 00000000 00:00 0 
7fd05c5b9000-7fd05c5ba000 ---p 00000000 00:00 0 
7fd05c5ba000-7fd05c6ba000 rwxp 00000000 00:00 0 
7fd05c6ba000-7fd05c6bb000 ---p 00000000 00:00 0 
7fd05c6bb000-7fd05c7d1000 rwxp 00000000 00:00 0 
7fd05c7d1000-7fd05c87b000 rwxp 00000000 00:00 0 
7fd05c87b000-7fd05cdfb000 rwxp 00000000 00:00 0 
7fd05cdfb000-7fd05f87b000 rwxp 00000000 00:00 0 
7fd05f87b000-7fd05f889000 r-xp 00000000 08:03 32769303                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libzip.so
7fd05f889000-7fd05f98b000 ---p 0000e000 08:03 32769303                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libzip.so
7fd05f98b000-7fd05f98e000 rwxp 00010000 08:03 32769303                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libzip.so
7fd05f98e000-7fd05f98f000 rwxp 00000000 00:00 0 
7fd05f98f000-7fd05f99b000 r-xp 00000000 08:01 261152                     /lib64/libnss_files-2.12.so
7fd05f99b000-7fd05fb9b000 ---p 0000c000 08:01 261152                     /lib64/libnss_files-2.12.so
7fd05fb9b000-7fd05fb9c000 r-xp 0000c000 08:01 261152                     /lib64/libnss_files-2.12.so
7fd05fb9c000-7fd05fb9d000 rwxp 0000d000 08:01 261152                     /lib64/libnss_files-2.12.so
7fd05fb9e000-7fd05fba6000 rwxs 00000000 08:01 793951                     /tmp/hsperfdata_admin/117184
7fd05fba6000-7fd05fbcf000 r-xp 00000000 08:03 32769302                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libjava.so
7fd05fbcf000-7fd05fcce000 ---p 00029000 08:03 32769302                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libjava.so
7fd05fcce000-7fd05fcd5000 rwxp 00028000 08:03 32769302                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libjava.so
7fd05fcd5000-7fd05fce2000 r-xp 00000000 08:03 32769301                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libverify.so
7fd05fce2000-7fd05fde1000 ---p 0000d000 08:03 32769301                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libverify.so
7fd05fde1000-7fd05fde4000 rwxp 0000c000 08:03 32769301                   /export/servers/jdk1.6.0_25/jre/lib/amd64/libverify.so
7fd05fde4000-7fd05fde7000 ---p 00000000 00:00 0 
7fd05fde7000-7fd05fe25000 rwxp 00000000 00:00 0 
7fd05fe25000-7fd06073f000 r-xp 00000000 08:03 32769284                   /export/servers/jdk1.6.0_25/jre/lib/amd64/server/libjvm.so
7fd06073f000-7fd060841000 ---p 0091a000 08:03 32769284                   /export/servers/jdk1.6.0_25/jre/lib/amd64/server/libjvm.so
7fd060841000-7fd0609f6000 rwxp 0091c000 08:03 32769284                   /export/servers/jdk1.6.0_25/jre/lib/amd64/server/libjvm.so
7fd0609f6000-7fd060a33000 rwxp 00000000 00:00 0 
7fd060a33000-7fd060a3a000 r-xp 00000000 08:03 32769296                   /export/servers/jdk1.6.0_25/jre/lib/amd64/jli/libjli.so
7fd060a3a000-7fd060b3b000 ---p 00007000 08:03 32769296                   /export/servers/jdk1.6.0_25/jre/lib/amd64/jli/libjli.so
7fd060b3b000-7fd060b3d000 rwxp 00008000 08:03 32769296                   /export/servers/jdk1.6.0_25/jre/lib/amd64/jli/libjli.so
7fd060b3d000-7fd060b3e000 rwxp 00000000 00:00 0 
7fd060b3e000-7fd060b3f000 r-xs 00003000 08:03 41681170                   /export/data/workRoot/es-search/plugins/analysis-pinyin/elasticsearch-analysis-pinyin-1.2.1.jar
7fd060b3f000-7fd060b42000 r-xs 000cb000 08:03 32769321                   /export/servers/jdk1.6.0_25/jre/lib/ext/localedata.jar
7fd060b42000-7fd060b45000 r-xs 0001b000 08:03 41681006                   /export/data/workRoot/es-search/bin/service/lib/wrapper.jar
7fd060b45000-7fd060b46000 rwxp 00000000 00:00 0 
7fd060b46000-7fd060b47000 r-xp 00000000 00:00 0 
7fd060b47000-7fd060b48000 rwxp 00000000 00:00 0 
7fff78a52000-7fff78a68000 rwxp 00000000 00:00 0                          [stack]
7fff78b55000-7fff78b56000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Ddeploy.project.id=2706 -Ddeploy.app.id=107 -Ddeploy.app.name=ElasticSearchCluster -Ddeploy.instance.id=1100220 -Dins_id=1100220 -Ddeploy.dynamic.config.dir=/export/data/workRoot/es-search/ -Delasticsearch-service -Des.path.home=/export/data/workRoot/es-search -Xss256k -Xmn2048M -XX:+HeapDumpOnOutOfMemoryError -XX:ErrorFile=/export/home/tomcat/logs/logsearch.360buy.com/hs_err_pid&lt;pid&gt;.log -XX:HeapDumpPath=/export/home/tomcat/logs/logsearch.360buy.com/java_pid&lt;pid&gt;.hprof -XX:PermSize=128M -XX:MaxPermSize=128M -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:SurvivorRatio=7 -XX:GCTimeRatio=49 -XX:MaxTenuringThreshold=12 -Xloggc:/export/home/tomcat/logs/logsearch.360buy.com/es_gc.log -XX:+PrintPromotionFailure -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -XX:OnOutOfMemoryError="/export/data/workRoot/es-search/bin/restart.sh" -Xms16384m -Xmx16384m -Djava.library.path=/export/data/workRoot/es-search/bin/service/lib -Dwrapper.key=TIZb0SJ_Ww7TT1fJ -Dwrapper.port=32000 -Dwrapper.jvm.port.min=31000 -Dwrapper.jvm.port.max=31999 -Dwrapper.disable_console_input=TRUE -Dwrapper.pid=124445 -Dwrapper.version=3.5.14 -Dwrapper.native_library=wrapper -Dwrapper.service=TRUE -Dwrapper.cpu.timeout=10 -Dwrapper.jvmid=6 
java_command: org.tanukisoftware.wrapper.WrapperSimpleApp org.elasticsearch.bootstrap.ElasticSearchF
Launcher Type: SUN_STANDARD

Environment Variables:
JAVA_HOME=/export/servers/jdk1.6.0_25
JAVA_TOOL_OPTIONS= -Ddeploy.project.id=2706 -Ddeploy.app.id=107 -Ddeploy.app.name=ElasticSearchCluster -Ddeploy.instance.id=1100220 -Dins_id=1100220 -Ddeploy.dynamic.config.dir=/export/data/workRoot/es-search/
CLASSPATH=.:/export/servers/jdk1.6.0_25/lib/dt.jar:/export/servers/jdk1.6.0_25/lib/tools.jar
PATH=/export/servers/jdk1.6.0_25/bin:/export/servers/autodeploy_agent/bin:/usr/bin:/bin:/usr/local/sbin:/usr/sbin:/sbin:/home/admin/bin
LD_LIBRARY_PATH=/export/servers/jdk1.6.0_25/jre/lib/amd64/server:/export/servers/jdk1.6.0_25/jre/lib/amd64:/export/servers/jdk1.6.0_25/jre/../lib/amd64
SHELL=/bin/sh

Signal Handlers:
SIGSEGV: [libjvm.so+0x85f690], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGBUS: [libjvm.so+0x85f690], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGFPE: [libjvm.so+0x70e190], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGPIPE: [libjvm.so+0x70e190], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGXFSZ: [libjvm.so+0x70e190], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGILL: [libjvm.so+0x70e190], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGUSR1: SIG_DFL, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGUSR2: [libjvm.so+0x710fa0], sa_mask[0]=0x00000000, sa_flags=0x10000004
SIGHUP: [libjvm.so+0x710ba0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGINT: [libjvm.so+0x710ba0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGTERM: [libjvm.so+0x710ba0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGQUIT: [libjvm.so+0x710ba0], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004

---------------  S Y S T E M  ---------------

OS:CentOS release 6.6 (Final)

uname:Linux 2.6.32-504.el6.x86_64 #1 SMP Wed Oct 15 04:27:16 UTC 2014 x86_64
libc:glibc 2.12 NPTL 2.12 
rlimit: STACK 10240k, CORE 0k, NPROC 204800, NOFILE 204800, AS infinity
load average:0.70 0.63 0.91

/proc/meminfo:
MemTotal:       264288952 kB
MemFree:        221971548 kB
Buffers:          397380 kB
Cached:         32588432 kB
SwapCached:            0 kB
Active:          9627640 kB
Inactive:       29893068 kB
Active(anon):    6534948 kB
Inactive(anon):      156 kB
Active(file):    3092692 kB
Inactive(file): 29892912 kB
Unevictable:           0 kB
Mlocked:               0 kB
SwapTotal:      16777212 kB
SwapFree:       16777212 kB
Dirty:              7900 kB
Writeback:             0 kB
AnonPages:       6567988 kB
Mapped:            23636 kB
Shmem:               164 kB
Slab:            1205772 kB
SReclaimable:    1135236 kB
SUnreclaim:        70536 kB
KernelStack:        6872 kB
PageTables:        16164 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    148921688 kB
Committed_AS:   18389312 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      703332 kB
VmallocChunk:   34225708560 kB
HardwareCorrupted:     0 kB
AnonHugePages:   6170624 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:        7168 kB
DirectMap2M:     2015232 kB
DirectMap1G:    266338304 kB

CPU:total 32 (8 cores per cpu, 2 threads per core) family 6 model 63 stepping 2, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, ht

/proc/cpuinfo:
processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 0
cpu cores   : 8
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 1
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 0
cpu cores   : 8
apicid      : 16
initial apicid  : 16
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 2
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 1
cpu cores   : 8
apicid      : 2
initial apicid  : 2
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 3
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 1
cpu cores   : 8
apicid      : 18
initial apicid  : 18
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 4
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 2
cpu cores   : 8
apicid      : 4
initial apicid  : 4
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 5
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 2
cpu cores   : 8
apicid      : 20
initial apicid  : 20
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 6
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 3
cpu cores   : 8
apicid      : 6
initial apicid  : 6
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 7
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 3
cpu cores   : 8
apicid      : 22
initial apicid  : 22
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 8
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 4
cpu cores   : 8
apicid      : 8
initial apicid  : 8
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 9
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 4
cpu cores   : 8
apicid      : 24
initial apicid  : 24
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 10
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 5
cpu cores   : 8
apicid      : 10
initial apicid  : 10
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 11
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 5
cpu cores   : 8
apicid      : 26
initial apicid  : 26
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 12
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 6
cpu cores   : 8
apicid      : 12
initial apicid  : 12
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 13
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 6
cpu cores   : 8
apicid      : 28
initial apicid  : 28
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 14
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 7
cpu cores   : 8
apicid      : 14
initial apicid  : 14
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 15
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 7
cpu cores   : 8
apicid      : 30
initial apicid  : 30
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 16
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 0
cpu cores   : 8
apicid      : 1
initial apicid  : 1
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 17
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 0
cpu cores   : 8
apicid      : 17
initial apicid  : 17
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 18
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 1
cpu cores   : 8
apicid      : 3
initial apicid  : 3
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 19
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 1
cpu cores   : 8
apicid      : 19
initial apicid  : 19
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 20
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 2
cpu cores   : 8
apicid      : 5
initial apicid  : 5
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 21
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 2
cpu cores   : 8
apicid      : 21
initial apicid  : 21
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 22
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 3
cpu cores   : 8
apicid      : 7
initial apicid  : 7
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 23
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 3
cpu cores   : 8
apicid      : 23
initial apicid  : 23
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 24
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 4
cpu cores   : 8
apicid      : 9
initial apicid  : 9
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 25
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 4
cpu cores   : 8
apicid      : 25
initial apicid  : 25
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 26
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 5
cpu cores   : 8
apicid      : 11
initial apicid  : 11
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 27
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 5
cpu cores   : 8
apicid      : 27
initial apicid  : 27
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 28
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 6
cpu cores   : 8
apicid      : 13
initial apicid  : 13
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 29
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 6
cpu cores   : 8
apicid      : 29
initial apicid  : 29
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 30
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 0
siblings    : 16
core id     : 7
cpu cores   : 8
apicid      : 15
initial apicid  : 15
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5200.27
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

processor   : 31
vendor_id   : GenuineIntel
cpu family  : 6
model       : 63
model name  : Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz
stepping    : 2
microcode   : 39
cpu MHz     : 2600.138
cache size  : 20480 KB
physical id : 1
siblings    : 16
core id     : 7
cpu cores   : 8
apicid      : 31
initial apicid  : 31
fpu     : yes
fpu_exception   : yes
cpuid level : 15
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm ida arat epb xsaveopt pln pts dts tpr_shadow vnmi flexpriority ept vpid fsgsbase bmi1 avx2 smep bmi2 erms invpcid
bogomips    : 5199.25
clflush size    : 64
cache_alignment : 64
address sizes   : 46 bits physical, 48 bits virtual
power management:

Memory: 4k page, physical 264288952k(221971548k free), swap 16777212k(16777212k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (20.0-b11) for linux-amd64 JRE (1.6.0_25-b06), built on Apr 14 2011 01:22:12 by "java_re" with gcc 3.2.2 (SuSE Linux)

time: Mon Sep  7 10:23:19 2015
elapsed time: 82 seconds
</description><key id="105141577">13368</key><summary>es 0.90.2 plus jdk6.0_25-b06 crashed on production</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-09-07T03:59:59Z</created><updated>2015-09-07T04:57:49Z</updated><resolved>2015-09-07T04:57:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-07T04:57:45Z" id="138186155">This version is not maintained anymore.
Upgrade!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchResponse question</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13367</link><project id="" key="" /><description>SearchResponse has a method called getTookInMillis()
I'd like to ask how this timecost is calculated?
I mean if it is the timecost between it hits the server and leaves the server with timecost when it is waited in queue if search worker thread is full or only the timecost between query and fetch without queue time?
if it is the later, how to get the complete search timecost? 
</description><key id="105129874">13367</key><summary>SearchResponse question</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">makeyang</reporter><labels /><created>2015-09-07T01:27:53Z</created><updated>2015-09-09T01:37:45Z</updated><resolved>2015-09-07T09:31:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-09-07T09:31:24Z" id="138250595">Please use [the forums](http://discuss.elastic.co) to ask questions.
</comment><comment author="makeyang" created="2015-09-08T02:16:10Z" id="138410204">@jpountz 
as I mentioned in https://github.com/elastic/elasticsearch/issues/13172, I can't visit discuss.elastic.co. before I can, can you help to answer this question?
</comment><comment author="jpountz" created="2015-09-08T21:39:08Z" id="138711212">@makeyang when the coordinating node receives the request, it stores the current timestamp. Later, after the coordinating node got all required data from all shards, it computes the delta between the current timestamp and the start time. So the only things that it does not contain are the time it takes to send the initial request to elasticsearch and serialize the response back to the client.
</comment><comment author="makeyang" created="2015-09-09T01:37:45Z" id="138749002">@jpountz 
thank you very much.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Packaging tests should use jdk 8 on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13366</link><project id="" key="" /><description>Master requires Java 8 so we'll need to use the jdk packages for Java 8 there.

We should continue to use Java 7 in 2.0 and 2.x because that is the minimum requirement.
</description><key id="105125618">13366</key><summary>Packaging tests should use jdk 8 on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Packaging</label><label>test</label><label>v5.0.0-alpha1</label></labels><created>2015-09-07T00:35:47Z</created><updated>2015-09-14T12:26:04Z</updated><resolved>2015-09-14T12:26:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>RestUtils.decodeQueryString ignores the URI fragment when parsing a query string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13365</link><project id="" key="" /><description>Fixes #13320 
</description><key id="105117066">13365</key><summary>RestUtils.decodeQueryString ignores the URI fragment when parsing a query string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">camilojd</reporter><labels><label>:REST</label><label>bug</label><label>review</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-06T21:07:29Z</created><updated>2015-09-22T15:55:08Z</updated><resolved>2015-09-21T20:19:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="camilojd" created="2015-09-06T21:15:28Z" id="138126080">FWIW: Just signed the CLA!
</comment><comment author="nik9000" created="2015-09-21T18:18:11Z" id="142065967">Fix looks fine - I'm going to pull it locally and rerun the tests for paranoia's sake.
</comment><comment author="nik9000" created="2015-09-21T19:14:34Z" id="142081878">And tests passed for me locally. I'll merge this to 2.x and master soon.
</comment><comment author="nik9000" created="2015-09-21T20:21:51Z" id="142098033">Merged to master: 6876321581d2154cb4f73b15bae304bdc99f5ed8

I'm backporting to 2.x.
</comment><comment author="nik9000" created="2015-09-21T20:36:56Z" id="142101433">And cherry-picked to 2.x. Thanks @camilojd!
</comment><comment author="camilojd" created="2015-09-22T15:55:08Z" id="142331853">Great. Thank you @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"http.max_content_length" setting ignored?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13364</link><project id="" key="" /><description>I'm unable to specify the `http.max_content_length` setting via either of the following:

``` sh
curl -XPUT localhost:9200/_cluster/settings -d '{ "transient" : { "http.max_content_length" : "400mb" } }'
{"acknowledged":true,"persistent":{},"transient":{}}

curl -XPUT localhost:9200/_cluster/settings -d '{ "persistent" : { "http.max_content_length" : "400mb" } }'
{"acknowledged":true,"persistent":{},"transient":{}}
```

If changing HTTP settings dynamically is prohibited, can this be documented please?

**Update**. Just found from logs that it's indeed not dynamically updatable:

```
[2015-09-06 19:49:54,552][WARN ][action.admin.cluster.settings] [Griffin] ignoring transient setting [http.max_content_length], not dynamically updateable
```

Please, **update the documentation or close the ticket at will**.
</description><key id="105103997">13364</key><summary>"http.max_content_length" setting ignored?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">gmile</reporter><labels><label>adoptme</label><label>docs</label></labels><created>2015-09-06T16:55:25Z</created><updated>2015-09-22T09:55:34Z</updated><resolved>2015-09-22T09:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[cloud-aws] Remove duplicated cloud.aws settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13363</link><project id="" key="" /><description>As we split cloud-aws plugin into two separate plugins, I think we don't need to keep both `cloud.aws` and `cloud.aws.[s3|ec2]` settings in two separate settings.

For example we have both `cloud.aws.protocol` and `cloud.aws.ec2.protocol` for ec2, they are totally same now.
</description><key id="105075738">13363</key><summary>[cloud-aws] Remove duplicated cloud.aws settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Discovery EC2</label><label>:Plugin Repository S3</label><label>discuss</label><label>enhancement</label></labels><created>2015-09-06T06:51:33Z</created><updated>2015-10-28T05:05:20Z</updated><resolved>2015-10-28T05:05:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-06T06:56:43Z" id="138047879">I don't think so.

From a end user perspective, giving only once my settings should be enough. And it could apply to s3 and ec2 (most of the cases).

But would love to hear also what others think.
</comment><comment author="xuzha" created="2015-10-28T05:05:20Z" id="151726385">Looks like no need to do any change, closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Template not applied to some documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13362</link><project id="" key="" /><description>I've distilled this from logstash, it appears that the template is not applied to the last document in some cases, so the value of field2 (cat) doesn't get included in all, so the following query:

``` json
GET /repro/_search
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "bye cat"
        }
      }
    }
  }
}
```

returns both documents the first time, then after waiting 20s when additional fields are added to the same document and the index recreated, only one is returned.

``` bash
curl -XDELETE localhost:9200/repro
curl -XDELETE localhost:9200/_template/repro
curl -XPUT localhost:9200/_template/repro -d '{"template":"repro","mappings":{"_default_":{"include_in_all":false,"_all":{"analyzer":"keyword"},"properties":{"field1":{"type":"string"},"field2":{"type":"string","include_in_all":true}}}}}'
curl -XPOST localhost:9200/_bulk --data-binary '
{"index":{"_id":null,"_index":"repro","_type":"logs"}}
{"field1":"hello","field2":"bye"}
{"index":{"id":null,"_index":"repro","_type":"logs"}}
{"field1":"dog","field2":"cat"}

'
sleep 20
curl -XDELETE localhost:9200/repro
curl -XPOST localhost:9200/_bulk --data-binary '
{"index":{"_id":null,"_index":"repro","_type":"logs"}}
{"field1":"hello","field2":"bye","@version":"1"}
{"index":{"_id":null,"_index":"repro","_type":"logs"}}
{"field1":"dog","field2":"cat","@version":"1"}

'
```
</description><key id="105045163">13362</key><summary>Template not applied to some documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jimmyjones2</reporter><labels /><created>2015-09-05T19:47:29Z</created><updated>2015-09-06T16:24:55Z</updated><resolved>2015-09-06T14:20:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-09-06T14:20:37Z" id="138089429">Hi @jimmyjones2 

I'm guessing you're on a version before 1.6?  There is a bug (which appears to be fixed in 1.6.0) but it is different from what you describe. It can be reproduced as follows:

```
DELETE *
DELETE /_template/repro

PUT /_template/repro
{
  "template": "repro",
  "mappings": {
    "_default_": {
      "include_in_all": false,
      "_all": {
        "analyzer": "keyword"
      },
      "properties": {
        "field1": {
          "type": "string"
        },
        "field2": {
          "type": "string",
          "include_in_all": true
        }
      }
    }
  }
}
```

Note: Your template sets the `analyzer` on the `_all` field to `keyword`, which is definitely not what you want to do for reasons described below.

Create documents with the `@version` field:

```
POST _bulk 
{"index":{"_id":null,"_index":"repro","_type":"logs"}}
{"field1":"hello","field2":"bye","@version": "1"}
{"index":{"_id":null,"_index":"repro","_type":"logs"}}
{"field1":"dog","field2":"cat","@version": "1"}
```

The mapping indicates that `field1` and `@version` have `include_in_all` set to `false`, and `field2` set to `true`, which is what you expect:

```
GET _mapping
```

However, the `_all` field is not as expected:

```
GET _search?size=0
{
  "aggs": {
    "NAME": {
      "terms": {
        "field": "_all",
        "size": 10
      }
    }
  }
}
```

The `_all` field includes `bye`, which is correct, but it also includes the single term `cat 1` which is incorrect.  For some reason it included the `@version` field value as well, even though the final mapping is correct.

This also demonstrates why you don't want to set the analyzer for the `_all` field to `keyword`. Field values are not added to `_all` as multiple values but as a single string joined by spaces, so the `keyword` analyzer just ends up indexing a single term for the `_all` field. On top of that, the order that fields are added to `_all` is non-deterministic.

Either way, the index template/mapping issue appears to have been fixed already so I'll close this issue
</comment><comment author="jimmyjones2" created="2015-09-06T16:24:54Z" id="138097670">@clintongormley Running 1.7.1 and that shows the issue. Tried 2.0beta1 and that worked fine.

The reason I used the keyword analyzer is I've got mutliple fields containing filenames which can contain spaces and I didn't want the additional terms created that the default analyzer of _all would, but I didn't realise _all_ the terms marked with include_in_all got merged - thanks for clearing that up! Was trying to workaround #13214, nevermind!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>number_of_pending_tasks keeps increasing indefinitely</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13361</link><project id="" key="" /><description>I had to restart the master elasticsearch, the status was red, then after some time the status went yellow (I guess primary shards went assigned).

Now when I'm doing the query `curl http://x.x.x.x/_cluster/health?pretty` I can see that **the "number_of_pending_tasks" keeps increasing (now it is at 200k)**

I had a look at the pending tasks and I can see that it is mainly this tasks that get buffered:

```
    , {
        "insert_order" : 58176,
        "priority" : "NORMAL",
        "source" : "indices_store",
        "executing" : false,
        "time_in_queue_millis" : 619596,
        "time_in_queue" : "10.3m"
      },
```

In the meantime I get the error about a rejected execution due to the queue capacity:

```
Caused by: org.elasticsearch.common.util.concurrent.EsRejectedExecutionException: rejected execution (queue capacity 200) on org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler@34c87ed9
```

How can I solve this? 
</description><key id="105030825">13361</key><summary>number_of_pending_tasks keeps increasing indefinitely</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Unitech</reporter><labels /><created>2015-09-05T14:53:51Z</created><updated>2015-09-08T07:31:51Z</updated><resolved>2015-09-05T16:35:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2015-09-05T16:11:36Z" id="137971896">&gt; I had a look at the pending tasks and I can see that it is mainly this tasks that get buffered:

This is the task that gets executed when deleting shard copies that are no longer assigned to the node. Can you check what the currently executing task is? it's marked with `executing: true`. 

Also - what is the max time in queue? are thy all low-ish (10m)?

&gt; In the meantime I get the error about a rejected execution due to the queue capacity:

The task queue is unbound. What operations due you get this error from? by the queue capacity , I would guess that these are indexing requests.

Anything else of interest in the node logs?
</comment><comment author="bleskes" created="2015-09-05T16:11:51Z" id="137971904">and of course, what version of ES are you using?
</comment><comment author="Unitech" created="2015-09-05T16:35:23Z" id="137972990">After that all shards got assigned, 10minutes after the `number_of_pending_tasks` went from 2 millions to around 50k automatically :)

Sometimes using ES without deep knowledge about it can be quite stressful in case of outage, but it so performant!

I'm using elasticsearch 1.6,

I think I can close the issue, thanks for your help
</comment><comment author="bleskes" created="2015-09-05T17:41:30Z" id="137980762">thanks for coming back with the info. If you don't mind I would like to know more- what you describe shouldn't happen under normal circumstances. How many shards &amp; nodes do you have in your cluster? Do you run on local disks or shared file system? I also understand you don't have dedicated master nodes at the moment. Is that correct?

On Sat, Sep 5, 2015 at 6:35 PM, Alexandre Strzelewicz
notifications@github.com wrote:

&gt; After that all shards got assigned, 10minutes after the `number_of_pending_tasks` went from 2 millions to around 50k automatically :)
&gt; Sometimes using ES without deep knowledge about it can be quite stressful in case of outage, but it so performant!
&gt; I'm using elasticsearch 1.6,
&gt; 
&gt; ## I think I can close the issue, thanks for your help
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elastic/elasticsearch/issues/13361#issuecomment-137972990
</comment><comment author="iravid" created="2015-09-07T10:09:03Z" id="138258890">Hi @bleskes, I'd like to chime in since I'm seeing the same issue. I'm running a cluster of 5 nodes on AWS, with around 28k shards. We're running on EBS volumes. ES version is 1.5.2.

The issue started after adding the 5th node (due to the shard rebalancing, I assume). During debugging, I noticed that pending tasks rises indefinitely; counting the tasks by type using _cat/pending_tasks resulted in a large number of indices_store tasks. When setting cluster.routing.allocation.cluster_concurrent_rebalance to 0, the tasks quickly disappear (as @Unitech mentioned).

While the rebalance is enabled, the master node's heap usage rises to around 90-95% (causing big GC cycles). When the rebalance is disabled as mentioned above, heap usage drops sharply.
</comment><comment author="Unitech" created="2015-09-07T15:32:29Z" id="138326015">&gt; what you describe shouldn't happen under normal circumstances. 

My setup is one master (with data) and 2 slaves. I've approximately 24k shards.

The ES master was not responding to some queries (/_cat/indices for example). So I stopped the 2 slaves and then I restarted the master. I put back online the 2 slaves and I waited about 2hours but the shard assignement was taking too much time, so I tuned some recovery configuration options and it was faster.

I know I made some mistakes. I then followed this tutorial [Full Cluster restart upgrade](https://www.elastic.co/guide/en/elasticsearch/reference/master/restart-upgrade.html#restart-upgrade)

&gt; How many shards &amp; nodes do you have in your cluster? 

25k

&gt; Do you run on local disks or shared file system?

Local disk on bare metal servers (SSD)

&gt; I also understand you don't have dedicated master nodes at the moment. Is that correct?

Yes I have one dedicated master

Here is my configuration file:
https://gist.github.com/Unitech/9faa8d7e943e02b76895
</comment><comment author="bleskes" created="2015-09-08T07:31:51Z" id="138461102">@iravid those task are expected. They are the last protection in a set of checks we do before deleting data from disk. More specifically, when a shard is moved from node A to B, node A will not delete it's copy until it get's a confirmation from B and all other nodes that have a copy of the shard (typically, 1 node if you have 1 replica) that they shard is active is OK. I can see how in the case of 25K shards ( a lot for such a small cluster, you should look at reducing this number for other reasons too- it's typically inefficient) these can accumulate when a lot of other things happen (they don't have high priority) but they are really light and the node should process them super quickly.

@Unitech I see thx. there are a couple of issues in your config file (though I don't see yet how it explains what you were seeing).:

1) setting node.master: true , doesn't make a node a dedicated master node. To do so you also need to make the node not hold data by setting node.data: false
2) recovery settings (gateway.recover_after_nodes, gateway.expected_nodes) suggest you expect 1 node, but I think it should be two (or three, depending whether you want data on your master).
3) discovery.zen.ping.timeout: 30s  - why is this so high?
4) discovery.zen.minimum_master_nodes: 1 - this is the default and can be removed (see point about running with one master later on)
5) Unless you had a very reason (which I would be curious to hear), I would remove all the thread pool settings. The default should be good. Also index.store.type: mmapfs can be removed (we have a smart default of using this where it helps).

Last - are you sure you want to run with a single dedicated master? if so, when the master dies, the cluster will not except writes until a new master is brought back to life.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable S3SignerType</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13360</link><project id="" key="" /><description>This PR enable user with S3SignerType, which is v2 signer type some of S3 compatible services but without V4 signer type would need this setting.

Related to https://github.com/elastic/elasticsearch/issues/13332
</description><key id="104993134">13360</key><summary>Enable S3SignerType</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xuzha</reporter><labels><label>:Plugin Cloud AWS</label><label>:Plugin Repository S3</label><label>enhancement</label><label>v2.1.0</label><label>v5.0.0-alpha1</label></labels><created>2015-09-05T00:50:00Z</created><updated>2015-11-20T14:10:56Z</updated><resolved>2015-09-06T06:01:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-05T14:53:24Z" id="137962700">So if I understand correctly, S3Signer is specific to S3 endpoints.
Does it mean that AWSxSignerType are specific to EC2 endpoints?

Any chance you can fill this table with `X` where a type is available for a given API?

| Signer | EC2 | S3 |
| :-: | :-: | :-: |
| QueryStringSignerType |  |  |
| AWS3SignerType |  |  |
| AWS4SignerType |  |  |
| NoOpSignerType |  |  |
| S3SignerType |  | X |

Thanks!
</comment><comment author="xuzha" created="2015-09-05T18:15:30Z" id="137982551">@dadoonet  sure. Yes from the aws-sdk, S3SignerType is only for S3, [source](https://github.com/aws/aws-sdk-java/blob/1.10.12/aws-java-sdk-s3/src/main/java/com/amazonaws/services/s3/AmazonS3Client.java#L275) . This is indeed a very hacky solution and Beijing and Frankfurt region don't support it. But there's not much we can do about it for these service only support V2. Do you think we should log a warning msg here?

And the form : 

| Signer | EC2 | S3 |
| :-: | :-: | :-: |
| QueryStringSignerType | ? | ? |
| AWS3SignerType | X | X |
| AWS4SignerType | X | X |
| NoOpSignerType | X | X |
| S3SignerType |  | X |
| AWSS3V4SignerType |  | X |

For QueryStringSignerType, both EC2 and S3 client accept them. I think QueryStringSignerType should be used only for AWS query string request. I think we could it there let user decided if they want to use it.
</comment><comment author="dadoonet" created="2015-09-05T20:06:12Z" id="137990132">I left some comments. 

I think we should also document it. I might have forgotten that part when I split AWS plugin in 2 plugins.

Could you change the following?
- https://github.com/elastic/elasticsearch/blob/master/docs/plugins/repository-s3.asciidoc#L3-2 (add the section which is in ec2 doc here)
- https://github.com/elastic/elasticsearch/blob/master/docs/plugins/discovery-ec2.asciidoc#L113-113 (remove reference to S3 there)
</comment><comment author="xuzha" created="2015-09-05T22:35:36Z" id="138004865">thanks for the review @dadoonet 

I updated the PR. Right now it only logging a warning message rather than block user. 
I also updated the signer part in the document.
</comment><comment author="dadoonet" created="2015-09-05T22:45:37Z" id="138005184">I left some new comments. I think we are super close now.
</comment><comment author="xuzha" created="2015-09-05T22:52:55Z" id="138005843">@dadoonet  thanks, updated.
</comment><comment author="dadoonet" created="2015-09-05T23:00:32Z" id="138006051">LGTM! Thanks!
</comment><comment author="dadoonet" created="2015-09-05T23:01:30Z" id="138006066">Note that if we want to push that in 2.x branch, it will require you some work as we split cloud-aws plugin only in master branch.
</comment><comment author="xuzha" created="2015-09-06T06:42:55Z" id="138047277">Thanks  @dadoonet.
For 2.x branch, I did slightly differently here to avoid making the change complicated. https://github.com/elastic/elasticsearch/commit/736e21f021301c2b13f891a056e4f35ebd99dba8.  Let me know if you think we need more changes for 2.x. 
</comment><comment author="dadoonet" created="2015-09-06T06:58:30Z" id="138047971">That's fine I think. Thanks for fixing that!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a "conditional install" option to plugin for easier automation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13359</link><project id="" key="" /><description>I have been working with Docker quite a bit lately, and have found that there are some difficulties with using Elastic's commercial plugins within Docker.  Not any technical incompatibilities, per se.  But, here is the issue that comes up because of different operational assumptions.
1. The "[official Elasticsearch image](https://github.com/dockerfile/elasticsearch)" for Docker does not include plugins like Shield, Watcher, or Marvel.
2. Docker expects a single command to be run when a 
3.  You need to install these yourself, not a big deal.  
4. You can do this with a command like:

```
sh -c 'plugin -i elasticsearch/license/latest\
                &amp;&amp; plugin -i elasticsearch/shield/latest\
                &amp;&amp; plugin -i elasticsearch/marvel/latest\
                &amp;&amp; plugin -i elasticsearch/watcher/latest\
                &amp;&amp; elasticsearch'
```

Groovy, no problem.

**Except for The Problem**

The issue is that a Docker build will fail if a single command fails.  So, if your plugin command installs the plugins the first time it runs, the next time it runs it will fail, causing the entire build to fail.

What would be great is if there were a conditional installation option where I could try to install a plugin.  Something like:

`bin/plugin --install --conditional elasticsearch/shield/latest`

If the plugin were installed, or up-to-date then there wouldn't be any error thrown and we can continue on our merry way.  No error code returned, just a message to stdout that the plugin is already up-to-date.

BTW, I know that you _can_ create your own Dockerfile that does a lot of this for you with shell scripting logic.  However, the overwhelming majority of users are going to use the stock "official image" and don't want to build it themselves.  
</description><key id="104977645">13359</key><summary>Add a "conditional install" option to plugin for easier automation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">morgango</reporter><labels><label>:Plugins</label><label>discuss</label></labels><created>2015-09-04T22:02:23Z</created><updated>2017-03-15T21:27:43Z</updated><resolved>2017-03-15T21:27:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-05T21:49:11Z" id="138002399">I understand. 

Would it be possible to run something like the following instead? (not tested)

```
sh -c 'plugin -i elasticsearch/license/latest || true \
                &amp;&amp; plugin -i elasticsearch/shield/latest || true \
                &amp;&amp; plugin -i elasticsearch/marvel/latest || true \
                &amp;&amp; plugin -i elasticsearch/watcher/latest || true \
                &amp;&amp; elasticsearch'
```

Could this work in your case?
</comment><comment author="nik9000" created="2015-09-07T01:24:56Z" id="138151887">This seems like a fine thing to do. Can we just make plugin WARN if you try to install a plugin you already have?

As a workaround can you try to something like:

``` bash
sh -c '(find /usr/share/elasticsearch/plugins | grep license || plugin -i elasticsearch/license/latest)
  &amp; (find /usr/share/elasticsearch/plugins | grep shield || plugin -i elasticsearch/shield/latest) 
  &amp; (find /usr/share/elasticsearch/plugins | grep marvel || plugin -i elasticsearch/marvel/latest) 
  &amp; (find /usr/share/elasticsearch/plugins | grep watcher || plugin -i elasticsearch/watcher/latest)'
```
</comment><comment author="rjernst" created="2017-03-15T21:27:43Z" id="286884903">The plugin cli was reworked a while ago, and since 2.0 plugins must be built for a specific version of elasticsearch.  I don't think this issue is relevant anymore, as plugins must be removed and then reinstalled upon upgrading elasticsearch.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shade joda-convert</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/13358</link><project id="" key="" /><description>Closes #13356
</description><key id="104976335">13358</key><summary>Shade joda-convert</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasontedor</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.6.3</label><label>v1.7.2</label></labels><created>2015-09-04T21:51:27Z</created><updated>2015-09-14T09:24:59Z</updated><resolved>2015-09-05T12:48:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2015-09-05T01:09:01Z" id="137890646">LGTM 
</comment><comment author="jasontedor" created="2015-09-05T14:22:33Z" id="137960011">Thanks a lot for reviewing @dadoonet.
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>