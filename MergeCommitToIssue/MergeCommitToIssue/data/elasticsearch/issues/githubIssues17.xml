<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Fixed value count so it can be used in terms order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7051</link><project id="" key="" /><description>Closes #7050
</description><key id="38869769">7051</key><summary>Fixed value count so it can be used in terms order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T08:19:41Z</created><updated>2015-06-07T19:14:41Z</updated><resolved>2014-07-28T08:40:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-28T08:20:00Z" id="50310965">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Value Count Agg cannot be used for sort order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7050</link><project id="" key="" /><description>If a value count agg is used for the sort column of a terms aggregation an exception is thrown stating:

```
ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ElasticsearchIllegalArgumentException[Invalid order path [grades_count]. Missing value key in [grades_count] which refers to a multi-value metric aggregation]; 
```

Marvel commands to reproduce error: https://gist.github.com/colings86/4dfcb7de6c474ae69c32

Setting order to:

```
"order": {
    "grades_count": "desc"
}
```

produces the following error:

```
{
   "error": "ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: ClassCastException[org.elasticsearch.search.aggregations.metrics.valuecount.InternalValueCount cannot be cast to org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation$MultiValue]; ",
   "status": 503
}
```
</description><key id="38868421">7050</key><summary>Aggregations: Value Count Agg cannot be used for sort order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-28T07:56:01Z</created><updated>2014-07-29T08:48:23Z</updated><resolved>2014-07-28T08:40:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update span-not-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7049</link><project id="" key="" /><description>The example given does not clearly explain what the query does. span_not query is unituitive by nature therefore a more clearer example as given above will be better.
</description><key id="38848466">7049</key><summary>Update span-not-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">pachanta</reporter><labels /><created>2014-07-27T19:18:24Z</created><updated>2014-09-07T09:28:44Z</updated><resolved>2014-09-07T09:28:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-27T19:39:10Z" id="50283365">Hi @pachanta 

Thanks for the PR, but I'm afraid your example will not work unless the field is `not_analyzed`.  The `span` query family are term-level, and `la hoya` would be split into two terms by most analyzers.

Perhaps choose another term as an example?
</comment><comment author="pachanta" created="2014-07-27T20:26:18Z" id="50284678">Hi @clintongormley  I just verified that the above query returns a document with sentence "xyz hoya" but the document with a sentence "xyz la hoya" 
Thinking about it, you cannot use a span_not query with purely span_term queries. Because 2 span_term queries are always mutually exclusive. There is no overlap.
</comment><comment author="pachanta" created="2014-07-29T17:11:49Z" id="50507166">Hi @clintongormley  Pleas check the revised version. 
</comment><comment author="brusic" created="2014-07-30T00:22:11Z" id="50558310">I agree that the example could use some updating, but also that multi word terms are not the way to go.

Just a reminder that I implemented some of the latest features of the span not query and it includes additional tests (I haven't rebased in a while):

https://github.com/elasticsearch/elasticsearch/pull/4452

:) 
</comment><comment author="clintongormley" created="2014-07-30T10:13:49Z" id="50597016">@brusic i've never used the `span` queries - is the example given in this PR correct? Can I merge it?

Also, I've marked #4452 for review - sorry it has taken so long. We're making a concerted effort to work our way through the backlog atm.
</comment><comment author="brusic" created="2014-07-30T17:00:36Z" id="50646302">While I agree with @pachanta that it appears span not queries do not work directly with term queries, the Lucene tests cases disagree: :)

https://github.com/apache/lucene-solr/blob/3ad6ca696c1e40812b5696d9b8208e2fa77a573b/lucene/core/src/test/org/apache/lucene/search/spans/TestSpans.java#L542-L544

The other span query examples in the documentation use generic field/values. IMHO, if this example uses a less generic example, perhaps an example document and result will help. But yes, the example is technically correct. It is confusing, even the test cases in my PR are not ideal!
</comment><comment author="clintongormley" created="2014-07-31T07:58:33Z" id="50728248">Hi @pachanta 

Please could you sign our CLA so that I can get your PR merged in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-08-18T10:41:32Z" id="52476361">Hi @pachanta 

Any chance of signing the CLA? I can't merge this in without it.  http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="pachanta" created="2014-08-19T16:58:42Z" id="52664640">Hi @clintongormley 
I just did. 
</comment><comment author="clintongormley" created="2014-09-07T09:28:35Z" id="54741866">Thanks @pachanta 

Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `nested` aggregator should also resolve and use the parentFilter of the closest `reverse_nested` aggregator.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7048</link><project id="" key="" /><description>PR for #6994
</description><key id="38848193">7048</key><summary>The `nested` aggregator should also resolve and use the parentFilter of the closest `reverse_nested` aggregator.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>bug</label><label>v1.3.1</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-27T19:04:01Z</created><updated>2015-06-07T19:14:54Z</updated><resolved>2014-07-28T08:10:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-28T06:41:15Z" id="50304830">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API doesn't recognize plugin analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7047</link><project id="" key="" /><description>Issuing a GET on `http://localhost:9200/_analyze?analyzer=myanalyzer&amp;text=foo` provides the following response:

``` json
{
error: "ElasticsearchIllegalArgumentException[failed to find analyzer [myanalyzer]]",
status: 400
}
```

However, `myanalyzer` is definitely present (supplied via code as plugin). Elasticsearch also picked it up correctly as I was able to reference it from a mapping and perform indexing and searches correctly.
</description><key id="38844539">7047</key><summary>Analyze API doesn't recognize plugin analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>discuss</label></labels><created>2014-07-27T15:49:20Z</created><updated>2015-11-21T16:22:43Z</updated><resolved>2015-11-21T16:22:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-27T15:53:43Z" id="50277306">I think that the only work around is to create first an index and then apply _analyze API to this index.

With plugins, you can now register it as a default analyzer. Check how we did it in official analyzer plugins.

HTH

## 

David ;-)
Twitter : @dadoonet / @elasticsearchfr / @scrutmydocs

&gt; Le 27 juil. 2014 &#224; 17:49, Itamar Syn-Hershko notifications@github.com a &#233;crit :
&gt; 
&gt; Issuing a GET on http://localhost:9200/_analyze?analyzer=myanalyzer&amp;text=foo provides the following response:
&gt; 
&gt; {
&gt; error: "ElasticsearchIllegalArgumentException[failed to find analyzer [myanalyzer]]",
&gt; status: 400
&gt; }
&gt; However, myanalyzer is definitely present (supplied via code as plugin). Elasticsearch also picked it up correctly as I was able to reference it from a mapping and perform indexing and searches correctly.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="synhershko" created="2014-07-27T15:58:58Z" id="50277434">I can confirm executing this against an existing index (even without defining a default analyzer) works, also for plugin analyzers which aren't being used in this index via its mapping or _analyzer.

This is not an obvious gotcha. I suggest to either fix it so `_analyze` works when executed on the cluster the same like it was executed on an index, or the very least have a comment on that in doco...
</comment><comment author="clintongormley" created="2014-07-27T18:42:57Z" id="50281820">@synhershko do you register it as a global analyzer? 
</comment><comment author="synhershko" created="2014-07-27T19:47:42Z" id="50283604">@clintongormley register it where? it's being picked up from a jar file I installed via a plugin, and as such I expected it to just be available...
</comment><comment author="clintongormley" created="2014-07-27T20:28:38Z" id="50284744">@synhershko Well, I don't know what you are or are not doing.  Why not take a look at the kuromoji analyzer plugin and copy what it does?

https://github.com/elasticsearch/elasticsearch-analysis-kuromoji
</comment><comment author="synhershko" created="2014-07-27T20:54:27Z" id="50285449">@clintongormley aye, here is the relevant difference:

https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/blob/master/src/main/java/org/elasticsearch/indices/analysis/KuromojiIndicesAnalysis.java
https://github.com/elasticsearch/elasticsearch-analysis-kuromoji/commit/a8ad05143548fd3051d40d1b62325fcf7ac5b6bf

Question is why once it was registered with the `AnalysisModule` it's not enough to be used by the Analyze API?

This is how I register the analyzers:

https://github.com/synhershko/elasticsearch-analysis-hebrew/blob/master/src/main/java/com/code972/elasticsearch/plugins/AnalysisPlugin.java#L31
https://github.com/synhershko/elasticsearch-analysis-hebrew/blob/master/src/main/java/com/code972/elasticsearch/plugins/HebrewAnalysisBinderProcessor.java#L26
</comment><comment author="clintongormley" created="2014-07-27T21:12:52Z" id="50285943">Currently, it's just the way it is.  We have plans for making this easier though.
</comment><comment author="brusic" created="2014-07-30T06:43:27Z" id="50579373">I have definitely come across this issue before. From what I remember, the issue is that the code uses the analysis service to retrieve the analyzer and it in turn requires an index.

https://github.com/elasticsearch/elasticsearch/blob/dbcc4e9255435bd1c5ded6a2553449fb141f9249/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java#L137-L146
</comment><comment author="clintongormley" created="2015-11-21T16:22:43Z" id="158659795">Closing in favour of #8961
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to solve &#8220;failed to merge org.apache.lucene.store.AlreadyClosedException: this Directory is closed&#8221;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7046</link><project id="" key="" /><description>I saw a lot of error log and we need to fix this problem but we don't know solution please help us.

Example log

[2014-07-25 06:00:23,366][WARN ][cluster.action.shard ] [Host88] [instagram][26] sending failed shard for [instagram][26], node[sQEMBQGMT1eyDrPwF-xGeA], [P], s[STARTED], indexUUID [nG3g-7ZyQqC0a2usE05oWw], reason [engine failure, message [MergeException[java.io.EOFException: read past EOF: MMapIndexInput(path="/var/lib/elasticsearch/zocialinc/nodes/0/indices/instagram/26/index/_tsw3_es090_0.pos")]; nested: EOFException[read past EOF: MMapIndexInput(path="/var/lib/elasticsearch/zocialinc/nodes/0/indices/instagram/26/index/_tsw3_es090_0.pos")]; ]]

[2014-07-25 06:00:23,367][WARN ][index.merge.scheduler ] [Host88] [instagram][26] failed to merge org.apache.lucene.store.AlreadyClosedException: this Directory is closed
</description><key id="38837559">7046</key><summary>How to solve &#8220;failed to merge org.apache.lucene.store.AlreadyClosedException: this Directory is closed&#8221;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thangman22</reporter><labels><label>feedback_needed</label></labels><created>2014-07-27T11:55:40Z</created><updated>2014-07-29T12:37:40Z</updated><resolved>2014-07-29T12:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-27T11:59:14Z" id="50262547">Hi @thangman22 

Which version of Elasticsearch is this?  Those errors look like you have a corrupted index. Did you run out of disk space at some time, or have an OOM exception in the past?
</comment><comment author="thangman22" created="2014-07-27T12:02:20Z" id="50262626">Yep long time ago is runout disk space for elastic search this node is 1.2.2 
</comment><comment author="clintongormley" created="2014-07-27T12:05:37Z" id="50262691">It sounds like you're going to have to reindex your data.  Hopefully you have a snapshot from before the time that your disk filled up.

/cc @s1monw - just in case you have any other ideas.
</comment><comment author="thangman22" created="2014-07-27T12:08:17Z" id="50262740">What the best way to re-index?
</comment><comment author="clintongormley" created="2014-07-27T12:12:37Z" id="50262841">Do you have your data somewhere outside Elasticsearch?  If so, delete the index and reindex from the original data source.

If you are running with replicas turned on, you may be lucky and find that you have another copy of the same shard that isn't corrupt.  You may be able to just restart the bad node, and your index will recover from a different shard copy...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make indices.memory.index_buffer_size dynamically updateable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7045</link><project id="" key="" /><description>Elasticsearch updates this internally, when an index becomes idle, or when active shards change but we don't let the user change it today, I think.
</description><key id="38773122">7045</key><summary>Make indices.memory.index_buffer_size dynamically updateable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-07-25T21:42:38Z</created><updated>2015-12-17T03:58:31Z</updated><resolved>2015-08-10T22:16:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-09-08T12:23:51Z" id="54809954">I moved this to 1.5.0 ... the test still intermittently fails and I can't figure out why.
</comment><comment author="mikemccand" created="2015-08-10T22:16:01Z" id="129631661">I don't think we should make this setting dynamically updatable: this isn't a setting you should need to dynamically update, and I think it's risky to make it possible...
</comment><comment author="makeyang" created="2015-12-17T03:58:31Z" id="165331248">so there will be no this feature?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch can't resolve environment variables in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7044</link><project id="" key="" /><description>I have the following two settings in my elasticsearch.yml file. They are the only ones that pull from environment variables.

```
cloud.aws.access_key: ${AWS_ACCESS_KEY_ID}
cloud.aws.secret_key: ${AWS_SECRET_KEY}
```

When I restart elasticsearch to load these from the environment, I get an error that it can't resolve them. I've tested it and it will not resolve either, so this error applies to both (it just fails on the bottom one first)

```
- IllegalArgumentException[Could not resolve placeholder 'AWS_SECRET_KEY']
  java.lang.IllegalArgumentException: Could not resolve placeholder 'AWS_SECRET_KEY'
  at org.elasticsearch.common.property.PropertyPlaceholder.parseStringValue(PropertyPlaceholder.java:124)
  at org.elasticsearch.common.property.PropertyPlaceholder.replacePlaceholders(PropertyPlaceholder.java:81)
  at org.elasticsearch.common.settings.ImmutableSettings$Builder.replacePropertyPlaceholders(ImmutableSettings.java:1060)
  at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareSettings(InternalSettingsPreparer.java:101)
  at org.elasticsearch.bootstrap.Bootstrap.initialSettings(Bootstrap.java:106)
  at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:177)
  at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)

```

I did some investigating through elasticsearch's code repository on github and discovered this bit of code that pulls from the environment variables.

[`ImmutableSettings.java#resolvePlaceholder` from elasticsearch@github](https://github.com/elasticsearch/elasticsearch/blob/04b412b5972a08a79989d82cc9d17842fe3c3e5a/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java#L1033-1047)

Namely. the lines inside that function that _should_ be pulling from the environment variables are these one:

[Code from `resolvePlaceholder` that pulls out environment variables](https://github.com/elasticsearch/elasticsearch/blob/04b412b5972a08a79989d82cc9d17842fe3c3e5a/src/main/java/org/elasticsearch/common/settings/ImmutableSettings.java#L1042-1045)

However, after `resolvePlaceholder` is run from inside function [`PropertyPlaceholder#parseStringValue`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/property/PropertyPlaceholder.java#L107-125), the `System.getenv` call must be returning null as that is the only way for that error to be thrown.

I wrote a simple test program that is essentially a copy of `ImmutableSettings.java#resolvePlaceholder` to test that  `System.getenv` was pulling out the environment variables correctly on my system. This in fact returns the values I expect.

```
public class TestingPlaceholders {
  public static void main(String[] args) {
    System.out.println(resolvePlaceholder(args[0]));
  }
  public static String resolvePlaceholder(String placeholderName) {
    if (placeholderName.startsWith("env.")) {
      // explicit env var prefix
      System.out.println("1: placeholderName.startsWith(\"env.\")");
      return System.getenv(placeholderName.substring("env.".length()));
    }
    String value = System.getProperty(placeholderName);
    if (value != null) {
      System.out.println("2: System.getProperty");
      return value;
    }
    value = System.getenv(placeholderName);
    if (value != null) {
      System.out.println("3: System.getenv");
      return value;
    }
    return "Map should've had it";
  }
}
```

When run, this is the output, showing we are getting the set environment variables (keys hidden for obvious reasons):

```
[ec2-user@ip-172-31-34-195 ~]$ java TestingPlaceholders AWS_SECRET_KEY
3: System.getenv
XXXXXXXXXXXXXXXXXX
[ec2-user@ip-172-31-34-195 ~]$ java TestingPlaceholders AWS_ACCESS_KEY_ID
3: System.getenv
XXXXXXXXXXXXXXXXXX
```

What is it about elasticsearch that isn't able to parse my environment variables from `elasticsearch.yml`? I've done quite a bit of digging at this point but I'm sure there is a simple solution around the corner. Any help would be very much appreciated.
</description><key id="38767754">7044</key><summary>elasticsearch can't resolve environment variables in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pgibler</reporter><labels /><created>2014-07-25T20:35:27Z</created><updated>2014-07-26T03:12:55Z</updated><resolved>2014-07-26T03:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pgibler" created="2014-07-26T03:12:55Z" id="50221746">I figured out the issue.

As I am running elasticsearch as a linux service, rather than a shell application, it has access to no environment variables except for a very select few.

I added the following line to the end of `/etc/sysconfig/elasticsearch` to load the environment variables I wanted available to the program:

`. /path/to/environment/variables`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Ambiguous resolution of `_doc.score`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7043</link><project id="" key="" /><description>Some scripting languages don't make a difference between property access `foo.bar` and map lookups `foo['bar']`. This is an issue for the `DocLookup` class (which is the class behind the `_doc` variable and implements `java.util.Map`) since there are two ways that `_doc.score` can be resolved:
1. `DocLookup.getScore()`
2. `DocLookup.get("score")`.

The interesting thing is that mvel translates `_doc.score` to `getScore` while Groovy seems to translate it to `get("score")`. So when trying to lookup the score in Groovy, you might see the following error:

```
GroovyScriptExecutionException[ElasticsearchIllegalArgumentException[No field found for [score] in mapping with types [index_name]].
```

For reference, if you encounter this issue, you can work around it by using `_score` instead of `_doc.score`. So maybe the way to go would be to deprecate `_doc.score` and make scripting engines expose the score via `_score` instead. I tend to find it cleaner as well since the score is not really a property of the document (it also depends on the query).
</description><key id="38766364">7043</key><summary>Scripting: Ambiguous resolution of `_doc.score`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-25T20:18:21Z</created><updated>2015-10-09T12:46:48Z</updated><resolved>2014-09-11T10:52:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-25T20:19:25Z" id="50199234">+1 on using `_score` instead of `_doc.score`!
</comment><comment author="jpountz" created="2014-07-25T20:23:07Z" id="50199661">For reference, here is a confirmation that in case of a conflict Groovy prefers the map access: http://groovy.329449.n5.nabble.com/Property-resolution-clash-for-Map-subclasses-td5717713.html
</comment><comment author="dakrone" created="2014-09-10T08:08:01Z" id="55083586">@jpountz I looked and we already support `_score` in Groovy scripts (there's a test for it too). So I think all that needs to be done for this is to add documentation about `_score` being the preferred method instead of `_doc.score`. Does that sound good to you?
</comment><comment author="jpountz" created="2014-09-11T09:36:37Z" id="55241408">+1
</comment><comment author="stavrosicsd" created="2015-10-09T12:46:48Z" id="146858265">Hello to everyone,

I just upgraded elasticsearch from 1.3.4 to 1.4.4 and I am getting the aforementioned error:
org.elasticsearch.script.groovy.GroovyScriptExecutionException: ElasticsearchIllegalArgumentException[No field found for [score] in mapping with types [type_name]]

I have read the initial workaround but I did not understand unfortunately how to proceed.
Any assistance would be very helpful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Discovery] add cluster name and cluster state version to fault detection pinging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7042</link><project id="" key="" /><description>Add the cluster name and cluster state version to NodesFaultDetection pings. This allows resolving the case where a old master node become unresponsive and later wakes up and pings all the nodes in the cluster, allowing the newly elected master to decide whether it should step down or ask the old master to rejoin.

Add the cluster name to MasterFaultDetection ping request, to verify that the master we're pinging is of the same cluster. This is useful where multiple test clusters run on the same network and adds another protection layer.

To simulate situations where this is helpful, a new test is added utilising a new  ClusterSchemeDisruption to long GCs on a cluster's master node.

Note: this PR is against the improve zen brunch.
Note2: DiscoveryWithNetworkFailuresTests class is renamed to DiscoveryWithServiceDisruptions and marks it as Slow
</description><key id="38763311">7042</key><summary>[Discovery] add cluster name and cluster state version to fault detection pinging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>resiliency</label></labels><created>2014-07-25T19:38:03Z</created><updated>2014-08-11T08:50:42Z</updated><resolved>2014-08-05T12:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-05T12:57:05Z" id="51193195">Implemented via: #7063, #7082 and #7110
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count API: exploit terminate_after setting for performing minimal shard requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7041</link><project id="" key="" /><description>Currently the Count API broadcasts requests to all the shards, even when terminate_after setting has been set. It would be ideal to terminate ongoing shard requests, when the count == terminate_after. This would make the API a bit faster when there is a large number of shards to be queried but the terminate_after setting is relatively a small number.
</description><key id="38762751">7041</key><summary>Count API: exploit terminate_after setting for performing minimal shard requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2014-07-25T19:31:00Z</created><updated>2017-06-20T08:55:23Z</updated><resolved>2017-06-20T08:55:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2017-06-16T16:53:02Z" id="309077769">given that the count API doesn't exist anymore, I think that this issue is no longer valid. Do you agree @colings86 ?</comment><comment author="colings86" created="2017-06-20T08:55:22Z" id="309689226">@javanna I agree</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Testing plugins is slower and more difficult with 1.3</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7040</link><project id="" key="" /><description>Testing plugins requires using a suite scope cluster and overriding some settings.  The settings are no problem - it'd be nice to have a abstract test you can inherit from to make these kinds of things transparent but that isn't a big deal.  It'd be nice to be able to reuse the global cluster and still load plugins though.

The diff required to run plugin integration tests for posterity:
https://gist.github.com/nik9000/a6740da83afc840b23ff
</description><key id="38742307">7040</key><summary>Testing plugins is slower and more difficult with 1.3</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-07-25T15:29:58Z</created><updated>2014-08-27T15:48:07Z</updated><resolved>2014-08-27T15:48:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-25T17:13:45Z" id="50177986">Agreed. You can't have a global scope when testing plugins.
That said, putting all test methods in the same integration test class could be a workaround.

This is actually what I'm doing for plugins now. For example, Twitter river here: https://github.com/elasticsearch/elasticsearch-river-twitter/commit/fc91db2e56e507f16cda738b8beb1fe43189dff3
</comment><comment author="javanna" created="2014-08-27T15:48:07Z" id="53594280">Closing, as #7482 makes it possible to override global cluster settings by modifying `InternalTestCluster.DEFAULT_SETTINGS_SOURCE` from a static block in your tests. No need for SUITE scope if all of your tests share the same settings (e.g. load a plugin).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `ignore_unmapped` work for sorting cross-index queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7039</link><project id="" key="" /><description>Close #2255
</description><key id="38739816">7039</key><summary>Make `ignore_unmapped` work for sorting cross-index queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Search</label><label>bug</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-25T15:04:03Z</created><updated>2015-06-07T19:15:11Z</updated><resolved>2014-08-01T13:31:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-07-29T17:25:38Z" id="50508979">`ignore_umapped` still sounds like something that should be a boolean value.
Would `unmapped_type` be clearer?
</comment><comment author="jpountz" created="2014-07-30T14:46:34Z" id="50624561">@areek @markharwood  @kimchy Pushed new commits to address your concerns.
</comment><comment author="kimchy" created="2014-08-01T12:15:08Z" id="50877464">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unable to re-nest on same field after reverse nesting </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7038</link><project id="" key="" /><description>I'm having trouble performing a certain aggregation on a nested field.

I have a field of nested type, that holds an array of object. What I want to do is this:

Aggregate on the field to find all the nested documents that contain a certain value, then reverse-nest back to the root, giving me the entire set of root documents which have the nested field object I'm looking for.

Then, further aggregate on that set of documents, against the same nested field, this time against a different value. 

The purpose is to build something like a funnel, where I keep narrowing down the aggregation with more and more criteria, based on that nested field.

What appears to happen though is that when perform my second nested agg, itself with a nested-&gt;reverse_nested agg, it looks as though it can't see any of nested fields anymore. If you run this simple CURL, looks at the value of 'into_nested_again'. It's 0, but it should be 3, just like the value of the original 'into_nested' agg. 

```
curl -XDELETE "http://localhost:9200/nested-test"

curl -XPUT "http://localhost:9200/nested-test" -d'
{
  "mappings": {
    "my_type": {
      "properties": {
        "my_nested_field": {
          "type": "nested"
       }
      } 

    }
  }
}'

curl -XPOST "http://localhost:9200/nested-test/my_type" -d'
{
  "my_nested_field" : [
    {
      "my_key": "value1"
    }
  ]
}'

curl -XPOST "http://localhost:9200/nested-test/my_type" -d'
{
  "my_nested_field" : [
    {
      "my_key": "value1"
    },
    {
      "my_key": "value2"
    }
  ]
}'

curl -XGET "http://localhost:9200/nested-test/_search" -d'
{
  "aggs": {
    "into_nested": {
      "nested": {
        "path": "my_nested_field"
      },
      "aggs": {
        "by_value1": {
          "filter": {
            "term": {
              "my_nested_field.my_key": "value1"
            }
          },
          "aggs": {
            "by_parent": {
              "reverse_nested": {}
              , "aggs": {
                "into_nested_again": {
                  "nested": {
                    "path": "my_nested_field"
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}'
```
</description><key id="38738092">7038</key><summary>unable to re-nest on same field after reverse nesting </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Kallin</reporter><labels><label>bug</label></labels><created>2014-07-25T14:47:45Z</created><updated>2015-01-06T21:06:40Z</updated><resolved>2015-01-06T21:06:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Kallin" created="2014-07-25T15:49:14Z" id="50167647">I may have been approaching this wrong. I can get the aggregates I want by starting with filter aggs instead of nested aggs like so:

```
curl -XGET "http://localhost:9200/nested-test/_search" -d'
{
  "aggs": {
    "by_value1": {
      "filter": {
        "nested": {
          "path": "my_nested_field",
          "filter": {
            "term": {
              "my_nested_field.my_key": "value1"
            }
          }
        }
      },
        "aggs": {
          "by_value2": {
            "filter": {
              "nested": {
                "path": "my_nested_field",
                "filter": {
                  "term": {
                    "my_nested_field.my_key": "value2"
                  }
                }
              }
            }
        }
      }
    }
  }
}'
```

However, the issue I described still seems like a bug, though perhaps one that can be worked around.
</comment><comment author="clintongormley" created="2014-10-31T10:28:17Z" id="61242439">@martijnvg could you look at this one please
</comment><comment author="martijnvg" created="2015-01-06T21:06:40Z" id="68933658">@Kallin Running your example on version 1.4.2 does produce the results you expect. (into_nested_again has doc_count 3) . 

I don't know what version you were using back then, but if the problem still occurs then please re-open this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced the notion of a FixedBitSetFilter that guarantees to produce a FixedBitSet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7037</link><project id="" key="" /><description>Nested and parent/child rely on the fact that type filters produce a FixedBitSet, the RandomAccessFilter does this. By moving away from filter cache this filters will also never be evicted because of LRU reasons, which is what is desired for nested and parent/child.

Also if nested and parent/child is configured the type filters are eagerly loaded by default.

PR for #7031
</description><key id="38735102">7037</key><summary>Introduced the notion of a FixedBitSetFilter that guarantees to produce a FixedBitSet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-25T14:20:58Z</created><updated>2015-06-07T12:34:21Z</updated><resolved>2014-08-27T19:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-28T07:29:14Z" id="50307601">Left some comments, but I really like the idea of having a different caching infrastructure for these filters with eager loading, no evictions and the warranty to get fixed bit sets!
</comment><comment author="martijnvg" created="2014-07-29T15:40:42Z" id="50494278">@jpountz I've updated the PR and addressed your comments. In addition I moved the FixedBitSetCache to the index.cache package and add `fixed_bit_set_memory_in_bytes` statistic to segments stats.
</comment><comment author="jpountz" created="2014-08-01T08:11:06Z" id="50859859">@martijnvg Left a couple more comments. I think it would be nice to also check the size of this cache after integration tests finish to make sure it gets cleaned when indices are closed?
</comment><comment author="martijnvg" created="2014-08-04T10:29:16Z" id="51043802">@jpountz Thanks for looking into this. I update the PR to address your comments.
</comment><comment author="jpountz" created="2014-08-06T12:31:52Z" id="51327875">LGTM

Maybe @kimchy should look at the caching logic before getting this in?
</comment><comment author="kimchy" created="2014-08-22T14:57:40Z" id="53070751">LGTM, I would add some docs to the FixedBitSet cache that its intentionally not bounded (by size or time), and that it should be used very carefully only for components that require a FixedBitSet that is always there.
</comment><comment author="jpountz" created="2014-08-26T17:01:06Z" id="53453600">@martijnvg I think it would be nice to merge this one as it would help clean up a couple of other nested and parent/child related changes?
</comment><comment author="martijnvg" created="2014-08-26T19:50:08Z" id="53478121">@jpountz I totally agree and I'll merge it in this week before I work on any other major PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add new `default` option for timestamp field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7036</link><project id="" key="" /><description>Index process fails when having `_timestamp` enabled and `path` option is set.
It fails with a `TimestampParsingException[failed to parse timestamp [null]]` message.

Reproduction:

```
DELETE test
PUT  test
{
    "mappings": {
        "test": {
            "_timestamp" : {
                "enabled" : "yes",
                "path" : "post_date"
            }
        }
    }
}
PUT test/test/1
{
  "foo": "bar"
}
```

You can now define a default value for when timestamp is not provided
within the index request or in the `_source` document.

By default, the default value is `now` which means the date the document was processed by the indexing chain.

You can disable that default value by setting `default` to `null`. It means that `timestamp` is mandatory:

```
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "default" : null
        }
    }
}
```

If you don't provide any timestamp value, indexation will fail.

You can also set the default value to any date respecting timestamp format:

```
{
    "tweet" : {
        "_timestamp" : {
            "enabled" : true,
            "format" : "YYYY-MM-dd",
            "default" : "1970-01-01"
        }
    }
}
```

If you don't provide any timestamp value, indexation will fail.

Closes #4718.
</description><key id="38729143">7036</key><summary>Add new `default` option for timestamp field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Mapping</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-25T13:21:51Z</created><updated>2015-06-06T18:27:52Z</updated><resolved>2014-07-31T18:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-28T06:45:20Z" id="50305075">Should the default be `null` rather than `now` when a path is specified? I don't like the fact that a malformed document would get the current time as a timestamp instead of raising an error?
</comment><comment author="dadoonet" created="2014-07-28T14:04:46Z" id="50342116">About default value it was more about not to break bwc. Before this change, when you activate `_timestamp` in mapping, if you don't set any timestamp in the request, "now" is applied by default.

I think we should keep it as it was previously.
</comment><comment author="jpountz" created="2014-07-29T16:24:09Z" id="50500549">@dadoonet Left one comment about the null case
</comment><comment author="dadoonet" created="2014-07-29T17:39:25Z" id="50510910">@jpountz PR updated. Let me know
</comment><comment author="jpountz" created="2014-07-30T14:59:41Z" id="50626590">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic Template Mapping Fails After Upgrade </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7035</link><project id="" key="" /><description>Hello, After upgrading from 0.90.1 to 1.2.2 and now 1.3.0 my dynamic templates are no longer working for my mapping.
## Setup index

```
curl -XPUT "http://localhost:9200/myindex" -d'
{
   "mappings": {
      "_default_": {
         "dynamic_templates": [
            {
               "date_string": {
                  "match": "*_s",
                  "mapping": {
                     "type": "string"
                  }
               }
            }
         ]
      }
   }
}'
```
## Add a test document

```
curl -XPUT "http://localhost:9200/myindex/test_type" -d'
{
    "updated_dt" : "20140501",
    "updated_dt_s" : "20140501"
}'
```
## Check the mapping

```
curl -XGET "http://localhost:9200/myindex/_mapping?pretty=true"
{
  "myindex" : {
    "mappings" : {
      "test_type" : {
        "properties" : {
          "updated_dt" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "updated_dt_s" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
        }
      }
    }
  }
}
```

Notice the mapping for entering the data as a string has been ignored. According to the dynamic map entered before, the "updated_dt_s" field should be set to a string. 

Did the syntax for dynamic templates mapping get changed in versions &gt; 1?
</description><key id="38725114">7035</key><summary>Dynamic Template Mapping Fails After Upgrade </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skystebnicki</reporter><labels><label>feedback_needed</label></labels><created>2014-07-25T12:35:08Z</created><updated>2014-07-25T13:49:01Z</updated><resolved>2014-07-25T13:43:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T12:48:48Z" id="50144102">Hi @skystebnicki 

You seem to have something else going on there.  Two weird things:
1. your `20140501` strings are not recognisable dates
2. your mapping doesn't include the `_default_` mapping that you specified at the beginning of your recreation

Here is my recreation:

```
DELETE /_all

PUT /myindex
{
   "mappings": {
      "_default_": {
         "dynamic_templates": [
            {
               "date_string": {
                  "match": "*_s",
                  "mapping": {
                     "type": "string"
                  }
               }
            }
         ]
      }
   }
}

POST /myindex/test_type
{
    "updated_dt" : "2014/05/01",
    "updated_dt_s" : "2014/05/01"
}

GET /myindex/_mapping
```

And the mapping looks like this:

```
{
   "myindex": {
      "mappings": {
         "_default_": {
            "dynamic_templates": [
               {
                  "date_string": {
                     "mapping": {
                        "type": "string"
                     },
                     "match": "*_s"
                  }
               }
            ],
            "properties": {}
         },
         "test_type": {
            "dynamic_templates": [
               {
                  "date_string": {
                     "mapping": {
                        "type": "string"
                     },
                     "match": "*_s"
                  }
               }
            ],
            "properties": {
               "updated_dt": {
                  "type": "date",
                  "format": "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd"
               },
               "updated_dt_s": {
                  "type": "string"
               }
            }
         }
      }
   }
}
```
</comment><comment author="skystebnicki" created="2014-07-25T13:43:58Z" id="50150634">@clintongormley Thanks so much for the help! It got me testing further and it was a problem with my API calls, the dynamic mapping was not being sent right and no error was being returned. Your help is much appreciated!!!
</comment><comment author="clintongormley" created="2014-07-25T13:49:01Z" id="50151241">No problem :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Add a future 1.5.0 Version constant</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7034</link><project id="" key="" /><description>I'm not sure if this is something we want to do, but I think it makes it easier to plan PRs. I wanted to submit this and see if people had thoughts about it.

Since our git workflow includes not rebasing a branch once a PR has been submitted (rightly so!), this makes it possible to submit PRs during the 1.4.0 development cycle that target 1.5.0, without this all PRs have to target 2.0.0 and then fix-up their version checks once 1.4.0 is actually released since the new `Version` is not added until then.
</description><key id="38724670">7034</key><summary>Internal: Add a future 1.5.0 Version constant</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label></labels><created>2014-07-25T12:28:50Z</created><updated>2014-09-09T13:53:30Z</updated><resolved>2014-07-29T14:20:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-25T12:34:52Z" id="50142868">&gt; Since our git workflow includes not rebasing a branch once a PR has been submitted (rightly so!),

Can someone document that workflow on the contributing page?  On the page now it says:

&gt; 1. Rebase your changes
&gt; 
&gt; Update your local repository with the most recent code from the main Elasticsearch repository, and rebase your branch on top of the latest master branch. We prefer your initial changes to be squashed into a single commit. Later, if we ask you to make changes, add them as separate commits. This makes them easier to review. As a final step before merging we will either ask you to squash all commits yourself or we&#8217;ll do it for you.

When you say you don't rebase it feels like you mean that final rebase/squash step.
</comment><comment author="dakrone" created="2014-07-25T12:40:16Z" id="50143353">&gt; Can someone document that workflow on the contributing page? On the page now it says:

The contributing doc is still accurate, contributors should rebase before submitting a PR (see below)

&gt; When you say you don't rebase it feels like you mean that final rebase/squash step.

Yea, in CONTRIBUTING.md the rebase is before submitting the PR, which everyone should definitely do. Once the PR is submitted however, a contributor shouldn't force-push (squash, amend, or rebase) until either someone asks for it (ie, "you changes look good, can you squash to a single commit?") or _right_ before merging it.

I'm talking about that interim when the PR is still being discussed, so it shouldn't be force-pushed because it destroys review comments/feedback, after it's out in the open, but before the final squash/merge.
</comment><comment author="nik9000" created="2014-07-25T12:47:52Z" id="50143999">Cool.  I imagine its OK to push a new commit that switches the feature to a
new version.  But it'd be more convenient if you knew that it wouldn't make
it in the next version to initially target it at version after that.  When
I need the Version constant I typically just go with the highest one
defined in the 1.X branch.  Maybe add the future version constant when you
want new features to target the future release?

On Fri, Jul 25, 2014 at 8:40 AM, Lee Hinman notifications@github.com
wrote:

&gt; Can someone document that workflow on the contributing page? On the page
&gt; now it says:
&gt; 
&gt; The contributing doc is still accurate, contributors should rebase before
&gt; submitting a PR (see below)
&gt; 
&gt; When you say you don't rebase it feels like you mean that final
&gt; rebase/squash step.
&gt; 
&gt; Yea, in CONTRIBUTING.md the rebase is before submitting the PR, which
&gt; everyone should definitely do. Once the PR is submitted however, a
&gt; contributor shouldn't force-push (squash or rebase) until either someone
&gt; asks for it (ie, "you changes look good, can you squash to a single
&gt; commit?") or _right_ before merging it.
&gt; 
&gt; I'm talking about that interim when the PR is still being discussed, so it
&gt; shouldn't be force-pushed because it destroys review comments/feedback,
&gt; after it's out in the open, but before the final squash/merge.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7034#issuecomment-50143353
&gt; .
</comment><comment author="dakrone" created="2014-07-25T12:55:25Z" id="50144796">&gt;  I imagine its OK to push a new commit that switches the feature to a new version.

The issue is that you can't do this without rebasing. An example:
- I create a branch for my feature after 1.2.0 but before 1.3.0
- I need to check version compatibility somewhere in the code, I use the 1.3.0 Version constant
- I submit a PR for this feature
- My PR is not going to make it to 1.3.0, so I add a commit to change it to 2.0.0 because that's the only other constant that's available
- 1.3.0 is released, the 1.4.0 `Version` constant is now created
- I can't change my PR to use the newly created 1.4.0 constant since by branch is based on an older branch. In order to use it, I'd need to rebase and push, destroying history.

By creating the `Version` constant early, I can just add a commit to the PR bumping it from 1.3.0 to 1.4.0 without a force-push.

&gt; Maybe add the future version constant when you want new features to target the future release?

That's definitely an option, however I do think it's easier to preemptively add the constant than have multiple PRs all add the same `Version` constant because they need it (less merge conflicts this way).
</comment><comment author="nik9000" created="2014-07-25T13:17:30Z" id="50146874">&gt; I create a branch for my feature after 1.2.0 but before 1.3.0
&gt; I need to check version compatibility somewhere in the code, I use the 1.3.0 Version constant
&gt; I submit a PR for this feature
&gt; My PR is not going to make it to 1.3.0, so I add a commit to change it to 2.0.0 because that's the only other constant that's available
&gt; 1.3.0 is released, the 1.4.0 Version constant is now created
&gt; I can't change my PR to use the newly created 1.4.0 constant since by branch is based on an older branch. In order to use it, I'd need to rebase and push, destroying history.

Ah!  I see now.  This is where no changing history on the pull requests becomes a pain.  You could merge master back to your pull request branch but that'd even more painful.  Maybe just tugging in the new Version constant but that is still annoying.  I suppose the right thing to do is create the version constants preemptively.  You are right.

&lt;rant&gt;
I have to admit that I'm not a fan of the way github handles pull requests compared to gerrit and have told them so every chance I get to meet them in person.  They smile and nod because they won the popularity contest.  The nice thing about the gerrit process is that you amend the commit and push it clean every time and you keep history.  The bad thing is that git wasn't built for it so you need a plugin which is a barrier to entry.....
&lt;/rant&gt;
</comment><comment author="clintongormley" created="2014-07-25T13:31:08Z" id="50149152">&gt; I can't change my PR to use the newly created 1.4.0 constant since by branch is based on an older branch. In order to use it, I'd need to rebase and push, destroying history.

Not sure if I follow the above, but you can rebase, preserving all of your commits, and push, without squashing.  You're going to have to rebase before merging anyway, so not sure I see the problem.
</comment><comment author="nik9000" created="2014-07-25T14:55:55Z" id="50160866">If you don't want to allow rebasing the pull request against master during
the comment phase then you're stuck with what was in master when the pull
request was opened.

On Fri, Jul 25, 2014 at 9:31 AM, Clinton Gormley notifications@github.com
wrote:

&gt;  I can't change my PR to use the newly created 1.4.0 constant since by
&gt; branch is based on an older branch. In order to use it, I'd need to rebase
&gt; and push, destroying history.
&gt; 
&gt; Not sure if I follow the above, but you can rebase, preserving all of your
&gt; commits, and push, without squashing. You're going to have to rebase before
&gt; merging anyway, so not sure I see the problem.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7034#issuecomment-50149152
&gt; .
</comment><comment author="s1monw" created="2014-07-29T14:13:39Z" id="50480866">Regarding the rebasing I think it should be communiated with the reviewers before it's done other than that i think that is just fine at some point. It's just easier to track changes etc.

Regarding the future version, @dakrone IMO this should happen in the PR itself - I think it's perfectly find to add it in a PR even though it already exists in master but you didn't rebase yet.
</comment><comment author="dakrone" created="2014-07-29T14:20:31Z" id="50481946">@s1monw sounds good, closing this then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update search-template.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7033</link><project id="" key="" /><description>hello,

just need to remove extra commas in template query ;-)

great job with the 1.3 version!

regards,
chris.
</description><key id="38723843">7033</key><summary>Update search-template.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">ccodina</reporter><labels /><created>2014-07-25T12:17:28Z</created><updated>2014-08-18T10:35:41Z</updated><resolved>2014-08-18T10:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T12:21:11Z" id="50141667">Hi @ccodina 

Thanks for the fix! Please could I ask you to sign the CLA so that we can merge it in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-08-18T10:34:24Z" id="52475745">CLA not signed - treating as bug report.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enforce non-null settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7032</link><project id="" key="" /><description>Now that we are using the index created version to make index-time decisions,
assuming that the version is the current version when settings are null is
very error-prone. Instead we should ensure that settings are always non-null
and contain the version when the index was created.
</description><key id="38716621">7032</key><summary>Enforce non-null settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-25T10:22:31Z</created><updated>2015-06-07T12:34:30Z</updated><resolved>2014-07-25T19:03:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-25T17:22:55Z" id="50179057">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested and parent child filters should live outside of filter cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7031</link><project id="" key="" /><description>Query and filters that rely on filters to be cached and have random access (FixedBitSet) to run fast should be kept around **outside** of the filter cache. The filter cache may evict these filters and nested and parent/child need then to be cached in order to be fast.

There should be separate service that keeps just FixedBitSet instance around. 
</description><key id="38711925">7031</key><summary>Nested and parent child filters should live outside of filter cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-07-25T09:18:08Z</created><updated>2014-08-27T19:29:40Z</updated><resolved>2014-08-27T19:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support RFC 6902 style PATCH updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7030</link><project id="" key="" /><description>It would be nice to have a "patch language" that can be used to update documents, or in the `transform` API without using scripting.
</description><key id="38710427">7030</key><summary>Support RFC 6902 style PATCH updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-07-25T08:58:05Z</created><updated>2015-11-21T16:20:28Z</updated><resolved>2015-11-21T16:20:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-08-01T10:33:44Z" id="50870675">If we do choose to implement this patch RFC an ASL-licensed impl is here: https://github.com/fge/json-patch 
However, do we know how useful this approach will be in practice?
The concerns I have are:

1) Limited addressability: the [path syntax](http://tools.ietf.org/html/rfc6901) used in commands only appears to support addressing by field name and not value e.g. I can't reference the particular `car` object in an array of my cars that has the `"registrationPlate":"GD32 1FU"` to set the `insuredDate`. 
2) Limited operations - setting values are allowed but incrementing existing values is not
3) Limited data structures - simple arrays as opposed to maps or priority queues.

Uodates are central to the use case of "entity-centric" indexes where summaries are maintained for entities as they accumulate a number of events over time e.g. customers and their purchases or web sessions and their log entries.  In these scenarios the curation of the profiles have several requirements:
1) Derived attributes - the `duration` of a web session is the difference between the min and max timestamps of multiple log events attached over time.
2) Ageing windows - arrays that keep the last N entries rather than accruing endless entries.
3) Counters - e.g. simple incrementers or more sophisticated count-distinct structures like HyperLogLog for counting the number of other bank accounts an account has transacted with.
4) Maps - arrays where the object elements are addressable by a key held in an a field value

While these more complex requirements can be solved using stored scripts today I can appreciate the motivation to move away from a reliance on script and so we could potentially consider a declarative approach e.g. where our _mapping_ definitions are extended to try to encapsulate some of these common higher-level data types that are maintained over time with additions?
</comment><comment author="clintongormley" created="2014-08-22T07:25:13Z" id="53030920">Also see #5291
</comment><comment author="iohannis" created="2015-03-19T09:07:41Z" id="83431424">Also looking for a solution to this, having an index with rental properties availability data (as nested objects part of the property) that we update multiple times daily. Question is how we replace the array of nested objects with new ones, so the field values are not simply merged (keeping obsolete availability dates for the properties)? 

I have been working just about a month with implementing ElasticSearch, and just now ran into this issue. I cannot find anything in the documentation, and this and the linked issues are the closest to what I am looking for. Any suggestion would be very welcome - even if it's a workaround.
</comment><comment author="vsiv" created="2015-04-08T03:42:32Z" id="90795012">@clintongormley a bit curious why scripting isn't a better fit for advanced patching/merging needs? this would help keep the update api simpler, this would allow u guys to move forward with the simpler use cases folks want - ex #7332 and #5291... Just a thought. I do think however a patch language would be cool..
</comment><comment author="clintongormley" created="2015-04-08T09:51:19Z" id="90865899">@vishalshah-org scripting obviously allows you to do whatever you want, which is fine.  That said, scripting today requires you to add a script to every node, which is quite clumsy.  It'd be nice to have a simple patch language that doesn't require you to expose scripting.
</comment><comment author="clintongormley" created="2015-11-21T16:20:28Z" id="158659697">I think improving scripting (#13084) is a better idea than trying to invent a patch language in JSON. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mvel should not be default language in 1.3.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7029</link><project id="" key="" /><description>From the mailing list: https://groups.google.com/d/msgid/elasticsearch/b518a387-a02c-40ac-af7f-01ff41f50bc3%40googlegroups.com?utm_medium=email&amp;utm_source=footer

Although default scripting language should be `groovy` you can get a `dynamic scripting for [mvel] disabled` message.

```
DELETE test
PUT test
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  }
}
GET test/_search
{
  "query": {
    "function_score": {
      "functions": [
        {
          "script_score": {
            "script": "1"
          }
        }
      ]
    }
  }
}
```

Gives `dynamic scripting for [mvel] disabled`.

``` js
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[yarIIfooTWuCBTu8NK3FsA][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"function_score\": {\n      \"functions\": [\n        {\n          \"script_score\": {\n            \"script\": \"1\"\n          }\n        }\n      ]\n    }\n  }\n}\n]]]; nested: QueryParsingException[[test] script_score the script could not be loaded]; nested: ScriptException[dynamic scripting for [mvel] disabled]; }]",
   "status": 400
}
```
</description><key id="38707907">7029</key><summary>mvel should not be default language in 1.3.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-07-25T08:27:17Z</created><updated>2014-08-20T19:55:15Z</updated><resolved>2014-07-25T08:29:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-25T08:29:35Z" id="50121640">Sorry. Closing. 

Actually `mvel` is still the default for 1.3.0.
`groovy` will be the default in 1.4.0.

We will fix the doc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Version 1.3.0 error indexing document with icu_analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7028</link><project id="" key="" /><description>When I index a document I get this error

```
java.lang.NoClassDefFoundError: org/apache/lucene/util/AttributeSource$AttributeFactory
        at org.apache.lucene.analysis.icu.segmentation.ICUTokenizer.&lt;init&gt;(ICUTokenizer.java:84)
        at org.apache.lucene.analysis.icu.segmentation.ICUTokenizer.&lt;init&gt;(ICUTokenizer.java:71)
        at org.elasticsearch.indices.analysis.IcuIndicesAnalysis$1.create(IcuIndicesAnalysis.java:59)
        at org.elasticsearch.index.analysis.CustomAnalyzer.createComponents(CustomAnalyzer.java:83)
        at org.apache.lucene.analysis.AnalyzerWrapper.createComponents(AnalyzerWrapper.java:102)
        at org.apache.lucene.analysis.AnalyzerWrapper.createComponents(AnalyzerWrapper.java:102)
        at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:180)
        at org.apache.lucene.document.Field.tokenStream(Field.java:554)
        at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:597)
        at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:342)
        at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:301)
        at org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:258)
        at org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:412)
        at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1321)
        at org.apache.lucene.index.IndexWriter.addDocuments(IndexWriter.java:1282)
        at org.elasticsearch.index.engine.internal.InternalEngine.innerIndex(InternalEngine.java:555)
        at org.elasticsearch.index.engine.internal.InternalEngine.index(InternalEngine.java:486)
        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:409)
        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:195)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:527)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

this is the system out of the version

```
{
  "status" : 200,
  "name" : "Boom Boy",
  "version" : {
    "number" : "1.3.0",
    "build_hash" : "1265b1454eee7725a6918f57415c480028700fb4",
    "build_timestamp" : "2014-07-23T13:46:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="38705933">7028</key><summary>Version 1.3.0 error indexing document with icu_analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">fabiofumarola</reporter><labels /><created>2014-07-25T08:04:11Z</created><updated>2014-08-07T06:43:12Z</updated><resolved>2014-08-07T06:43:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-25T08:07:58Z" id="50119901">What is your icu plugin version?
</comment><comment author="morus" created="2014-08-06T12:16:20Z" id="51326552">I get more or less the same issue when trying to search a field analyzed with the icu analyzer.
ES 1.3.1, ICU 2.1.0

As far as I can see (I'm not a java guy) the problem is, that es 1.3.x comes with lucene 4.9 while the icu plugin provides lucene-analyzers-icu-4.7.0.jar.
If I replace the latter with  lucene-analyzers-icu-4.9.0.jar (in  elasticsearch-1.3.1/plugins/analysis-icu) from the lucene distribution, things get better.

Not sure if that's really a full fix, but you might give it a try.

Don't ask me how to build an installable fixed plugin.

best
  Morus

PS: shouldn't this bug be on the elasticsearch/elasticsearch-analysis-icu project?
</comment><comment author="dadoonet" created="2014-08-06T14:14:38Z" id="51339971">Update to 2.3.0. See https://github.com/elasticsearch/elasticsearch-analysis-icu/tree/es-1.3
</comment><comment author="morus" created="2014-08-07T06:39:31Z" id="51436393">ok.

Not sure what I did yesterday (I did upgrade the icu plugin, but for some reason I considered 2.1.0 to be the newest. Probably google took me to the page for the es-1.1 branch).

I guess you can close the issue then.
The error shows up in case of an outdated plugin and that's most probably the original posters issue either.

Thanks a lot.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update scripting.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7027</link><project id="" key="" /><description>a small change to correct the type of the script in an example. Groovy is used and mvel is stated.
</description><key id="38704814">7027</key><summary>Update scripting.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">jettro</reporter><labels><label>docs</label></labels><created>2014-07-25T07:45:52Z</created><updated>2014-08-18T11:12:06Z</updated><resolved>2014-08-18T11:12:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T10:18:02Z" id="50131837">Hi @jettro 

Thanks for the fix. Please could I ask you to sign our CLA so that I can merge this PR in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jettro" created="2014-07-25T10:58:55Z" id="50135001">Going to try it again later, but it seems my holiday wifi does not support
signing the contract. Going to try later today, will let you know whether
it works

On Fri, Jul 25, 2014 at 12:18 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @jettro https://github.com/jettro
&gt; 
&gt; Thanks for the fix. Please could I ask you to sign our CLA so that I can
&gt; merge this PR in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/7027#issuecomment-50131837
&gt; .

## 

Jettro Coenradie
http://www.gridshore.nl
</comment><comment author="jettro" created="2014-07-25T13:14:56Z" id="50146620">I think I have done it.

regards Jettro

On Fri, Jul 25, 2014 at 12:58 PM, Jettro Coenradie jettro@gridshore.nl
wrote:

&gt; Going to try it again later, but it seems my holiday wifi does not support
&gt; signing the contract. Going to try later today, will let you know whether
&gt; it works
&gt; 
&gt; On Fri, Jul 25, 2014 at 12:18 PM, Clinton Gormley &lt;
&gt; notifications@github.com&gt; wrote:
&gt; 
&gt; &gt; Hi @jettro https://github.com/jettro
&gt; &gt; 
&gt; &gt; Thanks for the fix. Please could I ask you to sign our CLA so that I can
&gt; &gt; merge this PR in?
&gt; &gt; http://www.elasticsearch.org/contributor-agreement/
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elasticsearch/elasticsearch/pull/7027#issuecomment-50131837
&gt; &gt; .
&gt; 
&gt; ## 
&gt; 
&gt; Jettro Coenradie
&gt; http://www.gridshore.nl

## 

Jettro Coenradie
http://www.gridshore.nl
</comment><comment author="clintongormley" created="2014-08-18T11:12:06Z" id="52478912">thanks @jettro 

i was too slow. already fixed by 98e62e6b18a27a1d80efaa6541c7dbec83b843c2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add search-exists API to check if any matching documents exist for a given query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7026</link><project id="" key="" /><description>Implements a new Exists API allowing users to do fast exists check on any matched documents for a given query.

This API should be faster then using the Count API as it will:
- early terminate the search execution once any document is found to exist
- return the response as soon as the first shard reports matched documents

closes #6995
</description><key id="38677258">7026</key><summary>Add search-exists API to check if any matching documents exist for a given query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Search</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T21:44:56Z</created><updated>2015-06-06T18:28:07Z</updated><resolved>2014-07-31T19:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-07-24T21:55:23Z" id="50083714">The request format would be identical to that of the Count API:

``` bash
curl -XGET 'http://localhost:9200/_exists' -d '
{
    "query" : {
        ...
    }
}'
```

Additionally one can specify `preference`, `type`, `min_score` and `routing`. Additionally filter on index(s) and type(s).

The Response would be as simple as:

``` bash
{
    "exists": true,
    "_shards": {
       ...
    }
}
```

I have intentionally not integrated it with the `_search` endpoint, as yet another flag has to be introduced in the response, as `hits` would not be a good container for the `exists` flag. 
</comment><comment author="areek" created="2014-07-24T21:57:44Z" id="50083976">TODO:
- Figure out a nicer way of early termination of shard requests, any feedback on this would be greatly appreciated! (see: https://github.com/elasticsearch/elasticsearch/pull/7026/files#diff-bdd1ee7f7701d4264050053b19cacaebR207)
- Documentation
- Rest Spec/Tests
</comment><comment author="areek" created="2014-07-24T23:54:40Z" id="50093335">Updated PR:
- fixed concurrency error for early termination of shard requests (thanks @kimchy)

TODO:
- Documentation
- Rest Spec/Tests
</comment><comment author="areek" created="2014-07-28T16:01:50Z" id="50358489">@clintongormley would be awesome if you could take a look at the docs for the new API. The docs are quite similar to that of the count API.

TODO:
- rest spec/tests
</comment><comment author="clintongormley" created="2014-07-28T16:06:44Z" id="50359141">Hi @areek 

This looks good, but I think the REST API needs to change to be consistent.  Exists requests in the REST layer are implemented as HEAD requests which don't return a body, just a 200 or a 404. (they can return other HTTP error codes if some other error occurs)

I think this should be:

```
HEAD /{index}/{type}/_search
```

And in the rest specs, it should probably  be called `search_exists` (can you think of a better name?) to avoid confusion with the `exists` API.
</comment><comment author="areek" created="2014-07-28T19:39:02Z" id="50387917">@clintongormley I changed the REST API to handle `HEAD`, I still have support for `GET` and `POST` due to users ability to send a request body. 

Regarding the naming, I was thinking `hit_exists`. Thoughts? 
</comment><comment author="areek" created="2014-07-28T22:36:15Z" id="50411955">@clintongormley As far as I have seen, it seems like we don't have an explicit 'exists' API. I am assuming you are referring to `HEAD /{index}/{type}/{id}` in the GET API used to determine doc existence based on id. So I think it is ok to use `_exists` endpoint here. Thoughts always welcome and thanks for suggesting HTTP `HEAD`!

 The updated REST resp/request are as follows:

``` bash
curl -XHEAD -i "localhost:9200/{index|null}/{type|null}/_exists" -d '
{
  "query": ...
}
'
or 
curl -XHEAD -i "localhost:9200/{index|null}/{type|null}/_exists?q=.."
```

The HTTP `GET` and `POST` are also supported for having request body.

The response header is a simple:

``` bash
HTTP/1.1 200 OK
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```

or

``` bash
HTTP/1.1 404 Not Found
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```

The response will not have any content for any of the supported HTTP methods.
</comment><comment author="clintongormley" created="2014-07-29T11:20:40Z" id="50463703">@areek Bah, I'd completely forgotten about the issue with HEAD and bodies!  Apologies.  And yes, by "exists" API I was referring to `HEAD /{index}/{type}/{id}`.

OK, so rethinking this:  We already have `/_search/scroll`, `/_search/percolate`, `/_search/template`, wondering if this should be `/_search/exists`.  

Also, the clients expect a body in the response for all requests except for HEAD requests, but as you've pointed out, eg the JS client won't be able to use a HEAD request here, so it looks like returning a body will be required.

Sorry for the runaround
</comment><comment author="s1monw" created="2014-07-29T13:23:20Z" id="50475017">aside of @clintongormley comments this looks fantastic!
</comment><comment author="areek" created="2014-07-29T20:10:56Z" id="50531330">@clintongormley I have updated the REST req/res as follows:
- Support HTTP `HEAD`, `GET` and `POST` methods
  - If HTTP `HEAD` is used, no body is returned only the status
  - else response body looks as follows:

``` bash
     {
       "exists" : true | false
     }
```
- Added additional endpoint `/_search/exists` (`/_exists` can be used to as described in previous comment)
- Updated Docs
</comment><comment author="clintongormley" created="2014-07-30T10:07:19Z" id="50596473">Hi @areek 

I'm tempted to remove support for HEAD and for `/_exists` completely:
-  Remove HEAD because, even for clients where it works, there may be a proxy in-between the client and ES where it doesn't work, so we'll all end up implementing it as GET or POST anyway.
- Remove `/_exists` because it uses up a namespace which is more explicitly described by `/_search/exists`.  `/{index}/_exists` looks like it would check the existence of an index.

So I think this should be exposed only as : 

```
GET | POST /_search/exists
GET | POST /{index}/_search/exists
GET | POST /{index}/{type}/_search/exists
```
</comment><comment author="clintongormley" created="2014-07-30T10:07:57Z" id="50596534">also, the response body looks good.
</comment><comment author="areek" created="2014-07-31T03:03:31Z" id="50707644">Hi @clintongormley thanks for the review. I have updated the documentation and the REST endpoints as suggested.
</comment><comment author="clintongormley" created="2014-07-31T08:10:05Z" id="50729423">LGTM
</comment><comment author="areek" created="2014-07-31T17:36:34Z" id="50792126">I think this is ready. I will commit this soon, if there are no objections.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master election and discovery in ZenDiscovery module should be pluggable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7025</link><project id="" key="" /><description>We are implementing a custom master election strategy, but still want to use the zen module  for other actions such as fault detection, sending join requests, processing cluster state, etc.  There should be a way to pass in custom implementations of ElectMasterService and findMaster() to enable reuse of code without requiring copy-pasted code in custom discovery plugins.
</description><key id="38667233">7025</key><summary>Master election and discovery in ZenDiscovery module should be pluggable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">hgesserit</reporter><labels /><created>2014-07-24T19:49:03Z</created><updated>2015-11-21T16:18:04Z</updated><resolved>2015-11-21T16:18:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-25T09:53:24Z" id="50129661">the code for the discovery module is going through changes in the improve_zen branch, so if we add such an extension, I suggest we wait till improve_zen lands.

Do you find the elect master service enough to plug your custom logic? cause it is being initialized by the gossip of other nodes, and the imrpove_zen branch major change was to run this gossip phase on a master failover. May I ask what you exactly you are after with the custom logic?
</comment><comment author="kimchy" created="2014-09-07T11:16:16Z" id="54743851">improve zen has landed in master / 1.x now, still wondering what the custom logic for this is, I suspect that extension point in the elect master service is not enough for what you are after, but would love to understand the scenario better.
</comment><comment author="clintongormley" created="2015-11-21T16:18:04Z" id="158659583">No feedback in over a year. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is it possible to index rest web services..?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7024</link><project id="" key="" /><description>Hi,

Is there a way to index rest web service (lets say that web service gives some list of XML objects)..?

I am having a requirement to index the list of product details which will be returned from a web service.

Any suggestions will be helpful to me.

Thanks,
Srinivas
</description><key id="38657179">7024</key><summary>Is it possible to index rest web services..?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">srinivasv2</reporter><labels /><created>2014-07-24T17:55:29Z</created><updated>2014-07-24T18:00:26Z</updated><resolved>2014-07-24T18:00:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T18:00:26Z" id="50054477">Hi Srinivas. Please use the mailing list[1] to ask such questions. We try to keep Github for bugs/features/enhancements only. Thank you!

[1] https://groups.google.com/forum/#!forum/elasticsearch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed filters execution order and fix potential concurrency issue in filter chains</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7023</link><project id="" key="" /><description>Fix filters ordering which seems to be the opposite to what it should be (#7019). Added missing tests for rest filters.

Solved concurrency issue in both rest filter chain and transport action filter chain (#7021).

Closes #7019
Closes #7021
</description><key id="38649507">7023</key><summary>Fixed filters execution order and fix potential concurrency issue in filter chains</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>breaking</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T16:29:12Z</created><updated>2015-06-06T16:44:09Z</updated><resolved>2014-07-28T14:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-25T17:33:35Z" id="50180254">LGTM with 2 minor comments
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix infinite loop in the histogram reduce logic.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7022</link><project id="" key="" /><description>The histogram reduce method can run into an infinite loop if the
Rounding.nextRoundingValue value is buggy, which happened to be the case for
DayTimeZoneRoundingFloor.

DayTimeZoneRoundingFloor is fixed, and the histogram reduce method has been
changed to fail instead of running into an infinite loop in case of a buffy
nextRoundingValue impl.

Close #6965
</description><key id="38649430">7022</key><summary>Fix infinite loop in the histogram reduce logic.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T16:28:21Z</created><updated>2015-06-07T19:15:19Z</updated><resolved>2014-08-01T07:09:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-29T14:38:42Z" id="50484634">I think this LGTM - left a minor comment
</comment><comment author="jpountz" created="2014-07-29T16:13:43Z" id="50499126">@s1monw I pushed a commit that should address your comment.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: concurrency issue in rest and action filter chains</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7021</link><project id="" key="" /><description>A test failure (`TransportActionFilterChainTests#testTooManyContinueProcessing`) revealed a concurrency issue in both rest filter chain and action filter chain. We use a `volatile` int to keep track of the current position in the chain, which might get incremented from concurrent threads. `volatile` is not enough if the filters call `continueProcessing` concurrently.
</description><key id="38649115">7021</key><summary>Internal: concurrency issue in rest and action filter chains</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T16:24:48Z</created><updated>2014-07-28T14:58:25Z</updated><resolved>2014-07-28T14:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>exclude aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7020</link><project id="" key="" /><description>This feature allows to exclude parts of a query when defining the result set used by an aggregator.

Based on: https://wiki.apache.org/solr/SimpleFacetParameters#Tagging_and_excluding_Filters

I work at a company that develops ecommerce solutions and we use the solr feature a lot to provide a better search experience while keeping the query size manageable. And the one excuse people gave me not to use elasticsearch was that this behaviour would be to verbose, having to write the whole query in each aggregator. I love elasticsearch, and want to use it in future projects.

I experimented with a more generic approach which allowed to remove/cut a branch of the filter tree at any point, but there were some inconsistencies in behaviour when excluding in a 'or' filter vs 'and' filter, because having a null for a filter is not treated the same way everywhere.
</description><key id="38643290">7020</key><summary>exclude aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">miguelnmiranda</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2014-07-24T15:31:09Z</created><updated>2017-05-09T17:24:51Z</updated><resolved>2016-03-08T15:18:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T15:44:31Z" id="50036172">@miguelnmiranda I don't think you need to have the query in each aggregator. Users usually solve this problem by using `post_filter`:

``` json
{
  "query": {
    "filtered": {
      "query": "your query goes here",
      "filter": "filters to take into account for top-hits and aggs"
    }
  },
  "post_filter" : "filters to take into account for top-hits only",
  "aggs": {
    "my_filter": {
      "filter": "filter to take into account for aggs only"
    }
  }
}
```

Would it work for you?
</comment><comment author="miguelnmiranda" created="2014-07-24T15:58:32Z" id="50038317">@jpountz I believe that with post_filter you can only add more filter not remove some of the query filters.

The example I wrote for the documentation you have the case where you have a facet, lets say colour facet,
and although the result set is being filtered for a specific colour I want to show all colours that I would display if no colour was selected. Not the same as no match_all query, just the some query without the colour part.
</comment><comment author="jpountz" created="2014-07-24T16:26:36Z" id="50042459">&gt; The example I wrote for the documentation you have the case where you have a facet, lets say colour facet, and although the result set is being filtered for a specific colour I want to show all colours that I would display if no colour was selected.

Wouldn't it work if the colour filter was a post_filter (but neither a query filter nor an aggregation filter)?
</comment><comment author="miguelnmiranda" created="2014-07-24T16:45:34Z" id="50044852">&gt; Wouldn't it work if the colour filter was a post_filter (but neither a query filter nor an aggregation filter)?

Yes it would, but if you have two or more facets, lets say colour and gender you cannot follow that approach.
In a case I remember we had gender, category, size, colour, features, price range and rating facets (maybe more).
</comment><comment author="jpountz" created="2014-07-24T17:23:39Z" id="50049690">I think I understand the issue now. Using `post_filter` would work, but would not be easy to use given that if you have N filters, you would need to put them all in the `post_filter` and have `N` filter aggregations that would be a combination of `N-1` filters (so you would have a filter on `colour` that would filter on everything but `colour`, a filter on `gender` that would filter on everything but `gender`, etc.)
</comment><comment author="miguelnmiranda" created="2014-07-24T17:26:46Z" id="50050126">Yes. Sorry for not putting it in those terms to start with.
</comment><comment author="jpountz" created="2014-07-24T17:29:38Z" id="50050511">No worries, I was a bit slow to understand on my end as well. :-)
</comment><comment author="miguelnmiranda" created="2014-07-24T17:31:37Z" id="50050802">I think my example for the documentation is incorrect. I used cardinality aggregator when I just wanted the number of docs. Will fix that later if this goes forward.

Also there is a live example.. the website of a brand, but it seems to be down for maintenance at the moment. I will post it as an example here when it goes back up.
</comment><comment author="jpountz" created="2014-07-24T17:42:01Z" id="50052105">I don't like much having the ability to exclude filters from the query since it breaks the expectation that aggregations apply to the documents that match the query (the only exception being the `global` aggregation.

However, I think we could have an aggregation that would accept a list of filters and would build a bucket for every combination of (N-1) filters. Maybe this functionnality could even be folded into the [`filters`](https://github.com/elasticsearch/elasticsearch/issues/6118) aggregation (not sure, just wondering).
</comment><comment author="miguelnmiranda" created="2014-07-24T18:35:34Z" id="50059152">&gt; I don't like much having the ability to exclude filters from the query since it breaks the expectation that aggregations apply to the documents that match the query (the only exception being the global aggregation.

Is it because of the semantics of `exclude`? It actually extends the `global` aggregator, and could be done the other way around, where you say which filters to include instead.

&gt; However, I think we could have an aggregation that would accept a list of filters and would build a bucket for every combination of (N-1) filters. Maybe this functionality could even be folded into the filters aggregation (not sure, just wondering).

This would work, and is ideal for the case where you have selected at least a value for each available facet. But if you only filter based on M (&lt;N) of the available facets, meaning you don't have a filter for all facets yet, it won't generate buckets for the remaining N-M facets.
</comment><comment author="roytmana" created="2014-07-24T19:37:24Z" id="50067039">If I may add I would lowe a feature to control sub aggs on bucket level. One of my cases is to be able to produce output for multilevel agg where lower level aggs will be slightly difgerent from each other not in terms of agg nature but in following:
- whether to do sub agg for a given bucket at all
- max sub agg size for a a given bucket
- fulters different for sub agg per bucket

Imagine a drill down tree UI where user can start from top level agg and drill down int idividual buckets and then change query and see the changes to yhe expanded trer in one call to elastic. It is acievalbe now but at the cost of multiple aggs 

Say we have agg on country state and city

I drilled down into United States/Montana and Alaska

In order to reload the data in one call

I would have to aggs on the same level

Countries and countries/states with filter including alaska and montana and then post process the data to merge second agg results into the first

It is doable but if you add need to handle missing and other buckets not supported by ES in sumilar way it becomes rather messy and hard to implement in general way.

What I would like to see is abulity to specify extra options inside states  agg per possible bucket

For example

Bucket-config: {
Montana:{size:20}
_others:{calculate:false}
}

In short I would like to be able to exercise some control over sub aggs on parent agg bucket level rather than all of rhem be exactly the same such drill down into individual branches instead of all of them
</comment><comment author="dmitry" created="2014-07-24T22:23:43Z" id="50086445">As I understand clearly, the current solution is to include `filter` in every aggregation bucket from the main filter with exclusion (one of the filter you don't want to be in the aggs)?

Something like described here: http://stackoverflow.com/questions/8908325/elasticsearch-excluding-filters-while-faceting-possible-like-in-solr (it's for the facets, but follows the same idea)
</comment><comment author="jpountz" created="2014-07-24T22:27:11Z" id="50086735">Correct. @clintongormley explained it much better than me!
</comment><comment author="roytmana" created="2014-07-24T22:33:34Z" id="50087268">@dmitry 
That's for a single level agg (facets). I want to be able to control what would happen in the second level for individual buckets produced by the first level of agg. particularly I want to say which first level buckets should have their sub aggs calculated rather than having them all calculated. Say I have an agg by country/state
I want aggs over all countries but only want sub-aggs calculated for USA and UK but not for the others

so in the countries/states agg definition I want to specify instructions for each country bucket such whether to calculate matching coutry sub-aggs, max size of the sub-agg  etc
</comment><comment author="miguelnmiranda" created="2014-07-24T22:53:57Z" id="50088879">@dmitry Yes. The example describes exactly the behaviour pretended.
With two facets is still manageable, with N facets we come to the case described by @jpountz.
The behaviour can be seen [here](http://www.asicsamerica.com/search/-/producttypefacet/Footwear/gender/Male/0-45-featured). Filtering for Male products does not remove count from the other gender options.
</comment><comment author="dmitry" created="2014-07-25T02:52:01Z" id="50103102">@miguelnmiranda and currently it's not possible to have the same behavior without including all the filters in the `aggs` with an exception of the field that is aggregated by?

In my case I have something like that:

``` json
{
    "body": {
        "post_filter": {
            "and": [{
                "terms": {
                    "type": ["apartment"]
                }
            }, {
                "terms": {
                    "location_ids": [386]
                }
            }]
        },
        "aggregations": {
            "types": {
                "filter": {
                    "and": [{
                        "terms": {
                            "location_ids": [386]
                        }
                    }]
                },
                "aggs": {
                    "types": {
                        "terms": {
                            "field": "type",
                            "size": 0
                        }
                    }
                }
            },
            "locations": {
                "filter": {
                    "and": [{
                        "terms": {
                            "type": ["apartment"]
                        }
                    }]
                },
                "aggs": {
                    "locations": {
                        "terms": {
                            "field": "location_ids",
                            "size": 0
                        }
                    }
                }
            }
        }
    },
    "index": "properties",
    "type": ["property"]
}
```

I thought there should be some better solution for that most used case of elasticsearch or I'm wrong?
</comment><comment author="clintongormley" created="2014-07-25T05:41:21Z" id="50109963">Hi @miguelnmiranda 

Thanks for the PR, but I agree wholeheartedly with @jpountz. We used to have named "scopes" in facets, back in the day, but they were removed.  This PR suffers from the same problem that they did.

The DSL allows for complicated nesting of clauses, while scopes refer to individual filters, regardless of their position in the query. You could apply a name to a filter which is a sub-clause of another filter, but in the aggregation, you'd get documents that you are not expecting because the filter is treated as though it were at the top.  You include an option to exclude the query - same problem: which query are you referring to? there could be several.

I like @jpountz 's idea of extending the `filters` agg (see #6974) to allow application of all filters but one, in a loop.  It seems like a good general solution to this issue.  

&gt; But if you only filter based on M (&lt;N) of the available facets, meaning you don't have a filter for all facets yet, it won't generate buckets for the remaining N-M facets.

But you'd know up front which clauses don't have values, so you'd just specify these as normal aggs. 

(@roytmana you're talking about something completely separate, please don't hijack this issue)
</comment><comment author="clintongormley" created="2014-07-25T10:43:59Z" id="50133834">@miguelnmiranda yes, you have a point... The only thing I could come up with looks like this:

```
GET _search
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "gender": "male"
              }
            },
            {
              "term": {
                "size": "xl"
              }
            },
            {
              "range": {
                "price": {
                  "lt": 20
                }
              }
            }
          ]
        }
      }
    }
  },
  "aggs": {
    "global": {
      "global": {},
      "aggs": {
        "colour": {
          "terms": {
            "field": "colour"
          }
        },
        "filtered": {
          "combinatorial_filters": [
            {
              "filter": {
                "term": {
                  "gender": "male"
                }
              },
              "aggs": {
                "gender": {
                  "terms": {
                    "field": "gender"
                  }
                }
              }
            },
            {
              "filter": {
                "term": {
                  "gender": "size"
                }
              },
              "aggs": {
                "size": {
                  "terms": {
                    "field": "size"
                  }
                }
              }
            },
            {
              "filter": {
                "term": {
                  "gender": "male"
                }
              },
              "aggs": {
                "price_range": {
                  "range": {
                    "field": "price",
                    "ranges": [
                      {
                        "from": 0,
                        "to": 20
                      },
                      {
                        "from": 20,
                        "to": 40
                      },
                      {
                        "from": 40,
                        "to": 60
                      }
                    ]
                  }
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

In this example, `colour` is not being filtered, so it is run as just a normal agg (under the `global` scope).  The filtered fields are passed in a `combinatorial_filters` agg (which doesn't exist yet).  Each entry includes: (1) a filter and (2) any aggs.

Execution would iterate through the entries and apply all filters except for the current filter, where it would calculate the aggs instead.

This is completely different from any other aggs as they are today, so not sure how well this API would fit.
</comment><comment author="miguelnmiranda" created="2014-07-25T11:13:34Z" id="50136222">I deleted my previous the comment by mistake!

@clintongormley the behaviour seems odd.. and as you say does not fit well with the API.

&gt; You could apply a name to a filter which is a sub-clause of another filter, but in the aggregation, you'd get documents that you are not expecting because the filter is treated as though it were at the top. You include an option to exclude the query - same problem: which query are you referring to? there could be several.

The "current" exclude filter only looks at the the names inside the top level `and` only excludes those. 

I implemented a different approach where you could "cut" the filter tree at any point.

``` javascript
"filter": {
  "and": {
    _name : root
    filters : [
    "or": {
       _name : orBranch
       filters : [
         "filter": {
           _name : f2
         }
         "filter": {
           _name : f3
         }
       ]
    ], {
    "filter": {
      _name : f1
    }
  ]
}
```

```
. and (root)
|_. or (orBrach)
|  |_. filter (f2)
|  |_. filter (f3)
|_.filter (f1)
```

But while writting it I found that the behaviour when a sub filter is null is not consistent across filters.
</comment><comment author="markharwood" created="2014-08-22T16:43:57Z" id="53087546">One way of getting counts for each dimension independent of that dimension's clauses would be to use a minimum_number_should_match value of 1 less than the number of clauses e.g.

```
curl -XGET "http://localhost:9200/pr7020/product/_search?pretty=1" -d'
{
   "query" : {
       "bool":{
           "minimum_number_should_match": 2, 
           "should": [
              {
                    "term" : {
                        "gender" : "M"
                    }
              },
            {
                    "term" : {
                        "size" : "large"
                    }
              },              
              {
                    "term" : {
                        "colour" : "blue"
                    }
              }
           ]
       }
   },
   "aggs" : {
            "colors":{
                "terms":{
                    "field":"colour"
                }
            },
            "genders":{
                "terms":{
                    "field":"gender"
                }
            },
            "sizes":{
                "terms":{
                    "field":"size"
                }
            }
   }
}'
```

Each _terms_ agg in the above would then collect all terms where only 2 of the 3 clauses were present. 
Obviously there may be some extra work in filtering displayed hits. The ranking algos would ideally show the hits with 3 out of 3 clause matches on top of the 2-out-of-3 ones anyway. 
</comment><comment author="clintongormley" created="2014-10-21T10:18:09Z" id="59906753">@markharwood I tried out your solution and it doesn't do quite what we're after.  For instance, given the following query:

```
    "bool": {
      "must": [
          { "term": { "size":  "large" }},
          { "term": { "color": "red"   }},
          { "term": { "type":  "shirt" }}
        ]
     }
```

... we want to know what count we would get for:
- count `type` where size:large AND color:red
- count `color` where size:large AND type:shirt
- count `size` where color:red AND type:shirt

While your approach actually gives us counts `type`, `color`, and `size` where:

```
(size:large AND color:red) OR (size:large AND type:shirt) OR (color:red AND type:shirt)
```

I don't see any concise way of doing this out of the box.  

That said, this is a common and very specific use case.  We could possibly provide a simple (but inflexible) aggregation that does exactly what is needed here.  I say inflexible because we want to keep it simple - if you want flexibility you can go the verbose route instead.

What about something like this:

```
GET /_search?
{
  "post_filter": {
    "bool": {
      "must": [
          { "term": { "size":  "large" }},
          { "term": { "color": "red"   }},
          { "term": { "type":  "shirt" }}
        ]
     }
  },
  "aggs": {
    "combos": {
      "filtered_terms": {
        "filters": [
          { "term": { "size": "large"}},
          { "term": { "color": "red" }},
          { "term": { "type": "shirt"}}
        ]
      }
    }
  }
}
```

Of course, this syntax doesn't reduce the amount of work that has to be performed.  3 terms means 6 filters, 4 terms means 12 filters, 5 terms means 20 filters...
</comment><comment author="markharwood" created="2014-10-21T10:38:18Z" id="59908905">Not sure I follow.

&gt; we want to know what count we would get for:
&gt; count type where size:large AND color:red
&gt; count color where size:large AND type:shirt
&gt; count size where color:red AND type:shirt

The OP primarily asked for a list not a count:  _"I want to show all colours that I would display if no colour was selected"_.
So in my example I produce the following aggs :
a) list all of the available colours for large shirts.
b) list all of the available types of clothing that are large and red.
c) list all of the sizes of the red shirts.

Each of these lists include counts e.g. how many large shirts are available in blue but the count is perhaps not the primary concern - the typical shopper just wants to know the large shirt is also available in blue. 
</comment><comment author="clintongormley" created="2014-10-21T10:47:39Z" id="59909831">&gt; The OP primarily asked for a list not a count:

Actually, in a later comment the OP says _"...I just wanted the number of docs"_*

And this is the typical use case - how many red, green, blue products do I have which are type:shirt and size:large, ie what will I see if I remove this particular filter.
</comment><comment author="markharwood" created="2014-10-21T11:10:22Z" id="59912000">Still confused then.

The full quote from the OP re numbers is 

&gt; _"I think my example for the documentation is incorrect. I used cardinality aggregator when I just wanted the number of docs. Will fix that later"_

Cardinality aggs is about count distinct and I don't know what example he refers to.

I think the typical requirement is simple - if I have a dimension that supports multiple selections (red OR blue OR green checkboxes) then I don't want the options for blue or green to immediately disappear when I select red. However, if I make a selection in a different dimension that says I'm only interested in Large shirts then I don't want to see colours that are not available in large. That's the behaviour I assumed the user was after and which I think my example provides (along with the related counts).
</comment><comment author="sicarrots" created="2015-01-29T23:44:47Z" id="72127733">What is the status of this pull request? It's possible to have this feature in near future?
</comment><comment author="clintongormley" created="2016-03-08T15:18:49Z" id="193821145">It is clear that this PR isn't going to be merged as is.  I haven't seen a good suggestion yet for how to implement this with term counts (although [@markharwood's suggestion](https://github.com/elastic/elasticsearch/pull/7020#issuecomment-53087546) works without term counts).  I'd welcome more suggestions in a new issue.
</comment><comment author="ssetem" created="2017-05-09T12:15:03Z" id="300144903">we have this issue in https://github.com/searchkit/searchkit

SOLR solve this with tags which is a bit like this PR solution

http://yonik.com/multi-select-faceting/

We need to put filters applied by aggregators in post_filter 
and generate filtered aggregators per aggregator which excludes just its own filter, which is very verbose, it works but would probably cut the query by 50% at least if this was done at ElasticSearch layer</comment><comment author="markharwood" created="2017-05-09T16:12:36Z" id="300216504">I think [my suggestion](https://github.com/elastic/elasticsearch/pull/7020#issuecomment-53087546) should give you the terms and counts you need in aggregation results but the end tail of docs in the hits it produces may have false positives (docs that match n-1 dimensions when you want them to match n dimensions). 
If the quality of results at the tail-end of the hits is a concern you can always tighten it up using a copy of the agg filters composed in a `must` expression. This would mean that as a max the query JSON would only need one repetition of the user selections rather than some combinatorial explosion of them based on the numbers of dimensions.</comment><comment author="ssetem" created="2017-05-09T17:24:51Z" id="300240034">thanks @markharwood I will test out the the n-1 on root bool query
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest filters execution order doesn't reflect javadocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7019</link><project id="" key="" /><description>`RestFilter`s allow to configure their execution order through the `order` method. Their javadocs say:

&gt; Execution is done from lowest value to highest.

The filters are ordered and executed the opposite way though, from highest to lowest.
</description><key id="38642201">7019</key><summary>Rest filters execution order doesn't reflect javadocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>breaking</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T15:20:58Z</created><updated>2014-07-28T15:16:26Z</updated><resolved>2014-07-28T14:58:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-28T15:16:26Z" id="50352112">This issue is marked as breaking as plugins that register multiple REST filters might rely on their wrong ordering, quite a rare usecase though I believe.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Re-factored benchmark infra</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7018</link><project id="" key="" /><description>Major re-factoring to use a dual-channel strategy for executing
benchmarks. Uses cluster metadata for managing lifecycle events, but
transport channel to send benchmark definitions and results between
master and executor nodes.
</description><key id="38638049">7018</key><summary>Benchmarks: Re-factored benchmark infra</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-24T14:42:22Z</created><updated>2014-07-28T21:44:47Z</updated><resolved>2014-07-25T20:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="aleph-zero" created="2014-07-25T21:00:56Z" id="50203848">Not sure why github decided to close this PR. Seems like it was automatic in response to the most recent commit. I'll try to re-open it, but may have to create a new one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Refactoring to support multiple requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7017</link><project id="" key="" /><description>Support for multiple wildcard requests. Not complete yet.
</description><key id="38637159">7017</key><summary>Benchmarks: Refactoring to support multiple requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-24T14:33:23Z</created><updated>2014-07-24T14:33:30Z</updated><resolved>2014-07-24T14:33:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geo: Fix comparison of doubles in ShapeBuilder.intersections()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7016</link><project id="" key="" /><description>Code in question: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java#L297-299

This code seems to compare the current point with the one after next and continue if their comparison value is equal.  The reason for doing this is not clear and there are no tests which rely on this code.

The compare method is not limited to return -1, 0 and 1 so the chance of this condition being met seems slim, further adding to the confusion as to what its purpose is.
</description><key id="38636253">7016</key><summary>Geo: Fix comparison of doubles in ShapeBuilder.intersections()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T14:24:49Z</created><updated>2014-08-12T10:01:36Z</updated><resolved>2014-08-12T09:27:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a periodic cleanup thread for IndexFieldCache caches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7015</link><project id="" key="" /><description>Defaults to every 10 minutes.

Fixes #7010
</description><key id="38632053">7015</key><summary>Add a periodic cleanup thread for IndexFieldCache caches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Cache</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T13:41:37Z</created><updated>2015-06-07T12:34:48Z</updated><resolved>2014-07-24T15:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-24T15:21:00Z" id="50032732">LGTM, I wonder if we should be more aggressive with the interval? I think something like 60 seconds, specifically since it doesn't spawn another thread, would work well
</comment><comment author="dakrone" created="2014-07-24T15:41:05Z" id="50035676">Changing the interval to 1 minute.
</comment><comment author="kimchy" created="2014-07-24T15:48:50Z" id="50036872">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to using the multi-termvectors API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7014</link><project id="" key="" /><description>The term vector API can now generate term vectors on the fly, if the terms are
not already stored in the index. This commit exploits this new functionality
for the MLT query. Now the terms are directly retrieved using multi-
termvectors API, instead of retrieving the texts using the multi-get API. The
terms are then directly passed to Lucene MLT.
</description><key id="38631600">7014</key><summary>Switch to using the multi-termvectors API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T13:36:44Z</created><updated>2015-06-07T12:34:58Z</updated><resolved>2014-08-21T10:21:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-30T09:40:14Z" id="50594013">Left one minor comment but other than that it looks good to me!
</comment><comment author="alexksikes" created="2014-07-30T09:45:32Z" id="50594480">Thank you. @s1monw would mind having a look at it as well?
</comment><comment author="jpountz" created="2014-08-21T07:55:59Z" id="52889410">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use the provided cluster state instead of fetching a new cluster state from cluster service.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7013</link><project id="" key="" /><description /><key id="38631399">7013</key><summary>Use the provided cluster state instead of fetching a new cluster state from cluster service.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T13:34:11Z</created><updated>2015-06-07T19:15:34Z</updated><resolved>2014-07-24T14:24:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-24T13:42:34Z" id="50019071">LGTM I mean this should likely go into 1.3.1 and 1.2.4 as well but lets wait and let it bake in on master and 1.x first?
</comment><comment author="kimchy" created="2014-07-24T14:00:46Z" id="50021325">LGTM
</comment><comment author="martijnvg" created="2014-07-24T14:13:31Z" id="50023180">@s1monw agreed, lets bake it first.
</comment><comment author="martijnvg" created="2014-07-24T14:24:37Z" id="50024682">Closed via: cf30cf12f065ceff44b959f9484054ab0454fe84
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove use of recycled set in filters eviction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7012</link><project id="" key="" /><description /><key id="38624889">7012</key><summary>Remove use of recycled set in filters eviction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T12:44:43Z</created><updated>2015-06-07T12:35:10Z</updated><resolved>2014-07-24T13:01:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T12:56:39Z" id="50005746">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>no clustering on ec2 due to  ClassNotFoundException[org.elasticsearch.gateway.blobstore.BlobStoreGatewayModule]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7011</link><project id="" key="" /><description>Hello,
in EC@,i installed elasticsearch with
wget http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.3.0.zip
sudo unzip elasticsearch-1.3.0.zip -d /usr/local/elasticsearch
cd /usr/local/elasticsearch/elasticsearch-1.3.0
sudo bin/plugin -install elasticsearch/elasticsearch-cloud-aws/2.1.1

config.yml is

cluster.name: elasticsearch-demo-js
cloud.aws.access_key: xxx..
cloud.aws.secret_key: yyy...
cloud.aws.discovery.type: ec2
gateway.type: s3
gateway.s3.bucket: codetest

but i get this error as soon as i add the gateway configuration
{1.3.0}: Initialization Failed ...
- NoClassDefFoundError[org/elasticsearch/gateway/blobstore/BlobStoreGatewayModule]
  ClassNotFoundException[org.elasticsearch.gateway.blobstore.BlobStoreGatewayModule]

any ideas ?
</description><key id="38621747">7011</key><summary>no clustering on ec2 due to  ClassNotFoundException[org.elasticsearch.gateway.blobstore.BlobStoreGatewayModule]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anthonyoleary</reporter><labels /><created>2014-07-24T12:00:06Z</created><updated>2014-07-24T12:03:08Z</updated><resolved>2014-07-24T12:03:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-24T12:03:08Z" id="49999265">You'd better ask the mailing list. 

That said, try with cloud plugin version 2.2.0.
It should work.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Look into adding a service that cleans Guava caches occasionally</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7010</link><project id="" key="" /><description>When an item has been expired from a Guava cache, the cache does not actually clean up the entry until periodic maintenance is performed. In the event that an entry is expired but the cache is no longer read or written to, it's possible that the entry can stick around longer than intended.

We should add a scheduled action to manually call `Cache.cleanUp()` every N minutes, where N could default to 30 minutes.
</description><key id="38620176">7010</key><summary>Look into adding a service that cleans Guava caches occasionally</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T11:37:06Z</created><updated>2014-07-24T15:51:03Z</updated><resolved>2014-07-24T15:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-24T12:24:50Z" id="50001649">@dakrone i've labelled this `adoptme`.  If you're planning on working on it, please assign it to yourself.
</comment><comment author="dakrone" created="2014-07-24T12:29:21Z" id="50002105">I grabbed it since I'm working on it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cardinality aggregation causes potential memory leak after breaking </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7009</link><project id="" key="" /><description>After running many such queries for cardinality aggregation, the query fails with error. After that I suppose the memory is not released and as it usually consumes 20-50GB of heap, after that it runs 40-60GB. Even after GC, and waiting several hours the memory is not released.

{
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "st.d": "gazeta.pl" 
          }
        },
        {
          "range": {
            "st.c": {
              "from": "2014-05-01T00:00:00.000Z",
              "to": "2014-05-31T00:00:00.000Z" 
            }
          }
        }
      ]
    }
  },
  "size": 0,
  "aggs": {
    "url_count": {
      "cardinality": {
        "field": "st.u",
        "precision_threshold": 100,
        "rehash": true
      }
    }
  }
}

[ERROR][indices.fielddata.breaker] [a1] New used memory 38627884568 [35.9gb] from field [u] would be larger than configured breaker: 38534342246 [35.8gb], breaking
</description><key id="38618069">7009</key><summary>Cardinality aggregation causes potential memory leak after breaking </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rzulf</reporter><labels /><created>2014-07-24T11:03:06Z</created><updated>2014-10-01T04:28:51Z</updated><resolved>2014-07-24T12:24:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-24T11:10:07Z" id="49994802">Hi @Rzulf, this is because the circuit breaker is operating on the fielddata cache, and the data in the field data cache is not expired by default once loaded. You can do one of 3 things:
- manually clear the cache with `curl -XPOST 'http://localhost:9200/index/_cache/clear?field_data=true'` if a request fails and you don't want to keep any loaded field data
- set a limit on field data with `indices.fielddata.cache.size`, data is then expired in an LRU fashion (might not be applicable if single aggregation needs all of this data)
- set an expiration of the field data with `indices.fielddata.cache.expire` to expire it after some time of not being used.
</comment><comment author="Rzulf" created="2014-07-24T11:20:36Z" id="49995619">I have already such settings:

index.cache.field.expire: 30m
index.cache.field.type: soft

After 30 minutes, the memory is not released.

I have version 1.2.2
</comment><comment author="dakrone" created="2014-07-24T11:29:31Z" id="49996349">This is an implementation detail of the Guava cache, from the javadoc:

"Expired entries may be counted in Cache.size(), but will never be visible to read or write operations. Expired entries are cleaned up as part of the routine maintenance described in the class javadoc."

Which references the following:

"Certain cache configurations will result in the accrual of periodic maintenance tasks which will be performed during write operations, or during occasional read operations in the absence of writes."

An expiration time of 30 means that the cache entry is removed, not that the memory is released. Using the cache (either reads or writes) will clean this up.

I think in the future it may be useful to have a scheduled thread that manually calls `Cache.cleanUp()`, but there is none currently. I will open a separate issue for it.
</comment><comment author="dakrone" created="2014-07-24T11:37:16Z" id="49996929">Opened #7010 for this.
</comment><comment author="clintongormley" created="2014-07-24T12:24:03Z" id="50001566">Closed in favour of #7010 
</comment><comment author="kimchy" created="2014-07-24T12:30:41Z" id="50002236">@Rzulf regarding your config, I strongly suggest not to use the `index.fielddata` level settings, which were mostly there for backward comp. aspect, and instead use the `indices.fielddata.cache.`  settings, which are on the node level, and not per index. Also, those don't expose the trappy soft reference option, and instead allow to set evictions based on size or time. You would want to do size, as time is not that relevant.
</comment><comment author="Rzulf" created="2014-07-24T12:56:26Z" id="50005308">@kimchy thanks for advice. But what about the higher heap consumption after running cardinality search. By looking at graphs in bigdesk I can clearly see that before running aggregations the GC would drop heap consumption to 20GB, but after running many aggregations the heap usage would not go below 40/50GB even 24h after running it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Total index memory in _cat/indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7008</link><project id="" key="" /><description>Currently memory stats are available separately (which is good), but it not in accumulated metric like total memory used by index. Some `awk` could fix that, but having built-in ability would be great.

```
# curl -s http://es:9200/_cat/indices?h=index,totalMemory
statistics-20140620 45.2mb
```

Also, bloom filter memory usage is not available in `_cat/indices`. Not sure if something else is missing. Is there a hope to see full memory usage per index?
</description><key id="38617544">7008</key><summary>Total index memory in _cat/indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T10:54:17Z</created><updated>2014-10-08T12:07:17Z</updated><resolved>2014-10-08T12:04:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-24T10:55:47Z" id="49993739">sounds useful - do you wanna come up with a PR?
</comment><comment author="bobrik" created="2014-07-24T12:33:46Z" id="50002512">I'm not a java programmer, I can only come up with something like this:

```
[~] % curl -s http://127.0.0.1:9200/_cat/indices?h=index,fm,fcm,im,pm,sm,siwm,svvm,tm
whatever 0b 0b 0b -1b 6.6kb 0b 6.6kb
```

``` diff
diff --git a/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java b/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
index a4dd483..744209d 100644
--- a/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
+++ b/src/main/java/org/elasticsearch/action/admin/indices/stats/CommonStats.java
@@ -24,6 +24,7 @@ import org.elasticsearch.common.Nullable;
 import org.elasticsearch.common.io.stream.StreamInput;
 import org.elasticsearch.common.io.stream.StreamOutput;
 import org.elasticsearch.common.io.stream.Streamable;
+import org.elasticsearch.common.unit.ByteSizeValue;
 import org.elasticsearch.common.xcontent.ToXContent;
 import org.elasticsearch.common.xcontent.XContentBuilder;
 import org.elasticsearch.index.cache.filter.FilterCacheStats;
@@ -463,6 +464,18 @@ public class CommonStats implements Streamable, ToXContent {
         return stats;
     }

+    public ByteSizeValue getTotalMemory() {
+        long size = this.getFieldData().getMemorySizeInBytes() +
+                this.getFilterCache().getMemorySizeInBytes() +
+                this.getIdCache().getMemorySizeInBytes() +
+                this.getPercolate().getMemorySizeInBytes() +
+                this.getSegments().getMemoryInBytes() +
+                this.getSegments().getIndexWriterMemoryInBytes() +
+                this.getSegments().getVersionMapMemoryInBytes();
+
+        return new ByteSizeValue(size);
+    }
+
     @Override
     public void readFrom(StreamInput in) throws IOException {
         if (in.readBoolean()) {
diff --git a/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java b/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java
index 2001a4b..c771787 100644
--- a/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java
+++ b/src/main/java/org/elasticsearch/rest/action/cat/RestIndicesAction.java
@@ -264,6 +264,8 @@ public class RestIndicesAction extends AbstractCatAction {
         table.addCell("suggest.total", "sibling:pri;alias:suto,suggestTotal;default:false;text-align:right;desc:number of suggest ops");
         table.addCell("pri.suggest.total", "default:false;text-align:right;desc:number of suggest ops");

+        table.addCell("memory.total", "sibling:pri;alias:tm,memoryTotal;default:false;text-align:right;desc:total used memory");
+        table.addCell("pri.memory.total", "default:false;text-align:right;desc:total user memory");

         table.endHeaders();
         return table;
@@ -443,6 +445,9 @@ public class RestIndicesAction extends AbstractCatAction {
             table.addCell(indexStats == null ? null : indexStats.getTotal().getSuggest().getCount());
             table.addCell(indexStats == null ? null : indexStats.getPrimaries().getSuggest().getCount());

+            table.addCell(indexStats == null ? null : indexStats.getTotal().getTotalMemory());
+            table.addCell(indexStats == null ? null : indexStats.getPrimaries().getTotalMemory());
+
             table.endRow();
         }


```

Is that ok?
</comment><comment author="bobrik" created="2014-10-08T12:07:17Z" id="58347515">Awesome!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PatternTokenizer RegEx not recognizing boundary matcher ^</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7007</link><project id="" key="" /><description>For me, the pattern-tokenizer does not play well with the beginning of line boundary matcher **'^'**
(see http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#bounds)

In strings like `"Lastname, Firstname/ Lastname2, Firstname2 Middlename"`, or 
`"Lastname1/Lastname2/Lastname3"` I want to match the last names.

In order to achieve this, I define a regex, that takes the next word after the beginning of the line or after a forward slash, like this: `(/ *|^)([\w]+)`

translated: _(forward slash and maybe whitespace OR beginning of the line) FOLLOWED BY one or more word characters_.

This works fine for me in java, python, but not so in elasticsearch. See my elasticsearch/curl example versus the java testcode below:
# curl example

```
curl -XPUT "http://localhost:9200/testidx" -d '{
  "settings": {
    "index" : {
      "analysis" : {
        "analyzer" : {
          "lastname" : {
            "filter" : ["lowercase", "trim"],
            "tokenizer" : "lastname"
          }
        },
        "tokenizer" : {
          "lastname" : {
              "pattern" : "(?:/ *|^)([\\w]+),*",
              "group": 1,
              "type" : "pattern"
          }
        }
      }
    }
  }
}'
```
# Testing the analyzer:

```
curl -XGET "http://localhost:9200/testidx/_analyze?analyzer=lastname&amp;pretty" -d '{
  "Bach, Johann Sebastian/Haendel, Georg Friedrich"
}'

# Expected tokens: 'bach' and 'haendel'
# Actual outcome:

#{
#   "tokens": [
#      {
#         "token": "haendel",
#         "start_offset": 28,
#         "end_offset": 35,
#         "type": "word",
#         "position": 1
#      }
#   ]
#}
```

I tried this with v1.1.2, v.1.3.1, v2.0.0-SNAPSHOT.
# Java test code

When using the same RegEx in java I do get the desired result:

``` java
package com.konradkonrad;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

public class RegexAnchorTest {

  public static void main(String[] args) {

    String raw = "Bach, Johann Sebastian/Haendel, Georg Friedrich";
    List&lt;String&gt; expected = Arrays.asList("Bach", "Haendel");

    Matcher last = Pattern.compile("(?:/ *|^)([\\w]+),*").matcher(raw);

    List&lt;String&gt; lastNames = new ArrayList&lt;&gt;();
    while (last.find()) lastNames.add(last.group(1));

    if (lastNames.size() != expected.size()) {
      throw new AssertionError("wrong number of matches, is " + lastNames.size() + "should be " + expected.size());
    }

    for (int i = 0; i &lt; expected.size(); i++) {
      if (! lastNames.get(i).equals(expected.get(i))) {
        throw new AssertionError("results not equal: " + lastNames + "; expected: " + expected);
      }
    }
    System.out.println("Tokens are: " + lastNames);
  }
}
```

This outputs:

```
Tokens are: [Bach, Haendel]
```
</description><key id="38613933">7007</key><summary>PatternTokenizer RegEx not recognizing boundary matcher ^</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels /><created>2014-07-24T09:59:58Z</created><updated>2014-07-24T12:53:22Z</updated><resolved>2014-07-24T12:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-24T12:20:14Z" id="50001235">Actually, it does work correctly. The problem is that you are not using the `analyze` API correctly.  Currently, the `analyze` API accepts plain text in the body, not JSON (#5866), so the beginning of the line is actually `{`.

Try this instead:

```
curl -XGET "http://localhost:9200/testidx/_analyze?analyzer=lastname&amp;text=Bach, Johann Sebastian/Haendel, Georg Friedrich
```
</comment><comment author="konradkonrad" created="2014-07-24T12:36:21Z" id="50002715">Oh thanks @clintongormley! I am glad, that it was an error on my side and is working feature wise.

SN: the "Try this" you gave me does not actually work but instead throws `java.lang.IllegalArgumentException: empty text`. Which seems to be related to whitespace in the `text` argument. So +1 on #5866 :)

```
curl -XGET "http://localhost:9200/testidx/_analyze?analyzer=lastname" -d 'Bach, Johann Sebastian/Haendel, Georg Friedrich'
```

does the trick.
</comment><comment author="clintongormley" created="2014-07-24T12:45:21Z" id="50003561">@konradkonrad sorry, bad URL encoding.  Try this instead:

```
curl -XGET 'http://localhost:9200/testidx/_analyze?analyzer=lastname&amp;pretty=1&amp;text=Bach%2C+Johann+Sebastian%2FHaendel%2C+Georg+Friedrich'
```
</comment><comment author="konradkonrad" created="2014-07-24T12:53:22Z" id="50004430">thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recover recent indices first</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7006</link><project id="" key="" /><description>For logging and analytics where writes happen in "current" (this day, this hour) index it is preferred to see fresh indices recovered first and old indices recovered last. Currently indices are recovered in random order.

With this option during restart queue with updates could be paused for 1 minute instead of 20-30 in our setup.
</description><key id="38613862">7006</key><summary>Recover recent indices first</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels><label>adoptme</label><label>feature</label></labels><created>2014-07-24T09:58:54Z</created><updated>2015-09-28T14:33:22Z</updated><resolved>2015-09-28T14:33:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-08-01T08:25:57Z" id="50860894">Needs #7119 
</comment><comment author="jpountz" created="2015-09-28T14:33:21Z" id="143760649">This has been implemented, see https://www.elastic.co/guide/en/elasticsearch/reference/2.0/recovery-prioritization.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Speed up string sort with custom missing value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7005</link><project id="" key="" /><description>Today if the user supplies a custom missing value for a string sort, we do it in an extremely slow way, not using ordinals but dereferencing bytes for every document. Ordinals are only used if the missing value is _first or _last. 

Instead, use ordinals with custom missing values too.
</description><key id="38611684">7005</key><summary>Speed up string sort with custom missing value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T09:28:16Z</created><updated>2015-06-07T12:35:45Z</updated><resolved>2014-07-24T10:29:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-24T10:06:20Z" id="49989784">LGTM
</comment><comment author="jpountz" created="2014-07-24T10:26:30Z" id="49991471">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Geo bounds aggregation does not output aggregation name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7004</link><project id="" key="" /><description>The geo bounds aggregation does not surround its JSON output with the aggregation name.  For 1.3.x this should be fixed directly. For 1.4+ we should move the serialisation of the aggregation name to InternalAggregation so an aggregation can only output JSON in its own scope.
</description><key id="38610339">7004</key><summary>Aggregations: Geo bounds aggregation does not output aggregation name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.3.1</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T09:10:09Z</created><updated>2014-07-28T13:24:40Z</updated><resolved>2014-07-24T11:14:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T09:10:35Z" id="49984599">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Remove HttpClient to only use one HTTP client implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7003</link><project id="" key="" /><description>The HTTP client implementation used by the Elasticsearch REST tests is
backed by apache http client instead of a self written helper class,
that uses HttpUrlConnection. This commit removes the old simple HttpClient
class used for some plugin tests and uses the more powerful and reliable one for all tests.

It also fixes a minor bug, that when sending a 301 redirect, a Location
header needs to be added as well, which was uncovered by the switching
to the new client.
</description><key id="38608931">7003</key><summary>Test: Remove HttpClient to only use one HTTP client implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-24T08:51:23Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-07-25T08:27:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-24T10:10:19Z" id="49990103">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>1.3.0 ElasticsearchException: Failed to load logging configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7002</link><project id="" key="" /><description>LogConfigurator.configure() now requires a config file, while previously it did not.

code to reproduce:

```
        final Node node = NodeBuilder.nodeBuilder()
                .local(true)
                .build();
        LogConfigurator.configure(node.settings());
```

exception:

```
Exception in thread "main" org.elasticsearch.ElasticsearchException: Failed to load logging configuration
    at org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:117)
    at org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:81)
```
</description><key id="38597365">7002</key><summary>1.3.0 ElasticsearchException: Failed to load logging configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">brackxm</reporter><labels><label>discuss</label></labels><created>2014-07-24T04:53:02Z</created><updated>2015-11-21T16:17:39Z</updated><resolved>2015-11-21T16:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T16:17:38Z" id="158659560">Yes, this is true and will remain so.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Drupal Search API Elasticsearch module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7001</link><project id="" key="" /><description>The module at drupal.org/project/elasticsearch has been abandoned. The Search API Elasticsearch module allows Drupal to use Elasticsearch as a backend for Search API.
</description><key id="38592596">7001</key><summary>Add Drupal Search API Elasticsearch module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">VeggieMeat</reporter><labels /><created>2014-07-24T02:33:25Z</created><updated>2014-07-28T08:53:53Z</updated><resolved>2014-07-28T08:47:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-24T12:06:53Z" id="49999713">Hi @VeggieMeat 

Please can I ask you to sign our CLA so that i can merge your PR in? http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="VeggieMeat" created="2014-07-24T17:22:08Z" id="50049486">No problem. Just signed it.
</comment><comment author="clintongormley" created="2014-07-28T08:53:53Z" id="50313706">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Flow control</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/7000</link><project id="" key="" /><description>Improve flow control for testing simultaneous benchmarks.
</description><key id="38590262">7000</key><summary>Benchmarks: Flow control</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-24T01:31:53Z</created><updated>2014-07-24T01:31:57Z</updated><resolved>2014-07-24T01:31:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6999</link><project id="" key="" /><description>Run in verbose mode
</description><key id="38590106">6999</key><summary>Benchmarks: REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-24T01:28:28Z</created><updated>2014-07-24T01:28:35Z</updated><resolved>2014-07-24T01:28:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Fix REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6998</link><project id="" key="" /><description>Properly format numeric values in JSON output to be consistent floating
point format.
</description><key id="38582585">6998</key><summary>Benchmarks: Fix REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-23T23:19:54Z</created><updated>2014-07-23T23:19:58Z</updated><resolved>2014-07-23T23:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make `_source` parsing in `top_hits` aggregation consistent with the search api </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6997</link><project id="" key="" /><description /><key id="38574860">6997</key><summary>Make `_source` parsing in `top_hits` aggregation consistent with the search api </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.1</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T21:35:45Z</created><updated>2017-03-31T10:06:20Z</updated><resolved>2014-07-24T09:36:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T06:19:07Z" id="49971448">Left one comment but the change looks good to me. Feel free to push if it is not applicable.
</comment><comment author="pierrre" created="2014-07-24T09:21:01Z" id="49985547">`"_source": false` doesn't work in `top_hits` aggregation.
Is it related?

My fix:

```
            "_source": {
              "exclude": [
                "*"
              ]
            }
```
</comment><comment author="martijnvg" created="2014-07-24T09:42:39Z" id="49987626">@pierrre Yes it is related, this PR fixes this.
</comment><comment author="pierrre" created="2014-07-24T09:44:44Z" id="49987817">@martijnvg thank you :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `wait_if_ongoing` option to _flush requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6996</link><project id="" key="" /><description>This commit adds the ability to force blocking on the flush operaition
to make sure all files have been written and synced to disk. Without
this option a flush might be executing at the same time causing the
current flush to fail and return before all files being synced.
</description><key id="38566090">6996</key><summary>Add `wait_if_ongoing` option to _flush requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T20:02:36Z</created><updated>2015-06-07T12:36:04Z</updated><resolved>2014-07-24T13:44:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-23T20:08:18Z" id="49927801">LGTM, really minor comment
</comment><comment author="s1monw" created="2014-07-23T20:09:39Z" id="49927955">pushed a new commit @clintongormley can you take a look at the doc / spec changes?
</comment><comment author="clintongormley" created="2014-07-24T12:49:34Z" id="50003990">Added doc comments
</comment><comment author="s1monw" created="2014-07-24T12:54:40Z" id="50004577">@clintongormley pushed a new commit
</comment><comment author="clintongormley" created="2014-07-24T12:56:18Z" id="50005045">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exists API: Allow user to find if any document exists for a given query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6995</link><project id="" key="" /><description>The idea behind the new API is to enable fast exists functionality for documents, given a query. It would be very similar to the existing count API.

This API will be faster then using the Count API (setting terminate_after=1) in the general case, as it would return the response after the first shard reports existing documents, instead of blocking for responses from all the shards. In the worst case, if no documents actually exists, it would be as performant as using the Count API with terminate_after=1.

(Check corresponding PR for req/response)

related to #6876 
</description><key id="38564093">6995</key><summary>Exists API: Allow user to find if any document exists for a given query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T19:41:32Z</created><updated>2014-07-31T19:53:45Z</updated><resolved>2014-07-31T19:53:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Within a sub-aggregation of a reverse_nested aggregation, cannot filter on nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6994</link><project id="" key="" /><description>I'm unsure if this is a bug or something that wasn't intended to work. In the example below, I'm trying to find the 'foo' object that has the nested 'bar' object named 'bar0'. I then want to know how many nested 'baz' objects the 'foo' contains. After using reverse_nested I can use the first-level fields of 'foo0', but not the nested fields.

Using my actual data this sometimes did work, which is why I thought there might be a bug here. So for example two root objects would contain a certain nested object ('bar'), but only one was returning a proper count of another nested object (baz). With my example I wasn't able to recreate this.

```
DELETE /_all

PUT /foos

PUT /foos/foo/_mapping
{
  "foo": {
    "properties": {
      "bar": {
        "type": "nested",
        "properties": {
          "name": {
            "type": "string"
          }
        }
      },
      "baz": {
        "type": "nested",
        "properties": {
          "name": {
            "type": "string"
          }
        }
      },
      "name": {
        "type": "string"
      }
    }
  }
}

PUT /foos/foo/0
{
  "bar": [
    {
      "name": "bar0"
    },
    {
      "name": "bar1"
    }
  ],
  "baz": [
    {
      "name": "baz0"
    },
    {
      "name": "baz1"
    }
  ],
  "name": "foo0"
}
PUT /foos/foo/1
{
  "bar": [
    {
      "name": "bar2"
    },
    {
      "name": "bar3"
    }
  ],
  "baz": [
    {
      "name": "baz2"
    },
    {
      "name": "baz3"
    }
  ],
  "name": "foo1"
}

#this should give a count of 2 under baz, but it's 0 instead

POST /foos/foo/_search?pretty
{
  "size": 0,
  "aggs": {
    "bar": {
      "nested": {
        "path": "bar"
      },
      "aggs": {
        "bar_filter": {
          "filter": {
            "bool": {
              "must": {
                "term": {
                  "bar.name": "bar0"
                }
              }
            }
          },
          "aggs": {
            "foo": {
              "reverse_nested": {},
              "aggs": {
                "name": {
                  "terms": {
                    "field": "name"
                  },
                  "aggs": {
                    "baz": {
                      "nested": {
                        "path": "baz"
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}

# this works and give the right number of baz
POST /foos/foo/_search?pretty
{
  "size": 0,
  "aggs": {
    "foo_filter": {
      "filter": {
        "bool": {
          "must": {
            "term": {
              "name": "foo0"
            }
          }
        }
      },
      "aggs": {
        "baz": {
          "nested": {
            "path": "baz"
          }
        }
      }
    }
  }
}
```
</description><key id="38560716">6994</key><summary>Within a sub-aggregation of a reverse_nested aggregation, cannot filter on nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">lmeck</reporter><labels><label>bug</label></labels><created>2014-07-23T19:05:05Z</created><updated>2014-07-28T08:10:20Z</updated><resolved>2014-07-28T08:10:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-24T11:57:29Z" id="49998647">@martijnvg I've confirmed this behaviour - looks like a bug to me.
</comment><comment author="martijnvg" created="2014-07-27T19:05:32Z" id="50282403">@lmeck @clintongormley This is indeed a bug. I opened #7048 for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Investigate null Settings in mappers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6993</link><project id="" key="" /><description>In #6967 Adrien mentioned he was concerned about this, as e.g. the NumberFieldMapper has to do a null check, but it seems to me its currently designed to be null (its marked Nullable etc in some related apis). Maybe we can avoid this?
</description><key id="38552733">6993</key><summary>Investigate null Settings in mappers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-07-23T17:36:18Z</created><updated>2015-04-23T17:47:45Z</updated><resolved>2015-04-23T17:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2015-04-23T17:47:44Z" id="95668188">I fixed this in a later change. Field mappers no longer allow null index settings (or even empty). They always expect the index_created_version setting.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Add assertions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6992</link><project id="" key="" /><description>Attempt to catch where abort test is failing by adding assertions.
</description><key id="38550693">6992</key><summary>Benchmarks: Add assertions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-23T17:13:53Z</created><updated>2014-07-23T17:13:58Z</updated><resolved>2014-07-23T17:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Fix for failing test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6991</link><project id="" key="" /><description>Fixes an attempted read/write of a null value in readFrom()/writeTo().
</description><key id="38548969">6991</key><summary>Benchmarks: Fix for failing test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-23T16:54:30Z</created><updated>2014-07-23T16:54:39Z</updated><resolved>2014-07-23T16:54:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Streamline use of IndexClosedException introduced with #6475</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6990</link><project id="" key="" /><description>Single index operations to use the newly added IndexClosedException introduced with #6475. This way we can also fail faster when we are trying to execute operations on closed indices and their use is not allowed (depending on indices options). Indices blocks are still checked but we can throw error earlier on while resolving indices (MetaData#concreteIndices).

Effectively this change also affects what we return when using one of the following apis: analyze, bulk, index, update, delete, explain, get, multi_get, mlt, term vector, multi_term vector. We now return: 

```
{"error":"IndexClosedException[[test] closed]","status":403}
```

instead of 

```
{"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}
```

Closes #6988
</description><key id="38545911">6990</key><summary>Streamline use of IndexClosedException introduced with #6475</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Exceptions</label><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T16:22:54Z</created><updated>2015-06-06T16:45:35Z</updated><resolved>2014-07-24T08:40:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-23T20:11:33Z" id="49928189">Besides @s1monw comment, LGTM
</comment><comment author="javanna" created="2014-07-23T20:21:10Z" id="49929472">Agreed, thanks for the review @mvg and @s1monw , pushed a new commit that adds the missing constants. I think it's ready.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactor TransportActions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6989</link><project id="" key="" /><description>Get rid of boilerplate code for handling transport actions.
Make these transport actions extend HandledTransportAction where this code
now lives.
</description><key id="38541427">6989</key><summary>Refactor TransportActions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T15:38:45Z</created><updated>2015-06-07T12:36:14Z</updated><resolved>2014-07-24T09:14:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-23T16:01:36Z" id="49895025">LGTM, should be tagger 1.4 as well, right?
</comment><comment author="javanna" created="2014-07-23T16:02:01Z" id="49895092">I like this a lot! In fact when you introduced the `HandledTransportAction` I wondered why you didn't use it all over the place ;)
We might be able to do the same in some other places I think, like for instance `TransportIndexReplicationOperationAction` and probably others? I would try and go over all the transport actions and see whether we can do the same, I think it can go on 1.4 too without bw comp problems.
</comment><comment author="s1monw" created="2014-07-23T16:40:40Z" id="49900379">I love the stats!! LGTM
</comment><comment author="GaelTadh" created="2014-07-24T09:37:44Z" id="49987151">@javanna I'll do the others in a separate change.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: streamline use of IndexClosedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6988</link><project id="" key="" /><description>With #6475 we introduced the use of a new `IndexClosedException` that gets thrown whenever we refer to closed indices within apis that don't allow them (behaviour can also be changed through indices options in some cases). We should streamline the use of this exception and use it within single index apis like index api, which currently return an index block during the execution while we could throw the newly added `IndexClosedException` instead.
</description><key id="38539985">6988</key><summary>Internal: streamline use of IndexClosedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T15:24:37Z</created><updated>2014-07-24T08:40:34Z</updated><resolved>2014-07-24T08:40:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T15:39:19Z" id="49891701">@javanna I've labelled this as adoptme, unless you're planning on doing it? if so, could you assign it to yourself please?
</comment><comment author="javanna" created="2014-07-23T15:39:53Z" id="49891779">Working on it @clintongormley ;)
</comment><comment author="javanna" created="2014-07-23T16:25:19Z" id="49898253">Marking this as breaking as this change affects what we return when using one of the following apis against closed indices: analyze, bulk, index, update, delete, explain, get, multi_get, mlt, term vector, multi_term vector. We now return: 

```
{"error":"IndexClosedException[[test] closed]","status":403}
```

instead of 

```
{"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Fix nested root object indexing documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6987</link><project id="" key="" /><description>Types can no longer be specified when indexing, see: #4552
</description><key id="38533922">6987</key><summary>Docs: Fix nested root object indexing documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T14:27:08Z</created><updated>2014-09-09T13:52:42Z</updated><resolved>2014-07-23T16:35:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T15:17:42Z" id="49888440">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES does not bind to interface specified in network.host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6986</link><project id="" key="" /><description>ES 1.2.2 binds to more interfaces than specified in ES/elasticsearch.yml.

Non-default contents of ES/elaticsearch.yml:

node.name: elasticsearch-me
network.host: zzz.zzz.zzz.150
http.host: zzz.zzz.zzz.150
script.disable_dynamic: true

ip addr for this machine:

: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN 
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
2: pub: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether 00:a0:d1:e1:df:2c brd ff:ff:ff:ff:ff:ff
    inet xxx.xxx.xxx.71/17 brd xxx.xxx.xxx.255 scope global pub
    inet xxx.xxx.xxx.111/17 brd xxx.xxx.xxx.255 scope global secondary pub:dns-name1
    inet xxx.xxx.xxx.207/17 brd xxx.xxx.xxx.255 scope global secondary pub:dns-name2
    inet xxx.xxx.xxx.128/17 brd xxx.xxx.xxx.255 scope global secondary pub:dns-name3
    inet xxx.xxx.xxx.214/17 brd xxx.xxx.xxx.255 scope global secondary pub:dns-name4
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000
    link/ether 00:a0:d1:e1:df:2d brd ff:ff:ff:ff:ff:ff
4: it1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 00:1b:21:97:37:98 brd ff:ff:ff:ff:ff:ff
5: it2: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN qlen 1000
    link/ether 00:1b:21:97:37:99 brd ff:ff:ff:ff:ff:ff
6: eth1.2@eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP 
    link/ether 00:a0:d1:e1:df:2d brd ff:ff:ff:ff:ff:ff
    inet zzz.zzz.zzz.59/24 brd zzz.zzz.zzz.255 scope global eth1.2
    inet zzz.zzz.zzz.1/24 brd zzz.zzz.zzz.255 scope global secondary eth1.2:dns-name5
    inet zzz.zzz.zzz.150/24 brd zzz.zzz.zzz.255 scope global secondary eth1.2:dns-name1
    inet zzz.zzz.zzz.105/24 brd zzz.zzz.zzz.255 scope global secondary eth1.2:dns-name2
    inet zzz.zzz.zzz.92/24 brd zzz.zzz.zzz.255 scope global secondary eth1.2:dns-name3
7: eth1.3@eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP 
    link/ether 00:a0:d1:e1:df:2d brd ff:ff:ff:ff:ff:ff
    inet yyy.yyy.yyy.59/24 brd yyy.yyy.yyy.255 scope global eth1.3
    inet yyy.yyy.yyy.1/24 brd yyy.yyy.yyy.255 scope global secondary eth1.3:dns-name5
    inet yyy.yyy.yyy.150/24 brd yyy.yyy.yyy.255 scope global secondary eth1.3:dns-name1
    inet yyy.yyy.yyy.105/24 brd yyy.yyy.yyy.255 scope global secondary eth1.3:dns-name2
    inet yyy.yyy.yyy.92/24 brd yyy.yyy.yyy.255 scope global secondary eth1.3:dns-name3

netstat -tupan | grep elasticsearch-pid:

tcp        0      0 0.0.0.0:9200            0.0.0.0:\*               LISTEN      2756/java
tcp        0      0 0.0.0.0:9300            0.0.0.0:\*               LISTEN      2756/java
tcp        0      0 xxx.xxx.xxx.71:38883    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38887    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38888    zz.zzz.zzz.214:9300     ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38887    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38890    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38879    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38880    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38889    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38881    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38888    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38885    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38878    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38890    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38881    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38883    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38879    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38884    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38884    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38886    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38882    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38889    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38878    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38885    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 zzz.zzz.zzz.214:9300    xxx.xxx.xxx.71:38880    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38886    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
tcp        0      0 xxx.xxx.xxx.71:38882    zzz.zzz.zzz.214:9300    ESTABLISHED 2756/java  
udp        0      0 0.0.0.0:54328           0.0.0.0:\*               2756/java       

Same results for setting network.host to 127.0.0.1, or _eth1.2_

I'm a little surprised that it doesn't bind to zzz.zzz.zzz.150.
</description><key id="38533691">6986</key><summary>ES does not bind to interface specified in network.host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gholder</reporter><labels><label>adoptme</label></labels><created>2014-07-23T14:24:46Z</created><updated>2015-11-21T16:17:04Z</updated><resolved>2015-11-21T16:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T16:17:04Z" id="158659523">Fixed in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better JSON output scoping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6985</link><project id="" key="" /><description>Before this change each aggregation had to output an object field with its name and write its JSON inside that object.  This allowed for badly behaved aggregations which could write JSON content in the root of the 'aggs' object.  this change move the writing of the aggregation name to a level above the aggregation itself, ensuring that aggregations can only write within there own scope in the JSON output.

Closes #7004
</description><key id="38533410">6985</key><summary>Better JSON output scoping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T14:21:43Z</created><updated>2015-06-07T12:36:26Z</updated><resolved>2014-07-24T11:14:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-23T14:22:46Z" id="49880294">This PR supersedes #6982 as it also fixes that bug. Maybe #6982 should go into 1.3.1 since its a true bug fix and this should go into 1.4 and 2.0 as its more of an enhancement?
</comment><comment author="jpountz" created="2014-07-23T15:35:24Z" id="49891154">+1 on enforcing the serialization of the name.

Regarding the impl, I think it would be better to do the same thing that is done for the mappers: the base class has a `toXContent` method (that is not final because of some mappers but should be) that writes the name but then delegate to a `doXContentBody` method that sub-classes can extend.

This way, if we get other call sites of Aggregation.toXContent in the future, they won't need to care about serializing the agg name themselves.

+1 on fixing 1.3 as well but I think both changes should reference the same issue.
</comment><comment author="colings86" created="2014-07-24T09:11:26Z" id="49984675">Fixes #7004
</comment><comment author="jpountz" created="2014-07-24T10:28:04Z" id="49991608">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made the handeling of the join request batch oriented.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6984</link><project id="" key="" /><description>In large clusters when a new elected master is chosen, there are many join requests to handle. By batching them up the the cluster state doesn't get published for each individual join request, but many handled at the same time, which results into a single new cluster state which ends up be published.

This PR is for the improve zen branch.
</description><key id="38532610">6984</key><summary>Made the handeling of the join request batch oriented.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>resiliency</label></labels><created>2014-07-23T14:13:03Z</created><updated>2015-05-18T23:30:46Z</updated><resolved>2014-07-23T19:47:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-23T15:38:24Z" id="49891570">@martijnvg wondering about the need for another thread? can we maybe do something similar to what we do with things like shard started events, or refresh/update mappings? so there isn't another thread, just the cluster state task drain the queue and process it?
</comment><comment author="martijnvg" created="2014-07-23T15:49:25Z" id="49893204">@kimchy I was not 100% sure about the separate thread as well. I'll change it to just drain the queue and process it on the same thread.
</comment><comment author="martijnvg" created="2014-07-23T18:31:22Z" id="49915322">@kimchy I updated the PR to batch process on the same thread.
</comment><comment author="kimchy" created="2014-07-23T19:16:01Z" id="49921252">left minor comments, other than than, LGTM
</comment><comment author="martijnvg" created="2014-07-23T19:23:09Z" id="49922138">@kimchy Updated the PR with your comments.
</comment><comment author="kimchy" created="2014-07-23T19:27:07Z" id="49922623">LGTM
</comment><comment author="martijnvg" created="2014-07-23T19:47:56Z" id="49925217">Closed via: https://github.com/elasticsearch/elasticsearch/commit/1d0b32b4433066608429aca0bdd17f7111e4e50b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[doc] Correct decay function equations in function_score description</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6983</link><project id="" key="" /><description>Impact of decay and scale was missing from the equations.
</description><key id="38530975">6983</key><summary>[doc] Correct decay function equations in function_score description</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>docs</label></labels><created>2014-07-23T13:54:52Z</created><updated>2015-02-25T11:33:26Z</updated><resolved>2014-07-23T15:34:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-07-23T13:55:07Z" id="49876434">@clintongormley check it out!
</comment><comment author="clintongormley" created="2014-07-23T15:31:24Z" id="49890519">LGTM - thanks @brwe!
</comment><comment author="matiastealdi" created="2015-02-24T19:46:05Z" id="75830854">@brwe 
How are you generating the equation images? 
Exponential.png has a typo (minus before lambda should not be there).  I would like to make a pull request replacing the image with the right one and adding value = max(0, mod(value_doc - origin) - offset) to the equation. I wanted to reproduce the plot and took me a while to figure out the final equation.

Thanks.
</comment><comment author="brwe" created="2015-02-25T11:33:26Z" id="75945788">@matiastealdi I used http://www.codecogs.com/latex/eqneditor.php For the exp decay I typed 

S(doc) = \exp(-\lambda|fieldvalue_{doc}-origin|)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Fix JSON output of geo bounds aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6982</link><project id="" key="" /><description>The geo bounds aggregation was not outputting the aggregation name and in the case where there was no data would output malformed JSON (extra } )
</description><key id="38530271">6982</key><summary>Aggregations: Fix JSON output of geo bounds aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-07-23T13:47:19Z</created><updated>2014-08-21T15:07:40Z</updated><resolved>2014-07-24T12:54:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-24T09:10:58Z" id="49984635">Fixes #7004
</comment><comment author="jpountz" created="2014-07-24T12:42:51Z" id="50003344">LGTM
</comment><comment author="colings86" created="2014-07-24T12:54:16Z" id="50004529">pushed to 1.3 branch only as #6985 fixes this in 1.x and master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fielddata: Remove custom comparators and use Lucene's instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6981</link><project id="" key="" /><description>This commit removes custom comparators in favor of the ones that are in Lucene.

The major change is for nested documents: instead of having a comparator wrapper
that deals with nested documents, this is done at the fielddata level by having
a selector that returns the value to use for comparison.

Sorting with custom missing string values might be slower since it is using
TermValComparator since Lucene's TermOrdValComparator only supports sorting
missing values first or last. But other than this particular case, this change
will allow us to benefit from improvements on comparators from the Lucene side.

Close #5980
</description><key id="38530107">6981</key><summary>Fielddata: Remove custom comparators and use Lucene's instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T13:45:26Z</created><updated>2015-06-07T12:47:58Z</updated><resolved>2014-07-23T18:11:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-23T14:16:24Z" id="49879394">I left a bunch of cosmetic comments - this looks very clean and a nice reduction of complexity :)
</comment><comment author="jpountz" created="2014-07-23T15:49:46Z" id="49893261">@s1monw pushed a new commit to address your comments
</comment><comment author="s1monw" created="2014-07-23T16:45:01Z" id="49900981">LGTM maybe @rmuir can take another look?
</comment><comment author="rmuir" created="2014-07-23T17:20:30Z" id="49905913">Awesome, this looks great. I just added minor questions/comments/TODOs, but i think its ready.
</comment><comment author="jpountz" created="2014-07-23T18:02:23Z" id="49911450">@rmuir pushed a new commit
</comment><comment author="rmuir" created="2014-07-23T18:07:35Z" id="49912125">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added pre and post offset to histogram aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6980</link><project id="" key="" /><description>Added preOffset and postOffset parameters to the API for the histogram aggregation which work in the same way as in the date histogram

Closes #6605
</description><key id="38526712">6980</key><summary>Added pre and post offset to histogram aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T13:05:09Z</created><updated>2015-06-06T18:28:19Z</updated><resolved>2014-07-24T13:34:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T13:30:13Z" id="50017073">LGTM
</comment><comment author="colings86" created="2014-07-24T13:34:52Z" id="50017873">pushed to master and 1.x
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for the `_name` parameter to the `simple_query_string` query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6979</link><project id="" key="" /><description /><key id="38526552">6979</key><summary>Add support for the `_name` parameter to the `simple_query_string` query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Query DSL</label><label>enhancement</label><label>v1.3.2</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T13:02:54Z</created><updated>2015-06-07T12:36:57Z</updated><resolved>2014-07-29T11:08:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-28T08:27:21Z" id="50311489">LGTM
</comment><comment author="martijnvg" created="2014-07-28T14:31:21Z" id="50345803">LGTM
</comment><comment author="martijnvg" created="2014-07-28T14:31:52Z" id="50345884">maybe should be backported to the 1.3 branch as well?
</comment><comment author="dakrone" created="2014-07-28T17:45:15Z" id="50372262">&gt; maybe should be backported to the 1.3 branch as well?

Is not supporting the `_name` feature a bug though, or just a missing feature?
</comment><comment author="martijnvg" created="2014-07-28T18:51:12Z" id="50381469">I lean towards a bug, since any query/filter should support since it named queries has been introduced.
</comment><comment author="s1monw" created="2014-07-28T18:56:06Z" id="50382160">yeah I think this can go into `1.3.2`
</comment><comment author="dakrone" created="2014-07-29T11:08:03Z" id="50462782">Merged into 1.3, 1.x, and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportInstanceSingleOperation retry fails if shard is null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6978</link><project id="" key="" /><description>Around line 211 in TransportInstanceSingleOperation handleException in the ResponseHandler. If the node is null the retry will fail, this may have be caused due to the shard being moved to another node.

A log from a failed test is available at : https://gist.githubusercontent.com/GaelTadh/5528eddfddc4663ce0c9/raw/failed%20log
</description><key id="38525302">6978</key><summary>TransportInstanceSingleOperation retry fails if shard is null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-07-23T12:46:22Z</created><updated>2015-11-21T16:16:46Z</updated><resolved>2015-11-21T16:16:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="GaelTadh" created="2014-07-23T13:03:34Z" id="49870115">Here is the reproduce line : 

```
mvn test -Dtests.seed=727EF7B39446CBEC -Dtests.class=org.elasticsearch.update.UpdateTests -Dtests.prefix=tests -Dtests.slow=true -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="stressUpdateDeleteConcurrency" -Des.logger.level=DEBUG -Des.node.mode=network -Dtests.security.manager=true -Dtests.nightly=true -Dtests.heap.size=512m -Dtests.bwc.path=/home/jenkins/workspace/es_core_13_debian/jdk/random/label/debian/backwards -Dtests.jvm.argline="-server -XX:+UseParallelGC -XX:+UseCompressedOops" -Dtests.processors=8
```

I have not been able to repro this on ubuntu.
</comment><comment author="clintongormley" created="2015-11-21T16:16:46Z" id="158659508">Long time since we've seen this. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6977</link><project id="" key="" /><description>example fails in bash
</description><key id="38517247">6977</key><summary>typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">missinglink</reporter><labels /><created>2014-07-23T10:40:23Z</created><updated>2014-07-23T12:15:02Z</updated><resolved>2014-07-23T10:44:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T10:44:00Z" id="49858384">thanks @missinglink - merged
</comment><comment author="missinglink" created="2014-07-23T12:15:02Z" id="49865503">heya @clintongormley 

there are still a bunch of errors with the way the curl commands are written across the docs, as I'm too lazy to check them I wrote a spider this morning to run around and check them for me.

It's pretty boshed together but it works:
https://gist.github.com/missinglink/72bc85c57c34e53e0c09

hopefully you can find some use for it. The syntax stuff is not a big deal but sometimes the json is missing commas and the -d arg is missing from POST/PUT and that must be frustrating for newbies.

-P
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes parse error with complex shapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6976</link><project id="" key="" /><description>The bug reproduces when the point under test for the placement of the hole of the polygon has an x coordinate which only intersects with the ends of edges in the main polygon. The previous code threw out these cases as not relevant but an intersect at 1.0 of the distance from the start to the end of an edge is just as valid as an intersect at any other point along the edge.  The fix corrects this and adds a test.

Closes #5773
</description><key id="38516187">6976</key><summary>Fixes parse error with complex shapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T10:25:20Z</created><updated>2015-06-07T19:16:11Z</updated><resolved>2014-07-24T14:19:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-24T07:13:25Z" id="49974746">LGTM

It would be nice to also fix the comparison of the outputs of Double.compare as mentioned in the issue.
</comment><comment author="colings86" created="2014-07-24T14:14:00Z" id="50023249">@jpountz Agreed it would be nice to fix the Double.compare. However I am having trouble working out what it is supposed to be achieving.  It looks like it was probably put in as a bug fix but there don't seem to be any tests that rely on it. I don't want to just change it or remove it without some understanding since it could break something, although I don't know what.
</comment><comment author="jpountz" created="2014-07-24T14:15:08Z" id="50023407">Let's push this PR and create a new issue for that suspicious line of code?
</comment><comment author="colings86" created="2014-07-24T14:25:41Z" id="50024826">opened #7016 for the Double.compare issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Maven reproductions should always include 'clean' target</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6975</link><project id="" key="" /><description /><key id="38513527">6975</key><summary>Test: Maven reproductions should always include 'clean' target</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T09:48:31Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-07-23T09:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-23T09:49:28Z" id="49853601">LGTM

Although I will not copy the `clean` most of time, I think it's good to be on the safe side.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added Filters aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6974</link><project id="" key="" /><description>A multi-bucket aggregation where multiple filters can be defined (each filter defines a bucket). The buckets will collect all the documents that match their associated filter.

This aggregation can be very useful when one wants to compare analytics between different criterias. It can also be accomplished using multiple definitions of the single filter aggregation, but here, the user will only need to define the sub-aggregations only once.

(this is a continuation of Pull Request #6119)

Closes #6118,#6119
</description><key id="38510534">6974</key><summary>Added Filters aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T09:07:24Z</created><updated>2015-06-06T18:28:28Z</updated><resolved>2014-08-01T15:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-23T16:38:47Z" id="49900115">LGTM, @rashidkpc mind looking at it for the proposed API format?
</comment><comment author="rashidkpc" created="2014-07-23T16:41:49Z" id="49900551">API looks good so far. I'm going to build this, try to integrate it into Kibana 4 and see how it goes.
</comment><comment author="rashidkpc" created="2014-07-23T21:13:15Z" id="49936517">Ok, I prototyped this out and got it working in Kibana 4. The only input I have into the API format is that it is, slightly, inconsistent with how we do other aggregations that require the buckets to be explicitly defined. The currently proposed filters agg format looks like this:

```
{
    "aggs" : {
        "messages" : {
            "filters" : {
                "errors" : { "query" : { "match" : { "body" : "error" } } },
                "warnings" : { "query" : { "match" : { "body" : "warning" } } }
            },
            "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }
        }
    }
}
```

Its a bit awkward, since the keys of the of filters agg are not configuration parameters, but rather names. We don't do that anywhere else. The only other place were we have explicit definitions of buckets like this in in the range agg, where we use an array to define the ranges:

```
{
    "aggs" : {
        "price_ranges" : {
            "range" : {
                "field" : "price",
                "ranges" : [
                    { "to" : 50 },
                    { "from" : 50, "to" : 100 },
                    { "from" : 100 }
                ]
            }
        }
    }
}
```

 I wonder if we should do the same thing, and return the buckets as an array, like the range agg does. Granted if we did this with the filters agg we'd end up with filters.filters, which is also weird:

```
{
    "aggs" : {
        "messages" : {
            "filters" : {
                "filters" : [
                    { "query" : { "match" : { "body" : "error" } } },
                    { "query" : { "match" : { "body" : "warning" } } }
                ]
            }
            "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }
        }
    }
}
```

Of course, you bring up a good point, people might want the response to be keyed. In which case, we could again use the convention defined by the range agg, using the keyed property (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-range-aggregation.html#_keyed_response)

If we wanted to do the same thing in the filters agg we could use the _name convention from named filters/queries (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html). Eg:

```
{
    "aggs" : {
        "messages" : {
            "filters" : {
                "filters" : [
                    { "query" : { "match" : { "body" : "error", "_name": "errors" } } },
                    { "query" : { "match" : { "body" : "warning", "_name": "warnings" } } }
                ]
            }
            "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }
        }
    }
}
```

I don't love the filters.filters thing, but it is consistent.
</comment><comment author="clintongormley" created="2014-07-24T12:03:37Z" id="49999338">@rashidkpc I'm not sure we need the `filters.filters`.  `filters` can either have an array as value, in which case it is ordered, or an object, in which case it is keyed.  See https://github.com/elasticsearch/elasticsearch/pull/6119#issuecomment-46562440

This assumes that Jackson and client languages can differentiate.  @polyfractal - I know that PHP does some funky things with converting hashes to arrays, but I think that's only for empty hashes?
</comment><comment author="clintongormley" created="2014-07-25T05:36:53Z" id="50109734">Hmm, I may have to eat my words about not requiring the `filters.filters` setting.  See #7020 
</comment><comment author="colings86" created="2014-07-28T11:19:44Z" id="50326545">Updated so that you request is filters.filters again and can support providing an array of anonymous filters in the request which are returned in the same order in the response.  Not sure if I like it however, since unlike the histogram aggregator the 'keyed: false' option does not have a key field which links back to the request so the only link between the bucket in the response and the filter in the request is its position in the array
</comment><comment author="colings86" created="2014-07-28T11:20:15Z" id="50326579">cc @rashidkpc @clintongormley 
</comment><comment author="clintongormley" created="2014-07-28T11:42:54Z" id="50328229">@colings86 (Ignore my comments about #7020 )

Not sure we need the double level `filters.filters`?  Surely just `filters: {....}` or `filters: [...]` would be sufficient?
</comment><comment author="clintongormley" created="2014-07-28T12:24:11Z" id="50331416">@rashidkpc can you confirm that you'd be happy with support for these two syntax options?

Results returned with keys:

```
"aggs" : {
    "messages" : {
        "filters" : {
            "errors" : { "query" : { "match" : { "body" : "error" } } },
            "warnings" : { "query" : { "match" : { "body" : "warning" } } }
        }
    },
    "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }

}
```

Results returned as an array:

```
"aggs" : {
    "messages" : {
        "filters" : [
            { "query" : { "match" : { "body" : "error" } }, "name": "errors"},
            { "query" : { "match" : { "body" : "warning" } }, "name" : "warnings" }
        ]
    },
    "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }        
}
```
</comment><comment author="rashidkpc" created="2014-07-28T14:14:36Z" id="50343487">After reviewing the range aggregation, I understand why the syntax is how it is. I think the syntax as proposed by @clintongormley will be fine for the filters aggregation. `filters.filters` is probably not required here unless we plan to add any other parameters to the aggregation.
</comment><comment author="clintongormley" created="2014-07-28T14:15:09Z" id="50343573">w00t
</comment><comment author="colings86" created="2014-07-30T11:43:53Z" id="50604073">Tried to implement this but the aggregation framework throws an error if the aggregation is not an object so does not work for the Array case.  At least for the array case we may need to go back to having filters.filters as it doesn't seem correct to change the agg framework for what seems like a special case here.

thoughts?
</comment><comment author="colings86" created="2014-07-30T12:17:11Z" id="50606707">because of the above issue the syntax is going to stay as the following:

Results returned with keys:

```
"aggs" : {
    "messages" : {
        "filters" : {
            "filters" : {
                "errors" : { "query" : { "match" : { "body" : "error" } } },
                "warnings" : { "query" : { "match" : { "body" : "warning" } } }
            }
        }
    },
    "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }

}
```

Results returned as an array:

```
"aggs" : {
    "messages" : {
        "filters" : {
            "filters" : [
                { "query" : { "match" : { "body" : "error" } }, "name": "errors"},
                { "query" : { "match" : { "body" : "warning" } }, "name" : "warnings" }
            ]
        }
    },
    "aggs" : { "monthly" : { "histogram" : { "field" : "timestamp", "interval" : "1M" } } }        
}
```
</comment><comment author="jpountz" created="2014-07-30T13:21:02Z" id="50612856">@colings86 Left some comments.
</comment><comment author="colings86" created="2014-07-31T12:28:00Z" id="50751381">@jpountz I implemented thhe changes you suggested and also I added tests and documentation for array type request as well as updating the Builder to be able to construct array requests since I realised these were missing
</comment><comment author="jpountz" created="2014-08-01T13:41:52Z" id="50885038">LGTM
</comment><comment author="JasonPunyon" created="2014-08-06T16:22:28Z" id="51359369">https://twitter.com/JasonPunyon/status/497052551003377664
</comment><comment author="clintongormley" created="2014-08-07T12:25:18Z" id="51464379">@JasonPunyon fixed, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add parameter to GET API for checking if generated fields can be retrieved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6973</link><project id="" key="" /><description>Fields of type `token_count`, `murmur3`, `_all` and `_field_names` are generated only when indexing.
If a GET requests accesses the transaction log (because no refresh
between indexing and GET request) then these fields cannot be retrieved at all.
Before the behavior was so:

`_all, _field_names`: The field was siletly ignored
`murmur3, token_count`: `NumberFormatException` because GET tried to parse the values from the source.

In addition, if these fields were not stored, the same behavior occured if the fields were
retrieved with GET after a `refresh()` because here also the source was used to get the fields.

Now, GET accepts a parameter `ignore_errors_on_generated_fields` which has
the following effect:
- Throw exception with meaningful error message explaining the problem if set to false (default)
- Ignore the field if set to true
- Always ignore the field if it was not set to stored

This changes the behavior for `_all` and `_field_names` as now an Exception is thrown if a user
tries to GET them before a `refresh()`.

closes #6676
</description><key id="38505951">6973</key><summary>Add parameter to GET API for checking if generated fields can be retrieved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:CRUD</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-23T07:57:13Z</created><updated>2015-06-07T19:16:56Z</updated><resolved>2014-08-04T06:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-23T08:59:04Z" id="49848942">I left some comments - looks good
</comment><comment author="brwe" created="2014-07-24T13:11:52Z" id="50013646">implemented all changes
</comment><comment author="jpountz" created="2014-08-01T13:51:02Z" id="50886065">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update names.txt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6972</link><project id="" key="" /><description>Added a friends name to the file. Hope it's ok with you guys.
</description><key id="38461929">6972</key><summary>Update names.txt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">malnahlawi</reporter><labels /><created>2014-07-22T23:04:34Z</created><updated>2014-07-23T10:32:14Z</updated><resolved>2014-07-23T10:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T10:32:14Z" id="49857424">Sorry :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>retry logic to unwrap exception to check for illegal state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6971</link><project id="" key="" /><description>it probably comes wrapped in a remote exception, which we should unwrap in order to detect it..., also, simplified a bit the retry logic

Note, this is against improve_zen branch
</description><key id="38460406">6971</key><summary>retry logic to unwrap exception to check for illegal state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-07-22T22:51:49Z</created><updated>2014-08-18T22:05:21Z</updated><resolved>2014-07-22T22:59:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-22T22:58:19Z" id="49812798">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better join retry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6970</link><project id="" key="" /><description>Note, this is against improve_zen branch
</description><key id="38460110">6970</key><summary>Better join retry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-07-22T22:49:06Z</created><updated>2014-08-18T22:05:21Z</updated><resolved>2014-07-22T22:49:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-22T22:49:49Z" id="49812097">failed pull request :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Discovery] verify we have a master after a successful join request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6969</link><project id="" key="" /><description>After master election, nodes send join requests to the elected master. Master is then responsible for publishing a new cluster state which sets the master on the local node's cluster state. If something goes wrong with the cluster state publishing, this process will not successfully complete. We should check it after the join request returns and if it failed, retry pinging.
</description><key id="38454461">6969</key><summary>[Discovery] verify we have a master after a successful join request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>resiliency</label></labels><created>2014-07-22T22:05:21Z</created><updated>2014-07-22T22:35:28Z</updated><resolved>2014-07-22T22:35:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-22T22:11:50Z" id="49808575">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Add simple sort assertions for bwc tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6968</link><project id="" key="" /><description>Today we only do count searches to ensure sane results are returned
after upgrading etc. This change adds sorting to the picture asserting
on simple numeric sorting that uses field data etc. after upgrading.

Relates to #6967
</description><key id="38440304">6968</key><summary>Test: Add simple sort assertions for bwc tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T20:25:34Z</created><updated>2015-06-07T11:46:37Z</updated><resolved>2014-07-22T20:51:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-22T20:27:52Z" id="49795854">looks great, thanks!
</comment><comment author="jpountz" created="2014-07-22T20:51:39Z" id="49799028">Thanks for doing it!
</comment><comment author="s1monw" created="2014-07-22T20:52:00Z" id="49799078">YW
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change numeric data types to use SORTED_NUMERIC docvalues type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6967</link><project id="" key="" /><description>Change numeric data types to use SORTED_NUMERIC docvalues type instead of a custom encoding in BINARY.

In low level benchmarks this is 2x to 5x faster: its also optimized for the common case where fields actually only contain at most one value for each document.

Additionally SORTED_NUMERIC doesn't lose values if they appear more than once, so mathematical computations such as averages are correct.
</description><key id="38431070">6967</key><summary>Change numeric data types to use SORTED_NUMERIC docvalues type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T18:49:13Z</created><updated>2015-06-07T12:37:22Z</updated><resolved>2014-07-23T18:56:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-22T19:26:55Z" id="49788072">I did a review and it looks great. One thing that I really would want to see here is a BWC test that creates &amp; uses the numeric variants with DV on a mixed version cluster and then upgrades the cluster and checks if we are still operating fine. One way of doing this is to simply add some sorting with doubles / longs to `BasicBackwardsCompatibilityTest#testIndexRollingUpgrade` as well as `BasicBackwardsCompatibilityTest#testIndexAndSearch` the dynamic index template should take care of randomly selecting docvalues there so simple sorting or faceting should do the job
</comment><comment author="rmuir" created="2014-07-23T03:29:06Z" id="49829171">@jpountz spotted a horrible backwards break here, I'm too used to the lucene system. I will make sure back compat tests are failing with the current PR first and then update...
</comment><comment author="rmuir" created="2014-07-23T16:22:41Z" id="49897919">I fixed the back compat: backwards tests pass now.
</comment><comment author="s1monw" created="2014-07-23T16:43:30Z" id="49900782">LGTM @jpountz can you take another look please 
</comment><comment author="jpountz" created="2014-07-23T16:43:55Z" id="49900845">I just started. :)
</comment><comment author="jpountz" created="2014-07-23T16:50:57Z" id="49901900">LGTM
</comment><comment author="jpountz" created="2014-07-23T16:51:24Z" id="49901965">sorry, closed by mistake. Just reopened
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>With unicast discovery, only disconnect from temporary connected nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6966</link><project id="" key="" /><description>In unicast discovery, we try to reuse existing discovery nodes based on the node address they have. If we find an existing node based on its address, and for some reason its not connected, don't add it to the list of nodes to disconnect from, as that (full) connection is useful down the road
 Also, fixed a rogue count down on a latch in case the ping cycle has ended (in practice, it didn't do any harm, since the latch has timed out already by definition, but still cleaner)
</description><key id="38424865">6966</key><summary>With unicast discovery, only disconnect from temporary connected nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Discovery</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T17:45:00Z</created><updated>2015-06-07T12:37:33Z</updated><resolved>2014-07-22T19:30:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-22T19:30:03Z" id="49788496">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[date_histogram aggregation] OutOfMemoryError when using "time_zone" attribute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6965</link><project id="" key="" /><description>Elasticsearch version : 1.2.2
Operation : date_histogram aggregation

In date_histogram aggregation [(documentation)](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#search-aggregations-bucket-datehistogram-aggregation), when I use the "time_zone" attribute, I have an OutOfMemoryError (see response below).

```
{
   "error": "ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: OutOfMemoryError[Java heap space]; ",
   "status": 503
}
```

The complete stacktrace is: 

```
[2014-07-22 19:01:15,800][DEBUG][action.search.type       ] [Briquette] failed to reduce search
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] 
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:140)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$1.onResult(TransportSearchQueryThenFetchAction.java:112)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$1.onResult(TransportSearchQueryThenFetchAction.java:106)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:526)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram.createBucket(InternalDateHistogram.java:146)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram.createBucket(InternalDateHistogram.java:36)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.reduce(InternalHistogram.java:324)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:545)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:151)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:138)
    ... 6 more
```

Query detail : 

```
POST index/type/_search
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      }
    }
  },
  "aggregations": {
    "tendance": {
      "date_histogram": {
        "field": "date_creation",
        "interval": "month",
        "min_doc_count": 0,
        "time_zone": 1,
        "extended_bounds": {
          "min": "2013-01-01T00:00:00.000+01:00",
          "max": "2013-01-31T23:59:59.999+01:00"
        }
      }
    }
  }
}
```

With `"time_zone": "Europe/Paris"`, I have the same error.
With `"time_zone": 0`, everything is good and the query tooks 19ms.

The document mapping is:

```
"mappings":{  
      "marche_prive":{  
         "properties":{  
            "date_creation":{  
               "include_in_all":false,
               "store":true,
               "format":"dateOptionalTime",
               "type":"date"
            }
         }
      }
   }
```

I haven't any ideas of what happens! 
</description><key id="38422140">6965</key><summary>[date_histogram aggregation] OutOfMemoryError when using "time_zone" attribute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">boillodmanuel</reporter><labels /><created>2014-07-22T17:16:24Z</created><updated>2014-08-07T22:55:29Z</updated><resolved>2014-08-01T07:08:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="boillodmanuel" created="2014-07-22T17:24:03Z" id="49771292">The java heap histogram shows a very high number of InternalDateHistogram$Bucket instances:

```
 num     #instances         #bytes  class name
----------------------------------------------
   1:      45469412     1818776480  org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram$Bucket
```
</comment><comment author="boillodmanuel" created="2014-07-23T12:17:12Z" id="49865689">Without using "min_doc_count" attribute, the query ends normally. 

```
"date_histogram": {
        "field": "date_creation",
        "interval": "month",
        "min_doc_count": 0,
        "time_zone": 1
      }
```

does not work, but

```
"date_histogram": {
        "field": "date_creation",
        "interval": "month",
        "time_zone": 1
      }
```

works fine
</comment><comment author="jpountz" created="2014-07-24T15:07:50Z" id="50030923">Thanks for the report, this looks like an infinite loop in the reduce logic. I will look into it...
</comment><comment author="annirvine" created="2014-08-01T20:41:15Z" id="50932351">Do you know if this was a bug in 1.1 also and, if so, if it has been fixed there as well?
</comment><comment author="wojcikstefan" created="2014-08-06T22:30:32Z" id="51406720">Hi there, I encountered the same issue in ES 1.1.2.
</comment><comment author="wojcikstefan" created="2014-08-07T22:55:28Z" id="51543512">@jpountz, are you planning to include this fix in the `1.2` branch as well?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix connect concurrency, can cause connection nodes to close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6964</link><project id="" key="" /><description>Looking at the connect code, if 2 threads at the same time try and connect to a node, and both enter sequentially the connectLock code block, the second one would try and put the connection in the map, and close the replaced channels, which will cause the existing connection to close as well (since it removes the node from the connectedNodes map)
To fix this, simply make sure we properly check the existence of the connection within the connectionLock block, so there won't be concurrent connections going on.
While doing this, also went over all the mutation code that handles disconnections, and made sure they are properly done only within a connection lock.
</description><key id="38418025">6964</key><summary>Fix connect concurrency, can cause connection nodes to close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T16:34:10Z</created><updated>2015-06-07T19:17:08Z</updated><resolved>2014-07-22T17:49:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-22T17:17:40Z" id="49770295">Left two minor comments. LGTM.
</comment><comment author="andreasch" created="2015-03-24T05:39:27Z" id="85348826">Would this issue cause intermittent exceptions with the following messages?

org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes were available 
or
org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available

We are running ES 1.3.2 and seeing intermittent cases where we get these 2 exceptions on the JVMs that run the transport client even though there is no load on the cluster or any sort of networking issues between the JVM running the transport client and cluster.
</comment><comment author="bleskes" created="2015-03-24T12:19:20Z" id="85475300">@andreasch I think it might, if you only have one node configured in your transport client? also if you turn on debugging logging it should see some complaints about node not connected.
</comment><comment author="andreasch" created="2015-03-24T17:38:14Z" id="85612821">@bleskes 
We do only have one node in our transport client (points to a VIP/load balancer).

"Node not connected" seems to be the root exception message for all of our exceptions with the message "org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available"
</comment><comment author="bleskes" created="2015-03-24T19:26:36Z" id="85658218">Ok. sounds plausible then. But as these things go - it's hard to be sure.. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Drop UnsafeUtils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6963</link><project id="" key="" /><description>This class potentially does unaligned memory access and does not bring much
now that we switched to global ords for terms aggregations.

Close #6962
</description><key id="38414837">6963</key><summary>Drop UnsafeUtils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T16:00:36Z</created><updated>2015-06-07T19:18:20Z</updated><resolved>2014-07-23T06:55:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-22T16:05:07Z" id="49760278">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Remove unsafe unaligned memory access - illegal on SPARC</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6962</link><project id="" key="" /><description>Unaligned memory access is illegal on several architectures (such as SPARC, see https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/Nh-kXI5J6Ek/WXIZKhhGVHkJ for context).
</description><key id="38412246">6962</key><summary>Internal: Remove unsafe unaligned memory access - illegal on SPARC</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T15:35:05Z</created><updated>2014-10-20T16:36:27Z</updated><resolved>2014-07-23T06:55:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-22T15:47:28Z" id="49757713">oh boy
</comment><comment author="s1monw" created="2014-07-23T07:22:26Z" id="49840928">I added the labels for `1.3` and `1.2.3` this is basically making us not portable anymore and we should fix that! you should also remove the `pom.xml` entry that makes the unsafe utils and exception for forbidden APIs
</comment><comment author="jprante" created="2014-10-20T15:08:10Z" id="59772443">ES 1.3.4 still has issues, but this time in lzf compression codec. See file hs_err_pid26623.log at
https://gist.github.com/jprante/3c9ca8a85da13bd65226
</comment><comment author="rjernst" created="2014-10-20T16:36:27Z" id="59794411">@jprante The fix to remove unsafe usage in lzf compression was backported to 1.3.5 in #8078
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add new entry for the Elasticsearch SIREn plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6961</link><project id="" key="" /><description>The link to a new plugin for Elasticsearch.
</description><key id="38411896">6961</key><summary>Add new entry for the Elasticsearch SIREn plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rendel</reporter><labels /><created>2014-07-22T15:31:50Z</created><updated>2014-07-25T10:50:42Z</updated><resolved>2014-07-25T10:50:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T16:59:46Z" id="49767858">Hi @rendel 

Thanks for your PR.  Please could I ask you to sign the CLA so that we can get it merged in:
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="rendel" created="2014-07-22T17:05:25Z" id="49768637">Hi, I have signed the CLA.
Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better heuristic for setting default `shard_size` in terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6960</link><project id="" key="" /><description>The default shard size in the terms aggregation now uses BucketUtils.suggestShardSideQueueSize() to set the shard size if the user does not specify it as a parameter.

Closes #6857
</description><key id="38409349">6960</key><summary>Better heuristic for setting default `shard_size` in terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T15:07:23Z</created><updated>2015-06-07T12:39:05Z</updated><resolved>2014-07-24T07:08:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-23T07:36:57Z" id="49841968">Left one comment but it looks good!
</comment><comment author="jpountz" created="2014-07-24T06:21:05Z" id="49971551">LGTM
</comment><comment author="colings86" created="2014-07-24T07:08:03Z" id="49974415">Merged into master and 1.x
</comment><comment author="javanna" created="2015-02-18T05:12:09Z" id="74813562">Hey @colings86 can we update our docs according to this change?
</comment><comment author="colings86" created="2015-02-25T11:09:17Z" id="75942929">Docs updated in https://github.com/elasticsearch/elasticsearch/pull/9873
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable loading of bloom filters by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6959</link><project id="" key="" /><description>This commit changes the default for index.codec.bloom.load to false,
because bloom filters can use a sizable amount of RAM on indices with
many tiny documents, and now only gives smallish index-time
performance gains for apps that update (not just append) documents,
since we've separately improved performance for ID lookups with
#6298.

Closes #6349
</description><key id="38406550">6959</key><summary>Disable loading of bloom filters by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T14:41:31Z</created><updated>2015-06-07T12:39:18Z</updated><resolved>2014-07-23T09:57:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-22T15:38:34Z" id="49756377">can we randomly set this setting for the index in `ElasticsearchIntegrationTest#setRandomSettings` other than that it looks good
</comment><comment author="mikemccand" created="2014-07-22T16:07:21Z" id="49760581">OK I added randomization for loading bloom filters or not ...
</comment><comment author="s1monw" created="2014-07-23T07:46:04Z" id="49842652">LGTM
</comment><comment author="bobrik" created="2014-07-27T19:52:41Z" id="50283732">Looking at diff, `PUT /old_index/_settings?index.codec.bloom.load=false` doesn't look right. I think it should be `true` in docs now.
</comment><comment author="mikemccand" created="2014-07-27T22:41:31Z" id="50288314">Thanks @bobrik I just fixed this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to configure eager fielddata loading for _timestamp field via PUT mapping API for type with existing mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6958</link><project id="" key="" /><description>When creating a new index, if I have eager fielddata loading configured for the `_timestamp` field, that sticks.

However when updating this setting via the PUT mapping API, although the request is acknowledged when I check the mapping via GET, fielddata configuration for that field is blank.

I have been able to successfully configure it for other fields via the PUT mapping API, though. Seems like this problem may be specific to the special underscore-prefixed fields, though I've only really tried `_timestamp`.
</description><key id="38397219">6958</key><summary>Unable to configure eager fielddata loading for _timestamp field via PUT mapping API for type with existing mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">shikhar</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T13:00:05Z</created><updated>2014-09-08T15:20:06Z</updated><resolved>2014-09-08T15:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T13:02:22Z" id="49735271">Hi @shikhar 

What version are you using? Also, could you provide a small curl recreation of the problem?

thanks
</comment><comment author="shikhar" created="2014-08-20T21:00:26Z" id="52843297">```
curl -XPOST localhost:9200/test -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "type1" : {
            "_timestamp" : { "enabled" : true },
            "properties" : {
                "field1" : { "type" : "string", "index" : "not_analyzed" }
            }
        }
    }
}'
```

```
curl -XPUT 'localhost:9200/test/type1/_mapping' -d '{                                                                                                            
        "type1" : {
            "_timestamp" : { "enabled" : true, "fielddata": { "loading": "eager" } },
            "properties" : {
                "field1" : { "type" : "string", "index" : "not_analyzed", "fielddata": { "loading": "eager" } }
            }
        }
    }'
```

acknowledged: true

but:

```
curl -XGET 'localhost:9200/test/type1/_mapping?pretty=true'
{
  "test" : {
    "mappings" : {
      "type1" : {
        "_timestamp" : {
          "enabled" : true
        },
        "properties" : {
          "field1" : {
            "type" : "string",
            "index" : "not_analyzed",
            "fielddata" : {
              "loading" : "eager"
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="shikhar" created="2014-08-21T06:51:52Z" id="52884563">forgot to mention, this is on ES 1.3.0
</comment><comment author="brwe" created="2014-08-21T08:11:13Z" id="52890667">The fielddata settings are not merged when the mapping is updated (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/internal/TimestampFieldMapper.java#L280). This is similar to issue #777 and #5772. I will go through all the different root mappers and fix one by one, just have not gotten further than `_ttl` (#7316) yet.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Reflect that 'field_value_factor' is only in 1.2.x</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6957</link><project id="" key="" /><description>While the blogpost http://www.elasticsearch.org/blog/2014-04-02-this-week-in-elasticsearch/ states, that feature #5519 was added to 1.x, the release notes for, e.g. v1.1.2, however tell otherwise.
Only the release notes for 1.2.0 list #5519 as a new feature.

Since the 1.x docs deprecate/discourage from using `_boost`, and seemingly give a migration example at
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-boost-field.html#function-score-instead-of-boost
users of 1.1.x should be warned.
</description><key id="38396540">6957</key><summary>Docs: Reflect that 'field_value_factor' is only in 1.2.x</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">konradkonrad</reporter><labels><label>docs</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T12:50:28Z</created><updated>2014-09-08T19:32:50Z</updated><resolved>2014-07-23T13:50:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-23T13:38:52Z" id="49874215">Hi @konradkonrad I left a small comment, this looks great to me otherwise, can you remove the extra `added[1.2.0]` tag, squash your commits and I'll merge this in?
</comment><comment author="konradkonrad" created="2014-07-23T13:44:28Z" id="49874961">Hi @dakrone thanks for looking at it. I agree to your comment and applied the change.
</comment><comment author="dakrone" created="2014-07-23T13:50:04Z" id="49875731">Thanks! Merged this to master and 1.x in 48812ff and 57ae70c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix InternalSearchHits serialization to be deterministic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6956</link><project id="" key="" /><description>The assertion on binary equality for streamable serialization
sometimes fails due to the usage of identify hashmaps inside
the InternalSearchHits serialization. This only happens if
the number of shards the result set is composed of is very high.
This commit makes the serialziation deterministic and removes
the need to serialize the ordinal due to in-order serialization.
</description><key id="38387448">6956</key><summary>Fix InternalSearchHits serialization to be deterministic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-07-22T10:31:17Z</created><updated>2014-07-22T12:44:46Z</updated><resolved>2014-07-22T12:21:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-22T10:36:20Z" id="49722691">LGTM
</comment><comment author="s1monw" created="2014-07-22T12:44:46Z" id="49733463">FYI - I reverted this change due to random failurs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make function scores functions tunable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6955</link><project id="" key="" /><description>With the `function_score` query, each function can return a range of values: the decay functions return a value between 0 and 1, and `field_value_factor` function can return any value but with (eg) logarithmic functions a typical score is between 0 and 3, the `random_score` function currently returns a very large number (but see #6907), and the `script_score` can return whatever value you calculate.

It isn't easy to tune the contribution of each function.  If you have two decay clauses: one for location and one for price, you can't easily say that "location is more important than price".

What about making each function accept the `boost_factor` parameter which is multiplied with the output of the function?
</description><key id="38385976">6955</key><summary>Make function scores functions tunable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T10:09:46Z</created><updated>2014-09-01T09:21:40Z</updated><resolved>2014-09-01T09:21:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T10:10:13Z" id="49720485">/cc @brwe @rjernst 
</comment><comment author="nik9000" created="2014-07-22T12:51:01Z" id="49734111">Can you use multiple reviews as a temporary workaround for this?  I haven't
checked the all API but I image so. If so it might be worth checking how
much this improves performance over the work around. Could be substantial
because multiple rescores has to sort between each.
On Jul 22, 2014 6:10 AM, "Lee Hinman" notifications@github.com wrote:

&gt; +1 on boost_factor for any sub function
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6955#issuecomment-49720535
&gt; .
</comment><comment author="brwe" created="2014-07-23T15:12:27Z" id="49887680">@nik9000  I do not think there is a workaround currently which is unfortunate. 

I think having the boost factor for each function would make a lot of sense. 
Alternatively I though we could also do something like nest `function_score` in functions like in the below example and then use each function again together with the score computed inside the query. That would allow lots of flexibility but would be a json nightmare I guess...

@s1monw mentioned we could use expressions and expose many of the current functions as expression. That way everyone could write their custom function easily (and add whatever factor they wish) without performance loss.

But implementing a boost factor for each function seems like the easiest thing to do right now.

```
{
  "query": {
    "function_score": {
      "query": {},
      "functions": [
        {
          "boost_factor": 2,
          "query": {
            "function_score": {
              "functions": [
                {
                  "gauss": {
                    "FIELD": {
                      "origin": "...",
                      "scale": "..."
                    }
                  }
                }
              ]
            }
          }
        },
        {
          "boost_factor": 2,
          "query": {
            "function_score": {
              "functions": [
                {...}
              ]
            }
          }
        }
      ]
    }
  }
}
```

(also cc @HonzaKral )
</comment><comment author="brwe" created="2014-07-25T12:00:50Z" id="50140030">If we implement the boost_factor thing, I think `weight` might be a better name for it. 
Here is how this could look like:

```
{
  "query": {
    "function_score": {
      "functions": [
        {
          "exp": {
            ...
          },
          "weight": 2
        },
        {
          "random_score": {
            ...
          }, 
          "weight": 0.01
        }
      ]
    }
  }
}
```

If only one function is in there then this weight does not seem to make sense so I would not allow it. 
</comment><comment author="clintongormley" created="2014-07-25T12:06:23Z" id="50140415">I like `weight`, although `boost_factor` wouldn't need it.  I'm torn about supporting it if there is only one function.  I understand that you should use `boost` there instead, but...  

Hmmm, I think you're right.
</comment><comment author="HonzaKral" created="2014-07-25T12:15:49Z" id="50141190">I see the use case for single function - a lot of people have simple
queries that they then expand upon by adding more clauses. At that point it
would be simpler to allow this.
</comment><comment author="alexksikes" created="2014-07-25T13:07:32Z" id="50145946">This seems to be an interesting use case until we get another use case. Why not more simply using mathematical expressions to express the different decay functions or a combination them? So I'm definitely +1 on exposing the various functions with Lucene expressions, and on implementing new ones.
</comment><comment author="brwe" created="2014-07-29T14:19:43Z" id="50481825">The main reason why we added the decay functions as json with parameters was that not all users like scripting. So, maybe we should have both?

Adding the different functions we have now in `function_score` to expressions is not tied to function score so I think we should have a different issue for that. 

@HonzaKral sorry, I was not clear with the single function, I was referring to the case where the score function is not in the function list, like this:

```
"function_score": {
    "boost_mode": "replace",
    "query": {...},
    "script_score/random/...": {
        ....
    }
}
```

Does not make sense to me to allow it there but I could do it.
</comment><comment author="rjernst" created="2014-07-31T15:50:02Z" id="50777985">I'm ok with adding a `weight` parameter to all functions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Is es really load balance?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6954</link><project id="" key="" /><description>When i restart the nodes, i find all shards go to the node1, and all replicas go to the node0.
So, when i bulk write to the es, i find the node1 has more LOAD then the node0.
I want to ask whether some thing i must set to avoid all shards go to one node or not? 
</description><key id="38382538">6954</key><summary>Is es really load balance?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ghost</reporter><labels /><created>2014-07-22T09:22:37Z</created><updated>2014-07-22T10:11:30Z</updated><resolved>2014-07-22T10:11:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T10:11:30Z" id="49720614">@shanghailangke 

Please ask questions like this on the mailing list.  This issues list is for bug reports and feature requests.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Add BWC tests to release script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6953</link><project id="" key="" /><description>This commit adds the ability to run bwc tests during the release
process to ensure the current release is backwards compatible with
the latest installed previous version. Installed means available
in the configured bwc test path.
</description><key id="38381768">6953</key><summary>[BUILD] Add BWC tests to release script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T09:11:36Z</created><updated>2014-07-22T09:47:44Z</updated><resolved>2014-07-22T09:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-22T09:20:40Z" id="49715988">LGTM, tested that `find_bwc_version("1.3.0")` returned 1.2.2 with a proper setup when run in a python repl.
</comment><comment author="s1monw" created="2014-07-22T09:25:14Z" id="49716401">thanks lee!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Add min_score support to function_score query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6952</link><project id="" key="" /><description>I've sometimes needed to use the `function_score` query to generate some custom score, and then I want to exclude documents that don't meet a minimum score.

Currently I can do this by setting the `_score` to zero then using the `min_score` parameter  in the search request.  However, sometimes I want to apply the same logic further down in the hierarchy.

Adding `min_score` support to the `function_score` query would allow me to achieve that, ie filter out all documents whose score is lower than the specified.  
</description><key id="38371321">6952</key><summary>Query DSL: Add min_score support to function_score query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>feature</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-22T05:58:32Z</created><updated>2014-11-28T13:20:49Z</updated><resolved>2014-11-28T11:37:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ccodina" created="2014-08-08T06:55:46Z" id="51569195">hi,
that's a good idea .. i need this feature too in the function_score ^^
regards,
chris
</comment><comment author="clintongormley" created="2014-11-28T13:16:43Z" id="64892832">w00t
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto completion using completion suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6951</link><project id="" key="" /><description>While using the completion suggester, the entire content will be matching for the query.
Instead of that, I want to have the completion to be enabled for the terms in the index.

I looked into the ES documentation, I dint get the enough information from it. Is it possible using completion suggester or should I have to go for other options?

Thanks, 
Anand
</description><key id="38369390">6951</key><summary>Auto completion using completion suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anandkumark11</reporter><labels /><created>2014-07-22T05:05:46Z</created><updated>2014-07-22T05:54:29Z</updated><resolved>2014-07-22T05:54:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T05:54:29Z" id="49701007">Please ask these questions in the forum.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Auto completion issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6950</link><project id="" key="" /><description>Hi all, 

I've been indexing a large pile of source code using elasticsearch.

While searching I want the feature of autocomplete, but if I go for the ngram analyzed content, the query_string doesn't seems to be working with it.
So am using two different fields for indexing, one is ngram_analyzed(for autocomplete) and the other is standard analyzed one(for query_string).

And while indexing the a huge content using ngram_analyzer, its showing the below exception for the respective mingram and maxgram:

mingram:2 and maxgram:10
 java.lang.IllegalArgumentException: TokenStream expanded to 16200 finite strings. Only &lt;= 256 finite strings are supported

mingram:4 and maxgram:7
 java.lang.IllegalArgumentException: TokenStream expanded to 450 finite strings. Only &lt;= 256 finite strings are supported

Is there any solution for this?

Also is it possible to have single field for the index, instead of having multiple fields as I mentioned above?

Thanks,
Anand
</description><key id="38368008">6950</key><summary>Auto completion issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">anandkumark11</reporter><labels /><created>2014-07-22T04:23:24Z</created><updated>2014-07-22T05:54:57Z</updated><resolved>2014-07-22T05:54:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T05:54:57Z" id="49701027">Hi @anandkumark11 

Please ask questions like this in the forum.  The issues list is for feature requests and bug reports.

btw, you shouldn't create so many ngrams.  Really you want min/max to be the same value, eg 3 or 4, no more.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unneeded cluster state serialization during cluster join</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6949</link><project id="" key="" /><description>At the moment we serialize the cluster state in JoinResponse and ValidateJoinRequest. However this state is not used anywhere and can be removed to save on network overhead
</description><key id="38340723">6949</key><summary>Remove unneeded cluster state serialization during cluster join</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T20:20:46Z</created><updated>2015-06-07T12:39:28Z</updated><resolved>2014-07-21T20:32:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-21T20:27:10Z" id="49660565">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More lenient type parsing in histo/cardinality aggs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6948</link><project id="" key="" /><description>closes #6893
</description><key id="38335575">6948</key><summary>More lenient type parsing in histo/cardinality aggs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T19:23:10Z</created><updated>2015-06-07T19:18:31Z</updated><resolved>2014-07-21T19:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-21T19:28:08Z" id="49653263">@clintongormley the default is to coarse....
</comment><comment author="clintongormley" created="2014-07-21T19:29:13Z" id="49653409">Ok - right - just checked the code. never mind me :)
</comment><comment author="martijnvg" created="2014-07-21T19:30:16Z" id="49653547">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6947</link><project id="" key="" /><description>causes the example to fail in bash
</description><key id="38320599">6947</key><summary>typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">missinglink</reporter><labels /><created>2014-07-21T16:38:56Z</created><updated>2014-07-21T17:09:55Z</updated><resolved>2014-07-21T17:09:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-21T17:09:55Z" id="49635074">Thanks @missinglink, merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Remove TransportUpdateActionTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6946</link><project id="" key="" /><description>This test has been made obselete by the UpdateTests.
</description><key id="38316504">6946</key><summary>[TEST] Remove TransportUpdateActionTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T15:55:06Z</created><updated>2014-07-22T08:50:02Z</updated><resolved>2014-07-22T08:50:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-21T15:57:17Z" id="49624995">LGTM
</comment><comment author="s1monw" created="2014-07-22T08:50:02Z" id="49713343">this has been pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Lucene's IDVersionPostingsFormat for _uid field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6945</link><project id="" key="" /><description>Lucene's new ID+version optimized postings format (https://issues.apache.org/jira/browse/LUCENE-5675 ) should work well for the _uid field.

First, it stores the version with each _uid value directly in the terms index/dict, so we don't need a separate numeric DV lookup (saves one seek &amp; some CPU).

Second, in certain cases it can know just by looking at the in-RAM terms index that a given segment does not contain and ID with a version &gt; X, which can speed up the optimistic concurrency case ("update doc as long as indexed version is &lt;= N).  But taking advantage of this is tricky because ES lets the app pick its version semantics, so I think for this issue we should simply focus on switching to IDVPF and storing/retrieving the version in the terms dict instead of numeric DV.

This issue should also fix tests to randomize whether this PF is used for _uid.

We need to be careful w/ back-compat here as well, because this PF has no back-compat insurance from Lucene (it's not a core PF), and we also must recognize and read "old" segments that stored the version in numeric DV, on a segment by segment basis.
</description><key id="38313824">6945</key><summary>Use Lucene's IDVersionPostingsFormat for _uid field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>feedback_needed</label></labels><created>2014-07-21T15:27:38Z</created><updated>2015-11-21T16:58:15Z</updated><resolved>2015-11-21T16:37:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T16:16:08Z" id="158659478">@mikemccand is this something you're still interested in?
</comment><comment author="mikemccand" created="2015-11-21T16:37:19Z" id="158660850">Well, 1) I think it's not a huge win since few users take advantage of optimistic concurrency (I think?), and 2) this is still a sandbox postings format, so there's no back-compat.  I'll close ... we can revisit if things change.
</comment><comment author="bleskes" created="2015-11-21T16:58:15Z" id="158662040">a short note that we load the version all the time because we need to update it on the primary and replicas use it to deal with out of order delivery from the primary. That said +1 on not using a posting format with no bwc.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Stresstest Update(upsert) and Delete concurrency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6944</link><project id="" key="" /><description>This is a test to replace the TransportUpdateActionTest and runs deletes concurrently with updates in tight loops. After all operations conclude, it verifies that each operation received a response and the the versions are what should be expected.
</description><key id="38301711">6944</key><summary>[TEST] Stresstest Update(upsert) and Delete concurrency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T13:18:52Z</created><updated>2014-07-22T08:50:17Z</updated><resolved>2014-07-22T08:50:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-21T14:21:32Z" id="49611323">LGTM
</comment><comment author="s1monw" created="2014-07-22T08:50:17Z" id="49713365">this has been pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Discovery] join master after first election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6943</link><project id="" key="" /><description>Currently, pinging results are only used if the local node is elected master or if they detect another _already_ active master. This has the effect that master election requires two pinging rounds - one for the elected master to take is role and another for the other nodes to detect it and join the cluster. We can be smarter and use the election of the first round on other nodes as well. Those nodes can try to join the elected master immediately. There is a catch though - the elected master node may still be processing the election and may reject the join request if not ready yet. To compensate a retry mechanism is introduced to try again (up to 3 times by default) if this happens.

Note: this is against the improve zen branch
</description><key id="38300890">6943</key><summary>[Discovery] join master after first election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label></labels><created>2014-07-21T13:09:10Z</created><updated>2014-07-21T19:06:33Z</updated><resolved>2014-07-21T19:00:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-21T13:32:29Z" id="49605301">minor notes, LGTM
</comment><comment author="bleskes" created="2014-07-21T19:00:44Z" id="49649754">incorporated the feedback and pushed to improve_zen. Thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin on a system where JAVA_HOME contains spaces fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6942</link><project id="" key="" /><description>```
/usr/local/Cellar/elasticsearch/1.2.1/bin  $ echo $JAVA_HOME
/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home
```

results in 

```
/usr/local/Cellar/elasticsearch/1.2.1/bin  $ ./plugin 
./plugin: line 49: /Library/Internet: No such file or directory
./plugin: line 49: exec: /Library/Internet: cannot execute: No such file or directory
```

Suggested fix (worked for me, but I'm no bash guru)
put double quotes around $JAVA in the exec line of plugin
</description><key id="38299385">6942</key><summary>bin/plugin on a system where JAVA_HOME contains spaces fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikebaldry</reporter><labels><label>:Plugins</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-07-21T12:48:06Z</created><updated>2016-11-25T18:24:08Z</updated><resolved>2016-11-25T18:24:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-21T13:51:29Z" id="49607530">It has been fixed some days ago with this commit: https://github.com/elasticsearch/elasticsearch/commit/4eca0499fae2f40f3917b0b970a9a5958a738eec

cc @spinscale 
</comment><comment author="mikebaldry" created="2014-07-21T16:21:03Z" id="49628263">aha, thanks :)
</comment><comment author="ptheofan" created="2015-11-21T19:37:49Z" id="158676743">Bug reappeared? I'm having the exact same issue despite that $JAVA is in doublequotes.

``` bash
&#10140;  utils git:(master) plugin install jettro/elasticsearch-gui
/usr/local/Cellar/elasticsearch/2.0.0_1/libexec/bin/plugin: line 109: /Library/Internet: No such file or directory
```

whereas

``` bash
&#10140;  utils git:(master) echo $JAVA_HOME
/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home
```
</comment><comment author="nik9000" created="2015-11-23T15:31:58Z" id="158969980">Its possible this came back. We're much better at testing these scripts now but we don't test this case. I'll reopen this so we know to investigate. But I don't know when I'll get a chance to work on it though.
</comment><comment author="rmuir" created="2015-11-23T15:41:50Z" id="158972628">We can test this by putting a space in the path to JAVA_HOME on jenkins servers.
</comment><comment author="pbaille" created="2016-02-16T14:16:19Z" id="184697083">I've got this issue to, is there a workaround to install marvel?
</comment><comment author="clintongormley" created="2016-02-19T19:22:36Z" id="186367685">This is still broken in 2.2 and master
</comment><comment author="kdelchev" created="2016-02-21T15:20:37Z" id="186842189">I had similar issue (`line 114: /Library/Internet: No such file or directory`). Running `sudo bin/plugin license` did the job.
</comment><comment author="ptheofan" created="2016-07-03T10:48:45Z" id="230147020">Latest workaround that worked for me like a charm. Escape the JAVA_HOME path (and ignore the fact that it's wrapped in double quotes)!

I went from

``` bash
export JAVA_HOME="/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home"
```

to (diff is I escaped the space in Internet Plug-Ins)

``` bash
export JAVA_HOME="/Library/Internet\ Plug-Ins/JavaAppletPlugin.plugin/Contents/Home"
```

and it worked
</comment><comment author="OsamHdz" created="2016-09-22T18:17:44Z" id="248984356">I had the same problem that @kdelchev, but the sudo command didn't work. Adding ' single quotes at "$JAVA_HOME" in eval line worked for me.

``` bash
eval '"$JAVA"' -client -Delasticsearch -Des.path.home="\"$ES_HOME\"" $properties -cp "\"$ES_HOME/lib/*\"" org.elasticsearch.plugins.PluginManagerCliParser $args
```

Here is the info:
http://unix.stackexchange.com/questions/131766/why-does-my-shell-script-choke-on-whitespace-or-other-special-characters
</comment><comment author="clintongormley" created="2016-11-25T18:24:08Z" id="263007972">this is fixed in 5.0</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `index.fail_on_corruption` setting is not updateable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6941</link><project id="" key="" /><description>The `index.fail_on_corruption` was not updateable via the index settings
API. This commit also fixed the setting prefix to be consistent with other
setting on the engine. Yet, this feature is unreleased so this won't break anything.
</description><key id="38299214">6941</key><summary>The `index.fail_on_corruption` setting is not updateable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Engine</label><label>blocker</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T12:45:27Z</created><updated>2015-06-07T19:20:43Z</updated><resolved>2014-07-21T13:07:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-21T12:47:53Z" id="49600872">LGTM
</comment><comment author="jpountz" created="2014-07-21T12:48:31Z" id="49600935">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update nested-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6940</link><project id="" key="" /><description>top level "query" param was missing

see http://elasticsearch-users.115913.n3.nabble.com/Newbie-facet-search-on-nested-documents-failing-td3487400.html
</description><key id="38296445">6940</key><summary>Update nested-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikelrob</reporter><labels /><created>2014-07-21T11:58:31Z</created><updated>2014-07-22T05:35:53Z</updated><resolved>2014-07-22T05:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-22T05:35:53Z" id="49700139">Hi @mikelrob 

Thanks for the PR, but actually, this format is intentional.  Each query/filter clause is a standalone component that can be plugged into the query DSL at many levels, only one of which is as the top-level query.

I realise this can be confusing - our intention is to add runnable snippets that provide complete examples.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Geopoint with array broken (dynamic mapping): geo_point expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6939</link><project id="" key="" /><description>I use Elasticsearch with Logstash and dynamic mapping.
I my logs, I have geopoints with the array syntax.
It was OK with ES 1.0.X, but it's broken with ES 1.2.2.

Template:

```
curl -XPUT localhost:9200/_template/logstash -d '{
    "template": "logstash_*",
    "settings" : {
        "refresh_interval": "30s"
    },
    "mappings": {
        "logs": {
            "_all" : {
                "enabled": false
            },
            "dynamic_templates": [
                {
                    "location": {
                        "match": "location*",
                        "mapping": {
                            "type": "geo_point"
                        }
                    }
                },
                {
                    "generic": {
                        "match": "*",
                        "match_mapping_type": "string",
                        "mapping": {
                            "type": "string",
                            "index": "not_analyzed"
                        }
                    }
                }
            ],
            "dynamic_date_formats": [
                "dateOptionalTime",
                "yyyy-MM-dd",
                "yyyy-MM-dd HH:mm:ss"
            ]
        }
    }
}'
```

Delete index:

```
curl -XDELETE localhost:9200/logstash_test
```

Try to add a doc/log:

```
curl -XPOST localhost:9200/logstash_test/logs -d '{
  "location_array": [
    2.3069244,
    48.8881598
  ]
}'
```

It fails with this message:

```
[2014-07-21 11:31:49,168][INFO ][cluster.metadata         ] [fr-dev-01] [logstash_test] creating index, cause [auto(index api)], shards [5]/[1], mappings [logs]
[2014-07-21 11:31:49,465][DEBUG][action.index             ] [fr-dev-01] [logstash_test][1], node[zzMEQe9JSS6qEgw0oRgdVA], [P], s[STARTED]: Failed to execute [index {[logstash_test][logs][PhqP9fpgQkS4tHfOs105GQ], source[{"location_array":[2.3069244,48.8881598]}]}]
org.elasticsearch.index.mapper.MapperParsingException: failed to parse
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:536)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:373)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:534)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:433)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.ElasticsearchParseException: geo_point expected
    at org.elasticsearch.common.geo.GeoUtils.parseGeoPoint(GeoUtils.java:421)
    at org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parse(GeoPointFieldMapper.java:530)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parseDynamicValue(ObjectMapper.java:819)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:639)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:625)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:482)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)
    ... 8 more
```

If I insert a doc with a geopoint as an object, it works, and the mapping is created dynamically.
After that, I can insert a doc with a geopoint as an array without error.
</description><key id="38295103">6939</key><summary>Mapping: Geopoint with array broken (dynamic mapping): geo_point expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">pierrre</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T11:34:53Z</created><updated>2014-08-16T14:15:24Z</updated><resolved>2014-08-11T12:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bobpaulin" created="2014-07-26T03:25:27Z" id="50221948">This also appears to be happening on the master branch.
</comment><comment author="bobpaulin" created="2014-07-26T20:37:29Z" id="50247972">Seems like it's blowing up since the geo_point is only parsing the first value instead of the 2 item array as a whole.  It works the second time through because the mapper object within org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray caches location_array as a geo_point so the array logic properly pulls the mapper.  The first time through that entry is not there so the mapper comes up null and it tries to parse the array values individually. May I suggest adding some logic to the org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray method to run the template builder logic to find the GeoMapper

```
private void serializeArray(ParseContext context, String lastFieldName) throws IOException {
    String arrayFieldName = lastFieldName;
    Mapper mapper = mappers.get(lastFieldName);
    //Start Check TemplateBuilder for geo
    if (mapper == null) {
        BuilderContext builderContext = new BuilderContext(context.indexSettings(), context.path());
        Mapper.Builder builder = context.root().findTemplateBuilder(context, arrayFieldName, "geo_point");
        if (builder != null) {
            mapper = builder.build(builderContext);
            mappers.put(lastFieldName, mapper);
        }
    }
    //End Check TemplateBuilder for geo
    if (mapper != null &amp;&amp; mapper instanceof ArrayValueMapperParser) {
        mapper.parse(context);
    } else {
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail restore if snapshot is corrupted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6938</link><project id="" key="" /><description>today if a snapshot is corrupted the restore operation
never terminates. Yet, if the snapshot is corrupted there
is no way to restore it anyway. If such a snapshot is restored
today the only way to cancle it is to delete the entire index which
might cause dataloss. This commit also fixes an issue in InternalEngine
where a deadlock can occur if a corruption is detected during flush
since the InternalEngine#snapshotIndex aqcuires a topLevel read lock
which prevents closing the engine.
</description><key id="38294976">6938</key><summary>Fail restore if snapshot is corrupted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Snapshot/Restore</label><label>blocker</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T11:32:47Z</created><updated>2015-06-07T19:20:55Z</updated><resolved>2014-07-21T14:18:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-21T13:05:10Z" id="49602424">LGTM, except for the writeLock comment and taking flush outside of it
</comment><comment author="s1monw" created="2014-07-21T13:17:01Z" id="49603606">@jpountz @kimchy pushed another commit and incorporated your feedback
</comment><comment author="jpountz" created="2014-07-21T13:41:56Z" id="49606408">LGTM
</comment><comment author="kimchy" created="2014-07-21T13:47:10Z" id="49607020">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Core: Add extra write consistency validation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6937</link><project id="" key="" /><description>Add additional optional validation on top of the write consistency check. After the cluster state write validation check we have today, an optional extra write consistency validation may happens that actually goes the nodes containing the shard copies and verifies if that nodes are in a started state.

Note: this PR is based on improve_zen branch
</description><key id="38291487">6937</key><summary>Core: Add extra write consistency validation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-07-21T10:29:15Z</created><updated>2015-05-18T23:30:27Z</updated><resolved>2014-09-02T17:59:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add children aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6936</link><project id="" key="" /><description>Add `children` bucket aggregation that is able to map buckets between parent types and child types based on top of the parent/child support. It is the equivalent of the has_child filter/query in the query dsl.

Example request:

``` json
GET /stack/question/_search?search_type=count
{
  "aggs": {
    "top-tags": {
      "terms": {
        "field": "tags",
        "size": 10
      },
      "aggs": {
        "to-answers": {
          "children": {
            "child_type" : "answer"
          },
          "aggs": {
            "top-names": {
              "terms": {
                "field": "owner_display_name",
                "size": 10
              }
            }
          }
        }
      }
    }
  }
}
```

Example response:

``` json
{
   "took": 90,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 175275,
      "max_score": 0,
      "hits": []
   },
   "aggregations": {
      "top-tags": {
         "buckets": [
            {
               "key": "windows-7",
               "doc_count": 25365,
               "to-answers": {
                  "doc_count": 36004,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "molly7244",
                           "doc_count": 274
                        },
                        {
                           "key": "chris",
                           "doc_count": 19
                        },
                        {
                           "key": "david",
                           "doc_count": 14
                        },
                        {
                           "key": "dan",
                           "doc_count": 12
                        },
                        {
                           "key": "james",
                           "doc_count": 7
                        },
                        {
                           "key": "b",
                           "doc_count": 5
                        },
                        {
                           "key": "alex",
                           "doc_count": 4
                        },
                        {
                           "key": "peter",
                           "doc_count": 4
                        },
                        {
                           "key": "s",
                           "doc_count": 4
                        },
                        {
                           "key": "cody",
                           "doc_count": 3
                        }
                     ]
                  }
               }
            },
            {
               "key": "linux",
               "doc_count": 18342,
               "to-answers": {
                  "doc_count": 6655,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "abrams",
                           "doc_count": 25
                        },
                        {
                           "key": "ignacio",
                           "doc_count": 25
                        },
                        {
                           "key": "vazquez",
                           "doc_count": 25
                        },
                        {
                           "key": "chris",
                           "doc_count": 9
                        },
                        {
                           "key": "michael",
                           "doc_count": 7
                        },
                        {
                           "key": "basile",
                           "doc_count": 6
                        },
                        {
                           "key": "david",
                           "doc_count": 6
                        },
                        {
                           "key": "alex",
                           "doc_count": 4
                        },
                        {
                           "key": "botykai",
                           "doc_count": 3
                        },
                        {
                           "key": "paul",
                           "doc_count": 3
                        }
                     ]
                  }
               }
            },
            {
               "key": "windows",
               "doc_count": 18119,
               "to-answers": {
                  "doc_count": 24051,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "molly7244",
                           "doc_count": 265
                        },
                        {
                           "key": "david",
                           "doc_count": 27
                        },
                        {
                           "key": "chris",
                           "doc_count": 26
                        },
                        {
                           "key": "diago",
                           "doc_count": 9
                        },
                        {
                           "key": "john",
                           "doc_count": 7
                        },
                        {
                           "key": "paxdiablo",
                           "doc_count": 7
                        },
                        {
                           "key": "ben",
                           "doc_count": 6
                        },
                        {
                           "key": "mark",
                           "doc_count": 6
                        },
                        {
                           "key": "adam",
                           "doc_count": 5
                        },
                        {
                           "key": "c",
                           "doc_count": 5
                        }
                     ]
                  }
               }
            },
            {
               "key": "osx",
               "doc_count": 10971,
               "to-answers": {
                  "doc_count": 5902,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "diago",
                           "doc_count": 4
                        },
                        {
                           "key": "albert",
                           "doc_count": 3
                        },
                        {
                           "key": "asmus",
                           "doc_count": 3
                        },
                        {
                           "key": "molly7244",
                           "doc_count": 3
                        },
                        {
                           "key": "aaron",
                           "doc_count": 2
                        },
                        {
                           "key": "abizern",
                           "doc_count": 2
                        },
                        {
                           "key": "adam",
                           "doc_count": 2
                        },
                        {
                           "key": "duskwuff",
                           "doc_count": 2
                        },
                        {
                           "key": "johnsyweb",
                           "doc_count": 2
                        },
                        {
                           "key": "mark",
                           "doc_count": 2
                        }
                     ]
                  }
               }
            },
            {
               "key": "ubuntu",
               "doc_count": 8743,
               "to-answers": {
                  "doc_count": 8784,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "ignacio",
                           "doc_count": 9
                        },
                        {
                           "key": "abrams",
                           "doc_count": 8
                        },
                        {
                           "key": "molly7244",
                           "doc_count": 8
                        },
                        {
                           "key": "david",
                           "doc_count": 7
                        },
                        {
                           "key": "pate",
                           "doc_count": 6
                        },
                        {
                           "key": "roger",
                           "doc_count": 6
                        },
                        {
                           "key": "chris",
                           "doc_count": 5
                        },
                        {
                           "key": "vazquez",
                           "doc_count": 5
                        },
                        {
                           "key": "paul",
                           "doc_count": 3
                        },
                        {
                           "key": "rob",
                           "doc_count": 3
                        }
                     ]
                  }
               }
            },
            {
               "key": "windows-xp",
               "doc_count": 7517,
               "to-answers": {
                  "doc_count": 13610,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "molly7244",
                           "doc_count": 232
                        },
                        {
                           "key": "chris",
                           "doc_count": 9
                        },
                        {
                           "key": "john",
                           "doc_count": 9
                        },
                        {
                           "key": "david",
                           "doc_count": 8
                        },
                        {
                           "key": "dave",
                           "doc_count": 5
                        },
                        {
                           "key": "b",
                           "doc_count": 4
                        },
                        {
                           "key": "bart",
                           "doc_count": 4
                        },
                        {
                           "key": "joeqwerty",
                           "doc_count": 4
                        },
                        {
                           "key": "mike",
                           "doc_count": 4
                        },
                        {
                           "key": "s",
                           "doc_count": 4
                        }
                     ]
                  }
               }
            },
            {
               "key": "networking",
               "doc_count": 6739,
               "to-answers": {
                  "doc_count": 2076,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "molly7244",
                           "doc_count": 6
                        },
                        {
                           "key": "alnitak",
                           "doc_count": 5
                        },
                        {
                           "key": "chris",
                           "doc_count": 3
                        },
                        {
                           "key": "albin",
                           "doc_count": 2
                        },
                        {
                           "key": "brian",
                           "doc_count": 2
                        },
                        {
                           "key": "everett",
                           "doc_count": 2
                        },
                        {
                           "key": "fishdump",
                           "doc_count": 2
                        },
                        {
                           "key": "m",
                           "doc_count": 2
                        },
                        {
                           "key": "mike",
                           "doc_count": 2
                        },
                        {
                           "key": "p",
                           "doc_count": 2
                        }
                     ]
                  }
               }
            },
            {
               "key": "mac",
               "doc_count": 5590,
               "to-answers": {
                  "doc_count": 999,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "abrams",
                           "doc_count": 2
                        },
                        {
                           "key": "ignacio",
                           "doc_count": 2
                        },
                        {
                           "key": "vazquez",
                           "doc_count": 2
                        },
                        {
                           "key": "adam",
                           "doc_count": 1
                        },
                        {
                           "key": "anon",
                           "doc_count": 1
                        },
                        {
                           "key": "aravindhanarvi",
                           "doc_count": 1
                        },
                        {
                           "key": "arkaaito",
                           "doc_count": 1
                        },
                        {
                           "key": "ballard",
                           "doc_count": 1
                        },
                        {
                           "key": "bart",
                           "doc_count": 1
                        },
                        {
                           "key": "ben",
                           "doc_count": 1
                        }
                     ]
                  }
               }
            },
            {
               "key": "wireless-networking",
               "doc_count": 4409,
               "to-answers": {
                  "doc_count": 6497,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "molly7244",
                           "doc_count": 61
                        },
                        {
                           "key": "chris",
                           "doc_count": 5
                        },
                        {
                           "key": "mike",
                           "doc_count": 5
                        },
                        {
                           "key": "tom",
                           "doc_count": 5
                        },
                        {
                           "key": "bart",
                           "doc_count": 4
                        },
                        {
                           "key": "user48838",
                           "doc_count": 4
                        },
                        {
                           "key": "alex",
                           "doc_count": 2
                        },
                        {
                           "key": "anon31097",
                           "doc_count": 2
                        },
                        {
                           "key": "joeqwerty",
                           "doc_count": 2
                        },
                        {
                           "key": "kevin",
                           "doc_count": 2
                        }
                     ]
                  }
               }
            },
            {
               "key": "windows-8",
               "doc_count": 3601,
               "to-answers": {
                  "doc_count": 4263,
                  "top-names": {
                     "buckets": [
                        {
                           "key": "molly7244",
                           "doc_count": 3
                        },
                        {
                           "key": "msft",
                           "doc_count": 2
                        },
                        {
                           "key": "user172132",
                           "doc_count": 2
                        },
                        {
                           "key": "aj",
                           "doc_count": 1
                        },
                        {
                           "key": "algorithms",
                           "doc_count": 1
                        },
                        {
                           "key": "andersson",
                           "doc_count": 1
                        },
                        {
                           "key": "ashafiee",
                           "doc_count": 1
                        },
                        {
                           "key": "balakrishnan",
                           "doc_count": 1
                        },
                        {
                           "key": "bobby",
                           "doc_count": 1
                        },
                        {
                           "key": "brockschmidt",
                           "doc_count": 1
                        }
                     ]
                  }
               }
            }
         ]
      }
   }
}
```
</description><key id="38281237">6936</key><summary>Add children aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-21T07:29:08Z</created><updated>2015-06-06T18:28:37Z</updated><resolved>2014-08-19T10:48:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-21T21:39:46Z" id="49669747">Looks good overall, but I am suspicious that the aggregator does not always do the right thing (see comment in ChildrenAggregator).
</comment><comment author="clintongormley" created="2014-07-28T11:48:09Z" id="50328621">Any reason you use `child_type` when the `has_child` filter supports just `type`?  I'd prefer `type`, as the `child_` part is redundant.
</comment><comment author="martijnvg" created="2014-07-28T12:33:19Z" id="50332158">@clintongormley Make sense, the context makes clear that it must be a child type. I will change `child_type` into `type`
</comment><comment author="martijnvg" created="2014-08-18T07:20:43Z" id="52459302">Finally got back to this PR. I applied the feedback. Main changes are:
- Renamed the `child_type` option to `type`
- Support multiple buckets per parent id. The only situation I can think of when multiple buckets belong to the same parent id, is when the parent agg is a terms agg and that is ran on a multivalued field. 
</comment><comment author="jpountz" created="2014-08-18T13:30:26Z" id="52491592">@martijnvg Just did another review.
</comment><comment author="martijnvg" created="2014-08-18T16:49:03Z" id="52520264">@jpountz Thanks for the review! I implemented your feedback and updated the PR.
</comment><comment author="jpountz" created="2014-08-19T08:51:18Z" id="52606352">LGTM
</comment><comment author="jsnod" created="2014-09-16T23:29:57Z" id="55829217">Can we add "Coming in 1.4.0." to the docs page at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-children-aggregation.html ?  Just wasted a few hours trying to figure out why this wouldn't work with my 1.3.2 install
</comment><comment author="jsnod" created="2014-09-16T23:52:59Z" id="55830982">Made PR for doc fix: https://github.com/elasticsearch/elasticsearch/pull/7755
</comment><comment author="martijnvg" created="2014-09-17T07:25:36Z" id="55858160">Sorry for the lost time @afx114, your doc change is in now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>snowball analyzer misstems some words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6935</link><project id="" key="" /><description>Found a few cases today where the snowball analyzer was misstemming things in ways that cause problems:

```
anorectics (the word for people with anorexia) =&gt; anorect
anorectal (relating to the anus and rectum) =&gt; anorect
overeating =&gt; over
```

Elasticsearch 1.2.2
</description><key id="38270317">6935</key><summary>snowball analyzer misstems some words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">danni</reporter><labels /><created>2014-07-21T00:29:47Z</created><updated>2014-07-21T08:09:11Z</updated><resolved>2014-07-21T08:09:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-21T08:09:11Z" id="49580426">Hi @danni 

The Snowball filter for English (the default) uses the [Porter2 Stemming algorithm](http://snowball.tartarus.org/algorithms/english/stemmer.html), and it is what it is.

I suggest you build a custom analyzer and specify custom rules for the words that are important to you, using the [stem override token filte](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-stemmer-override-tokenfilter.html#analysis-stemmer-override-tokenfilter)

You can read all about stemming here: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/stemming.html 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Track scores should be applied properly for `top_hits` aggregation.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6934</link><project id="" key="" /><description /><key id="38265248">6934</key><summary>Track scores should be applied properly for `top_hits` aggregation.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.1</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-20T19:58:51Z</created><updated>2017-03-31T10:06:19Z</updated><resolved>2014-07-21T08:05:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-21T07:13:06Z" id="49577012">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose the indices names in every action relates to if applicable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6933</link><project id="" key="" /><description>If a request relates to indices, expose which ones it relates to in a generic manner.
</description><key id="38248510">6933</key><summary>Expose the indices names in every action relates to if applicable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-20T01:10:03Z</created><updated>2015-06-07T12:39:44Z</updated><resolved>2014-07-24T12:43:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-21T01:39:42Z" id="49566423">Added a few comments... overall, LGTM, but please have someone else look at it as well (it's 3:40am here... I might have missed something ;))
</comment><comment author="javanna" created="2014-07-21T11:12:46Z" id="49593755">Pushed some new commits to address @uboness review. Also strengthen the code a bit to make sure the newly added `requestedIndices` method doesn't break or throws a proper error when run on top of a non valid request.
</comment><comment author="kimchy" created="2014-07-21T11:59:58Z" id="49597028">Added some comments. I think that the contact of `requestedIndices` need to be better defined, if it allows for duplicate index names or not. If it does, it should be documented, and if not, then certain implementations should change to reflect it. I am leaning towards having it unique in the contact, but not heavily...., would love to hear why @uboness thinks
</comment><comment author="javanna" created="2014-07-21T14:46:14Z" id="49614673">Pushed 2 more commits to address @kimchy 's review: clarified the docs and changed the api to not return duplicates, switched `requestedIndices` return type from `String[]` to `Set&lt;String&gt;`.
</comment><comment author="javanna" created="2014-07-21T15:35:02Z" id="49621724">Just pushed a new commit that addresses the last comment about using `ImmutableSet`
</comment><comment author="javanna" created="2014-07-22T09:05:33Z" id="49714694">This should be close, @kimchy can you please have another look?
</comment><comment author="s1monw" created="2014-07-23T09:58:34Z" id="49854472">hey luca, I can see your intention on this PR but I think we should do this differently. I think the idea of `IndicesRequest` is good but it should generify the `public String[] indices()` method that we already have and clearly document it's semantics. This way it will only return what the user specified without any processing depending on the request. In a second step we should add `public IndicesOptions indicesOptions()` method to that interface and return them from each implementing class such that we can make decision how to resolve names etc on top of the simple API. 

For requests like `MultiSearchRequest` we might wanna have a marker interface that allows us to retrieve the sub-requests and then figure out what we need to do in terms of name resolving. makes sense?
</comment><comment author="javanna" created="2014-07-23T12:49:35Z" id="49868684"> Makes sense @s1monw , pushed a new commit that tries to reuse the existing `indices()` method whenever possible.
</comment><comment author="javanna" created="2014-07-23T17:38:50Z" id="49908289">Pushed two more commits to address composite requests and added `indicesOptions()` to the `IndicesRequest` interface. Ready for nother review round.
</comment><comment author="s1monw" created="2014-07-24T09:05:56Z" id="49984148">I left some minor comments. other than that this LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match query rewrite and fuzzy_rewrite not applied</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6932</link><project id="" key="" /><description>There are various bugs to do with `match` query rewriting.
## Wrong `fuzzy_rewrite` default

The docs for the [boolean match query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-match-query.html#_boolean) say that fuzziness should use a `constant_score` rewrite method by default.  Instead it uses the `top_terms_N` method.
## `fuzzy_rewrite` never applied

The `fuzzy_rewrite` parameter is never applied, so it is impossible to change it.  This is fixed with the following patch:

```
diff --git a/src/main/java/org/elasticsearch/index/search/MatchQuery.java b/src/main/java/org/elasticsearch/index/search/MatchQuery.java
index b776416..7cc1f4f 100644
--- a/src/main/java/org/elasticsearch/index/search/MatchQuery.java
+++ b/src/main/java/org/elasticsearch/index/search/MatchQuery.java
@@ -303,10 +303,11 @@ public class MatchQuery {
                 if (query instanceof FuzzyQuery) {
                     QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod);
                 }
+                return query;
             }
             int edits = fuzziness.asDistance(term.text());
             FuzzyQuery query = new FuzzyQuery(term, edits, fuzzyPrefixLength, maxExpansions, transpositions);
-            QueryParsers.setRewriteMethod(query, rewriteMethod);
+            QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod);
             return query;
         }
         if (mapper != null) {
@@ -318,4 +319,4 @@ public class MatchQuery {
         return new TermQuery(term);
     }

-}
\ No newline at end of file
+}
```
## Unused `rewrite` parameter

The `match` query accepts a `rewrite` parameter which is never used.  
## Bad rewriting of `match_phrase_prefix`

The `match_phrase_prefix` should (IMO) use a `constant_score_auto` rewrite on the final term (and this should be settable with `rewrite`), but this functionality is implemented with the MultiPhraseQuery which doesn't support the rewrite methods.  
</description><key id="38234075">6932</key><summary>Match query rewrite and fuzzy_rewrite not applied</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-07-19T14:13:40Z</created><updated>2015-07-08T15:03:19Z</updated><resolved>2015-07-08T15:03:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-10-02T12:06:38Z" id="57620315">@clintongormley Your fix for the fuzzy rewrite looks good. However, where do you see that the default rewrite method for fuzzy is `top_terms_n`?

&gt; The match_phrase_prefix should (IMO) use a constant_score_auto rewrite on the final term (and this should be settable with rewrite), but this functionality is implemented with the MultiPhraseQuery which doesn't support the rewrite methods. 

The rewrite methods are for multi-term queries but `match_phrase_prefix` is a bit more complicated than that. In particular positions need to be taken into account so I'm not sure if we can allow to take all possible terms into account (even without taking care of scoring) without making this query super costly.

Indeed rewrite is not used, and apart from fuzzy queries the `match` query never generates multi-term queries so we should probably remove that parameter.
</comment><comment author="clintongormley" created="2014-10-17T07:09:06Z" id="59474269">&gt; @clintongormley Your fix for the fuzzy rewrite looks good. However, where do you see that the default rewrite method for fuzzy is top_terms_n?

The `fuzzy_rewrite` param defaults to `null`, so if you don't set it, you get the default setting:
https://github.com/apache/lucene-solr/blob/trunk/lucene/core/src/java/org/apache/lucene/search/FuzzyQuery.java#L101
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unassigned replica shards after node startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6931</link><project id="" key="" /><description>I've had an issue during recovery of shards after node startup in a two node cluster. Both nodes are defined as master, data, and the cluster has 4 indexes with 1 shard and 1 replica each.

I get the issue with these steps-
1. Bring node-1 up (master)
2. Bring node-2 up. At this point, both primary and replicas are allocated.
3. Bring node-1 down. Node-2 gets elected as the master. The 4 replica shards (of each index) is now unassigned.
4. Bring node-1 up. At this point, a couple of the replica shards (randomly belonging to any of the 4 indexes) remain unassigned.

Both servers are running elasticsearch-1.0.0 embedded and use Oracle JRE 1.6.0_26. I have a custom analyzer plugin defined.

```
Node-2 JVM
RecoverySource.recover(StartRecoveryRequest)
- throws DelayRecoveryException("source node does not have the shard listed in its state as allocated on the node")

MessageChannelHandler
- transportChannel.sendResponse(e)
```

```
Node-1 JVM
In response to the sendResponse call on Node-VM,
- Message not fully read (response) for [23] handler future(org.elasticsearch.indices.recovery.RecoveryTarget$4@59df9cf4), error [true], resetting
and then from RecoveryTarget.doRecovery
- java.util.concurrent.ExecutionException: org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
```

Now, without the analyzer plugin it works fine- even though the plugin itself did not throw any exceptions. The issue was because I had a custom classloader set in Settings on node initialization, but this was later overwritten to null by PluginService.updatedSettings during plugin initialization. I have changed the code in PluginService.updatedSettings to-

```
    public Settings updatedSettings() {
        ImmutableSettings.Builder builder = ImmutableSettings.settingsBuilder()
                .put(this.settings);
        for (Tuple&lt;PluginInfo, Plugin&gt; plugin : plugins) {
            builder.put(plugin.v2().additionalSettings().getAsMap());
        }
        return builder.build();
    }
```

This overloaded version does not set the classloader variable (assuming that plugin's don't need to set classloader on Settings).

All tests ran fine with the change. I couldn't find a similar fix in the latest version, do let me know if I should submit a pull request.

Exception trace for reference:

```
[2763cc060c8d75d85dbb357f5e220d8c] Message not fully read (response) for [80] handler future(org.elasticsearch.indices.recovery.RecoveryTarget$4@5445c50d), error [true], resetting
[2763cc060c8d75d85dbb357f5e220d8c] [data][0] failed to start shard

org.elasticsearch.indices.recovery.RecoveryFailedException: [data][0]: Recovery failed from [ca6450277735991d8085f84de36d6ecd][wfO9NNewTHuFdNRPhiE7og][MYHOST][inet[/10.225.115.166:9301]]{master=true} into [2763cc060c8d75d85dbb357f5e220d8c][agXTk8sKQKeMnHtspaWBNw][MYHOST][inet[/10.225.115.166:9300]]{master=true}
    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:303)
    at org.elasticsearch.indices.recovery.RecoveryTarget.access$300(RecoveryTarget.java:65)
    at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:171)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.InvalidClassException: failed to read class descriptor
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1566)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:167)
    ... 23 more
Caused by: java.lang.ClassNotFoundException: org.elasticsearch.transport.RemoteTransportException
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1680)
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1526)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.loadClass(ThrowableObjectInputStream.java:93)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.readClassDescriptor(ThrowableObjectInputStream.java:67)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1564)
    ... 28 more
[2763cc060c8d75d85dbb357f5e220d8c] [data][0] state: [RECOVERING]-&gt;[CLOSED], reason [recovery failure [RecoveryFailedException[[data][0]: Recovery failed from [ca6450277735991d8085f84de36d6ecd][wfO9NNewTHuFdNRPhiE7og][MYHOST][inet[/10.225.115.166:9301]]{master=true} into [2763cc060c8d75d85dbb357f5e220d8c][agXTk8sKQKeMnHtspaWBNw][MYHOST][inet[/10.225.115.166:9300]]{master=true}]; nested: RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: InvalidClassException[failed to read class descriptor]; nested: ClassNotFoundException[org.elasticsearch.transport.RemoteTransportException]; ]]
```
</description><key id="38233051">6931</key><summary>Unassigned replica shards after node startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">binoyaf</reporter><labels /><created>2014-07-19T13:07:23Z</created><updated>2015-11-21T16:15:17Z</updated><resolved>2015-11-21T16:15:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-19T14:16:47Z" id="49510652">Are you using different versions of Java on the two nodes?
</comment><comment author="binoyaf" created="2014-07-19T16:49:29Z" id="49514570">No, both use Oracle JRE 1.6.0_26. The reason why it fails is because I use a custom classloader in my application and so initialize the embedded elasticsearch node as follows-

```
ImmutableSettings.Builder settingsBuilder = 
    ImmutableSettings.settingsBuilder().classLoader(getClass().getClassLoader()).put("cluster.name", "myCluster")...
```

But this gets overwritten later on during plugin initialization (as part of node startup) due to the code fragment I had highlighted earlier, and the classloader instance in ImmutableSettings get set to null. When Node-2 throws the DelayRecoveryException, the default webapp classloader will not be able to find this class (it requires the custom classloader I use).
</comment><comment author="clintongormley" created="2014-07-19T18:45:47Z" id="49526430">sorry, I've just seen the full issue report above - missed that when reading the email. I'll leave this to somebody who knows :)
</comment><comment author="ern" created="2014-08-05T14:20:31Z" id="51203696">Would like to comment that I am seeing the same stack trace using elasticsearch in an embedded tomcat where we have a custom classloader, see https://jira.sakaiproject.org/browse/SAK-27658
suggestions are welcome :)
</comment><comment author="clintongormley" created="2015-11-21T16:15:17Z" id="158659432">We no longer use Java serialization for exceptions. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Embedding query parameter dynamically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6930</link><project id="" key="" /><description>```
Hi,
I have for loop with this query:
$intervalStr="4h";
$timeFrom = strtotime("-24 hour", $time);
$timeTo =  $time;

$json = '{
"aggs" : {
    "value_ranges" : {
        "range" : {
            "field" : "timestamp",
            "ranges" :[
        { "from": {$timeFrom} ,
          "to":   {$timeTo} }
        ]
        },
        "aggs" : {
            "value_stats" : {
                "stats" : { "field" : "value" }

        },
                    "aggs" : {
          "date_histogram" : {
            "field" : "timestamp",
            "interval" : {$intervalStr}
        },
        "aggs" : {
            "value_stats" : {
                "stats" : { "field" : "value" }
        }
    }
 }
```

  }
}
}
}';

```
$params['index'] = 'sensors_1';
$params['type']  = 'sensor_raw_data';
$params['body']  = $json;
$results = $ElasticsearchPHPclient-&gt;search($params);
```

I want to chang in every iteration the time ranges and interval parameters.
Is it possible? 
How?

Thank You.
</description><key id="38231142">6930</key><summary>Embedding query parameter dynamically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">deddy83</reporter><labels /><created>2014-07-19T10:46:32Z</created><updated>2014-07-19T11:56:22Z</updated><resolved>2014-07-19T11:56:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-19T11:56:22Z" id="49507435">Hi @deddy83 

Please ask these questions on the mailing list.  The github issues list is for bug reports and feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Collate option in PhraseSuggester should allow returning phrases with no matching docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6929</link><project id="" key="" /><description>The new option `return_all_phrases` in PhraseSuggester collate will allow the user to control whether all the generated suggestions would be returned. Setting the option will have an additional field `match_exists` in the suggestion options, indicating if there were matched documents for the phrase. The default value for this is `false`.

Currently the request would look as follows:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
   "suggest" : {
     "text" : "Xor the Got-Jewel",
     "simple_phrase" : {
       "phrase" : {
         "field" :  "bigram",
         "size" :   1,
         "direct_generator" : [ {
           ...
         } ],
         "collate": {
           "query": { 
              ...
           },
           "return_all_phrases": true
         }
       }
     }
   }
 }
```

and the response looks as follows (only when `return_all_phrases` is set to `true`)

``` bash
"suggest" : {
    "simple_phrase" : [ {
      "text" : "Xor the Got-Jewel",
      "offset" : 0,
      "length" : 17,
      "options" : [ {
        "text" : ...,
        "highlighted": ...,
        "score" : ...,
        "match_exists" : true
      }, {
        "text" : ...,
        "highlighted": ...,
        "score" : ..,
        "match_exists": false
      } ]
    } ]
  }
```

Closes #6927
</description><key id="38220017">6929</key><summary>Collate option in PhraseSuggester should allow returning phrases with no matching docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T23:29:11Z</created><updated>2015-06-07T12:39:59Z</updated><resolved>2014-07-22T21:19:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-22T14:35:29Z" id="49747242">I left some naming comments
</comment><comment author="areek" created="2014-07-22T16:08:12Z" id="49760703">Thanks for the review, @s1monw! I updated the PR with the feedback.
</comment><comment author="areek" created="2014-07-22T16:48:13Z" id="49766235">Now the request looks like:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
   "suggest" : {
     "text" : "Xor the Got-Jewel",
     "simple_phrase" : {
       "phrase" : {
         "field" :  "bigram",
         "size" :   1,
         "direct_generator" : [ {
           ...
         } ],
         "collate": {
           "query": { 
              ...
           },
           "prune": true
         }
       }
     }
   }
 }
```

and the response:

``` bash
"suggest" : {
    "simple_phrase" : [ {
      "text" : "Xor the Got-Jewel",
      "offset" : 0,
      "length" : 17,
      "options" : [ {
        "text" : ...,
        "highlighted": ...,
        "score" : ...,
        "collate_match" : true
      }, {
        "text" : ...,
        "highlighted": ...,
        "score" : ..,
        "collate_match": false
      } ]
    } ]
  }
```

If the `collate` option is not set, then `collate_match` is not included in the response.
</comment><comment author="s1monw" created="2014-07-22T19:28:01Z" id="49788207">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a `index.query.parse.allow_unmapped_fields` setting that fails if queries refer to unmapped fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6928</link><project id="" key="" /><description>The percolator and filter parsing for aliases should forcefully enforce strict query parsing.

Strict parsing for percolator and filter alias parsing is only enforced on indices created after to upgrade to `1.4.0`

PR for #6664
</description><key id="38211271">6928</key><summary>Add a `index.query.parse.allow_unmapped_fields` setting that fails if queries refer to unmapped fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Settings</label><label>breaking</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T20:52:11Z</created><updated>2015-06-06T16:46:19Z</updated><resolved>2014-09-09T13:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-21T07:38:30Z" id="49578478">@martijnvg Left some comments.
</comment><comment author="martijnvg" created="2014-07-21T19:26:56Z" id="49653080">Thanks @jpountz for the comments, I updated the PR.
</comment><comment author="martijnvg" created="2014-07-28T16:27:21Z" id="50361881">@jpountz Thanks for your review. I updated the PR.
</comment><comment author="jpountz" created="2014-08-12T07:03:44Z" id="51880300">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PhraseSuggester: Collate option should allow returning phrases with no matching docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6927</link><project id="" key="" /><description>Currently `collate` option in `PhraseSuggester` only filters out phrases that did not match any documents. It would be useful if it also allowed returning all the generated phrases with  some indicator to whether there were any doc matches for the entries.

Related #3482
</description><key id="38206798">6927</key><summary>PhraseSuggester: Collate option should allow returning phrases with no matching docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T19:50:45Z</created><updated>2014-07-22T21:19:01Z</updated><resolved>2014-07-22T21:19:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Completion mapping type throws a misleading error on null value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6926</link><project id="" key="" /><description>When the mapper service gets a null value for a field, it tries to parse it with the mapper of any previously seen field, if that mapper happens to be CompletionMapper, it throws an error, as it only accepts certain fields.

Closes #6399
</description><key id="38206328">6926</key><summary>Completion mapping type throws a misleading error on null value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T19:44:05Z</created><updated>2015-06-07T19:21:16Z</updated><resolved>2014-08-01T19:28:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-07-25T18:50:58Z" id="50189425">Updated PR: Now having a null value for a completion field will throw an exception. 
</comment><comment author="jpountz" created="2014-08-01T14:26:30Z" id="50890191">LGTM
</comment><comment author="areek" created="2014-08-01T19:14:26Z" id="50923660">@jpountz thanks for the review, I have changed the name to `supportsNullValue` (I like it better). I will commit this shortly if there are not objections.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `index.merge.scheduler.max_thread_count` to be dynamically changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6925</link><project id="" key="" /><description>Lucene allows the max_thread_count to be updated, but this wasn't
fully exposed in Elasticsearch.

Closes #6882
</description><key id="38191491">6925</key><summary>Allow `index.merge.scheduler.max_thread_count` to be dynamically changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T16:30:01Z</created><updated>2015-06-07T12:40:09Z</updated><resolved>2014-07-22T15:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-18T16:37:44Z" id="49452222">Thanks!
</comment><comment author="s1monw" created="2014-07-22T14:37:40Z" id="49747528">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow GET access to _all field (return value was always null before)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6924</link><project id="" key="" /><description>GET only returned null even when stored if requested with GET like this:

`curl -XGET "http://localhost:9200/test/test/1?fields=_all"`

Instead, it should simply behave like a String field and return the concatenated fields as String.
</description><key id="38187431">6924</key><summary>Allow GET access to _all field (return value was always null before)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:CRUD</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T15:41:41Z</created><updated>2015-06-07T12:40:30Z</updated><resolved>2014-07-23T07:15:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-18T17:29:57Z" id="49457841">Left small comments. LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: Support regular expressions for CORS allow-origin to match against</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6923</link><project id="" key="" /><description>This commit adds regular expression support for the allow-origin
header depending on the value of the request `Origin` header.

Relates #5601
Closes #6891
</description><key id="38175996">6923</key><summary>Security: Support regular expressions for CORS allow-origin to match against</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T13:29:56Z</created><updated>2015-06-07T12:40:45Z</updated><resolved>2014-07-25T08:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-21T08:25:56Z" id="49581545">@spinscale I reviewed it and left some comments. Overall I think it looks good
</comment><comment author="spinscale" created="2014-07-21T11:00:56Z" id="49592983">@s1monw added tests, incorporated your comments
</comment><comment author="s1monw" created="2014-07-21T11:05:20Z" id="49593256">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexed scripting should be controlled from a different settings flag than dynamic scripting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6922</link><project id="" key="" /><description>Currently if dynamic scripting is disabled indexed scripts won't work, this is due to the fact that if an attacker has access to the REST endpoints they could just index a doc into .scripts and then run it from a query.

I would be great if there was a different config setting so sysadmins could protect the endpoints and script index with a proxy disable dynamic script but allow running indexed scripts.
</description><key id="38175964">6922</key><summary>Indexed scripting should be controlled from a different settings flag than dynamic scripting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>enhancement</label></labels><created>2014-07-18T13:29:34Z</created><updated>2015-03-19T15:05:23Z</updated><resolved>2014-12-31T12:08:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-18T13:34:03Z" id="49430799">@GaelTadh indexed scripts should still work through the sandbox, correct? They don't bypass it in any way do they?
</comment><comment author="GaelTadh" created="2014-07-18T13:37:10Z" id="49431121">Yes they work exactly like inline scripts right now, but they are loaded from the index instead of being read from the query.
</comment><comment author="gibrown" created="2014-07-30T17:35:27Z" id="50651208">I'd go further and say that disabling dynamic scripts should be controllable on an endpoint by endpoint basis. 

For instance we do pass queries through to ES (and do lots of checking to prevent perf and security problems), but index and update ops are not exposed at all. Right now in order to do update ops I have to enable dynamic scripting for all endpoints. I'd feel better if I could enable dynamic scripting only for those endpoints I actually want them to run on.
</comment><comment author="clintongormley" created="2014-12-31T12:08:16Z" id="68438385">Closed in favour of #6418
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced pluggable filter chain to be able to filter transport actions execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6921</link><project id="" key="" /><description>It's now possible to inject action filters from plugins via `ActionModule#registerFilter` through the following code:

```
    public void onModule(ActionModule actionModule) {
          actionModule.registerFilter(MyFilter.class);
    }
```

Also made `TransportAction#execute` methods final to enforce the execution of the filter chain. By default the chain is empty though.

Note that the action filter chain is executed right after the request validation, as the filters might rely on a valid request to do their work.
</description><key id="38170587">6921</key><summary>Introduced pluggable filter chain to be able to filter transport actions execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T12:09:03Z</created><updated>2015-06-07T12:41:31Z</updated><resolved>2014-07-18T14:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-18T13:00:34Z" id="49427718">@uboness I pushed some new commits to address your comments, can you have another look please?
</comment><comment author="uboness" created="2014-07-18T13:10:20Z" id="49428574">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Qualify termQuery method under the Boolean Query section</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6920</link><project id="" key="" /><description>As static keyword was not used when importing QueryBuilders, the termQuery method needs properly qualified when being passed to the must/mustNot methods in the Boolean Query section of the documentation
</description><key id="38165998">6920</key><summary>Qualify termQuery method under the Boolean Query section</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">mikelrob</reporter><labels><label>docs</label></labels><created>2014-07-18T10:49:31Z</created><updated>2014-09-23T10:41:05Z</updated><resolved>2014-09-23T10:41:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T11:19:27Z" id="49420373">Wouldn't it be better to use a static import instead? Would make the example look less intimidating.

Also, please could you sign the CLA? http://www.elasticsearch.org/contributor-agreement/
thanks
</comment><comment author="mikelrob" created="2014-07-18T12:01:51Z" id="49423144">yes @clintongormley, i agree. however the import statement at the top appears to apply to the whole document. That would require changing all other qualified methods for consistency, something i didn't fancy doing.
</comment><comment author="clintongormley" created="2014-07-18T12:55:57Z" id="49427338">I ran into the same issue as you and the solution was to do the static import, rather than qualifying every call, so that's what we should document.  If you're keen to do it, please do, otherwise i'll get to it sometime.
</comment><comment author="clintongormley" created="2014-07-23T10:19:31Z" id="49856344">@dadoonet Please could you look through the Java API docs and `static` imports where necessary, and remove the `QueryBuilders.` prefix where no longer necessary?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update import QueryBuilders statement in query-dsl-queries.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6919</link><project id="" key="" /><description /><key id="38165031">6919</key><summary>Update import QueryBuilders statement in query-dsl-queries.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikelrob</reporter><labels /><created>2014-07-18T10:34:24Z</created><updated>2014-07-18T11:17:59Z</updated><resolved>2014-07-18T10:38:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T11:17:00Z" id="49420207">@mikelrob did you mean to close this?
</comment><comment author="clintongormley" created="2014-07-18T11:17:59Z" id="49420271">AH I see you did: #6920 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force faster failures on slow requests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6918</link><project id="" key="" /><description>I'm purposefully issuing a badly formed query into my cluster which has enough indices for the returned exception message to exceed 200kb (around ~630 indices).

```
POST /_search?timeout=1000&amp;master_timeout=1000
{
  "query": { "badjson": {}  }
}
```

I then start issuing ~20 of these in repetition:

![image](https://cloud.githubusercontent.com/assets/245275/3624732/0404796a-0e61-11e4-891e-204e4fc482b9.png)

While the first request comes back with a `400` all the others return `503` and some of them take `14s` to complete even if I'm specifying timeouts on the request. I suppose they are waiting for the queue since a single search takes 600 of the 1000 slots (for each index?), I'm assuming this because the 503's return and im seeing a 503 straight after the first request.

EsRejectedExecutionException[rejected execution (queue capacity 1000) 

Am i missing a toggle here? Ideally the timeout also forces requests to never exceed the specified timeout and return with an appropiate HTTP status code (504?).
</description><key id="38163857">6918</key><summary>Force faster failures on slow requests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2014-07-18T10:14:54Z</created><updated>2014-07-18T11:12:39Z</updated><resolved>2014-07-18T11:12:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T11:12:39Z" id="49419916">Closed in favour of #4586 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Make order more flexible for terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6917</link><project id="" key="" /><description>Terms aggregations only allow to sort based on the UTF-8 bytes of the term, its count or a sub-aggregation.

We should add more flexibility here to allow for doing things like having compound orders, sorting by collation or by a script that would apply to the sub aggregations.
</description><key id="38161529">6917</key><summary>Aggregations: Make order more flexible for terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>feature</label></labels><created>2014-07-18T09:38:58Z</created><updated>2016-11-06T07:41:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-25T10:29:37Z" id="50132794">its tricky for collation because there is not a 1-to-1 mapping of keys to original values. collation rules may collapse two terms into the same key, for example because the strength determines it should ignore case, so "Test" and "test" get the same key... so what should the aggregation do in such a case, what would be the output?
</comment><comment author="maik2102" created="2014-08-20T10:58:25Z" id="52760850">In a first step it would be great to combine a sub-aggregation with the always given _count aggregation.
E.g.: I want to boost my best eCommerce categories, but if the "boost" is equal, sort by the count of matching products.
</comment><comment author="brusic" created="2014-08-20T14:28:35Z" id="52785632">@maik2102, sounds like you want secondary sorting, which is a feature I would like as well.
</comment><comment author="hsm3" created="2015-02-16T16:54:00Z" id="74538535">We are also doing ecommerce, and have groups of products being returned via a terms agg.  We would really like to use a secondary sort to improve the fine-grain ordering, and break ties to make the sort stable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better message for invalid internal transport message format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6916</link><project id="" key="" /><description /><key id="38160542">6916</key><summary>Better message for invalid internal transport message format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T09:25:14Z</created><updated>2015-06-07T12:41:48Z</updated><resolved>2014-07-18T11:53:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-18T09:27:49Z" id="49411992">LGTM
</comment><comment author="kimchy" created="2014-07-18T11:53:56Z" id="49422599">closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring to make MessageChannelHandler extensible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6915</link><project id="" key="" /><description>Small refactorings to make the MessageChannelHandler more extensible.
Also allowed access to the different netty pipelines

This is the fix after the first version had problems with the HTTP
transport due to wrong reusing channel handlers, which is the reason
why tests failed. This version now uses its own ChannelPipelineFactory
again.

Relates #6889 (which had been reverted in the first place)
</description><key id="38156777">6915</key><summary>Refactoring to make MessageChannelHandler extensible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-18T08:22:56Z</created><updated>2015-06-07T12:42:02Z</updated><resolved>2014-07-18T14:31:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-18T11:58:39Z" id="49422928">LGTM
</comment><comment author="kimchy" created="2014-07-18T12:04:40Z" id="49423336">LGTM
</comment><comment author="spinscale" created="2014-07-18T14:12:39Z" id="49435039">created static classes and added tests, should be ready now
</comment><comment author="uboness" created="2014-07-18T14:18:01Z" id="49435616">few comments, other than that LGTM
</comment><comment author="uboness" created="2014-07-18T14:26:30Z" id="49436571">LGTM
</comment><comment author="mattweber" created="2014-07-18T14:36:28Z" id="49437711">Awesome, this should make it possible for someone to write the SSL transport support as a plugin.  I had investigated going down this route a while back but got stuck because of the shading of the Netty jar.  It tossed a bunch of the Netty code that was not used in ES but needed if I wanted to add the SSL support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Task management</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6914</link><project id="" key="" /><description>We need a task management API to allow management of long running tasks, like snapshot/restore, benchmarking, update-by-query etc.

The API should allow listing and aborting of ongoing tasks.
</description><key id="38154842">6914</key><summary>Task management</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Task Manager</label><label>feature</label></labels><created>2014-07-18T07:49:05Z</created><updated>2016-04-01T16:05:22Z</updated><resolved>2015-11-30T15:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="1st" created="2014-07-18T11:21:02Z" id="49420492">+1. Waiting for this new feature!!!
</comment><comment author="markharwood" created="2014-08-01T08:33:28Z" id="50861463">Part of killing long running tasks will be tied up in this new generic timeout handling mechanism: https://github.com/elasticsearch/elasticsearch/pull/4586
</comment><comment author="shikhar" created="2014-08-05T04:03:43Z" id="51147564">Will this cover search requests? Since one can't know in advance very well if a search request will be long-running, I guess this task tracking would need to apply to all of them.

If this is not the plan, I think #7157 should be reopened.
</comment><comment author="martijnvg" created="2014-08-05T09:58:46Z" id="51176753">@shikhar The task infrastructure will be generic enough that it can be adopted in the search api as well.
</comment><comment author="efuquen" created="2014-08-13T19:36:25Z" id="52099060">:+1:  Not having this has been a pain.
</comment><comment author="shadow000fire" created="2014-08-20T17:42:59Z" id="52813976">+1
</comment><comment author="ashvins" created="2014-08-28T20:38:07Z" id="53793781">+1
</comment><comment author="JDvorak" created="2014-10-28T17:07:42Z" id="60792719">:+1: 
</comment><comment author="sflint" created="2015-01-12T21:13:19Z" id="69646963">+1
</comment><comment author="niemyjski" created="2015-01-20T22:17:01Z" id="70745716">+1
</comment><comment author="raf64flo" created="2015-01-27T15:05:37Z" id="71662203">Looking at it carefully. :+1: 
</comment><comment author="smramdani" created="2015-02-23T09:57:23Z" id="75515045">+1
Would be a great feature as "running queries" and "long queries" in the classical RDBMSs.
</comment><comment author="kennycason" created="2015-03-01T18:08:15Z" id="76621683">+1
</comment><comment author="tostasqb" created="2015-03-11T17:44:55Z" id="78316034">+1
</comment><comment author="kiryam" created="2015-04-07T10:55:11Z" id="90506308">+1
</comment><comment author="chenryn" created="2015-04-08T03:23:19Z" id="90792605">+1
</comment><comment author="kjelle" created="2015-08-19T18:28:58Z" id="132733605">+1
</comment><comment author="amontalenti" created="2015-09-05T20:12:11Z" id="137990364">+1
</comment><comment author="BrickXu" created="2015-10-21T10:15:52Z" id="149846548">+1
</comment><comment author="imotov" created="2015-11-30T15:31:25Z" id="160661976">I created a new meta issue #15117 where work on the task management will be tracked. Closing this one.
</comment><comment author="garyelephant" created="2016-01-04T07:23:28Z" id="168602096">+1
</comment><comment author="rfyiamcool" created="2016-03-31T09:07:45Z" id="203835698">thanks 
</comment><comment author="garyelephant" created="2016-04-01T02:22:13Z" id="204214248">Will this cover resource limit(similar with spark job executor's cpu,mem resource limit) using cgroups or something ? For Users who want to aggregate more than 1TB of index, it's likely to cause overall elasticsearch cluster overload, no response.
</comment><comment author="imotov" created="2016-04-01T16:05:22Z" id="204451410">@garyelephant there are no immediate plans to do that as a part of task management effort. However, we are working on some improvements in this area. Please see the discussion in #11511.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>completion suggester for more than one field?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6913</link><project id="" key="" /><description>Is it possible to use completion suggester for more than one field? Assuming all of them are of type "completion".
Something like this 

{
    "song-suggest" : {
        "text" : "n",
        "completion" : {
            "fields" : ["suggest", "name", "author", "smthElse"]
        }
    }
}
</description><key id="38153402">6913</key><summary>completion suggester for more than one field?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vasupft</reporter><labels /><created>2014-07-18T07:19:34Z</created><updated>2014-07-22T12:55:32Z</updated><resolved>2014-07-18T10:07:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-18T10:07:22Z" id="49415299">No it isn't.  But you could use `copy_to` to copy the values from multiple fields to a single field, and add the suggester on that field instead.
</comment><comment author="vasupft" created="2014-07-22T12:55:32Z" id="49734564">Thanks, let me try this out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: REST specs, 'resume' test, various minor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6912</link><project id="" key="" /><description>Adds REST API JSON definitions.
Adds test for resume functionality.
Various minor changes, e.g. adding getter

Benchmarks: Remove obsolete class.

Benchmarks: Remove broken REST tests

These tests need to be re-written so just removing them for now as they
are obsolete.

Benchmarks: Ignore incomplete tests

Benchmarks: Integration tests

Add test for aborting a running benchmark.

Benchmarks: Test for 'resume' functionality

Adding a test for resuming a paused benchmark.

Benchmarks: Verbose responses

Pass 'verbose' flag through to allow detailed responses.

Benchmarks: Add missing getter

Missing getter method.

Backing out mistaken commit
</description><key id="38146614">6912</key><summary>Benchmarks: REST specs, 'resume' test, various minor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-18T04:02:19Z</created><updated>2014-07-21T17:43:32Z</updated><resolved>2014-07-21T17:43:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Sort Throws Parse Failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6911</link><project id="" key="" /><description>In a query I have written I use the [exists filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-exists-filter.html) to bring back documents that contain a certain field, then I try to sort the documents by that field.

```
GET /index/type/_search
{
    "query": {
        "constant_score": {
            "filter": {
                "exists": {
                    "field": "some_field_that_doesnt_exist"
                }
            }
        }
    },
    "sort": [
        {
            "some_field_that_doesnt_exist": {
                "order": "asc"
            }
        }
    ]
}
```

If the query doesn't match any documents the sort part of the query throws a error

```
Parse Failure [No mapping found for [some_field_that_doesnt_exist] in order to sort on]]
```

Not sure if this is an issue, but I  kind of expected the sort part of the query not to run if the total hits is 0. But it could just be my issue
</description><key id="38130292">6911</key><summary>Sort Throws Parse Failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">TheoKouzelis</reporter><labels /><created>2014-07-17T21:51:30Z</created><updated>2014-07-18T08:57:33Z</updated><resolved>2014-07-18T07:32:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-18T07:32:57Z" id="49403239">I think you are doing something wrong. I can not reproduce it.
This works fine for me:

```
DELETE /test

PUT /test
{
  "mappings": {
    "test": {
      "properties": {
        "foo": {
          "type": "string"
        },
        "newfield": {
          "type": "string"
        }

      }
    }
  }
}
PUT /test/test/1
{
    "foo": "bar"
}
PUT /test/test/2
{
    "foo": "bar"
}

GET /test/test/_search
{
    "query": {
        "constant_score": {
            "filter": {
                "exists": {
                    "field": "newfield"
                }
            }
        }
    },
    "sort": [
        {
            "newfield": {
                "order": "asc"
            }
        }
    ]
}
```

Closing. Feel free to reopen with more details but my first guess is that you are querying on a type which does not know that field at all.
</comment><comment author="TheoKouzelis" created="2014-07-18T08:57:33Z" id="49409510">Thanks for your reply, sorry I should have provided a mapping with my ticket (doh). 

The problem arises when the field isn't included in the mapping. If you run my original query against your test mapping you should get the error.

But after looking at the docs again I found that adding &lt;code&gt;"ignore_unmapped" : true&lt;/code&gt; to the sort object solved the problem. 

Full query below.

```
{
    "query": {
        "constant_score": {
            "filter": {
                "exists": {
                    "field": "some_field_that_doesnt_exist"
                }
            }
        }
    },
    "sort": [
        {
            "some_field_that_doesnt_exist": {
                "ignore_unmapped" : true,
                "order": "desc"
            }
        }
    ]
}
```

Thanks for your help, and apologies if I have wasted anybody&#8217;s time.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update as-a-service.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6910</link><project id="" key="" /><description>There is a step missing where the auto-start on boot isn't enabled for RPM based installs.
</description><key id="38103621">6910</key><summary>Update as-a-service.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mobsniuk</reporter><labels /><created>2014-07-17T16:58:38Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2014-07-17T16:59:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Prevent init script from returning when the service isn't actually started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6909</link><project id="" key="" /><description>Hello,
Since the init script uses start-stop-daemon's `-b` option, it might return from _start_ before the service is actually started. If the init script's _status_ action is called immediately after that, It will return `elasticsearch is not running` and exit with a non-zero code.
This causes systems like Heartbeat to incorrectly start multiple instances of Elasticsearch at once.
This pull request aims at fixing this issue by waiting until the Elasticsearch process is started.
There might be a nicer way to achieve this but this is what came to mind.
</description><key id="38094365">6909</key><summary>Prevent init script from returning when the service isn't actually started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">sbraz</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-17T15:37:01Z</created><updated>2014-12-05T09:36:06Z</updated><resolved>2014-12-05T09:35:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T13:38:45Z" id="59754767">@spinscale please could you take a look
</comment><comment author="electrical" created="2014-10-20T13:52:34Z" id="59756839">With all the tests i do with the packages and init scripts i have never seen this sort of race condition happen.
</comment><comment author="clintongormley" created="2014-10-21T07:55:04Z" id="59891621">@sbraz Have you seen this problem occur, or is this a theoretical concern?
</comment><comment author="sbraz" created="2014-10-21T08:14:56Z" id="59893501">I have seen it occur when I started ElasticSearch with Pacemaker, it would sometimes start ES, check its status and find it stopped.
</comment><comment author="jpountz" created="2014-12-05T09:36:06Z" id="65766381">Sorry for the delay, but I just merged this PR. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch fielddata to use Lucene doc values APIs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6908</link><project id="" key="" /><description>This commits removes BytesValues/LongValues/DoubleValues/... and tries to use
Lucene's APIs such as NumericDocValues or RandomAccessOrds instead whenever
possible.

The next step would be to take advantage of the fact that APIs are the same in
Lucene and Elasticsearch in order to remove our custom comparators and use
Lucene's.

There are a few side-effects to this change:
- GeoDistanceComparator has been removed, DoubleValuesComparator is used instead
  on top of dynamically computed values (was easier than migrating
  GeoDistanceComparator).
- SortedNumericDocValues doesn't guarantee uniqueness so long/double terms
  aggregators have been updated to make sure a document cannot fall twice in
  the same bucket.
- Sorting by maximum value of a field or running a `max` aggregation is
  potentially significantly faster thanks to the random-access API.

Our aggs and p/c aggregations benchmarks don't report differences with this
change on uninverted field data. However the fact that doc values don't need
to be wrapped anymore seems to help a lot. For example
TermsAggregationSearchBenchmark reports ~30% faster terms aggregations on doc
values on string fields with this change, which are now only ~18% slower than
uninverted field data although stored on disk.
</description><key id="38094300">6908</key><summary>Switch fielddata to use Lucene doc values APIs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-17T15:36:23Z</created><updated>2015-06-07T12:47:58Z</updated><resolved>2014-07-22T13:16:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-18T15:23:32Z" id="49443510">overall looks good, i took a pass thru and added comments (it seems like you already responded/addressed most of them). Thanks for doing this!
</comment><comment author="jpountz" created="2014-07-21T10:28:00Z" id="49590729">@rmuir I pushed new commits to address your comments.
</comment><comment author="rmuir" created="2014-07-21T13:29:07Z" id="49604934">+1 Looks good, i added a comment to StringTermsAggregator.java to ask for a followup as it can have performance implications.
</comment><comment author="s1monw" created="2014-07-21T15:31:01Z" id="49621119">I left a bunch of comments mostly cosmetic looks good though
</comment><comment author="jpountz" created="2014-07-21T16:27:00Z" id="49629058">@rmuir @s1monw Pushed a new commit addressing your comments.
</comment><comment author="s1monw" created="2014-07-22T12:23:07Z" id="49731482">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Random score order changes on doc updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6907</link><project id="" key="" /><description>Not sure how the random score is calculated, but it seems like the docs positions are changing on updates.
Our docs are continuously getting updates in some cron-jobs or by user actions, thus the pagination is not really possible on random scored lists.

What exactly is elasticsearch using to generate random score besides the provided seed?
Why not just taking the doc UID &amp; the seed to calculate the score?

Btw. there was a post a while ago on your forums:
https://groups.google.com/forum/#!topic/elasticsearch/QOP3kSK5qR0
</description><key id="38088359">6907</key><summary>Random score order changes on doc updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">heyarny</reporter><labels /><created>2014-07-17T14:50:53Z</created><updated>2014-08-27T15:58:47Z</updated><resolved>2014-08-27T15:43:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-07-18T16:15:53Z" id="49449799">The order docs are scored in (and thus what order the RNG is generating random scores with) is based on the order the documents appear in lucene segments. Merges will also change this by creating new segments.  So even adding or deleting docs can eventually cause the order of existing docs to change with the same seed.

I think we could fix this by having another parameter, `field`, which the RNG can use per document to generate the score.  Instead of updating the seed, the RNG will use that per document value as the randomness (but still incorporate the seed, so a different order can be achieved without reindexing documents).  If the field is numeric, it would use the value of the field.  If it is anything else, it would use a hash of the byte values. Thoughts?
</comment><comment author="heyarny" created="2014-07-18T23:47:28Z" id="49492048">Yes, actually ANY other solution other than the current one would make more sense, as long the scores/positions stay the same per seed and per doc/field.

Is there any workaround one could use as of now? Preferably one which is robust and performant enough.
</comment><comment author="clintongormley" created="2014-07-20T16:34:11Z" id="49551775">@rjernst While you're working on this, I think that `random_score` needs a `max` parameter.  Currently it gives a large number like 8207847.5.  It is impossible to combine this with other `function_score` functions to just introduce a little randomness to the results.  

For instance, imagine you are filtering on "features":

```
GET /_search
{
  "query": {
    "function_score": {
      "filter": { 
        "term": { "city": "Barcelona" }
      },
      "functions": [
        {
          "filter": { "term": { "features": "wifi" }}, 
          "boost_factor": 1
        },
        {
          "filter": { "term": { "features": "garden" }}, 
          "boost_factor": 1
        },
        {
          "filter": { "term": { "features": "pool" }}, 
          "boost_factor": 2 
        }
      ],
      "score_mode": "sum", 
    }
  }
}
```

Documents can have a score of 1, 2, 3, 4 or 5.  It would be nice to use randomization here to randomize all docs that have a score of 2.  In other words, docs with an original score of 2 should end up with: `2.0 &lt;= score &lt;= 3.0.`   Currently, the result from `random_score` would completely swamp the other functions.

You could do this by specifying a `max` of `0.99`:

```
GET /_search
{
  "query": {
    "function_score": {
      "filter": { 
        "term": { "city": "Barcelona" }
      },
      "functions": [
        {
          "filter": { "term": { "features": "wifi" }}, 
          "boost_factor": 1
        },
        {
          "filter": { "term": { "features": "garden" }}, 
          "boost_factor": 1
        },
        {
          "filter": { "term": { "features": "pool" }}, 
          "boost_factor": 2 
        },
        {
          "random_score": {
            "seed": "foo",
            "max":  0.99
          }
        }
      ],
      "score_mode": "sum", 
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-07-20T17:07:36Z" id="49552793">And while you're about it, probably worth supporting a `min` parameter as well...
</comment><comment author="rjernst" created="2014-07-21T17:57:38Z" id="49641325">I've done an initial implementation using _uid, and it is about 3x slower than the current random score.  I discussed with @rmuir a bunch and after thinking about it more, it seems this will always be a problem, even when using something "consistent" from a document (whether _uid or a unique numeric field), since segments can always be moved around as adds/updates/deletes happen.

@heyarny Have you tried using random score with a [scroll](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html)? This _should_ allow a consistent view, given the same seed for each request (although it is possible there is an edge case; I'm working on adding a test).

@clintongormley I think the return values for random score are just wrong right now.  Java's Random.nextFloat() returns a value 0.0 - 1.0.  We should fix random score to do exactly this, and the user can scale it to their range however they want.
</comment><comment author="clintongormley" created="2014-07-21T18:58:42Z" id="49649453">@rjernst Is the performance drop off as bad for a numeric field instead of `_uid`.  Also, is 3x slower significant?  I mean, if it is 3 x very fast, it's still fast.

Scrolling isn't really useful here - the user would have to pull, eg, 10 pages of results and cache them somewhere.  Scrolling doesn't let you click forward then back (although the page could be cached i suppose). Also, keeping lots of scrolls open uses a lot of extra filehandles.

Re the return value, 0.0 - 1.0 may be OK, but if you want to change that range, it is very difficult to do so.  Supporting it directly in `random_score` would make it a whole lot cleaner.
</comment><comment author="rjernst" created="2014-07-31T17:40:15Z" id="50792614">@clintongormley You are right, I think it is still pretty fast? Sorting 1 million random docs takes on average 13ms with _uid, instead of 4ms as it is now.  Using a numeric field is about the same (5ms for 1mil docs).

I think the `weight` parameter in #6955 will work if you want a value outside of 0.0-1.0?
</comment><comment author="clintongormley" created="2014-07-31T18:29:05Z" id="50799180">@rjernst great, and re the  `weight` parameter: yes a much better solution
</comment><comment author="jpountz" created="2014-08-04T06:41:12Z" id="51022418">Big +1 on making this function return a score between 0 and 1.
</comment><comment author="stha" created="2014-08-15T10:51:06Z" id="52294625">@rjernst Is it possible that the explain on random score also does not work as intended?

Example Query:

``` json
{
  "explain": true,
  "size": 1,
  "_source": [
    "age"
  ],
  "query": {
    "function_score": {
      "boost_mode": "replace",
      "score_mode": "sum",
      "boost": 1,
      "functions": [
        {
          "random_score": {
            "seed": 123
          }
        }
      ]
    }
  }
}
```

Result:

``` json
{
   "took": 87,
   "timed_out": false,
   "_shards": {
      "total": 2,
      "successful": 2,
      "failed": 0
   },
   "hits": {
      "total": 1758039,
      "max_score": 16777215,
      "hits": [
         {
            "_shard": 1,
            "_node": "EC5O22oVR-aBhjrlgjESqg",
            "_index": "index_profile",
            "_type": "type_profile",
            "_id": "697639",
            "_score": 16777215,
            "_source": {
               "age": 27
            },
            "_explanation": {
               "value": 0,
               "description": "function score, product of:",
               "details": [
                  {
                     "value": 0,
                     "description": "Math.min of",
                     "details": [
                        {
                           "value": 0,
                           "description": "function score, score mode [sum]",
                           "details": [
                              {
                                 "value": 0,
                                 "description": "function score, product of:",
                                 "details": [
                                    {
                                       "value": 1,
                                       "description": "match filter: *:*"
                                    },
                                    {
                                       "value": 0,
                                       "description": "random score function (seed: -1443338702012022662)",
                                       "details": [
                                          {
                                             "value": 1,
                                             "description": "ConstantScore(*:*), product of:",
                                             "details": [
                                                {
                                                   "value": 1,
                                                   "description": "boost"
                                                },
                                                {
                                                   "value": 1,
                                                   "description": "queryNorm"
                                                }
                                             ]
                                          }
                                       ]
                                    }
                                 ]
                              }
                           ]
                        },
                        {
                           "value": 3.4028235e+38,
                           "description": "maxBoost"
                        }
                     ]
                  },
                  {
                     "value": 1,
                     "description": "queryBoost"
                  }
               ]
            }
         }
      ]
   }
}
```
</comment><comment author="s1monw" created="2014-08-19T20:40:26Z" id="52695334">I agree what @rjernst proposes is a much better solution. Even if we loose some perf here I think we can bring it back by improving _uid in the future. +1 to fix this
</comment><comment author="rjernst" created="2014-08-19T22:35:19Z" id="52709794">@stha What version of elasticsearch are you running on? There are existing tests to check explain correctly contains the original seed.
</comment><comment author="stha" created="2014-08-20T06:34:29Z" id="52738975">@rjernst It's ES `1.2.1`.

Edit: I can reproduce the `explain` issue as well with ES `1.3.2`.
</comment><comment author="rjernst" created="2014-08-25T23:29:15Z" id="53351882">@stha I found the issue, and I've added a fix and test.

I have a PR open now to fix the consistency, original seed reporting, and range of values produced.
See #7446
</comment><comment author="stha" created="2014-08-26T07:13:05Z" id="53383503">@rjernst Thanks! Are you going to fix this in `1.2` and `1.3` too?
</comment><comment author="rjernst" created="2014-08-26T15:57:58Z" id="53444394">Since the PR changes existing behavior, it will only be added to 1.4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When serializing HttpInfo, return null info if service is not started</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6906</link><project id="" key="" /><description>Http/TransportInfo throw NPE if the bound address is null and the info
object is serialized. There is a small window where at least the
HttpInfo can be requested before the service is fully started.
</description><key id="38080906">6906</key><summary>When serializing HttpInfo, return null info if service is not started</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-17T13:33:30Z</created><updated>2015-06-07T19:21:30Z</updated><resolved>2014-07-17T14:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-17T14:40:43Z" id="49315690">Left two very minor comments but other than that it looks good to me
</comment><comment author="s1monw" created="2014-07-17T14:46:52Z" id="49316564">@jpountz thx pushed a new commit
</comment><comment author="jpountz" created="2014-07-17T14:49:59Z" id="49317045">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[CLIENT] Unknown node version should be a lower bound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6905</link><project id="" key="" /><description>Today when we start a `TransportClient` we use the given transport
addresses and create a `DiscoveryNode` from it without knowing the
actual nodes version. We just use the `Version.CURRENT` which is an
upper bound. Yet, the other node might be a version less than the
currently running and serialisation of the nodes info might break. We
should rather use a lower bound here which is the version of the first
release with the same major version as `Version.CURRENT` since this is
what we officially support.

This commit moves to use the minimum major version or an RC / Snapshot
if the current version is a snapshot.

Closes #6894
</description><key id="38076930">6905</key><summary>[CLIENT] Unknown node version should be a lower bound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-07-17T12:41:36Z</created><updated>2014-07-18T09:32:31Z</updated><resolved>2014-07-18T09:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-17T14:21:32Z" id="49312888">looks great, wonder if we can write a specific small test for it, but we do verify it through backwards compatibility test which seems enough
</comment><comment author="s1monw" created="2014-07-17T14:28:05Z" id="49313903">I don't know how to really test if that has implications except of doing bwc test... do you have a good idea @javanna ?
</comment><comment author="javanna" created="2014-07-17T14:33:06Z" id="49314624">A bit out of ideas too...maybe we can simply check that whenever we add a transport address to a transport client the proper version is used, just to make sure we have a specific failing test if we lose this change? we should be able to retrieve the listed nodes and check their version.
</comment><comment author="s1monw" created="2014-07-17T20:10:13Z" id="49358726">@javanna yeah I agree I should test this method at least
</comment><comment author="s1monw" created="2014-07-17T20:10:30Z" id="49358768">I'd want to have @kimchy look at this too though...
</comment><comment author="s1monw" created="2014-07-17T21:00:46Z" id="49364983">@javanna I pushed some fixes and testcases
</comment><comment author="javanna" created="2014-07-17T21:41:43Z" id="49370708">@s1monw left a couple of small comments, looks better though!
</comment><comment author="s1monw" created="2014-07-18T08:35:35Z" id="49407728">@javanna fixed the test though...
</comment><comment author="javanna" created="2014-07-18T08:51:59Z" id="49409037">Looks great @s1monw go push!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Discovery: If not enough possible masters are found, but there are masters to ping then these nodes should be resolved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6904</link><project id="" key="" /><description>After the findMaster() call we try to connect to the node and if it isn't the master we start looking for a new master via pinging again.

Note this PR is for the improve_zen branch.
</description><key id="38070520">6904</key><summary>Discovery: If not enough possible masters are found, but there are masters to ping then these nodes should be resolved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-07-17T10:59:31Z</created><updated>2015-05-18T23:30:48Z</updated><resolved>2014-07-17T20:36:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-17T12:11:52Z" id="49298582">Looking good. Left some comments here and there...
</comment><comment author="martijnvg" created="2014-07-17T12:54:06Z" id="49302359">@bleskes I updated the PR
</comment><comment author="bleskes" created="2014-07-17T17:43:23Z" id="49339580">LGTM!
</comment><comment author="martijnvg" created="2014-07-17T20:35:57Z" id="49361941">Closed via: https://github.com/elasticsearch/elasticsearch/commit/49d7bd8e4d245898f8e17df0e6eeae0357d830a2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API Mapping Annotations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6903</link><project id="" key="" /><description>It would be very helpful to have a set of Java annotations used to define mappings.

Right now the only option is to create a json using json builder, but that is cumbersome job especially since there are no good code examples.

The idea with annotation is to have all required annotations to define ES type mappings. These annotations would be defined on a POJO beans. 

Beside these annotation it would be good to have a classpath scanner that would scan for those annotated classes and create defined mappings. This scanner can also have an option (like many ORM tools have) to either CREATE (create mappings if they do not exist) or REINDEX (to re-index documents if mapping merge is not possible).

Please let me know if you are interested in this feature, since I am interested in contributed a code for it.
</description><key id="38069681">6903</key><summary>Java API Mapping Annotations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ssljivic</reporter><labels><label>discuss</label></labels><created>2014-07-17T10:45:26Z</created><updated>2014-10-31T10:11:19Z</updated><resolved>2014-10-31T10:11:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-17T10:48:52Z" id="49291498">FYI: I remember that @aloiscochard started a project some years ago here: https://github.com/aloiscochard/elasticsearch-osem

Not updated. :)
</comment><comment author="aloiscochard" created="2014-07-17T11:00:40Z" id="49292462">Woow, that's quite a long shot @dadoonet :-)

Indeed I did that 3 years ago (actually in my mind it feel like a century as I moved to two different programming languages since then!)

I would probably start with that fork:
https://github.com/poblish/elasticsearch-osem

I really don't maintain this at all, and have no plan to do it. But I would be very happy to help if you guys have any question :-)

Regards
</comment><comment author="ssljivic" created="2014-07-17T11:10:29Z" id="49293279">Yes @aloiscochard, I'll definitely review the code and try to reuse as much as possible.

There is also another repo by @lucboutier: 
https://github.com/lucboutier/elasticsearch-mapping-parent
</comment><comment author="clintongormley" created="2014-10-31T10:11:19Z" id="61240732">This seems like it would be better implemented outside Elasticsearch, as has been done for other languages like Ruby, Python, and Perl.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete unallocated shards under a cluster state task</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6902</link><project id="" key="" /><description>This is to prevent a rare racing condition where the very same shard gets allocated to the node after our sanity check that the cluster state didn't check and the actual deletion of the files.
</description><key id="38066688">6902</key><summary>Delete unallocated shards under a cluster state task</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Store</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-17T10:01:07Z</created><updated>2015-06-07T19:21:44Z</updated><resolved>2014-07-17T12:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-17T10:05:56Z" id="49287535">LGTM
</comment><comment author="s1monw" created="2014-07-17T12:47:15Z" id="49301663">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Script] use a specialize variable resolver for _score in mvel scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6901</link><project id="" key="" /><description /><key id="38060522">6901</key><summary>[Script] use a specialize variable resolver for _score in mvel scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-07-17T08:57:02Z</created><updated>2014-07-17T09:03:40Z</updated><resolved>2014-07-17T09:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-17T09:02:00Z" id="49277529">LGTM
</comment><comment author="s1monw" created="2014-07-17T09:03:40Z" id="49277693">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArrayIndexOutOfBoundsException when using 2-level terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6900</link><project id="" key="" /><description>Hi, I'm using ES 1.0.0.
And when using 2-level terms aggregation. In my response I get some failures consistently:
         {
            "index": "blah",
            "shard": 8,
            "status": 500,
            "reason": "ArrayIndexOutOfBoundsException[0]"
         },
I'm wondering what could be the reason for these errors? and does it imply that I may not be getting all the values?
</description><key id="38052739">6900</key><summary>ArrayIndexOutOfBoundsException when using 2-level terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">praj2</reporter><labels><label>bug</label></labels><created>2014-07-17T06:58:04Z</created><updated>2014-07-17T13:52:30Z</updated><resolved>2014-07-17T13:52:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-17T09:46:39Z" id="49285346">Hi @praj2 

Please provide a recreation of what you are doing.

thanks
</comment><comment author="praj2" created="2014-07-17T10:54:48Z" id="49291965">My ES node is setup to have 32 shards.

I store survey data in my ES, and each survey can have multiple versions.
I need to find out all possible survey names and their respective versions list.

Hence I query thus:
POST /blah/blah_type/_search
{
  "size": 0, 
  "aggs": {
    "SURVEYS": {
      "terms": {
        "field": "survey_type",
        "size": 100
      },
      "aggs": {
        "VERSIONS": {
          "terms": {
            "field": "version",
            "size": 10
          }
        }
      }
    }
  }
}

In my local setup with 5 shards, I get no errors. However in a remote cloud setup I get errors from 3-4 shards. What could be the issue?
Also, in case it helps, I set custom routing as MMyyyy from the date of the survey response.
</comment><comment author="clintongormley" created="2014-07-17T11:11:36Z" id="49293364">Can you paste the stack trace from the logs?  It may be related to this issue: #5048
</comment><comment author="praj2" created="2014-07-17T11:39:32Z" id="49295975">JSON response:
   "took": 1078,
   "timed_out": false,
   "_shards": {
      "total": 32,
      "successful": 28,
      "failed": 4,
      "failures": [
         {
            "index": "blah",
            "shard": 8,
            "status": 500,
            "reason": "ArrayIndexOutOfBoundsException[0]"
         },
         {
            "index": "blah",
            "shard": 3,
            "status": 500,
            "reason": "ArrayIndexOutOfBoundsException[0]"
         },
         {
            "index": "blah",
            "shard": 26,
            "status": 500,
            "reason": "ArrayIndexOutOfBoundsException[0]"
         },
         {
            "index": "blah",
            "shard": 20,
            "status": 500,
            "reason": "ArrayIndexOutOfBoundsException[0]"
         }
      ]
   },
   "hits": {
      "total": 14241209,

STACKTRACE:
[2014-07-17 07:46:48,465][DEBUG][action.search.type       ] [Dragonfly] [blah][8], node[C9YP6YFJQ2alZpUfevE73g], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@46e0309e] lastShard [true]
java.lang.ArrayIndexOutOfBoundsException: 0
        at org.elasticsearch.common.util.BigArrays$ObjectArrayWrapper.set(BigArrays.java:264)
        at org.elasticsearch.search.aggregations.AggregatorFactories$1.&lt;init&gt;(AggregatorFactories.java:68)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createSubAggregators(AggregatorFactories.java:62)
        at org.elasticsearch.search.aggregations.Aggregator.&lt;init&gt;(Aggregator.java:78)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.&lt;init&gt;(BucketsAggregator.java:46)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.&lt;init&gt;(StringTermsAggregator.java:64)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$WithOrdinals.&lt;init&gt;(StringTermsAggregator.java:264)
        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.create(TermsAggregatorFactory.java:111)
        at org.elasticsearch.search.aggregations.support.ValueSourceAggregatorFactory.create(ValueSourceAggregatorFactory.java:58)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:123)
        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:79)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:91)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:502)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:474)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:467)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
[2014-07-17 07:46:48,466][DEBUG][action.search.type       ] [Dragonfly] [blah][3], node[C9YP6YFJQ2alZpUfevE73g], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@46e0309e] lastShard [true]
java.lang.ArrayIndexOutOfBoundsException: 0
        at org.elasticsearch.common.util.BigArrays$ObjectArrayWrapper.set(BigArrays.java:264)
        at org.elasticsearch.search.aggregations.AggregatorFactories$1.&lt;init&gt;(AggregatorFactories.java:68)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createSubAggregators(AggregatorFactories.java:62)
        at org.elasticsearch.search.aggregations.Aggregator.&lt;init&gt;(Aggregator.java:78)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.&lt;init&gt;(BucketsAggregator.java:46)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.&lt;init&gt;(StringTermsAggregator.java:64)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$WithOrdinals.&lt;init&gt;(StringTermsAggregator.java:264)
        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.create(TermsAggregatorFactory.java:111)
        at org.elasticsearch.search.aggregations.support.ValueSourceAggregatorFactory.create(ValueSourceAggregatorFactory.java:58)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:123)
        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:79)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:91)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:502)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:474)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:467)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
[2014-07-17 07:46:49,178][DEBUG][action.search.type       ] [Dragonfly] [blah][26], node[C9YP6YFJQ2alZpUfevE73g], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@46e0309e] lastShard [true]
java.lang.ArrayIndexOutOfBoundsException: 0
        at org.elasticsearch.common.util.BigArrays$ObjectArrayWrapper.set(BigArrays.java:264)
        at org.elasticsearch.search.aggregations.AggregatorFactories$1.&lt;init&gt;(AggregatorFactories.java:68)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createSubAggregators(AggregatorFactories.java:62)
        at org.elasticsearch.search.aggregations.Aggregator.&lt;init&gt;(Aggregator.java:78)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.&lt;init&gt;(BucketsAggregator.java:46)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.&lt;init&gt;(StringTermsAggregator.java:64)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$WithOrdinals.&lt;init&gt;(StringTermsAggregator.java:264)
        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.create(TermsAggregatorFactory.java:111)
        at org.elasticsearch.search.aggregations.support.ValueSourceAggregatorFactory.create(ValueSourceAggregatorFactory.java:58)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:123)
        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:79)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:91)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:502)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:474)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:467)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
[2014-07-17 07:46:49,287][DEBUG][action.search.type       ] [Dragonfly] [blah][20], node[C9YP6YFJQ2alZpUfevE73g], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@46e0309e] lastShard [true]
java.lang.ArrayIndexOutOfBoundsException: 0
        at org.elasticsearch.common.util.BigArrays$ObjectArrayWrapper.set(BigArrays.java:264)
        at org.elasticsearch.search.aggregations.AggregatorFactories$1.&lt;init&gt;(AggregatorFactories.java:68)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createSubAggregators(AggregatorFactories.java:62)
        at org.elasticsearch.search.aggregations.Aggregator.&lt;init&gt;(Aggregator.java:78)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.&lt;init&gt;(BucketsAggregator.java:46)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.&lt;init&gt;(StringTermsAggregator.java:64)
        at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$WithOrdinals.&lt;init&gt;(StringTermsAggregator.java:264)
        at org.elasticsearch.search.aggregations.bucket.terms.TermsAggregatorFactory.create(TermsAggregatorFactory.java:111)
        at org.elasticsearch.search.aggregations.support.ValueSourceAggregatorFactory.create(ValueSourceAggregatorFactory.java:58)
        at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:123)
        at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:79)
        at org.elasticsearch.search.query.QueryPhase.preProcess(QueryPhase.java:91)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:502)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:474)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:467)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
</comment><comment author="clintongormley" created="2014-07-17T13:35:17Z" id="49306923">@jpountz please could you take a look at this one?
</comment><comment author="jpountz" created="2014-07-17T13:52:30Z" id="49308985">@clintongormley This one is different from #5048

@praj2 This bug is fixed in recents releases of Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6899</link><project id="" key="" /><description>While i restart my cluster, i saw the exception: 

org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [tmh_session][4] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [tmh_session][4] failed to open reader on writer
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:301)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:709)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:204)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    ... 3 more
Caused by: org.apache.lucene.index.CorruptIndexException: Invalid fieldsStream maxPointer (file truncated?): maxPointer=3411245664, length=214433792
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.&lt;init&gt;(CompressingStoredFieldsReader.java:136)
    at org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113)
    at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:129)
    at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:101)
    at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:142)
    at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:236)
    at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:99)
    at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:385)
    at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:112)
    at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
    at org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1364)
    at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:291)
    ... 6 more
</description><key id="38043999">6899</key><summary>Failure recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">letrungtrung</reporter><labels><label>feedback_needed</label></labels><created>2014-07-17T02:52:35Z</created><updated>2014-12-30T20:24:45Z</updated><resolved>2014-12-30T20:24:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-17T19:07:32Z" id="49350096">I think you have a corrupted index here. This might likely be a hardware corruption which has not been detected while the cluster was running. Is this a replica shard or a primary?
</comment><comment author="letrungtrung" created="2014-07-20T07:26:36Z" id="49539420">@s1monw this shard is a replica and a primary too. Maybe your mention is right, so i will observe my system, if that still occurs , i will report to you. 
</comment><comment author="clintongormley" created="2014-12-30T20:24:45Z" id="68394387">No further info.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make ScoreAccessor utility class publicly available for other script engines</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6898</link><project id="" key="" /><description>With the removal of setNextScore in #6864, script engines must use
the Scorer to find the score of a document.  The DocLookup is updated
appropriately to do this, but most script engines require a Number to be
bound for numeric variables.  Groovy already had an encapsulation for
this funtionality, and this moves it out to be shared with other script
engines.
</description><key id="38034835">6898</key><summary>Make ScoreAccessor utility class publicly available for other script engines</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T23:20:37Z</created><updated>2015-06-07T12:48:26Z</updated><resolved>2014-07-16T23:34:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-16T23:32:03Z" id="49241663">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ids filter in percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6897</link><project id="" key="" /><description>As the ids filter is not of any usage in the percolator api it should not be possible to add it to a percolator.

Heres a gist to test it:
https://gist.github.com/julianhille/f4d1014a0072cc3233fb
</description><key id="38032628">6897</key><summary>ids filter in percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">julianhille</reporter><labels><label>bug</label><label>discuss</label></labels><created>2014-07-16T22:44:44Z</created><updated>2014-11-27T11:50:51Z</updated><resolved>2014-11-27T11:50:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-17T09:32:34Z" id="49283711">Hi @julianhille 

You have a couple of bugs in your script.  The `ids` filter that you specify is invalid, and the `_uid` field should be `{type}#{id}`, ie `bug#1`

Here is the corrected script below:

```
DELETE /_all 

# configure the mapping of the bug type in the test index
PUT /test/bug/_mapping
{
  "bug": {
    "_id": {
      "path": "id"
    },
    "properties": {
      "id": {
        "type": "integer",
        "index": "not_analyzed",
        "omit_norms": true
      }
    }
  }
}

# Index a single percolator with terms filter on id
PUT /test/.percolator/terms_filter
{
  "query": {
    "filtered": {
      "filter": {
        "terms": {
          "values": [1,2,3]
        }
      },
      "query": {
        "match_all": {}
      }
    }
  },
  "type": "bug"
}

# Index a single percolator with ids filter
PUT /test/.percolator/ids_filter
{
  "query": {
    "filtered": {
      "filter": {
        "ids": {
          "values": [1,2,3]
        }
      },
      "query": {
        "match_all": {}
      }
    }
  },
  "type": "bug"
}
```

This matches the `terms_filter`, but not the `ids_filter`

```
GET /test/bug/_percolate?pretty
{
  "doc": {
    "id": "1"
  }
}
```

This matches the `ids_filter`

```
GET /test/bug/_percolate?pretty
{
  "doc": {
    "_id": "1"
  }
} 
```

This matches the `ids_filter`

```
GET /test/bug/_percolate?pretty
{
  "doc": {
    "_uid": "bug#1"
  }
}
```

So the only thing missing is that the `_id` hasn't been extracted from the `id` field in the document.  Given that we're considering deprecating extracting IDs from documents (see #6730) I'm wondering whether we should fix this...
</comment><comment author="julianhille" created="2014-07-17T09:55:26Z" id="49286369">updated the gist but still does not match the ids filter in any way.
I don't know if this should be fixed, but it should be at least be mentioned on any documentation page (percolator / ids filter).

Fixed your typo in the terms filter percolator.
</comment><comment author="clintongormley" created="2014-07-17T10:23:31Z" id="49289392">&gt; Fixed your typo in the terms filter percolator.

ah yes, that was a copy and paste reformat of the array :)

You have another typo in your script:

```
# Index a single percolator with ids filter
curl -X PUT ES_HOST/$ES_INDEX/test/.percolator/ids_filter -s -o /dev/null -d '
```

Should be `$ES_HOST`
</comment><comment author="julianhille" created="2014-07-17T10:45:32Z" id="49291221">forgot to update that. locally its with a $ damn.
</comment><comment author="clintongormley" created="2014-07-17T10:49:34Z" id="49291554">Right, next error in the script (in the same line)

&gt; curl -X PUT ES_HOST/$ES_INDEX/test/.percolator/ids_filter -s -o /dev/null -d '

You have `$ES_INDEX/test` - this throws an error, and is the reason your script fails
</comment><comment author="julianhille" created="2014-07-17T11:02:50Z" id="49292633">copy and pasted too much from you. corrected. now works. perfectly. 
</comment><comment author="OlegYch" created="2014-08-22T17:46:43Z" id="53095602">hm, why is it not of any usage? i use ids filter in percolators and would be sad if you remove it
</comment><comment author="clintongormley" created="2014-10-31T10:06:51Z" id="61240323">I agree that the `ids` filter should not be removed.  However, this ticket does raise an interesting question regarding the possibility of removing the `path` parameter from the `_id` field.

The example above works because the `_id` or `_uid` is passed in the body of the document.  If we remove the `path` parameter, this may no longer work.  Also need to check whether, when percolating an indexed document, do we make the `_id` and other metadata fields available?

@martijnvg any ideas about this?
</comment><comment author="clintongormley" created="2014-11-27T11:50:51Z" id="64780980">Closed in favour of #6730
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added three frequency levels for resource watching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6896</link><project id="" key="" /><description> It's now possible to register watchers along with a specified check frequency. There are three frequencies: low, medium, high. Each one is associated with a check interval that determines how frequent the watchers will check for changes and notify listeners if needed. By default, the intervals are 5s, 30s and 60s respectively, but they can also be customized in the settings. also:
- Added the WatcherHandle construct by which one can stop it (remove it) and resume it (re add it). Also provides access to the watcher itself and the frequency by which it's checked
- Changed the default frequency to 30 seconds interval (used to be 60 seconds). The only watcher that is currently effected by this is the script watcher (now auto-loading scripts will auto-load every 30 seconds if changed)
</description><key id="38030517">6896</key><summary>Added three frequency levels for resource watching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T22:13:41Z</created><updated>2015-06-07T12:49:45Z</updated><resolved>2014-07-17T12:59:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-17T12:12:01Z" id="49298591">LGTM
</comment><comment author="uboness" created="2014-07-17T12:59:21Z" id="49302857">closed by cc8f7ddb9a8562b482b5000523cfb3f69ec34505
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In top_hits agg, when parent doc is found stop iterating over segments and continue with the next top matching child doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6895</link><project id="" key="" /><description>The parent doc has been found and there is no need to check subsequent segments.

Relates to #1843
</description><key id="38029245">6895</key><summary>In top_hits agg, when parent doc is found stop iterating over segments and continue with the next top matching child doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T21:56:42Z</created><updated>2017-03-31T10:06:19Z</updated><resolved>2014-07-16T22:03:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-16T21:59:00Z" id="49233434">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unknown node version should be a lower bound</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6894</link><project id="" key="" /><description>Today when we start a `TransportClient` we use the given transport addresses and create a `DiscoveryNode` from it without knowing the actual nodes version. We just use the `Version.CURRENT` which is an upper bound. Yet, the other node might be a version less than the currently running and serialisation of the nodes info might break. We should rather use a lower bound here which is the version of the first release with the same major version as `Version.CURRENT` since this is what we officially support. 

We changed the format of the `NodesInfo` serialisation today and BWC tests broken on that. Yet we found a away to work around changing it but in the future we should be able to change transport protocol even if it's `NodesInfo`

Note: this is not a problem until today but in the future this might prevent us from enhancing the protocol here.
</description><key id="38015985">6894</key><summary>Unknown node version should be a lower bound</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T19:26:35Z</created><updated>2014-07-28T19:21:55Z</updated><resolved>2014-07-18T09:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-16T22:02:08Z" id="49233788">FYI, @javanna did some work around this one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: histo "interval" should allow coercion from string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6893</link><project id="" key="" /><description>This fails with `Unexpected token VALUE_STRING in aggregation [histo]` because "50" is passed as a string:

```
curl -XGET 'http://localhost:9200/_search?pretty=1' -d '
{
   "aggs" : {
      "histo" : {
         "histogram" : {
            "interval" : "50",
            "field" : "number"
         }
      }
   }
}
'
```

Perl won't guarantee that a number is a number and not a string.
</description><key id="37992362">6893</key><summary>Aggregations: histo "interval" should allow coercion from string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T15:12:17Z</created><updated>2014-07-21T19:32:16Z</updated><resolved>2014-07-21T19:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-16T15:16:44Z" id="49180705">This is probably an issue for a lot of number fields in the aggregations.  All will need to be checked for this
</comment><comment author="s1monw" created="2014-07-17T10:01:54Z" id="49287117">I guess we should just check for:

```
            } else if (token == XContentParser.Token.VALUE_NUMBER || token == XContentParser.Token.VALUE_STRING) {

```
</comment><comment author="kimchy" created="2014-07-21T18:50:59Z" id="49648361">yea, or just check `if token.isValue` and try and get the relevant type, the parsers will automatically convert strings to number if you ask for a long value
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove indicesLifecycle.Listener from IndexingMemoryController</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6892</link><project id="" key="" /><description>The IndexingMemoryController determines the amount of indexing buffer size and translog buffer size each shard should have. It takes memory from inactive shards (indexing wise) and assigns it to other shards. To do so it needs to know about the addition and closing of shards. The current implementation hooks into the indicesService.indicesLifecycle() mechanism to receive call backs, such shard entered the POST_RECOVERY state. Those call backs are typically run on the thread that actually made the change. A mutex was used to synchronize those callbacks with IndexingMemoryController's background thread, which updates the internal engines memory usage on a regular interval. This introduced a dependency between those threads and the locks of the internal engines hosted on the node. In a _very_ rare situation (two tests runs locally) this can cause recovery time outs where two nodes are recovering replicas from each other.

 This commit introduces a a lock free approach that updates the internal data structures during iterations in the background thread.
</description><key id="37991089">6892</key><summary>Remove indicesLifecycle.Listener from IndexingMemoryController</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T14:59:53Z</created><updated>2015-06-07T19:28:08Z</updated><resolved>2014-07-17T12:39:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-17T08:38:22Z" id="49273533">Tagged as 1.3
</comment><comment author="s1monw" created="2014-07-17T09:41:57Z" id="49284800">@bleskes I think this looks good though. I left some comments but on the commit :/ I like the removal of the listeners and the sync stuff
</comment><comment author="bleskes" created="2014-07-17T11:08:55Z" id="49293142">@s1monw I pushed another update
</comment><comment author="s1monw" created="2014-07-17T12:26:50Z" id="49299853">NICE LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: Add regular expression support to CORS for easier matching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6891</link><project id="" key="" /><description>In order to support CORS better, we should add an option to also match a regular expression instead of the current options when returning the `Access-Control-Allow-Origin` first.
</description><key id="37984427">6891</key><summary>Security: Add regular expression support to CORS for easier matching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-07-16T13:58:51Z</created><updated>2014-07-25T08:54:49Z</updated><resolved>2014-07-25T08:52:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-18T12:17:48Z" id="49424265">@spinscale ping what is the status of this?
</comment><comment author="spinscale" created="2014-07-18T13:31:03Z" id="49430491">@s1monw waiting for a final test by @rashidkpc but I just created #6923 so maybe you can have a code review, especially to make sure I didnt introduce a performance penalty by trying to compile the regular expression more often than necessary
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Tests] Add ClusterDiscoveryConfiguration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6890</link><project id="" key="" /><description>A utility class that simplifies testing with Unicast based discovery (and allows for randomly choose between mutilcast and unicast based pings)

Note: this is against the improve_zen branch
</description><key id="37974498">6890</key><summary>[Tests] Add ClusterDiscoveryConfiguration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-07-16T12:00:31Z</created><updated>2014-07-21T18:42:07Z</updated><resolved>2014-07-21T18:42:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-16T15:13:51Z" id="49180266">@bleskes this looks good! and I left a few comments.
</comment><comment author="bleskes" created="2014-07-16T17:10:30Z" id="49196691">@martijnvg thx. Pushed an update.
</comment><comment author="bleskes" created="2014-07-17T08:24:10Z" id="49272064">@martijnvg I pushed another update. The port # should be unique now.
</comment><comment author="martijnvg" created="2014-07-17T08:32:59Z" id="49272944">LGTM!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring to make Netty MessageChannelHandler extensible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6889</link><project id="" key="" /><description>Small refactorings to make the MessageChannelHandler more extensible.
Also allowed access to the different netty pipelines
</description><key id="37969635">6889</key><summary>Refactoring to make Netty MessageChannelHandler extensible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T10:46:14Z</created><updated>2015-06-07T12:50:31Z</updated><resolved>2014-07-17T06:30:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-16T11:00:49Z" id="49150570">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename FieldMapper.termsFilter to fieldDataTermsFilter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6888</link><project id="" key="" /><description>FieldMapper has two methods
`Filter termsFilter(List values, @Nullable QueryParseContext)` which is supposed
to work on the inverted index and
`Filter termsFilter(QueryParseContext, List, QueryParseContext)` which is
supposed to work on field data. Let's rename the second one to
`fieldDataTermsFilter` and remove the unused `QueryParseContext`.
</description><key id="37969383">6888</key><summary>Rename FieldMapper.termsFilter to fieldDataTermsFilter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-16T10:42:47Z</created><updated>2015-06-07T12:50:51Z</updated><resolved>2014-07-17T10:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-17T09:46:09Z" id="49285285">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es.default.path.data conflicts with path.data when path.data is a list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6887</link><project id="" key="" /><description>**Version 1.2.2**

An elastic search instance is running with the following arguments:

```
/usr/lib/jvm/java-7-oracle/bin/java -Xms15g -Xmx15g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.pidfile=/var/run/elasticsearch.pid -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-1.2.2.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.Elasticsearch
```

`es.default.path.data` is coming from the deb package's init.d script.

Inside the `elasticsearch.yml`, if we declare `path.data` as an array, we end up with 3 data directories according to ES.

``` yaml
path:
  data:
    - /var/lib/elasticsearch/data1
    - /var/lib/elasticsearch/data2
```

When ES runs, it now thinks there are 3 data directories. The one being set as default through the CLI arguments, plus the 2 new ones that were added in the config.

According to `/_nodes/stats?all=true`, we have this (snipped to relevant parts):

``` json
{
    "nodes": {
        "FOSlksI8ToK6G4YdIylYvQ": {
            "fs": {
                "data": [{
                    "path": "/var/lib/elasticsearch/home/nodes/0",
                    "mount": "/",
                    "dev": "/dev/sda6",
                    "total_in_bytes": 982560202752,
                    "free_in_bytes": 860723675136,
                    "available_in_bytes": 810788864000,
                    "disk_reads": 12402994,
                    "disk_writes": 14437097,
                    "disk_io_op": 26840091,
                    "disk_read_size_in_bytes": 364877214720,
                    "disk_write_size_in_bytes": 1342705430528,
                    "disk_io_size_in_bytes": 1707582645248,
                    "disk_queue": "2.4",
                    "disk_service_time": "2.1"
                }, {
                    "path": "/var/lib/elasticsearch/data1/home/nodes/0",
                    "mount": "/var/lib/elasticsearch/data1",
                    "dev": "/dev/sdc1",
                    "total_in_bytes": 787239469056,
                    "free_in_bytes": 787167297536,
                    "available_in_bytes": 747154268160,
                    "disk_reads": 691,
                    "disk_writes": 49627,
                    "disk_io_op": 50318,
                    "disk_read_size_in_bytes": 2851840,
                    "disk_write_size_in_bytes": 12656103424,
                    "disk_io_size_in_bytes": 12658955264,
                    "disk_queue": "2.4",
                    "disk_service_time": "2.1"
                }, {
                    "path": "/var/lib/elasticsearch/data2/home/nodes/0",
                    "mount": "/var/lib/elasticsearch/data2",
                    "dev": "/dev/sdd1",
                    "total_in_bytes": 787239469056,
                    "free_in_bytes": 787142717440,
                    "available_in_bytes": 747129688064,
                    "disk_reads": 9461,
                    "disk_writes": 883718,
                    "disk_io_op": 893179,
                    "disk_read_size_in_bytes": 39285760,
                    "disk_write_size_in_bytes": 174312841216,
                    "disk_io_size_in_bytes": 174352126976,
                    "disk_queue": "2.4",
                    "disk_service_time": "2.1"
                }]
            },
        }
    }
}
```

And if we look at `/_nodes` we see what the raw settings are:

``` json
{
    "nodes": {
        "FOSlksI8ToK6G4YdIylYvQ": {
            "settings": {
                "path": {
                    "data": "/var/lib/elasticsearch",
                    "work": "/tmp/elasticsearch",
                    "home": "/usr/share/elasticsearch",
                    "conf": "/etc/elasticsearch",
                    "logs": "/var/log/elasticsearch",
                    "data.0": "/var/lib/elasticsearch/data1",
                    "data.1": "/var/lib/elasticsearch/data2"
                }
            }
        }
    }
}
```

So in this case, we see that there are 3 different data keys according to settings, and 3 different data directories. A **data**, **data.0** and **data.1**.

Now, if we declare in our yaml, **data** as just a comma separated as a string, this does the correct behavior and overrides what the default was.

``` yaml
path:
    data: /var/lib/elasticsearch/data1,/var/lib/elasticsearch/data2
```

``` json
                "data": [{
                    "path": "/var/lib/elasticsearch/data1/home/nodes/0",
                    "mount": "/var/lib/elasticsearch/data1",
                    "dev": "/dev/sdc1",
                    "total_in_bytes": 787239469056,
                    "free_in_bytes": 787167391744,
                    "available_in_bytes": 747154362368,
                    "disk_reads": 687,
                    "disk_writes": 49803,
                    "disk_io_op": 50490,
                    "disk_read_size_in_bytes": 2835456,
                    "disk_write_size_in_bytes": 12657053696,
                    "disk_io_size_in_bytes": 12659889152,
                    "disk_queue": "0",
                    "disk_service_time": "0"
                }, {
                    "path": "/var/lib/elasticsearch/data2/home/nodes/0",
                    "mount": "/var/lib/elasticsearch/data2",
                    "dev": "/dev/sdd1",
                    "total_in_bytes": 787239469056,
                    "free_in_bytes": 787167391744,
                    "available_in_bytes": 747154362368,
                    "disk_reads": 3436,
                    "disk_writes": 684613,
                    "disk_io_op": 688049,
                    "disk_read_size_in_bytes": 14308352,
                    "disk_write_size_in_bytes": 132454891520,
                    "disk_io_size_in_bytes": 132469199872,
                    "disk_queue": "0",
                    "disk_service_time": "0"
                }]
```

``` json
                "path": {
                    "data": "/var/lib/elasticsearch/data1,/var/lib/elasticsearch/data2",
                    "work": "/tmp/elasticsearch",
                    "home": "/usr/share/elasticsearch",
                    "conf": "/etc/elasticsearch",
                    "logs": "/var/log/elasticsearch"
                },
```

So the behavior that we're seeing is that the data array is being flattened into a list, and them being appended to the default. Whereas I'd expect this new list to override the default.

I feel that the problem here is that data can be declared both as a string and as a list. It may be more _correct_ if data were always a list internally, and when declaring as a string, it's coerced into a list, even if that list had one item. But I'm not proposing the solution, just a thought.

I understand that this may not necessarily be a bug, but it is extremely unexpected behavior and took quite a bit of debugging to track down exactly what was going on.
</description><key id="37957202">6887</key><summary>es.default.path.data conflicts with path.data when path.data is a list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattrobenolt</reporter><labels><label>:Settings</label><label>adoptme</label><label>bug</label></labels><created>2014-07-16T07:46:56Z</created><updated>2015-01-06T08:14:01Z</updated><resolved>2015-01-06T08:14:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-16T08:45:03Z" id="49138035">I agree about unexpected! Thanks for reporting this.
</comment><comment author="mattrobenolt" created="2014-07-16T09:23:54Z" id="49140767">@clintongormley Let me know if there's anything else I can do to help, but I assume this is easily reproduced. :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Ignore incomplete tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6886</link><project id="" key="" /><description /><key id="37928679">6886</key><summary>Benchmarks: Ignore incomplete tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-15T21:40:10Z</created><updated>2014-07-15T21:40:16Z</updated><resolved>2014-07-15T21:40:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add an option to early terminate document collection when searching/counting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6885</link><project id="" key="" /><description>The idea is to add an option which will let the user control the number of matched documents collected per shard before scoring (if applicable). This will be helpful for exists-type functionality or when not all matched document has to be scored.

Closes #6876 
</description><key id="37927592">6885</key><summary>Add an option to early terminate document collection when searching/counting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Search</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T21:27:10Z</created><updated>2015-06-06T18:28:47Z</updated><resolved>2014-07-23T19:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-07-15T21:31:16Z" id="49095630">TODO: 
- replace all usage of `Lucene.ExistsCollector` with the new `Lucene.EarlyTerminatingCountCollector` to take advantage of early termination.
- use `threshold` for collation in PhraseSuggester
</comment><comment author="s1monw" created="2014-07-15T21:41:58Z" id="49096813">left some comments - I like it...
</comment><comment author="areek" created="2014-07-16T03:21:28Z" id="49119567">Updated PR:
- incorporated feedback
- replaced `Lucene.ExistsCollector` usage with the new `EarlyTerminatingCountCollector` (this might speed things up a little bit in the Percolator service)

TODO:
- use `threshold` param in `PhraseSuggester` collation.

It seems like there is no equivalent of `MultiSearchRequest` and family for `CountRequest`. To use `threshold` for `PhraseSuggester` collation, it would be ideal to use `CountRequest` (for setting up the `threshold` param) and also use something similar to `MultiSearchRequest` for `CountRequest` (to dispatch requests for all the generated phrases). 

Another way would be to somehow add the count-specific param `threshold` to the `SearchRequest`. Thoughts?
</comment><comment author="areek" created="2014-07-18T02:53:35Z" id="49390587">I have added a new `terminate_after_count` parameter to `search` and `count` API, which represents the number of documents to collect per shard. Upon reaching that number, the query will early terminate. 

The search request with the new param looks as follows:

``` bash
curl -XGET 'localhost:9200/_search' -d '
  { 
    terminate_after_count: 1, 
    "query" : ...
  }'
```

or 

``` bash
curl -XGET 'localhost:9200/_search?terminate_after_count=1' -d '
  { 
    "query" : ...
  }'
```

and for `Count` API, the request is as follows:

``` bash
curl -XGET 'localhost:9200/_count?terminate_after_count=1' -d '
  { 
    "query" : ...
  }'
```

Both the `Search` &amp; `Count` Response will have an added field `terminated_after_count` which will be true if any of the shards actually early terminated.
</comment><comment author="areek" created="2014-07-18T03:00:40Z" id="49390921">@clintongormley any thoughts on the naming/api for this?
</comment><comment author="areek" created="2014-07-18T03:06:50Z" id="49391180">Updated PR:
- add support for early termination to the `Search` API
- use early termination in collation for `PhraseSuggester`
- Added tests 
- Added `terminated_after_count` to `Search` and `Count` response, to indicate if any of the shards actually early terminated query exec. or not

TODO:
- add docs for new option in `Search` API. (not exactly sure where would the best place to add it though)
- add more tests for new option in `Search` API
</comment><comment author="clintongormley" created="2014-07-18T10:05:34Z" id="49415171">@areek perhaps `terminate_after`, just to keep it short and simple. Wondering if the response should be true/false, or if it would be better to return `terminated_after: 1000` (and leave out the key if it wasn't terminated early)
</comment><comment author="areek" created="2014-07-18T18:27:53Z" id="49464105">@clintongormley the problem with `terminate_after` is that its a little ambiguous, users might think it is a timeout or what not, rather than a threshold to doc collection. Though `terminate_after` is much nicer then `terminate_after_count`. 
Regarding the response, I don't see the value in specifying the `terminated_after` count, as if the early termination did happen then this will always be the same as the input param. Specifying it also might confuse users as this is a threshold/shard, it might report `num_shards*terminate_after_count` hits/counts while having `terminated_after: terminate_after_count` in the resp.
I think `terminate_after` is a better name but a little ambiguous (might just roll with it) and for the response I will leave `terminated_after` out if it was false.
Thoughts?
</comment><comment author="clintongormley" created="2014-07-19T11:53:02Z" id="49507378">@areek i agree with you about the terminate_after count.  True/false does make more sense.  But if the user does specify `terminate_after`, I think the flag should be in the response.  Perhaps `terminated`? or `terminated_early`? Neither of those are perfect, but `terminated_after` feels like it is missing something...
</comment><comment author="areek" created="2014-07-22T00:37:38Z" id="49684805">Updated PR:
- The new request param has been renamed to `terminate_after` from `terminate_after_count`.
- Added documentation
- The response for both search &amp; count api looks like the following:

``` bash
  {
    "took": 1,
    "timed_out": false,
    "terminated_early": true,
    "_shards": {
       ...
    },
    "hits": {
       ...
    }
}
```

 The `terminated_early` will only be visible when the user explicitly sets `terminate_after` param in the request
</comment><comment author="areek" created="2014-07-22T00:39:48Z" id="49684950">@s1monw It would be awesome to have this reviewed.
</comment><comment author="s1monw" created="2014-07-22T15:08:37Z" id="49752063">I left a couple of comments but it looks great so far
</comment><comment author="clintongormley" created="2014-07-23T15:43:29Z" id="49892292">@areek Docs look good, I'd just add a mention of the key in the response when the request has been terminated early
</comment><comment author="s1monw" created="2014-07-23T15:44:37Z" id="49892480">did another review and this LGTM too 
</comment><comment author="areek" created="2014-07-23T16:03:19Z" id="49895259">Mentioned the added key `terminated_early` in response when `terminate_after` is set. I will merge this soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Remove broken REST tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6884</link><project id="" key="" /><description>These tests need to be re-written so just removing them for now as they
are obsolete.
</description><key id="37926937">6884</key><summary>Benchmarks: Remove broken REST tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-15T21:19:21Z</created><updated>2014-07-15T21:19:25Z</updated><resolved>2014-07-15T21:19:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Remove obsolete class.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6883</link><project id="" key="" /><description>This class was failing the build due to using a Java 8 feature in the Map interface. The class had been obsoleted however by another class, so the solution was simply to remove it.
</description><key id="37923891">6883</key><summary>Benchmarks: Remove obsolete class.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-07-15T20:46:04Z</created><updated>2014-07-15T20:46:11Z</updated><resolved>2014-07-15T20:46:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Log when dynamically changing merge thread pool size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6882</link><project id="" key="" /><description>I tried to dynamically set `index.merge.scheduler.max_thread_count` and it looked as though nothing happened.  No log or anything.  It looks like that is supposed to be possible in ConcurrentMergeSchedulerProvider.java:161 but it wasn't working.
</description><key id="37914960">6882</key><summary>Log when dynamically changing merge thread pool size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-07-15T19:01:28Z</created><updated>2014-08-01T09:27:09Z</updated><resolved>2014-08-01T09:27:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-07-15T19:20:22Z" id="49079760">Hmm are you sure it didn't "work" though?  E.g. if you get stack traces for all threads do you see that they are still busy merging vs waiting for other merge threads to finish first?
</comment><comment author="nik9000" created="2014-07-15T20:05:37Z" id="49085092">I had some long running merges on the box that were eating all the IO.  I ended up restarting Elasticsearch on that machine after it didn't look like anything changed so I don't have good records.  The stack traces I saw didn't look to have changed.  I had imagined some of the merges would finish and new ones wouldn't start/block but without some log I wasn't going to wait another twenty minutes of brownout to be sure. 
</comment><comment author="clintongormley" created="2014-07-16T08:42:50Z" id="49137872">We have a test that shows that this works (#6842), so the thing that is missing is a log message to inform the user that their change has been accepted.
</comment><comment author="mikemccand" created="2014-07-16T09:58:47Z" id="49144382">Actually, this issue (changing index.merge.scheduler.max_thread_count) is different from #6842 (changing index.store.throttle_type).  We should first add a test here; the test could probably just peek at the lucene.iw TRACE logs to confirm merges are being paused.

@nik9000 the max_thread_count just controls how many merges are allowed to run at once, not how many merges are "pending".  I don't think anything is logged at the default verbosity, so you wouldn't know things had actually changed.  The simplest way to verify might be to take a full thread dump and then count how many merge threads (Lucene Merge Thread #XXX) are actually doing something vs. paused waiting "their turn" to run.
</comment><comment author="nik9000" created="2014-07-16T21:08:42Z" id="49227374">@mikemccand yeah - when I did that I still got quite a few threads but I imagine that is because they were in flight when I changed the setting.  I'm happy to call the issue settled.  I was mostly panicking when I filed the bug because servers were falling over because they were merging when we were close to the edge.

In addition to the log, it'd be nice if the result of the curl command contained what was changed like it usually does.  That parameter just didn't come back to me which is normally the signal I use to know that changing something dynamically works.
</comment><comment author="mikemccand" created="2014-07-18T16:20:21Z" id="49450303">OK I dug into this, and when I made a test, it hit an exception claiming index.merge.scheduler.max_thread_count is not a dynamic setting.  @nik9000 did you see any error when you tried?

Yet, in ConcurrentMergeSchedulerProvider, it's clearly prepared to update these settings, so I added them to IndexDynamicSettingsModule and the test passes.  The test also verifies that an INFO message is logged with the change.  I'll open a PR.
</comment><comment author="mikemccand" created="2014-07-18T16:30:27Z" id="49451403">OK I opened #6925
</comment><comment author="nik9000" created="2014-07-18T16:37:09Z" id="49452155">&gt; OK I dug into this, and when I made a test, it hit an exception claiming index.merge.scheduler.max_thread_count is not a dynamic setting. @nik9000 did you see any error when you tried?

No BUT I didn't look at all the nodes.  I imagine it'd have been on the master?
</comment><comment author="mikemccand" created="2014-07-18T16:46:30Z" id="49453133">Hmm I'm honestly not sure how the error would have propagated back to how you requested; my test case just uses the Java client API (I think!) and somehow the error came back through there.

Maybe there's another issue, that the error fails to properly propagate back to the HTTP response to your curl request?
</comment><comment author="nik9000" created="2014-07-18T17:13:35Z" id="49456008">That is certainly possible.  I don't really use the Java api at all except
when I'm working on Elasticsearch itself.

On Fri, Jul 18, 2014 at 12:46 PM, Michael McCandless &lt;
notifications@github.com&gt; wrote:

&gt; Hmm I'm honestly not sure how the error would have propagated back to how
&gt; you requested; my test case just uses the Java client API (I think!) and
&gt; somehow the error came back through there.
&gt; 
&gt; Maybe there's another issue, that the error fails to properly propagate
&gt; back to the HTTP response to your curl request?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6882#issuecomment-49453133
&gt; .
</comment><comment author="mikemccand" created="2014-08-01T09:27:09Z" id="50865734">This was fixed with #6925
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resend failed shard messages when receiving a cluster state still referring to the failed shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6881</link><project id="" key="" /><description>In rare cases we may fail to send a shard failure event to the master, or there is no known master when the shard has failed (ex. a couple of node leave the cluster canceling recoveries and causing a master to step down at the same time). When that happens and a cluster state arrives from the (new) master we should resend the shard failure in order for the master to remove the shard from this node.
</description><key id="37910443">6881</key><summary>Resend failed shard messages when receiving a cluster state still referring to the failed shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T18:13:41Z</created><updated>2015-06-07T12:51:05Z</updated><resolved>2014-07-16T08:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T19:32:50Z" id="49081246">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[FIX] normalize serialization of ScriptType in UpdateRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6880</link><project id="" key="" /><description /><key id="37905152">6880</key><summary>[FIX] normalize serialization of ScriptType in UpdateRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-07-15T17:12:32Z</created><updated>2014-08-27T12:28:20Z</updated><resolved>2014-08-27T12:28:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-15T17:13:49Z" id="49063580">LGTM
</comment><comment author="clintongormley" created="2014-08-22T07:32:13Z" id="53031412">Why is this still open? Shouldn't it be merged?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Recovery] don't start a gateway recovery if source node is not found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6879</link><project id="" key="" /><description>Due to a change introduced in #6825, we now start a local gateway recovery for replicas when the source node can not be found. The recovery then fails because we never recover replicas from disk.
</description><key id="37903694">6879</key><summary>[Recovery] don't start a gateway recovery if source node is not found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-07-15T16:55:47Z</created><updated>2014-07-16T08:08:03Z</updated><resolved>2014-07-16T08:04:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-15T19:04:59Z" id="49077753">LGTM, I think this should go to 1.3 as well...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Explain that SerialMergeScheduler just maps to CMS, for back compat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6878</link><project id="" key="" /><description /><key id="37896017">6878</key><summary>Docs: Explain that SerialMergeScheduler just maps to CMS, for back compat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>docs</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T15:35:58Z</created><updated>2014-07-16T11:03:22Z</updated><resolved>2014-07-15T15:37:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-15T15:36:29Z" id="49050141">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more randomization to the tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6877</link><project id="" key="" /><description>randomizes preference and FS translog impls
</description><key id="37894111">6877</key><summary>Add more randomization to the tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-07-15T15:19:03Z</created><updated>2014-07-18T10:35:53Z</updated><resolved>2014-07-18T10:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-15T15:20:43Z" id="49047681">LGTM
</comment><comment author="s1monw" created="2014-07-15T15:21:12Z" id="49047755">will push this tomorrow...
</comment><comment author="s1monw" created="2014-07-18T09:50:42Z" id="49414002">@kimchy @jpountz I pushed another commit
</comment><comment author="jpountz" created="2014-07-18T09:52:32Z" id="49414152">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to Count API to terminate early if a specified count is reached</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6876</link><project id="" key="" /><description>It would be nice if the Count API had an option to terminate once a specified count has been reached. This would be a nice enhancement if the purpose of the count request is just to figure out if matching documents exists given a query (as is the case for collate option in PhraseSuggester #3482)
</description><key id="37893784">6876</key><summary>Add option to Count API to terminate early if a specified count is reached</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T15:16:07Z</created><updated>2014-07-23T19:41:32Z</updated><resolved>2014-07-23T19:19:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>script.disable_dynamic: true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6875</link><project id="" key="" /><description>Hey guys ,  I'm not sure this is a default in the latest version of ES ,  but it should be for every version ,  the CVE is getting some popularity.

Thanks!
</description><key id="37889237">6875</key><summary>script.disable_dynamic: true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bechampion</reporter><labels /><created>2014-07-15T14:35:56Z</created><updated>2014-07-15T14:48:36Z</updated><resolved>2014-07-15T14:48:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-15T14:48:36Z" id="49042461">Hi @bechampion , dynamic scripting is disabled by default from the 1.2.0 release and newer. Thanks for the comment!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Add ElasticsearchSingleNodeTest.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6874</link><project id="" key="" /><description>This test makes it easy to create a lightweight node (no http, indices stored
in RAM, ...) whose main purpose is to get an instance of the Guice injector
for unit tests.

This should help not have to update lots of unit tests when we add a new
Guice dependency.
</description><key id="37873122">6874</key><summary>Test: Add ElasticsearchSingleNodeTest.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T11:20:03Z</created><updated>2015-06-06T18:28:54Z</updated><resolved>2014-07-15T13:57:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T12:27:30Z" id="49024047">left some small comments - thanks for doing this
</comment><comment author="jpountz" created="2014-07-15T12:52:43Z" id="49026725">@s1monw pushed a new commit
</comment><comment author="s1monw" created="2014-07-15T13:19:22Z" id="49029808">LGTM
</comment><comment author="s1monw" created="2014-07-15T13:19:31Z" id="49029821">oh wait...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrap filter only once in ApplyAcceptedDocsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6873</link><project id="" key="" /><description>We potentially wrap the given filter multiple times when iterating the
subreaders. We only need to do this once.
</description><key id="37870329">6873</key><summary>Wrap filter only once in ApplyAcceptedDocsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T10:40:10Z</created><updated>2015-06-07T12:51:15Z</updated><resolved>2014-07-15T10:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-15T10:41:52Z" id="49015512">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch crashes when applying 3-level term aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6872</link><project id="" key="" /><description>I have to say, "aggs" is a beautiful functionality added by you guys. Its like I'm addicted to it. But, suppose I group on 3 fields(apply 3 level terms aggregation) it stops in marvel-sense(Thank you for that too!) and the logs show "Java: out of memory heap space". Sometimes, I have to restart the server, coz it doesn't respond. What should I do? Suggestions..
</description><key id="37864743">6872</key><summary>elasticsearch crashes when applying 3-level term aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raghavj1991</reporter><labels /><created>2014-07-15T09:19:26Z</created><updated>2014-07-15T11:14:23Z</updated><resolved>2014-07-15T10:45:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-15T10:45:44Z" id="49015832">Hi @raghavj1991 

I don't know what version of Elasticsearch your are using, but in v1.2.1 multi-level aggregations assign fewer buckets than before, which helps with memory usage.  And in v1.3, we introduced the `collect` mode which allows you to control how aggregations are executed in order to trade time for memory: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_collect_mode
</comment><comment author="raghavj1991" created="2014-07-15T10:49:11Z" id="49016099">Sorry for the vagueness. I'm using ES - 1.0.0 . So, if we shift to 1.2 can I get rid of Out of Heap Memory space errors ? . 
</comment><comment author="clintongormley" created="2014-07-15T11:14:23Z" id="49018265">Hopefully that will be sufficient.  If not, then you may have to wait for 1.3.  You can also switch to using doc_values instead of fielddata, which writes the required data to disk at index time, instead of having to load it into memory at search time.

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#_doc_values_format
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"matched_queries" does not include queries within a wrapper query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6871</link><project id="" key="" /><description>I have a document in an index such that the following request returns a match, with the `matched_queries` element containing `query1`:

```
curl -XPOST 'http://localhost:9200/index/_search' &#8211;d '
{
  "query": {
    "match": {
      "stuff": {
        "_name": "query1",
        "query": "blah"
      }
    }
  }
}
'
```

However, if that query is wrapped in a wrapper query, then the name does not appear in the list of `matched_queries`:

```
curl -XPOST 'http://localhost:9200/index/_search' &#8211;d '
{
  "query": {
    "wrapper": {
      "query": "eyJtYXRjaCI6IHsic3R1ZmYiOiB7Il9uYW1lIjogInF1ZXJ5MSIsICJxdWVyeSI6ICJibGFoIn19fQ=="
    }
  }
}
'
```

I am using the java api and the `WrapperQueryBuilder`, but the effect is the same whether using the java api or the rest interface.

(gist with set-up steps in case it's useful: https://gist.github.com/tstibbs/645e01c5dcdfa9d2a193)
</description><key id="37862177">6871</key><summary>"matched_queries" does not include queries within a wrapper query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">tstibbs</reporter><labels><label>bug</label></labels><created>2014-07-15T08:42:24Z</created><updated>2015-01-07T08:21:13Z</updated><resolved>2015-01-07T08:21:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Send shard exists requests if shard exists locally but is not allocated to the node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6870</link><project id="" key="" /><description>Store: Only send shard exists requests if shard / shard files exist locally and is not allocated on that node according to cluster state.
</description><key id="37861938">6870</key><summary>Send shard exists requests if shard exists locally but is not allocated to the node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T08:38:21Z</created><updated>2015-06-07T12:51:25Z</updated><resolved>2014-07-16T07:27:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-15T08:41:11Z" id="49004963">LGTM, I tagged it with 1.3
</comment><comment author="bleskes" created="2014-07-15T09:21:51Z" id="49008690">LGTM. Good catch.
</comment><comment author="s1monw" created="2014-07-15T10:47:51Z" id="49016014">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make XContentBuilder Releasable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6869</link><project id="" key="" /><description>make the builder releasable (auto closeable), and use it in shards state
also make XContentParser releasable (AutoCloseable) and not closeable since it doesn't throw an IOException
</description><key id="37860229">6869</key><summary>Make XContentBuilder Releasable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T08:12:32Z</created><updated>2015-06-07T12:51:38Z</updated><resolved>2014-07-15T19:28:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T10:49:36Z" id="49016132">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Count distinct value by date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6868</link><project id="" key="" /><description>I have the same issue to count distinct value by date, that is the same issue mentioned here:
http://elasticsearch-users.115913.n3.nabble.com/Count-distinct-value-by-date-td4036320.html
This issue was posted a year ago. Is there any new update?
Thank you.
</description><key id="37858761">6868</key><summary>Count distinct value by date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fulinlin924</reporter><labels /><created>2014-07-15T07:47:48Z</created><updated>2014-07-15T10:37:43Z</updated><resolved>2014-07-15T10:37:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-15T10:37:43Z" id="49015123">Yes. See the [cardinality aggregation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add multi_field support for Mapper externalValue (plugins)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6867</link><project id="" key="" /><description>In context of mapper attachment and other mapper plugins, when dealing with multi fields, sub fields never get the `externalValue` although it was set.

Related to https://github.com/elasticsearch/elasticsearch-mapper-attachments/issues/57

Closes #5402.
</description><key id="37857710">6867</key><summary>Add multi_field support for Mapper externalValue (plugins)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-15T07:27:52Z</created><updated>2015-06-07T19:28:21Z</updated><resolved>2014-07-25T15:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-15T07:35:36Z" id="48999559">The change looks good to me but I think it would be good to have another pair of eyes on it.
</comment><comment author="s1monw" created="2014-07-15T11:05:17Z" id="49017446">sorry @dadoonet I left some comments on the commit here https://github.com/dadoonet/elasticsearch/commit/4542fa466fa5d0de111b64083fc5f9b3555b14e9 
I didn't realize I am not on the PR :)
</comment><comment author="dadoonet" created="2014-07-15T14:37:22Z" id="49040706">@s1monw PR updated with your comments. May I push?
</comment><comment author="s1monw" created="2014-07-16T13:23:40Z" id="49164109">I think it's good too but this particular part of the mappers I think @kimchy should look at too please
</comment><comment author="s1monw" created="2014-07-23T07:51:20Z" id="49843076">I did another review and this LGTM feel free to push
</comment><comment author="dadoonet" created="2014-07-25T15:05:58Z" id="50162226">Pushed in `master` and `1.x`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fuziness ignored when multi_match query type is set to cross_fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6866</link><project id="" key="" /><description>Is there any reason for this behavior ?
It can be very usefull to use fuziness on cross_fields multi_match queries.
</description><key id="37857592">6866</key><summary>Fuziness ignored when multi_match query type is set to cross_fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">afoucret</reporter><labels><label>discuss</label></labels><created>2014-07-15T07:25:39Z</created><updated>2017-07-03T14:36:02Z</updated><resolved>2014-07-15T13:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-15T10:36:34Z" id="49015042">Hi @afoucret 

Yes, fuzziness really complicates the logic with the blended term query that cross_fields uses internally, and has a significant performance impact. Currently we have no plans to add support for fuzziness to cross_fields.
</comment><comment author="afoucret" created="2014-07-15T11:34:04Z" id="49019801">Really sad to heard this.

Did you have an alternative or some magic tricks to obtains some this kind of feature ?
</comment><comment author="clintongormley" created="2014-07-15T13:41:18Z" id="49032582">You can use `copy_to` to make a custom `_all` field and then the query becomes a simple `match` query with fuzziness
</comment><comment author="afoucret" created="2014-07-18T12:46:39Z" id="49426499">Hi @clintongormley,

One of the shortcoming of the method you propose is that it seems impossible to have weighted fields with copy_to. The other one is the need to reindex all when you want to change the fields used into the query.

I propose the issue stays open because the cross_fields fuzziness is an important feature IMHO.

From what I experienced during my tests, the problem seems to be into blended() which does not support the Lucene fuzziness operator : 

The query string I have used fro my test :

```
blended("bath", field: [name_fr, description_fr])             &lt;-- works
blended("bath~0.2", field: [name_fr, description_fr])       &lt;-- does not works
```

Am I right ?
If so, is this a ES patch or a Lucene problem (I can find where blended is implemented yet)
</comment><comment author="Kaidanov" created="2014-11-10T09:48:38Z" id="62361858">I have a query with should with function_score with fuzziness 0.75 with multi_match with cross_field
My query is a term without one last character - for instance "NAM" instead of "NAME"
By fuzziness rules it must be returned to me , but I don't get results.
What would you suggest on this matter?
</comment><comment author="ebuildy" created="2015-01-21T16:32:45Z" id="70870024">What about throw a QueryParsingException error? I lost 2 hours before arriving on this issue ;-(

Thanks
</comment><comment author="markwalsh-liverpool" created="2015-03-30T13:38:36Z" id="87685273">+1, would be really handy
</comment><comment author="linkwoman" created="2015-04-30T21:03:24Z" id="97965743">+1
</comment><comment author="cpitzak" created="2015-06-16T18:29:48Z" id="112522104">+1 I need this :)
</comment><comment author="Kaidanov" created="2015-06-17T07:54:13Z" id="112703714">Best_fields worked for me
On Jun 16, 2015 9:30 PM, "Clinton J. Pitzak" notifications@github.com
wrote:

&gt; +1 I need this :)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/6866#issuecomment-112522104
&gt; .
</comment><comment author="tegansnyder" created="2015-07-13T03:41:59Z" id="120800444">:+1: 
</comment><comment author="ctassparker" created="2016-05-11T01:12:01Z" id="218336782">:+1: 
</comment><comment author="PatrickHuetter" created="2017-07-03T14:36:02Z" id="312662077">+1</comment></comments><attachments /><subtasks /><customfields /></item><item><title>copy_to for Nested fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6865</link><project id="" key="" /><description>Hello,

I have a mapping like this:

```
curl -XPUT 'http://localhost:9200/onesite/drupal/_mapping' -d '
{
  "drupal" : {
    "properties" : {
     "id" : {"type" : "string", "store" : true, copy_to: "url" },
     "title" : {"type" : "string", "store" : true },
     "type" : {"type" : "string", "store" : true },
     "changed" : {"type" : "date", "format" : "YYYY-MM-dd", "store" : true, copy_to : "date" },
     "date" : {"type": "string"},
     "url" : {"type" : "string", "store" : true },
     "topic" : {"type" : "string", "store" : true },
     "lang" : {"type" : "string", "store" : true },
     "path_alias" : {"type" : "string", "store" : true },
     "language" : {"type" : "string", "store" : true ,"copy_to" : "lang"},
     "body": {
        "type": "nested",
        "properties": {
           "und": {
              "type": "nested",
              "properties": {
                 "format": {"type": "string"},
                 "safe_value": {"type": "string"},
                 "value": {"type": "string", "copy_to": "content"}
              }
            }
        }
       },
       "field_intro": {
          "type": "nested",
          "properties": {
             "und": {
                "type": "nested",
                "properties": {
                   "format": {"type": "string"},
                   "safe_value": {"type": "string"},
                   "value": {"type": "string"}
                }
             }
           }
        },
        "content" : {"type" : "string", "store" : true, index: "analyzed" }
    }
 }
}'
```

As you can see, I'm trying to get the value of "body.und.value" into "content".

I can query body.und.value but I cannot query on "content". 

Can someone help in identifying what I'm doing wrong? 

I'm using Elasticsearch 1.1.2

```
curl -XGET localhost:9200
{
  "status" : 200,
  "name" : "Eric Williams",
  "version" : {
    "number" : "1.1.2",
    "build_hash" : "e511f7b28b77c4d99175905fac65bffbf4c80cf7",
    "build_timestamp" : "2014-05-22T12:27:39Z",
    "build_snapshot" : false,
    "lucene_version" : "4.7"
  },
  "tagline" : "You Know, for Search"
}

```
</description><key id="37834502">6865</key><summary>copy_to for Nested fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">thevman</reporter><labels /><created>2014-07-14T22:24:20Z</created><updated>2014-07-15T10:33:44Z</updated><resolved>2014-07-15T10:33:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-15T10:33:44Z" id="49014818">Hi @thevman 

Thanks for reporting.  This looks like a duplicate of #6701 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove setNextScore in SearchScript.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6864</link><project id="" key="" /><description>While it would be nice to do this all the way up the chain (into
score functions), this at least removes the weird dual
setNextScore/setScorer for SearchScripts.
</description><key id="37834281">6864</key><summary>Remove setNextScore in SearchScript.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T22:21:11Z</created><updated>2015-06-07T12:51:49Z</updated><resolved>2014-07-16T22:11:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T12:36:25Z" id="49024939">I left some comments... look good in general
</comment><comment author="kimchy" created="2014-07-16T09:22:24Z" id="49140610">LGTM, we should make sure we fix mvel as well, not just groovy
</comment><comment author="s1monw" created="2014-07-16T19:33:05Z" id="49215169">@kimchy Mvel it removed in master that is why it's not in the PR it's still on 1.x so I guess ryan is fixing it once he is porting the commit
</comment><comment author="rjernst" created="2014-07-16T22:08:27Z" id="49234504">Yes, I will fix mvel in 1.x when I backport to there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Randomize `preference` on SearchRequest et al.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6863</link><project id="" key="" /><description>We should make sure that the preference values that don't restrict the search to only certain shards don't have impact on the actual result ie. miss shards etc. we should randomize them using the `RandomizedClient`
</description><key id="37819675">6863</key><summary>[TEST] Randomize `preference` on SearchRequest et al.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>adoptme</label><label>test</label></labels><created>2014-07-14T19:34:06Z</created><updated>2014-07-18T10:35:53Z</updated><resolved>2014-07-18T10:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update API: Detect noop updates when using doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6862</link><project id="" key="" /><description>This allows you to request that Elasticsearch figure out if an update using a document is a noop and then skip it.  This is useful for clients where it is difficult to figure out if the request is really a noop.

Also adds two tolerance measures that can be used to deem a request a noop even if it isn't quite.  The idea being that it isn't _that_ important to know that a page has 100 vs 101 links - so you may as well wait until the field is out of date by more then, say, 10%.

Close #6822
</description><key id="37817045">6862</key><summary>Update API: Detect noop updates when using doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:CRUD</label><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T19:03:49Z</created><updated>2015-06-06T18:29:00Z</updated><resolved>2014-07-22T12:57:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-14T19:04:43Z" id="48944931">I need to add docs and hit it over REST a few more times to make sure it works, but so far so good.
</comment><comment author="nik9000" created="2014-07-14T19:05:47Z" id="48945084">Might also be good to see what the performance cost is but I don't imagine its huge.  Next to 0 if you disable it.
</comment><comment author="nik9000" created="2014-07-14T20:17:43Z" id="48953772">Tried a quick and dirty performance test

``` bash
echo '{
    "doc": {"foo": 2},
    "doc_as_upsert": true,
    "detect_noop": {
        "foo": "10%"
    }
}' &gt; /tmp/test2
ab -n 1000 -c 20 -p /tmp/test2 http://localhost:9200/test/test/1/_update
```

Request dropped from 11ms to 8ms.  Cut but not a big deal - I'm more interested in creating less garbage that has to be merged out later.
</comment><comment author="jpountz" created="2014-07-15T13:02:36Z" id="49027870">I understand the point of not scheduling future merges if they are not necessary, but I'm not very happy with the part about tolerances. Eg if you have one update that increments a counter by 10 it could be rejected while two updates that would increment the same counter by 5 would be accepted. I think that can be confusing and should rather be dealt with from client side.
</comment><comment author="nik9000" created="2014-07-15T13:09:02Z" id="49028575">&gt; I understand the point of not scheduling future merges if they are not necessary, but I'm not very happy with the part about tolerances. Eg if you have one update that increments a counter by 10 it could be rejected while two updates that would increment the same counter by 5 would be accepted. I think that can be confusing and should rather be dealt with from client side.

I could certainly document that adding tolerances to counters is unlikely to be a good idea.  In my use case I don't have counters but instead recount on the fly.  In that case the tolerances make this more interesting by allowing me to trade off a tiny bit of accuracy for performance.
</comment><comment author="jpountz" created="2014-07-16T09:28:03Z" id="49141181">I'd still rather have it implemented on client side, where you have all the flexibility to define what you want to use as a distance between documents to know whether an update is worth it. I'm worried about making the API complicated for something that doesn't bring much value.
</comment><comment author="nik9000" created="2014-07-16T21:22:29Z" id="49229093">Is it still worth having the noop detection without the tolerances?  I'm happy to roll them back.  I can't use the noop detection without the tolerances but if its worth having its worth having.

I imagine I'll give scripted updates another shot in 1.3 when I can use groovy.  They'll probably be much less brittle then MVEL was.
</comment><comment author="jpountz" created="2014-07-16T21:56:27Z" id="49233165">Detecting no-ops without tolerances sounds ok to me, but I'm wondering how common it is to submit a no-op update.

I was thinking about making it an automatic optimization rather than an option, but it would sometimes not work as expected. For example, it is allowed to update mappings to add a new multi field. So even an update that would re-submit the same document could result in a different inverted index. So if we decide to add such detection, I think we would need to make it an opt-in to avoid surprises.

My current feeling is that it has a high cost (in terms of development/maintenance) compared to the potential speedup but if no-op updates prove to be common I could change my opinion. @clintongormley What do you think?
</comment><comment author="clintongormley" created="2014-07-17T09:21:23Z" id="49281145">@jpountz i agree with you about not liking the tolerances, although what would need to be done to make this work in the application is to retrieve the document, check the tolerances, and then decide whether to reindex or not.  

I do hear of people wanting noop updates - I don't think it is a corner case, so I'd be inclined to accept the PR without tolerances, but keeping noop as an option for the reasons you cite. Another reason to make it opt-in: the user updates a synonyms file.
</comment><comment author="nik9000" created="2014-07-17T13:41:29Z" id="49307676">Removed tolerances.  I believe one side effect of the way the PR is implemented now is that updates that are literally empty objects (`{}`) are considered noops even with detection off.  I imagine I can fix that pretty easily if its a problem.  
</comment><comment author="jpountz" created="2014-07-17T22:39:24Z" id="49375944">Indeed I think that the behaviour when `detect_noop` is false should be the same as today, so to always perform an update and increase the version?
</comment><comment author="nik9000" created="2014-07-17T22:50:53Z" id="49376817">&gt; Indeed I think that the behaviour when detect_noop is false should be the same as today, so to always perform an update and increase the version?

Can do.
</comment><comment author="nik9000" created="2014-07-18T15:59:50Z" id="49447873">Done with updates from this round of comments.  Thanks for reviewing!
</comment><comment author="jpountz" created="2014-07-22T12:57:32Z" id="49734782">Thanks Nik, I just merged this change. FYI I did a minor change while merging to replace `dec` with `inc` on the type stats in ShardIndexingService.
</comment><comment author="nik9000" created="2014-07-22T13:09:32Z" id="49736065">Thanks!  I saw the comment and, yeah, it agree it was supposed to be inc.

Thanks again!

Nik

On Tue, Jul 22, 2014 at 8:57 AM, Adrien Grand notifications@github.com
wrote:

&gt; Thanks Nik, I just merged this change. FYI I did a minor change while
&gt; merging to replace dec with inc on the type stats in ShardIndexingService.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/6862#issuecomment-49734782
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove (mostly) unused 'failure' member from ShardSearchFailure.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6861</link><project id="" key="" /><description>closes #6837
</description><key id="37813332">6861</key><summary>Remove (mostly) unused 'failure' member from ShardSearchFailure.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T18:32:16Z</created><updated>2015-06-07T12:51:58Z</updated><resolved>2014-08-07T20:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T12:41:38Z" id="49025528">this looks good to me - I wonder what @kimchy thinks
</comment><comment author="kimchy" created="2014-07-23T16:25:20Z" id="49898254">LGTM
</comment><comment author="clintongormley" created="2014-08-07T19:11:27Z" id="51517751">@rjernst want to get this merged in?
</comment><comment author="rjernst" created="2014-08-07T20:05:08Z" id="51524205">Yes, will do so shortly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make transport action name available in TransportAction base class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6860</link><project id="" key="" /><description>Each transport action is associated with at least an action name, which is the action name that gets serialized together with the request and identifies what to do with the request itself. Also, the action name is the name of the registered transport handler that handles incoming request for the transport action.

This PR makes the action name available in a generic manner in the TransportAction base class, so that it can be used when needed by subclasses, or in the base class for instance for action filtering.
</description><key id="37807319">6860</key><summary>Make transport action name available in TransportAction base class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>breaking</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T17:24:20Z</created><updated>2015-06-06T16:46:44Z</updated><resolved>2014-07-15T12:36:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-14T17:35:35Z" id="48931068">LGTM (I would have called the field `actionName` instead... but it's not critical)
</comment><comment author="javanna" created="2014-07-14T17:37:40Z" id="48931349">Funny thing is...I had called it `actionName` in the first place, then renamed it back to `transportAction` to be compliant to what we used up until now...I do prefer `actionName`
</comment><comment author="s1monw" created="2014-07-14T19:15:23Z" id="48946261">LGTM - thanks for doing this.
</comment><comment author="javanna" created="2014-07-15T12:37:18Z" id="49025032">As a side note, this change will cause compilation errors within plugins that implement their own `TransportAction`s, easily solvable by just passing in their action name when calling the `super` constructor.
</comment><comment author="s1monw" created="2014-07-15T12:37:51Z" id="49025093">mark it as `breaking` then?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Add docs for indexed script and templates.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6859</link><project id="" key="" /><description>These are the docs for the indexed script and templates. 
These describe both new APIs and enhancements and additions to others.
</description><key id="37804605">6859</key><summary>Scripting: Add docs for indexed script and templates.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GaelTadh</reporter><labels><label>docs</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T16:52:41Z</created><updated>2014-07-16T11:08:29Z</updated><resolved>2014-07-16T10:43:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-14T17:04:22Z" id="48927070">@GaelTadh I'd opt for `put_template` rather than `put_searchtemplate`.  It doesn't clash with index templates, as those live in the `indices` namespace, eg `indices.put_template`
</comment><comment author="GaelTadh" created="2014-07-14T17:05:05Z" id="48927160">ok as long as you don't think there will be any confusion I'm fine with it
</comment><comment author="clintongormley" created="2014-07-15T14:35:37Z" id="49040429">Some docs changes, but otherwise LGTM
</comment><comment author="s1monw" created="2014-07-15T19:30:56Z" id="49080992">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Significant Terms: Add google normalized distance and chi square </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6858</link><project id="" key="" /><description>For google normalized distance, see
"The Google Similarity Distance", Cilibrasi and Vitanyi, 2007
http://arxiv.org/pdf/cs/0412098v3.pdf

For chi square, see
"Information Retrieval", Manning et al., Eq. 13.19
</description><key id="37801113">6858</key><summary>Significant Terms: Add google normalized distance and chi square </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T16:12:49Z</created><updated>2015-06-06T18:30:25Z</updated><resolved>2014-08-04T06:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-15T12:40:25Z" id="49025382">I left a couple of comments. Something that looks important to me is to explain how these heuristics compare, otherwise it would be hard to find out which one best fits given data/use-cases?
</comment><comment author="hkorte" created="2014-07-24T14:54:30Z" id="50028994">Regarding the heuristic comparison for the docs: How about an experiment using a large corpus like the Wikipedia. You could plot the distribution over document frequencies of the top-X significant terms to a large number of randomly generated queries for each of the different heuristics. My gut feeling tells me that you see differences in the distributions. Then you could advise to use heuristic A if you are interested in rather rare terms with a very strong correlation or to use heuristic B if you are interested in rather common words with a possibly slightly lower correlation.
</comment><comment author="brwe" created="2014-07-25T10:43:23Z" id="50133781">Thanks for the review! I added commits to address the comments. 
Next I will try to come up with a way to show the difference between the heuristics. I'll start with what @hkorte proposed.
</comment><comment author="brwe" created="2014-07-28T10:39:04Z" id="50323466">I made some plots. Let me know what you think. I also added example results but I do not think that looking at examples tells a lot about quality of the result.

#### Experiment

Index wikipedia, 243361 pages, 2 shards

For 200 random words, compute top 100 significant terms. This should then be the terms that co-occur with the query term most often. Here is a small illustration:

![cooc-illu 001](https://cloud.githubusercontent.com/assets/4320215/3718159/99e5d950-1632-11e4-9856-fb002dc3c19f.jpg)

"query term" here refers to the set of documents that contain one of the random terms. "other term" is the set of documents that contain another term which is potentially significant to the query term. "num co-occurences" is the set of documents that contain both terms.

For each of the 200 query terms I collected the top 100 most significant terms and then plotted
plotted the mean (over the 200 random words) of 
1. # co-occurrence: How big is the set of co-occurring documents?
2. # other term: How often does the significant term appear in the whole set?
3. # co-occurrences/ # num query term: How many (in percent) of the documents containing query term also contain the significant term?
4. # co-occurrences/ # other term: How many (in percent) of the documents containing the significant term also contain the query term?

For example, a high value for 1. and 3. and low value for 2. and 4. would indicate a preference of the heuristic to select terms that often appear together with the query term even if it they also appear in the background frequently.

In addition, show the resulting significant terms for one term that occurs often in documents and one that occurs rarely just to get an idea.

### Plots

Just to get an idea of how frequent the query terms are, here is a histogram for the number of documents containing each of the 200 randomly chosen query terms (number of terms falling in bucket/ number of documents containing the terms):

![image](https://cloud.githubusercontent.com/assets/4320215/3718681/5363e7c2-1639-11e4-9cbe-700ee3f923eb.png)
1. # co-occurrence: How big is the set of co-occurring documents? (mean over 200 query terms, plot for the top 100 significant terms)

![image](https://cloud.githubusercontent.com/assets/4320215/3718639/e0fe7332-1638-11e4-84f1-b271c5ffeead.png)
2. # other term: How often does the significant term appear in the whole set? (mean over 200 query terms, plot for the top 100 significant terms)

![image](https://cloud.githubusercontent.com/assets/4320215/3718653/14a4b58e-1639-11e4-8fba-295317e461ed.png)
3. # co-occurrences/ # num query term: How many (in percent) of the documents containing query term also contain the significant term?(mean over 200 query terms, plot for the top 100 significant terms)

![image](https://cloud.githubusercontent.com/assets/4320215/3718657/1dd6451e-1639-11e4-98c8-4cbefbd47156.png)
4. # co-occurrences/ # other term: How many (in percent) of the documents containing the significant term also contain the query term?(mean over 200 query terms, plot for the top 100 significant terms)

![image](https://cloud.githubusercontent.com/assets/4320215/3718664/27e9a8a2-1639-11e4-8a2d-86bb246f5ccc.png)

#### Result examples

Example results for term "shoe" (appearing in few - 1512 - documents)

| jlh | chi_square | mutual_information | gnd |
| --: | --: | --: | --: |
| shoe | shoe | shoe | shoe |
| shoes | shoes | shoes | shoes |
| footwear | footwear | one | footwear |
| leather | leather | have | boot |
| factory | factory | new | boots |
| boot | boot | old | leather |
| boots | boots | many | heel |
| street | heel | be | shoemaking |
| heel | shoemaking | time | soles |
| clothing | soles | home | soled |
| old | clothing | first | tenspeed |
| home | shop | their | sneakers |
| shop | wear | two | laces |
| shoemaking | soled | when | adidas |
| soles | street | into | toe |
| just | store | they | cordwainers |
| wear | old | around | medalen |
| store | foot | three | sqornshellous |
| back | home | well | decristofaro |
| around | just | by | buildering |

Example results for term "germany" (appearing in many - 21662 - documents)

| jlh | chi_square | mutual_information | gnd |
| --: | --: | --: | --: |
| germany | germany | germany | germany |
| german | german | german | german |
| france | france | france | france |
| italy | austria | europe | italy |
| austria | italy | italy | austria |
| europe | poland | world | europe |
| poland | nazi | austria | poland |
| nazi | europe | poland | countries |
| netherlands | netherlands | european | netherlands |
| berlin | berlin | netherlands | nazi |
| european | switzerland | countries | berlin |
| countries | european | nazi | european |
| world | countries | international | republic |
| switzerland | russia | kingdom | russia |
| russia | belgium | war | switzerland |
| belgium | world | during | spain |
| kingdom | hungary | berlin | belgium |
| republic | sweden | after | sweden |
| sweden | spain | russia | hungary |
| hungary | republic | republic | von |

#### Conclusion

Mutual information seems to select frequent terms more often than rare terms. This might mean that you end up with lots of stopwords as for example in the "shoe" example.
Google normalized distance seems to favor less frequent terms. This might be good for avoiding stopwords but you might also end up with, for example, misspellings as for example in the "shoe" example.
Chi square and jlh perform comparably and are somewhat in between.

It will be hard to say which one is "better" because the usefulness of the significant terms seems to be tied to the application they are used in (see for example [Yang and Petersen](http://courses.ischool.berkeley.edu/i256/f06/papers/yang97comparative.pdf) where they evaluate goodness by measuring performance of feature selection methods by evaluating the performance of a subsequent text classification using the features).

If no one objects I will add this little analysis to the documentation.
</comment><comment author="brwe" created="2014-07-31T13:59:31Z" id="50761933">I addressed all comments in separate commits. Let me know if they make sense!
</comment><comment author="jpountz" created="2014-08-01T14:52:29Z" id="50893211">Left one minor comment, other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Better shard_size default for terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6857</link><project id="" key="" /><description>For the Terms Aggregation the shardSize currently defaults to size.  This is not a particularly good default value and also is not inline with the defaults set by the significant terms and geohash grid aggregaitons.  We should change the defaults for the terms aggregation to use BucketUtils.suggestShardSideQueueSize() to be consistent and provide a more useful default
</description><key id="37798642">6857</key><summary>Aggregations: Better shard_size default for terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T15:46:50Z</created><updated>2014-07-29T08:32:10Z</updated><resolved>2014-07-24T07:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Don't close/reopen IndexWriter when changing RAM buffer size</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6856</link><project id="" key="" /><description>Today we close/reopen IW when we change the RAM buffer but that's
costly because it means the next NRT reader is a full reopen.  The RAM
buffer size setting is a live one in IndexWriter, even if there are no
buffered docs in RAM when you call it.

Separately it would be nice if Lucene let you manage a "reader pool"
that could outlive individual IW instances ...
</description><key id="37795390">6856</key><summary>Don't close/reopen IndexWriter when changing RAM buffer size</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T15:12:31Z</created><updated>2015-06-07T12:52:16Z</updated><resolved>2014-07-15T12:31:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T11:17:55Z" id="49018554">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use KeyedLock in IndexFieldDataService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6855</link><project id="" key="" /><description>Today we synchronize when updating the IndexFieldDataService
datastructures. This might unnecessarily block progress if multiple
request need different fielddata instance for different fields.

This commit also fixes clear calls to actually consistently clear
the caches in the case of an exception.
</description><key id="37794228">6855</key><summary>Use KeyedLock in IndexFieldDataService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T15:00:09Z</created><updated>2015-06-07T12:52:27Z</updated><resolved>2014-07-16T14:05:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-15T08:46:52Z" id="49005459">left a comment about obtaining a global lock on clear, other than that, LGTM
</comment><comment author="s1monw" created="2014-07-15T11:16:31Z" id="49018440">@kimchy pushed a new commit
</comment><comment author="kimchy" created="2014-07-15T15:49:18Z" id="49052109">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose IndexWriter and VersionMap RAM usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6854</link><project id="" key="" /><description>Implements exposing the IndexWriter and VersionMap RAM usage added in #6443 to the _cat endpoint. 

Closes #6483
</description><key id="37790765">6854</key><summary>Expose IndexWriter and VersionMap RAM usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels /><created>2014-07-14T14:24:36Z</created><updated>2014-07-15T00:26:00Z</updated><resolved>2014-07-15T00:26:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="areek" created="2014-07-14T14:27:39Z" id="48906128">First stab at exposing the indexWriter &amp; VersionMap stats. Is there any way to ensure that the versionMap and IndexWriter has non-zero size in the tests? @bleskes &amp; @mikemccand Any feedback/thoughts welcome.  
</comment><comment author="mikemccand" created="2014-07-14T14:40:06Z" id="48907822">@areek if you index one document without using auto-id (i.e. pass in your own ID) then that should put a single entry into the versionMap, and a single doc in IW's ramBuffer, and then both counters should be non-zero.  Probably you should disable refresh in the test, because that would clear IW's RAM usage.
</comment><comment author="areek" created="2014-07-14T14:46:53Z" id="48908738">Thanks @mikemccand, will give that a shot.
</comment><comment author="s1monw" created="2014-07-14T20:36:11Z" id="48955931">This looks good to me - I think it needs some docs for the cat part but other than that it's seems good to go
</comment><comment author="areek" created="2014-07-14T23:35:56Z" id="48973741">Added docs; incorporated all the feedback
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update query-string-syntax.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6853</link><project id="" key="" /><description>Im not sure if you will like this change. But the documentation should contain an indication, that the boolean operators can be used in this situation
</description><key id="37789920">6853</key><summary>Update query-string-syntax.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mschirmacher</reporter><labels /><created>2014-07-14T14:15:10Z</created><updated>2014-07-14T14:35:53Z</updated><resolved>2014-07-14T14:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-14T14:20:56Z" id="48905199">Hi @mschirmacher 

Looks good to me, except that `ommit` should have only one `m`
</comment><comment author="mschirmacher" created="2014-07-14T14:23:44Z" id="48905590">@clintongormley  oops, sorry -- fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>AbstractDoubleSearchScript params does not support any kind of Object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6852</link><project id="" key="" /><description>Hello,

I'm trying to pass an Object inside the parameters Map in a native script. When I'm accesing it inside the script I have an exception "java.lang.String can not be casted to yourObjectClassType" even the Object implements Serializable.

``` java
QueryBuilder query = QueryBuilders.customScoreQuery( QueryBuilders.matchAllQuery() )
    .lang("native")
    .script("Test")
    .param("testobject", new TestObject() );
```

``` java
package com.imagenii.elasticsearch.scripts;

import  com.imagenii.elasticsearch.scripts.TestObject;
import java.util.Map;

import org.elasticsearch.common.Nullable;
import org.elasticsearch.script.AbstractDoubleSearchScript;

public class Test extends AbstractDoubleSearchScript 
{
    public Test(@Nullable Map&lt;String,Object&gt; params) 
    {
        try
        {
            TestObject testobject= (TestObject)params.get("testobject");

        }catch(Exception e)
        {
            System.out.println(e.getMessage());
        }
    }

    @Override
    public double runAsDouble() 
    {
        return 0.0;
    }
}
```

``` java
package com.imagenii.elasticsearch.scripts;

import java.io.Serializable;

public class TestObject implements Serializable
{
    private static final long serialVersionUID = 1L;

    public String say = "Hello";
}

```
</description><key id="37788488">6852</key><summary>AbstractDoubleSearchScript params does not support any kind of Object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">DavidGOrtega</reporter><labels /><created>2014-07-14T13:58:21Z</created><updated>2014-07-14T17:10:03Z</updated><resolved>2014-07-14T17:10:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-07-14T15:33:14Z" id="48915030">At this time, arbitrary objects are serialized with toString() when converted to json.  What exactly are you trying to encoded (more than just the simple String example you have here?)? Could you encode the data as multiple parameters?
</comment><comment author="DavidGOrtega" created="2014-07-14T16:09:02Z" id="48919930">I left ObjectTest as a valid example of Object that can no be deseralized/casted. 
My objects are  more complex so far but the essence is the same: the native script can't cast to the original object.

I have seen that sending Lists or Maps works but also for Strings or numbers, oviously, if the serialization is done by toString method the only thing I'm going to have on the other side is going to be the pointer reference.

I dont understand why they are being converted to json. It's java code talking to java code. 
</comment><comment author="rjernst" created="2014-07-14T16:33:39Z" id="48923154">&gt; I dont understand why they are being converted to json. It's java code talking to java code.

You're code is building a request, with java code, to send to the ES server. The request is serialized into json when sent to the server.

If you _really_ need this, then you could implement toString() in your object to serialize to a byte array, then to a base64 string.  In your script constructor, then decode the base64 and back to unserialize.  But this would be _horribly_ inefficient.  It would be better to just pass your data as json-"isable" to the parameters.   
</comment><comment author="DavidGOrtega" created="2014-07-14T17:02:14Z" id="48926813">&gt; You're code is building a request, with java code, to send to the ES server. The request is serialized into json when sent to the server.

In reality is a request inside a rest plugin. 
</comment><comment author="rjernst" created="2014-07-14T17:10:03Z" id="48927772">In summary, you either need to explicitly serialize your object to json friendly structures when adding to the parameters map, or implement toString with your own deserialization upon extraction from parameters in your script.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Docs for the indexed templates/script features</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6851</link><project id="" key="" /><description>Pull Request #5921 adds new functionality for storing and retrieving indexed scripts and templates from a .scripts index. We need to update the documentation to reflect this new feature.
</description><key id="37784468">6851</key><summary>Scripting: Docs for the indexed templates/script features</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels><label>docs</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T13:07:30Z</created><updated>2014-07-16T11:09:16Z</updated><resolved>2014-07-16T09:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve Hunspell error messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6850</link><project id="" key="" /><description>The Hunspell service would throw a confusing error message if more than
one affix file was present.  This commit distinguishes between the two
error cases: where there are no affix files and when there are too many
affix files.

Also implements lazy dictionary loading, which was used in the tests
but not implemented.
</description><key id="37772274">6850</key><summary>Improve Hunspell error messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T09:47:00Z</created><updated>2015-06-07T12:52:37Z</updated><resolved>2014-07-14T10:14:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-14T09:50:57Z" id="48882230">left cosmetic comments other than that LGTM
</comment><comment author="clintongormley" created="2014-07-14T10:10:33Z" id="48883793">@s1monw pushed changes as per your comments
</comment><comment author="s1monw" created="2014-07-14T10:10:52Z" id="48883818">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possible NPE during shutdown for requests using timeouts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6849</link><project id="" key="" /><description /><key id="37768491">6849</key><summary>Fix possible NPE during shutdown for requests using timeouts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-14T08:45:52Z</created><updated>2015-06-07T19:28:32Z</updated><resolved>2014-07-14T08:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-14T08:46:27Z" id="48877131">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Controlled repeat filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6848</link><project id="" key="" /><description>Adds filter that repeats every token it recieves a configured number of times.
That number of times is the integer value of the first token it receives.

Closes #6847
</description><key id="37765149">6848</key><summary>Controlled repeat filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-07-14T07:43:22Z</created><updated>2014-07-16T21:10:29Z</updated><resolved>2014-07-16T21:10:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-14T07:45:09Z" id="48873072">This is almost surely the wrong way to get weighted all fields, but it looks like it doesn't fail horribly.  Even phrase queries run against it reasonably quickly in local testing.
</comment><comment author="clintongormley" created="2014-07-14T11:23:47Z" id="48889118">This feels really hacky...  As I commented in #6847 you should just use the `cross_fields` search type with multi-match, no?
</comment><comment author="nik9000" created="2014-07-16T21:09:33Z" id="49227483">After playing more with it this putting the terms on top of one another saves analysis speed but makes phrase queries not work properly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Weighted copy_to</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6847</link><project id="" key="" /><description>It'd be cool to have weighted `copy_to`.  You could use it as a faster stand in for querying many fields with different weights.
</description><key id="37765046">6847</key><summary>Weighted copy_to</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>discuss</label></labels><created>2014-07-14T07:41:35Z</created><updated>2014-07-25T11:21:59Z</updated><resolved>2014-07-25T10:05:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-14T11:21:35Z" id="48888964">Hi @nik9000 

We specifically decided against supporting field-level boosts on `copy_to` fields, as it requires storing a payload against every term and loading those payloads at query time.

For this use case, better to use the `cross_fields` search type on the multi-match query, where you can boost each field separately at query time.
</comment><comment author="nik9000" created="2014-07-14T14:18:39Z" id="48904921">&gt; We specifically decided against supporting field-level boosts on copy_to fields, as it requires storing a payload against every term and loading those payloads at query time.
&gt; 
&gt; For this use case, better to use the cross_fields search type on the multi-match query, where you can boost each field separately at query time.

Hi @clintongormley!

Thanks for the response so quickly.  Its nice to wake up to something!

Anyway, the issue is one of scale for me.  Right now I do way too much random IO and this offers me a chance to reduce it pretty significantly.  Its like a 1:8 savings for me right now and it'll only get better as we add more fields that we have to query with weight.  I wouldn't propose something so hacky if I felt I knew another way to get that kind of performance boost.

I'm happy to relegate this to the land of plugins.
</comment><comment author="clintongormley" created="2014-07-14T15:09:57Z" id="48911945">@nik9000 I'll reopen this and mark it for discussion 
</comment><comment author="nik9000" created="2014-07-14T20:37:09Z" id="48956051">Not that it makes it less janky, but this dovetails with #6599.  At least, I wouldn't want to to this without #6599.
</comment><comment author="nik9000" created="2014-07-16T21:11:27Z" id="49227718">So you can, right now, build a weighted all field by copying to the same field over and over again.  Its not super efficient to build but it does spit out the right answers.
</comment><comment author="clintongormley" created="2014-07-25T10:05:12Z" id="50130682">@nik9000 turns out you can have "weighted" copy_to fields already, and automatically, by repeating the field in the `copy_to` array:

```
PUT /test 
{
  "mappings": {
    "test": {
      "properties": {
        "title": {
          "type": "string",
          "copy_to": ["title_body","title_body"]
        },
        "body": {
          "type": "string",
          "copy_to": ["title_body"]
        }
      }
    }
  }
}
```

Also, it seems to work well with phrase queries. 

/cc @roytmana - this will perform much better than storing payloads as we do in the `_all` field.
</comment><comment author="clintongormley" created="2014-07-25T10:17:19Z" id="50131771">Also, to ensure that phrases don't overlap, you can set the `position_offset_gap` on the field being copied to:

```
PUT /test 
{
  "mappings": {
    "test": {
      "properties": {
        "title": {
          "type": "string",
          "copy_to": ["title_body","title_body"]
        },
        "body": {
          "type": "string",
          "copy_to": ["title_body"]
        },
        "title_body": {
          "type": "string",
          "position_offset_gap": 5
        }
      }
    }
  }
}
```
</comment><comment author="nik9000" created="2014-07-25T11:21:59Z" id="50136882">I thought i mentioned somewhere that that was possible!  In starting to
roll that out on Monday actually. Thanks!
On Jul 25, 2014 6:17 AM, "Clinton Gormley" notifications@github.com wrote:

&gt; Also, to ensure that phrases don't overlap, you can set the
&gt; position_offset_gap on the field being copied to:
&gt; 
&gt; PUT /test
&gt; {
&gt;   "mappings": {
&gt;     "test": {
&gt;       "properties": {
&gt;         "title": {
&gt;           "type": "string",
&gt;           "copy_to": ["title_body","title_body"]
&gt;         },
&gt;         "body": {
&gt;           "type": "string",
&gt;           "copy_to": ["title_body"]
&gt;         },
&gt;         "title_body": {
&gt;           "type": "string",
&gt;           "position_offset_gap": 5
&gt;         }
&gt;       }
&gt;     }
&gt;   }
&gt; }
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6847#issuecomment-50131771
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FilterBuilders.termsFilter bug report</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6846</link><project id="" key="" /><description>ES v 1.1.0, java client v1.1.0

assume that there is a document like below
{
      "_index" : "index1",
      "_type" : ".percolator",
      "_id" : "10201",
      "_score" : 1.0, "_source" : {"projects":["project1","project2"]}
}

.setPercolateFilter(FilterBuilders.termsFilter("projects","project1"))
.setPercolateFilter(FilterBuilders.termsFilter("projects","project2"))

both of them should work but actually, filter on project2 works well, but filter on project1 returns nothing.

I think it's not a problem of java client because I can easily reproduce this problem using curl command also.
</description><key id="37759240">6846</key><summary>FilterBuilders.termsFilter bug report</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sweetest</reporter><labels /><created>2014-07-14T04:58:40Z</created><updated>2014-07-14T11:18:53Z</updated><resolved>2014-07-14T05:34:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-14T11:18:53Z" id="48888760">@sweetest did you mean to close this issue?

If not, please could you post a full `curl` recreation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse IndexFieldData instances between percolator queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6845</link><project id="" key="" /><description>This is an improvement that came out of #6806. 

Instead of each percolator query having its own IndexFieldData instance, it is better to reuse the same instance between percolator queries. For example in case when many percolator queries use the same `geo_distance` filter, this can improve percolator query indexing and percolating document execution speed.
</description><key id="37751489">6845</key><summary>Reuse IndexFieldData instances between percolator queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-13T23:19:31Z</created><updated>2015-06-07T12:52:49Z</updated><resolved>2014-07-29T19:10:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-14T08:09:04Z" id="48874481">left a comment, should this go into 2.0 as well?
</comment><comment author="martijnvg" created="2014-07-15T08:54:26Z" id="49006176">Thanks. I tagged it with 2.0, I'll update this PR, once #6855 is in.
</comment><comment author="s1monw" created="2014-07-16T14:18:47Z" id="49171821">@martijnvg #6855 is pushed
</comment><comment author="martijnvg" created="2014-07-16T14:19:17Z" id="49171907">@s1monw awesome, I will update this PR today.
</comment><comment author="martijnvg" created="2014-07-16T19:57:26Z" id="49218215">@s1monw Updated the PR.
</comment><comment author="jpountz" created="2014-07-16T22:16:34Z" id="49235290">It would be nice to try to share more code between getForField and getForFieldDirect. I can easily see them getting out of sync otherwise. 

Other than that, LGTM
</comment><comment author="martijnvg" created="2014-07-29T19:10:24Z" id="50523498">Closing in favour for #7081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Increment Store refcount on RecoveryTarget</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6844</link><project id="" key="" /><description>We should make sure we have incremented the store refcount
before we start the recovery on the recovyer target.
</description><key id="37748438">6844</key><summary>Increment Store refcount on RecoveryTarget</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-13T20:02:46Z</created><updated>2015-06-07T19:28:43Z</updated><resolved>2014-07-14T07:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-14T00:20:59Z" id="48856210">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Share numeric date analyzer instances between mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6843</link><project id="" key="" /><description>use similar mechanism that shares numeric analyzers for long/double/... for dates as well. This has nice memory save properties with many date fields mapping case, as well as analysis saves (thread local resources)
</description><key id="37739391">6843</key><summary>Share numeric date analyzer instances between mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-13T11:21:06Z</created><updated>2015-06-07T12:52:59Z</updated><resolved>2014-07-15T19:25:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-15T11:19:35Z" id="49018684">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Add test case verifying that dynamically enabling/disabling merge IO throttling works</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6842</link><project id="" key="" /><description>For #6626
</description><key id="37726987">6842</key><summary>Test: Add test case verifying that dynamically enabling/disabling merge IO throttling works</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>non-issue</label><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-12T19:33:21Z</created><updated>2014-07-18T14:10:13Z</updated><resolved>2014-07-14T12:42:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-13T18:28:04Z" id="48848245">left some small comments -- thanks for  the test :)
</comment><comment author="s1monw" created="2014-07-14T10:07:21Z" id="48883542">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't attempt to start or fail shard if no master node can be found</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6841</link><project id="" key="" /><description /><key id="37726375">6841</key><summary>Don't attempt to start or fail shard if no master node can be found</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-12T19:05:30Z</created><updated>2015-06-07T12:53:39Z</updated><resolved>2014-07-13T14:16:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-13T08:50:02Z" id="48835205">LGTM. Left one comment regarding logging levels
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added more utility methods to Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6840</link><project id="" key="" /><description>- names() to return the direct settings names
- getAsSettings(String) to return the settings mapped to the given name (like getByPrefix(...) except no need to provide a tailing '.')
</description><key id="37724840">6840</key><summary>Added more utility methods to Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-12T17:59:09Z</created><updated>2015-06-07T12:53:53Z</updated><resolved>2014-07-14T18:31:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-07-14T16:10:15Z" id="48920105">LGTM.
</comment><comment author="uboness" created="2014-07-14T18:31:04Z" id="48938208">closed by 04b412b5972a08a79989d82cc9d17842fe3c3e5a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature request: automated deletion of indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6839</link><project id="" key="" /><description>When you create an index it would be good to be able to specify a date that it could be auto-deleted on.   Either a fixed, hard coded date or a parameter in how many days before deleting.   We are using ELK for monitoring of a lot of production actiivity and the indices can get very big.  Auto-deletion is a very attractive possible feature.
</description><key id="37698535">6839</key><summary>Feature request: automated deletion of indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sitkawoof</reporter><labels /><created>2014-07-11T21:20:51Z</created><updated>2014-07-11T22:47:03Z</updated><resolved>2014-07-11T22:47:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-11T22:47:03Z" id="48791723">Duplicate of #2114

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed invalid word count in geodistance agg doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6838</link><project id="" key="" /><description>There are three, not two distance calculation modes
</description><key id="37674953">6838</key><summary>Fixed invalid word count in geodistance agg doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels /><created>2014-07-11T16:18:57Z</created><updated>2014-07-11T16:36:16Z</updated><resolved>2014-07-11T16:36:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T16:36:14Z" id="48753123">Off by one errors-- :)

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>failure member in ShardSearchFailure is never serialized</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6837</link><project id="" key="" /><description>The `Throwable failure` member of `ShardSearchFailure` is taken on construction, but only used to generate a detailed error message to place in the `reason` member. It is never serialized/deserialized.  But there is a `failure()` function that returns the `Throwable`.  In the case of a transported failure, it is always `null`.

I think we should just remove `failure` (it is only set in one of the many constructors), and in the two tests that use it, use `reason` instead. 
</description><key id="37672551">6837</key><summary>failure member in ShardSearchFailure is never serialized</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2014-07-11T15:50:36Z</created><updated>2014-08-07T20:13:43Z</updated><resolved>2014-08-07T20:13:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] For connection rules make TransportAddress the identity instead of DiscoveryNode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6836</link><project id="" key="" /><description>For unicast ping the DiscoveryNode identity is based on its id, which in that stage is a dummy value, this breaks any rule in the mock transport service.
However the TransportAddress is a valid value in unicast ping and all other places, so that is a better alternative.
</description><key id="37671547">6836</key><summary>[TEST] For connection rules make TransportAddress the identity instead of DiscoveryNode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>test</label></labels><created>2014-07-11T15:39:53Z</created><updated>2015-05-18T23:30:57Z</updated><resolved>2014-07-14T10:58:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-14T10:16:26Z" id="48884265">LGTM. I think the `// TODO, if we miss on node by UID, we should have an option to lookup based on address?` can be removed now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Tests]  Enhance ZenUnicastDiscoveryTest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6835</link><project id="" key="" /><description>This started out as a simple correction to a missing setting problem, but go bigger into more general work on the ZenUnicastDiscoveryTets suite. It now works with both network and local mode. I also merge the different ZenUnicast test suites into a single place. 
</description><key id="37663204">6835</key><summary>[Tests]  Enhance ZenUnicastDiscoveryTest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>test</label></labels><created>2014-07-11T14:09:24Z</created><updated>2014-07-11T14:39:49Z</updated><resolved>2014-07-11T14:39:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-11T14:12:35Z" id="48734821">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup of the transport request/response messages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6834</link><project id="" key="" /><description>Now both TransportRequest and TransportResponse inherit from a base TransportMessage that holds the message headers and also now added the remote transport address (where this message came from).
</description><key id="37659989">6834</key><summary>Cleanup of the transport request/response messages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-11T13:30:04Z</created><updated>2015-06-07T12:54:04Z</updated><resolved>2014-07-14T20:06:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-11T14:14:57Z" id="48735113">LGTM
</comment><comment author="s1monw" created="2014-07-14T19:18:06Z" id="48946602">LGTM too
</comment><comment author="uboness" created="2014-07-14T20:06:21Z" id="48952449">closed by 25a21c6a01dbde90ac7710d274423245b34b2f72
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch's init script should run after NTP and syslog are up.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6833</link><project id="" key="" /><description>Closes #6832
</description><key id="37653209">6833</key><summary>ElasticSearch's init script should run after NTP and syslog are up.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tiran</reporter><labels /><created>2014-07-11T11:43:21Z</created><updated>2014-07-11T13:16:54Z</updated><resolved>2014-07-11T13:16:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T13:16:54Z" id="48728554">Thanks for the commit, but I think this would be problematic for those who are not using NTP.  Also, I don't think it's the root cause of the problem described in #6382. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch should start after NTP</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6832</link><project id="" key="" /><description>The elasticsearch init script requires $network $remote_fs $named as dependencies but not NTP. This causes an issue on virtual machines with a bad clock source. For example this just happened after a reboot of one of our ES nodes:

```
[2014-07-11 13:08:32,413][INFO ][node                     ] [Elixir] closing ...
[2014-07-11 13:08:32,678][INFO ][node                     ] [Elixir] closed
```

**reboot**

```
[2014-07-11 13:07:26,993][WARN ][common.jna               ] Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out. Increase RLIMIT_MEMLOCK or run elasticsearch as root.
[2014-07-11 13:07:27,261][INFO ][node                     ] [Equilibrius] version[1.2.1], pid[1867], build[ffc086b/2014-06-23T13:26:38Z]
[2014-07-11 13:07:27,261][INFO ][node                     ] [Equilibrius] initializing ...
[2014-07-11 13:07:27,368][INFO ][plugins                  ] [Equilibrius] loaded [river-rabbitmq, analysis-phonetic], sites [paramedic, head, bigdesk]
[2014-07-11 13:07:31,769][INFO ][node                     ] [Equilibrius] initialized
[2014-07-11 13:07:31,769][INFO ][node                     ] [Equilibrius] starting ...
[2014-07-11 13:07:32,096][INFO ][transport                ] [Equilibrius] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.107.60.184:9300]}
```

**NTP kicked in and corrects clock**

```
[2014-07-11 13:09:30,957][INFO ][cluster.service          ] [Equilibrius] new_master [Equilibrius][knWq6pPaQByj4cgULAqczg][shop-api-search4][inet[/10.107.60.184:9300]], reason: zen-disco-join (elected_as_master)
[2014-07-11 13:09:30,972][INFO ][discovery                ] [Equilibrius] ant-core-search/knWq6pPaQByj4cgULAqczg
[2014-07-11 13:09:31,065][INFO ][http                     ] [Equilibrius] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.107.60.184:9200]}
[2014-07-11 13:09:31,148][DEBUG][action.search.type       ] [Equilibrius] All shards failed for phase: [query_fetch]
```

The clock skew caused the ES node to announce itself as master although it should have joined a cluster with five nodes.
</description><key id="37652984">6832</key><summary>ElasticSearch should start after NTP</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tiran</reporter><labels /><created>2014-07-11T11:39:14Z</created><updated>2014-07-11T13:17:12Z</updated><resolved>2014-07-11T13:14:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T13:14:22Z" id="48728307">Hi @tiran 

I've just tested this out by setting a 60 second `discovery.zen.ping_timeout`, starting a node, then resetting the time into the future.  Elasticsearch waited for the full 60 seconds before continuing to elect itself as master. So I don't think that NTP was the issue here.  For some reason, your node didn't get a response from the existing cluster within the default timeout of 3s.

Looking at your logs, you are not setting `ulimit -l` correctly, so your `mlockall` setting is not being applied.  I assume this is the same on your other nodes, so it is quite likely that they weren't responding promptly because of long GCs.  This could be the reason for the timeout being exceeded.

Also, with a total of 5 nodes (or do you have a total of 6?) you should be setting `minimum_master_nodes` to a majority, ie 3 (or 4 if you have 6 nodes).  If you had had that setting, then your restarted node would continue pinging until it could see enough master-eligible nodes, instead of forming a cluster by itself.
</comment><comment author="clintongormley" created="2014-07-11T13:17:12Z" id="48728583">If I've misunderstood something here, please feel free to reopen
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Heap Memory analysis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6831</link><project id="" key="" /><description>Hi.

I am struggling with Elasticsearch (v1.2.1) consuming too much heap memory, forcing me to restart it every two weeks. To better understand the problem, I went as far as analyzing a memory dump. This analysis was not performed after a crash because I cannot afford waiting for a crash if I can avoid it, but I waited for the heap reaching 1.7GB (on 1.9GB allowed) to perform a heap dump and make an analysis. Once clean of every free object the final heap dump size is about 1.5GB. Here is the overview I got from MAT.

![00_overview](https://cloud.githubusercontent.com/assets/2630481/3551512/3e2f1da8-08e8-11e4-88d9-f388989c43a2.png)

Next I opened the dominator tree and grouped by class name. Here is the result:

![01_dominator_tree_group_by_class](https://cloud.githubusercontent.com/assets/2630481/3551547/9836bb1c-08e8-11e4-945f-66617d865942.png)

As you can see there are 63% (1GB) of HashMap objects. In some situations where HashMaps are used as a cache this can be normal. But still it seems worth investigating. Lets see what is nested in:

![02_hashmap_global](https://cloud.githubusercontent.com/assets/2630481/3551576/edfbe856-08e8-11e4-8e4c-c6af44501ca2.png)

Wow! 755MB of strings. That is quite a lot. Now lets take a look at what can be inside it. First we will stop grouping by class in the dominator tree, and rather filter on HashMaps objects. Here it is:

![03_hashmap_list](https://cloud.githubusercontent.com/assets/2630481/3551600/54a5ad9e-08e9-11e4-927d-1ef26e9310b9.png)

Ok so there are 9000 of them. Next lets open one of them to inspect what is inside it:

![04_hashmap_detail](https://cloud.githubusercontent.com/assets/2630481/3551630/c162ab58-08e9-11e4-9ea8-bf3897aeedfa.png)

Seems kinda like a log entry if you ask me. Should have mentioned it earlier, but I am using the usual stack Logstash/Elasticsearch/Kibana to monitor a java application. We have stacktraces in the logs and some of them are quite long and we use a multiline filter in logstash to gather them in a single message. No doubt that it is what happened here.

Please note that I am not saying that all 9000 Hashmaps contain log entries, actually even in the toped ranked 300ko Hashmaps I found some that do not contain that kind of stuff. But as far as I have seen, it happens quite often. The tokenizer seem to have a fixed 4096 char array size, and  since there was around 750M char[] in the heap dump, I would assume that there may be 92000 of them.

Now I know that it is normal that the heap would keep growing since I am continuously indexing new stuff and such, but every day at midnight when it compress the index, it should as well free all those tokenizer stuff. Since I get the memory back to 1.1GB every time I restart ElasticSearch, this is the proof that the memory from 1.1GB to 1.7GB is just leaking. Or maybe there is something wrong with the configuration of ElasticSearch? I am using the default, I just changed allowed memory from 1GB to 2GB, was there something else important to do? 

What do you think?
</description><key id="37651394">6831</key><summary>Elasticsearch Heap Memory analysis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Aldian-fr</reporter><labels><label>feedback_needed</label></labels><created>2014-07-11T11:10:16Z</created><updated>2014-08-25T08:15:49Z</updated><resolved>2014-08-25T08:15:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-07-11T12:34:02Z" id="48724820">Do you have many unique field names that are analyzed?

If so, you may be hitting
https://github.com/elasticsearch/elasticsearch/pull/6714 which is fixed in
1.3 (along with some other optimizations for the many-unique-fields case).
 Are you able to test on the 1.x snapshot build to see if that fixes it?

Mike McCandless

http://blog.mikemccandless.com

On Fri, Jul 11, 2014 at 7:10 AM, Aldian-fr notifications@github.com wrote:

&gt; Hi.
&gt; 
&gt; I am struggling with Elasticsearch consuming too much heap memory, forcing
&gt; me to restart it every two weeks. To better understand the problem, I went
&gt; as far as analyzing a memory dump. This analysis was not performed after a
&gt; crash because I cannot afford waiting for a crash if I can avoid it, but I
&gt; waited for the heap reaching 1.7GB (on 1.9GB allowed) to perform a heap
&gt; dump and make an analysis. Once clean of every free object the final heap
&gt; dump size is about 1.5GB. Here is the overview I got from MAT.
&gt; 
&gt; [image: 00_overview]
&gt; https://cloud.githubusercontent.com/assets/2630481/3551512/3e2f1da8-08e8-11e4-88d9-f388989c43a2.png
&gt; 
&gt; Next I opened the dominator tree and grouped by class name. Here is the
&gt; result:
&gt; 
&gt; [image: 01_dominator_tree_group_by_class]
&gt; https://cloud.githubusercontent.com/assets/2630481/3551547/9836bb1c-08e8-11e4-945f-66617d865942.png
&gt; 
&gt; As you can see there are 63% (1GB) of HashMap objects. In some situations
&gt; where HashMaps are used as a cache this can be normal. But still it seems
&gt; worth investigating. Lets see what is nested in:
&gt; 
&gt; [image: 02_hashmap_global]
&gt; https://cloud.githubusercontent.com/assets/2630481/3551576/edfbe856-08e8-11e4-8e4c-c6af44501ca2.png
&gt; 
&gt; Wow! 755MB of strings. That is quite a lot. Now lets take a look at what
&gt; can be inside it. First we will stop grouping by class in the dominator
&gt; tree, and rather filter on HashMaps objects. Here it is:
&gt; 
&gt; [image: 03_hashmap_list]
&gt; https://cloud.githubusercontent.com/assets/2630481/3551600/54a5ad9e-08e9-11e4-927d-1ef26e9310b9.png
&gt; 
&gt; Ok so there are 9000 of them. Next lets open one of them to inspect what
&gt; is inside it:
&gt; 
&gt; [image: 04_hashmap_detail]
&gt; https://cloud.githubusercontent.com/assets/2630481/3551630/c162ab58-08e9-11e4-9ea8-bf3897aeedfa.png
&gt; 
&gt; Seems kinda like a log entry if you ask me. Should have mentioned it
&gt; earlier, but I am using the usual stack Logstash/Elasticsearch/Kibana to
&gt; monitor a java application. We have stacktraces in the logs and some of
&gt; them are quite long and we use a multiline filter in logstash to gather
&gt; them in a single message. No doubt that it is what happened here.
&gt; 
&gt; Please not that I am not saying that all 9000 Hashmaps contain log
&gt; entries, actually even in the toped ranked 300ko Hashmaps I found some that
&gt; do not contain that kind of stuff. But as far as I have seen, it happens
&gt; quite often. The tokenizer seem to have a fixed 4096 char array size, and
&gt; since there was around 750M char[] in the heap dump, I would assume that
&gt; there may be 92000 of them.
&gt; 
&gt; Now I know that it is normal that the heap would keep growing since I am
&gt; continuously indexing new stuff and such, but every day at midnight when it
&gt; compress the index, it should as weel free all those tokenizer stuff. Since
&gt; I get the memory back to 1.1GB every time I restart ElasticSearch, this is
&gt; the proof that the memory from 1.1GB to 1.7GB is just leaking. Or maybe
&gt; there is something wrong with the configuration of ElasticSearch? I am
&gt; using the default, I just changed allowed memory from 1GB to 2GB, was there
&gt; something else important to do?
&gt; 
&gt; What do you think?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6831.
</comment><comment author="Aldian-fr" created="2014-07-11T13:36:42Z" id="48730654">I believe I don't have that many. According to Kibana I only have 20 different fields:

@timestamp
@version
_id
_index
_type
classname
command
exception
host
id_cam
id_com
idsession
ip
log_date
log_level
message
msg
port
tags
type

But I have 27 input types (The application is splitted across 27 servers). If you are making the same thing as for the mapping and treating separately each input type, that would make 20*27=540 fields.

Also I can't beta test since this is a production server and I don't have the problem on my test platform. But I will surely upgrade once you release 1.3
</comment><comment author="mikemccand" created="2014-07-11T14:01:07Z" id="48733460">Hmm I don't think the 27 different input types would multiply out the fields.  You don't use any dynamic_templates (this creates new Lucene fields under-the-hood).
</comment><comment author="Aldian-fr" created="2014-07-11T14:05:34Z" id="48734012">No nothing like this. Except if Logstash does it. In fact, only logstash writes in ES (I believe Kibana also saves its dashboards), but I did not tune Elasticsearch at all, except for setting its allowed memory to 2GB
</comment><comment author="mikemccand" created="2014-08-01T09:30:49Z" id="50866021">Did you try upgrading to 1.3.1?  Did memory usage improve?
</comment><comment author="Aldian-fr" created="2014-08-06T11:36:35Z" id="51323270">I am planning to try the upgrade this afternoon. I will tell you in a week or two.

Edit: Upgrade successful but the deactivation by default of JSONP broke my Bigdesk. For the time being I am blind about the server health. Will take a look about how to reactivate it.
</comment><comment author="Aldian-fr" created="2014-08-11T16:03:49Z" id="51800964">Unfortunately I had to restart the server to reactivate JSONP (required for Bigdesk to work). There should be some tuning about this security, like allways allowing JSONP for requests coming from the localhost.

I will take a look in two weeks to see if 1.3.1 solved the heap problem.
</comment><comment author="lukas-vlcek" created="2014-08-11T17:04:26Z" id="51808964">Speaking of Bigdesk, it hasn't been announced yet but there are some new
releases including fix for missing JSONP support (for Elasticsearch 1.3.x).
See release notes for details [1].

Before this is "officially" announced I want to upload new online versions
[2] too, however, if you install Bigdesk via plugin script you can use it
right now.

Regards,
Lukas

[1] https://github.com/lukas-vlcek/bigdesk#release-notes
[2] bigdesk.org/v/

On Mon, Aug 11, 2014 at 6:03 PM, Aldian-fr notifications@github.com wrote:

&gt; Unfortunately I had to restart the server to reactivate JSONP (required
&gt; for Bigdesk to work). There should be some tuning about this security, like
&gt; allways allowing JSONP for requests coming from the localhost.
&gt; 
&gt; I will take a look in two weeks to see if 1.3.1 solved the heap problem.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6831#issuecomment-51800964
&gt; .
</comment><comment author="Aldian-fr" created="2014-08-25T08:15:49Z" id="53238546">Two weeks after upgrading to 1.3.1, the heap consumption is stable. BigDesk reports 900 MB used over 1.9GB committed, which is about the same as after the last restart. Seems like the heap memory leak is gone. I am closing this. Thank you very much.

@lukas-vlcek thanks for the info. I will be trying this for the next upgrade ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed Histogram key_as_string bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6830</link><project id="" key="" /><description>The key as string field in the response for the histogram aggregation will now only show if format is specified on the request.

Closes #6655
</description><key id="37649592">6830</key><summary>Fixed Histogram key_as_string bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-11T10:41:25Z</created><updated>2015-06-07T19:28:51Z</updated><resolved>2014-07-16T10:41:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rnonnon" created="2014-07-15T08:30:06Z" id="49004053">Hi,
Thanks, but that "semi-fix" the problem because Elasticsearch core return the key_as_string, then if you don't use the JAVA API, the problem is still here. Moreover, the JSON returned from Elasticsearch to the JAVA API is still ~1/3 too big.
</comment><comment author="s1monw" created="2014-07-15T12:42:33Z" id="49025629">@colings86 can we have a test for this too?
</comment><comment author="colings86" created="2014-07-16T08:42:35Z" id="49137851">@rnonnon I'm not sure I fully understand what you mean.  After this change is merged, if you don't specify the format parameter on the histogram aggregation, the `key_as_string' field will not be present in the JSON response and the response will resemble the following:

```
...
"aggregations": {
      "test": {
         "buckets": [
            {
               "key": 450,
               "doc_count": 23
            },
            {
               "key": 500,
               "doc_count": 2452
            },
            {
               "key": 550,
               "doc_count": 12430
            },
            {
               "key": 600,
               "doc_count": 14982
            },
            {
               "key": 650,
               "doc_count": 13951
            }
            ...
```

This change does not affect the Java API as the serialisation to the Transpoort layer already does not serialise the key as a string
</comment><comment author="colings86" created="2014-07-16T10:15:56Z" id="49146146">@s1monw added a rest test to check the JSON output both with and without the format parameter specified
</comment><comment author="s1monw" created="2014-07-16T10:21:09Z" id="49146618">LGTM
</comment><comment author="colings86" created="2014-07-16T10:42:05Z" id="49148764">merged but did not auto-close
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed the node retry mechanism which could fail without trying all the connected nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6829</link><project id="" key="" /><description>The RetryListener was notified twice for each single failure, which caused some additional retries, but more importantly was making the client reach the maximum number of retries (number of connected nodes) too quickly, meanwhile ongoing retries which could succeed are not completed yet.

The `TransportService` already notifies the listener of any exception received from a separate thread through the request holder, no need to notify the retry listener again in any other place (either catch or `onFailure` method itself).
</description><key id="37648563">6829</key><summary>Fixed the node retry mechanism which could fail without trying all the connected nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-11T10:25:52Z</created><updated>2015-06-07T19:29:11Z</updated><resolved>2014-07-28T18:57:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-11T14:03:55Z" id="48733798">The fix look good to me (left some comments regarding comments :) ). I'd love to see a unit test as opposed to an integration test. I think we'd get much more out of it.
</comment><comment author="javanna" created="2014-07-25T16:58:02Z" id="50175990">Pushed new commits to address comments and a unit test for it as suggested by @bleskes . I also changed a bit how we catch exceptions given how they get thrown by the `TransportService`. Ready for reviews!
</comment><comment author="kimchy" created="2014-07-28T17:37:55Z" id="50371266">LGTM, very clean now indeed!
</comment><comment author="javanna" created="2014-07-28T18:59:45Z" id="50382666">Side note: as part of the work to fix this issue the `throwConnectException` option was removed from the `TransportService`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Config file placeholder for environment variables</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6828</link><project id="" key="" /><description>I spent a little time trying to get enviroment variables into placeholders in the configuration file as described here:

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html

..where is says that you should be able to include enviroment variables like this:

{
    "network" : {
        "host" : "${ES_NET_HOST}"
    }
}

..but I found a test in the repo that appears to reference the variable as part of "env", so that the example above would be:

{
    "network" : {
        "host" : "${env.ES_NET_HOST}"
    }
}

...and indeed that appears to work in my own application, but before I changed the documentation I wanted to double-check with someone who might understand the source code better than me that this is the intended behaviour.
</description><key id="37643141">6828</key><summary>Config file placeholder for environment variables</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KludgeKML</reporter><labels><label>adoptme</label><label>bug</label><label>docs</label></labels><created>2014-07-11T09:07:33Z</created><updated>2015-11-21T16:12:31Z</updated><resolved>2015-11-21T16:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T16:12:31Z" id="158659308">Not sure when it changed, but the version without `env.` seems to work correctly. closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>script_field leaks value on failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6827</link><project id="" key="" /><description>**Seen on v1.2.1**

I'm new to MVEL and below is the script I was using to find the lowest value in an array like structure:

``` mvel
set = doc[field].values;
lowest = set[0];

for (candidate : set) {
    lowest = min(lowest, candidate);
}

return lowest;
```

It worked perfectly (or so I thought) always returning the minimum, or a null (if the array was empty or the field was missing).

However I started to notice something a bit strange. While the nulls occurred as expected at the start of the result set, as soon as a real result was found, any subsequent results that should have contained null, now held the last good result, and weirder still, as a string (all good results were ints).

Once I realised what was happening I added a guard clause to my script solving the issue for me.

``` mvel
if (set.size() &lt; 1) {
  return null;
}
```

My assumption is that the out of bounds array access failed silently and somehow the last known good result got leaked into the current hit. In the event that failing silently is intentional behaviour, the value certainly shouldn't leak.
</description><key id="37641450">6827</key><summary>script_field leaks value on failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mal</reporter><labels><label>bug</label><label>feedback_needed</label></labels><created>2014-07-11T08:41:14Z</created><updated>2014-07-14T08:53:36Z</updated><resolved>2014-07-11T16:30:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mal" created="2014-07-11T08:52:02Z" id="48708129">The query in quest was:

``` json
{
  "query": {
    "match_all": {}
  },
  "script_fields": {
    "lowest_seed": {
      "script": "lowest",
      "params": {
        "field": "optional_seeds"
      }
    }
  },
  "size": 50
}
```
</comment><comment author="clintongormley" created="2014-07-11T12:53:32Z" id="48726420">Hi @mal 

Is that the full script you were using? If I try it, I get an assertion error when it encounters a doc with an empty array or missing value:

```
DELETE /_all 

PUT /t
{
  "settings": {
    "number_of_shards": 1
  }
}

PUT /t/t/1
{
  "vals": []
}

PUT /t/t/2
{
  "vals": [0,2,1]
}

PUT /t/t/3
{  "vals": []
}

GET /_search
{
  "sort": "_uid",
  "script_fields": {
    "vals": {
      "lang": "mvel",
      "script": "set = doc['vals'].values; lowest = set[0]; for (candidate : set) { lowest = min(lowest, candidate); } return lowest;"
    }
  }
}
```

Results in:

```
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[7QebqBx6Sbmrh0fu5zPCrQ][t][0]: ElasticsearchException; nested: AssertionError; }]",
   "status": 500
}
```
</comment><comment author="mal" created="2014-07-11T14:39:31Z" id="48738360">Thanks @clintongormley, I tried your example, and frustratingly got the same result. &gt;_&lt;

I kept bashing away and have now created a script that recreates the issue consistently on my local machine, and our staging cluster (both v1.2.1).

[https://gist.github.com/mal/2a36f5caa91e47fbcff7 (v1; integer mapping; see below)](https://gist.github.com/mal/2a36f5caa91e47fbcff7/b122e61b0debc6c420f9498d2a7e4b533076497d)

The final hit of the result for document 5 is:

``` json
{
  "_index":"t",
  "_type":"t",
  "_id":"5",
  "_score":null,
  "fields":{
    "lowest_val": [ 54 ]
  },
  "sort":[ "t#5" ]
}
```

But document 5 has nothing in it's `vals` property.

As an aside the weird string coercion I mentioned appears to come down to the mapping. If you change `vals` to be a `string` then you get `null`s and stringified leakage. The example as provided in the gist maps `vals` as an `integer` for simplicity, so rather than `null`s it defaults to `0` until the leak starts, at which point the leaked values are integers.

I think that means there's two issues here potentially, the leak, and the fallback value. While `null` is a good fallback for strings, `0` is an odd fallback for numbers, as, at least in this case, a valid result of `0` and an invalid result are indistinguishable, using `null`s as the fallback for all types seems more sane.
</comment><comment author="mal" created="2014-07-11T15:39:54Z" id="48746258">Here is the gist with a few more documents, and the string mapping, which shows the issue exactly as I first encountered it: https://gist.github.com/mal/2a36f5caa91e47fbcff7
</comment><comment author="clintongormley" created="2014-07-11T16:30:45Z" id="48752496">Hi @mal 

So I managed to reproduce your findings in v1.2.1.  In the 1.x branch, the same code still throws an assertion error when it hits a null value.  Mvel is deprecated and will be replaced by Groovy, so I tried it with Groovy and it works correctly.

Btw, your gist maps `vals` as a string, but I changed it to use `integer` and tried it again: groovy correctly returns null values instead of zeroes, another point in its favour.

So I think i'm going to close this as a won't fix and just recommend that you use groovy instead.  From 1.3 it'll be in core, but for 1.2.1 you can install it as a plugin: https://github.com/elasticsearch/elasticsearch-lang-groovy/tree/es-1.2

Note: one odd thing I found with the groovy script is that there must be a newline character after the `for` loop, eg when passing it in as a dynamic script, I had to do:

```
POST /t/t/_search
{
  "_source": true,
  "script_fields": {
    "lowest_val": {
      "lang": "groovy",
      "script": "set = doc['vals'].values; lowest = set[0]; for (candidate in set) { if (lowest &gt; candidate) { lowest = candidate}  }\nreturn lowest"
    }
  }
}
```

thanks for reporting
</comment><comment author="mal" created="2014-07-11T23:20:30Z" id="48793773">Fair enough, thanks for the suggestion :+1: 

For the benefit of future readers, "groovy" has a built-in solution for getting the lowest value in a list, which can be used like so:

``` json
{
  "script_fields": {
    "lowest_val": {
      "lang": "groovy",
      "script": "GroovyCollections.min(doc['vals'].values as Integer[])"
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-07-12T10:23:04Z" id="48807787">Ah nice - thanks for pointing that out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Catch the NumberFormatException and handle it in GET</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6826</link><project id="" key="" /><description>If a field is a multi field and requested with a GET request then
this will return the value of the parent field in case the document
is retrieved from the transaction log.
If the type is numeric but the value a string, the numeric value will
be parsed. This caused the NumberFprmatException in case the field
is of type `token_count`.

closes #6676
</description><key id="37640014">6826</key><summary>Catch the NumberFormatException and handle it in GET</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2014-07-11T08:19:20Z</created><updated>2014-07-23T07:58:07Z</updated><resolved>2014-07-23T07:58:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rjernst" created="2014-07-14T18:56:50Z" id="48941562">This seems sneaky to have the output change whether something is in the transaction log or not.  How would a user know what to do if they get back null?
</comment><comment author="brwe" created="2014-07-14T19:56:01Z" id="48951180">Null vales or empty arrays are not returned at all. The field is simply missing from the response in the above example. 
I agree it is odd, but I wonder what the expected behavior here is. The problem always occurs when a field is generated only when indexing. Another example where this happens is the [mapper attachment plugin](https://github.com/elasticsearch/elasticsearch-mapper-attachments). 
If we actually say the expected behavior is that all fields are there also if the document is retrieved from the transaction log then it seems to me we would have to index on the fly in case GET is called before refresh.
</comment><comment author="s1monw" created="2014-07-14T20:23:09Z" id="48954417">I agree with @rjernst  that just catching this and drop it on the floor. IMO we should throw a good exception here by default and maybe have an option to drop it on the floor on the get request. But maybe more important we need to know from the field mapper if the field is created at index time or now. I think that is the more important change having a `FieldMapper#isGenerated()` or something like this
</comment><comment author="nik9000" created="2014-07-14T20:28:55Z" id="48955092">&gt; I think that is the more important change having a FieldMapper#isGenerated() or something like this

That makes sense.  I do think it makes sense to throw back an error if the user explicitly asks for the field and it hasn't yet been synthesized.  I think that is less surprising then just not returning it.
</comment><comment author="brwe" created="2014-07-14T21:17:09Z" id="48961029">ok, meaningful error message and option to switch it off and just drop the field sounds good. 
</comment><comment author="brwe" created="2014-07-23T07:58:07Z" id="49843626">Closing this in favor of #6973
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve handling of failed primary replica handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6825</link><project id="" key="" /><description>Out of #6808, we improved the handling of a primary failing to make sure replicas that are initializing are properly failed as well. After double checking it, it has 2 problems, the first, if the same shard routing is failed again, there is no protection that we don't apply the failure (which we do in failed shard cases), and the other was that we already tried to handle it (wrongly) in the elect primary method.
This change fixes the handling to work correctly in the elect primary method, and adds unit tests to verify the behavior
The change also expose a problem in our handling of replica shards that stay initializing during primary failure and electing another replica shard as primary, where we need to cancel its ongoing recovery to make sure it re-starts from the new elected primary
</description><key id="37633051">6825</key><summary>Improve handling of failed primary replica handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-11T07:12:24Z</created><updated>2015-06-07T12:54:25Z</updated><resolved>2014-07-11T08:52:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-11T08:04:18Z" id="48704415">LGTM
</comment><comment author="martijnvg" created="2014-07-11T08:50:40Z" id="48708023">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changing ES_MAX_MEM default from '1gb' to '1g'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6824</link><project id="" key="" /><description>If you set ES_HEAP_SIZE to '1gb' as suggested, Java will yield an "Invalid initial heap size".
</description><key id="37607441">6824</key><summary>Changing ES_MAX_MEM default from '1gb' to '1g'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">astrostl</reporter><labels><label>docs</label></labels><created>2014-07-10T21:34:44Z</created><updated>2014-07-25T10:51:19Z</updated><resolved>2014-07-25T10:51:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T11:55:44Z" id="48721947">Hi @astrostl 

Thanks for the fix. Please could I ask you to sign our CLA so that I can get your PR merged in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="astrostl" created="2014-07-11T13:46:34Z" id="48731809">Shockingly fast response!  Sure, I signed the individual agreement.
</comment><comment author="clintongormley" created="2014-07-25T10:51:19Z" id="50134428">Merged, thanks :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Command to add apt-key does not work under sudo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6823</link><project id="" key="" /><description>This command, as given on the instructions for using the Elasticsearch repositories (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html), only works as root.

```
wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
```

A user using non-passwordless sudo must modify the flags to wget so as to properly respond to the sudo password prompt, as well as use sudo for the apt-key portion of the command, ala:

```
wget -qO - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add -
```

Since it's generally regarded as bad practice to run as root, I'd argue that the command included in the instructions should be the version that works with sudo. Or at the very least, there should be a note that the command needs to be run as root. The error message encountered when running as a non-root user is difficult to read and unhelpful at best.

```
$ wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
--2014-07-10 20:30:29--  http://packages.elasticsearch.org/GPG-KEY-elasticsearch
Resolving packages.elasticsearch.org (packages.elasticsearch.org)... 54.231.2.113
Connecting to packages.elasticsearch.org (packages.elasticsearch.org)|54.231.2.113|:80... ERROR: This command can only be used by root.
connected.
HTTP request sent, awaiting response... 200 OK
Length: 1768 (1.7K) [binary/octet-stream]
Saving to: &#8216;STDOUT&#8217;

 0% [                                                                                                                                            ] 0           --.-K/s   in 0s


Cannot write to &#8216;-&#8217; (Broken pipe).
```

Happy to submit a pull request if it makes sense.
</description><key id="37602171">6823</key><summary>Command to add apt-key does not work under sudo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">arowla</reporter><labels><label>docs</label><label>feedback_needed</label></labels><created>2014-07-10T20:34:29Z</created><updated>2014-07-23T09:13:25Z</updated><resolved>2014-07-23T09:13:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T11:50:17Z" id="48721569">makes sense to me - a PR would be appreciated. Also, please could you sign our CLA when you submit the PR (http://www.elasticsearch.org/contributor-agreement/)

thanks
</comment><comment author="arowla" created="2014-07-11T19:07:00Z" id="48770347">Ah, didn't know about the CLA. As an employee of the U.S. federal government, all code I write on the job is public domain. If the CLA can be waived under those circumstances, then I can contribute, however, if it cannot be waived, then it may take some time to run the CLA by our legal department. (In other words, it might be better for someone else to do it if you want a quick fix!)
</comment><comment author="clintongormley" created="2014-07-23T09:13:25Z" id="49850218">many thanks @arowla 

fixed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API: Detect noop updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6822</link><project id="" key="" /><description>Some source systems (like mine) don't have an easy way to check if an update is actually a noop.  You could implement this check using update scripts but its a bit jangly to do generically.  It'd be way more convenient if you could have Elasticsearch do the detection for you.  Something like:

``` js
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "doc" : {
        "name" : "new_name"
    },
    "doc_as_upsert" : true,
    "detect_noop": true
}'
```

Detecting the noop rather then just performing the update could prevent extra writes and deleted documents.

Secondarily, it might be cool to be able to specify tolerances for numeric fields so small changes could be ignored but that might be out of scope for any initial implementation.
</description><key id="37599163">6822</key><summary>Update API: Detect noop updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>feature</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T19:59:33Z</created><updated>2014-09-10T19:00:49Z</updated><resolved>2014-07-22T12:56:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-11T14:08:33Z" id="48734353">If this is interesting to folks I can implement it.  It feels like it'd be useful to me and MVEL was not cutting it to do this.
</comment><comment author="nik9000" created="2014-07-14T16:02:32Z" id="48919119">Implementing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add endpoint to test/debug/view a rendered template query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6821</link><project id="" key="" /><description>It would be nice to be able view what the rendered mustache template query looks like after passing in various params.  

An endpoint where you could send a template along with the parameters and it returned the resulting query without actually executing the query would be extremely useful.
</description><key id="37598593">6821</key><summary>Add endpoint to test/debug/view a rendered template query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>:Indexed Scripts/Templates</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T19:53:16Z</created><updated>2015-06-30T15:58:29Z</updated><resolved>2015-06-30T15:58:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T11:47:51Z" id="48721399">++
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Long GC pause</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6820</link><project id="" key="" /><description>Hi,

We are seeing split-brain problem in our Elasticsearch (v1.1.2) cluster where two nodes host hundred of indices. The bigger index has about 30 million documents while others are quite small. It seems that at some point in time Java GC was hogging the CPUs so that both nodes failed to ping each other. The logs are attached below.

We had never had such issue before when we had just a handful of indexes (each of them had more documents). At this point, we have a few theories:
- We can tune node filter cache and field data cache. But our total documents remains about the same as before when we had smaller set of indexes and did not have such problem. So we are not sure if this will help. Is it that each index will incur additional memory usage in addition to cache? If so, how to estimate the usage? We also flush index and clear cache periodically but it does not seem to help.
- Is it that some of our aggregation queries that take up too much memory. Is there way to estimate memory usage of the queries?
- Is it advisable to tune Java GC to be less aggressive?
- Finally, we know this has been asked before. Is it advisable to have hundreds or even thousands of indexes in a cluster?

Thanks for the help.
- Wei Shao

[2014-07-10 08:21:54,020][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221250][80781] duration [25.2s], collections [1]/[2
8.8s], total [25.2s]/[1.2h], memory [3.6gb]-&gt;[3.7gb]/[4.9gb], all_pools {[young] [18.8mb]-&gt;[3.1mb]/[133.1mb]}{[survivor] [16.6mb]-&gt;[
16.6mb]/[16.6mb]}{[old] [3.6gb]-&gt;[3.7gb]/[4.8gb]}
[2014-07-10 08:23:38,768][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221253][80782] duration [1s], collections [1]/[1.3s
], total [1s]/[1.2h], memory [3.8gb]-&gt;[3.7gb]/[4.9gb], all_pools {[young] [124.2mb]-&gt;[265.1kb]/[133.1mb]}{[survivor] [16.6mb]-&gt;[16.6
mb]/[16.6mb]}{[old] [3.7gb]-&gt;[3.7gb]/[4.8gb]}
[2014-07-10 08:23:42,667][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221254][80783] duration [1.9s], collections [1]/[3.
8s], total [1.9s]/[1.2h], memory [3.7gb]-&gt;[3.7gb]/[4.9gb], all_pools {[young] [265.1kb]-&gt;[7.2mb]/[133.1mb]}{[survivor] [16.6mb]-&gt;[16
.6mb]/[16.6mb]}{[old] [3.7gb]-&gt;[3.7gb]/[4.8gb]}
[2014-07-10 08:23:44,052][INFO ][discovery.zen            ] [Zero-G] master_left [[Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elas
tic2][inet[/192.168.0.45:9300]]], reason [do not exists on master, act as master failure]
[2014-07-10 08:23:45,702][INFO ][cluster.service          ] [Zero-G] master {new [Zero-G][ECw8eS5zTMKXSf1bIFOCKA][ha-elastic1][inet[/192.168.0.46:9300]], previous [Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elastic2][inet[/192.168.0.45:9300]]}, removed {[Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elastic2][inet[/192.168.0.45:9300]],}, reason: zen-disco-master_failed ([Mary Jane Parker][F4QVviuGQ_Sg6h9NBlVVsQ][ha-elastic2][inet[/192.168.0.45:9300]])
[2014-07-10 08:23:47,587][WARN ][monitor.jvm              ] [Zero-G] [gc][young][221258][80786] duration [1s], collections [1]/[1.6s], total [1s]/[1.2h], memory [3.7gb]-&gt;[3.6gb]/[4.9gb], all_pools {[young] [7.2mb]-&gt;[7.1mb]/[133.1mb]}{[survivor] [7.9mb]-&gt;[8mb]/[16.6mb]}{[old] [3.7gb]-&gt;[3.6gb]/[4.8gb]}
</description><key id="37598320">6820</key><summary>Long GC pause</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaoweite</reporter><labels /><created>2014-07-10T19:50:18Z</created><updated>2014-07-11T11:47:13Z</updated><resolved>2014-07-11T11:47:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T11:47:13Z" id="48721359">Hi @shaoweite 

By the looks of it you are suffering memory pressure. You need to increase your heap size (remembering that you should keep at least 50% of your available RAM for file system caches), increase the number of nodes you have or decrease your memory usage.

Having hundreds of indices on just two boxes uses a lot of resources, especially if you have just used the defaults of 5 primaries per index and most of your shards are pretty empty. Try creating indices with 1 primary shard instead.

Also, fielddata is a big user of memory, especially when applied to high cardinality string fields.  Clearing the caches won't help here as the fielddata just gets reloaded the next time you run a query which needs it.

Essentially, it's time to scale... either vertically (more RAM) or horizontally (more nodes).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add script engine for Lucene expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6819</link><project id="" key="" /><description>Adds support for new "expression" script engine.

This is backed by lucene expressions.  These are javascript expressions, which can only access numeric fielddata, parameters, and _score.  They can only be used for searches (not document updates).

closes #6818
</description><key id="37593381">6819</key><summary>Add script engine for Lucene expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels><label>:Scripting</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T18:56:45Z</created><updated>2015-06-06T18:30:37Z</updated><resolved>2014-07-15T14:59:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-10T19:13:02Z" id="48650339">Hi ryan, I left an initial comment. Is there anything we can do to remove the expensive per-document stuff? I realize these might be limitations of the scripting API, but maybe we can fix that? Otherwise, if we are doing things like hashing strings etc per-document to resolve variable names, its going to be very slow.
</comment><comment author="s1monw" created="2014-07-10T20:09:42Z" id="48657304">I left some initial  comments.. I will remove the review label for now feel free to put it back once you have new stuff to review
</comment><comment author="rjernst" created="2014-07-10T21:25:24Z" id="48666545">@rmuir @s1monw I think I addressed all of your comments.
</comment><comment author="dakrone" created="2014-07-11T10:02:59Z" id="48713842">This looks really good so far! I left some more comments. Can you add documentation for exrpessions to `docs/reference/modules/scripting.asciidoc`?

Also, I'm able to reproduce a failure in the tests with the following seed/cmd:

```
mvn -Dtests.jvms=2 -Dtests.seed=AC6C19E56409AB8D:3CB2F7F3B87BB7D9 -Dtests.class=org.elasticsearch.script.expression.ExpressionScriptTests clean compile test
```

```
FAILURE 8.72s | ExpressionScriptTests.testStringSpecialValueVariable &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError: Unknownshould have contained ExpressionScriptExecutiontException
   &gt; Expected: &lt;true&gt;
   &gt;      got: &lt;false&gt;
   &gt;
   &gt;    at __randomizedtesting.SeedInfo.seed([AC6C19E56409AB8D:3CB2F7F3B87BB7D9]:0)
   &gt;    at org.junit.Assert.assertThat(Assert.java:780)
   &gt;    at org.elasticsearch.script.expression.ExpressionScriptTests.testStringSpecialValueVariable(ExpressionScriptTests.java:265)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
```
</comment><comment author="rjernst" created="2014-07-11T15:56:07Z" id="48748367">The failure was due to the test trying to match against the original shard failure, which was null.  See #6837.
</comment><comment author="rjernst" created="2014-07-12T00:19:46Z" id="48796559">Ok, I have some benchmark numbers.  The following tests were done with 1 million random docs, and 1k iterations of a `match_all` query using a script for sorting.  The script is described in each column and each cell is the average time per request.

| lang | x | x + y | 1.2 \* x / y | sqrt(abs(z)) + ln(abs(x \* y)) |
| --- | --: | --: | --: | --: |
| expression | 5 ms | 14 ms | 14 ms | 26 ms |
| native | 14 ms | 22 ms | 23 ms | 38 ms |
| groovy | 37 ms | 71 ms | 74 ms | 116 ms |
</comment><comment author="clintongormley" created="2014-07-12T10:25:16Z" id="48807818">awesome numbers!  expressions will be a very welcome addition to scripting
</comment><comment author="s1monw" created="2014-07-14T19:27:34Z" id="48947842">I left some cosmetic comments. LGTM
</comment><comment author="rjernst" created="2014-07-14T21:10:20Z" id="48960168">Ok, I think I addressed all your issues.  I will fix setScorer in another PR.
</comment><comment author="rjernst" created="2014-07-14T22:22:12Z" id="48967978">I implemented a partial fix for setScorer in #6864.
</comment><comment author="s1monw" created="2014-07-15T11:22:21Z" id="49018887">LGTM I think we should get this into `1.3` I will tag it as that so feel free to push it there
</comment><comment author="uschindler" created="2014-07-20T12:01:55Z" id="49544620">Maybe open another issue about this:
The Expressions module is safe to use for anybody, because you cannot invoke any random java functions (just what is provided via the bindings and the Map of Double-Methods.
Currently you can only disable scripting completely in ES (if you want to make search APIs public to internet via reverse proxy). It would be cool to make the script engines that are available via REST configureable.
Should I open an issue or is this already solved here (I have not yet looked closely into this patch).
</comment><comment author="clintongormley" created="2014-07-20T12:13:27Z" id="49544821">@uschindler from 1.3, sandboxed languages (ie groovy and expressions) are enabled by default: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_enabling_dynamic_scripting
</comment><comment author="uschindler" created="2014-07-20T12:15:03Z" id="49544852">Thanks @clintongormley !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Add script engine for lucene expressions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6818</link><project id="" key="" /><description>These are compiled to byte code and meant for fast manipulation of fielddata/docvalues.

See https://issues.apache.org/jira/browse/LUCENE-5207

There are a couple of limitations:
- They only work with numeric fields.
- No loops, variables, etc. Just a single expression/statement.
</description><key id="37589723">6818</key><summary>Scripting: Add script engine for lucene expressions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rjernst</reporter><labels /><created>2014-07-10T18:17:35Z</created><updated>2014-08-08T17:06:32Z</updated><resolved>2014-07-15T14:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>bin/plugin removes itself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6817</link><project id="" key="" /><description>If you call `bin/plugin --remove es-plugin` the plugin got removed but the file `bin/plugin` itself was also deleted.

We now don't allow the following plugin names:
- elasticsearch
- elasticsearch-service-x86.exe
- plugin
- elasticsearch-service-mgr.exe
- elasticsearch.bat
- plugin.bat
- elasticsearch-service-x64.exe
- elasticsearch.in.sh
- service.bat

Closes #6745
</description><key id="37586279">6817</key><summary>bin/plugin removes itself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-07-10T17:40:01Z</created><updated>2014-08-21T13:55:30Z</updated><resolved>2014-07-17T06:59:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-07-11T01:17:42Z" id="48684508">@dadoonet Minor suggestion. LGTM.
</comment><comment author="s1monw" created="2014-07-15T10:39:44Z" id="49015343">I left some comments
</comment><comment author="dadoonet" created="2014-07-15T15:42:45Z" id="49051116">Thanks! PR Updated. Let me know if I push it and in which branch.
</comment><comment author="s1monw" created="2014-07-16T07:41:46Z" id="49133330">LGTM @dadoonet can you push this to `1.2` &amp; `1.3` as well as `1.x` &amp; `master` please
</comment><comment author="dadoonet" created="2014-07-17T06:59:37Z" id="49265542">Merged in `1.2`, `1.3`, `1.x` and `master`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve handling of failed primary replica handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6816</link><project id="" key="" /><description>Out of #6808, we improved the handling of a primary failing to make sure replicas that are initializing are properly failed as well. After double checking it, it has 2 problems, the first, if the same shard routing is failed again, there is no protection that we don't apply the failure (which we do in failed shard cases), and the other was that we already tried to handle it (wrongly) in the elect primary method.
This change fixes the handling to work correctly in the elect primary method, and adds unit tests to verify the behavior
</description><key id="37577412">6816</key><summary>Improve handling of failed primary replica handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Allocation</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T16:03:07Z</created><updated>2015-06-07T19:29:54Z</updated><resolved>2014-07-10T16:30:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-10T16:12:53Z" id="48626795">left some cosmetic comments. This looks much better testing wise - I love the unit test :)
</comment><comment author="s1monw" created="2014-07-10T16:28:12Z" id="48628840">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New clustering aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6815</link><project id="" key="" /><description>Bucket documents if they contain at least `n` values in common, such that given four documents:

``` json
{"a": [4,5,34,712]}
{"a": [3,4]}
{"a": [100, 200, 300]}
{"a": [712]}
```

A req/res pair in the case of `n=1` (defined by setting `overlap` in this example) would look something like (not suggested output format, for illustrative purposes only):

Request:

```
{
    "aggs" : {
        "my_clusters" : {
            "cluster" : { "field" : "a", "overlap": 1 }
        }
    }
}
```

Response:

```
{
    ...

    "aggregations" : {
        "my_clusters" : {
            "buckets" : [
                {
                    "key" : [3, 4, 5, 34, 712],
                    "doc_count" : 3
                },
                {
                    "key" : [100, 200, 300],
                    "doc_count" : 1
                },
            ]
        }
    }
}
```

While this would be useful on it's own, using it together with the forthcoming top hits aggregation, would allow the selection of fields from the highest scoring document from each cluster, which would be amazingly useful.
</description><key id="37573064">6815</key><summary>New clustering aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mal</reporter><labels><label>discuss</label></labels><created>2014-07-10T15:20:14Z</created><updated>2014-10-31T09:56:44Z</updated><resolved>2014-10-31T09:56:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-10T16:16:01Z" id="48627256">Hi @mal 

I'm not really following the use case here. Could you expand on what you're trying to achieve?
</comment><comment author="mal" created="2014-07-11T08:23:26Z" id="48705885">We have documents that store references to each other, each document can reference multiple other documents, or none at all. Each set of inter-referential documents are discrete, forming thousands of isolated document clusters. The references are stored in an array like field.

The use case is to be able to aggregate on these clusters, to be able, for example:
- to count the number of clusters
- to find the biggest cluster
- to be able to get a single document that best represents a cluster (i.e. top_hit after sorting a cluster)
- etc

Hope that makes more sense!

P.S. To clarify; in our case we'd be using `overlap=1` because one term in common between documents, would be enough to cluster the way we want, the idea to allow varying it was to hopefully make it more useful for others that may wish to use it.
</comment><comment author="clintongormley" created="2014-07-11T12:40:34Z" id="48725339">@markharwood any input here?
</comment><comment author="mal" created="2014-09-04T13:43:42Z" id="54478531">@clintongormley @markharwood I've updated the original issue example to hopefully better show the desired functionality, be interested in your thoughts on this
</comment><comment author="markharwood" created="2014-09-04T15:14:50Z" id="54492431">What you are talking about is essentially a graph. If you don't have too many terms (and that may be a big "if"...) then you can use 2 terms aggs to create a graph e.g. 
    {
      "aggs": {
        "src":{
          "terms": {
            "field": "a",
            "size": 10000
          },
          "aggs": {
            "target": {
              "terms": {
                "field": "a",
                "size": 10000,
                "min_doc_count": 2
              }
            }
          }
        }
      }
    }

Each "leaf" bucket created is essentially defining an "edge" between pairs of nodes with a count of the number of docs. The "min_doc_count" is used to trim doc_counts of 1 which are essentially a self-reference of numbers in a single doc. You can create a graph from this data an run centrality measures etc.
</comment><comment author="clintongormley" created="2014-10-31T09:56:44Z" id="61239306">@mal I hope mark's demo was helpful.  There are certainly more things that we can do to provide graph-like functionality, but we'll need to explore and scope them out.  I'm going to close this ticket for now. 

thanks for the suggestion
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Fixed DateHistogramBuilder to accept preOffset and postOff...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6814</link><project id="" key="" /><description>...set as Strings

This is what DateHistogramParser expects so will enable the builder to build valid requests using these variables.
Also added tests for preOffset and postOffset since these tests did not exist

Closes #5586
</description><key id="37572185">6814</key><summary>Aggregations: Fixed DateHistogramBuilder to accept preOffset and postOff...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-07-10T15:12:31Z</created><updated>2014-08-21T15:07:40Z</updated><resolved>2014-07-11T08:26:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-10T15:23:41Z" id="48619681">LGTM
</comment><comment author="colings86" created="2014-07-11T08:26:06Z" id="48706068">Closing as this is merged but did not autoclose
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not ignore ConnectTransportException for shard replication operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6813</link><project id="" key="" /><description>A ConnectTransportException on replicating an index operation should fail the replica shard.

This case came out of the `improve_zen` branch for index requests that are mid-flight while partitioning is forced.
</description><key id="37562269">6813</key><summary>Do not ignore ConnectTransportException for shard replication operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T13:31:18Z</created><updated>2015-06-07T19:30:12Z</updated><resolved>2014-07-10T17:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-10T13:51:44Z" id="48606666">LGTM, left two minor comments
</comment><comment author="kimchy" created="2014-07-10T15:57:06Z" id="48624560">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show proper date format in response on max aggregation over timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6812</link><project id="" key="" /><description>If you currently do a [max aggregation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-metrics-max-aggregation.html) on a date field, elasticsearch prints the output as a unix timestamp including milliseconds.

**Example request:**

``` json
{
...
"aggregations": {
    "last_hit": {
      "max": {
        "field": "@timestamp"
      }
    }
  }
...
}
```

**Example current response:**

``` json
{
...
 "aggregations": {
      "last_hit": {
         "value": 1404196200000
      }
   }
...
}
```

It would be more preferable if the output would be given in the following format:

**Improved response:**

``` json
{
...
 "aggregations": {
      "last_hit": {
         "value": "01.07.2014T06:30:00.000Z"
      }
   }
...
}
```
</description><key id="37558125">6812</key><summary>Show proper date format in response on max aggregation over timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">stha</reporter><labels><label>:Aggregations</label></labels><created>2014-07-10T12:41:11Z</created><updated>2015-01-09T16:22:16Z</updated><resolved>2015-01-09T16:22:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="antong" created="2014-09-10T19:11:04Z" id="55167041">I also got hit by this. Of course it feels like an unnecessary chore to decode what is left after max("2014-09-10T19:04:19.725694", "2014-09-10T11:12:13.432432") -&gt; 1410375855.35215. However, the _source of the compared docs may have many different formats for the time stamp. So what should the result format be for max("2014-09-10T19:04:19.725694", 1410375055.1234, ...)?

If a string format is implemented, IMHO it must be ISO format without time zone and nothing else.

UPDATE/EDIT: Oh, with _including milliseconds_ it means that the value is like UTC-milliseconds since UNIX epoch. Which really is surprising for the user. I first thought it really was a UNIX timestamp.
</comment><comment author="bly2k" created="2014-09-30T15:29:19Z" id="57332124">Perhaps adding a format specifier just like in the date_histogram agg? I do agree this is a nice and useful feature to have.

```
{
"aggregations": {
    "last_hit": {
      "max": {
        "field": "@timestamp",
        "format": "YYYY-MM-dd"
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add local node to cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6811</link><project id="" key="" /><description>Today, the tribe node needs the local node so it adds it when it starts, but other APIs would benefit from adding the local node, also, adding the local node should be done in a cleaner manner, where it belongs, which is right after the discovery service starts in the cluster service
</description><key id="37554409">6811</key><summary>Add local node to cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T11:40:48Z</created><updated>2015-06-07T12:54:39Z</updated><resolved>2014-07-10T12:50:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-10T12:45:04Z" id="48599254">Left two super minor comments around naming, LGTM otherwise
</comment><comment author="kimchy" created="2014-07-10T12:50:27Z" id="48599722">@javanna cool, renamed and pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlingting Fragment Option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6810</link><project id="" key="" /><description>Hi ,

As of now , we are using simple fragment - is it possible shall we use custom fragmeter ? If possible please let me know the procedure .

"highlight": {
        "order": "score",
        "fields": {
            "content": {  
                "fragment" : "simple"
            }
        }
}

Reason : 
Code snippet is not clear . 
we needn code fragment as number of lines instead of number of characters . 

Thanks,
Siva
</description><key id="37554242">6810</key><summary>Highlingting Fragment Option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">85siva</reporter><labels><label>:Highlighting</label></labels><created>2014-07-10T11:38:17Z</created><updated>2017-04-19T13:35:14Z</updated><resolved>2017-04-19T13:35:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-10T14:12:26Z" id="48609429">The fragmenter is pluggable at a low level but Elasticsearch doesn't expose a mechanism to do so via extensions.  Also I believe it is only pluggable for the "plain" highlighter.  I think you have two options:
1.  Submit a patch to Elasticsearch that adds a your fragmenter.
2.  Submit a patch to Elasticsearch that allows you to plug new fragmenter into the plain highlighter and then implement your fragmenter as a plugin.
3.  Submit a patch to the [experimental highlighter plugin](https://github.com/wikimedia/search-highlighter) that implements your fragmenter there.

I'm partial to number 3 because I maintain that plugin.  Despite the name its really quite stable.  We served eight and a half million searches with it yesterday.  I chose the name to reflect that I'd release it more frequently the Elasticsearch or Lucene.
</comment><comment author="aanchal9" created="2017-04-19T13:24:06Z" id="295268780">Is it still an issue?</comment><comment author="jimczi" created="2017-04-19T13:35:14Z" id="295272302">We're moving away from the `plain` highlighter and plan to deprecate it in favor of the `unified` highlighter. Therefore I am closing this issue due to lack of activity, please open a new issue targeting the `unified` highlighter if you're still after this feature. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Added ability to provide settings for external nodes in backwards compatibility tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6809</link><project id="" key="" /><description /><key id="37553743">6809</key><summary>Test: Added ability to provide settings for external nodes in backwards compatibility tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T11:29:31Z</created><updated>2014-07-16T11:14:09Z</updated><resolved>2014-07-10T14:45:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-10T13:39:36Z" id="48605078">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Recovering replicas might get stuck in initializing state </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6808</link><project id="" key="" /><description>If a primary fails while a replica starts recovery but has not yet initialized the recovery process the replica will retry until the primary is allocated again on the node. This never happens and the replica gets stuck in INITIALIZING state and will never cleaned up.
</description><key id="37550541">6808</key><summary>Resiliency: Recovering replicas might get stuck in initializing state </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-10T10:35:45Z</created><updated>2015-01-13T20:22:47Z</updated><resolved>2014-07-10T13:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OlegYch" created="2014-08-22T22:46:28Z" id="53130828">was this fixed?
i think i just experienced this after rolling upgrade from 1.3.1 to 1.3.2
i disabled cluster.routing.allocation before upgraded then installed new version on each node and set cluster.routing.allocation=all
then i waited several hours for cluster to become green (with dogslow response times in the meantime)
then i restarted each node one by one again
then waited a bit more and noticed there were 4 primary shards stuck in initializing on one of them and killed it - and the cluster went back up green in no time
there were errors like this in the failed node log:

```
[2014-08-22 22:01:45,221][DEBUG][action.search.type       ] [thisnode] [myidx][1], node[oDw5wWJ-S6etTr0cGLNbGw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@57228563] lastShard [true]
org.elasticsearch.transport.SendRequestTransportException: [anothernode][inet[/10.35.62.130:9300]][search/phase/query]
Caused by: org.elasticsearch.transport.NodeNotConnectedException: [anothernode][inet[/10.35.62.130:9300]] Node not connected
        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:874)
        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:556)
        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:206)
        ... 40 more
```
</comment><comment author="OlegYch" created="2014-08-23T00:49:26Z" id="53137876">welp there are some shards stuck initializing on that node again (from newly created indexes)
the only exceptions in log are

```
[2014-08-23 00:02:12,622][ERROR][marvel.agent.exporter    ] [thisnode] remote target didn't respond with 200 OK response code [404 Not Found]. content: [:)
^E&#65533;errorrIndexMissingException[[.marvel-2014.08.23] missing]&#65533;status$^L&#65533;&#65533;]
```

note the garbled strings
and on other nodes only stuff like

```
[2014-08-23 00:38:31,588][DEBUG][action.bulk              ] [othernode] observer: timeout notification from cluster service. timeout setting [1m], time since start [1m]
[2014-08-23 00:38:31,589][ERROR][marvel.agent.exporter    ] [othernode] create failure (index:[.marvel-2014.08.23] type: [node_stats]): UnavailableShardsException[[.marvel-2014.08.23][0] [2] shardIt, [0] active : Timeout waiting for [1m], request: org.elasticsearch.action.bulk.BulkShardRequest@4f0b6ef7]
```
</comment><comment author="clintongormley" created="2014-08-24T10:22:32Z" id="53187062">/cc @bleskes @s1monw 
</comment><comment author="s1monw" created="2014-08-25T09:19:07Z" id="53243619">hey @OlegYch I personally can't see really evidence that your issue is related to this. Did you really see a shard initialising that was supposed to recover from a shard that is not actually allocated. You also said:

&gt; then waited a bit more and noticed there were 4 primary shards stuck in initializing on one of them and killed it - and the cluster went back up green in no time

and in this issue the shards that got stuck were replicas in this issue. Can you provide more infos that what you already added?
</comment><comment author="OlegYch" created="2014-08-25T13:55:10Z" id="53266286">oh sorry, didn't understand that this issue was specifically about replica shards as opposed to primary shards, perhaps this is better described in #6816 ?
i've removed that node from the cluster and stopped elastic, so i can upload files from it if that would help diagnosing
</comment><comment author="garyelephant" created="2015-01-07T13:10:33Z" id="69019401">is this problem resolved after 1.4.0 ? Mine 1.4.0 still has this problem.

```
curl es_host:9200/_cat/shards 2&gt;1 |grep [UI]N
test-2015.01.07      4 p INITIALIZING                   127.0.0.1 10.71.16.121 
test-2015.01.07      4 r UNASSIGNED                                            
test-2015.01.07      4 r UNASSIGNED 
```
</comment><comment author="ioc32" created="2015-01-09T19:26:43Z" id="69385080">@garyelephant I just run into this same issue after resetting the number of replicas using the API. 1.4.0, too.

FWIW here's the status of the shards comprising one of the stuck indices:

index-2015.01.06      0 r INITIALIZING                  10.0.0.162 es2.io.example.com
index-2015.01.06      0 r UNASSIGNED
index-2015.01.06      1 r UNASSIGNED
index-2015.01.06      1 r UNASSIGNED
index-2015.01.06      2 r UNASSIGNED
index-2015.01.06      2 r UNASSIGNED
</comment><comment author="clintongormley" created="2015-01-13T20:22:47Z" id="69812782">@garyelephant @ioc32 this issue is closed. If you're still seeing problems in 1.4, please open a new issue with more information about the problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update plugins.asciidoc to fix typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6807</link><project id="" key="" /><description>Changed the name of the European Environment Agency (from European Environmental Agency)
</description><key id="37544590">6807</key><summary>Update plugins.asciidoc to fix typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">iulia-pasov</reporter><labels /><created>2014-07-10T09:11:28Z</created><updated>2014-07-10T12:04:41Z</updated><resolved>2014-07-10T12:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolate performance difference in 1.0.0 and 1.2.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6806</link><project id="" key="" /><description>Hi,

I was doing some test with percolate and I noticed huge difference in performance of percolate query between 1.0.0 and 1.2.2. In 1.0.0 indexing of percolate queries takes much longer than in 1.2.2 (around 10 times), but percolate query is much faster. With 400k queries I get tens of ms in 1.0.0 to percolate document and couple of seconds in 1.2.2. I performed all tests with default settings always on clean ES instance. Percolate document looks like:

```
{
"query": {
   "filtered": {
      "filter": {   
         "and": [      
            {
               "geo_distance": {
                  "distance": "94km",
                  "location": {
                     "lat": "97",
                     "lon": "-76"
                  }       
               }       
            }       
         ]       
      },
      "query": {
         "match": {
            "_all": "note father surprise"
         }       
      }       
   }       
},
"type": "offer"
}
```

and percolate query:

```
GET /test-index/offer/_percolate
{
    "doc": {
        "name": "note",
        "location": [-7,-80]
    }
}
```

Has something changes with default setting of ES between these versions regarding percolate? I couldn't find anything in release notes.
</description><key id="37518521">6806</key><summary>Percolate performance difference in 1.0.0 and 1.2.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">quhar</reporter><labels><label>regression</label></labels><created>2014-07-09T23:21:13Z</created><updated>2015-02-17T05:53:03Z</updated><resolved>2014-08-04T08:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-10T09:56:44Z" id="48585726">I can't reproduce the slowdown that you're reporting. I benchmarked with different types of queries (term, range and geo_distance) between 1.0 and 1.2.2. In fact on 1.2.2 I get a slightly better performance. Can you share a more detailed reproduction of the big performance difference that you're experiencing?

The indexing slowdown in 1.0.0 was due to [1] and has been resolved in 1.0.2.
1: https://github.com/elasticsearch/elasticsearch/issues/5339
</comment><comment author="quhar" created="2014-07-10T22:11:33Z" id="48671446">I've performed additional tests. I've used 1.0.2 (as inserting percolate queries was very slow) and 1.2.2. Difference is much smaller (previously i had difference in nr of queries), but still 1.2.2 is 3 times slower.
[Test script](https://github.com/quhar/misc_scripts/blob/master/test_es_percolate.py)
I did two tests, one with 200k queries, second one with 300k. In both cases for test I percolated same document 100 times and computed average. [Here](https://gist.github.com/quhar/40283786d103c4ec0139) are detailed results.

With 1.0.2 I got average 198 ms per percolate with 200k queries and 312 ms with 300k queries.
With 1.2.2 values are 667 and 1008 respectively.

Edit:
In both cases it's standard ES from deb without any config or runtime changes.
</comment><comment author="martijnvg" created="2014-07-11T14:22:54Z" id="48736109">@quhar I can confirm the performance regression:

```
1.0.3:
No handlers could be found for logger "elasticsearch"
Indexed 300000 queries into precolator, took 234.444020987 seconds
After 100 tests, average query time: 288.63

1.2.2:
No handlers could be found for logger "elasticsearch"
Indexed 300000 queries into precolator, took 250.046157837 seconds
After 100 tests, average query time: 846.89
```

The following change introduced in 1.2.2 is causing this slowdown:
https://github.com/elasticsearch/elasticsearch/pull/6578

On 1.2.1 there is similar performance as is on 1.0.3:

```
1.2.1:
No handlers could be found for logger "elasticsearch"
Indexed 300000 queries into precolator, took 217.544229984 seconds
After 100 tests, average query time: 270.79
```

This change disabled caching for both filter cache and field data at all times in the percolator. In general this is a good improvement, since the caching for an in memory index make no sense. However the disabling of the field data caching has a drawback in that even constructing data structures for the percolator in memory index is relatively expensive, and that is what you're noticing if you percolating across 300k docs with a geo_distance filter that relies on field data.

So the percolator is creating the same data structure 300k times, which is causing the performance regression. Instead the percolator should **temporarily** cache field data during the execution of a percolator request and that should fix the performance regression.
</comment><comment author="martijnvg" created="2014-07-13T23:10:06Z" id="48854635">The performance regression only occurs if percolator queries contain filters that rely on field data. (e.g. `geo_distance` filter). The best way to get around the field data loading overhead costs is to enable doc values for geo fields.

@quhar If you set `doc_values` to `true` on the `location` field the performance is similar to `1.0.3`:

``` python
mappings = {
    'mappings': {
        TYPE: {
            'properties': {
                'name': {
                    u'type': u'string',
                },
                'location': {
                    u'type': u'geo_point',
                    u'doc_values': u'true',
                }
            }
        }
    }
}
```

By enabling doc_values the cost of building field data is done only once instead of 300k times. I think this the best way to get around this performance regression.
</comment><comment author="quhar" created="2014-07-14T08:13:17Z" id="48874748">Thanks. I will take a look at `doc_value` then
</comment><comment author="dennisgorelik" created="2015-02-16T18:19:15Z" id="74551372">We found that when we use doc_values, percolator finds about 30% less matches (queries).
Percolator (against index with doc_values setting on Location property) does not return matches that it clearly should have find (query is searching for keyword that is present in percolated item; percolated item is within location range specified by the query).

Execution time was smaller by about 30% too.

We do batch percolation.
We percolate 50 items per batch.
It took:
28.225s to percolate 50 items without "doc_values" option (found 14787 matching queries).
19.855s to percolate 50 items with "doc_values" option (found 10781 matching queries).

We made this change to our index:
Replaced:

```
""Location"": {
    ""type"": ""geo_point""
}
```

with:

```
""Location"": {
    ""type"": ""geo_point"",
    ""doc_values"": ""true""
}
```

Number of queries to percolate against: 154,195
Version: ElasticSearch 1.2.2
</comment><comment author="martijnvg" created="2015-02-16T21:42:33Z" id="74576844">@dennisgorelik ouch, does this also occur with a more recent version of ES? (1.3.8 / 1.4.3) If so can you open a new issue for this? and if possible share a smaller reproduction of the bug?
</comment><comment author="dennisgorelik" created="2015-02-16T23:02:12Z" id="74586061">Martin,
We are going to install and test doc_values effect on ElasticSearch 1.4.3 (probably in a day or two).
If we reproduce the same percolation mismatch issue on a smaller set or data we will file a separate issue.
</comment><comment author="dennisgorelik" created="2015-02-17T05:42:37Z" id="74619767">Martin,
We were able to reproduce that doc_value mismatch.
I created separate issue for that:
https://github.com/elasticsearch/elasticsearch/issues/9714
</comment><comment author="dennisgorelik" created="2015-02-17T05:53:03Z" id="74620450">We tried performance of the same percolate batch (of 50 items) on ElasticSearch 1.4.3.
The performance improved by ~25%
28.225s on ElasticSearch 1.2.2
21.270s on ElasticSearch 1.4.3

Using doc_value=true on ElasticSearch 1.4.3 did not improve performance (but caused percolator to miss ~30% of queries that it should have found - the same issue as in ElasticSearch 1.2.2).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enables plugins to define default logging configuration for their needs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6805</link><project id="" key="" /><description>- adds support for multiple logging configurations under the config dir (will pick up any logging.xxx in the config folder tree)
- plugins can now define a top level config directory that will be copied under es config dir and will be renamed after the plugin name (same as the support we have the plugin "bin" dirs)
  
  Closes #6802
</description><key id="37497093">6805</key><summary>Enables plugins to define default logging configuration for their needs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>:Plugins</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-09T18:55:44Z</created><updated>2015-06-07T12:54:50Z</updated><resolved>2014-07-10T11:35:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-09T19:06:35Z" id="48519920">LGTM
</comment><comment author="uboness" created="2014-07-10T00:35:13Z" id="48552792">closed 610900b
</comment><comment author="clintongormley" created="2014-07-10T11:35:04Z" id="48593521">Closed by https://github.com/elasticsearch/elasticsearch/commit/610900b7812639dad56f2e8ed0897c16843889a8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add an option to create "other" bucket for Terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6804</link><project id="" key="" /><description>When using "terms" aggregation, it's often useful to get top X terms (achieved by using `size` parameter), but as well get a separate bucket for all other terms together (possibly constrained by minimum doc count). 

The query syntax might be:

```
{
    "aggs" : {
        "tags" : {
            "terms" : { 
                "field" : "tag",
                "size": 3,
                "min_doc_count": 10,
                "other": "_other_terms"
             }
        }
    }
}
```

And the response might look like:

```
{
    ...
    "aggregations" : {
        "tags" : {
            "buckets" : [
                {
                    "key" : "soccer",
                    "doc_count" : 500
                },
                {
                    "key" : "hockey",
                    "doc_count" : 400
                },
                {
                    "key" : "basketball",
                    "doc_count" : 300
                },
                {
                    "key" : "_other_terms",
                    "doc_count" : 150
                },
            ]
        }
    }
}
```

The `_other_terms` bucket will be based on all tags with doc_count &gt; 10 per tag, excluding already listed (top 3).

Related to #5324
</description><key id="37493565">6804</key><summary>Add an option to create "other" bucket for Terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kostiklv</reporter><labels><label>discuss</label></labels><created>2014-07-09T18:19:29Z</created><updated>2014-08-18T14:45:29Z</updated><resolved>2014-07-24T22:00:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-18T11:15:45Z" id="49420138">One question that is related to that change is whether `other` should only track doc counts (cheap, and could be done by default) or also sub aggregations (potentially costly, so would require an option).
</comment><comment author="clintongormley" created="2014-07-18T11:28:18Z" id="49420962">I'd say just the doc counts, at least by default.
</comment><comment author="kostiklv" created="2014-07-22T18:26:59Z" id="49780307">The suggested syntax is already an option, so the developer using this option should understand the cost. Based on that, I think the default should include all sub aggregations.
Consider the following query:

```
"aggs": {
    "top_selling": {
       "terms": {
          "field": "make",
          "size": 5,
          "other": "_other_terms"
       },
       "aggs": {
          "avg_price": {
             "avg": { "field": "price" }
          }
       }
    }
 }
```

What's the point of using `_other_terms` if we don't get the average price for them? I also doubt if the option to disable sub aggregations is needed at all. What's the use case when you want sub aggregations on specific terms, but don't want it for `other`?

Anyway, the syntax can be future-proof, so instead of `"other": "_other_terms"` it can be:

```
...
"other": {
   "bucket_key": "_other_terms",
   "some_future_option": "option_value"
}
...
```

It may also check if the value of `other` option is a string, and use it as `other.bucket_key` by default as syntactic sugar.
</comment><comment author="jpountz" created="2014-07-24T22:00:57Z" id="50084312">I have thought more about this issue and computing the document count for other buckets is not possible in the general case without doing another pass over the data (think about multi-valued fields).

The only thing that it can do would be to return the number of other values (as opposed to documents). But we already have the `value_count` aggregation for that.

If a bucket or count for other docs is really needed, the right way to build it would be to run a first query with the terms aggregation, and a second query that would have a filter aggregation that would exclude the returned terms.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add basic authentication authentication for plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6803</link><project id="" key="" /><description>The plugin downloader which is invoked by the plugin command line does not support downloading the plugin from a URL which is secured using basic auth.
## Use Case

In the continuous integration process if we want to install a plugin where the plugin artifact is available on the CI server which is restricted with basic auth this is not possible.
## Workaround

Download using wget passing auth details and use the elasticsearch plugin command line passing the _file_ url instead of _http_ url
## Good to have

Plugin command line to have options to take in basic auth credentials and pass it on to the downloader.

Thanks to @katta for the original PR!

Closes #2550.
</description><key id="37491394">6803</key><summary>Add basic authentication authentication for plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>feature</label></labels><created>2014-07-09T17:55:28Z</created><updated>2015-08-18T11:27:03Z</updated><resolved>2015-06-05T12:17:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T19:11:53Z" id="48520591">left some comments
</comment><comment author="dadoonet" created="2014-07-10T17:11:26Z" id="48634365">@s1monw PR updated. Thanks!
</comment><comment author="pickypg" created="2014-07-11T01:36:20Z" id="48685523">@dadoonet Cool PR. Minor comments.
</comment><comment author="dadoonet" created="2014-07-11T09:32:13Z" id="48711341">Thanks @pickypg. I updated the PR. I think it looks better now.
</comment><comment author="pickypg" created="2014-07-11T21:53:09Z" id="48787328">@dadoonet LGTM. I left some really minor stuff that I think is more style-related than substance.
</comment><comment author="s1monw" created="2014-07-15T12:46:28Z" id="49026057">added more comments.
</comment><comment author="dadoonet" created="2014-07-15T17:20:48Z" id="49064476">The way I see `Credentials` class is that it's more an utility class which add `username/password` to a URLconnection if needed than a POJO.

I agree that the naming for that class is not appropriate.

May be something like `Authenticator` could be a better naming? With a `NoAuthenticator` and `BasicAuthenticator` as implementations. 

I'm not sure we really need a `Credentials` class to hold username and password.

That said, we could also think of simplifying all that as we use it only in the plugin manager and just have the `username` and `password` going from `PluginManager#main()` to `HttpDownloadHelper#openConnection(URL aSource, String username, String password)`.

What do you think?
</comment><comment author="pickypg" created="2014-08-30T22:49:02Z" id="53972836">I prefer the modularization of authentication rather than just plugging it all into the download helper directly.

I tend to agree that the `Credentials` class may be a bit of overkill as it's really up to the `Authenticator` (like the name) to determine what _it_ needs.
</comment><comment author="s1monw" created="2015-03-24T10:05:51Z" id="85431582">ping... what's the status on this @dadoonet 
</comment><comment author="clintongormley" created="2015-04-05T10:24:05Z" id="89749108">Or @tlrx?
</comment><comment author="clintongormley" created="2015-06-05T12:17:57Z" id="109272770">This has been stalled a year, and nobody has requested this functionality.  Closing.
</comment><comment author="beiske" created="2015-07-02T14:47:13Z" id="118055882">This would be really convenient for Found
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable plugins to define default logging configurations for their needs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6802</link><project id="" key="" /><description>Currently we load the logging configuration from a single file (the `logging.xxx` under the config dir, where xxx can be on of: `yml`, `json`, `properties`).

It should be possible to have multiple logging configuration files under the config dir (potentially under different sub directories) and merge all to form the final the logging configuration. This will enable plugins to define their own extension to the logging infra (so they can come with default logging settings)
</description><key id="37485179">6802</key><summary>Enable plugins to define default logging configurations for their needs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels /><created>2014-07-09T16:44:12Z</created><updated>2014-07-16T11:15:02Z</updated><resolved>2014-07-09T19:48:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-10T11:36:26Z" id="48593606">@uboness would be good to have this documented somewhere
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>What to do with a problem node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6801</link><project id="" key="" /><description>Twice we've made some silly mistake and added a broken machine to the cluster.  Typically the machine joins the cluster fine and everything looks OK until shards migrate to the node and it can't handle the traffic and starts to thrash its disk or GC forever or whatever.  My question is, can Elasticsearch do anything to help minimize the impact of this broken node?

Right now if you set a timeout on search requests then the node will timeout and you'll just get responses from the nodes that did respond.  That's a start, but I'm wondering if Elasticsearch can do better - like maybe throttle requests back from the node when it goes over a certain number of timeouts.  Or something.  The problem is that I'm not really sure what is a safe thing to do that won't put the whole cluster in danger of a cascading failure.

Whatever Elasticsearch does, this is certainly something that you'd want to see on the cluster health api.

So, discussion?
</description><key id="37480168">6801</key><summary>What to do with a problem node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>feedback_needed</label></labels><created>2014-07-09T15:51:31Z</created><updated>2014-12-30T20:34:22Z</updated><resolved>2014-12-30T20:30:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-24T10:40:40Z" id="60371288">This is a hard hard problem to solve automatically.  As you say, the wrong action could easily bring down the whole cluster. I'd be reluctant to do anything other than to alert.

Something like the search and indexing stats allow problem nodes to be spotted.  One issue at the moment is that these numbers are running totals, while really you need eg avg search time over the last hour.

#8110 will help to make these numbers more useful, eg to calculate derivatives across time.

Do you have any other ideas, or are you OK with closing this?
</comment><comment author="nik9000" created="2014-12-30T20:34:22Z" id="68395358">I don't have any good ideas, no:)  We stuff most of those numbers in rrdtool which exposes those derivatives.  Its super nice.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Deprecated index_name in the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6800</link><project id="" key="" /><description>Relates to #6677
</description><key id="37479655">6800</key><summary>Docs: Deprecated index_name in the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-07-09T15:46:46Z</created><updated>2015-08-09T16:25:10Z</updated><resolved>2015-04-04T18:06:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-10T11:52:41Z" id="48594819">LGTM
</comment><comment author="clintongormley" created="2014-07-10T12:14:13Z" id="48596577">I won't merge it in yet as #6677 needs some code changes first.  but it's here when needed.
</comment><comment author="s1monw" created="2015-03-20T21:47:45Z" id="84161824">@clintongormley #6677 is closed - do you wanna merge this in?
</comment><comment author="clintongormley" created="2015-04-04T18:06:40Z" id="89632167">Closing - superceded by #9570
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hunspell doesn't merge .aff files</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6799</link><project id="" key="" /><description>The documentation states that when multiple .aff files are placed in the same directory they will be merged :
http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/hunspell.html#CO127-1

When I tried this I got the following error : 
"_Missing affix file for hunspell dictionary_"

This error is generated by `org.elasticsearch.indices.analysis.HunspellService#loadDictionary`. 
This error is a bit misleading because there are two .aff files in that directory. The error will appear when `affixFiles.length != 1` and should only apppear when the condition `affixFiles.length == 0` in my opinion.

There also is a small typo in the `loadDictionary` method (huspell instead of hunspell) 
</description><key id="37463854">6799</key><summary>Hunspell doesn't merge .aff files</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jvwilge</reporter><labels /><created>2014-07-09T13:29:19Z</created><updated>2014-07-09T13:54:25Z</updated><resolved>2014-07-09T13:54:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-09T13:54:25Z" id="48472241">@jvwilge Yes, the documentation is incorrect.  Only a single affix file is supported. See https://github.com/elasticsearch/elasticsearch-definitive-guide/issues/118

I will fix the docs. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM-Installation not complete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6798</link><project id="" key="" /><description>I'm on a `SUSE Linux Enterprise Server 11 (x86_64)` and evaluating ElasticSearch to replace an existing solution.

I downloaded [the rpm-release](https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.1.noarch.rpm) and tried to install it using `rpm -i elasticsearch-1.2.1.noarch.rpm` but neither the group shown in [preinstall](https://github.com/elasticsearch/elasticsearch/blob/master/src/rpm/scripts/preinstall) is added nor the [init-script](https://github.com/elasticsearch/elasticsearch/blob/master/src/rpm/init.d/elasticsearch) is moved/copied to `/etc/init.d`.

Am I missing something or is the installation routine messed up for SLES?
</description><key id="37460708">6798</key><summary>RPM-Installation not complete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">einfallstoll</reporter><labels /><created>2014-07-09T12:47:43Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2015-11-21T16:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-07-09T13:10:21Z" id="48467239">Hi,

the current init script won't work for SuSE enterprise. We are working though on a version that will work ( See #6533 )
I will do some testing on our side with a SLES VM and see what's going wrong because as far as we know it should work.
</comment><comment author="einfallstoll" created="2014-07-09T13:14:05Z" id="48467631">But the init-script does not even get copied to `/etc/init.d`?
</comment><comment author="electrical" created="2014-07-09T13:19:16Z" id="48468192">I agree its very weird, I'll do some digging on our side and see what's happening.
</comment><comment author="electrical" created="2014-07-09T13:29:16Z" id="48469260">Hi,

did some digging.

The user seems to be done correctly on my test VM:

```
# id elasticsearch
uid=106(elasticsearch) gid=107(elasticsearch) groups=107(elasticsearch)
```

The init script weirdly gets copied to `/etc/init.d/init.d`

Will do some more checking on this issue.
</comment><comment author="einfallstoll" created="2014-07-09T13:35:26Z" id="48469932">Seems like I mustn't run `rpm -i` as `root`.

```
pofab@svrl1sles11t01:/var/tmp&gt; sudo rpm -i elasticsearch-1.2.1.noarch.rpm
### NOT starting on installation, please execute the following statements to configure elasticsearch to start automatically using chkconfig
 sudo /sbin/chkconfig --add elasticsearch
### You can start elasticsearch by executing
 sudo service elasticsearch start
pofab@svrl1sles11t01:/var/tmp&gt; id elasticsearch
uid=152(elasticsearch) gid=153(elasticsearch) groups=153(elasticsearch)
```

But I'm having the same issue. The init-script is copied to `/etc/init.d/init.d`. Weird.
</comment><comment author="clintongormley" created="2015-11-21T16:09:26Z" id="158659163">Closing in favour of #12555
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Investigate using FST for ImmutableSettings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6797</link><project id="" key="" /><description /><key id="37458953">6797</key><summary>Investigate using FST for ImmutableSettings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-07-09T12:21:53Z</created><updated>2015-11-21T16:08:41Z</updated><resolved>2015-11-21T16:08:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T16:08:41Z" id="158659115">Nothing further here. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>significant_terms agg new sampling option.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6796</link><project id="" key="" /><description>Avoids the use of FieldData in high-cardinality free-text fields by re-tokenizing samples of the top matching documents.
Also provides a de-duplication option to remove duplicated sections of text commonly found in free-text document fields in order to avoid skewing word usage stats and making bad keyword suggestions as a result.
</description><key id="37458463">6796</key><summary>significant_terms agg new sampling option.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">markharwood</reporter><labels><label>:Aggregations</label><label>feature</label></labels><created>2014-07-09T12:13:54Z</created><updated>2015-06-06T17:51:46Z</updated><resolved>2015-05-13T09:48:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-10T12:29:47Z" id="48597855">I have two concerns about this change:
1. It adds very expert settings to the API, I would like to make sure significant terms don't end up looking like the phrase suggester API.
2. It does a lot of work at search time as opposed to indexing time. Would it be possible eg. to expect common content to be removed at indexing time?

On a side note, I'm wondering if we should change the significant terms aggregation to be a metrics aggregation instead of a bucket aggregation and stop returning document counts. I think this would help have more freedom in terms of implementation in the future?
</comment><comment author="markharwood" created="2014-07-10T12:58:49Z" id="48600507">All of the "sampling" options are optional with sensible defaults so we could simply choose not to document them if we want to avoid API bloat or find some way to hide the "expert" settings.

The biggest advantages to sampling are:
1) The avoidance of the FieldData overheads for free-text which has prevented users from being able to use this functionality (see http://stackoverflow.com/questions/23239625/significant-terms-causes-a-circuitbreakingexception )
2) The speed-ups that can be obtained when you are looking up background term frequencies for only a subset of the matching content rather than the long-tail of all of it (see https://groups.google.com/forum/#!searchin/elasticsearch/significant/elasticsearch/droJ1d740xk/CQObzMppf8cJ )

When I have used the significant terms algo in other products previously it was always used in a sampling top-matches-only context rather than the current agg implementation of using a memory cache of all terms. That's the implementation that feels alien to me.

&gt; It does a lot of work at search time as opposed to indexing time. Would it be possible eg. to expect common content to be removed at indexing time?

That shifts the dev effort back upstream into the content prep stage which makes life harder for our users. Also, at index time it may prove hard to maintain a window that is large/efficient enough to spot duplicates that are spread apart widely whereas in a typical search result set the similar documents tend to be bunched together tightly as they rank similarly.

Also - the practice of removing content from the search index is not the same as removing content from a sampling process.

&gt; I'm wondering if we should change the significant terms aggregation to be a metrics aggregation instead of a bucket aggregation and stop returning document counts.

Some uses of it on low-cardinality fields (e.g. my "crime types" demo) may justify having child aggs but for the free-text mode I can certainly see that child aggs are less viable.
</comment><comment author="markharwood" created="2014-07-13T21:55:38Z" id="48852948">Ran some tests computing significant terms on a larger index - 1.2GB of indexed Twitter content.

"map" exec hint took 3.4 secs to query (once warmed) and required 220Mb of RAM for FieldData.
"global ordinals" exec hint took 600 msecs to query (once warmed) and required 350Mb of RAM for FieldData.

The sampling option with de-dup took 25ms once warmed and no FieldData overhead was required.
We are still only talking about a comparatively small index here so sampling seems a compelling option for the larger indexes.

I need to  complete some work on making the sampling option avoid loading FieldData (the current impl here still does) but this is requiring changes to existing parser logic that currently always assumes ValueSources/FieldData are always required.
</comment><comment author="markharwood" created="2014-07-16T19:44:09Z" id="49216520">I have completed some tests with a representative data set (fairly big and noisy data in the form of 30m tweets).
The sampling approach shows obvious benefits in: 
- Reduced memory overheads
- Faster execution
- Better quality suggestions

The tables below shows the results for various single-term searches, showing the suggestions made by the existing FieldData-backed impl vs the new sampling approach. 

| Data source | Response time (ms) | Suggested keywords for "batman" |
| --- | --- | --- |
| _FieldData_ | 886 | batman mvagusta datto lucent_ac mvagusta_canada bieberbatbike bradford delincuente comisar&#237;a tgpzruhbmz h98yjig24e arkham suspect jeremybieber thebatman burglar entrega sumunod nauso dressed bbcnews v8j8xko3kn superman superherro neeson police 19th inglesa entreg&#243; comisaria |
| _Sampling (1,000 docs x 3 shards + duplicate paragraph detection)_ | 723 | batman arkham bradford darkknight catwoman robin |

| Data source | Response time (ms) | Suggested keywords for "search" |
| --- | --- | --- |
| _FieldData_ | 1415 | search engine xzzmttvtuq optimization hpq5akf2tl jm9lmzqtoq graph gruksdotcom google hot944 searchcap affiliater rdmclep018 connessioni estate goodroid mengetik tweetsauce phoenix engines turbocharges swu3yxa4bv zeptolab's i6usfn6dmt tweez emansangels shopakira nom's itamarketing seo |
| _Sampling (1,000 docs x 3 shards + duplicate paragraph detection)_ | 816 | search engine graph google optimization swu3yxa4bv sengineland |

| Data source | Response time (ms) | Suggested keywords for "marvel" |
| --- | --- | --- |
| _FieldData_ | 494 | marvel iron_man's capcom ironman3 ave0evh05w comics fpcwde8r1s marvelcharacteroftheday marvelnow avengers marvelcomics pnxilbu7rs graphicaudio feige iron trailer |
| _Sampling (1,000 docs x 3 shards + duplicate paragraph detection)_ | 418 | marvel comics marvelnow avengers capcom marvelcomics ironman3 feige |
|  |  |  |

| Data source | Response time (ms) | Suggested keywords for "obama" |
| --- | --- | --- |
| _FieldData_ | 1720 | obama barack president sequester administration michelle cuts gop tcot jedi republicans meld dictator burwell l2k5skptpd biden trek pardons impeach nominates budget caddell drones ktbsksozld benghazi overturn pres socialist americans charlespga |
| _Sampling (1,000 docs x 3 shards + duplicate paragraph detection)_ | 820 | obama barack michelle nominates president e.p.a sequester administration impeach barrack burwell biden |
|  |  |  |

#### Summary

##### Speed benefits

Although the sampling approach has to go to disk to get sample docs it ends up being faster than the existing FieldData impl because it is not looking up the background frequency of quite so many unique terms. With popular terms like "Obama" this speed difference becomes more apparent. 

##### Quality benefits

The quality of suggestions is noticably better than the FieldData impl because the sampling approach can remove the repeated sequences of text that are commonly found in noisy content like emails, tweets or web pages. The strange tokens in the FieldData-solutions' output are typically shortened URLs that are found in heavily re-tweeted or spammed content that the sampling solution can identify as duplicates.
One can imagine that with sloppier multi-term queries there will be a long tail of low-quality matches which will only serve to further dilute the quality of the FieldData-based suggestions which includes _all_ results while the sample-based solution would remain focused on the high-quality matches.

##### Memory benefits

My default server setting of 2GB was all that was required to run the sampling approach on the index of 30 million tweets (8.4GB index size on-disk). The FieldData backed solution required that I increase my heap size to 4GB in order to accommodate the 2.3GB of RAM required for the text field.

I can only recommend that we adopt sampling-based approaches for analysing free-text fields for significant terms. 
</comment><comment author="s1monw" created="2014-07-24T11:12:08Z" id="49994961">hey mark, I looked at this admittedly not too deep but I have a bunch of concerns:
- this seems to solve more than one problem to me it seems like it adds the ability to build a foreground set from a set of documents and it allows to apply certain analysis to it. It also tries to reduce memory as well as making results better. I wonder if we can split it up?
- it adds a crazy amount of complexity and I think we should try to reduce the complexity

I think this addition should really be split up IMO. I wonder if like @jpountz suggested in a chat today have a single bucket aggregator that replays the top N to significant terms to get the ability to do the top N part. Then in a second step we can add the ability to build foreground sets from term vectors that would allow us to detach the feature selection from the actual consumer and maybe add a nice API / feature to term vectors to build foreground sets that can also be used by other features like MLT? /cc @alexksikes 
</comment><comment author="markharwood" created="2014-07-25T10:24:57Z" id="50132417">Yep there's several pieces internally here that can potentially be broken out. I was hoping that could be done as a later refactoring exercise but we can create a series of issues to build it up bit by bit if you want. I'd outline them as follows with suggested collaborators:

1) Lucene stuff - The de-dup logic in DeDuplicatingTokenFilter and ByteStreamDuplicateSequenceSpotter have no ES dependencies and could be a Lucene contribution subject to review by @jpountz  or @s1monw ? I expect it may have limited uses outside of the statistical sampling use case here where dup-docs typically appear close together in search result sets as opposed to their randomized positions in index-time streams of docs (which makes them harder to spot using a limited window of historical content).

2) ES content sampling/retrieval - the FieldTokenStreamProvider is ES-specific and needs @alexksikes input/coordination on shared use within MLT.  

3) Aggregation sampling - the notion that a "top N" docs filter/sampler has applications outside of significant terms aggs - me and @jpountz to review how this would be designed.

We'd need to get all these foundations committed before I can progress this issue. Does this make sense?
</comment><comment author="markharwood" created="2014-08-08T14:09:28Z" id="51606742">Rebased on latest master.
I had a call with @jpountz to discuss concerns with this PR a summary of which is given below:

#### Concerns with this PR as it stands

Generally the concern was that the various text-processing functions in here (doc sampling, TokenStream sequence de-duping and signif terms) aren't broken out as user-composable services in the same way that the existing aggregations are organised.

#### Text is different

However, the challenge with composing chains of pluggable text-processors is that this requires a fundamentally different interface between the processing units than the one offered by the aggregations framework today. The aggregations working on structured data can simply organise documents into discrete "buckets" and then delegate to child aggregators with a simple "collect(parentBucketId, docId)" call. Unstructured text processing however is a more complex affair - frameworks like Lucene's TokenFilter chains or document processing pipelines like GATE offer examples of frameworks for composing pluggable text processing logic. 

#### Should we even attempt free-text analysis in the aggs tree?

We discussed the idea that an aggregation tree composed of "structured" aggregators could, at a designated point in the tree, introduce a free-text-based aggregator/processor which then contains a branch of only child/grandchild aggregators that can collaborate on processing unstructured streams of data.  These are then arguably not "aggregators" but some form of text processor. This feels messy and the only text processor widgets we could think of were significant_terms.

The above concerns made us re-think the notion that significant_terms for free-text fields (and text analysis in general) should be something that is attempted in the aggregation tree. While it may be useful to embed a significant_terms agg under a top-level structured agg such as geo or time to find keywords, you would never choose to then add a child agg under the significant_terms agg (due to the volume of low-quality terms that are typically returned from each shard and then discarded on the reducing node). This breaks the usual model of "composable" aggs that support nesting.

#### What is the alternative?

We would like to explore the idea of a new REST endpoint for performing significant_terms analysis on free text given a query and various settings that define the sample set for analysis. This sounds like the APIs that are underdevelopment by @alexksikes for the MLT api and so will need his input.
The existing significant_terms agg that works on structured fields e.g.  low-cardinality fields is still likely to be useful but it looks like we should steer users to this new specialised end-point for more specialised free-text analysis based on sampling.

#### Conclusions
- Creating composable text processing widgets in the mostly-structured agg tree is likely to be too complex (for users and us)
- MLT and the significant_terms_on_free_text code here may merge in the creation of a new keyword-suggestion endpoint (text sampling, de-duping, phrase detection etc).
- If keeping the ability to embed free-text significance analysis under various structured buckets in the agg tree is considered important then this PR may still have value/relevance.
</comment><comment author="markharwood" created="2015-05-13T09:48:18Z" id="101597265">The "best doc sampling" part of this PR got rolled into https://github.com/elastic/elasticsearch/issues/8108

This PR still contains code that is useful for other concerns, notably the de-duping token stream that helps eliminate repetitive noise from free-text which otherwise skews any statical analysis. 

Parts of this may be resurrected at a later point if we ever shift to performing text-based significance e.g. eliminating dups, processing TokenStreams not FieldData/DocValues so we can detect phrases etc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: Disable JSONP by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6795</link><project id="" key="" /><description>By default, disable the option to use JSONP in our REST layer
</description><key id="37450027">6795</key><summary>Security: Disable JSONP by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:REST</label><label>breaking</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-09T10:02:18Z</created><updated>2015-06-06T16:46:58Z</updated><resolved>2014-07-09T19:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-09T10:12:55Z" id="48452329">LGTM, left a minor comment.
</comment><comment author="dakrone" created="2014-07-09T10:13:45Z" id="48452418">Actually, `docs/reference/api-conventions.asciidoc` needs to be updated as well to reflect this change.
</comment><comment author="kimchy" created="2014-07-09T10:28:47Z" id="48453638">@dakrone pushed fixes to relevant comments
</comment><comment author="dakrone" created="2014-07-09T10:43:19Z" id="48454801">LGTM, +1
</comment><comment author="kimchy" created="2014-07-09T15:32:46Z" id="48489504">@s1monw fixed doc
</comment><comment author="s1monw" created="2014-07-09T19:03:51Z" id="48519588">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Engine] checkVersionMapRefresh shouldn't use indexWriter.getConfig()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6794</link><project id="" key="" /><description>We run it out of lock, the indexWriter may be closed..

Relates to #6443, #6786
</description><key id="37439048">6794</key><summary>[Engine] checkVersionMapRefresh shouldn't use indexWriter.getConfig()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-07-09T07:06:11Z</created><updated>2014-07-09T08:58:03Z</updated><resolved>2014-07-09T08:45:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T07:46:50Z" id="48439417">I don't think we should do this. If somebody configures the threadpool in an odd way we might block on on the threadpool until an executor is free and it has potential for deadlocks. I think we should get the ram buffer under the lock and then pass it to the `checkVersionMapRefresh` 
</comment><comment author="bleskes" created="2014-07-09T08:07:52Z" id="48441084">agreed. Update coming up.
</comment><comment author="bleskes" created="2014-07-09T08:16:37Z" id="48441803">@s1monw I pushed another commit with a different approach. I'll change the PR description if we agree this is the way.
</comment><comment author="s1monw" created="2014-07-09T08:23:37Z" id="48442385">++
</comment><comment author="mikemccand" created="2014-07-09T08:36:24Z" id="48443479">+1, thanks @boaz!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Control whether MapperService docMapper iterator should contain DEFAULT_MAPPING</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6793</link><project id="" key="" /><description>At the moment one can iterate the MapperService to go through all document mappers. This includes the document mapper of DEFAULT_MAPPING, which may be surprising and lead to unintended results. This commit removes the Iterable implementation and add a docMappers method that asks the caller to make an explicit choice
</description><key id="37437438">6793</key><summary>Control whether MapperService docMapper iterator should contain DEFAULT_MAPPING</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-09T06:28:44Z</created><updated>2015-06-07T12:55:10Z</updated><resolved>2014-07-09T09:34:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-09T07:20:56Z" id="48437634">LGTM, left one minor comment
</comment><comment author="bleskes" created="2014-07-09T08:29:58Z" id="48442934">pushed another commit based on the feedback
</comment><comment author="jpountz" created="2014-07-09T08:53:01Z" id="48444992">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Lucene's deprecated PatternAnalyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6792</link><project id="" key="" /><description>Instead of using the PatternAnalyzer, the functionality was replicated by using Lucene's StopFilter, PatterTokenizer and LowerCaseFilter

Closes #6717
</description><key id="37427296">6792</key><summary>Remove Lucene's deprecated PatternAnalyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels /><created>2014-07-09T01:41:38Z</created><updated>2014-07-09T15:18:51Z</updated><resolved>2014-07-09T15:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T07:48:31Z" id="48439550">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Generate formatted strings from datetime values at indexing time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6791</link><project id="" key="" /><description>Use case is to be able to take an input datetime value and extract portions of the value for indexing/searching, eg.  2014-06-23T17:01:49.571-07:00 into separate fields of year:2014, month:06, date:23, day_of_week:1, etc...  Something like the formatted string functionality for aggregation/date histogram, but perhaps used in a mapping to spawn individual fields that are formatted strings of the datetime value.
</description><key id="37421819">6791</key><summary>Generate formatted strings from datetime values at indexing time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppf2</reporter><labels><label>discuss</label><label>enhancement</label></labels><created>2014-07-08T23:35:14Z</created><updated>2014-10-24T10:25:23Z</updated><resolved>2014-10-24T10:25:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-09T08:52:00Z" id="48444897">@ppf2 what is the final use case here?  

one issue with just extracting day, hour, etc is that the timezone is fixed.  there is no way to say "give me everything between 3-4pm in EST"
</comment><comment author="ppf2" created="2014-07-09T17:40:48Z" id="48508382">The final use case is to be able to search/filter for documents created in a specific month, on a Friday, etc..
</comment><comment author="clintongormley" created="2014-07-10T11:25:28Z" id="48592774">OK - so extracting the day/hour etc at index time means that you are limited to a single time zone. That may be fine for a number of use cases, but it does rather limit things. 

The alternative is to add query and aggregation support which can do the calculations at query time, adjusted for time zone.
</comment><comment author="clintongormley" created="2014-10-24T10:25:23Z" id="60369923">Closing in favour of #6785 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not fail whole request on closed index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6790</link><project id="" key="" /><description>The bulk API request was marked as completely failed,
in case a request with a closed index was referred in
any of the requests inside of a bulk one.

Implementation Note: Currently the implementation is a bit more verbose in order to prevent an `instanceof` check and another cast - if that is fast enough, we could execute that logic only once at the beginning of the loop (thinking this might be a bit overoptimization here).

Closes #6410
</description><key id="37418703">6790</key><summary>Do not fail whole request on closed index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T22:42:35Z</created><updated>2015-06-07T19:30:21Z</updated><resolved>2014-09-19T09:16:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-09T08:49:14Z" id="48444633">@spinscale would this fix also handle the case mentioned in https://github.com/elasticsearch/elasticsearch/issues/6410#issuecomment-48298353 when indexing into a non-existent index with `action.auto_create_index` set to `false`?
</comment><comment author="s1monw" created="2014-07-09T19:38:54Z" id="48523870">added some comments
</comment><comment author="spinscale" created="2014-07-11T09:22:37Z" id="48710557">@clintongormley added another test when `action.auto_create_index` is set to false
@s1monw added an interface and thus refactored to the code...
</comment><comment author="s1monw" created="2014-07-15T13:05:42Z" id="49028195">I added some comments 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Jackson 2.4.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6789</link><project id="" key="" /><description>I ran into this one https://github.com/FasterXML/jackson-core/issues/145, which doesn't seem to happen in 2.3.3. Lets see how the investigation proceeds here, but potentially we need to revert the upgrade before releasing 1.3.
</description><key id="37412897">6789</key><summary>Upgrade to Jackson 2.4.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>upgrade</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T21:17:52Z</created><updated>2015-06-07T12:56:55Z</updated><resolved>2014-07-09T15:50:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rescorer ignores track_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6788</link><project id="" key="" /><description>The track_scores param is ignored when combined with the rescore features.
This is not the behavior I expect with this feature.

Can you confirm this is a bug ?
If so can we excpect a bugfix ?

Thanks,
</description><key id="37411604">6788</key><summary>Rescorer ignores track_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">afoucret</reporter><labels><label>:Search</label><label>enhancement</label><label>won't fix</label></labels><created>2014-07-08T21:00:54Z</created><updated>2017-07-26T08:18:10Z</updated><resolved>2017-07-26T08:17:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-09T08:08:50Z" id="48441158">Did you specify sorting? The `track_scores` feature only has effect when defining `sort` in the search request.
</comment><comment author="clintongormley" created="2014-07-09T09:21:50Z" id="48447622">Could you provide a short curl recreation of what you are seeing to demonstrate the issue?
</comment><comment author="afoucret" created="2014-07-09T18:47:15Z" id="48517297">Here is a simple test that demonstrate the "bug" :

First create a doc :

&lt;pre&gt;&lt;code&gt;
curl -XPOST http://localhost:9200/test/content?pretty -d '{
  position: 2
}'
&lt;/code&gt;&lt;/pre&gt;

Then try to rescore without specific sort order (so score is used) : 

&lt;pre&gt;&lt;code&gt;
curl -XPOST http://localhost:9200/test/content/_search?pretty -d '{
  rescore : {
    window_size: 10,
    query : {
      rescore_query : {
        function_score: {
          boost_factor : 10
        }
      }
    }
  }
}'
&lt;/code&gt;&lt;/pre&gt;

Into the result the score is 11 just as expected : 

&lt;pre&gt;&lt;code&gt;
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 11.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "content",
      "_id" : "pgd5f6KEST-VPQ5-CZmXbw",
      "_score" : 11.0,
      "_source":{
          position: 2,
       }
    } ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

Now trying to executes the rescore with position as sort order :
&lt;code&gt;&lt;pre&gt;
curl -XPOST http://localhost:9200/test/content/_search?pretty -d '{
  sort : {position: "asc"},
  track_scores: true,
  rescore : {
    window_size: 10,
    query : {
      rescore_query : {
        function_score: {
          boost_factor : 10
        }
      }
    }
  }
}'
&lt;/code&gt;&lt;/pre&gt;

I was expected the same behavior than previously but the score into the result the score is now 1 :

&lt;code&gt;&lt;pre&gt;
{
  "took" : 0,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "content",
      "_id" : "pgd5f6KEST-VPQ5-CZmXbw",
      "_score" : 1.0,
      "_source":{
          position: 2,
          count_views: 3
      },
      "sort" : [ 2 ]
    } ]
  }
}
&lt;/code&gt;&lt;/pre&gt;

My real use cases is more complicated but the idea is to apply a rescore on a small subset of document and get a secondary sort order for docs at the same position. It is not possible even when adding. In my opinion the track_scores param should be applied to the full chain of the query including rescores.
</comment><comment author="martijnvg" created="2014-07-10T08:16:20Z" id="48576721">The rescorer only is active when sorting by relevancy. In your second search request you're not sorting by relevancy and therefore your boost isn't applied. The `track_score` is active since the score is being kept track off in the hit, but because the rescorer isn't active the score isn't what you're expecting.

The rescorer isn't active, because it makes no sense relevancy wise to apply a rescorer on the top hits if the results aren't sorted by relevancy. I think ES needs to perhaps return a parse error if rescorer is being specified and sort isn't relevancy. Also the documentation can be clearer about this.
</comment><comment author="afoucret" created="2014-07-10T09:14:55Z" id="48581875">My first test was with this query (same result as previously :

&lt;code&gt;&lt;pre&gt;
curl -XPOST http://localhost:9200/test/content/_search?pretty -d '{
  sort : [{position: "asc"}, {_score: "desc"}]
  track_scores: true,
  rescore : {
    window_size: 10,
    query : {
      rescore_query : {
        function_score: {
          boost_factor : 10
        }
      }
    }
  }
}
'&lt;/code&gt;&lt;/pre&gt;

As you can see the "secondary sort is on the score. So if I get your point rescorer should be active since I sort by score even not as a primary sort order (at least it make sense from my point of view). 
But it is does not work. And I don't think ES should raise an error in this precise case (since score is specified as one of the sort order).
</comment><comment author="afoucret" created="2014-07-11T13:16:15Z" id="48728495">Any news about this ?
</comment><comment author="afoucret" created="2014-07-16T09:23:10Z" id="49140691">Hi,

I really need a response about my last comment.

Thanks by advance
</comment><comment author="clintongormley" created="2014-07-16T10:09:33Z" id="49145512">@afoucret please don't keep asking for a response. we have this on the list of issues to discuss, but you will see that there are many issues on this list.

@martijnvg i can see the use case for rescoring when not sorting by `_score`, eg give me the most recent tweets, and then rescore the top 100 by some other value. i realise there are other ways to achieve the same thing but I could understand wanting to use the rescore phase for this.
</comment><comment author="martijnvg" created="2014-07-16T10:33:05Z" id="49147849">The rescorer only works if no sort is specified at all. In that case the hits are ordered by relevancy (_score desc). I think we should allow rescoring if the sort contains `_score`? @s1monw What do you think?
</comment><comment author="clintongormley" created="2014-07-16T10:34:05Z" id="49147954">@martijnvg even if it doesn't include `_score`, no?  why not allow it?
</comment><comment author="martijnvg" created="2014-07-16T10:35:58Z" id="49148150">@clintongormley I think rescoring only makes sense if the regular sort is _score based? Otherwise the reordering that the rescore performs for the top N docs doesn't make sense?
</comment><comment author="s1monw" created="2014-07-16T10:38:01Z" id="49148363">@martijnvg I think it could make sense if you want for instance the latest results but then also rank them by another factory like geo distance or importance? so ++ to allow rescorer here
</comment><comment author="martijnvg" created="2014-07-16T10:39:56Z" id="49148566">@s1monw Ok, yes that make sense, then +1 on allowing rescore if any kind of sort is defined.
</comment><comment author="s1monw" created="2014-07-18T10:35:19Z" id="49417432">after thinking about this I think the only thing we can do here is to change the score but we can't reorder the results. The problem here is that we would need to rescore before we sort inorder to get the correct sort order. I wonder if this is the expected behavior or if we should just not do it at all. If we do it it will be inconsistent since if you sort by score the actual score after rescore will be different to the score used for sorting and if you are on a multi shard index it will again be different to a single shard.
</comment><comment author="clintongormley" created="2014-07-18T11:16:13Z" id="49420167">@s1monw not sure i understand? if the use case is:
- select the 1000 most recent tweets
- then sort them by _score / geo location / whatever

then you would first sort, then only rescore (and resort) the `window_size`, no?
</comment><comment author="afoucret" created="2014-07-18T11:49:13Z" id="49422303">@clintongormley 

I was thinking exactly the same. Sort have to be applied before rescore (easy to understand why) to get the n first doc and after rescore to resort. I don't see what is difficult but I can believe it is.

About rescoring being inconsistent in multi-shard environments it seems to me this is consubstantial of the way rescoring works even without this sort order problem. The window_size per shard implies that the number of rescored docs depends the number of shards ... and therefore the final ordering depends the number of shards. Not shocking from a functional point of view !!!

About the need, order is more important to me than score value.
</comment><comment author="afoucret" created="2014-12-02T16:17:24Z" id="65256653">Hi everyone,

Any news about this feature ?

Thank you very much
</comment><comment author="s1monw" created="2014-12-02T16:38:30Z" id="65260385">@afoucret I think the last time I tried to implemented this there were lots of problems that I couldn't solve easily. I try to find the branch somewhere but no promise
</comment><comment author="afoucret" created="2014-12-02T17:13:19Z" id="65266792">@s1monw : Hi thank you for your quick response. I am very sad to heard this since I meet an issues I don't see any substitution for the rescore feature to get resolved.

Another option to get a similar feature would be to allow query into the function_score and to use the nested query score to compute the final score. It would be something like : 

&lt;code&gt;&lt;pre&gt;
query : {
   function_score : {
      query : MY_FULLTEXT_QUERY,
      query_score : {
        has_child : {
        }
      }
   }  
}
&lt;/pre&gt;&lt;/code&gt;

As you can see the main use case is to act on the final score by using the scoring of chlidren doc (or nested) as a component of the scoring (sum, multiplication, ...)

We would lose the windowing feature (which is not optimal for the biggest datasets) but it is a choice in my use case where the dataset is not huge and we prefer computation of the boost factor for the popularity to be done at the query time rather than computing it at indexing (since changing the parameter can be done live).

From my POV, it could be a good idea to refactor rescore into function_score and append : 
- the query_score I described as rescore function just like weight, decay or script
- a windowing method
</comment><comment author="Wedjaa" created="2015-04-23T15:46:44Z" id="95629853"> Hi - any news on this feature? 
 Is there another way of using rescoring and sorting in the same query?
</comment><comment author="clintongormley" created="2015-04-26T14:59:10Z" id="96394595">Hi @Wedjaa 

You may find the `terminate_after` parameter in http://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-body.html a useful alternative to rescoring.
</comment><comment author="Wedjaa" created="2015-04-26T15:12:22Z" id="96398681">Hi @clintongormley,

looked into it, but I don't seem to be able to figure out how it substitutes rescoring. Am I missing something here?

The use case: search for some inexpensive query, rescore using expensive queries and then sort by relevance scoring and - for example - item name. I don't think is such an extreme case, considering that rescoring is targeted to exactly this use case - minus the sorting.

I'm at a loss on why the score is not added to the hits; especially considering that enabling "explain" the hits will contain the rescored value in the explanation; which means is somehow available when the hits are merged together to be given out as a response.

  cheers
  Fabio
</comment><comment author="clintongormley" created="2015-04-26T18:14:15Z" id="96417522">@Wedjaa I don't know the details of why this change is complex - just taking it on trust.

Actually, ignore my comments about `terminate_after` - I realise that it doesn't fit here at all.
</comment><comment author="tvarvadoukas" created="2015-06-17T09:39:52Z" id="112737437">Hey all! I would also agree that this is a very common usecase. It's a must-feature on our side as well. Are there any news on this?
</comment><comment author="silashansen" created="2017-07-12T09:24:48Z" id="314706809">I've just done a simple test and it seems that this issue has been fixed and that sort is not disabling rescoring anymore.

I'm on ES 5.3.2

First create some documents:
```
POST test/rescores
{
  "Title": "Document A"
}

POST test/rescores
{
  "Title": "Document B"
}
```

Query the index to see the what the response looks like without any sorting and rescoring:

```
GET test/rescores/_search
{
  "query": {
    "query_string": {
      "fields": ["Title"], 
      "query": "Document B"
    }
  }
}
```

Response:
```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 0.78549397,
    "hits": [
      {
        "_index": "test",
        "_type": "rescores",
        "_id": "AV02FVaYJKvO25AaxC0D",
        "_score": 0.78549397,
        "_source": {
          "Title": "Document B"
        }
      },
      {
        "_index": "test",
        "_type": "rescores",
        "_id": "AV02FQv7JKvO25AaxC0C",
        "_score": 0.16358379,
        "_source": {
          "Title": "Document A"
        }
      }
    ]
  }
}
```

Now issue a request that includes both rescoring and sorting.

```
GET test/rescores/_search
{
  "query": {
    "query_string": {
      "fields": ["Title"], 
      "query": "Document B"
    }
  },
  "sort": [
    {
      "_score": {
        "order": "desc"
      }
    }
  ],
    "rescore": {
    "window_size": 100,
    "query": {
      "rescore_query": {
        "function_score": {
          "script_score": {
            "script": {
              "inline": "return 10"
            }
          },
          "boost_mode": "multiply"
        }
      },
      "query_weight": 1,
      "rescore_query_weight": 1
    }
  }
}
```

Response:

```
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "failed": 0
  },
  "hits": {
    "total": 2,
    "max_score": 10.785494,
    "hits": [
      {
        "_index": "test",
        "_type": "rescores",
        "_id": "AV02FVaYJKvO25AaxC0D",
        "_score": 10.785494,
        "_source": {
          "Title": "Document B"
        }
      },
      {
        "_index": "test",
        "_type": "rescores",
        "_id": "AV02FQv7JKvO25AaxC0C",
        "_score": 10.163584,
        "_source": {
          "Title": "Document A"
        }
      }
    ]
  }
}

```

Now, it's obvious that the rescoring is in fact active, as the scores are clearly factored by the boost in the rescore query.

So, what is the status from Elastic on this? Should the docs be updated or am I misunderstanding something?

</comment><comment author="javanna" created="2017-07-12T21:16:36Z" id="314899594">hi @silashansen unfortunately this issue wasn't solved. I can still reproduce the initial recreation above. I suspect everything works in your case because your sort clause is the default one, as you are sorting by score descending which is the same as leaving the sort clause out of the request.</comment><comment author="silashansen" created="2017-07-13T09:16:56Z" id="315021606">You are absolutely right - what a shame!
Is it anywhere on the roadmap to get this fixed?</comment><comment author="jimczi" created="2017-07-13T12:37:45Z" id="315065083">The issue reported here is not a bug and cannot be "fixed". The rescorer by design should always rescore top hits sorted by `_score`. It's the only way to have the correct behavior because rescoring is done at the shard level. In the recreation of this issue the rescorer is used on a result sorted by something else than `_score`, a field named `position`. To get the N best documents sorted by `position` we need to fetch the N best results for all shards. 
Though the rescoring reorders the top N+M with a function score at the shard level and returns a new top N. So it's not returning the best N documents per shard sorted by position but the best N documents sorted by the function score among the N+M best documents sorted by position. 
To be accurate the rescoring should be applied to the N best documents  globally, not per shard, otherwise the returned documents may not represent the first sort.

I am closing this issue because this is a known limitation. Rescoring works only when _score is the first sort criteria.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting require_field_match option does not always work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6787</link><project id="" key="" /><description>I have this record:

```
{
   "field1": "x y",
   "field2": "y"
}
```

and this query:

```
{
    "query": {
        "multi_match": {
             "query":  "x y",
             "fields": [ "field1", "field2" ],
             "operator": "and"
        }
    },
    "highlight" : {
        "fields" : {
            "field1" : { "number_of_fragments" : 0 },
            "field2" : { "number_of_fragments" : 0 }
        },
        "require_field_match": true
    }
}
```

The highlights returned are:

```
      "highlight" : {
        "field2" : [ "&lt;em&gt;y&lt;/em&gt;" ],
        "field1" : [ "&lt;em&gt;x&lt;/em&gt; &lt;em&gt;y&lt;/em&gt;" ]
      }
```

So field2 is highlighted although it was not part of the match (and it does not show up in the explain output).
</description><key id="37410149">6787</key><summary>Highlighting require_field_match option does not always work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">davidlbowen</reporter><labels><label>:Highlighting</label></labels><created>2014-07-08T20:41:57Z</created><updated>2015-04-02T10:54:39Z</updated><resolved>2015-04-02T10:54:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-20T14:30:50Z" id="84032093">Sorry it took ages to get to this. I spent some time looking at this, we can call it a bug, or just asking too much of highlighting :) 

The meaning of `requireFieldMatch` is different from what is expected here. It is really only about field names, not really matching queries. By default (`require_field_match: false`) the field name gets ignored, for instance with the following document

```
{
  "field1":"value",
  "field2":"value"
}
```

the following search request will cause `field2` to be highlighted simply based on the fact that it contains the term `value` that was mentioned in the query, the field name gets ignored. 

```
{
  "query" : {
    "match" : {
        "field1" : "value"
    }
  },
  "highlight" : {
    "fields" : {
      "field2" : { }
    }
  }
}
```

By setting `require_field_match` to `true` the field name will be taken into account and only `field1` will be highlighted for a query executed against `field1`.

I can give you some more explanation of how lucene works internally here, just to explain why `field2` gets highlighted in your original example. It might seem strange but highlighting doesn't always highlight matching queries, but it's more about matching terms extracted from queries, which is close... The internal lucene query generated in your case is a `DisjunctionMaxQuery` that contains two `BooleanQuery`s, each of which holds two `TermQuery`s as clauses.

```
((+field2:x +field2:y) | (+field1:x +field1:y))
```

while highlighting `field2`, each of the leaf queries (the term queries) are checked against the field content, regardless of the context. When checking `field2:y` for instance, the highlighter doesn't really look at whether the clause was in and with some other clause, as each leaf is highlighted independently. If the `y` term appears in the field, it gets highlighted. By setting `require_field_match` to `true` you are just making sure that the match was really on that same field, which it is, but there is no way to make sure that the query is an actual match overall.

I think there isn't much that we can do to fix this, we just need to live with this limitation, or eventually report this issue (that doesn't have much to do with require field match anyway) upstream to lucene.
</comment><comment author="javanna" created="2015-04-02T10:54:35Z" id="88864722">I think we can close this, nothing we can do here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Prevent NPE if engine is closed while version map is checked</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6786</link><project id="" key="" /><description>We check if the version map needs to be refreshed after we released
the readlock which can cause the the engine being closed before we
read the value from the volatile `indexWriter` field which can cause an
NPE on the indexing thread. This commit also fixes a potential uncaught
exception if the refresh failed due to the engine being already closed.
</description><key id="37406246">6786</key><summary>Prevent NPE if engine is closed while version map is checked</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T19:49:22Z</created><updated>2015-06-07T19:30:36Z</updated><resolved>2014-07-08T22:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-08T19:51:24Z" id="48391228">This relates to #6443
</comment><comment author="kimchy" created="2014-07-08T19:58:50Z" id="48392197">left one small comment, other than that, LGTM
</comment><comment author="s1monw" created="2014-07-08T20:02:22Z" id="48392625">agreed... pushed a new commit
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create time filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6785</link><project id="" key="" /><description>At present there is no way to filter with time range, only date range - for example: you cannot do 7PM - 11PM (ignoring the date) without storing the time in a separate field.
</description><key id="37405063">6785</key><summary>Create time filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tgmweb</reporter><labels><label>adoptme</label><label>high hanging fruit</label></labels><created>2014-07-08T19:35:16Z</created><updated>2015-11-21T16:03:43Z</updated><resolved>2015-11-21T16:03:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-17T06:28:09Z" id="49263119">Hey,

Sounds useful. As a current workaround you could use a script filter (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-script-filter.html), which access the hour of the day like this: `doc['created_at'].date.hourOfDay &gt;= 19 &amp;&amp; doc['created_at'].date.hourOfDay &lt; 23`
</comment><comment author="clintongormley" created="2014-10-24T10:24:43Z" id="60369869">It would be nice to have a specialised date/time filter which allows you to filter on, eg `day_of_week: [ Mon, Tues, Wed]` and `time: { gte: "5pm", lt: "7pm"}`, all in one filter.

This can possibly be done more efficiently than with scripting.
</comment><comment author="clintongormley" created="2015-11-21T16:03:43Z" id="158658868">I just don't think this is feasible without indexing time (or day of week, etc) separately. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw exception if function in function score query is null</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6784</link><project id="" key="" /><description>closes #6292
</description><key id="37394242">6784</key><summary>Throw exception if function in function score query is null</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T17:36:06Z</created><updated>2015-06-07T19:59:30Z</updated><resolved>2014-07-11T11:56:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T10:36:22Z" id="48454273">LGTM
</comment><comment author="clintongormley" created="2014-07-11T09:06:06Z" id="48709265">@brwe can we get this merged in?
</comment><comment author="brwe" created="2014-07-11T11:56:41Z" id="48722015">closed by 6d8fff6
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set default translog `flush_threshold_ops` to unlimited, to flush by byte size by default and not penalize tiny documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6783</link><project id="" key="" /><description>I think it's safe to do this (again) now that #6443 is in.
</description><key id="37388840">6783</key><summary>Set default translog `flush_threshold_ops` to unlimited, to flush by byte size by default and not penalize tiny documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T16:37:18Z</created><updated>2015-06-07T12:57:20Z</updated><resolved>2014-07-08T16:41:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-08T16:37:53Z" id="48365176">LGTM this issue needs labels
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggs: filtering values using array of values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6782</link><project id="" key="" /><description>In facets, we can filter a Terms Facet using an [array of values](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_excluding_terms):

``` json
{
    "query" : {
        "match_all" : { }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag",
                "exclude" : ["term1", "term2"]
            }
        }
    }
}
```

In aggs, we can't use the same syntax anymore as [IncludeExclude](https://github.com/elasticsearch/elasticsearch/blob/1.2/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java#L141-155) does not support arrays.

Same apply for include.

We can probably use `|` character to separate terms but it could be handy to be able to specify directly an array of terms.

cc @jpountz 
</description><key id="37385106">6782</key><summary>Aggs: filtering values using array of values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-07-08T15:57:47Z</created><updated>2014-09-12T16:10:47Z</updated><resolved>2014-09-12T15:17:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-08T16:45:56Z" id="48366330">@dadoonet the idea is indeed to rely on the regexp to express multiple options... I'm wondering whether adding an array support really/significantly enhances usability... would love to keep it simple and I (personally) find `"term1 | terms2 | term3"` be as usable as `[ "term1", "term2", "terms3" ]` (maybe even more?) and it's more expressive
</comment><comment author="rjernst" created="2014-07-08T18:16:02Z" id="48378530">&gt; `"term1 | terms2 | term3"` be as usable as `[ "term1", "term2", "terms3" ]`

What if a term has a pipe character in it?  Or spaces? Using an array is nice because it is deterministically parseable. Sure you can do escapes when using a regex syntax, but it is hard to extend it (adding meaning to special characters) in the future without potentially breaking existing users.
</comment><comment author="clintongormley" created="2014-07-08T20:43:35Z" id="48396986">@rjernst regexophobe!

spaces are fine in regexes. how many fields have pipes in them? and regexes do give you a lot more flexibility, eg `foo_.*`

that said, i'd be in favour of supporting both a single string(==regex) or an array of simple strings
</comment><comment author="micpalmia" created="2014-08-22T09:44:30Z" id="53042281">Another problem with the current exclude syntax is that it's impossible to use it on fields that are not strings. The exclude feature on facets allowed numeric input, so I don't see why its aggregations counterpart shouldn't.

You can find a more detailed description in [this email thread](https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/8g74ov0run0/yLfGwVF7WVgJ).

Should I open a new, different issue for this?
</comment><comment author="nezda" created="2014-08-22T12:48:07Z" id="53056043">I agree with @micpalmia - this is a "feature regression" compared to Terms Facet for numerical fields.  Because of this, in porting an app from facets to aggregations, I'm using a hybrid now.

A more general approach might be a transformer that can reshape/filter output of an aggregation?I agree with @micpalmia - this is a "feature regression" compared to Terms Facet for numerical fields.  Because of this, in porting an app from facets to aggregations, I'm using a hybrid now.

A more general approach might be a transformer that can reshape/filter output of (including aggregation output)? Overkill?
</comment><comment author="jdoranster" created="2014-08-22T14:23:25Z" id="53066239"> +1 for @nezda  as this currently is a hassle in porting facet code to aggregations
</comment><comment author="jplehmann" created="2014-08-22T15:50:03Z" id="53077953">+1 
</comment><comment author="npilon" created="2014-08-22T17:20:16Z" id="53091979">:+1: - facets-style exclude arrays are very useful.
</comment><comment author="markharwood" created="2014-09-01T15:20:14Z" id="54071272">Pull Request opened here: https://github.com/elasticsearch/elasticsearch/pull/7529
</comment><comment author="nezda" created="2014-09-12T15:36:42Z" id="55420157">Does this work for numerical fields like analogous term facet feature did?  Otherwise I'll open an issue for that.
</comment><comment author="markharwood" created="2014-09-12T15:43:15Z" id="55421098">No - that is a bigger piece of work which is why it did not go into this commit. A separate issue would helps us keep track of that requirement so that would be appreciated.
</comment><comment author="nezda" created="2014-09-12T16:10:47Z" id="55424804">I see - thought there might be a chance :smiley: Opened #7714 to track that.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixes Logger class for BackgroundIndexer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6781</link><project id="" key="" /><description>Fixes https://github.com/elasticsearch/elasticsearch/issues/6780
</description><key id="37366632">6781</key><summary>Fixes Logger class for BackgroundIndexer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">valdisrigdon</reporter><labels><label>:Logging</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T12:53:54Z</created><updated>2015-06-07T19:31:04Z</updated><resolved>2014-07-09T14:52:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T20:29:49Z" id="48395632">Hi @valdisrigdon 

Please could I ask you to sign our CLA. http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="valdisrigdon" created="2014-07-08T20:40:53Z" id="48396705">I signed the CLA this morning.
</comment><comment author="bleskes" created="2014-07-09T10:11:47Z" id="48452241">@valdisrigdon I left one little comment. Thx for reporting this!
</comment><comment author="valdisrigdon" created="2014-07-09T12:20:13Z" id="48462379">@bleskes fixed!
</comment><comment author="bleskes" created="2014-07-09T14:54:56Z" id="48483487">pushed. thx @valdisrigdon !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BackgroundIndexer using RecoveryWhileUnderLoadTests for logging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6780</link><project id="" key="" /><description>BackgroundIndex is using the wrong Class for it's logger.

```
public class BackgroundIndexer implements AutoCloseable {

    private final ESLogger logger = Loggers.getLogger(RecoveryWhileUnderLoadTests.class);
```
</description><key id="37366020">6780</key><summary>BackgroundIndexer using RecoveryWhileUnderLoadTests for logging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">valdisrigdon</reporter><labels /><created>2014-07-08T12:45:51Z</created><updated>2014-07-16T11:45:50Z</updated><resolved>2014-07-10T14:05:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-10T14:05:30Z" id="48608514">Fixed by #6781
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During discovery, verify connect when sending a rejoin cluster request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6779</link><project id="" key="" /><description>When a master receives a cluster state from another node, it compares the local cluster state with the one it got. If the local one has a higher version, it sends a JoinClusterRequest to the other master to tell it step down. Because our network layer is asymmetric, we need to make sure we're connected before sending.
</description><key id="37364619">6779</key><summary>During discovery, verify connect when sending a rejoin cluster request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T12:25:09Z</created><updated>2015-06-07T12:57:42Z</updated><resolved>2014-07-08T12:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-08T12:27:22Z" id="48329219">LGTM except for the comment phrasing
</comment><comment author="bleskes" created="2014-07-08T12:41:13Z" id="48330528">thx. comment fixed and extended.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added an option to show the upper bound of the error for the terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6778</link><project id="" key="" /><description>...he terms aggregation.

This is only applicable when the order is set to _count.  The upper bound of the error in the doc count is calculated by summing the doc count of the last term on each shard which did not return the term.  The implementation calculates the error by summing the doc count for the last term on each shard for which the term IS returned and then subtracts this value from the sum of the doc counts for the last term from ALL shards.

Closes #6696
</description><key id="37358573">6778</key><summary>Added an option to show the upper bound of the error for the terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T10:47:19Z</created><updated>2015-06-07T12:57:52Z</updated><resolved>2014-07-25T13:28:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-11T12:53:00Z" id="48726377">I just played with it and I think this is an interesting feature to raise awareness about the accuracy issues of the terms aggregation and although as a way to test the impact of the shard_size parameter. The per-term error is interesting, but I think the global error that you added is also interesting because it also gives information about terms that didn't make it to the top terms.

To move forward, I think it would be nice to have it on all sort orders (potentially by using a special value of eg. -1 when the maximum error cannot be estimated or would be so large that it would not be really useful).
</comment><comment author="jpountz" created="2014-07-11T12:54:08Z" id="48726476">On a side note, if it goes into release X, I think we should try to have another change in the same release that would change the default value of shard_size.
</comment><comment author="colings86" created="2014-07-11T13:03:18Z" id="48727269">I largely agree although during my testing of this feature I have had quite a few situations where the error for the whole aggregation has been quite big relative to the doc count for the last returned term (e.g error of 3600 with a doc count for the last returned term of 5400) but the error on all of the terms was 0.  This seems confusing for a user?  Although maybe this just highlights the importance of clearly explaining the way the error is calculated and what it means?

Agree regarding your suggestions for moving forward and the issue around default shard size
</comment><comment author="jpountz" created="2014-07-16T10:25:51Z" id="49147071">I left some comments, but I think it's moving in the right direction!

&gt; Although maybe this just highlights the importance of clearly explaining the way the error is calculated and what it means?

+1 on that
</comment><comment author="colings86" created="2014-07-16T13:42:01Z" id="49166584">@jpountz I have made most of the requested changes and have left some comments too
</comment><comment author="colings86" created="2014-07-22T12:41:34Z" id="49733163">@jpountz  updated with your review comments.  I had to change the serialisation of the term error back to a Long rather than a VLong as it can be -1 and VLong doesn't allow negative numbers. I can change it so I serialise a boolean which indicated whether the value is -1 and then the VLong if its not but I wanted your opinion on whether it is worth it?
</comment><comment author="jpountz" created="2014-07-24T07:49:58Z" id="49977373">I think long is ok if we can avoid serializing the errors when `show_term_doc_count_error` is false.

I left some comments, but I think it's close!
</comment><comment author="jpountz" created="2014-07-25T09:17:49Z" id="50126464">LGTM but can you fix indentation issues and trailing spaces that have been added on some lines before pushing?
</comment><comment author="colings86" created="2014-07-25T13:28:15Z" id="50148770">Merged into 1.x and master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't replace indices within ActionRequest and check blocks against concrete indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6777</link><project id="" key="" /><description>If the request is being executed on the master node, there is no need to replace its content with the concrete indices obtained by calling MetaData#concreteIndices. Leave the request as it is and use the concrete indices to perform the requested operation (commit #1).

Concrete indices is now called multiple times when needed instead of changing what's inside the incoming request with the concrete indices. Ideally we want to keep the original aliases or indices or wildcard expressions in the request (commit #2).

Also made sure that the check blocks is done against the concrete indices, which wasn't the case for delete index, delete mapping, open index, close index, types exists and indices exists (commit #2).

Note: I think replacing the indices within the request is a bad pattern and I would love to find a way to make the indices final somehow, so that we don't make the same mistake again in the future. On the other hand, requests are exposed through java api and serialized over the wire, which both make it hard to make the indices field final and to remove the setter, which needs to be exposed to the request builders. I thought about making the setter package private, but that wouldn't help as the transport action is usually in the same package and the same could happen again. Ideas are welcome here!
</description><key id="37354842">6777</key><summary>Don't replace indices within ActionRequest and check blocks against concrete indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-08T10:13:15Z</created><updated>2015-06-07T12:58:05Z</updated><resolved>2014-07-08T12:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-08T10:57:32Z" id="48322330">I think we should also change `TransportMasterNodeOperationAction` so that when executing on the coord node, a `checkClusterBlock` is called (before sending to the master). I think we can do it right before calling `processBeforeDelegationToMaster` ?
</comment><comment author="kimchy" created="2014-07-08T11:28:22Z" id="48324572">Regarding the indices field, I would love to not expose being able to set it, its much cleaner. I would not expose a setter for it, not in the request layer and not in the builder layer, and only allow to create it as a constructor parameter. That is a much bigger change, so maybe for starters we can do this one, and then do the other one.
</comment><comment author="javanna" created="2014-07-08T12:11:34Z" id="48327814">@uboness regarding the `checkClusterBlock`, that makes sense but I would do it as part of a different issue, as this one doesn't change where checks are made, but just makes sure that they are done on the proper concrete indices (`checkBlock` ran and still runs on the master).
</comment><comment author="javanna" created="2014-07-08T12:13:00Z" id="48327936">@kimchy agreed, that would require changes to request builders and/or action hierarchy though, we'll see what we can do on a separate issue.
</comment><comment author="uboness" created="2014-07-08T12:15:35Z" id="48328167">LGTM
</comment><comment author="kimchy" created="2014-07-08T12:21:36Z" id="48328695">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[STORE]: Make use of Lucene build-in checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6776</link><project id="" key="" /><description>Since Lucene version 4.8 each file has a checksum written as it's
footer. We used to calculate the checksums for all files transparently
on the filesystem layer (Directory / Store) which is now not necessary
anymore. This commit makes use of the new checksums in a backwards
compatible way such that files written with the old checksum mechanism
are still compared against the corresponding Alder32 checksum while
newer files are compared against the Lucene build in CRC32 checksum.

Since now every written file is checksummed by default this commit
also verifies the checksum for files during recovery and restore if
applicable.

Closes #5924

Note: This PR still has a bunch of nocommits and I need to crank up some more bwcompat tests but I wanted to get the word out... all tests pass
</description><key id="37344647">6776</key><summary>[STORE]: Make use of Lucene build-in checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-07-08T08:40:09Z</created><updated>2014-07-10T13:05:19Z</updated><resolved>2014-07-10T13:04:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-10T10:29:48Z" id="48588568">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date histogram with interval hour </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6775</link><project id="" key="" /><description>I was sort of expecting the following to give me an aggregation which groups the results only by hour:

```
curl http://localhost:9000/stream/_search -d '{
    "aggs" : {
        "visitor_count" : { "date_histogram" : { "field" : "created_at", "interval" : "hour"} }
    }
}'
```

As it stands, it does group by hour, but it's also grouped by day. (I end up with 24 results for each day I have data).

Is it possible to group this only by the hour so I have 24 results only? Have I missed the point?! (Am on 1.2.1)
</description><key id="37324476">6775</key><summary>Date histogram with interval hour </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">minotaur62</reporter><labels /><created>2014-07-08T00:59:55Z</created><updated>2014-07-08T04:41:43Z</updated><resolved>2014-07-08T04:41:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-08T04:41:43Z" id="48271529">Please use the mailing list for questions. You'll get definitely a better support there.

That said, yes we create a bucket for each hour (absolute) and not for each hour of the day.
The result you are getting is expected.

You need to change either your model or do some scripting. But let's discuss that on the mailing list.

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added a little note to scroll docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6774</link><project id="" key="" /><description>It wasn't so clear the first time around. Thought it would help a little. Likewise, this section might also need the same edits: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-search-type.html#scan.
</description><key id="37304491">6774</key><summary>Added a little note to scroll docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">abhikalakuntla</reporter><labels><label>docs</label></labels><created>2014-07-07T19:58:30Z</created><updated>2014-07-08T18:38:15Z</updated><resolved>2014-07-08T09:55:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T09:56:45Z" id="48293018">Hi @abhikalakuntla 

Thanks for the pull request.  I thought the scrolling docs needed a bigger cleanup, so I rewrote them and incorporated your change.  Hope they are easier to read now.
</comment><comment author="abhikalakuntla" created="2014-07-08T18:38:15Z" id="48381713">@clintongormley awesome, looks good! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase Suggester: Add option to filter out phrase suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6773</link><project id="" key="" /><description>The newly added filter option will let the user provide a template query which will be executed for every
phrase suggestions generated to ensure that the suggestion matches at least one document for the query.
The filter query is only executed on the local node for now. When the new filter option is used, the
size of the suggestion is restricted to 20.

Closes #3482
</description><key id="37301524">6773</key><summary>Phrase Suggester: Add option to filter out phrase suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">areek</reporter><labels><label>:Suggesters</label><label>feature</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-07T19:25:02Z</created><updated>2015-06-06T18:49:14Z</updated><resolved>2014-07-14T20:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-08T13:57:52Z" id="48339072">I like this PR a lot. left a bunch of comments
</comment><comment author="areek" created="2014-07-08T23:35:16Z" id="48413011">Updated PR; incorporated feedback and Thanks for the review!
</comment><comment author="s1monw" created="2014-07-09T07:36:09Z" id="48438633">I left some more comments. This looks great though. I'd like to get @clintongormley to give some feedback to make sure the REST interface is good 
</comment><comment author="areek" created="2014-07-10T21:57:15Z" id="48670006">I have incorporated the feedback (would like to get some comments on the `Preference` (see comment above).

After discussion on the REST interface, this is what it looks like now:

``` bash
curl -XPOST 'localhost:9200/_search' -d {
    "suggest": {
        "text": "Xor the Got-Jewel",
        "simple_phrase": {
            "phrase": {
                "field": "body",
                "size": 5,
                "shard_size": 10,
                "confidence": 2,
                "collate": {
                    "query": {
                        "body": "{{suggestion}}"
                    },
                    "preference": "_primary"
                }
            }
        }
    }
}
```

`collate` also accepts a `filter` instead of the `query` and there is an optional `params` which the user can use to inject additional params to the query/filter template
</comment><comment author="s1monw" created="2014-07-11T06:21:11Z" id="48697975">I left some cosmetic comments. I think this looks very good @clintongormley WDYT... In a second PR we could add the ability to just mark the results if the query / filter matched or not instead of filtering that might help folks to see what is going on.
</comment><comment author="clintongormley" created="2014-07-11T12:38:18Z" id="48725143">Suggested some doc changes, but otherwise LGTM
</comment><comment author="areek" created="2014-07-11T16:47:19Z" id="48754415">I have updated the PR with the feedback. I think the enhancement is ready but still not happy with the state of `Preference` (see comment in  `Preference`).
</comment><comment author="s1monw" created="2014-07-14T08:12:19Z" id="48874686">&gt;  ...but still not happy with the state of Preference (see comment in Preference)

I didn't see a comment, what are you not happy with?

this LGTM actually and I think we should move forward with it unless the issue is major which I didn't see yet?
</comment><comment author="areek" created="2014-07-14T14:06:59Z" id="48903520">It seems the way I have `Preference` currently, there is more boiler plate added rather then being a clean solution. Examples: 

``` java
Preference.ONLY_LOCAL.type()
```

is needed for '_local_only'. 

The values() business feels wrong on the enum, some options have to have values others don't and its not obvious wants happening.

That was my concern, I think if we plan to have it as is, I will atleast want to use the new `Preference` type in more places than in PhraseSuggester (at least use it in the `PlainOperationRouting`)
</comment><comment author="areek" created="2014-07-14T17:54:30Z" id="48933556">Updated PR:
- added static helper methods to parse Preference type &amp; value
- use new Preference enum in `PlainOperationRouting`
</comment><comment author="s1monw" created="2014-07-14T19:57:25Z" id="48951349">LGTM please squash and push 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During recovery, only send mapping updates to master if needed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6772</link><project id="" key="" /><description>The change added in #6762 helps making sure the pending mapping updates are processed on all nodes to prevent moving shards to nodes which are not yet fully aware of the new mapping. However it introduced a racing condition delete_mapping operations, potentially causing a type to be added after it's deletion. This commit solves this by only sending a mapping update if the mapping source has actually changed.
</description><key id="37301506">6772</key><summary>During recovery, only send mapping updates to master if needed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-07T19:24:52Z</created><updated>2015-06-07T19:31:25Z</updated><resolved>2014-07-07T19:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-07T19:38:12Z" id="48230138">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't update default mapping on phase2 recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6771</link><project id="" key="" /><description>the default mapping is not merged, but updated in place, and only put mapping API can change it, no need to make sure it has been properly updated on the master. This can cause conflicts when a put mapping for it happens at the same time.
</description><key id="37285505">6771</key><summary>Don't update default mapping on phase2 recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels /><created>2014-07-07T16:26:57Z</created><updated>2014-07-07T16:58:23Z</updated><resolved>2014-07-07T16:40:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-07T16:35:02Z" id="48203043">LGTM. Only comment I had to prevent this in future, I think we should add an assert to the `MappingUpdatedAction#updateMappingOnMaster` to block attempts to update the default mapping
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add missing pre built analysis components</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6770</link><project id="" key="" /><description /><key id="37275746">6770</key><summary>Add missing pre built analysis components</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-07T14:51:13Z</created><updated>2015-06-07T12:58:25Z</updated><resolved>2014-07-08T17:54:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-07T14:55:00Z" id="48189091">+1, thanks for cleaning up here.
</comment><comment author="s1monw" created="2014-07-08T16:26:00Z" id="48363601">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add full support for join queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6769</link><project id="" key="" /><description>Using lucene's org.apache.lucene.search.join package we could be able to generate full join query with the proper "from, to, on" parameters.
</description><key id="37272058">6769</key><summary>Add full support for join queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gamars</reporter><labels><label>stalled</label></labels><created>2014-07-07T14:12:48Z</created><updated>2017-07-18T16:42:21Z</updated><resolved>2016-11-25T18:16:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-07T14:36:58Z" id="48186490">Hi @gamars are you aware of the parent/child support? With the has_child [1] and has_parent [2] queries you're able to create joins in your queries.

1: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-has-child-query.html#query-dsl-has-child-query
2: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-has-parent-query.html#query-dsl-has-parent-query
</comment><comment author="clintongormley" created="2014-07-07T14:39:12Z" id="48186798">Hi @gamars 

The join query is not suitable for use in a distributed environment like Elasticsearch. It would perform very badly.  Rather look at the parent-child support that @martijnvg mentioned.
</comment><comment author="gamars" created="2014-07-07T14:50:49Z" id="48188447">@clintongormley I understand it would perform very bad without any useful hints (no routing, no helpers, ...). Yet I am convinced that a Join method going slightly beyond what the parent-child provides (and without the n-to-1 obligation) is entirely feasible. 

@martijnvg don't get me wrong I like the has_child and has_parent implementations of ES (someone finally made joins simple ;). Yet I am times to times wishing I could join (on generally fairly small indexes which could be replicated on each shard) for sake of simplicity.

My typical example is ACL. Say I have an access control list (which is in term of lucene a fairly small index, even in the millions of entries) I would like to be able to use a JOIN on this index by documents without having to enter the n-ary possible combinations of parents-to-document-withint-acls. 

Say we'd create an index present on all shards (fully present, not sharded itself) then it woudl be trivial to join on this index wouldn't it? 
</comment><comment author="clintongormley" created="2014-07-07T15:22:49Z" id="48193050">Sorry @gamars - I may have closed the issue too promptly. I'll reopen for discussion.
</comment><comment author="gamars" created="2014-07-07T15:25:52Z" id="48193471">Thx @clintongormley. 

I understand the limitation of joins within distributed archs. I guess this "feature request" would be 2-folds: 
- Have a "non-distributed" type of index
- Support joins with non-distributed index (on the to part of join)

Would that make more sense? I am confident this could be achieved with a plugin, but I'd really like to be able to have it at the core level without additional add-ons. 

WDYT?
</comment><comment author="martijnvg" created="2014-07-08T08:23:01Z" id="48284457">@gamars Did you see terms lookup by query?
https://github.com/elasticsearch/elasticsearch/pull/3278

I think this kind of join covers what you need, but this PR is still not merged in.
</comment><comment author="gamars" created="2014-07-09T15:54:27Z" id="48494233">@martijnvg Yes it does! 

I haven't had the chance to look deeply into it yet, but I assume that it would work well for my sample use case if I was to filter with lookup from a non-sharded-all-replicated index right? Are there some heuristics that would take advantage of the availability of such an index? 
</comment><comment author="martijnvg" created="2014-07-10T08:26:32Z" id="48577531">Yes, this would work for your use case and if shards of the index the terms filter is targeting are local to the node, then no networking in involved at all.
</comment><comment author="gamars" created="2014-07-10T08:36:03Z" id="48578304">@martijnvg this is an excellent news I must say! I am looking forward to the feature. This spawns 2 new questions (rather logistical ones): 
- When will this PR be merged? 
- Will it be back ported to the 1.1.x branch (we have a java 1.6 limitation...)
</comment><comment author="martijnvg" created="2014-07-10T08:54:38Z" id="48580059">1) I can't tell. The development of this PR is paused, because getting this feature in right is tricky. Due to the fact that all possible related documents can be on any shard, there is potentially a lot of networking involved and this can lead to memory issues and other slowness. This needs to be addressed properly.
2) No, because 1.1.x has already been released and only bug fixes are eligible to be ported back to the 1.1 branch. Once a major version has been released (1.0.0, 1.1.0, 1.2.0 etc) no new features are added to these versions.
</comment><comment author="clintongormley" created="2014-10-24T10:10:29Z" id="60368503">Stalled by #3278
</comment><comment author="JDvorak" created="2014-10-28T17:07:55Z" id="60792756">+1
</comment><comment author="matthiasg" created="2016-06-22T08:31:36Z" id="227677663">Is apache spark capable of joins on top of es ? https://www.elastic.co/guide/en/elasticsearch/hadoop/current/spark.html does not mention it ...
</comment><comment author="clintongormley" created="2016-11-25T18:16:45Z" id="263007228">There are no plans to support full joins.  Closing</comment><comment author="mrdanadams" created="2017-07-18T16:42:21Z" id="316123959">@clintongormley We've been using the siren-join plugin (https://github.com/sirensolutions/siren-join) pretty heavily but the company behind it just told us they are going to stop supporting it. Still, with a large data set we've had some good results with it. The idea is similar to using a terms filter with values from another document except that the values come from the result of a search.

Would there be any re-considering supporting this at the ES level? Using siren-join also requires turning on 'cache everything' on your index (which isn't ideal). Perhaps if it were native it could be more optimally integrated?</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Add all netty classes during shading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6768</link><project id="" key="" /><description>In order to have access to all codecs and handlers by netty, they
need to be exposed during shading, otherwise only the classes, which
are used by the built are exposed.
</description><key id="37266906">6768</key><summary>Build: Add all netty classes during shading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-07-07T13:11:20Z</created><updated>2014-07-07T15:16:02Z</updated><resolved>2014-07-07T15:16:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-07T13:35:14Z" id="48178399">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Improved explanation for match_phrase_prefix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6767</link><project id="" key="" /><description>The match_phrase_prefix provided the same explanation as the match_phrase
query. There was no indication that the last term was run as a prefix
query.

This change marks the last term (or terms if there are multiple terms
in the same position) with a *

Closes #2449
</description><key id="37264410">6767</key><summary>Query DSL: Improved explanation for match_phrase_prefix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-07T12:35:15Z</created><updated>2015-06-08T00:08:31Z</updated><resolved>2014-07-07T14:29:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-07T12:38:21Z" id="48172720">LGTM, but should the version be 1.2.2 instead of 1.2.1?
</comment><comment author="clintongormley" created="2014-07-07T12:40:25Z" id="48172912">@martijnvg oops yes, fixed thanks.
</comment><comment author="clintongormley" created="2014-07-07T14:29:20Z" id="48185453">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Configuration: Add an option for the script folder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6766</link><project id="" key="" /><description>Since dynamic scripting is off by default since elasticsearch &gt;=1.2 (which is good!), we have to move all our scripts to files in the `config/scripts` folder. It would be awesome to have an option to change that folder in the config to make it easier for using different folders in different environments. Best would be to set it through ENV variables `ELASTICSEARCH_SCRIPTS_FOLDER` and through the config file

```
path:
  logs: /var/log/elasticsearch
  data: /var/data/elasticsearch
  scripts: /my/custom/script/folder
```

thanks!
</description><key id="37256213">6766</key><summary>Configuration: Add an option for the script folder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benben</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-07-07T10:21:14Z</created><updated>2015-11-21T16:00:02Z</updated><resolved>2015-11-21T16:00:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-18T12:17:46Z" id="49424260">Hey,

did you see the new indexed scripts feature, this might already solve your problem, albeit in a little different way. See http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/modules-scripting.html#_indexed_scripts

Would be interested if this solves your requirement.
</comment><comment author="benben" created="2014-07-18T14:25:16Z" id="49436416">&gt; Note that you must have dynamic scripting enabled to use indexed scripts at query time.

thats exactly what we dont want.
</comment><comment author="dakrone" created="2014-07-18T14:28:17Z" id="49436777">@benben that documentation only accurate for the 1.2.x releases (/cc @GaelTadh) , with the release of sandboxed scripting languages in 1.3, you can use indexed scripts without enabling dynamic scripting since Groovy scripts will be sandboxed.
</comment><comment author="spinscale" created="2014-07-29T08:20:37Z" id="50448069">@benben does the sandboxing feature solve your issue or is there still something missing that requires a configurable scripts config dir?
</comment><comment author="benben" created="2014-07-29T08:39:13Z" id="50449639">I'm not sure. Probably I don't get it right. What we want to do: We want to have scripts available on all environments (local developer machines, production systems, testing systems). Now we have puppet manifests for ES and just creating the script file there. Developers have to copy them to a different folder on their machines. So if we change this scripts, we have to change it at least two times. I think the best would be storing the scripts in our application repo, because its application related logic. But for this we need a configurable script folder. So developers set it in their ES config once to their application repo folder and on production systems managed with puppet, I would set it to the applications deploy folder. With this we can update the scripts as we want, check them in and every machine has it as soon as it pulls the new code. If you can explain me how we can do this with the indexed scripts then we could use this otherwise I would say there is still a need for this config option :smiley_cat: 
</comment><comment author="clintongormley" created="2014-10-24T10:03:56Z" id="60367917">I think we should take this further and make eg the config file name configurable.

General review of hardcoded filenames required.
</comment><comment author="clintongormley" created="2015-11-21T16:00:01Z" id="158658717">Closed by #12668
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve Settings#get lookup for camel case support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6765</link><project id="" key="" /><description>Today, if we miss on a get on setting, we try its camel case version. The assumption is that in our code, we never use camel case to lookup a setting, but we want to support camel case if the user provided one.
This can be expensive (#toCamelCase) when the get on the setting is done in a tight call, which is evident when running the allocation deciders as part of the reroute logic.
Instead of doing the camel case check on get, prepare an additional map that includes all the settings that are provided as came case, and try and lookup from it if needed.
</description><key id="37250885">6765</key><summary>Improve Settings#get lookup for camel case support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-07T09:06:55Z</created><updated>2015-06-07T12:58:36Z</updated><resolved>2014-07-09T12:24:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-07T12:39:46Z" id="48172852">LGTM
</comment><comment author="kimchy" created="2014-07-08T11:00:14Z" id="48322545">@jpountz / @martijnvg pushed changes based on the review
</comment><comment author="s1monw" created="2014-07-08T16:34:52Z" id="48364783">I wonder if we want to implement this as an FST that holds both key versions? @mikemccand WDYT
</comment><comment author="kimchy" created="2014-07-08T17:56:06Z" id="48375821">@s1monw we could, though almost all times, the underscore map is simply an empty one. I think its worth to keep it simple for now
</comment><comment author="s1monw" created="2014-07-09T12:09:09Z" id="48461455">@kimchy lets get this in and them open a new ticket for the FST?
</comment><comment author="kimchy" created="2014-07-09T12:22:54Z" id="48462650">@s1monw opened an issue: https://github.com/elasticsearch/elasticsearch/issues/6797 regarding FST, will push this in
</comment><comment author="mikemccand" created="2014-07-09T12:41:56Z" id="48464413">Single FST instead of (up to) 2 maps would be less RAM, but not sure that matters much here: there are not that many settings.

It would also be a single pass lookup instead of (up to) 2 hash lookups, but FST lookups are in general slower than hash lookups (doing vInt decodes for each character in the input key) ... so I'm not sure it'd be better overall.
</comment><comment author="mikemccand" created="2014-07-09T12:42:59Z" id="48464521">Also, I think it's strange/dangerous to allow two variants of settings like this: I think such leniency, while well-intended, only leads to confusion/problems down the road.  But it's probably way too late to change that...
</comment><comment author="clintongormley" created="2014-07-09T12:44:26Z" id="48464652">I wonder how many people use the camel-cased versions.  Should we consider removing them in 2.x?
</comment><comment author="kimchy" created="2014-07-09T12:47:40Z" id="48464962">@clintongormley I think its worth keeping them around, but we can start a discussion around it, I might be in a minority :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update filtered-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6764</link><project id="" key="" /><description /><key id="37247851">6764</key><summary>Update filtered-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">wheredevel</reporter><labels /><created>2014-07-07T08:22:28Z</created><updated>2014-07-23T10:41:35Z</updated><resolved>2014-07-23T09:01:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wheredevel" created="2014-07-07T08:23:06Z" id="48151991">Small typo fix.
</comment><comment author="martijnvg" created="2014-07-07T08:26:10Z" id="48152202">Thanks for the PR. Please could you sign our CLA so that I can get your request merged in: http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-07-23T08:58:31Z" id="49848898">CLA not signed. Treating as bug report.
</comment><comment author="wheredevel" created="2014-07-23T09:18:01Z" id="49850647">It's strange: I did sign the CLA.
</comment><comment author="clintongormley" created="2014-07-23T10:41:35Z" id="49858230">hmmm - i apologise, it didn't show up there.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search suggestions not provided on 404 page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6763</link><project id="" key="" /><description>Repro:
1. open http://www.elasticsearch.org/guide/en/elasticsearch/0.90/current/query-dsl-filtered-query.html#query-dsl-filtered-query
2. Click to the search text field (btw, why I can not navigate into it using the `TAB` key?)
3. No suggestions are provided as your type the text
</description><key id="37246457">6763</key><summary>Search suggestions not provided on 404 page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2014-07-07T07:57:16Z</created><updated>2015-06-24T06:33:23Z</updated><resolved>2015-06-24T06:33:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-07T08:26:35Z" id="48152234">hmmm - will take a look
</comment><comment author="lukas-vlcek" created="2015-06-24T06:33:22Z" id="114746644">Outdated...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During relocation, process pending mapping update in phase 2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6762</link><project id="" key="" /><description>During phase1 we copy over all lucene segments. These make refer to mapping updates that are still queued up to be sent to master. We must make sure those pending updates are sent before completing the relocation.

Relates to #6648
</description><key id="37245786">6762</key><summary>During relocation, process pending mapping update in phase 2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-07T07:45:41Z</created><updated>2015-06-07T12:58:48Z</updated><resolved>2014-07-07T11:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-07T08:59:54Z" id="48155062">I think that there is a cleaner way to do it, by simply sending an update mapping per type, with a listener, and waiting for the response. This will guarantee that all the mappings have been properly processed by the master node.
</comment><comment author="bleskes" created="2014-07-07T10:02:16Z" id="48160550">@kimchy pushed another commit based on your suggestion
</comment><comment author="kimchy" created="2014-07-07T10:45:12Z" id="48164075">left minor comment, LGTM otherwise
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bulk load issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6761</link><project id="" key="" /><description>If I want get 1000 users at a time from mysql to elasticsearch for every 5 sec. what is the best way to do this? 

now i am using this.

&#8226; curl -XPUT localhost:9200/_river/my_jdbc_river/_meta -d '{\"type\" : \"jdbc\",\"jdbc\" : { \"url\" : \"jdbc:mysql://localhost:3306/aqua\",\"user\" : \"****_\",\"password\" : \"**_**\",\"schedule\" : \" 0/5 \* \* \* \* ? \",\"sql\" : \"select \* from user\" , \"index\": \"Aqua\",\"type\": \"user\"}}'

Is there any thing to add . please suggest.
</description><key id="37239706">6761</key><summary>bulk load issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aparna-wenable</reporter><labels /><created>2014-07-07T05:07:22Z</created><updated>2014-07-07T06:19:44Z</updated><resolved>2014-07-07T06:19:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-07T06:19:44Z" id="48144414">Please use the mailing list.
We and the community can definitely help there.

Also, try to provide as much information as you can.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>open_file_descriptors from api is much smaller than from lsof</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6760</link><project id="" key="" /><description>$ curl localhost:9200/_nodes/10.XX.XX.XX/stats/process?pretty
{
  "cluster_name" : "toolsES",
  "nodes" : {
    "6WY7ku9FTU2N19yOAhPYbg" : {
      "timestamp" : 1404706290068,
      "name" : "10.XX.XX.XX",
      "transport_address" : "inet[/10.XX.XX.XX:9300]",
      "host" : "",
      "ip" : [ "inet[/10.XX.XX.XX:9300]", "NONE" ],
      "attributes" : {
        "master" : "true"
      },
      "process" : {
        "timestamp" : 1404706290068,
        "open_file_descriptors" : 2594,
        "cpu" : {
          "percent" : 416,
          "sys_in_millis" : 180076600,
          "user_in_millis" : 2345815150,
          "total_in_millis" : 2525891750
        },
        "mem" : {
          "resident_in_bytes" : 43864948736,
          "share_in_bytes" : 8939905024,
          "total_virtual_in_bytes" : 2267955445760
        }
      }
    }
  }
}
# 

$ lsof -p 45616|wc -l
38620

we get open_file_descriptors from es api as 2594 which is much smaller than 38620 from lsof

what is the difference between them?
how does es get open_file_descriptors? 
</description><key id="37238397">6760</key><summary>open_file_descriptors from api is much smaller than from lsof</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">childe</reporter><labels /><created>2014-07-07T04:15:32Z</created><updated>2014-07-07T05:42:08Z</updated><resolved>2014-07-07T05:41:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="childe" created="2014-07-07T05:42:08Z" id="48142671">just know es use nmapping which does not use file descriptors 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>enhance wait_for_status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6759</link><project id="" key="" /><description>It would be nice if `wait_for_status` could be enhanced so it does strict status checks not just the current (at least status with green&gt;yellow&gt;red). This would allow one to use it to potentially monitor critical changes for a certain index and react to them.
</description><key id="37228491">6759</key><summary>enhance wait_for_status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">costin</reporter><labels><label>discuss</label><label>enhancement</label></labels><created>2014-07-06T21:20:11Z</created><updated>2015-11-21T15:58:27Z</updated><resolved>2015-11-21T15:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="costin" created="2014-07-06T21:21:23Z" id="48127508">@imotov I've assigned the issue to you since we talked briefly about it at #EsAllHands. I've labeled 1.3 but feel free to change this accordingly.

Cheers!
</comment><comment author="clintongormley" created="2015-11-21T15:58:27Z" id="158658643">Unclear what these strict status checks are, and no progress in a year and a half. Closing, at least for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Separate parsing implementation from setter in SearchParseElement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6758</link><project id="" key="" /><description>This, in order to allow reuse of parsing logic by plugins or other internal parts.

Closes #3602
</description><key id="37228460">6758</key><summary>Separate parsing implementation from setter in SearchParseElement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-06T21:18:10Z</created><updated>2015-06-07T12:59:16Z</updated><resolved>2014-07-28T13:09:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-07T08:44:29Z" id="48153703">I took a quick look at this though and I think we should keep it simpler and just add a method

``` Java
public  SearchContextHighlight parse(XContentParser parser, IndexQueryParserService queryParserService) {
// do your q parsing
}
```

and do this in implementations where is makes sense... I don't think we should do different exception handling at all just stick to what is in there for now and can you try to not make things `static` here. I also see that they are all private so I wonder how you access them?
</comment><comment author="synhershko" created="2014-07-07T09:07:08Z" id="48155721">The reason why I did the re-throwing with another exception is to still be able to throw `SearchParseException` with the `context`. I think it's valuable in the exception. If you think I can drop it, that would definitely be easier.

The static methods are factory methods, they should indeed be public, my bad.
</comment><comment author="s1monw" created="2014-07-07T09:51:06Z" id="48159595">fail enough... then lets go with ElasticsearchIllegalArgumentException
</comment><comment author="synhershko" created="2014-07-07T09:52:20Z" id="48159722">Done, I'll go ahead and apply this to all the things
</comment><comment author="synhershko" created="2014-07-07T22:55:47Z" id="48252084">Applied this in a few more places, however most implementations of `SearchParseElement` are either too simple for this change (a la `VersionParseElement`) or are too complex and will require serious refactoring (see `SortParseElement` for example).

I assume the question is do we want to go farther than where we are now in this PR?
</comment><comment author="s1monw" created="2014-07-09T19:43:16Z" id="48524387">I left some comments... if this is enough for you we can just pull it without changing all the others
</comment><comment author="s1monw" created="2014-07-15T12:46:57Z" id="49026095">@synhershko any news on this?
</comment><comment author="synhershko" created="2014-07-15T12:47:37Z" id="49026150">@s1monw https://github.com/elasticsearch/elasticsearch/pull/6758#discussion_r14732661
</comment><comment author="synhershko" created="2014-07-16T11:14:13Z" id="49151659">@s1monw pushed fixes as per your comments
</comment><comment author="s1monw" created="2014-07-17T13:21:47Z" id="49305264">LGTM can you squash the commits
</comment><comment author="synhershko" created="2014-07-18T09:42:15Z" id="49413254">@s1monw done
</comment><comment author="synhershko" created="2014-07-27T20:57:48Z" id="50285550">@s1monw ping, before this gets stale :)
</comment><comment author="s1monw" created="2014-07-28T08:18:06Z" id="50310808">thanks for the ping I will take care of it soon
</comment><comment author="s1monw" created="2014-07-28T13:09:34Z" id="50335504">I adjusted the commit msg and pushed to master &amp; 1.x thanks @synhershko 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade Jackson to 2.4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6757</link><project id="" key="" /><description /><key id="37227355">6757</key><summary>Upgrade Jackson to 2.4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-06T20:30:31Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-07-07T07:50:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-07T07:48:58Z" id="48149441">LGTM, changelogs look good (especially the performance regression fix in jackson-core)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve large bytes request handling by detecting content composite buffer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6756</link><project id="" key="" /><description>There is a special type of request that tries to not allocate another buffer when sending bytes request (used by the public cluster state action). With the new pages bytes reference support, the content can already be a composite channel buffer, take that into account when building the actual composite buffer that will be sent over the network
</description><key id="37223324">6756</key><summary>Improve large bytes request handling by detecting content composite buffer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-06T17:22:13Z</created><updated>2015-06-07T12:59:49Z</updated><resolved>2014-07-09T19:40:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-07T08:49:16Z" id="48154141">left some comments 
</comment><comment author="s1monw" created="2014-07-08T16:37:21Z" id="48365092">replied to your answers
</comment><comment author="kimchy" created="2014-07-08T18:10:18Z" id="48377750">@s1monw cool, applies the changes. The change in MappingMetaData is needed because equals on MappingMetaData#Timestamp was broken, which I added a test for
</comment><comment author="s1monw" created="2014-07-09T19:20:36Z" id="48521643">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hide internal versions of dependencies from IDE auto-import</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6755</link><project id="" key="" /><description>Elasticsearch packages many dependencies. E.g. `com.google.common.base.Preconditions` can also be found at `org.elasticsearch.common.base.Preconditions` This leads to folks on my team often accidentally importing the wrong class and a frustrating experience including elasticsearch in our project.

These can be hidden from IDE auto-import. [Google Guice has done the same thing](https://code.google.com/p/google-guice/wiki/Guice40), so it would be a great example to look at:

&gt; Many things inside com.google.inject.internal changed and/or moved. This is especially true for repackaged Guava (formerly Google Collections), cglib, and asm classes. All these classes are now hidden from IDE auto-import suggestions and are in new locations. You will have to update your code if you relied on any of these classes. 
</description><key id="37221909">6755</key><summary>Hide internal versions of dependencies from IDE auto-import</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">benmccann</reporter><labels><label>feedback_needed</label></labels><created>2014-07-06T15:57:49Z</created><updated>2015-10-30T21:01:23Z</updated><resolved>2015-10-30T21:01:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-06T16:00:45Z" id="48115473">@benmccann we can, when we shade the dependencies, to do it into an "internal" package, but how did they managed to hide it from IDE imports? I haven't looked at Guice yet, but maybe you know.
</comment><comment author="benmccann" created="2014-07-06T20:45:56Z" id="48126560">Just asked the Guice folks and they said the repackaged classes are hidden by renaming to the something beginning with a $, through a jarjar renaming rule.  Here's the portion from the [ant buildfile](https://code.google.com/p/google-guice/source/browse/common.xml#139) &amp; the [maven pom](https://code.google.com/p/google-guice/source/browse/core/pom.xml#251).
</comment><comment author="kimchy" created="2014-07-06T20:47:36Z" id="48126600">@benmccann I see, I thought something like that. I would let other weigh in one downside is the fact that some plugins rely on this shading of guava (for example) for their dependencies....
</comment><comment author="dakrone" created="2015-10-30T21:01:23Z" id="152650201">Closing this as we no longer shade dependencies.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New entry/front-end</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6754</link><project id="" key="" /><description>Adding, Calaca, a simple search client for Elasticsearch.
</description><key id="37218623">6754</key><summary>New entry/front-end</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">romansanchez</reporter><labels /><created>2014-07-06T12:46:46Z</created><updated>2014-07-09T14:37:17Z</updated><resolved>2014-07-09T14:37:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-07T08:19:49Z" id="48151727">Duplicate of #6541
</comment><comment author="clintongormley" created="2014-07-07T08:21:53Z" id="48151908">@mvg - this was the version with the corrected casing of Elasticsearch, so I'm reopening and closing the other.

@romansanchez As mentioned in the other ticket, please can you sign the CLA so that we can get this in: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="romansanchez" created="2014-07-07T12:54:21Z" id="48174146">I've signed CLA.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Add assertBusy helper test method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6753</link><project id="" key="" /><description>We use awaitBusy in our tests, the problem is that we have to check if it awaited or not, and then try and keep around somehow more info around why the predicate failed and a timeout happened.
The idea of assertBusy is to allow to simply write "regular" test code, and if the test code trips, it will busy wait till a timeout. This allows us to keep around the assertion information and properly throw it for information that is inherently kept in the failure itself.
</description><key id="37215871">6753</key><summary>Test: Add assertBusy helper test method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-06T10:35:08Z</created><updated>2014-07-16T11:36:57Z</updated><resolved>2014-07-08T17:09:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-06T10:36:09Z" id="48108092">@s1monw would love to brain storm about this, potentially, this can replace completely `awaitBusy`, and help our tests with more info / less code, but want to make sure it makes sense first.
</comment><comment author="s1monw" created="2014-07-07T08:51:28Z" id="48154330">I think this can make sense though. We just have to be careful that we don't make use of it where we actually have an API or we should add one? Can you point me to a place where you wanna use it?
</comment><comment author="kimchy" created="2014-07-07T08:58:08Z" id="48154923">@s1monw agreed on adding APIs where needed. I actually want to replace all awaitBusy with assertBusy eventually, since it will give us much more information on why the await failed, and fail if the await failed. Now, if you will see, we hack around awaitBusy in some places to keep the last response around to have more details on why the await failed, with assertBusy, we won't have to, the failure will (or should) include enough information on why the assertion failed in the assert failure.
</comment><comment author="s1monw" created="2014-07-07T08:59:47Z" id="48155050">sure thing... can you maybe replace one or two places so we can see it?
</comment><comment author="kimchy" created="2014-07-07T10:03:56Z" id="48160712">@s1monw added usage of it in the integration base class, seems cleaner to me now, with much more info on why an away would fail
</comment><comment author="s1monw" created="2014-07-08T16:38:40Z" id="48365276">LGTM
</comment><comment author="kimchy" created="2014-07-08T17:09:58Z" id="48369632">thanks!, pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Windows: Modify command window title (windows)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6752</link><project id="" key="" /><description>When launching multiple nodes in MS Windows env, batch command window does not come by default with any title.

This patch add `Elasticsearch VERSION` as a title, where VERSION is the current elasticsearch version.

Closes #6336
</description><key id="37215093">6752</key><summary>Windows: Modify command window title (windows)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-06T09:37:04Z</created><updated>2015-06-07T13:00:07Z</updated><resolved>2014-07-09T16:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-06T09:38:03Z" id="48107170">@costin Wanna review it?
</comment><comment author="costin" created="2014-07-07T08:50:52Z" id="48154270">LGTM :) Potentially we can extend this to `service.bat` as well.
</comment><comment author="costin" created="2014-07-07T08:51:51Z" id="48154366">@Mpdreamz any other goodies we can add? Potentially change the window's color based on the error codes ? :)
</comment><comment author="s1monw" created="2014-07-07T08:55:59Z" id="48154731">this should not go into `1.2.2` it's not a bugfix
</comment><comment author="s1monw" created="2014-07-09T12:09:46Z" id="48461518">@dadoonet wanna get it in?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need some ideas: Getting visits from hits out of logstash index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6751</link><project id="" key="" /><description>### Problem:

I have aggregated accesslog data from different webservers in a large logstash index. My goal is to get the page _visits_ out of the accesslog hits.

A _visit_ is defined as following: A visit results out of one or more hits from a single ip address in a specific time frame. Due to different products on the webservers each domain should be considered separately.
### My questions are:
- Can this problem already be solved with build-in elasticsearch features? If **yes**, how?
- If **no**:
  - What kind of plugin would you suggest?

My own considerations lead from building a custom filter to retrieve just the data I need, to build a plugin which analyses the accesslog index and put the visit-data into a new index.

Maybe someone can help me? I appreciate every answer. Thank you for your time!
</description><key id="37214640">6751</key><summary>Need some ideas: Getting visits from hits out of logstash index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stha</reporter><labels /><created>2014-07-06T09:04:50Z</created><updated>2014-07-06T09:09:39Z</updated><resolved>2014-07-06T09:09:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-06T09:09:32Z" id="48106646">Please ask questions on the mailing list. We and the community can help you there.

This space is used for issues or feature requests after beeing discussed on the mailing list.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] YAML mappings use colons, not equals sign</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6750</link><project id="" key="" /><description>YAML  mappings use a colon and space (`:`) to mark each key: value pair, not the equals sign (`=`)

Also,
- `elasticsearch.yml` is a file, not a directory
- modify the AsciiDoc language attribute
</description><key id="37213143">6750</key><summary>[DOCS] YAML mappings use colons, not equals sign</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mrorii</reporter><labels /><created>2014-07-06T07:00:41Z</created><updated>2014-07-08T14:47:13Z</updated><resolved>2014-07-08T11:18:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-07T07:22:04Z" id="48147688">Hi @mrorii 

Thanks for the fix. Please could I ask you to sign our CLA so that I can get your PR merged in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="mrorii" created="2014-07-07T16:25:37Z" id="48201876">Thanks, I've signed the CLA
</comment><comment author="clintongormley" created="2014-07-08T11:18:27Z" id="48323855">Thanks @mrorii. Merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] YAML mappings use colons, not equals sign</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6749</link><project id="" key="" /><description>YAML  mappings use a colon and space (`:`) to mark each key: value pair, not the equals sign (`=`)

Also,
- `elasticsearch.yml` is a file, not a directory
- modify the AsciiDoc language attribute
</description><key id="37213065">6749</key><summary>[DOCS] YAML mappings use colons, not equals sign</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mrorii</reporter><labels /><created>2014-07-06T06:54:52Z</created><updated>2014-07-06T06:56:35Z</updated><resolved>2014-07-06T06:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrorii" created="2014-07-06T06:56:35Z" id="48104625">Sorry, somehow I must have mistakenly chosen `1.x` instead of `master`.
Will close and re-open a separate PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Call callback on actual mapping processed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6748</link><project id="" key="" /><description>only callback the registered callback listeners when mapping have actually been processed...
</description><key id="37208789">6748</key><summary>Call callback on actual mapping processed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-06T00:20:10Z</created><updated>2015-06-07T13:00:19Z</updated><resolved>2014-07-07T07:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-06T08:00:10Z" id="48105542">Left one minor comment o.w. LGTM
</comment><comment author="kimchy" created="2014-07-06T10:43:53Z" id="48108298">@bleskes replied, I think not spawning makes more sense (in retrospect, the previous code did spawn)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove intern calls on FieldMapper#Names for better performance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6747</link><project id="" key="" /><description>remove internal callas on FieldMappers#Names, we properly reuse FieldMapper, so there is no need to try and call intern in order to reuse the names. This can be heavy with many fields and continuous mapping parsing.
</description><key id="37202846">6747</key><summary>Remove intern calls on FieldMapper#Names for better performance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-05T17:31:48Z</created><updated>2015-06-07T13:00:32Z</updated><resolved>2014-07-08T18:18:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-08T08:21:10Z" id="48284310">LGTM, I quickly looked if there were identify checks for field mappers names and couldn't find one.
</comment><comment author="s1monw" created="2014-07-08T16:39:02Z" id="48365325">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding Tiki Wiki CMS Groupware</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6746</link><project id="" key="" /><description /><key id="37198771">6746</key><summary>Adding Tiki Wiki CMS Groupware</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">marclaporte</reporter><labels /><created>2014-07-05T13:18:25Z</created><updated>2014-07-23T09:01:29Z</updated><resolved>2014-07-23T09:01:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-07T08:12:48Z" id="48151204">Hi @marclaporte 

Thanks for the commit. Please could I ask you to sign the CLA so that we can get it merged in.
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-07-23T08:57:01Z" id="49848770">No CLA signed.  Treating as bug report
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`bin/plugin` removes itself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6745</link><project id="" key="" /><description>I was experimenting a bit with plugins and created a test plugin and called it "es-plugin". Maybe this was not the best choice for a name but i did it.

If you call `bin/plugin --remove es-plugin` the plugin got removed but the file `bin/plugin` itself was also deleted.
</description><key id="37198521">6745</key><summary>`bin/plugin` removes itself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">stha</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-05T13:00:02Z</created><updated>2015-06-07T19:31:40Z</updated><resolved>2014-07-17T06:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-05T13:59:45Z" id="48087062">o_O thanks for reporting it!
Will fix that! Thanks
</comment><comment author="s1monw" created="2014-07-09T10:19:31Z" id="48452904">@dadoonet what  is the status here?
</comment><comment author="dadoonet" created="2014-07-09T18:15:05Z" id="48512923">@s1monw I checked it and I need to protect all known `bin/*` files:
- elasticsearch
- elasticsearch-service-x86.exe
- plugin
- elasticsearch-service-mgr.exe
- elasticsearch.bat
- plugin.bat
- elasticsearch-service-x64.exe
- elasticsearch.in.sh
- service.bat

My first intention was to remove the support of installing plugins in `bin` dir but it sounds like we still need it.

Will send a PR probably on friday.
</comment><comment author="dadoonet" created="2014-07-10T17:41:16Z" id="48638303">@s1monw PR opened here: #6817 waiting for review. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve pending api to include current executing class</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6744</link><project id="" key="" /><description>the pending tasks api will now include the current executing tasks (with a proper marker boolean flag)
this will also help in tests that wait for no pending tasks, to also wait till the current executing task is done
</description><key id="37198454">6744</key><summary>Improve pending api to include current executing class</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-05T12:55:29Z</created><updated>2015-06-07T13:00:45Z</updated><resolved>2014-07-05T15:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-05T13:00:09Z" id="48085945">left a small comment other than than LGTM....

oh the PR needs labels ;)
</comment><comment author="kimchy" created="2014-07-05T14:15:34Z" id="48087366">@s1monw put labels, and replied to your comment
</comment><comment author="s1monw" created="2014-07-05T15:38:08Z" id="48089230">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove Queries#optimizeQuery - already handled in BooleanQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6743</link><project id="" key="" /><description>This method tires to optimize boolean queries if there is only
one clause. Yet BooleanQuery already does that internally This
optimization is unneeded.
</description><key id="37194359">6743</key><summary>Remove Queries#optimizeQuery - already handled in BooleanQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-05T07:55:45Z</created><updated>2015-06-07T13:00:59Z</updated><resolved>2014-07-05T11:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-05T09:28:38Z" id="48082160">LGTM
</comment><comment author="kimchy" created="2014-07-05T09:38:19Z" id="48082329">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set a default of 5m to `recover_after_time` when any of the `expected*Nodes` is set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6742</link><project id="" key="" /><description>The `recovery_after_time` tells the gateway to wait before starting recovery from disk. The goal here is to allow for more nodes to join the cluster and thus not start potentially unneeded replications. The `expectedNodes` setting (and friends) tells the gateway when it can start recovering even if the `recover_after_time` has not yet elapsed. However, `expectedNodes` is useless if one doesn't set `recovery_after_time`. This commit changes that by setting a sensible default of 5m for `recover_after_time` _if_ a `expectedNodes` setting is present.
</description><key id="37185538">6742</key><summary>Set a default of 5m to `recover_after_time` when any of the `expected*Nodes` is set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T20:31:20Z</created><updated>2015-06-07T13:01:16Z</updated><resolved>2014-07-11T09:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-09T12:53:45Z" id="48465570">LGTM, except for a small comment on the test
</comment><comment author="bleskes" created="2014-07-11T09:30:11Z" id="48711181">I moved the cluster service noop class to our test infra package and pushed. Thx.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a best effort waiting for ongoing recoveries to cancel on close</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6741</link><project id="" key="" /><description>Currently one can close the engine while there are still on going recoveries. This is not a problem because the engine is close in tandem with the shard it belongs to, which in turn cancels the recoveries. This does cause some issues in our tests as we check the no resources were left behind after an index was deleted, which trips if the recoveries are not canceled.
</description><key id="37184778">6741</key><summary>Add a best effort waiting for ongoing recoveries to cancel on close</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T19:58:32Z</created><updated>2015-06-07T13:01:27Z</updated><resolved>2014-09-08T20:20:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T10:25:45Z" id="48453394">I don't understand this PR - why do we need to wait on the recovery and which resources are you talking about here?
</comment><comment author="bleskes" created="2014-07-09T12:47:25Z" id="48464924">push to 1.4 pending more discussion
</comment><comment author="s1monw" created="2014-09-08T15:43:40Z" id="54840194">@bleskes I think we should have a dedicated API for this that our tests can all before the index is deleted. I think it can be useful even in production when you want to wait until everything is stable. We might be able to extend / leverage the ClusterHealth API to do this - it might be already capable of doing this?
</comment><comment author="bleskes" created="2014-09-08T20:02:27Z" id="54878295">@s1monw the problem is that the master may _think_ things are done but the node have not yet completed acting on it. We can add something that checks all the nodes, but it feels like an over kill. 

I think we should just close this PR until we find a better solution. Agreed?
</comment><comment author="s1monw" created="2014-09-08T20:09:55Z" id="54879314">agreed :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fetch text from all possible fields if none are specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6740</link><project id="" key="" /><description>Items with no specified field now defaults to all the possible fields from the
document source. Previously, we had required 'fields' to be specified either
as a top level parameter or for each item. The default behavior is now similar
to the MLT API for each item.
</description><key id="37183849">6740</key><summary>Fetch text from all possible fields if none are specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T19:27:38Z</created><updated>2015-06-07T13:01:50Z</updated><resolved>2014-08-21T17:34:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2014-08-21T17:34:33Z" id="52954479">closed, replaced by #7382
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add HierarchyCircuitBreakerService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6739</link><project id="" key="" /><description>Adds a breaker for request BigArrays, which are used for parent/child
queries as well as some aggregations. Certain operations like Netty HTTP
responses and transport responses increment the breaker, but will not
trip.

This also changes the output of the nodes' stats endpoint to show the
parent breaker as well as the fielddata and request breakers.

There are a number of new settings for breakers now:

`indices.breaker.total.limit`: starting limit for all memory-use breaker,
defaults to 70%

`indices.breaker.fielddata.limit`: starting limit for fielddata breaker,
defaults to 60%
`indices.breaker.fielddata.overhead`: overhead for fielddata breaker
estimations, defaults to 1.03

(the fielddata breaker settings also use the backwards-compatible
setting `indices.fielddata.breaker.limit` and
`indices.fielddata.breaker.overhead`)

`indices.breaker.request.limit`: starting limit for request breaker,
defaults to 40%
`indices.breaker.request.overhead`: request breaker estimation overhead,
defaults to 1.0

The breaker service infrastructure is now generic and opens the path to
adding additional circuit breakers in the future.

Fixes #6129
</description><key id="37171181">6739</key><summary>Add HierarchyCircuitBreakerService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Circuit Breakers</label><label>breaking</label><label>enhancement</label><label>release highlight</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T14:18:26Z</created><updated>2015-06-06T16:47:23Z</updated><resolved>2014-07-28T09:45:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-04T14:19:43Z" id="48049208">Note that this is currently marked as a breaking change because I've changed the way that `/_node/stats` returns circuit breaker information. I think the new way of representing it is much better, but I'd like to discuss whether we want to add additional work to this in order to preserve the old node stats format as well.
</comment><comment author="jpountz" created="2014-07-08T10:03:25Z" id="48299839">The settings start with `indices.breaker` while Java packages are `common.breaker` and `indices.memory.breaker`. Should we put Java code in the `indices.breaker` package so that the logger name matches the configuration options?
</comment><comment author="dakrone" created="2014-07-09T09:13:11Z" id="48446844">@jpountz I agree it would fit better there, I've moved the classes and incorporated the other feedback you asked for as well.
</comment><comment author="s1monw" created="2014-07-09T10:20:24Z" id="48452971">moving out to 1.4 for now
</comment><comment author="jpountz" created="2014-07-16T13:00:19Z" id="49161438">This looks better but I'm not very happy with this boolean on every BigArrays method. I would rather have one instance that does accounting+tripping and another one that would only do accounting. API-wise maybe what we need is for the singleton BigArrays (the one created by Guice) to only do accounting (no breaking), and then have a `BigArrays withCircuitBreaker(CircuitBreaker breaker)` method that would return a `BigArrays` instance on the same page recycler but that would do circuit-breaking with the provided breaker?

Regarding circuit breaking, I'd like to see if we can make it simpler. For instance, do we actually need overheads, limits or the ability to disable distribution?

I think we also need to have logic in `ElasticsearchIntegrationTest` that makes sure that all breakers (but the one that is used for networking because of the shared cluster) don't use any bytes after a test finishes.
</comment><comment author="dakrone" created="2014-07-17T09:48:07Z" id="49285491">&gt; ... have a BigArrays withCircuitBreaker(CircuitBreaker breaker) method that would return a BigArrays instance on the same page recycler but that would do circuit-breaking with the provided breaker?

I think this would work and make it a bit simpler, I'll try to do this.

&gt; Regarding circuit breaking, I'd like to see if we can make it simpler. For instance, do we actually need overheads, limits or the ability to disable distribution?

I think we can get rid of maximum sizes and just use the limit as the max, but keep minimum sizes for breakers (so you can force space to always be available for some field data or aggregations).

For overhead, it's a nob in the event that the field data estimations don't estimate correctly, so I think it's important to keep. It's not used for BigArrays though, so it's set to 1.0. I'm not sure it (overheads) should be removed either.

&gt; I think we also need to have logic in ElasticsearchIntegrationTest that makes sure that all breakers (but the one that is used for networking because of the shared cluster) don't use any bytes after a test finishes.

I added the logic to do this already, see: https://github.com/elasticsearch/elasticsearch/pull/6739/files#diff-e6761c31a9d75e74580a93c5aeb3aaa3R198
</comment><comment author="jpountz" created="2014-07-17T10:04:11Z" id="49287343">&gt; I think we can get rid of maximum sizes and just use the limit as the max, but keep minimum sizes for breakers (so you can force space to always be available for some field data or aggregations).

Agreed on minimum sizes, they are important.

&gt; For overhead, it's a nob in the event that the field data estimations don't estimate correctly.

But does it really help to overestimate memory usage reports by 3%?

&gt; I added the logic to do this already, see: https://github.com/elasticsearch/elasticsearch/pull/6739/files#diff-e6761c31a9d75e74580a93c5aeb3aaa3R198

I had missed it, great!
</comment><comment author="dakrone" created="2014-07-17T10:06:35Z" id="49287617">&gt; But does it really help to overestimate memory usage reports by 3%?

The additional 3% is to err on the side of estimating too high rather than too low. I can change the estimation function to include the extra 3%, but having a configurable value in case my estimation isn't accurate for some people is important in my opinion, since it's dynamically changeable and the estimation formula is not.
</comment><comment author="jpountz" created="2014-07-17T10:32:34Z" id="49290200">Is it something that could be done outside of the circuit-breaking API only for field data? In the `PerValueEstimator` impls?
</comment><comment author="dakrone" created="2014-07-17T14:35:48Z" id="49314977">@jpountz I added the change to add `.withCircuitBreaker()` to BigArrays instead of passing through the boolean value, it's definitely cleaner. Let me know if this is what you meant.

Still working on the other changes.
</comment><comment author="dakrone" created="2014-07-18T11:26:55Z" id="49420871">@jpountz pushed a couple of new commits that remove the concept of a `limit` and uses `min` and `max` only.

I also removed configuring the Parent breaker entirely (now its only purpose is to track the total memory usage across all breakers and the number of times the breaker tripped and wasn't able to redistribute). I think this makes it a lot simpler.
</comment><comment author="dakrone" created="2014-07-18T13:36:46Z" id="49431086">&gt; Is it something that could be done outside of the circuit-breaking API only for field data? In the PerValueEstimator impls?

So the issue with this is that the `PerValueEstimator` can use really small amounts (like 8 or 16) for numeric terms and multiplying by 1.03 in estimation means they value is still rounded back to 8 or 16. I originally made these numbers doubles instead of longs, but not losing precision was terrible, which is why it's back to using the `overhead` parameter.
</comment><comment author="jpountz" created="2014-07-21T09:23:13Z" id="49585942">@dakrone Agreed that it is much simpler and on the need to keep the overhead! Thanks!

I left some minor comments about code cosmetics. The last thing I'd like to discuss is the redistribution logic, but other than that, this looks good to me!
</comment><comment author="dakrone" created="2014-07-23T08:53:21Z" id="49848449">@jpountz I think I've addressed all of the comments.

I also changed this entirely to use the parent-check method we talked about. Instead of redistributing the children's limits, each child has a set limit while still checking the global ("parent") breaker hasn't been tripped.

Changed the default limits to 60% for field data and 40% for bigarrays/requests, overall limit is 70%.
</comment><comment author="jpountz" created="2014-07-24T07:05:32Z" id="49974256">@dakrone Left minor comments. I think it's close now.
</comment><comment author="jpountz" created="2014-07-24T17:58:53Z" id="50054253">LGTM
</comment><comment author="dakrone" created="2014-07-25T12:20:10Z" id="50141574">Squashed this, rebased on master and resolved all the conflicts (there were quite a few since this PR was opened 21 days ago and we released a new version of ES between now and then).

I also added documentation and changed the version check to be 1.4.0 instead of 2.0.0.

I'm planning on merging this Monday since it received Adrien's +1 if there are no more comments by then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed query in the example for the histogram aggs page</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6738</link><project id="" key="" /><description>In the query example added missing "filter" key around the actual filter. Closes #6737
</description><key id="37170290">6738</key><summary>Fixed query in the example for the histogram aggs page</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">gakhov</reporter><labels /><created>2014-07-04T14:05:05Z</created><updated>2014-07-07T08:44:33Z</updated><resolved>2014-07-07T08:44:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T15:34:39Z" id="48056634">Hi @gakhov 

Thanks for the fix. Please could I ask you to sign the CLA so we can merge your change in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="gakhov" created="2014-07-04T15:44:23Z" id="48057270">just signed ;)
</comment><comment author="clintongormley" created="2014-07-07T08:44:22Z" id="48153696">many thanks, merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong query example for histogram aggregations doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6737</link><project id="" key="" /><description>On the _histogram aggregations_ documentation page example has an incorrect query. The filter in a filtered query should be under "filter" key inside "filtered", but it appears under top "filtered" key.
How it's now:
`
"filtered" : { "range" : { "price" : { "to" : "500" } } }
`

How it should be:
`
"filtered" : { "filter": { "range" : { "price" : { "to" : "500" } } } }
`
</description><key id="37169901">6737</key><summary>Wrong query example for histogram aggregations doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">gakhov</reporter><labels /><created>2014-07-04T13:59:30Z</created><updated>2014-07-09T09:55:47Z</updated><resolved>2014-07-09T09:55:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-09T09:55:47Z" id="48450759">Closed by #6738 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Define valid index, type, field, id, routing values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6736</link><project id="" key="" /><description>Currently we have no specification of allowed values for index names, type names, IDs, field names or routing values.

This issue is an attempt to document and improve the existing specs to prevent inconsistencies.
## Index names

Index names are limited by the file system.  They may only be lower case, and my not start with an underscore.  While we don't prevent index names starting with a `.`, we reserve those for internal use. Clearly, `.` and `..` cannot be used.

These characters are already illegal: `\`, `/`, `*`, `?`, `"`, `&lt;`, `&gt;`, `|`, ` `,`,`.  We should also add the null byte.

There are other filenames which are illegal in Windows, but we probably don't need to check for those.
## Type names

Type names can contain any character (except null bytes, which currently we don't check) but may not start with an underscore.  
## IDs

IDs can contain any character (except null bytes, which currently we don't check). IDs should not begin with an underscore.

Currently IDs are not checked for underscores and IDs with underscores may exist.  These can clash with eg `_mapping` and so should be prevented.  This is a backwards incompatible change.
## Routing &amp; Parent

Routing and parent values should be the same as IDs, ie any chars except for the null byte.  The problem is that multiple routing values are passed in the query string as comma-separated values, eg `?routing=foo,bar`.

If a single routing value contains a comma, it will be misinterpreted as two routing values.  One idea is to pass multiple routing values as eg `?routing=foo&amp;routing=bar,baz`. Unfortunately, this is not backwards compatible and isn't supported by a number of client libraries.

The only solution I can think of is to support some escaping of commas, eg `foo\,bar`.  This would mean that `\` would need to be escaped as well, ie: `foo\bar` -&gt; `foo\\bar`.  Support for this escaping would need to be added to Elasticsearch and to the client libraries. 
</description><key id="37167454">6736</key><summary>Define valid index, type, field, id, routing values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>discuss</label><label>enhancement</label></labels><created>2014-07-04T13:23:32Z</created><updated>2016-06-09T15:43:10Z</updated><resolved>2014-12-24T13:12:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-04T15:08:47Z" id="48053900">Should ensure that if any global discisions are made regarding naming that the aggregation names are also included
</comment><comment author="bleskes" created="2014-07-04T19:47:07Z" id="48067501">one more relevant point is that some of our endpoints mask what is a valid get doc by id REST request (according to the above spec). For example: `GET index/type/_mapping` (which masks a document where id is `_mapping`).  IMHO this is not a problem, but we should mention it for completeness.
</comment><comment author="clintongormley" created="2014-07-07T08:10:29Z" id="48151024">@bleskes i think it is a problem as there is no workaround. I've added this sentence to the original issue: "IDs should not begin with an underscore."
</comment><comment author="clintongormley" created="2014-07-09T10:07:13Z" id="48451854">Adding field names to the specs (see #5972).  Field names should not begin with an underscore, contain `.` or null bytes.

If a fieldname contains `.` when creating a mapping, we have two choices:
- throw an error
- convert it to an object eg `foo.bar: 5` -&gt; `{ foo: { bar: 5}}``

Throwing an error seems a more transparent way of dealing with this.
</comment><comment author="danielcweeks" created="2014-07-31T19:06:44Z" id="50804272">Having a dot in the field name is actually very useful.  Would it be possible to use an escape for referencing a field name instead of path?
</comment><comment author="clintongormley" created="2014-08-01T07:42:10Z" id="50857721">@dcw-netflix an escape?  do you mean `foo\.bar`?  Yes we can probably support that.
</comment><comment author="danielcweeks" created="2014-08-01T15:47:14Z" id="50900085">Yes, that would be perfect.  The reason is that there are a lot of use cases where property/config files get indexed, which results in many dot separated keys.
</comment><comment author="clintongormley" created="2014-08-04T15:20:39Z" id="51074333">Colons in index names are also invalid. See https://github.com/elasticsearch/elasticsearch/issues/7148
</comment><comment author="uboness" created="2014-08-13T10:54:09Z" id="52034128">This is a great start for having a format input validation rules in elasticsearch. I believe we need to centralize all these rules in one place. I also think we should have validation rules for every input in es (not just those listed above)... for example: field names, repository names, snapshot names, etc... basically everything that in one way or another can compromise the consistent state of the cluster. 

We currently have a lot of this logic (probably incomplete) scattered in different places, it's definitely time to formalize them (both in docs &amp; code)
</comment><comment author="dadoonet" created="2014-08-13T11:32:34Z" id="52037010">Note that for repository names, we also need to delegate the validation to plugins as there can be other rules with some cloud providers (azure for example). See also #7096 
</comment><comment author="ron-totango" created="2014-10-30T06:04:47Z" id="61049384">We have an issue with routing value with comma. Any workaround we should use? Thanks
</comment><comment author="rpedela" created="2014-10-30T23:55:22Z" id="61191156">A common use case for ES (and my use case) is to index a DB table which may have column names that start with an underscore. Renaming the columns is not an option in my use case as well. Currently this requires storing a mapping between DB column names and ES field names which adds complexity.

Is it possible to escape an underscore in a field name? Or more generally is it possible to escape any special character in a field name? A more general escaping solution would be optimal in my opinion because then a field name could have any arbitrary characters just like a quoted SQL identifier.
</comment><comment author="clintongormley" created="2014-12-24T13:12:38Z" id="68051425">Closing in favour of #9059
</comment><comment author="mcayland" created="2015-12-07T15:22:15Z" id="162555749">I've just come across this problem in the past week with ES 2.1 whilst trying to create documents with "."s in the field name. Am I correct in that even the field name escaping parts aren't included in ES 2.1? This is sadly a showstopper for our application as the field names we use are equipment serial codes, and we've recently added a supplier that includes "."s in their serial codes.
</comment><comment author="clintongormley" created="2015-12-15T11:32:41Z" id="164736569">@mcayland using serial numbers for field names is a bad design choice as you will end up with sparse fields, and much more disk usage than you actually need.
</comment><comment author="roisin-jin" created="2016-06-09T15:41:25Z" id="224935881">hi, I'm wondering is there any other wildcard characters allowed in the template names apart from the star symbol? We have several indexes named by the same pattern, i.e: ap-YYYY-MM, bg-YYYY-MM, cm-YYYY-MM, etc. And they all have the same mapping, we just want separate those data into different indexes. Is there anyway to create a single template with index name pattern like '??-*' ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removing plugin does not fail when plugin dir is read only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6735</link><project id="" key="" /><description>If you try to remove a plugin in read only dir, you get a successful result:

```
$ bin/plugin --remove marvel
-&gt; Removing marvel
Removed marvel
```

But actually the plugin has not been removed.

When installing, if fails properly:

```
$ bin/plugin -i elasticsearch/marvel/latest
-&gt; Installing elasticsearch/marvel/latest...

Failed to install elasticsearch/marvel/latest, reason: plugin directory /usr/local/elasticsearch/plugins is read only
```

This change throw an exception when we don't succeed removing the plugin.

Closes #6546.
</description><key id="37167352">6735</key><summary>Removing plugin does not fail when plugin dir is read only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T13:21:49Z</created><updated>2015-06-07T19:40:27Z</updated><resolved>2014-07-09T08:26:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-04T15:14:55Z" id="48054573">LGTM
</comment><comment author="s1monw" created="2014-07-08T16:43:01Z" id="48365825">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Reset all cluster if a test hit a failure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6734</link><project id="" key="" /><description /><key id="37165958">6734</key><summary>Test: Reset all cluster if a test hit a failure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T12:59:02Z</created><updated>2014-09-16T10:49:19Z</updated><resolved>2014-07-04T13:53:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-04T13:08:19Z" id="48042158">LGTM but I'd love someone who knowns the test failure semantics / clean up code flow to have a look. I don't know that area.
</comment><comment author="s1monw" created="2014-07-04T13:22:04Z" id="48043237">I think this is the only way to do it actually so not sure if we have much of a choice here..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The query_string cache should returned cloned Query instances.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6733</link><project id="" key="" /><description>The query string cache can't return the same instance, since Query is mutable changing the query else where in the execution path changes the instance in the cache too.

Fixes #2542
</description><key id="37165107">6733</key><summary>The query_string cache should returned cloned Query instances.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T12:46:16Z</created><updated>2015-06-07T19:31:53Z</updated><resolved>2014-07-04T13:07:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-07-04T12:47:58Z" id="48040039">+1, looks good
</comment><comment author="jpountz" created="2014-07-04T12:49:16Z" id="48040149">LGTM
</comment><comment author="s1monw" created="2014-07-04T12:49:20Z" id="48040156">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve cluster/index settings </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6732</link><project id="" key="" /><description>Currently there is no way of seeing all settings that are enforced.  For instance, defaults are not available and settings from the `config/elasticsearch.yml` are not available.

Any setting which is different from the default should be returned by these APIs:

```
GET /{index}/_settings
GET /_cluster/settings
```

Both should also support the `include_defaults` query string parameter, which would return all settings, including the defaults.

Also, deleting a setting should reset the setting to the default.

Setting an unknown setting, or a setting that can't be changed should throw an error.

Relates to #5018, #3670, #2738, #2730, #3572, #2628, #3671, #5019, #6309
</description><key id="37164487">6732</key><summary>Improve cluster/index settings </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Settings</label><label>enhancement</label><label>Meta</label><label>release highlight</label><label>v5.0.0-alpha1</label></labels><created>2014-07-04T12:35:18Z</created><updated>2016-03-25T12:47:33Z</updated><resolved>2016-01-19T11:26:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaytaylor" created="2014-07-16T22:57:30Z" id="49238947">How will this address https://github.com/elasticsearch/elasticsearch/issues/3572?  Not being able to remove/reset a per-index setting can be problematic and causes pain.
</comment><comment author="clintongormley" created="2014-07-17T09:34:29Z" id="49283923">@jaytaylor Did you read this line? 

```
Also, deleting a setting should reset the setting to the default.
```
</comment><comment author="jaytaylor" created="2014-07-17T19:25:51Z" id="49352287">Got it, I missed that.  Thanks Clinton!
</comment><comment author="georgi0u" created="2014-07-23T00:59:02Z" id="49821157">Similarly, there doesn't seem to be a way to update settings, _without merging them_. e.g., If I have an analyzer defined in my settings, and want to change the `"char_filter"` value, which happens to be a list, I can add something to said list, but I can't remove anything. 

This seems to be an issue with not being able to select or define a merge policy for these kinds of updates, as opposed to being able to strictly delete something, so I'm hoping this comment is relevant.
</comment><comment author="clintongormley" created="2014-07-23T10:35:30Z" id="49857720">@georgi0u good point.  
</comment><comment author="georgi0u" created="2014-07-23T14:24:55Z" id="49880603">Also, I'd like to make an argument for this being a more severe issue than meets the eye.

In my experience, if you add a _corrupt_ setting to an index -- e.g., an `analyzer` which references a `char_filter` that doesn't exist, perhaps because you spelled it wrong -- then you've put that index into a corrupt state and ElasticSearch won't allocate the shards when you try to re-open that index.

Not being able to modify the setting means that you need to reindex everything with brand new settings in order to recover your index. If you happen to run into this issue when targeting `_all` indexes, things get a lot worse.

_A temporary workaround, specifically in the case of analyzers: if you switch the `type` from `custom` to `standard` then ElasticSearch seems to ignore all of its settings, and thus your spelling mistake looses its ability to corrupt the index, but there's still a hacky, pointless analyzer in your settings._
</comment><comment author="robusto" created="2014-09-10T19:07:32Z" id="55166534">@georgi0u Thanks for that workaround. I've been wasting time recreating indexes while testing out custom analyzers and it was preventing me from re-opening my index. 
</comment><comment author="esetnik" created="2014-11-03T16:50:13Z" id="61508170">In elasticsearch v1.3.4 stable is there any safe way to delete a specific persistent cluster setting?
</comment><comment author="clintongormley" created="2014-11-04T08:45:38Z" id="61609074">Also see #6887
</comment><comment author="avleen" created="2014-12-15T16:36:26Z" id="67022389">Hi Clinton,
You closed #3671 but didn't mention which settings take priority: persistent or transient.
Is there any information about this particular bit? Or does it depend on which setting was applied to the cluster most recently?
</comment><comment author="clintongormley" created="2014-12-15T17:14:19Z" id="67028978">Hi @avleen 

From testing it looks like `transient` takes precedence over `persistent`.
</comment><comment author="avleen" created="2014-12-15T17:27:47Z" id="67031091">Thanks!
</comment><comment author="clintongormley" created="2014-12-17T12:43:02Z" id="67316856">Would it be possible, as part of this change, to (eg) keep boolean settings as real booleans, rather than as the strings "true" or "false"?  See https://github.com/elasticsearch/elasticsearch/issues/7265#issuecomment-67308834
</comment><comment author="matthuhiggins" created="2014-12-25T04:13:54Z" id="68086166">So what is the way to delete a persistent setting? In my case, I want to restore threadpool settings to defaults.
</comment><comment author="linkwoman" created="2015-02-11T05:46:19Z" id="73836884">@georgi0u Thank you so much for that explanation and workaround.  I made a copy/paste mistake and nested another set of analysis, etc. inside of analysis!  I did the following to set the 'analysis' analyzer (doh!) to "type": "standard" and now can get my index open again.

```
curl -XPUT 'localhost:9200/books/_settings' -d '
{
    "analysis": {
        "analyzer": {
            "analysis": {
              "type" : "standard"
              }
            }
        }
}'
```

Looking forward to there being a way to delete index settings.

There isn't a way for me to rename that bad analyzer, is there?
</comment><comment author="ghost" created="2015-06-25T20:48:12Z" id="115392624">A Google search results in people getting brought here... Unfortunately, empty stringing a value doesn't behave as one would expect; so the only way I've been able to actually remove settings is by deleting the global-X.st files under 0/_state/.
</comment><comment author="damienjoldersma" created="2015-06-26T20:01:44Z" id="115860241">+1 on being able to delete index settings.
</comment><comment author="RJ3000" created="2015-07-01T13:29:20Z" id="117676184">+1 on being able to delete index settings.
</comment><comment author="ncolomer" created="2015-07-09T10:13:08Z" id="119898935">+1 too
</comment><comment author="thenom" created="2015-07-22T08:47:53Z" id="123621202">+1
</comment><comment author="pemontto" created="2015-07-22T23:16:26Z" id="123902005">:+1: 
</comment><comment author="bsandvik" created="2015-07-23T20:16:31Z" id="124227759">+1 for ability to delete index settings. :pray: 
</comment><comment author="flavianh" created="2015-07-24T11:48:13Z" id="124497634">+1
</comment><comment author="drocsid" created="2015-07-28T23:01:13Z" id="125777115">+1 I created an undesired key for an index setting, and was surprised I'm unable to remove that key now. 
</comment><comment author="jaytaylor" created="2015-07-28T23:29:37Z" id="125781178">ES team doesn't seem to be very responsive here.  Unsubscribing to stop the daily inbox spam!

:-1: 
</comment><comment author="hjc" created="2015-08-05T16:24:13Z" id="128057613">Would love to see this implemented, especially this part:

&gt; Setting an unknown setting, or a setting that can't be changed should throw an error.

Would have saved me some time during a recent DevOps incident.

:+1:
</comment><comment author="drocsid" created="2015-08-05T19:16:55Z" id="128115508">I think creating an arbitrary setting is actually a feature that's already in place. However removing a setting key is a feature I want.
</comment><comment author="kjelle" created="2015-08-25T10:31:06Z" id="134549348">+1 for ability to delete persistent/transient index settings.
</comment><comment author="hjhart" created="2015-08-28T23:56:13Z" id="135916896">+1 for being able to delete index settings.
</comment><comment author="prikha" created="2015-09-01T22:27:10Z" id="136881213">+1 *)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add boolean or constant_score similarity</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6731</link><project id="" key="" /><description>The idea here is that some use cases don't need or want full-text ranking. In a lot of cases it can even hurt, for example: trying to use fuzzy query to find misspellings in a database of place names or something like that.

Today I think its too hard to turn off various measures such as IDF, we could just make it easy by providing a simple similarity out of box that users could enable.
</description><key id="37160595">6731</key><summary>Add boolean or constant_score similarity</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Similarities</label><label>adoptme</label><label>enhancement</label></labels><created>2014-07-04T11:25:03Z</created><updated>2017-03-28T14:17:24Z</updated><resolved>2017-03-28T14:17:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-08-01T10:07:09Z" id="50868730">I opened https://issues.apache.org/jira/browse/LUCENE-5867
</comment><comment author="clintongormley" created="2016-11-25T18:15:38Z" id="263007120">Boolean similarity is now in Lucene 6.4</comment><comment author="synhershko" created="2017-02-20T12:08:53Z" id="281063255">What is the status of this? is it exposed already?</comment><comment author="clintongormley" created="2017-02-20T13:31:59Z" id="281079985">Not yet</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deprecate extracting _routing and _id from document fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6730</link><project id="" key="" /><description>Currently routing and ID values can be passed in the query string and url respectively, but they can also be extracted from fields within the document.

This has a significant performance impact because each doc needs to be parsed on the node which receives the index/get/update/delete request in order to know which node should handle the request.

On top of that, there are clashes between (eg) routing values set in fields and parent settings.

We should deprecate the functionality to extract these values from fields, and make it the responsibility of the client code instead.

It should still be possible to set `_routing` to required.  Perhaps we should set this automatically if the user ever passes in a routing or parent value at index time?

Relates to #8870
</description><key id="37159739">6730</key><summary>Deprecate extracting _routing and _id from document fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>breaking</label></labels><created>2014-07-04T11:07:39Z</created><updated>2015-06-06T15:58:23Z</updated><resolved>2015-02-10T18:55:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-08T09:56:28Z" id="48293000">+1
</comment><comment author="javanna" created="2014-07-09T15:24:43Z" id="48488321">+1
</comment><comment author="clintongormley" created="2014-11-27T11:50:11Z" id="64780914">This change needs to be made in a backwards compatible way.

From 2.0:
- refuse to create new indices with the `path` setting in `_routing` or `_id`
- for bwc, maintain the ability to support these settings on old indices
- make the `upgrade` API complain about these settings (see #8682)

From 3.0:
- remove support completely

Users should still be able to set `_routing.required` as it is a useful safety check.  Perhaps we should set this value automatically if the user indexes a document with a custom routing value?
</comment><comment author="jpountz" created="2014-12-24T09:43:36Z" id="68040526">We should deprecate extracting the `_timestamp` field as well.
</comment><comment author="clintongormley" created="2014-12-24T11:37:57Z" id="68046910">&gt; We should deprecate extracting the _timestamp field as well.

@jpountz I'm not sure about this, but I think it requires a separate discussion, so I've opened https://github.com/elasticsearch/elasticsearch/issues/9058
</comment><comment author="clintongormley" created="2014-12-30T14:47:06Z" id="68361709">Also see #5558
</comment><comment author="ostefano" created="2015-04-07T15:04:30Z" id="90600791">Can someone confirm me that this is why `copy_to` inside `_id` mappings does not currently work?
</comment><comment author="rjernst" created="2015-04-07T15:43:33Z" id="90613937">This change was made only to master. As far as I can tell, `copy_to` was never supported for the `_id` field.
</comment><comment author="celesteking" created="2015-05-18T17:29:25Z" id="103142553">I had to dig through the bloat over internet in order to get to this information. You should really **DOCUMENT**  that [there will be no substitution for `path`](https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-id-field.html#_path) and that `copy_to` can't be used to "extract" `_id`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting of documents with missing field does not respect sort order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6729</link><project id="" key="" /><description>I am using Elastic Search 1.0.2, and it seems that documents which don't contain the field that is sorted on, are always sorted last - **no matter if the sort order is `asc` or `desc`**.

If I add a `missing="_first"` paramter, these documents are sorted first - but again it doesn't matter if the sort order is `asc`or `desc`.

This behavior seems quite strange since [`XFieldComparatorSource .missingObject(Object missingValue, boolean reversed)`](https://github.com/elasticsearch/elasticsearch/blob/v1.0.2/src/main/java/org/elasticsearch/index/fielddata/IndexFieldData.java#L109) method, **does** seem to take the sort order (the `reversed` parameter) into account, which is likely what the user would expect.
</description><key id="37159139">6729</key><summary>Sorting of documents with missing field does not respect sort order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">noxhoej</reporter><labels /><created>2014-07-04T10:56:30Z</created><updated>2014-07-04T15:25:03Z</updated><resolved>2014-07-04T15:25:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T15:25:03Z" id="48055717">@noxhoej that method does take the order into account.  eg if you sort on an int with order `asc` and missing `_first`, then the missing value is Integer.MIN_VALUE, but with missing `_last`, the missing value is Integer.MAX_VALUE.

So `_first` and `_last` behave the same way independent on sort order.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API for listing index file sizes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6728</link><project id="" key="" /><description>I think it would be useful when debugging if there was a way to get a snapshot of basically `ls -l` of the lucene index files. This would give us some basic information like size of some in-memory datastructures (norms, terms index), as well as some structural information (how large is the term dictionary? positions file? etc).

I know this might be tricky because of NRT and deleted files, but even if its not perfect, it would be handy.
</description><key id="37159113">6728</key><summary>API for listing index file sizes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels><label>adoptme</label><label>enhancement</label></labels><created>2014-07-04T10:55:54Z</created><updated>2015-11-21T15:56:40Z</updated><resolved>2015-11-21T15:56:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-07-04T10:57:35Z" id="48031063">+1

Hopefully we can also get the file sizes of the sub-files inside a compound file?
</comment><comment author="kimchy" created="2014-07-04T10:58:32Z" id="48031118">+1, I think this should be added to `CommonStats`, so we can get them through indices stats and node stats. I think size by file suffix summation is good enough, right?
</comment><comment author="rmuir" created="2015-01-07T17:56:57Z" id="69062206">I think it would also be useful here to recurse into .cfs files.
</comment><comment author="mikemccand" created="2015-01-07T17:58:03Z" id="69062386">+1
</comment><comment author="ppf2" created="2015-02-11T20:17:18Z" id="73956192">+1 would be great to see the file sizes used for doc values as more users switch from fd to dvs.
</comment><comment author="clintongormley" created="2015-11-21T15:56:40Z" id="158658557">Closed by #8832
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make similarities dynamically updatable where possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6727</link><project id="" key="" /><description>The core similarities can be swapped in dynamically on an existing index, as long as `discount_overlaps` is the same.  Currently we disallow updating similarities, because custom similarities may not be compatible.

The logic for deciding whether a similarity can be changed should be more fine grained.
</description><key id="37157836">6727</key><summary>Make similarities dynamically updatable where possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Mapping</label><label>adoptme</label><label>enhancement</label></labels><created>2014-07-04T10:32:22Z</created><updated>2016-11-25T18:14:17Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="villeapvirtanen" created="2015-06-17T06:16:22Z" id="112668768">This would be a really nice addition - for an example we have now millions of documents, and would like to experiment with the scoring to deliver that last 10%, but we need to reindex the whole lot each time we want to change the similarity.. This means that we probably end up with as many clusters as there are scoring algos available, which costs time, money, effort and motivation.
</comment><comment author="villeapvirtanen" created="2015-06-17T06:28:36Z" id="112670864">Maybe you should add interface to the [Similarity](https://lucene.apache.org/core/5_1_0/core/org/apache/lucene/search/similarities/Similarity.html) implementations and add 

``` java
/**
 * Returns the type names that are compatible metadata wise with this Similarity.
 **/
String[] getMetadataCompatibleTypeNames()
```

This would allow logic to be added to determine if the change should be allowed or not, solving the custom similarity compatibility problem. (As obviously the custom implementations should implement it too.) Downside for this is that you'd need a wrapper implementation for the Lucene provided Similarities and it would break existing custom ones. (Though a default wrapper that declares compatibility with none would allow everything to work as now.)

I'm not familiar enough with the Lucene classes to know if there is already a built in way to infer that knowledge though. Why this is not solved in Lucene level btw?
</comment><comment author="rmuir" created="2015-06-17T07:08:15Z" id="112683319">Because its not an issue with Lucene. You just call IndexWriterConfig.setSimilarity() and that is what IndexWriter will use to encode normalization factors.

Similarity impls can shove whatever it wants in there, up to 64-bits of stuff encoded in whatever form it wants. So ES does the right thing to prevent you from changing this here (in general). It is the same as changing index analyzer for a field, its generally just an unsafe thing to do. 

But the core similarities introduced in lucene 4.0 have a special property, in that by default they all encode the index-time information (normalization factor) in a backwards compatible way as DefaultSimilarity historically did: as 1/sqrt(length) with a certain single-byte encoding. 

This was done intentionally to make experimentation and "simple" testing of these ranking algorithms easier. It should not be enforced with any interface or anything like that, because subclasses and even setter methods can easily break it. It is just a way to quickly experiment with different ranking algorithms without reindexing.

I think its nice to expose (safely) this optimization to users of ES, too, so they can play in the same way. But it does not need any additional APIs for experts or custom implementations, that is misleading and dangerous. 

If you are really trying to get the last 10% then I don't think this issue is really relevant: its just not going to hold for "tuning". If you are really tuning, you will likely break this property yourself anyway: the default encoding used here is very general purpose and must support a crazy range for documents large and small and various values for index-time boost. If those assumptions don't hold, in many cases you can tweak normalization to be better by adjusting the encoding.
</comment><comment author="villeapvirtanen" created="2015-06-17T07:37:11Z" id="112694600">Hi, thanks for the response!

I meant that I'm trying to cater a better search experience for the end users, and tuning the relevance ranking, which for me as the user of ES, is the last 10%. (It does pretty well with the defaults, but there are cases where simple per field/query boosting is not delivering. Hence the need to experiment with similarities.)

I'd love to have this exposed to ES users too if possible, though I understand that I'm asking here the permission to (possibly) shoot myself to the foot :)

The API thingy was just a proposal to formalize the now unofficial contract which similarities are interchangeable, but as said I don't know if it makes sense or not. (Well, now I do know that it does not.)
</comment><comment author="rmuir" created="2015-06-17T08:05:48Z" id="112707056">I don't think we should give users the ability to shoot themselves in the foot, ever. Its easily prevented. 

A common use case for this issue would be to allow someone to switch from the default similarity to BM25 and then tweak `k1` and `b` parameter values all without reindexing. This is totally safe, and expert enough!

Having a custom similarity (subclass) is a much more expert thing and we don't need to make things complicated for that. If you already know enough to make your own similarity class, then you already have an expert way to tune without reindexing: you can tune your parameters by changing some constant in your code and ES is none the wiser. 
</comment><comment author="s1monw" created="2015-06-17T09:40:02Z" id="112737460">&gt; I don't think we should give users the ability to shoot themselves in the foot, ever. Its easily prevented.

:+1: 

I agree with rob here and I don't see really a need to do much on this issue.
</comment><comment author="villeapvirtanen" created="2015-06-17T10:43:57Z" id="112751585">Did I understand it correctly that you wish to close this as won't fix?
</comment><comment author="s1monw" created="2015-06-17T10:59:25Z" id="112754055">@villeapvirtanen yeah that is what I propose
</comment><comment author="rmuir" created="2015-06-17T11:03:04Z" id="112755012">I think the simple case is nice to have for the core similarities from lucene (see my BM25 example above). But i have no idea how tricky it is to implement this. 
</comment><comment author="rjernst" created="2015-06-17T17:21:15Z" id="112882985">The similarity parameters are set outside of the mappings (they are in a parallel section called "similarity"). But glancing at the code, I cannot see how it is possible they are updated (or even adding new ones after index creation). I agree this should be fixed: like with mappings, you should be able to tweak the parameters of the similarity, but not change the type, for a given name.
</comment><comment author="rmuir" created="2015-06-17T17:25:35Z" id="112883922">&gt; like with mappings, you should be able to tweak the parameters of the similarity, but not change the type, for a given name.

Its not like mappings at all though. 

Changing DefaultSimilarity to BM25Similarity is ok: the on-disk encoding is the same.
Changing BM25Similarity k1/b parameters is ok: the on-disk encoding is the same.
Changing BM25Similarity.discountOverlaps is not ok, you need to reindex.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use unlimited `flush_threshold_ops` for translog (again)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6726</link><project id="" key="" /><description>In #5900 we did this, but because this caused high RAM usage with versionMap, we reverted, but since we are now fixing versionMap to flush if its RAM usage is too high in #6443 we should be able to make this unlimited again so we don't have too-frequent fsyncs for apps with medium/small docs.
</description><key id="37156963">6726</key><summary>Use unlimited `flush_threshold_ops` for translog (again)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Translog</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T10:17:22Z</created><updated>2015-06-07T13:02:10Z</updated><resolved>2014-07-09T19:23:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-04T10:24:53Z" id="48028927">+1
</comment><comment author="mikemccand" created="2014-07-09T19:23:44Z" id="48522038">Fixed in 7335b5db22b96cb94fc441b0e47ef15297e71b5f / #6783.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add operation preference support to aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6725</link><project id="" key="" /><description>Allow setting preference (eg `_local`)  in aliases.

Fixes #1307 
</description><key id="37153193">6725</key><summary>Add operation preference support to aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>feature</label></labels><created>2014-07-04T09:18:27Z</created><updated>2015-11-21T15:52:10Z</updated><resolved>2015-11-21T15:52:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T15:52:09Z" id="158657825">Not sure why I suggested this. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DocumentMissingException is also thrown on update retries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6724</link><project id="" key="" /><description>- Closes #6355.
- Contains the bugfix and an integration test demonstrating the bug and validating the fix.
- The fix in TransportUpdateAction is a simple try-catch block calling the appropriate failure handler. The fix prevents DocumentMissingExceptions from ending up uncaught and thus makes sure that an HTTP response is actually sent to the client instead of not sending any at all.
</description><key id="37152799">6724</key><summary>DocumentMissingException is also thrown on update retries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">peschlowp</reporter><labels><label>:CRUD</label><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T09:12:21Z</created><updated>2015-06-07T19:54:55Z</updated><resolved>2014-07-15T12:57:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-08T16:44:18Z" id="48366060">sorry for the delay - I will look into this soon. Did you sign the CLA so I can pull it in once ready?
</comment><comment author="peschlowp" created="2014-07-08T19:00:47Z" id="48384847">Glad to hear that this is going to be reviewed! I recently signed a CLA for contributing some typo fixes to The Definitive Guide - if it is the same CLA, then I'm already done.
</comment><comment author="s1monw" created="2014-07-09T10:48:38Z" id="48455213">hey, 

I really appreciate your testcase though but it's kind of very involved and should use our testing infrastructure. Maybe take a look at `ElasticsearchIntegrationTest`?

I also wonder if we should fix this in more places I can see 2 more places where we do run `shardOperation` in a thread. I think maybe we can implement a special Runnable like

``` Java

public abstract class ActionRunnable implements Runnable {
  protected final ActionListener&lt;?&gt; listener 
  public ActionRunnable(ActionListener&lt;?&gt; listener ){
    this.listener = listener;
  } 
  public final void run() {
    try {
      doRun();
    } catch (Throwable t) {
        listener.onFailure(t);
    }
  }
  protected abstract void doRun();
}
```
</comment><comment author="peschlowp" created="2014-07-09T12:05:45Z" id="48461198">I'm currently adding test cases for the two other places in TransportUpdateAction, to have the logic in place at all. I'm going to update the pull request once I'm done. It's not a big deal to add the Runnable implementation you provide so I'll do that as well.

Actually, I took a look at ElasticsearchIntegrationTest before starting to write the initial test, but unfortunately it uses a specific implementation of Engine, and I need another one to force the timing between requests. As an Elasticsearch codebase newbie, I didn't want to mess with that so I went for a self-contained solution. Still, I'd be glad to have these tests based on the existing infrastructure. Note that another constraint is that I need to access the Engine instance from the test code, and it is not easy to get the respective injector at that point (as it is the index' injector, not the node's). Do you have a good suggestion for fitting it into ElasticsearchIntegrationTest?
</comment><comment author="s1monw" created="2014-07-09T19:46:58Z" id="48524851">@peschlowp if you want to use a different engine that is pretty simple you can create an index like this:

``` Java
public void testX() {
  assertAcked(prepareCreate("idx").setSettings(ImmutableSettings.builder()
      .put(IndexEngineModule.EngineSettings.ENGINE_TYPE, YourEngineModule.class.getName());
  // go do your thing...
}
```

lemme know if it works
</comment><comment author="peschlowp" created="2014-07-10T11:34:21Z" id="48593450">Pushed an update applying the try-catch fix to all three locations, for you to review and give me feedback on two open questions:
1. I turned my self-contained test class into an ElasticsearchIntegration test. However, while the former implementation was stable, now it seems that some of the random aspects of the test cluster interfere with the timing I'm faking for the tests. Like, some request doesn't reach internal engine even though it's supposed to. I'm not sure how difficult it will be for me to debug this and see what exactly is the problem there. Is there an option to disable some of the randomness so that we can at least assure the tests will be stable and green, then maybe investigate further (if needed at all)?
2. I was at a loss coming up with a test case for the "delete" case in TransportUpdateAction.shardOperation(), because for the error situation to happen the delete request needs to have a retry_on_conflict value set. Is this possible at all - via the API it doesn't seem to be? So do we need to handle this case at all in case of "delete" requests?

Note that I didn't create a separate ActionRunnable class yet, this can be added when the above points are resolved.
</comment><comment author="peschlowp" created="2014-07-10T13:31:51Z" id="48604176">Sorry for the confusion, I introduced a timing bug in one of my test cases while refactoring it and blamed the integration test infrastructure for it. It is fixed now and the tests are running stable on my machine :-)

Also, I added the ActionRunnable @s1monw proposed.

Aside from the question if the retry logic is needed at all for delete requests (see point 2 in my last comment), I'm satisfied now. If your review also turns out satisfactory, I can squash the various commits to a single one that you can actually pull.
</comment><comment author="s1monw" created="2014-07-10T20:36:44Z" id="48660545">I left a couple of cosmetic comments I think we are ready to squash good job!
</comment><comment author="peschlowp" created="2014-07-11T08:38:46Z" id="48707069">OK, all done, including a few more cosmetic changes, and squashed. Feel free to refactor whatever you like after pulling, I probably haven't followed all conventions of the project everywhere.

Happy that I could contribute to Elasticsearch.
</comment><comment author="s1monw" created="2014-07-15T12:56:16Z" id="49027132">thanks for the report and the fix! I will push this in a bit..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryParser can return null from a query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6723</link><project id="" key="" /><description>This causes a NPE since XContentStructure checks if the query is null
and takes this as the condition to parse from the byte source which is
actually null in that case.

Closes #6722
</description><key id="37151667">6723</key><summary>QueryParser can return null from a query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.1.3</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T08:53:52Z</created><updated>2015-06-07T19:33:31Z</updated><resolved>2014-07-04T09:20:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-04T09:12:57Z" id="48023491">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replace empty bool queries with match_all to prevent NullPointerExceptions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6722</link><project id="" key="" /><description>I have setup a parent/child relationship where parent is optional and i handle the routing in that case. And this worked fine in 1.0.1 but when i bumped to 1.2.1 to use aggregations it stopped working.

The exception i get

``` java
[2014-07-04 07:59:30,333][INFO ][node                     ] [Tantra] version[1.2.1], pid[23438], build[6c95b75/2014-06-03T15:02:52Z]
[2014-07-04 07:59:30,334][INFO ][node                     ] [Tantra] initializing ...
[2014-07-04 07:59:30,339][INFO ][plugins                  ] [Tantra] loaded [], sites []
[2014-07-04 07:59:32,581][INFO ][script                   ] [Tantra] compiling script file [/etc/elasticsearch/scripts/activity-period.mvel]
[2014-07-04 07:59:33,016][INFO ][node                     ] [Tantra] initialized
[2014-07-04 07:59:33,016][INFO ][node                     ] [Tantra] starting ...
[2014-07-04 07:59:33,092][INFO ][transport                ] [Tantra] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.0.2.15:9300]}
[2014-07-04 07:59:36,133][INFO ][cluster.service          ] [Tantra] new_master [Tantra][jUaCLA6GTyadduwbg_tTwg][packer-virtualbox][inet[/10.0.2.15:9300]], reason: zen-disco-join (elected_as_master)
[2014-07-04 07:59:36,171][INFO ][discovery                ] [Tantra] elasticsearch/jUaCLA6GTyadduwbg_tTwg
[2014-07-04 07:59:36,262][INFO ][http                     ] [Tantra] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.0.2.15:9200]}
[2014-07-04 07:59:37,094][INFO ][gateway                  ] [Tantra] recovered [33] indices into cluster_state
[2014-07-04 07:59:37,116][INFO ][node                     ] [Tantra] started
[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][2], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]
org.elasticsearch.search.SearchParseException: [application][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "has_parent": {
                "type": "clients",
                "query": {
                  "bool": {
                    "must": [],
                    "must_not": [],
                    "should": []
                  }
                },
                "filter": {
                  "bool": {
                    "must": [
                      {
                        "terms": {
                          "interests": [
                            "hello"
                          ]
                        }
                      }
                    ],
                    "must_not": [],
                    "should": []
                  }
                }
              }
            },
            {
              "geo_distance": {
                "distance": "20km",
                "coordinate": "59.85499699999999,17.6490213"
              }
            }
          ],
          "must_not": [],
          "should": []
        }
      },
      "query": []
    }
  },
  "size": 25
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)
    at org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)
    at org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)
    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)
    ... 9 more
[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][1], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]
org.elasticsearch.search.SearchParseException: [application][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "has_parent": {
                "type": "clients",
                "query": {
                  "bool": {
                    "must": [],
                    "must_not": [],
                    "should": []
                  }
                },
                "filter": {
                  "bool": {
                    "must": [
                      {
                        "terms": {
                          "interests": [
                            "hello"
                          ]
                        }
                      }
                    ],
                    "must_not": [],
                    "should": []
                  }
                }
              }
            },
            {
              "geo_distance": {
                "distance": "20km",
                "coordinate": "59.85499699999999,17.6490213"
              }
            }
          ],
          "must_not": [],
          "should": []
        }
      },
      "query": []
    }
  },
  "size": 25
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)
    at org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)
    at org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)
    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)
    ... 9 more
[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][0], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]
org.elasticsearch.search.SearchParseException: [application][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "has_parent": {
                "type": "clients",
                "query": {
                  "bool": {
                    "must": [],
                    "must_not": [],
                    "should": []
                  }
                },
                "filter": {
                  "bool": {
                    "must": [
                      {
                        "terms": {
                          "interests": [
                            "hello"
                          ]
                        }
                      }
                    ],
                    "must_not": [],
                    "should": []
                  }
                }
              }
            },
            {
              "geo_distance": {
                "distance": "20km",
                "coordinate": "59.85499699999999,17.6490213"
              }
            }
          ],
          "must_not": [],
          "should": []
        }
      },
      "query": []
    }
  },
  "size": 25
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)
    at org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)
    at org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)
    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)
    ... 9 more
[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][4], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]
org.elasticsearch.search.SearchParseException: [application][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "has_parent": {
                "type": "clients",
                "query": {
                  "bool": {
                    "must": [],
                    "must_not": [],
                    "should": []
                  }
                },
                "filter": {
                  "bool": {
                    "must": [
                      {
                        "terms": {
                          "interests": [
                            "hello"
                          ]
                        }
                      }
                    ],
                    "must_not": [],
                    "should": []
                  }
                }
              }
            },
            {
              "geo_distance": {
                "distance": "20km",
                "coordinate": "59.85499699999999,17.6490213"
              }
            }
          ],
          "must_not": [],
          "should": []
        }
      },
      "query": []
    }
  },
  "size": 25
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)
    at org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)
    at org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)
    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)
    ... 9 more
[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][3], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68]
org.elasticsearch.search.SearchParseException: [application][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "has_parent": {
                "type": "clients",
                "query": {
                  "bool": {
                    "must": [],
                    "must_not": [],
                    "should": []
                  }
                },
                "filter": {
                  "bool": {
                    "must": [
                      {
                        "terms": {
                          "interests": [
                            "hello"
                          ]
                        }
                      }
                    ],
                    "must_not": [],
                    "should": []
                  }
                }
              }
            },
            {
              "geo_distance": {
                "distance": "20km",
                "coordinate": "59.85499699999999,17.6490213"
              }
            }
          ],
          "must_not": [],
          "should": []
        }
      },
      "query": []
    }
  },
  "size": 25
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)
    at org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)
    at org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)
    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)
    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)
    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)
    ... 9 more
[2014-07-04 07:59:45,999][DEBUG][action.search.type       ] [Tantra] All shards failed for phase: [query]
```

Mapping

``` json
{
    "application": {
        "mappings": {
            "clients": {
                "properties": {
                    "createdAt": {
                        "format": "date_time_no_millis",
                        "type": "date"
                    },
                    "email": {
                        "type": "string"
                    },
                    "facebookId": {
                        "type": "long"
                    },
                    "favorites": {
                        "type": "integer"
                    },
                    "id": {
                        "type": "integer"
                    },
                    "interests": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "surname": {
                        "type": "string"
                    },
                    "updatedAt": {
                        "format": "date_time_no_millis",
                        "type": "date"
                    }
                }
            },
            "devices": {
                "_parent": {
                    "type": "clients"
                },
                "_routing": {
                    "required": true
                },
                "properties": {
                    "active": {
                        "type": "boolean"
                    },
                    "appName": {
                        "type": "string"
                    },
                    "appVersion": {
                        "type": "string"
                    },
                    "carrier": {
                        "type": "string"
                    },
                    "coordinate": {
                        "type": "geo_point"
                    },
                    "country": {
                        "index": "not_analyzed",
                        "type": "string"
                    },
                    "createdAt": {
                        "format": "date_time_no_millis",
                        "type": "date"
                    },
                    "deviceId": {
                        "index": "not_analyzed",
                        "type": "string"
                    },
                    "deviceModel": {
                        "type": "string"
                    },
                    "deviceName": {
                        "type": "string"
                    },
                    "id": {
                        "index": "not_analyzed",
                        "type": "string"
                    },
                    "inactivatedAt": {
                        "format": "date_time_no_millis",
                        "type": "date"
                    },
                    "mobileCountryCode": {
                        "index": "not_analyzed",
                        "type": "string"
                    },
                    "os": {
                        "type": "string"
                    },
                    "osVersion": {
                        "type": "string"
                    },
                    "pushNotificationId": {
                        "index": "not_analyzed",
                        "type": "string"
                    },
                    "updatedAt": {
                        "format": "date_time_no_millis",
                        "type": "date"
                    }
                }
            }
        }
    }
}
```
</description><key id="37148755">6722</key><summary>Replace empty bool queries with match_all to prevent NullPointerExceptions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">macnibblet</reporter><labels><label>bug</label><label>v1.1.3</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-04T08:04:06Z</created><updated>2015-06-07T19:33:12Z</updated><resolved>2014-07-04T09:20:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-04T09:11:07Z" id="48023338">thanks for opening this.. I have a fix for that already and will push it soon. as a workaround just use a `match_all` query instead of the:

``` JSON
  "bool" : {
    "must": [],
    "must_not": [],
    "should": []
  }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Link to eskka discovery plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6721</link><project id="" key="" /><description /><key id="37145494">6721</key><summary>[DOC] Link to eskka discovery plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shikhar</reporter><labels /><created>2014-07-04T06:51:07Z</created><updated>2014-07-04T15:07:33Z</updated><resolved>2014-07-04T15:07:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Indexing: index-time sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6720</link><project id="" key="" /><description>Lucene has index-time sorting and early query termination capabilities. It would be nice to integrate it into Elasticsearch in order to speed up queries whose sort order matches the index order. This [presentation](https://speakerdeck.com/elasticsearch/index-sorting-with-lucene) contains some information about these features and their implementation in Lucene.
</description><key id="37121827">6720</key><summary>Indexing: index-time sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jimczi/following{/other_user}', u'events_url': u'https://api.github.com/users/jimczi/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jimczi/orgs', u'url': u'https://api.github.com/users/jimczi', u'gists_url': u'https://api.github.com/users/jimczi/gists{/gist_id}', u'html_url': u'https://github.com/jimczi', u'subscriptions_url': u'https://api.github.com/users/jimczi/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/15977469?v=4', u'repos_url': u'https://api.github.com/users/jimczi/repos', u'received_events_url': u'https://api.github.com/users/jimczi/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jimczi/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jimczi', u'type': u'User', u'id': 15977469, u'followers_url': u'https://api.github.com/users/jimczi/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>feature</label></labels><created>2014-07-03T20:34:16Z</created><updated>2017-06-12T07:48:30Z</updated><resolved>2017-06-12T07:48:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-04T07:47:43Z" id="48017010">I think this is an awesome feature for specific problems. Yet I think we have to make sure that this never ever corrupts an index during a merge. IMO we should restrict this to numeric fields and allow only `asc / desc` and that field is automatically marked as using on-disk docvalues to make sure we don't fail the merge due to OOM.
</comment><comment author="rmuir" created="2014-07-04T09:27:12Z" id="48024603">why limit to numeric? whether a field is numeric or not is completely unrelated to this feature: sorry that just doesn't make sense. Since lucene 4.8 you can supply any arbitrary Sort here.
</comment><comment author="s1monw" created="2014-07-04T09:29:03Z" id="48024750">I am super concerned about memory consumption so I should have rather said restricted to on-disk DV. I don't think it's unrelated
</comment><comment author="rmuir" created="2014-07-04T12:51:30Z" id="48040429">my question was with regards to string fields though. I think those are ok too (as well as multiple-level sorts or whatever lucene has). If we want to limit it to DV to prevent traps in merging thats fine, but I think string is just as good as numeric.

Separately, the real issue here is how well early termination will work with situations like aggregations. I don't yet fully understand how this is working, but it looks like it simply plugs into collector. so you would get wrong aggregation counts with early termination if we aren't careful.
</comment><comment author="jpountz" created="2014-07-04T13:23:21Z" id="48043366">+1 on allowing string doc values and multi-level sorts. We don't need to have them from the first iteration but having them eventually would be nice. Then we could early-terminate on any prefix on the sort that is configured at index time.

Even without aggregations, simple things like the total number of hits could only be approximated. I guess there would need to be a per-query flag to turn early termination on.
</comment><comment author="rjernst" created="2014-07-08T17:07:52Z" id="48369375">This sounds like the PageRank use case, but there is another as well.

Sorting can be very useful for compression, to allow "clumps" of documents to be near each other (thus producing smaller deltas in postings).  For example, let's say we have songs lyrics as our documents, and we sort by genre.  If we restrict the search to a specific genre, we will quickly narrow into a small portion of the postings list, with small deltas.  Also, if we do a search across all genres, but for a term which is highly correlated with a genre (e.g. "cowboy" for "Country"), then again we will have good compression and search speed for that term.

My point is just that you don't need early termination for this to be useful.  It can help speed up queries if you just understand your data and the common types of queries you run.
</comment><comment author="ofavre" created="2014-08-22T18:43:42Z" id="53104292">[Algolia](https://www.algolia.com/) amazing speed is achieved through index-time ordering, and they do support typo-tolerance as well.
I think it's safe to say that this can very well be a major feature.
</comment><comment author="Vineeth-Mohan" created="2015-05-31T17:09:51Z" id="107224931">Looking forward for this feature. 
</comment><comment author="clintongormley" created="2015-06-06T18:12:10Z" id="109627124">Comment copied over from #8873:

Documents of different types tend to have different fields, which can lead to problems of sparsity when documents are stored in the order that they were indexed.  If we sort documents by type, then missing values (eg in doc values or norms) can be represented very efficiently.

On merge, documents should be sorted by type (assuming there is more than one type).  Additionally, for certain use cases, it can make sense to apply a secondary sort (eg on timestamp). This secondary level should be customizable per index, and should probably be dynamically updatable (although it would only have effect on later merges).

Relates to #8870
</comment><comment author="jpountz" created="2015-09-04T11:05:05Z" id="137705362">We probably want to have https://issues.apache.org/jira/browse/LUCENE-6766 first.
</comment><comment author="otisg" created="2015-09-05T16:13:44Z" id="137971985">@jpountz would this help in the typical ELK case where one indexes logs (older first, then newer ones) and typically short in the opposite order - first new ones, then old ones?

In other words, with a typical ELK case where logs are being indexed would the natural indexing order already result in ideally sorted index, or would one have to run something (whatever LUCENE-6766 and this ES issue expose) to inverse the sort order of such an index?
</comment><comment author="jpountz" created="2015-09-05T16:44:49Z" id="137974027">I don't think index sorting would be appealing for time series: typical ELK deployments are interested in ingestion speed and running aggregations and none of them would benefit from index sorting as it adds overhead at index time and aggregations can't be early-terminated.

In my opinion, this would be more useful for pure search use-cases that only need to fetch the top docs and have a low to moderate ingestion rate.
</comment><comment author="nik9000" created="2015-09-07T01:28:40Z" id="138152561">&gt; I don't think index sorting would be appealing for time series: typical ELK deployments are interested in ingestion speed and running aggregations and none of them would benefit from index sorting as it adds overhead at index time and aggregations can't be early-terminated.

I suspect it'd be faster to answer "find me the last 10 things that match this query" which is pretty useful. You'd have to not care about the count of total matches and I'm not sure if that outweighs the index time cost. I don't know this feature well so I can speak to what that index time cost amounts to.
</comment><comment author="jimczi" created="2015-12-07T15:11:20Z" id="162551551">After reading the description I though it would be pretty easy to add the feature but then I realized that it would break the blockjoin/nested support. Seems like we would need another sorting merge policy which takes the nested document into account...  
</comment><comment author="Vineeth-Mohan" created="2016-03-11T17:32:38Z" id="195468034">@ofavre - I am curious how this can speed up algolia. They offer various sorting options and one of them looks like TF-IDF. AFAIK , this technique can only speed up sorting based on a single factor and not something dynamic. 
</comment><comment author="JanJakes" created="2016-07-19T14:13:44Z" id="233645782">Any plans/updates on this issue? Using index time sorting would have huge performance impacts on queries like "give me 10 items with the highest rating" when having millions of them. Seems like Algolia is using exactly this approach: https://www.algolia.com/doc/faq/index-configuration/how-to-sort-the-results-with-a-specific-attribute
</comment><comment author="jpountz" created="2016-07-19T14:20:57Z" id="233647915">We are slowly making progress: index sorting is going to be better integrated into Lucene in the upcoming 6.2 release https://issues.apache.org/jira/browse/LUCENE-6766 so when 6.2 is out we can start working on the integration. This will take time so don't expect it to be implemented and automatically terminate queries early etc. soon, but there is definitely progress being made.
</comment><comment author="weiqiyiji" created="2017-01-17T15:10:30Z" id="273195433">Hey, guys, is there any progress? Currently I need index-time sorting in my product, and I wonder if I can wait util you guys implement this feature or I have to do it myself first.</comment><comment author="mikemccand" created="2017-01-17T15:48:35Z" id="273207354">There has been good progress on the Lucene side, including a change @jimczi just pushed to Lucene 6.x (https://issues.apache.org/jira/browse/LUCENE-7579) for its future 6.5.0 release, to give a big boost to indexing performance with index-time sorting.  You can see the impact of that change in the nightly sparse Lucene benchmarks: it's annotation V in this chart: https://home.apache.org/~mikemccand/lucenebench/sparseResults.html#index_throughput

But we still have plenty of work to do to expose sorted indices in ES.</comment><comment author="weiqiyiji" created="2017-01-17T16:59:54Z" id="273229782">@mikemccand Ok, I'll watch this issue first. Thanks!</comment><comment author="makeyang" created="2017-03-27T13:13:37Z" id="289449278">@mikemccand  any branch working on this so we can take a look at it ?</comment><comment author="jpountz" created="2017-03-27T13:23:42Z" id="289451813">There is no branch yet.</comment><comment author="makeyang" created="2017-03-31T14:15:12Z" id="290723841">sorry to bother to propose some thoughts on this issue:

1. add mapping parameters: index_sort with values[no, asc, desc] for datatype[long,integer,short,byte,double,float] and index_sort_order with no-negative int value if and only if fields setting index_sort
2. these 2 parameters can't be modified once it is set for a index
3. index_sort_order is used to decide order of multi fields and 1 stands for the first sequence to decide sort
4. it should not allow null value for fields with mapping parameter index_sort set not no for easy to implement
5. it should not allow indices with no-zero-doc to add fields with index_sort parameter for easy to implement
</comment><comment author="jimczi" created="2017-03-31T14:21:18Z" id="290725464">@makeyang thanks. 
I am planning to add the index sorting spec in the index settings rather than on the index mapping since you can define an index sort based on multiple fields.
Anyway I am currently iterating on this feature and I'll add updates (and link to the PR when I open it) to this issue so stay tuned !</comment><comment author="jimczi" created="2017-04-19T12:40:27Z" id="295254968">Index time sorting has landed in master:
https://github.com/elastic/elasticsearch/pull/24055

There are still missing pieces that we need to add in order to make index sorting useful:

- Documentation regarding when to (not) use index sorting and how

- Early termination for queries that rely on the index sort

- Faster sorted scroll queries when the query sort matches the index sort.</comment><comment author="jimczi" created="2017-06-12T07:48:30Z" id="307714803">Index sorting is now fully supported in master. 
The documentation for this feature can be found here:
https://www.elastic.co/guide/en/elasticsearch/reference/master/index-modules-index-sorting.html

We'll continue to add enhancements and features based on sorted indices but this issue can be closed. </comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add phase to prefetch doc for query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6719</link><project id="" key="" /><description>There are a number of APIs which require a doc to be fetched in order to execute a query, eg the geo shapes filter, the terms lookup filter, more like this etc.

Currently every shard which executes the query has to fetch the document separately.  We should consider adding a phase which would cause the required document to be fetched by the coordinating node, before the query is dispatched to each shard.

This would mean that the query would need to be parsed on the coordinating node, while today that doesn't happen.  Could this possibly be an opt-in flag which the user could specify in the request?
</description><key id="37118645">6719</key><summary>Add phase to prefetch doc for query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>adoptme</label><label>enhancement</label><label>high hanging fruit</label></labels><created>2014-07-03T19:58:08Z</created><updated>2015-11-21T15:50:37Z</updated><resolved>2015-11-21T15:50:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T09:35:24Z" id="48025268">We need benchmarks to decided whether this is worth implementing.
</comment><comment author="clintongormley" created="2014-07-04T09:36:01Z" id="48025301">Also, currently each shard fetching a doc separately could end up with different versions of the doc being fetched, which could be considered a bug.
</comment><comment author="clintongormley" created="2015-11-21T15:50:37Z" id="158657384">Closing in favour of #10217
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: fixed a typo in the docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6718</link><project id="" key="" /><description /><key id="37116118">6718</key><summary>Docs: fixed a typo in the docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">cebe</reporter><labels /><created>2014-07-03T19:33:41Z</created><updated>2014-07-07T13:43:52Z</updated><resolved>2014-07-07T08:41:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T14:59:39Z" id="48052681">HI @cebe 

Thanks for the fix. Please could you sign our CLA so that we can merge your commit in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="cebe" created="2014-07-05T14:27:17Z" id="48087679">Wow, you need a CLA for a typo fix? :) Signed it.
</comment><comment author="clintongormley" created="2014-07-07T08:41:56Z" id="48153472">Unfortunately, yes :)

merged thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PatternAnalyzer should use PatternTokenFilter instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6717</link><project id="" key="" /><description>The PatternAnalyzer in Lucene is deprecated and it should be implemented using the PatternTokenizer, LowerCaseFilter and StopTokenFilter instead.

Would fix the bug in #895
</description><key id="37114013">6717</key><summary>PatternAnalyzer should use PatternTokenFilter instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-03T19:10:03Z</created><updated>2015-06-07T13:02:28Z</updated><resolved>2014-07-09T15:18:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>grammatical error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6716</link><project id="" key="" /><description /><key id="37103922">6716</key><summary>grammatical error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nicksteele</reporter><labels /><created>2014-07-03T17:27:41Z</created><updated>2014-07-08T11:16:28Z</updated><resolved>2014-07-08T11:16:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T18:09:07Z" id="47964754">Hi @nicksteele 

Thanks for the commit. Please could I ask you to sign our CLA so that I can get it merged in?
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="nicksteele" created="2014-07-03T19:52:46Z" id="47976722">Signed! Merge away!
-ns

On Thu, Jul 3, 2014 at 2:09 PM, Clinton Gormley notifications@github.com
wrote:

&gt; Hi @nicksteele https://github.com/nicksteele
&gt; 
&gt; Thanks for the commit. Please could I ask you to sign our CLA so that I
&gt; can get it merged in?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/6716#issuecomment-47964754
&gt; .

## 

_Nicholas Steele_
_(917)846-4096_
_grand.st http://grand.st_
</comment><comment author="clintongormley" created="2014-07-08T11:16:28Z" id="48323691">Thanks @nicksteele. Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If the node initialisation fails, make sure the node environment is closed correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6715</link><project id="" key="" /><description>... (and thus all locks being properly released)
</description><key id="37087619">6715</key><summary>If the node initialisation fails, make sure the node environment is closed correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">gnodet</reporter><labels><label>:Internal</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-03T14:35:35Z</created><updated>2015-06-07T19:34:36Z</updated><resolved>2014-07-09T22:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-04T10:35:30Z" id="48029609">this looks good though. Can you sign the CLA so I can pull this in?
</comment><comment author="s1monw" created="2014-07-09T10:28:47Z" id="48453636">@gnodet can you sign the CLA otherwise I can't pull this in sorry.
</comment><comment author="gnodet" created="2014-07-09T12:09:00Z" id="48461442">Done, sorry, I missed the first comment
</comment><comment author="s1monw" created="2014-07-09T22:26:29Z" id="48543368">pushed thanks, I fixed the commit msg a bit hence the PR is not marked as `merged` sry github isn't terribly smart along those lines
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More resource efficient analysis wrapping usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6714</link><project id="" key="" /><description>Today, we take great care to try and share the same analyzer instances across shards and indices (global analyzer). The idea is to share the same analyzer so the thread local resource it has will not be allocated per analyzer instance per thread.
The problem is that AnalyzerWrapper keeps its resources on its own per thread storage, and with per field reuse strategy, it causes for per field per thread token stream components to be used. This is very evident with the StandardTokenizer that uses a buffer...
This came out of test with "many fields", where the majority of 1GB heap was consumed by StandardTokenizer instances...
</description><key id="37086859">6714</key><summary>More resource efficient analysis wrapping usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-03T14:27:03Z</created><updated>2015-06-07T13:04:23Z</updated><resolved>2014-07-03T19:03:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-03T14:34:21Z" id="47937356">@uschindler hey, I would love for a review of this as well? I think that if its good, this might be a good addition to Lucene?
</comment><comment author="rmuir" created="2014-07-03T14:48:44Z" id="47939234">+1 from me. I think the change is good. afterwards, we should fix this in Lucene too:

the 'restrictions' here are not harsh and IMO should somehow be the "default" behavior in lucene. The issue is there are two use cases baked into AnalyzerWrapper.java: 1. delegating use case (by field name). 2. actual wrapping use case, where you take an existing analyzer and tweak functionality. So long term, I am thinking we should separate the two in lucene. And this delegating use-case (which is way more typical) will be more efficient, the base class for PerFieldAnalyzerWrapper. The other wrapping use case, can be separate class which is base for ShingleANalyzerWrapper and those things.
</comment><comment author="uschindler" created="2014-07-03T15:09:02Z" id="47942183">Hi, I will look into this in a moment. Thanks for the _ping_ on twitter!
</comment><comment author="uschindler" created="2014-07-03T17:05:06Z" id="47957088">Hi,
I know the problem and was not happy with AnalyzerWrapper all the time, especially for PerFieldAnalyzerWrapper in Lucene (what is actually the difference of Elasticsearch's FieldNameAnalyzer)?
I agree, if you don't wrap the tokenstream, there is no problem at all.
Actually, you don't need to make wrapReader final, because the Reader is always recreated if the tokenstream is reused. getReusableComponents() would then return non-null and Analyzer base class would wrap the reader for you:

```
  public final TokenStream tokenStream(final String fieldName,
                                       final Reader reader) throws IOException {
    TokenStreamComponents components = reuseStrategy.getReusableComponents(this, fieldName);
    final Reader r = initReader(fieldName, reader);
    if (components == null) {
      components = createComponents(fieldName);
      reuseStrategy.setReusableComponents(this, fieldName, components);
    }
    components.setReader(r);
    return components.getTokenStream();
  }
```

In any case we should put this thing into Lucene, too - and make PerFieldAnalyzerWrapper extend it! Strong +1 and fighting against memory waste :-)
</comment><comment author="uschindler" created="2014-07-03T17:08:46Z" id="47957537">In addition, I have the feeling, we should reorder the above code extract from Lucene, so the wrapping of reader is done after the components are created or at the beginning of the method. The current code is hard to understand because the initReader() method is somewhere in the middle of the other logic! This is a relict from the time when Tokenizers got the reader in the constructor (oh my god, thanks @rmuir for fixing this!)
</comment><comment author="uschindler" created="2014-07-03T17:11:33Z" id="47957840">In Lucene 4.x we still have the reader in Tokenizers constructor. But we still don't need to make the wrapReader() method final. If the delegating AnalyzerWrapper wraps the reader and stores the TokenStream with the wrapped reader in the delegate, its still no problem, because if the component is reused, both - the delegate and the delegator - can set a new reader - wrapped or not. In addition, when the Tokenizer is closed, it unsets the reader, so the cache does not contain a reader anymore.
</comment><comment author="kimchy" created="2014-07-03T19:02:39Z" id="47971340">cool, great, I will push this then for now, and we will move to whatever improvement happens in Lucene
</comment><comment author="uschindler" created="2014-07-03T21:56:29Z" id="47989318">Hi,
I opened https://issues.apache.org/jira/browse/LUCENE-5803
</comment><comment author="uschindler" created="2014-07-05T12:41:45Z" id="48085601">Hi Shay,
I resolved the issue in Lucene. The new DelegatingAnalyzerWrapper will be available in Lucene 4.10! Maybe you want to add a note into ES code base, or use XDelegatingAnalyzerWrapper as new class name.
The Lucene class has additional tests and also supports one use-case which does not happen in ES: If you wrap the delegating analyzer with another non-delegating analyzer and use "new OtherAnalyzer(delegatingWrapper.getReuseStrategy())" (the common pattern), this would break reuse. To prevent this, DelegatingAnalyzerWrapper now gets a ReuseStrategy like all other analyzers as ctor param and delegates to this "fallback", if delegating to the underlying analyzer is not possible (if the Analyzer is not top-level).
I would also use PerFieldAnalyzerWrapper in Elasticsearch now, as your class is more or less a clone.
</comment><comment author="kimchy" created="2014-07-05T12:48:06Z" id="48085720">@uschindler cool, yea, both additional cases are not relevant in ES case, I added an assert that will trip once we update to 4.10 to remove this class, and use the one from the issue. 

we can't use per field analyzer wrapper (I assume you mean FieldNamesAnalyzer class), this is going through additional changes that will cause it to diverge from it even more, which I think is fine.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filters: Terms filter with `fielddata` execution mode should use ordinals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6713</link><project id="" key="" /><description>The terms filter supports a `fielddata` execution mode that runs on field data instead of the inverted index. However, in case of strings, the current implementation filters based on values, which can be super slow. It should use ordinals instead.
</description><key id="37084169">6713</key><summary>Filters: Terms filter with `fielddata` execution mode should use ordinals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label></labels><created>2014-07-03T13:58:42Z</created><updated>2015-05-29T17:00:00Z</updated><resolved>2015-05-29T16:59:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-05-29T16:59:53Z" id="106871002">Closing - no longer relevant in 2.0
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Update plugins.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6712</link><project id="" key="" /><description>I'd like to add our plugins to plugins page:
- Elasticsearch River Web Plugin
- Elasticsearch Taste Plugin
</description><key id="37081951">6712</key><summary>[DOC] Update plugins.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">marevol</reporter><labels><label>docs</label></labels><created>2014-07-03T13:33:26Z</created><updated>2014-07-04T15:51:52Z</updated><resolved>2014-07-04T15:51:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T15:45:20Z" id="47946983">Hi @marevol 

Thanks for the PR. Please could I ask you to sign our CLA so that I can get your change merged in?
http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="marevol" created="2014-07-03T21:35:42Z" id="47987573">Submitted CLA signed by marevol(Shinsuke Sugaya).
Could you check it?
</comment><comment author="johtani" created="2014-07-04T15:51:52Z" id="48057749">Thanks for your contribution. just pushed your change at https://github.com/elasticsearch/elasticsearch/commit/4bddb4e3463f258c3a90e01a8b9d77c7a3538f5a

please go directly against master next time, as we usually back port from there
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] fixed ICU plugin documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6711</link><project id="" key="" /><description>add ICU Normalization CharFilter to docs
</description><key id="37080695">6711</key><summary>[DOCS] fixed ICU plugin documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">johtani</reporter><labels><label>docs</label></labels><created>2014-07-03T13:18:50Z</created><updated>2014-07-03T13:22:42Z</updated><resolved>2014-07-03T13:22:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Hive Elasticsearch integration issue.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6710</link><project id="" key="" /><description>I am integrating Hive and elasticsearch as per:
http://www.elasticsearch.org/guide/en/elasticsearch/hadoop/current/hive.html#_writing_data_to_elasticsearch_2

and 

https://github.com/hortonworks/hadoop-tutorials/blob/master/Community/T07_Elasticsearch_Hadoop_Integration.md

I have started a EMR cluster on aws also started the elasticsearch cluster on 2 ec2 classic instances.
I am stuck up while trying to insert data from the hive table to hive-elasticsearch tbale it throws me the following error log:

Diagnostic Messages for this Task:
Error: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:427)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
        ... 9 more
Caused by: java.lang.RuntimeException: Error in configuring object
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)
        ... 14 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)
        ... 17 more
Caused by: java.lang.RuntimeException: Map operator initialization failed
        at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:126)
        ... 22 more
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:402)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:451)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:407)
        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:62)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:451)
        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:407)
        at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:186)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
        at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:543)
        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)
        at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:105)
        ... 22 more
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:339)
        ... 34 more

I have already tried many things like adding the jar files like : 
add jar hive/lib/hive-hbase-handler-0.11.0.jar;
add jar hive/lib/hbase-0.94.18.jar;
add jar hive/lib/zookeeper-3.4.3.jar;
add jar hive/lib/guava-15.0.jar;
add jar hive/lib/protobuf-java-2.5.0.jar;
add jar hive/lib/hive-serde-0.11.0.jar;
add jar hive/lib/hive-contrib-0.11.0.jar

it still does not work. 

I have downloaded the version of elasticsearch-hadoop jar file from the elasticsearch.org.

Could anyone help me track the issue. 

Thanks in advance.
Regards,
Surender.
</description><key id="37079788">6710</key><summary>Hive Elasticsearch integration issue.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/costin/following{/other_user}', u'events_url': u'https://api.github.com/users/costin/events{/privacy}', u'organizations_url': u'https://api.github.com/users/costin/orgs', u'url': u'https://api.github.com/users/costin', u'gists_url': u'https://api.github.com/users/costin/gists{/gist_id}', u'html_url': u'https://github.com/costin', u'subscriptions_url': u'https://api.github.com/users/costin/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/76245?v=4', u'repos_url': u'https://api.github.com/users/costin/repos', u'received_events_url': u'https://api.github.com/users/costin/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/costin/starred{/owner}{/repo}', u'site_admin': False, u'login': u'costin', u'type': u'User', u'id': 76245, u'followers_url': u'https://api.github.com/users/costin/followers'}</assignee><reporter username="">surrey-kapkoti</reporter><labels /><created>2014-07-03T13:07:25Z</created><updated>2014-07-04T15:31:08Z</updated><resolved>2014-07-04T15:31:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="surrey-kapkoti" created="2014-07-04T01:47:40Z" id="48002063">Hi Elasticsearch, I'm researching search in cloud for one of my products, and I started with elastic search though I am a bit skeptical about the support, I am stuck up at this point and not able to make further progress. If you don't mind could you help fix the issue on priority as it is a bit urgent if you don't mind, before I start exploring other options like aws cloudsearch, or solr. 

Thanks &amp; Regards,
Surender.
</comment><comment author="surrey-kapkoti" created="2014-07-04T06:43:01Z" id="48013257">I was able to fix the issue, had to copy the elasticsearch-hadoop jar file to hive/auxlib folder instead of hive/lib folder.
</comment><comment author="surrey-kapkoti" created="2014-07-04T11:51:23Z" id="48034900">I am facing another issue while doing the insert into the elasticsearch hive table: it shows the following on the console forever:

hive&gt; INSERT OVERWRITE TABLE eslogs SELECT s.time, s.ext, s.ip, s.req, s.res, s.agent FROM logs s;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_1404399520795_0102, Tracking URL = http://hostIpaddress:9046/proxy/application_1404399520795_0102/
Kill Command = /home/hadoop/bin/hadoop job  -kill job_1404399520795_0102
Hadoop job information for Stage-0: number of mappers: 1; number of reducers: 0
2014-07-04 11:47:34,762 Stage-0 map = 0%,  reduce = 0%
2014-07-04 11:48:34,792 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:35,843 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:36,885 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:37,939 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:38,983 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:40,073 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:41,122 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:42,166 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:43,358 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:44,399 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:45,549 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:46,595 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:47,645 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:48,689 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:49,805 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:50,892 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:51,936 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:52,979 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:54,042 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:55,083 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:56,126 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:57,176 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:58,221 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:48:59,321 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:49:00,388 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec
2014-07-04 11:49:01,473 Stage-0 map = 0%,  reduce = 0%, Cumulative CPU 2.27 sec

Any help would be really appreciated.

Thanks &amp; Regards,
Surender.
</comment><comment author="clintongormley" created="2014-07-04T15:31:08Z" id="48056404">Hi @surrey-kapkoti 

The GitHub issue list is for bug reports and feature requests. Please use the forums for general questions.  Have you read the docs? http://www.elasticsearch.org/guide/en/elasticsearch/hadoop/current/hive.html

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Native search script working for searches but not for percolation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6709</link><project id="" key="" /><description>The same native AbstractSearchScript that works in classic searches do not work from percolators (tested with Elasticsearch 2.0.0 snapshot).

Script code:
package test;
import java.util.Map;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.component.AbstractComponent;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.node.Node;
import org.elasticsearch.script.ExecutableScript;
import org.elasticsearch.script.NativeScriptFactory;
public class CooccurenceScriptFactory extends AbstractComponent implements NativeScriptFactory{
    private final Node node;
    @SuppressWarnings("unchecked")
    @Inject
    public CooccurenceScriptFactory(Node node, Settings settings) {
        super(settings);
        this.node = node;
    }
    @Override public ExecutableScript newScript (@Nullable Map&lt;String,Object&gt; params){
        return new CooccurenceScript(node.client(), logger, params);
      }
}
package test;
import org.elasticsearch.ElasticsearchIllegalArgumentException;
import org.elasticsearch.client.Client;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.xcontent.support.XContentMapValues;
import org.elasticsearch.script.AbstractSearchScript;
import org.elasticsearch.search.lookup.SourceLookup;
import java.util.List;
import java.util.Map;
public class CooccurenceScript extends AbstractSearchScript {
    private List&lt;String&gt; list = null;
    @SuppressWarnings("unchecked")
    public CooccurenceScript(Client client, ESLogger logger, @Nullable Map&lt;String,Object&gt; params) {
        Map&lt;String, Object&gt; map = params == null ? null : XContentMapValues.nodeMapValue(params.get("map"), null);
        if (map == null) {
            throw new ElasticsearchIllegalArgumentException("Missing the map parameter");
        }
        list = (List&lt;String&gt;) map.get("list");
        if (list == null || list.isEmpty()) {
            throw new ElasticsearchIllegalArgumentException("Missing the list parameter or list is empty");
        }
    }
    @Override
    public java.lang.Object run() {
        SourceLookup source = source();
        @SuppressWarnings("unchecked")
        List&lt;Object&gt; values = (List&lt;Object&gt;) source.get("source_field");
        if (values == null || values.isEmpty()) {
            return false;
        }
        for (Object localValue : values) {
            boolean result = true;
            for (String s : list) {
                result &amp;= ((String) localValue).contains(s);
            }
            if (result) {
                return true;
            }
        }
        return false;
    }
}

Search test:
# Create index and type arefresh

curl -XDELETE http://localhost:9200/index1
curl -XPOST http://localhost:9200/index1
curl -XPOST http://localhost:9200/index1/mytype/_mapping -d '{
  "mytype": {
    "properties": {
      "source_field": { "type": "string" }
    }
  }
}'
# Create one document

curl -XPOST "http://localhost:9200/index1/mytype" -d '{
  "source_field" : [ "this a that" ]
}'
curl -XGET "http://localhost:9200/index1/_refresh"
# Search the document

curl 'http://localhost:9200/index1/mytype/_search' -d '{
  "query": {
    "constant_score": {
      "filter": {
        "script": {
          "script": "cooccurenceScript",
          "params": {
            "map": { "list" : [ "a", "this" ] }
          },
          "lang": "native"
        }
      }
    }
  }
}'
Result:
{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"index1","_type":"mytype","_id":"cwrZpe8hR_KPPBVVB30AKw","_score":1.0,"_source":{
  "source_field" : [ "this a that" ]
}}]}}

Percolation test:
# Add a percolator using the same native script

curl -XPUT "http://localhost:9200/index1/.percolator/1" -d '{
  "query": {
    "constant_score": {
      "filter": {
        "script": {
          "script": "cooccurenceScript",
          "params": {
            "map": { "list" : [ "a" ] }
          },
          "lang": "native"
        }
      }
    }
  }
}'
# Percolate an identical document

curl -XPOST "http://localhost:9200/index1/mytype/_percolate" -d '
{
  "doc" : {
    "source_field" : [ "this a that" ]
  }
}'
Result:
{"took":3,"_shards":{"total":5,"successful":5,"failed":0},"total":0,"matches":[]}
</description><key id="37077201">6709</key><summary>Native search script working for searches but not for percolation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">petchema</reporter><labels /><created>2014-07-03T12:33:16Z</created><updated>2015-11-21T15:49:22Z</updated><resolved>2015-11-21T15:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-04T14:29:58Z" id="48050382">Did you see any error in your logs? I quickly tried out a mvel script in a script filter and that works out.
</comment><comment author="petchema" created="2014-07-07T12:47:29Z" id="48173497">No, there's nothing in the logs.

We switched to native scripting after some issues implementing the same logic with mvel scripts; Again it works for searching, but not for percolation (NullPointerException this time):

curl -XPOST 'http://localhost:9200/index1/mytype/_search' -d '{
  "filter": {
    "script": {
      "script": "foreach (String value : _source[field]) { boolean ok = true; foreach (String word: words) { if (!value.contains(word)) { ok = false; }} if (ok) return true; } return false;",
      "params": {
        "field": "source_field",
        "words": [
          "a"
        ]
      }
    }
  }
}'
Result:
{"took":4,"timed_out":false,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"index":"index1","shard":0,"status":500,"reason":"QueryPhaseExecutionException[[index1][0]: query[ConstantScore(cache(_type:mytype))],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "}]},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"index1","_type":"mytype","_id":"9I9u4oh0SomkXvzaM-KmEg","_score":1.0,"_source":{
  "source_field" : [ "this a that" ]
}}]}}

curl -XPUT "http://localhost:9200/index1/.percolator/1" -d '{
  "query": {
    "constant_score": {
      "filter": {
    "script": {
      "script": "foreach (String value : _source[field]) { boolean ok = true; foreach (String word: words) { if (!value.contains(word)) { ok = false; }} if (ok) return true; } return false;",
      "params": {
            "field": "source_field",
        "words": ["a"]
      },
          "lang": "mvel"
    }
      }
    }
  }
}'

curl -XPOST "http://localhost:9200/index1/mytype/_percolate" -d '
{
  "doc" : {
    "source_field" : [ "this a that" ]
  }
}'
Result:
{"took":3,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"index":"index1","shard":2,"reason":"BroadcastShardOperationFailedException[[index1][2] ]; nested: PercolateException[failed to percolate]; nested: PercolateException[failed to execute]; nested: NullPointerException; "}]},"total":0,"matches":[]}
</comment><comment author="petchema" created="2014-07-07T12:53:16Z" id="48174033">Log for the latter:
[2014-07-07 14:46:08,561][DEBUG][action.percolate         ] [Greer Grant Nelson] [index1][2], node[XkBXnyiRSZ6UhEoEDnXpHg], [P], s[STARTED]: failed to executed [org.elasticsearch.action.percolate.PercolateRequest@2d7281bf]
org.elasticsearch.percolator.PercolateException: failed to percolate
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:198)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.percolator.PercolateException: failed to execute
    at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:551)
    at org.elasticsearch.percolator.PercolatorService.percolate(PercolatorService.java:233)
    at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:194)
    ... 5 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.mvel2.ast.ForEachNode.getReducedValueAccelerated(ForEachNode.java:109)
    at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
    at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:196)
    at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter$ScriptDocSet.matchDoc(ScriptFilterParser.java:184)
    at org.elasticsearch.common.lucene.docset.MatchDocIdSet$NoAcceptDocsIterator.nextDoc(MatchDocIdSet.java:96)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:257)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:184)
    at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)
    at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)
    at org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:548)
    ... 7 more
</comment><comment author="petchema" created="2014-07-17T08:13:13Z" id="49271134">Up?
Can you reproduce the issue?
</comment><comment author="petchema" created="2014-08-18T09:57:06Z" id="52472254">Anything else I can try?
</comment><comment author="imotov" created="2014-08-19T02:46:26Z" id="52584013">@petchema as a workaround you can try adding mapping for `source_field` as `not_analyzed` field and then access it using `_doc[field].values` instead of `_source[field]`.

`_source` seems to be populated only if highlighting is [turned on](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/percolator/PercolatorService.java#L350) and even then there seem to be some issues accessing it through source lookup. Not really sure how to address this issue. We can extract and populate source every time for every record, but this will slow everything down. Alternatively, we can add some kind of `add_source` flag or replace source lookup with some lazy loading version. 
</comment><comment author="petchema" created="2014-08-21T15:57:32Z" id="52941061">Thanks, tests using this workaround seem to work fine, we need more tests to see if they're any drawbacks or limitations for our needs but we're making progress.
Regards,
Pierre.
</comment><comment author="clintongormley" created="2015-11-21T15:49:21Z" id="158656994">Accessing the `_source` in a percolator is a really bad idea, for performance (and thus cluster stability) reasons.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Aggregation names can now include dot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6708</link><project id="" key="" /><description>Aggregation name are now able to use any character except '[', ']', and '&gt;'. Dot syntax may still be used to reference values (e.g. in the order field) but may only reference the value directly beneath the last aggregation in the list. more complex structures will need to be accessed using the aggname[value] syntax

Closes #6702
</description><key id="37069454">6708</key><summary>Aggregations: Aggregation names can now include dot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-07-03T11:47:05Z</created><updated>2014-08-21T15:07:53Z</updated><resolved>2014-07-08T14:08:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-04T14:51:04Z" id="48051972">relates to #6736.
</comment><comment author="s1monw" created="2014-07-08T13:12:53Z" id="48333614">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve performance for many new fields introduction in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6707</link><project id="" key="" /><description>When we have many new fields keep being introduced, the immutable open map we used becomes more and more expensive because of its clone characteristics, and we use it in several places.

The usage semantics of it allows us to also use a CHM if we want to, but it would be nice to still maintain the concurrency aspects of volatile immutable map when the number of fields is sane.

Introduce a new map like data structure, that can switch internally to CHM when a certain threshold is met.

Also add a benchmark class to exploit the many new field mappings use case, which shows significant gains by using this change, to a level where mapping introduction is no longer a significant bottleneck.
</description><key id="37067945">6707</key><summary>Improve performance for many new fields introduction in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-03T11:23:18Z</created><updated>2015-06-07T13:04:36Z</updated><resolved>2014-07-05T15:40:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-03T11:52:54Z" id="47896929">I'm not sure how much I like introducing a mutable map here. Maybe we should also consider eg. using a persistent [hash array mapped trie](http://en.wikipedia.org/wiki/Hash_array_mapped_trie) (although I never used one myself so I'm unsure about the performance impact) or batching updates to the immutable map by having a wrapper around a big immutable map and a smaller one that only carries the last updates?
</comment><comment author="s1monw" created="2014-07-03T15:44:33Z" id="47946884">I am leaning towards what @jpountz said though. I think we should be careful with this mutability
</comment><comment author="kimchy" created="2014-07-04T13:55:42Z" id="48046580">This change doesn't use ImmutableOpenMap where it was used for concurrency story (copy on write), it is still a mutable behavior on the class itself. I looked into doing paginated immutable open map, but the performance was not good because of the cloning (and maintaining the pages). At the end, with many fields, CHM is the best data structure when there are many fields.

I like the idea that for the common case, with not many fields, we gain the concurrency story of copy on write. I like this change since it will fail if we misuse the data structure in mutating it concurrently, a protection we didn't have before.

I have added the ability to externally set using settings the switch size, mainly for testings, so now its fully randomized so we test the switch case also when we have small number of fields.
</comment><comment author="jpountz" created="2014-07-04T14:47:33Z" id="48051689">I left a couple of comments but in general this looks good to me!
</comment><comment author="kimchy" created="2014-07-04T15:19:52Z" id="48055114">@jpountz thanks!, applied the changes (had to force push, too many small ones accumulated)
</comment><comment author="jpountz" created="2014-07-04T23:21:27Z" id="48074139">Just left a comment about the implementation of `values()`. Could you also move the removal of the calls to intern() to another PR?
</comment><comment author="kimchy" created="2014-07-05T09:40:12Z" id="48082363">@jpountz didn't see the note on values since my last push, is it still relevant? I have removed the call to intern already....
</comment><comment author="jpountz" created="2014-07-05T09:45:51Z" id="48082464">That's weird, I couldn't see the changes yesterday but now I can... LGTM
</comment><comment author="s1monw" created="2014-07-05T12:57:27Z" id="48085896">I left a bunch of comments. I like the improvement though.
</comment><comment author="kimchy" created="2014-07-05T14:12:09Z" id="48087303">@s1monw applied your comments, ready for another round
</comment><comment author="s1monw" created="2014-07-05T15:37:26Z" id="48089219">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Start Master|Node fault detection pinging immediately during discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6706</link><project id="" key="" /><description>After a node joins the clusters, it starts pinging the master to verify it's health. Before, the cluster join request was processed async and we had to give some time to complete. With  #6480 we changed this to wait for the join process to complete on the master. We can therefore start pinging immediately for fast detection of failures. Similar change can be made to the Node fault detection from the master side.
</description><key id="37065660">6706</key><summary>Start Master|Node fault detection pinging immediately during discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-03T10:46:02Z</created><updated>2015-06-07T13:04:51Z</updated><resolved>2014-07-03T12:53:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-03T10:49:32Z" id="47892100">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch slow responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6705</link><project id="" key="" /><description>![screenshot-admin elast io 2014-07-02 22-42-29 copy](https://cloud.githubusercontent.com/assets/2588178/3467907/38ee1440-0297-11e4-9b6e-c39db90b8291.png)

Hi there,

During the last few weeks we have experienced some really slow query response times, something between `5 and 8` seconds per request. As you can see in the attached screenshot we have 11 nodes in our cluster while one node is always on red the other ones are on average, looks like the requests are not distributed the right way which causes one node to overload. To note that it's not that the same node is overloaded, it changes from time to time. Any help would be really appreciated. If more information about the cluster is needed, I can easily provide it.

Thank you!
</description><key id="37062062">6705</key><summary>Elasticsearch slow responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">teckays</reporter><labels /><created>2014-07-03T09:52:33Z</created><updated>2014-07-03T16:00:28Z</updated><resolved>2014-07-03T10:52:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T10:52:17Z" id="47892318">Hi @stalbe 

This issues list is for bug reports and feature requests. Please ask questions like these in the forum instead.  Btw, you should look at the hot threads output for the red node to find out what is using the CPU: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html#cluster-nodes-hot-threads
</comment><comment author="teckays" created="2014-07-03T15:21:24Z" id="47943851">Could you provide a link to an official forum where I could post my questions on?
</comment><comment author="dakrone" created="2014-07-03T15:22:16Z" id="47943957">@stalbe the forum is available here: https://groups.google.com/forum/?fromgroups#!forum/elasticsearch
</comment><comment author="teckays" created="2014-07-03T15:49:48Z" id="47947672">@dakrone Thank you!
</comment><comment author="teckays" created="2014-07-03T16:00:28Z" id="47949112">@dakrone do you know someone I could ask for some help, someone I could contact directly, the forum takes too long to get an answer while the problem can't really wait, we're loosing some traffic right now because of this overloaded node.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for size and order in all bucket aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6704</link><project id="" key="" /><description>At the moment not all of the bucket aggregations support returning buckets in a particular order or limiting the number of returned buckets.  Because of this the aggregations that support size and order have their own implementations for this.  It would be nice to have a common implementation which can be used by all bucket aggregations.  This would also ensure the implementations are consistent in functionality
</description><key id="37060352">6704</key><summary>Support for size and order in all bucket aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2014-07-03T09:28:22Z</created><updated>2016-02-01T17:30:47Z</updated><resolved>2015-11-23T09:03:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-03T09:47:01Z" id="47886849">+1
</comment><comment author="jpountz" created="2014-07-08T10:18:54Z" id="48318408">+1
</comment><comment author="s1monw" created="2014-07-09T10:26:54Z" id="48453488">moving out to 1.4 for now
</comment><comment author="djnelson9715" created="2014-09-03T22:11:26Z" id="54375647">+1 as well for this feature.  Is this going to be in an upcoming release?
</comment><comment author="Vineeth-Mohan" created="2014-09-04T02:32:43Z" id="54396493">+1
</comment><comment author="jaric" created="2014-09-24T11:51:45Z" id="56658659">+1
</comment><comment author="clintongormley" created="2015-11-21T15:41:05Z" id="158653490">@colings86 still of interest?
</comment><comment author="colings86" created="2015-11-23T09:03:26Z" id="158880694">@clintongormley No, with the addition of Pipeline Aggregations I think it would be better to have a 'sort and truncate' pipeline aggregation to cover this functionality in a generic way across all aggregations rather than having to implement it for each one individually. I have created an issue (https://github.com/elastic/elasticsearch/issues/14928) for this so I'll close this in favour of it
</comment><comment author="jpountz" created="2015-11-23T09:43:27Z" id="158889041">+1
</comment><comment author="McStork" created="2016-02-01T17:27:42Z" id="178082501">This feature seems pretty cool. If it was implemented outside of the Pipeline  Aggregations, it would perform faster though, right?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Expose `tests.filter` for elasticsearch tests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6703</link><project id="" key="" /><description>`-Dtests.filter` allows to pass filter expressions to the elasticsearch
tests. This allows to filter test annotaged with TestGroup annotations
like @Slow, @Nightly, @Backwards, @Integration with a boolean expresssion like:
- to run only backwards tests run:
   `mvn -Dtests.bwc.version=X.Y.Z -Dtests.filter="@backwards"`
- to run all integration tests but skip slow tests run:
   `mvn -Dtests.filter="@integration and not @slow"
- to take defaults into account ie run all test as well as backwards:
   `mvn -Dtests.filter="default and @backwards"

This feature is a more powerful alternative to flags like
`-Dtests.nighly=true|false` etc.
</description><key id="37059299">6703</key><summary>[TEST] Expose `tests.filter` for elasticsearch tests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2014-07-03T09:14:22Z</created><updated>2014-07-03T09:42:12Z</updated><resolved>2014-07-03T09:42:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-03T09:23:00Z" id="47884714">the `and` and `and not` special syntax is not mentioned anywhere, maybe put it into the `TESTING.asciidoc` as well?
</comment><comment author="jpountz" created="2014-07-03T09:23:37Z" id="47884763">Neat! LGTM
</comment><comment author="s1monw" created="2014-07-03T09:34:54Z" id="47885766">@spinscale can you take another look
</comment><comment author="spinscale" created="2014-07-03T09:38:30Z" id="47886117">minor typo, LGTM
</comment><comment author="s1monw" created="2014-07-03T09:41:38Z" id="47886415">thanks fixed...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend allowed characters in aggregation name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6702</link><project id="" key="" /><description>Currently aggregations names must match the pattern `[a-zA-Z0-9\\-_]+`. This is pretty restrictive and prevents, for example, a common naming pattern of using dots in names. We should extend this pattern to be less restrictive.

Currently an aggregation can refer to values within child aggregations using two methods:
1. agg_name['val_name']
2. agg_name.val_name

Allowing dots in the aggregation name will mean that we drop support for option 2 in favour of 1.
</description><key id="37058336">6702</key><summary>Extend allowed characters in aggregation name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-03T08:59:36Z</created><updated>2015-06-07T13:05:01Z</updated><resolved>2014-07-08T14:06:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="felixbarny" created="2014-07-03T10:00:46Z" id="47888065">Not understaning the reason why only alpa-numeric chars are allowed, wouldn't it be possible to allow any char by internally working with hexadecimal encoded strings? I would really wish, that arbitrary chars would still be allowed. My current workaround is to do the hex encoding manually. My aggregation names are computed dynamically and are displayed on the frontend, so avoiding non-alpha-numerics is not an option for me.

("cross-posted" from #5253)
</comment><comment author="clintongormley" created="2014-07-03T10:46:44Z" id="47891874">@colings86 What about allowing escaping of dots? eg "foo\.bar.baz"  (as JSON "foo\\.bar.baz")?
</comment><comment author="colings86" created="2014-07-04T14:51:29Z" id="48052006">relates to #6736 
</comment><comment author="felixbarny" created="2014-07-08T15:39:37Z" id="48356178">Good job!
Will it be included in 1.2.2? Is there some place that shows when the next release is due?
</comment><comment author="colings86" created="2014-07-08T16:09:43Z" id="48361243">This will be available from version 1.3 as 1.2.2 is intended for bug fixes. 

We try to release every couple of months, but it depends on the features we are working on.  We don't announce release dates ahead of time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"copy_to" behaviour in nested fields doesn't match the mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6701</link><project id="" key="" /><description>I have a nested field, containing some fields with the 'copy_to' attribute. I index a document. Then, I look at the mappings - these show the 'copy_to' field as appearing in the root object, not the nested objects.

(See https://gist.github.com/tstibbs/dc36483f96f8d1471385 for the necessary commands to reproduce.)

However, the fields appear to be copied into the 'copy_to' field in the nested object.

If the 'copied' field is created in the root object, the following query should return my document (but it doesn't):

```
{
  "query": {
    "bool": {
      "must": [
        {
          "match" : {
            "copied" : "rover"
          }
        },
        {
          "nested": {
            "path": "children",
            "score_mode": "avg",
            "query": {
              "match": {
                "colour": "red"
              }
            }
          }
        }
      ]
    }
  }
}
```

If the 'copied' field is created in each child, then following query should match (it does):

```
{
  "query": {
    "nested": {
      "path": "children",
      "score_mode": "avg",
      "query": {
        "bool": {
          "must": [
            {
              "match": {
                "copied": "rover"
              }
            },
            {
              "match": {
                "colour": "red"
              }
            }
          ]
        }
      }
    }
  }
}
```

To me, the current behaviour makes sense - the fields are copied to a field within the item itself. However, I think the issue is that the mapping doesn't match the behaviour, and the documentation doesn't specify which is correct.

So, I can't currently use this feature, because I don't know which issue is the bug!
</description><key id="37058026">6701</key><summary>"copy_to" behaviour in nested fields doesn't match the mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">tstibbs</reporter><labels /><created>2014-07-03T08:55:26Z</created><updated>2015-01-21T13:35:14Z</updated><resolved>2014-08-01T07:08:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T06:44:28Z" id="50113415">I think that copying to field `foo`,  then that field should be in the main document, but copying to field `some.nested.path.foo` should create that field in the nested document.
</comment><comment author="tstibbs" created="2014-07-25T07:08:35Z" id="50115203">I agree.

Being able to copy to a field within the nested document is very useful because it allows us to get around the fact that the _all field is only on the root object.
</comment><comment author="tstibbs" created="2014-07-25T07:35:40Z" id="50117346">Actually, your suggestion is more or less the current behaviour (I hadn't thought of specifying the full path). If give the full path (`children.copied` in my example) then the field does get indexed in the child and the mapping shows it in the correct place.

So actually, I guess the only bug is that it puts the value in the child even when you tell it to put it in the parent.
</comment><comment author="apatrida" created="2015-01-21T07:56:44Z" id="70797950">And it never was really clear what the final behaviour should be with nested docs, nor is it in the documentation.  Is the full path required to get the field into child, without path does it go to parent?
</comment><comment author="clintongormley" created="2015-01-21T13:35:14Z" id="70837115">@jaysonminard The full path is required to get the field into the nested doc.  This will become more obvious when we start requiring the full path for all fields, as in https://github.com/elasticsearch/elasticsearch/issues/8872
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Allow to default default_operator in query_string / field queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6700</link><project id="" key="" /><description>Hello!

I have a feature request to also add "indices.query.query_string.default_operator" to [#1540] so we can set it to default AND in a config file.

Also, it would be nice to set this per index (in e.g. templates' settings).
</description><key id="37055317">6700</key><summary>Query DSL: Allow to default default_operator in query_string / field queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kjelle</reporter><labels /><created>2014-07-03T08:12:21Z</created><updated>2015-11-21T15:40:30Z</updated><resolved>2015-11-21T15:40:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T15:40:30Z" id="158653327">The settings added in #1540 are about safety, while the default operator is about retrieval... I'm loathe to add yet another setting.

Given that there hasn't been any further interest in this ticket in the last year and a half, I'm going to close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature: Add ability to profile queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6699</link><project id="" key="" /><description>This PR adds the ability to profile queries and determine which portions of the query are the slowest.  All timings are relative and displayed in microseconds.

Timings are not guaranteed to match the `took` time of the query...in fact, they will likely be larger.  This is because timings are aggregated from all shards, often executing in parallel, so the profiled time is more akin to CPU time than wallclock time.

It is also important to understand that Profile times represetn relative costs, rather than absolute times.  The act of profiling adds considerable overhead to the querying process since all components in the query must be wrapped and timed.
### Enabling Profiling

Profling can be enabled with a query param, URI param or dedicated endpoint:

``` bash
# Query param
curl -XGET localhost:9200/my_index/my_type/_search -d '{
  "profile": true,
  "query" : { ... }
}'

# URI param
curl -XGET localhost:9200/my_index/my_type/_search?profile=true

# Endpoint
curl -XGET localhost:9200/my_index/my_type/_profile -d '{
  "query" : { ... }
}'
```
### Response output

When enabled, the response will contain an additional `profile` element.  This will contain a tree of "profile components", where each component represents a portion of the query.  Often, this tree of components will be larger than you expect - Elasticsearch and Lucene will expand and rewrite portions of the query, which introduces more pieces than you might expect.

Each component contains the following:
- **type** - The name of the Elasticsearch/Lucene class for this part of the query, e.g. `TermQuery`
- **time** - The total aggregated time at this level in the tree, which includes all children and all shards.  Displayed as microseconds
- **relative** - Relative contribution of this level in the tree.  This metric is `time / total_time`.  Displayed as a percentage
- **lucene** - A rough approximation of the Lucene syntax for the component, which is often helpful in differentiating many similar components (e.g. differentiating `term: { "field1": "fred" }` from `term: { "field1": "bob" }`, since both are `TermQuery`s)
- **components** - Zero or more additional components which are children to this level.  For example, a Bool will have children components representing the various clauses.

``` json
{
   "took": 99,
   "timed_out": false,
   "_shards": { ... },
   "hits": { ... },
   "profile": [
      {
         "type": "XFilteredQuery",
         "time": 153996,
         "relative": "100.00%",
         "lucene": "filtered(filtered(my_field:the my_field:quick my_field:fox)-&gt;cache(input1:[1 TO 1]))-&gt;cache(_type:test)",
         "components": [
            {
               "type": "XFilteredQuery",
               "time": 108036,
               "relative": "70.155%",
               "lucene": "filtered(my_field:the my_field:quick my_field:fox)-&gt;cache(input1:[1 TO 1])",
               "components": [
                  {
                     "type": "BooleanQuery",
                     "time": 62053,
                     "relative": "40.295%",
                     "lucene": "my_field:the my_field:quick my_field:fox",
                     "components": [
                        {
                           "type": "TermQuery",
                           "time": 20366,
                           "relative": "13.225%",
                           "lucene": "my_field:the"
                        },
                        {
                           "type": "TermQuery",
                           "time": 1117,
                           "relative": "0.72534%",
                           "lucene": "my_field:quick"
                        },
                        {
                           "type": "TermQuery",
                           "time": 391,
                           "relative": "0.25390%",
                           "lucene": "my_field:fox"
                        }
                     ]
                  },
                  {
                     "type": "FilterCacheFilterWrapper",
                     "time": 3983,
                     "relative": "2.5864%",
                     "lucene": "cache(input1:[1 TO 1])"
                  }
               ]
            },
            {
               "type": "FilterCacheFilterWrapper",
               "time": 3835,
               "relative": "2.4903%",
               "lucene": "cache(_type:test)"
            }
         ]
      }
   ]
}
```
### Todo / Known Issues
- More tests!  The current tests check for basic validity and overt failure, but I think we need a better way to check the contents of the profile response.  + Unit tests for things like the ProfileVisitors
- Shards can rewrite queries differently (e.g. if the term is not present on one shard), which means chunks of the output could potentially be skipped.  Naively handling this by just ignoring components which don't match.
- Potentially gross output, especially with numerics being expressed as byte strings (e.g. `\0001`)
- Fairly abusive use of Visitor pattern.  In particular, I'm not overly happy with the ProfileCollapsingVisitor which is used to collect timings from the query tree...but I have no idea how to better structure it.  The biggest problem is that all query types need to be hardcoded, so this will be a maintenance tar pit.
- Audit of the supported Query/Filter types...I imagine there are others that I'm missing which need to be added to both visitors
-  `/_profile` endpoint could use some love, ideally extending the search endpoint instead of c/p
</description><key id="37038776">6699</key><summary>Feature: Add ability to profile queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Search</label><label>feature</label></labels><created>2014-07-03T01:02:25Z</created><updated>2015-11-26T13:43:32Z</updated><resolved>2015-07-10T21:18:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T09:46:30Z" id="47886809">w00t
</comment><comment author="nik9000" created="2014-07-07T15:06:33Z" id="48190765">Sounds cool!
</comment><comment author="jpountz" created="2014-07-17T07:56:12Z" id="49269824">I left a couple of comments but this looks really great, I like the way that you implemented it.

API-wise, I think you should put the result of the profile under a `query` element, so that we could add more information to it in the future eg. about rescoring, fetching _source from disk, highlighting or aggregations?

Is there a reason to have 3 ways to enable query profiling? I'd rather only have one...
</comment><comment author="jprante" created="2014-09-16T07:31:09Z" id="55706239">From what I understand, the query profiler is not meant for shard level execution time tracing? I can think of situations where it is useful to examine query execution times on the shard level, for identifying slow nodes etc. This is what I tried to suggest in the issue #3041
</comment><comment author="polyfractal" created="2014-09-17T17:27:30Z" id="55929476">@jprante At the moment, it isn't supported.  But it would be fairly trivial to implement.  The coordinating node receives the execution profiles from all the individual shards and merges them together.  A flag that skips the merging process and returns a profile-per-node wouldn't be difficult.

I will be getting back to this PR soon and think such a flag would be a good addition.  The tricky bit is that each profile will need to include node identification.  I imagine it should be doable, but I'll need to investigate how to do it.
</comment><comment author="jprante" created="2014-09-17T19:13:07Z" id="55944818">@polyfractal one possibility is just to return the shard IDs to the user, this info should be present in the transport action. The corresponding node could be looked up in the cluster state by the user.
</comment><comment author="polyfractal" created="2014-10-01T21:25:00Z" id="57541198">- Merged 1.x into the branch
- Removed the `/_profile` endpoint, I didn't like it anyway :)  I left the URI option (`?profile=true`) and the request body option (`"profile" : true) since those are both convenient
- ProfileQuery and ProfileFilter now use a single Stopwatch instance
- bugfix for single shard results, bugfix for boosts, update serialization for v1.5
- Response format now includes both "query" and a new "all_shards" component.  The "all_shards" is so that we can break out per-node / per-shard responses later without breaking backwards compatibility

``` json
{
...
    "profile": {
      "query": {
         "all_shards": {
            "type": "XConstantScoreQuery",
            "time": 124214,
            "relative": "100.00%",
            "lucene": "ConstantScore(*:*)",
            "components": [
               {
                  "type": "MatchAllDocsFilter",
                  "time": 5442,
                  "relative": "4.3811%",
                  "lucene": "*:*"
               }
            ]
         }
      }
    }
}
```

Todo:
- After finding the boost bug, I'd like to add some tests that compare profiled search results to regular results, to make sure they are identical
- Expand the randomized query generator to cover more query types...I suspect there are some more exotic queries which are not being tested
- Docs
</comment><comment author="clintongormley" created="2014-10-20T13:33:31Z" id="59754006">@rmuir could you take a look at this one please?
</comment><comment author="aaneja" created="2015-03-23T19:30:22Z" id="85159433">Ping. Any updates on this ?
Also +1 for @jprante request; it would be helpful to determine what is happening at the shard and node level.
It would also be great to have a hook to log profiling data to the slow log.
</comment><comment author="polyfractal" created="2015-03-23T22:36:53Z" id="85234304">@aaneja I actually started to revisit this just last week.  The current branch went a bit stale because I was concerned about the maintainability of the approach.  It basically works by wrapping each component in the query with a special ProfileQuery / ProfileFilter.  To do this, it walks the query tree recursively using a double-dispatcher.  It does a similar process after the query is done, to "unwrap" the profile timings and build a response tree.

The problem is that the double-dispatcher needs a lot of "special cases", because many queries are special snowflakes.  E.g. need to call `getQueries()` on a `BooleanQuery` query, `getQuery()` and `getFilter()` on a `ConstantScoreQuery`, etc.  Basically the walker becomes a maintenance tarpit, because it will need to be updated any time new queries are added/removed, or if existing queries change.

I started working on a new approach (here: https://github.com/polyfractal/elasticsearch/tree/features/pq2) which hooks into the IndexSearcher, which is basically the main class that Lucene uses to drive queries.  A lot of recent changes in Lucene make this a lot cleaner.

The current strategy is to push some of the functionality back into Lucene (ProfileIndexSearcher, ProfileQuery, etc) and make Elasticsearch responsible for the rest.  

Still a WIP, but I'm optimistic it'll be a better solution.
</comment><comment author="clintongormley" created="2015-04-05T14:42:00Z" id="89784653">Copied across @kamaradclimber's comments from https://github.com/elastic/elasticsearch/issues/10348#issuecomment-88418315

&gt; @polyfractal this is a very great step towards what I am looking for, I'd really like this to be merged.
&gt; 
&gt; Improvements I can think of once your patch is in master:
&gt; - display if the query used a cache or not
&gt; - display merge time (time to merge the results of subqueries/filters)
</comment><comment author="tomryanx" created="2015-07-10T15:41:39Z" id="120439188">+1
</comment><comment author="clintongormley" created="2015-07-12T10:28:32Z" id="120706589">@polyfractal this was autoclosed when we deleted 1.x.  Please could you reopen against master
</comment><comment author="bakks" created="2015-07-27T21:16:57Z" id="125345557">+1 for this branch, this is a sorely needed feature!
</comment><comment author="mpereira" created="2015-08-27T22:18:32Z" id="135571865">+1
</comment><comment author="deimosfr" created="2015-09-02T08:33:27Z" id="136975982">+1
</comment><comment author="makeyang" created="2015-11-26T09:01:38Z" id="159852134">+1
</comment><comment author="makeyang" created="2015-11-26T09:03:34Z" id="159852548">when will this feature be released?
</comment><comment author="polyfractal" created="2015-11-26T13:43:31Z" id="159917286">See https://github.com/elastic/elasticsearch/pull/14889 for current status.  Probably 2.2 or 2.3, assuming the review process goes smoothly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOC] Regexp term clarification</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6698</link><project id="" key="" /><description>The `RegExp` query didn't behave like I expected, and only after some trial-and-error and searching StackOverflow did I finally realize that "term queries" was a very important part of the first sentence in the `RegExp` documentation.

This attempts to clarify that so that others do not suffer the same fate.

It also, incidentally, updates the Lucene javadoc link, which I just fixed while I was editing this file.
</description><key id="37035109">6698</key><summary>[DOC] Regexp term clarification</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdaniel</reporter><labels /><created>2014-07-02T23:42:25Z</created><updated>2014-07-03T15:26:52Z</updated><resolved>2014-07-03T09:40:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Memory-bound terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6697</link><project id="" key="" /><description>Our terms aggregations implementations use an amount of memory that is linear with the cardinality of the value source they run on. Thanks to global ordinals, we only require 4 bytes per unique term to track the count, so even with a field that has a cardinality of 10M, that would only be 40MB of memory.

However, things get worse when using sub aggregations, especially the memory-intensive ones such as `percentiles`, `cardinality`, `top_hits` or bucket aggregations. Ideally we would want memory usage to depend on `size` instead of the cardinality of the value source.

I have been looking recently at the `Space-Saving` algorithm described in section 3.1 of [`Efficient Computation of Frequent and Top-k Elements in Data Streams`](https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf). Although described for computing top buckets based on counts, I think it would be possible to use it when sorting by term or sub-aggregation. But you don't get optimized memory usage for free so it would have two main drawbacks compared to the current implementation:
1. it would only work correctly on skewed distributions
2. there is no clear separation between buckets and some buckets might aggregate document from other buckets

I think `1` is fine since top terms tend to make more sense on skewed distributions anyway, eg. most frequent terms in natural language text, or top ip addresses that hit a website.

`2` is more inconvenient, especially if there are other aggregations under this terms aggregation. One good news is that we could know what buckets might have aggregated data that is not theirs (they would be those whose term ordinal has been updated during collection), so we could have it as part of the response if necessary. On the other hand, if there are no sub aggregations, it would be more acceptable to use this implementation since [counts are inaccurate anyway](https://github.com/elasticsearch/elasticsearch/issues/1305). And it would also work nicely with https://github.com/elasticsearch/elasticsearch/issues/6696 since the maximum error can be estimated.
</description><key id="37032658">6697</key><summary>Aggregations: Memory-bound terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>discuss</label><label>enhancement</label></labels><created>2014-07-02T22:55:33Z</created><updated>2015-11-23T10:51:07Z</updated><resolved>2015-11-23T10:51:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="redox" created="2014-07-21T11:49:37Z" id="49596271">Hey there,

with such algorithm the top buckets are determined once all values have been processed, right? Therefore, the sub (bucket-specific &amp; nested) aggregations can only be computed once all documents have been collected in order to know the final list of buckets, isn't it? Because if I'm right, for each collected document (method `collect`) we need to call `collectableSugAggregators.collect` with the associated `bucketOrd` (which will be unknown at this step). Am I missing something?

Instead, in case of a count-based (or terms-based) sorting, any chance the sub aggregations could be collected in another pass?
</comment><comment author="jpountz" created="2014-07-21T13:21:53Z" id="49604128">&gt; Therefore, the sub (bucket-specific &amp; nested) aggregations can only be computed once all documents have been collected

I think sub aggregations could be collected on the fly. The algorithm works on a fixed set of `m` counters, so we could associate a bucket ordinal (between `0` and `m-1`) with each of these counters and use them to collect the sub aggregations.

It might introduce accuracy issues for the sub aggregations since counters can be relabeled during collection but this is the general trade-off of this algorithm: trading accuracy for memory, so I think it's fine? Moreover if the distribution of the data is skewed enough and if we oversize `m` a bit compared to the number of top terms that we are interested in, the likelyhood of the top terms being affected by this issue should remain low.
</comment><comment author="redox" created="2014-07-21T13:53:19Z" id="49607744">Makes sense, we're on the same page! Thank you @jpountz 
</comment><comment author="redox" created="2014-07-23T17:33:34Z" id="49907615">Just wanna let you know that I've released a plugin embedding such algorithm and targeting ES +1.2. Our first tests are pretty concluant :+1: 

It's still in alpha version but I would love any comment/review/pull-request of ES gurus reading this comment :)

Plugin is here: [elasticsearch-topk-plugin](https://github.com/algolia/elasticsearch-topk-plugin)
</comment><comment author="jpountz" created="2014-07-23T18:00:44Z" id="49911217">It looks good in general. FYI you would get better performance by specializing on numerics/strings like the terms aggregation does. On numerics this would only help a bit by saving the conversion to BytesRef and allowing to use more efficient data-structures, but on strings this could have a big impact. The way field data works on strings is that there is a first level of indirection that gives ordinals given a doc ID, and a second level of indirection that gives the values given an ordinal. Ordinals have the interesting property of being dense (so that you can use them as indices in arrays) and sorted (useful for comparisons, would help if you want to sort buckets of this top-k agg by term). Additionally they are fast to retrieve. On the other hand, retrieving values given an ordinal might be slow, especially if you are using doc values. That's why our aggregations try to use ordinals whenever possible as this brings important speedups.
</comment><comment author="redox" created="2014-07-23T18:03:57Z" id="49911666">Thanks for the inputs @jpountz, I'm definitely planning to use the ordinals but wanted to have a first working version first.
</comment><comment author="clintongormley" created="2015-11-21T15:38:10Z" id="158653064">@jpountz is this still something you plan on pursuing?
</comment><comment author="jpountz" created="2015-11-23T10:51:07Z" id="158903255">Yes. I think it can be useful for pagination in particular, when sorting by term. I'll close for now and reopen when I have more concrete plans.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Return an upper bound of the maximum error for terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6696</link><project id="" key="" /><description>The fact that terms aggregations don't give accurate counts is a bit [deceptive](https://github.com/elasticsearch/elasticsearch/issues/1305). Without changing the way they are implemented, maybe we should make terms aggregations return an upper bound of the maximum error on the document count as part of the response? I think this would help make clear that there are potential accuracy issues, as well as make this inaccuracy easier to manage since there is a known upper bound on the error?
</description><key id="37032079">6696</key><summary>Aggregations: Return an upper bound of the maximum error for terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T22:46:05Z</created><updated>2014-09-11T10:56:43Z</updated><resolved>2014-07-25T13:25:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Clean shard bulk mapping update to only use type name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6695</link><project id="" key="" /><description>today we track both teh index name and type for mapping updates in the shard bulk action, but we only work against on index in this level, so no need to track the index name itself
</description><key id="37022893">6695</key><summary>Clean shard bulk mapping update to only use type name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T20:48:47Z</created><updated>2015-06-07T13:05:31Z</updated><resolved>2014-07-02T22:39:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-02T21:33:21Z" id="47839856">LGTM.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for index blocks against concrete indices on master operations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6694</link><project id="" key="" /><description>Some master operations require to check for index blocks, done by overriding `TransportMasterNodeOperationAction#checkBlock`. We need to make sure though that the check is done against concrete indices, otherwise we might not get blocks back altough there are.

For instance you can currently delete a read-only index using an alias that points to it, or using the `_all` alias:

```
curl -XPUT localhost:9200/foo

curl -XPUT 'localhost:9200/foo/_settings' -d '
{
    "index" : {
        "blocks.read_only" : true
    } }
'

curl -XDELETE localhost:9200/foo
#{"error":"ClusterBlockException[blocked by: [FORBIDDEN/5/index read-only (api)];]","status":403}

curl -XDELETE localhost:9200/_all
#{"acknowledged":true}
```

The problem seems to affect the following apis:
- delete index
- delete mapping
- types exists
- indices exists
- open index
- close index
</description><key id="37019396">6694</key><summary>Check for index blocks against concrete indices on master operations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T20:10:37Z</created><updated>2015-06-07T19:34:54Z</updated><resolved>2014-07-08T12:37:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add additional Analyzers, Tokenizers, and TokenFilters from Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6693</link><project id="" key="" /><description>Add `irish` analyzer
Add `sorani` analyzer (Kurdish)

Add `classic` tokenizer: specific to english text and tries to recognize hostnames, companies, acronyms, etc.
Add `thai` tokenizer: segments thai text into words.

Add `classic` tokenfilter: cleans up acronyms and possessives from classic tokenizer
Add `apostrophe` tokenfilter: removes text after apostrophe and the apostrophe itself
Add `german_normalization` tokenfilter: umlaut/sharp S normalization
Add `hindi_normalization` tokenfilter: accounts for hindi spelling differences
Add `indic_normalization` tokenfilter: accounts for different unicode representations in Indian languages
Add `sorani_normalization` tokenfilter: normalizes kurdish text
Add `scandinavian_normalization` tokenfilter: normalizes Norwegian, Danish, Swedish text
Add `scandinavian_folding` tokenfilter: much more aggressive form of `scandinavian_normalization`
Add additional languages to stemmer tokenfilter: `galician`, `minimal_galician`, `irish`, `sorani`, `light_nynorsk`, `minimal_nynorsk`

Add support access to default Thai stopword set "_thai_"

Fix some bugs and broken links in documentation.

Closes #5935
</description><key id="37014443">6693</key><summary>Add additional Analyzers, Tokenizers, and TokenFilters from Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T19:13:47Z</created><updated>2015-06-07T13:05:40Z</updated><resolved>2014-07-03T09:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-07-02T19:15:07Z" id="47822069">Yay!
</comment><comment author="jpountz" created="2014-07-02T23:27:41Z" id="47850076">LGTM this is great!
</comment><comment author="s1monw" created="2014-07-03T07:19:58Z" id="47874566">LGTM good stuff
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Before deleting shard verify that another node holds an active shard instance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6692</link><project id="" key="" /><description>Before removing shard physically from disk verify that another node in the cluster actually holds an active shard instance.
</description><key id="37014392">6692</key><summary>Before deleting shard verify that another node holds an active shard instance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Store</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T19:13:11Z</created><updated>2015-06-07T13:05:56Z</updated><resolved>2014-07-09T10:59:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-03T15:07:55Z" id="47942048">@bleskes Good points, I updated the PR.
</comment><comment author="martijnvg" created="2014-07-03T15:31:44Z" id="47945226">@kimchy good point, I updated the PR.
</comment><comment author="kimchy" created="2014-07-04T15:33:44Z" id="48056574">thinking about it a bit more, i think that the logic when all nodes responded should only continue if the current cluster state is the same as the cluster state that we had during the clusterChangeEvent initiating the check on active on all nodes. If not, it will not do anything, but by definition another cluster state has happened, and will trigger the active check for deletion anyhow
</comment><comment author="bleskes" created="2014-07-04T16:31:05Z" id="48059703">++ on what kimchy said.
</comment><comment author="martijnvg" created="2014-07-07T09:24:56Z" id="48157242">@kimchy @bleskes I've updated the PR with the suggested cluster state check and added unit tests.
</comment><comment author="kimchy" created="2014-07-07T11:21:49Z" id="48166799">left a minor comment, LGTM
</comment><comment author="martijnvg" created="2014-07-07T14:28:35Z" id="48185339">@bleskes Applied the latest feedback.
</comment><comment author="bleskes" created="2014-07-09T09:53:08Z" id="48450515">LGTM - life a couple of minor improvement suggestions. Having these unit tests is awesome.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Take compatibility version into account for XContentType randomization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6691</link><project id="" key="" /><description>We randomize the XContentType to test deriving the content type on all
APIs. Yet, BWC tests run against versions where CBOR wasn't around
this commit ensures we don't use CBOR when compatibility version is
less than `1.2.0`
</description><key id="37005600">6691</key><summary>Test: Take compatibility version into account for XContentType randomization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T17:36:55Z</created><updated>2014-07-16T13:03:07Z</updated><resolved>2014-07-02T18:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-02T17:55:31Z" id="47812012">@rjernst I pushed a fix for your comment!
</comment><comment author="martijnvg" created="2014-07-02T17:55:44Z" id="47812045">LGTM
</comment><comment author="rjernst" created="2014-07-02T17:57:20Z" id="47812249">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Add ability to restore indices without their aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6690</link><project id="" key="" /><description>Closes #6457
</description><key id="37001131">6690</key><summary>Snapshot/Restore: Add ability to restore indices without their aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels /><created>2014-07-02T16:46:00Z</created><updated>2014-07-13T08:57:15Z</updated><resolved>2014-07-13T08:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-03T06:58:04Z" id="47873100">left two minor comments, LGTM otherwise
</comment><comment author="s1monw" created="2014-07-09T19:49:31Z" id="48525128">left one minor comment. LGTM otherwise - feel free to push once fixed 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Eats some stack traces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6689</link><project id="" key="" /><description>I have a bug in my highlighter but I'm having trouble tracking it down because Elasticsearch is eating the stack trace from the root cause exception.  My logs:

```
Caused by: org.elasticsearch.search.fetch.FetchPhaseExecutionException: [simplewiki_content_1404072971][0]: query[filtered(+ConstantScore(_uid:page#425633) +(auxiliary_text:what auxiliary_text:love?))-&gt;cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@17097ef8)],from[0],size[10]: Fetch Failed [Failed to highlight field [auxiliary_text]]
        at org.elasticsearch.search.highlight.ExperimentalHighlighter.highlight(ExperimentalHighlighter.java:89)
        at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)
        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:340)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:751)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:740)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.ArrayIndexOutOfBoundsException
```

Why only print the name of the cause?  I'd much prefer the full, gory stack trace to show up in the logs.  It'd make it easier to reproduce the problem in a development environment.

I'm not sure where the comes from.  I certainly don't attempt to clear the stack trace any where.  The code that catches my exception and rethrows it to Elasticsearch is just:

``` java
        } catch (Exception e) {
            throw new FetchPhaseExecutionException(context.context, "Failed to highlight field ["
                    + context.fieldName + "]", e);
        }
```

which seems pretty standard.
</description><key id="36992090">6689</key><summary>Eats some stack traces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rjernst/following{/other_user}', u'events_url': u'https://api.github.com/users/rjernst/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rjernst/orgs', u'url': u'https://api.github.com/users/rjernst', u'gists_url': u'https://api.github.com/users/rjernst/gists{/gist_id}', u'html_url': u'https://github.com/rjernst', u'subscriptions_url': u'https://api.github.com/users/rjernst/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/289412?v=4', u'repos_url': u'https://api.github.com/users/rjernst/repos', u'received_events_url': u'https://api.github.com/users/rjernst/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rjernst/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rjernst', u'type': u'User', u'id': 289412, u'followers_url': u'https://api.github.com/users/rjernst/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Exceptions</label><label>enhancement</label></labels><created>2014-07-02T15:14:56Z</created><updated>2015-11-22T00:15:47Z</updated><resolved>2015-11-22T00:15:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-04T09:41:39Z" id="48025726">I agree, I think we should preserve as much of the original exception as possible.

In addition, we should check all instances where we catch `Throwable` to ensure we don't catch an `OutOfMemoryError` and hide it from the user.
</comment><comment author="nik9000" created="2014-07-10T18:43:24Z" id="48646471">Makes sense to me!  The `OutOfMemoryError` problem sounds a bit sweeping for my tastes, but I can take a look at the not eating causes if you'd like.  I see there is someone assigned to this - does that mean that they are working on it or might work on it soon or something else?
</comment><comment author="clintongormley" created="2015-11-21T15:36:59Z" id="158652760">@nik9000 I think this has been resolved in recent versions, correct?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Aggregation that buckets documents based on co-occurance of terms within a document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6688</link><project id="" key="" /><description>The aggregation would take a list of terms as input (e.g. a list of email addresses) and creates buckets based upon co-occuring terms (terms which appear in the same document regardless of their relative position in the document).  These buckets then represent edges between the terms and can be used to create weighted graphs (e.g. who is sending emails to whom)

This aggregation can be used in eg social nets where the terms represent uniquely identifiable entities and combined with time based aggregations can summarise interactions over time
</description><key id="36990354">6688</key><summary>Add Aggregation that buckets documents based on co-occurance of terms within a document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>adoptme</label><label>feature</label></labels><created>2014-07-02T14:58:28Z</created><updated>2016-01-22T18:27:12Z</updated><resolved>2015-11-22T19:27:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-07-02T16:00:57Z" id="47795859">We may need to consider how me model the idea of entities and roles. E.g. `foo@hotmail.com` is an entity as is bank account `123535` but the entities could each appear in different roles e.g. appearing as email `sender` vs `recipient` or payment `payer` vs `payee`.
If we want to summarise how entities interact and don't have any special treatment of entities/roles then the client has to either:
a) Create a role-less field for the purposes of analysis e.g. the field `transactionParticipant` or
b) Provide terms for each of the roles e.g. sender:foo@hotmail.com, recipient:foo@hotmail.com etc etc

Perhaps a simpler option is to assume that the user lists the entities of interest once and separately defines the list of fields which represent roles e.g:

```
{
    "entities":["foo@hotmail.com", "bar@hotmail.com"],
    "roles": ["from", "to"]
}
```

The entities become the nodes in our graph and the edges are the type of line that connects entities e.g. a direction of payment. Edges would be agg buckets and could summarise interactions between a pair of entities through the use of child aggs e.g. summing the volumes of money transferred, month by month. It may be useful to do some form of "edge bundling" e.g. rolling up `to` and `cc` roles into a single `recipient` role for the purposes of bucketing. This could be defined as part of the agg settings
</comment><comment author="colings86" created="2014-07-07T09:04:10Z" id="48155454">In terms of the API for this aggregation at the moment I have the following:

Request:

```
{
    "size" : 100,
    "fields" : ["From", "To"],
    "nodeValues" : [ "foo@example.com", "bar@example.com"]
}
```

Response:

```
{
    {
        "key_as_string": "From:foo@example.com\u0000To:bar@example.com",
        "src": {
            "field": "From",
            "value": "foo@example.com"
        },
        "dest": {
            "field": "To",
            "value": "bar@example.com"
        },
        "doc_count": 113
    },
    {
        "key_as_string": "From:bar@example.com\u0000To:foo@aol.com",
        "src": {
            "field": "From",
            "value": "bar@example.com"
        },
        "dest": {
            "field": "To",
            "value": "foo@aol.com"
        },
        "doc_count": 80
    }
}
```

The src and dest fields are at the moment chosen arbitrarily. An improvement might be to split the fields into sourceFields and destinationFields but this might not fit well with use cases which don't care about directed graphs
</comment><comment author="clintongormley" created="2015-11-21T15:36:12Z" id="158652727">@markharwood @colings86 is this still of interest?
</comment><comment author="markharwood" created="2015-11-22T19:27:08Z" id="158791327">This is superseded by the graph API
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplifying JSON API in 2.0+</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6687</link><project id="" key="" /><description>_Note:  This is a rehash of my [mailing list post](https://groups.google.com/forum/#!topic/elasticsearch/Md3nOZ89eoU) which didn't seem to generate any discussion.  I wanted to bring it over here in hopes of finding the right audience._

I was curious if there were any plans to update or modify the JSON query API in ES 2.0+?

While I find the API to very powerful, it is confusing to construct a valid request and requires special casing a lot of rules.  I have some thoughts below on what I see as the current issues, and some suggestions to correct them.  I don't intend for this to be a rant, just to provoke discussion.  This is done purely from the point of view of constructing queries (not parsing them), and only for the JSON DSL query syntax for searching (not percolate or aggregators).

It is currently hard to construct small parts of a JSON query without knowing all of the elements involved. Looking at a simple query and a filtered query:

Simple Query:

``` json
{
    "query": {
        "match_all": {}
    }
}
```

Filtered Query:

``` json
{
  "query": {
    "filtered": {
      "filter": {
        "and": [
          {
            "term": {
              "foo": "bar"
            }
          }
        ]
      },
      "query": {
        "match_all": {}
      }
    }
  }
}
```

We see the syntax tree change so that the initial 'query' becomes nested and the root of the tree changes.  Once we add a scoring function, it morphs even further.

Scored Query:

``` json
{
  "query": {
    "function_score": {
      "query": {
        "filtered": {
          "filter": {
            "and": [
              {
                "term": {
                  "foo": "bar"
                }
              }
            ]
          },
          "query": {
            "match_all": {}
          }
        }
      },
      "script_score": {
        "script": "result = 0.0 + 1.0;"
      },
      "boost_mode": "replace"
    }
  }
}
```

This follows some of the same rules (nested inside a new scope), however, not all of the changes get placed together.  We have both a 'script_score' block and a 'boost_mode' section.  This means that when I want to add scoring to my query I need to know my scoring block as well as the rest of the query tree so that I can properly place 'boost_mode'. 

A simple(r) example.  In a simple scored query, if I want to modify my 'match_all' block, my path becomes
`"query" -&gt; "function_score" -&gt; "query"`

Once I add filtering to the query, the path changes, causing a broken query if I insert in the old location.
`"query" -&gt; "function_score" -&gt; "query" -&gt; "filtered" -&gt; "query"`

It would be much simpler if I could define my scoring block, and throw it in to a query at a static path without worrying what else is in the query.  This case is a simple illustration, but the JSON query DSL contains many instances, especially around cases like 'scoring' where using a single scoring block vs. multiple scoring functions radically changes the structure of the scoring section.

I understand that this was designed iteratively, and that the syntax will not be perfect of both parsing and construction.  Now that the JSON query DSL seems to have a stable set of elements, it would be useful to set it up so that it can be written in a simple manner.  A few considerations:
1. When adding an element such as scoring, have it only modify elements below it in the tree (aside from its initial insertion point)
2. Keep the root of the tree static and have the existence of a top level key modify behavior, instead of needing change the nesting of elements. 
3. Somehow stop nesting the term "query" all over the place, definitely the most confusing thing for new users in my experience. =D

Here is a proposed top level DSL example.  It's incomplete and probably missing some things but useful as an illustration:

``` json
{
  "filter": {
    "and": [
      {
        "term": {
          "foo": "bar"
        }
      }
    ]
  },
  "query": {
    "match_all": {}
  },
  "scoring": {
      "script_score": {
        "script": "result = 0.0 + 1.0;"
      },
      "boost_mode": "replace"
  },
  "sort": [
    {
      "foo": {
        "order": "desc",
        "mode": "average"
      }
    }
  ]
}
```

Thanks for reading over this.  I was unable to find a roadmap for prospective features, so if there are already plans to work on this feel free to disregard my comments.

Thanks,
Andrew
</description><key id="36989537">6687</key><summary>Simplifying JSON API in 2.0+</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">andrewgross</reporter><labels><label>high hanging fruit</label></labels><created>2014-07-02T14:50:25Z</created><updated>2015-11-21T15:35:19Z</updated><resolved>2015-11-21T15:35:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-02T23:32:58Z" id="47850447">I understand how it would make simple things easier, but this syntax doesn't seem like it would allow for eg. building a boolean query whose individual clauses are `function_score` queries?
</comment><comment author="kimchy" created="2014-07-03T08:02:31Z" id="47877811">btw, another interesting data point is @clintongormley work on simplified query DSL in the perl client (can't find the link now :) )
</comment><comment author="clintongormley" created="2014-07-03T09:29:53Z" id="47885340">Hi @andrewgross 

I read your post in the mailing list and ruminated on it a bit.  I think the approach that you outline above won't work, because it assumes a very simple query.  With this structure you lose the flexibility to combine multiple independent clauses with different rules and scoring.

I've also been thinking about changes we could make to simplify the DSL. There are two things which I'd love to do:

## Remove the distinction between queries and filters

Filters are:
- boolean, a doc either matches a filter or it doesn't
- for exact term-level matches only (ie not full text)
- fast (usually)
- cacheable (usually)

While a query is typically for:
- full text search
- anything to do with scoring

It annoys me that we have to separate queries and filters, and it is one of the things that adds verbosity to the DSL. I'd love to be able to pass a filter or a query in (eg) the same `bool` clause and have Elasticsearch just figure it out.

## Simplify filter syntax

Filters are usually pretty simple, and most SQL statements could be expressed with filters.  I'd love to be able to reduce the verbiage needed to express filter conditions, eg:

```
"where": [
    { "status": "active" },
    { "star": [ "feature", "on_sale" ] },
    { "price": { "gte": 10, "lt": 20 }},
    { "not": { "flag": "spam" }}
]
```

(Queries on the other hand are often more complex and need more parameters. Their current syntax is probably about as simple as it gets.)

The above  `where` clause doesn't handle all cases unfortunately.  For instance, how would I look for "foo" in any of the fields "one", "two" or "three"?  What if I want some custom setting (eg using a named filter, or specifying _cache: false). Are those clauses combined with AND or OR? (I'd say AND).  How would you express an OR condition then? Possibly you could use `where_any` vs `where_all`...

I tried to see how we could change the current DSL to support these two ideas in a backwards compatible way and I just couldn't figure out how to do it.  Something like this would require a new DSL. I haven't given up on this - would be interested in more ideas.
</comment><comment author="clintongormley" created="2014-07-03T10:37:04Z" id="47891124">@kimchy the Perlish syntax I wrote is here: https://metacpan.org/pod/ElasticSearch::SearchBuilder

It makes some things very simple, much like the `where` clause I mentioned above.  However, I found that there were certain cases where the syntax became cryptic (as one might expect in Perl...) 

The main thrust was changing the emphasis from:

```
"clause_type": { "field_name": ..... }
```

eg:
    {"term": { "status": "active" }}
    {"range": { "date": { "gt": "2000" }}}

to:

```
"field_name":  .....
```

eg:

```
{"status": "active"}
{"date": { "gt": "2000" }}
```

Plus clauses with boolean logic:

```
{  foo: "bar", "bar": "baz" }  # AND
[  foo: "bar", "bar": "baz" ]  # OR
```

This worked pretty well for filters, but seemed hackish for queries.   This is why I'm thinking that a `where` clause may be a useful shorthand.
</comment><comment author="dakrone" created="2014-07-03T12:53:56Z" id="47925419">@clintongormley I think inverting the clause type and field name makes things more difficult, for example, I think this reads better:

``` json
{
  "where": [
    {
      "range": {"age": {"lt": 82, "gte": 20}}
    },
    {
      "range": {
        "date": {"from": "2014/07/02", "to": "now"}}
    },
    {
      "match": {
        "body": "quick fo",
        "type": "phrase_prefix"
      }
    },
    {
      "term": {"color": "blue"}
    },
    {
      "or": [
        {"term": {"size": "small" }},
        {"term": {"size": "medium" }},
        {"term": {"size": "xlarge" }}
      ]
    },
    {
      "and": [
        {"term": {"instock": "true"}}
        {"term": {"available": "false"}}
      ]
    }
  ],
  "default_operator": "AND"
}
```

And I think `clause_type` first allows much simpler parsing logic since it can delegate to a separate class for parsing. It is also easier to make it pluggable (adding a new clause_type), and also makes boolean combining of queries feel more "natural".

The `default_operator` could be used for combining the `where` clauses.
</comment><comment author="andrewgross" created="2014-07-03T15:03:39Z" id="47941457">Thanks for the reply @clintongormley, exactly the response I was hoping for.  It certainly seems my proposed style excluded some features I don't have much experience with.  My other concern is to hopefully make it easier to isolate blocks of query JSON for re-use. 

As an example I want to point to a library I created to make simple filtering and querying easier.  I have a bit of hairy logic to build the first few levels of the tree:

https://github.com/Yipit/pyeqs/blob/master/pyeqs/query_builder.py#L95-L126

It would be helpful in this case to be able to guarantee that anything related a certain function would be contained below that function in the tree.  For example, `track_scores` and `min_score` relate to scoring, but need to sit at the same level as the root of the scoring section.  It makes it hard to give someone a block and say "add this black box here and itll score things properly".  Instead there is a fair but more work needed to be done to make sure everything fits together properly.

Just my $0.02.  I appreciate all the work done so far and realize that the requests that seem reasonable to me may not seem reasonable to someone with more experience with the internals of ES.
</comment><comment author="clintongormley" created="2015-11-21T15:35:19Z" id="158652682">This issue hasn't garnered any more interest in the last year and a half. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handle ConnectionTransportException during a Master/Node fault detection ping during Discovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6686</link><project id="" key="" /><description>Both the Master and Node fault detection register themselves to be notified when a node disconnects to be able to respond to it accordingly. As such, when a ConnectionTransportException was raised on a ping request, it was not handled as it is already handled somewhere else. However, this does introduce a racing condition, if the disconnect  happen during a period where there is no current master (minimum_master_node breach) at which time the fault detection is not active. In this case, we will only discover the disconnect error during the ping request, so we have to respond accordingly.
</description><key id="36987646">6686</key><summary>Handle ConnectionTransportException during a Master/Node fault detection ping during Discovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Discovery</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T14:32:42Z</created><updated>2015-06-07T19:35:07Z</updated><resolved>2014-07-02T18:52:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-02T15:34:28Z" id="47792148">Left one small comment, other than that LGTM.
</comment><comment author="kimchy" created="2014-07-02T15:35:26Z" id="47792276">same here, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Templates: Make `flat_settings` false by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6685</link><project id="" key="" /><description>In every API but the templates APIs, `flat_settings` is false by default. We should make it consistent in 2.0 and use `true` by default all the time.
</description><key id="36985153">6685</key><summary>Templates: Make `flat_settings` false by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>breaking</label></labels><created>2014-07-02T14:06:33Z</created><updated>2015-06-15T18:44:50Z</updated><resolved>2014-07-02T14:06:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-02T14:06:43Z" id="47779997">Closed via https://github.com/elasticsearch/elasticsearch/pull/6672
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_polygon filter with non-zero rule filling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6684</link><project id="" key="" /><description>Is it possible to apply a geo_polygon filter with a [non-zero rule](https://en.wikipedia.org/wiki/Nonzero-rule) ?
</description><key id="36984492">6684</key><summary>geo_polygon filter with non-zero rule filling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">altitude</reporter><labels /><created>2014-07-02T14:00:05Z</created><updated>2014-07-02T14:31:45Z</updated><resolved>2014-07-02T14:31:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-02T14:31:45Z" id="47783289">This issues list is for bug reports and feature requests. Please ask questions like these in the mailing list instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update plugins.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6683</link><project id="" key="" /><description>Updated the river plugins with the EEA RDF River Plugin. It allows harvesting metadata from SPARQL endpoints or plain RDF files into ElasticSearch.
</description><key id="36980926">6683</key><summary>Update plugins.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">iulia-pasov</reporter><labels /><created>2014-07-02T13:22:01Z</created><updated>2014-07-09T14:16:18Z</updated><resolved>2014-07-09T14:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-03T07:18:51Z" id="47874494">Hey,

thanks a lot for updating the docs (and of course for creating the river in the first place!). May I ask you to sign our CLA before getting this in? It is available at  http://www.elasticsearch.org/contributor-agreement/

Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>APT repository format can cause problems</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6682</link><project id="" key="" /><description>The current apt [repository format](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html) is not following the [Debian repository guidelines](https://wiki.debian.org/HowToSetupADebianRepository#APT_Archive_Types). The version number on the path of the repository can cause confusion. 

Package naming also violates Debian guidelines, leading to versioning problems. Version upgrade is also problematic, as the entire line in sources.list has to be changed for upgrade, while the Debian guidelines recommends package pinning to keep a specific version. 

There are several issues open about this (#5536, #1726) that can help solving this problem.
</description><key id="36976422">6682</key><summary>APT repository format can cause problems</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">ispmarin</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2014-07-02T12:21:31Z</created><updated>2015-11-21T15:34:29Z</updated><resolved>2015-11-21T15:34:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ispmarin" created="2014-07-14T19:09:40Z" id="48945570">Today ES was updated from 1.2.1 to 1.2 using the repo line

```
deb http://packages.elasticsearch.org/elasticsearch/1.2/debian stable main
```

so the idea is to keep the same branch on the same version? This is why Debian has stable, testing, unstable and experimental repos.
</comment><comment author="electrical" created="2014-07-15T10:27:56Z" id="49014325">Hi,

The repo's design is currently under review and we might change it at some point.
The initial design with the 'major' version number in the URL's is done so that people don't accidentally upgrade from 0.90.x to 1.x for example. 
</comment><comment author="ispmarin" created="2014-07-15T12:58:19Z" id="49027365">Hi there,

Thanks for your answer. I understand the concern with the unwanted upgrade, but APT has a mechanism to prevent this sort of upgrade (pinning), without disrupting the expected (and documented) behavior of an APT repository. 

The opposite happened: I was installing elasticsearch from a repo in sources.list.d and expecting that the latest package was installed, but as the repo line had the major version on it, I was installing a 0.9 version. It took a while to figure out that the problem was the elasticsearch version and not my queries.

Please consider the change.
</comment><comment author="erikrose" created="2014-07-30T17:14:47Z" id="50648375">I recommend following the pattern set by postgresql, open-jdk, python, and most other Debian packages which have significant version-to-version differences: embed the important version information in the package name itself: for example, elasticsearch-1.2. That way, you need only one repo, you don't end up mixing product version with package quality (which is roughly what unstable/testing/stable indicates), people still don't get upgrades by accident, and you can even install multiple ES versions simultaneously (with a little extra work).
</comment><comment author="ispmarin" created="2014-07-30T17:35:31Z" id="50651218">I agree, @erikrose. It's an elegant solution if there's a big concern about mixing up versions. 
</comment><comment author="ispmarin" created="2015-05-28T11:59:32Z" id="106290116">After almost one year and several changes, the repository format is still not conforming to the Debian guidelines and @erikrose comment. Any ideas on this front?
</comment><comment author="clintongormley" created="2015-11-21T15:34:29Z" id="158652635">We have moved to a single repo per major version. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomly tokenizing field data on dot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6681</link><project id="" key="" /><description>I have indexed a few hundred documents into Elasticsearch. There is one field named "awsService" which contains values such as "ec2.amazonaws.com" or "rds.amazonaws.com" or "iam.amazonaws.com" etc. When I run aggregations on this field, it splits "ec2.amazonaws.com" to "ec2" and "amazonaws.com" and produces two separate aggregation records, but all the other values like "rds.amazonaws.com" or "iam.amazonaws.com" are intact and produces only one aggregation record.

This surely seems to be a bug in aggregation component of Elasticsearch. Please let me know if any more information is required.
</description><key id="36971141">6681</key><summary>Randomly tokenizing field data on dot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kartikeya1989</reporter><labels><label>non-issue</label></labels><created>2014-07-02T10:54:48Z</created><updated>2014-07-02T12:23:26Z</updated><resolved>2014-07-02T12:23:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-02T10:56:02Z" id="47762283">I could imagine this being a bug in the standardtokenizer in lucene. @rmuir what do you think? It seems like it doesn't like the number there...
</comment><comment author="s1monw" created="2014-07-02T10:58:04Z" id="47762439">oh @kartikeya1989 I didn't ask, what is your mapping and what is the version you are running?
</comment><comment author="dadoonet" created="2014-07-02T11:00:10Z" id="47762607">FYI I can reproduce it with the default analyzer on 1.2.1
</comment><comment author="kartikeya1989" created="2014-07-02T11:02:45Z" id="47762828">@s1monw I am using version 1.2.1.
I am not using any mapping. Is a mapping required and compulsory?
</comment><comment author="dadoonet" created="2014-07-02T11:08:39Z" id="47763278">Indeed. To compute terms aggregations, you'd better use a mapping for those fields and set `"index" : "not_analyzed"`.

See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#string
</comment><comment author="kartikeya1989" created="2014-07-02T12:14:44Z" id="47768316">I am trying to create mappings as mentioned in the documentation but there seems to be another issue there. When I hit localhost:9200/indexName/_mapping -d '&lt;mapping JSON here&gt;', I get "Root type mapping not empty after parsing" error. When I try the alternative as suggested here - http://stackoverflow.com/questions/24254081/elasticsearch-1-2-1-exception-root-type-mapping-not-empty-after-parsing - I get "{"error":"IndexAlreadyExistsException[[indexName] already exists]","status":400}".
(Apologies for the formatting)

Please correct me if I am wrong, but the way Elasticsearch is parsing the different values differently(without a mapping) is still a bug.
</comment><comment author="s1monw" created="2014-07-02T12:23:18Z" id="47769012">actually this is not a bug, it's how breakiterators work under the hood. Yet, for your case I think you should follow @dadoonet suggestion and use `not_analyzed`. For question how to add a mapping etc. please ask on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Also wait for fields to have been applied in the mapping in cluster state during teh waitForConcreteMappingsOnAll call</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6680</link><project id="" key="" /><description>The concrete DocMapper on the master will be updated before the mapping in the cluster state. The DocMapper is updated during the cluster update task. This can lead to occasional assertion failures on the mapping response, because that is based on the mapping the cluster state, which may not yet have been updated. (time window between the DocMapping is updated, but the mapping in the cluster state isn't)
</description><key id="36971070">6680</key><summary>[TEST] Also wait for fields to have been applied in the mapping in cluster state during teh waitForConcreteMappingsOnAll call</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-07-02T10:53:38Z</created><updated>2015-05-18T23:31:21Z</updated><resolved>2014-07-02T15:37:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use EnumSets&lt;&gt; when testing for multiple shard states</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6679</link><project id="" key="" /><description>At the moment we have places that check whether a shard is in any of a couple of possible states before performing an action. This can look like this:

```
if (state != IndexShardState.STARTED &amp;&amp; state != IndexShardState.RELOCATED &amp;&amp; state != IndexShardState.RECOVERING &amp;&amp; state != IndexShardState.POST_RECOVERY) {
   throw new IllegalIndexShardStateException(shardId, state, "operations only allowed when started/relocated");
}
```

We should move to using static final enum sets that communicate the intention of the test, for example:

```
private static final EnumSet&lt;IndexShardState&gt; CAN_WRITE_TO_SHARD_STATES= EnumSet.of(IndexShardState.POST_RECOVERY, IndexShardState.STARTED, IndexShardState.RELOCATED, IndexShardState.RECOVERING);

if (!CAN_WRITE_TO_SHARD_STATES.contains(state)) {
   throw new IllegalIndexShardStateException(shardId, state, "operations only allowed when started/relocated");
}

```

This makes the code cleaner and easier to read
</description><key id="36969737">6679</key><summary>Use EnumSets&lt;&gt; when testing for multiple shard states</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>enhancement</label></labels><created>2014-07-02T10:32:54Z</created><updated>2016-11-06T07:41:48Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-02T14:27:03Z" id="47782621">+1
</comment><comment author="bleskes" created="2014-07-09T12:48:08Z" id="48465017">bumping to 1.4
</comment><comment author="clintongormley" created="2015-11-21T15:33:49Z" id="158652602">@bleskes is this still relevant?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic search records count difference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6678</link><project id="" key="" /><description>Hi All,

   I builded elastic search index with 20 million records.Using mysql river to pull out the data.i observed once scenario like the records count is differ from database table record count and elastic search index records count but all records are populated to ES index.is there any specific reason to show difference in between these two counts.

Thanks
phani.
</description><key id="36967112">6678</key><summary>Elastic search records count difference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phani546</reporter><labels /><created>2014-07-02T09:54:23Z</created><updated>2014-07-02T10:13:04Z</updated><resolved>2014-07-02T10:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-02T10:13:04Z" id="47758889">Hi @phani546 

The github issues list is for bug reports and feature requests. Please use the forums for questions like this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: Deprecate `index_name` and `path`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6677</link><project id="" key="" /><description>Today we allow for custom index names. This adds ambiguity as fields might be referred to either according to their index name or their field name. What should happen when several fields have the same index name or if the index name of a field is the same as the full name of another field?

My understanding is that custom `index_name`s are not useful anymore now that we have `copy_to` in order to index several logical fields into a single physical one, so I'd like to deprecate custom `index_name`s so that the index name would always be the same as the full name.

Relates to #4081, #8870
</description><key id="36963771">6677</key><summary>Mappings: Deprecate `index_name` and `path`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T09:06:23Z</created><updated>2015-02-05T20:46:20Z</updated><resolved>2015-02-05T20:46:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-02T09:27:28Z" id="47754860">++
</comment><comment author="s1monw" created="2014-07-02T09:40:47Z" id="47756072">huge +1 IMO we should start rejecting this setting on new created indices!
</comment><comment author="dakrone" created="2014-07-02T09:47:57Z" id="47756730">Yes! Deprecate in 1.3 and remove in master?
</comment><comment author="s1monw" created="2014-07-02T09:48:30Z" id="47756774">the removal is a problem IMO since this is a index level BWC issue :/
</comment><comment author="clintongormley" created="2014-11-27T11:40:10Z" id="64779945">The `index_name` and `path` parameters are no longer needed now that we have `copy_to`.  From 2.0, we should:
- refuse to create new indices which contain these mappings
- maintain the ability to understand `index_name` and `path` on existing indices
- change the `upgrade` API to complain about the use of deprecated params (see #8682)

From 3.0: 
- remove `index_name` and `path` completely
</comment><comment author="InfinitiesLoop" created="2014-12-17T15:48:08Z" id="67341534">For your consideration, please review this issue. Perhaps removing index_name will have a greater impact than intended.
https://github.com/elasticsearch/elasticsearch/issues/8980
</comment><comment author="clintongormley" created="2015-02-04T09:54:06Z" id="72827495">In 2.0, I think we can upgrade the mapping in a way that is not perfect, but will cover most use cases and help users to upgrade without reindexing.

`index_name` and `path` are used for two main purposes:
- the old way to do `copy_to`
- provide fieldname aliases, eg `tag` points to `tags`

We can't distinguish between these two use cases automatically, but we can handle the first use case gracefully, and the second use case is an easy change to make application side (ie just change all use of `tags` to `tag` in searches)

A mapping that looks like this (with `path` set to `just_name`):

```
{
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "object",
          "path": "just_name", 
          "properties": {
            "first": {
              "type": "string",
              "index_name": "fullname"
            },
            "last": {
              "type": "string",
              "index_name": "fullname"
            }
          }
        }
      }
    }
  }
}
```

could be rewritten to:

```
{
  "mappings": {
    "test": {
      "properties": {
        "fullname": {
          "type": "string"
        },
        "name": {
          "type": "object",
          "properties": {
            "first": {
              "index": "no",
              "copy_to": "fullname"
            },
            "last": {
              "index": "no",
              "copy_to": "fullname"
            }
          }
        }
      }
    }
  }
}
```

The mapping for the new `fullname` field can just be the same as the mapping of the first field which uses `index_name` (without the `index_name`) setting.  The original field will not be indexed (or searchable).

In the case where `path`  is set to `full`, the same rules apply, but the new field uses the full path name, ie this:

```
{
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "object",
          "path": "full", 
          "properties": {
            "first": {
              "type": "string",
              "index_name": "fullname"
            },
            "last": {
              "type": "string",
              "index_name": "fullname"
            }
          }
        }
      }
    }
  }
}
```

could be rewritten as:   

```
{
  "mappings": {
    "test": {
      "properties": {
        "name": {
          "type": "object",
          "properties": {
            "first": {
              "index": "no",
              "copy_to": "name.fullname"
            },
            "last": {
              "index": "no",
              "copy_to": "name.fullname"
            },
            "fullname": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GET: Add parameter to GET for checking if generated fields can be retrieved</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6676</link><project id="" key="" /><description>I index a text field with type `token_count`. When indexing, this creates an additional field that holds the number of tokens in the text field.
When a document is retrieved from the transaction log (because no flush happened yet),  and I want to get the `token_count` of my text field, I would assume that the `token_count` field is simply not retrieved, because it does not exist yet. Instead I get a `NumberFormatException`.

Here are the steps to reproduce:

```
DELETE testidx

PUT testidx
{
  "settings": {
    "index.translog.disable_flush": true,
    "index.number_of_shards": 1,
    "refresh_interval": "1h"
  },
  "mappings": {
    "doc": {
      "properties": {
        "text": {
          "fields": {
            "word_count": {
              "type": "token_count",
              "store": "yes",
              "analyzer": "standard"
            }
          },
          "type": "string"
        }
      }
    }
  }
}

PUT testidx/doc/1
{
  "text": "some text"
}

#ok, get document from translog
GET testidx/doc/1?realtime=true
#ok, get document from index but it is not there yet
GET testidx/doc/1?realtime=false
# try to get the document from translog but also field text.word_count which is not there yet: NumberFormatException
GET testidx/doc/1?fields=text.word_count&amp;realtime=true

```
</description><key id="36963180">6676</key><summary>GET: Add parameter to GET for checking if generated fields can be retrieved</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>:Highlighting</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T08:57:12Z</created><updated>2014-08-04T10:06:33Z</updated><resolved>2014-08-04T06:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-07-04T16:12:50Z" id="48058886">Here is what happens:

For multi-fields, the parent field is returned instead of `null` if a sub-field is requested. For the example above, when getting `text.word_count`, `text` is retrieved from the source and returned.
We could prevent this easily like this: 7f522fbd9542

However, the FastVectorHighlighter relies on this functionality to highlight on multi-fields (see [here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/highlight/vectorhighlight/SourceSimpleFragmentsBuilder.java#L55)), so this is not really a solution unless we want to prevent highlighting with the FastVectorHighlighter on multi-fields.

The other option is to simply catch the NumberFormatException and handle it like here: be999b1042
</comment><comment author="s1monw" created="2014-07-09T10:30:41Z" id="48453804">@brwe what is the status of this?
</comment><comment author="brwe" created="2014-07-09T11:56:07Z" id="48460402">@s1monw Need to write more tests, did not get to it yet. Will continue Friday. 
</comment><comment author="s1monw" created="2014-07-09T12:02:56Z" id="48460977">cool ok but it's going to be ready for 1.3 right?
</comment><comment author="brwe" created="2014-07-09T12:21:02Z" id="48462471">depends on when the release is
</comment><comment author="brwe" created="2014-07-18T19:07:00Z" id="49468226">A field of type `murmur3` actually has the same issue. In addition, if `murmur3` and `token_count` fields are not stored, GET will also return NumberFormatException after refresh, example below.

```
DELETE testidx
PUT testidx
{
  "settings": {
    "index.translog.disable_flush": true,
    "index.number_of_shards": 1,
    "refresh_interval": "1h"
  },
  "mappings": {
    "doc": {
      "properties": {
        "token_count": {
          "type": "token_count",
          "analyzer": "standard"
        },
        "murmur": {
          "type": "murmur3"
        }
      }
    }
  }
}

POST testidx/doc/1
{
  "murmur": "Some value that can be hashed",
  "token_count": "A text with five words."
}

GET testidx/doc/1?routing=2&amp;fields=murmur,token_count

POST testidx/_refresh

GET testidx/doc/1?routing=2&amp;fields=murmur,token_count

```
</comment><comment author="brwe" created="2014-07-18T19:22:02Z" id="49469706">Following the discussion on pull request #6826 I checked all field mappers and tried to figure out what they should return upon GET. I will call a field "generated" if the content is only available after indexing. 
We discussed that we either throw a meaningful exception if the field is generated or ignore the field silently if a (new) parameter is set with the GET request. `FieldMapper` should get a new method `isGenerated()` which indicates weather the field will be found in the source or not.

Here is what I think we should do:

For some core field types (`integer, float, string,...`), the behavior (`isGenerated()` returns `true` or `false`) should be configurable. The reason is that a different mapper might use them and store generated data in them. The Mapper attachment plugin does that: Fields like author (`string`), content_type (`string`) etc. are only available after tika parsing. 

There are currently four field types (detailed list below):
1. Fields that should not be configurable, because they are always generated
2. Fields that not be configurable because they are never generated
3. Fields that should not be configurable because they are never stored
4. Fields that should be configurable

For 1-3 we simply have to implement `isGenerated()` accordingly.

To make the fields configurable we could add a parameter `"is_generated"` to the mapping which steers the behavior.

Pro: would be easy to do and also allow different types in plugin to very easily use the feature.

Con: This would allow users to set `"is_generated"` accidentally - fields that are accessible via source would then still cause an exception if requested via GET while the document is not yet indexed

For fields that are not configurable, the parameter `"is_generated"` could be ignored without warning like so many other parameters.

List of types and their category:

There is core types, root types, geo an ip.

#### Core types

These should be configurable:

```
IntegerFieldMapper.java
ShortFieldMapper.java
BinaryFieldMapper.java      
DateFieldMapper.java        
LongFieldMapper.java        
StringFieldMapper.java
BooleanFieldMapper.java     
DoubleFieldMapper.java      
```

The following two should not be configurable because they are always generated:

```
Murmur3FieldMapper.java     
TokenCountFieldMapper.java
```

This should not be configurable because it is never stored:

```
CompletionFieldMapper.java
```

#### ip an geo

Should be configurable:

```
GeoPointFieldMapper.java    
GeoShapeFieldMapper.java
IpFieldMapper.java
```

#### root types

Never generated and should not be configurable:

```
RoutingFieldMapper.java     
TimestampFieldMapper.java
IdFieldMapper.java      
SizeFieldMapper.java        
TypeFieldMapper.java
BoostFieldMapper.java       
IndexFieldMapper.java       
SourceFieldMapper.java  
ParentFieldMapper.java      
TTLFieldMapper.java     
VersionFieldMapper.java
```

Always generated and should not be configurable:

```
AllFieldMapper.java     
FieldNamesFieldMapper.java  
```

The following should not be configurable, because they are never stored:

```
AnalyzerMapper.java     
UidFieldMapper.java
```
</comment><comment author="brwe" created="2014-07-19T14:13:01Z" id="49510559">hmpf. while writing tests I figured there are actually more cases to consider. will update soon...
</comment><comment author="brwe" created="2014-07-23T07:54:42Z" id="49843333">There are two numeric fields that are currently generated (`Murmur3FieldMapper.java` and `TokenCountFieldMapper.java`) and two string fields (`AllFieldMapper.java` and `FieldNamesFieldMapper.java` ).

These should only be returned with GET (`fields=...`) if set to `stored` and not retuned if not `stored` regardless of if source is enabled or not (this was not so, see example above). If refresh has not been called between indexing and GET then this should cause an Exception unless `ignore_errors_on_generated_fields=true` (working title) is set with the GET request.
Until now `_all` and `_field_names` where silently ignored and getting the numeric fields caused a `NumberFormatException`.

I am now unsure if we should make the core types configurable. By configurable, I actually meant adding a parameter to the type mapping such as

```
{
   type: string,
   is_generated: true/false
   ...
}
```

I'll make a pull request without that and then maybe we can discuss further. 

Just for completeness, below is a list of all ungenerated field types and how they behave with GET.

---

## Fields with fixed behavior:

Never stored -&gt; should never be returned via GET

`CompletionFieldMapper` 

Always stored -&gt; should always be returned via GET

```
ParentFieldMapper.java      
TTLFieldMapper.java   
```

Stored or source enabled -&gt; always return via GET, else never return

```
BoostFieldMapper.java  
```

Stored (but independent of source) -&gt; always return via GET, else never return

```
TimestampFieldMapper.java
SizeFieldMapper.java    
RoutingFieldMapper.java   
```

## Fields that might be configurable

```
IntegerFieldMapper.java
ShortFieldMapper.java
BinaryFieldMapper.java      
DateFieldMapper.java        
LongFieldMapper.java        
StringFieldMapper.java
BooleanFieldMapper.java     
DoubleFieldMapper.java      
GeoPointFieldMapper.java    
GeoShapeFieldMapper.java
IpFieldMapper.java
```

## Special fields which can never be in the "fields" list returned by GET anyway

```
IdFieldMapper.java       
TypeFieldMapper.java
IndexFieldMapper.java       
SourceFieldMapper.java  
VersionFieldMapper.java
AnalyzerMapper.java     
UidFieldMapper.java
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exclude list of fields for FLT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6675</link><project id="" key="" /><description>Right now fuzzy like this query search through all fields in document by default, or you can specify list of fields to query against. 
It would be convenient to have exclude list of fields to exclude some fields which contains  ids or some technical metadata we want to filter by but not to search through. For example if we search by 1984 and some unrelated to this particular book document has this number in any properties, search results will have it as hit.
</description><key id="36960631">6675</key><summary>Exclude list of fields for FLT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">klappvisor</reporter><labels /><created>2014-07-02T08:16:41Z</created><updated>2014-07-02T08:31:25Z</updated><resolved>2014-07-02T08:31:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-07-02T08:31:25Z" id="47749726">at the moment the FLT query defaults to the `_all` field. You can also make it run on specific fields, in which case excluding is not really relevant. I do think that an exclude feature may be nice if you allow using wild cards to specify field names as requested in #6674 . Maybe you can extend the description of that issue? 

I'm closing this one for now.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcards for fields in fuzzy like this query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6674</link><project id="" key="" /><description>Right now you can use fields property in fuzzy like this query to specify list of fields to query against, but if your documents have nested objects inside and you want to search only against all fields inside particular nested object, you have to specify list of all properties with full path. For example with the following document:

```
{"id": "some_id",
"en": {
    "title": "en_title",
    "description": "en_description",
    /* ... */
},
"nl": {
    "title": "nl_title",
    "description": "nl_description",
    /* ... */
}}
```

In some cases you want to search only through **en** subdocument, then fuzzy like this should be:

```
{"query": {
    "flt": {
        "like_text": "search string",
        "fields": [ "en.title", "en.description", /* all other fields under en */ ]
}}}
```

It would be convenient to use wildcards to specify fields especially if you don't have defined list of fields here or list is too big, to have query like this:

```
{"query": {
    "flt": {
        "like_text": "search string",
        "fields": [ "en.*" ]
}}}
```

May be it's worth to add include/exclude patterns. It's more or less the same feature as for _source: [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html)

```
{"query": {
    "flt": {
        "like_text": "search string",
        "fields": {
            "include": [ "obj1.*", "obj2.*" ],
            "exclude": [ "categories.*" ]
}}}}
```
</description><key id="36959734">6674</key><summary>Wildcards for fields in fuzzy like this query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">klappvisor</reporter><labels><label>low hanging fruit</label></labels><created>2014-07-02T08:01:28Z</created><updated>2015-11-21T15:33:19Z</updated><resolved>2015-11-21T15:33:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-09-17T09:50:26Z" id="55871879">For consistency's sake I did a round-up of the multi-field syntaxes we employ across our APIs. 
The list is as follows:

#### APIs with arrays of field names:

[Fuzzy Like This](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-flt-query.html#query-dsl-flt-query)
[Multi-get](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-multi-get.html#mget-fields)
[More Like This](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-mlt-query.html#query-dsl-mlt-query)

#### APIs with arrays of field names but with added "boost" syntax:

[Highlighting](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#matched-fields)
[Query String query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html#_multi_field_2)

#### Arrays of pattern-based field names with boost syntax:

[Simple Query String query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html#_multi_field_3)

We should consider standardizing the supported multi-field syntax as far as possible across all these APIs and not just FLT.
</comment><comment author="clintongormley" created="2014-09-25T18:09:16Z" id="56859539">Hi @markharwood 

Agreed that we should be consistent.  Some notes: 
- The `multi_match` query also supports wildcards and boost syntax.
- The `query_string` query supports wildcards in field names, but not leading wildcards...
- Highlighting doesn't support the boosting syntax, does it?
</comment><comment author="klappvisor" created="2014-10-08T14:30:38Z" id="58366188">Do we have any workaround to exclude fields from `fuzzy_like_this` query? I cannot just use `index:no` in mappings, because I still need to filter by this fields. 
I have posted question on [stackoverflow](http://stackoverflow.com/questions/26255701/elasticsearch-exclude-some-known-fields-from-full-text-seach)

Thanks beforehand!
</comment><comment author="clintongormley" created="2015-11-21T15:33:19Z" id="158652573">FLT has been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to use my-similarity in version 1.0.1?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6673</link><project id="" key="" /><description>Write a new similarity plugin, and it worked if i add "index.similarity.default.type: org.elasticsearch.plugin.similarity.MySimilarityProvider" into elasticsearch.xml.
But it doesn't work well when i only put mapping in create index code. I guess something is error in search code. Do i need add any code in search process?
</description><key id="36957262">6673</key><summary>How to use my-similarity in version 1.0.1?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">lifeprotocol</reporter><labels><label>feedback_needed</label></labels><created>2014-07-02T07:12:34Z</created><updated>2014-07-08T12:34:50Z</updated><resolved>2014-07-08T11:12:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-02T09:54:14Z" id="47757266">Could you provide more information about what you're doing, and what you see in the logs?
</comment><comment author="clintongormley" created="2014-07-08T11:12:54Z" id="48323452">No feedback received. Closing
</comment><comment author="clintongormley" created="2014-07-08T12:34:50Z" id="48329891">Please reopen this ticket with more info if this is still a problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GET templates doesn't honor the `flat_settings` parameter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6672</link><project id="" key="" /><description>Close #6671
</description><key id="36955940">6672</key><summary>GET templates doesn't honor the `flat_settings` parameter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Index Templates</label><label>breaking</label><label>v2.0.0-beta1</label></labels><created>2014-07-02T06:44:48Z</created><updated>2015-06-06T16:48:08Z</updated><resolved>2014-07-02T07:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-02T06:53:58Z" id="47742884">LGTM :-)
</comment><comment author="s1monw" created="2014-07-02T07:13:05Z" id="47743975">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Templates API: GET templates doesn't honor the `flat_settings` parameter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6671</link><project id="" key="" /><description>It appears there is no way to get nested settings from template GET.

```
$ curl localhost:9200
{
  "status" : 200,
  "name" : "Ezekiel",
  "version" : {
    "number" : "1.2.1",
    "build_hash" : "6c95b759f9e7ef0f8e17f77d850da43ce8a4b364",
    "build_timestamp" : "2014-06-03T15:02:52Z",
    "build_snapshot" : false,
    "lucene_version" : "4.8"
  },
  "tagline" : "You Know, for Search"
}

$ curl -XPUT localhost:9200/_template/test1 -d '{
    "template": "test1-*",
    "settings": {
        "index": {
            "mapper": {
                "dynamic": false
            }
        }
    }
}'
{"acknowledged":true}

$ curl -XGET 'localhost:9200/_template/test1?pretty'
{
  "test1" : {
    "order" : 0,
    "template" : "test1-*",
    "settings" : {
      "index.mapper.dynamic" : "false"
    },
    "mappings" : { },
    "aliases" : { }
  }
}

curl -XGET 'localhost:9200/_template/test1?pretty&amp;flat_settings=false'
{
  "test1" : {
    "order" : 0,
    "template" : "test1-*",
    "settings" : {
      "index.mapper.dynamic" : "false"
    },
    "mappings" : { },
    "aliases" : { }
  }
}
```
</description><key id="36939247">6671</key><summary>Index Templates API: GET templates doesn't honor the `flat_settings` parameter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">grantr</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T23:10:27Z</created><updated>2014-07-16T13:03:49Z</updated><resolved>2014-07-02T07:22:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-02T06:38:40Z" id="47742013">Thanks for the report, I'll look into it.
</comment><comment author="jpountz" created="2014-07-02T07:23:57Z" id="47744716">It will be fixed in the 1.3.0 release. I did not backport to 1.2.2 as there is a minor break since it changes the default for the GET template API from `flat_settings=false` to `flat_settings=true`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Document special case for type parameter requirement for _mget</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6670</link><project id="" key="" /><description>The current documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-multi-get.html#docs-multi-get) indicates that type is optional in the _mget request.

Per https://github.com/elasticsearch/elasticsearch/issues/6633, in the case of multiple types sharing the same ID, in order for _mget to fetch back documents sharing the same ID across types, a _type+_id parameter have to be specified (i.e. cannot simply specify _id to fetch documents across types sharing the same id).

```
GET /sameid/_mget/
{
  "docs" : [
        {
            "_type":"typeA",
            "_id" : "1"
        },
        {
            "_type":"typeB",
            "_id" : "1"
        }
    ]
}
```

Would be nice to add this as a special case in the documentation for _mget.
</description><key id="36935260">6670</key><summary>Docs: Document special case for type parameter requirement for _mget</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>docs</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T21:57:40Z</created><updated>2014-07-16T13:04:47Z</updated><resolved>2014-07-03T13:39:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-02T02:56:24Z" id="47732461">Agreed. May be with a link to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-get.html#type ?
</comment><comment author="clintongormley" created="2014-07-02T09:52:26Z" id="47757136">@ppf2 want to send a PR?
</comment><comment author="clintongormley" created="2014-07-03T13:33:44Z" id="47929775">@dadoonet your commit breaks the docs build because you're using an ID which is not unique across the docs, ie `[[type]]`

Please fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better logic on sending mapping update new type introduction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6669</link><project id="" key="" /><description>when an indexing request introduces a new mapping, today we rely on the parsing logic to mark it as modified on the "first" parsing phase. This can cause sending of mapping updates to master even when the mapping has been introduced in the create index/put mapping case, and can cause sending mapping updates without needing to.
 This bubbled up in the disabled field data format test, where we explicitly define mappings to not have the update mapping behaviour happening, yet it still happens because of the current logic, and because in our test we delay the introduction of any mapping updates randomly, it can get in and override updated ones.
</description><key id="36925111">6669</key><summary>Better logic on sending mapping update new type introduction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T19:41:17Z</created><updated>2015-06-07T13:06:08Z</updated><resolved>2014-07-02T15:31:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-01T23:03:38Z" id="47719455">@jpountz applies you suggestions
</comment><comment author="jpountz" created="2014-07-02T05:42:40Z" id="47739274">I just beasted `DisabledFieldDataFormatTests` with your patch and interestingly, I cannot reproduce the failure anymore when there are no replicas. However, if the number of replicas is one or more, then the failure gets easy to reproduce again.
</comment><comment author="kimchy" created="2014-07-02T09:53:56Z" id="47757245">@jpountz that was a test failure, we need to make sure the search request ends up properly loading field data on all copies of the shards, I pushed a fix
</comment><comment author="jpountz" created="2014-07-02T10:00:23Z" id="47757799">Good point. The change looks good to me now, can you just fix the handling of the SearchPhaseExecutionException and the formatting of the mappings when pushing?
</comment><comment author="jpountz" created="2014-07-02T14:33:59Z" id="47783621">LGTM. I beasted it and had no failure this time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow stats sub-aggregation on geo_distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6668</link><project id="" key="" /><description>The range agg supports a stats sub-agg, to return min/max/avg values.

I'd like the same support for geo_distance aggregations, to return the min/max/avg distance for each bucket.

The existing geo_distance facet returns these statistics, but I don't see a way to query them from a geo aggregation.

Trying to put a stats on a geo_distance gives:
java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.GeoPointDoubleArrayIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData

e.g.
aggs: { distance: {geo_distance: {field: 'location', origin: origin, ranges:[{to: distance}]},
                            aggs: { distance_stats: {stats: {field: 'location'} } } } }
</description><key id="36916705">6668</key><summary>Allow stats sub-aggregation on geo_distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mlenz</reporter><labels /><created>2014-07-01T17:57:42Z</created><updated>2014-07-01T19:54:45Z</updated><resolved>2014-07-01T19:54:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-07-01T18:09:50Z" id="47690468">`aggs: { distance_stats: {stats: {field: 'location'} } } } }`

this tries to apply stats on `geo_point` values which doesn't make much sense (think of it like this: given two geo points - p1 &amp; p2, what is max(p1, p2) ?). 

I guess what you're after is the stats over the **distance** between the points and the origin. You can do this using a script:

``` json
aggs: {
   distance_stats: {
      stats: {
         script: "doc['location'].geohashDistanceInKm(originGeohash)", 
         params : { origin : "&lt;origin_geohash&gt;" } 
      } 
   } 
}
```
</comment><comment author="mlenz" created="2014-07-01T19:38:13Z" id="47700575">Thank you, that's very helpful.

The feature request may still be valid based on (1) similar functionality in the geo_distance facet (return of min, max, avg distance) and (2) parallel with the range agg, given what geo_distance is aggregating over are distances and hence numeric values.
</comment><comment author="uboness" created="2014-07-01T19:54:45Z" id="47702420">&gt; Thank you, that's very helpful.

no worries

&gt;  (1) similar functionality in the geo_distance facet (return of min, max, avg distance) 

facets api are not being considered as a reference (there're loads of issues with the facets api)

&gt; (2) parallel with the range agg, given what geo_distance is aggregating over are distances and hence numeric values.

while I understand that comparing range with sub-aggs to geo_distance with aggs can be confusing, you should not look at it this way. The stats agg stands on its own and is not related to any other agg. The consistency is in the definition of itself - it computes the stats over values that are extracted from the documents. The values can be extracted by pointing to a field, or by using scripts (regardless of what parent agg it has). This is a core "value" in the aggs module - the aggs stand on their own and are independent/agnostic of/to other aggs.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexingMemoryController should only update buffer settings of fully recovered shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6667</link><project id="" key="" /><description>At the moment the IndexingMemoryController can try to update the index buffer memory of shards at any give moment. This update involves a flush, which may cause a FlushNotAllowedEngineException to be thrown in a concurrently finalizing recovery.

Closes #6642
</description><key id="36904463">6667</key><summary>IndexingMemoryController should only update buffer settings of fully recovered shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Internal</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T15:36:37Z</created><updated>2015-06-07T19:35:18Z</updated><resolved>2014-07-02T10:24:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-02T07:22:58Z" id="47744657">left a bunch of comments.... the functionality makes lots of sense IMO
</comment><comment author="s1monw" created="2014-07-02T10:00:48Z" id="47757829">2 minor nitpicks -- LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait for mapping updates during local recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6666</link><project id="" key="" /><description>when the primary shard is recovering its translog, make sure to wait for new mapping introductions till the mappings have been updated on the master before finalizing the recovery itself
also, this change performs the mapping updates in a more optimized manner by batching the types to change into a single set and sending after the translog has been replayed
</description><key id="36904058">6666</key><summary>Wait for mapping updates during local recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T15:32:23Z</created><updated>2015-06-07T13:06:17Z</updated><resolved>2014-07-01T17:37:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-07-01T16:53:13Z" id="47681339">Just left a question. Otherwise it looks good to me but I'm not familiar with this code so I might have missed obvious bugs.
</comment><comment author="uboness" created="2014-07-01T17:30:43Z" id="47685831">LGTM... would be nice to have the option for `LocalIndexShardGateway:277` to update the mappings in a single batch (so a single request)
</comment><comment author="kimchy" created="2014-07-01T17:34:06Z" id="47686249">@uboness yea, that would be nice, but it would be a bigger change, will push this for now
</comment><comment author="uboness" created="2014-07-01T17:35:31Z" id="47686421">sure
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>'IllegalArgumentException: No type mapped for [105]' on get document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6665</link><project id="" key="" /><description>Hi!

I have updated elasticsearch from 1.1.0 to 1.2.1 and faced with a strange issue. I'm not sure, but it can be related to the thread #6441  

Preconditions:
1. Elasticsearch 1.2.1 is newly installed. All settings have default values.
2. Index is newly created.
3. Index mapping created via API:

``` json
{
   "elasticbug": {
      "mappings": {
         "filesetdata": {
            "properties": {
               "fileContent": {
                  "type": "string",
                  "include_in_all": false
               },
               "format": {
                  "type": "integer",
                  "include_in_all": false
               },
               "metadata": {
                  "properties": {
                     "Fil_1": {
                        "properties": {
                           "OriginalFileID_2": {
                              "type": "string",
                              "index": "not_analyzed"
                           },
                           "OriginalFileName_3": {
                              "type": "string",
                              "index": "not_analyzed"
                           }
                        }
                     }
                  }
               },
               "state": {
                  "type": "byte",
                  "include_in_all": false
               }
            }
         }
      }
   }
}
```

'fileContent' field is used for storing text extracted from .pdf files via Apache Tika.
4. All other index settings have default values.

Steps:
Index this huge document:
_PUT elasticbug/filesetdata/54c87535-1baf-46d7-a6f9-2b0c914c7479 [huge doc](https://onedrive.live.com/redir?resid=50939A116F9A78E1!126&amp;authkey=!AJA5NQ6t__mQZqw&amp;ithint=file%2c)_
**Right after that** try to GET this document:
_GET elasticbug/filesetdata/54c87535-1baf-46d7-a6f9-2b0c914c7479_

[Expected]

```
Document was successfully obtained
```

[Actual]

```
[2014-07-01 16:01:45,009][DEBUG][action.get] [Shard] [elasticbug][3]: failed to execute [[elasticbug][filesetdata][54c87535-1baf-46d7-a6f9-2b0c914c7479]: routing [null]]
java.lang.IllegalArgumentException: No type mapped for [105]
    at org.elasticsearch.index.translog.Translog$Operation$Type.fromId(Translog.java:223)
    at org.elasticsearch.index.translog.TranslogStreams.readSource(TranslogStreams.java:59)
    at org.elasticsearch.index.engine.internal.InternalEngine.get(InternalEngine.java:340)
    at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:469)
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:195)
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:106)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:109)
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43)
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:163)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
```

**NOTE:** 
1. After a while (about 30m) GET works fine and obtains document without any problems. As far as i understand it depends on _index.translog.flush_threshold_period_ setting.
2. I also noticed that _refresh_ parameter set to true in index command (_?refesh=true_) fixes this issue. Interesting that force __refresh_ via api DOES NOT help.
3. As was mentioned in #6441, to work around the problem you can trigger a __flush_
before the get.
4. This issue was not reproduced in 1.1.0.

Thanks!
</description><key id="36897997">6665</key><summary>'IllegalArgumentException: No type mapped for [105]' on get document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">Zazik</reporter><labels /><created>2014-07-01T14:31:21Z</created><updated>2014-07-02T08:17:23Z</updated><resolved>2014-07-02T07:53:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-01T14:50:11Z" id="47665532">Thanks for the detailed bug report.  This does sound like the same issue as #6441.

@bleskes close this in favour of #6441?
</comment><comment author="uschindler" created="2014-07-01T19:45:57Z" id="47701412">This is the same issue like #6441: This happens after indexing a _large_ document and then get it from the translog.

To check if this is really the same issue do the following:
- Index the large document
- Don't flush and shutdown ES
- Restart ES

After that ES will no lomger come up, because it cannot apply the translog, which will show the same error messages.

The difference is only: Here we just cannot get the document until flush, but if we shutdown, ES has to replay the translog after shutdown and this fails to replay, so the index no longer comes up.
</comment><comment author="bleskes" created="2014-07-02T07:53:02Z" id="47746755">@Zazik it looks indeed very much like #6441 which was solved with https://github.com/elasticsearch/elasticsearch/pull/6576 . I think we can close this as resolved. If you want, you can verify it was fixed by checking out the 1.x branch and giving it a spin. Thx for the detailed report.
</comment><comment author="Zazik" created="2014-07-02T08:17:23Z" id="47748507">I performed all steps described by @uschindler and made sure that this is the same issue like #6441.

Thanks a lot to all of you. Looking forward to the new version.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Strict query parsing on percolators and alias filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6664</link><project id="" key="" /><description>Today, if we create a filtered alias or a percolator query which references a field that is not yet mapped, the field is interpreted as a string.

This is a problem when (eg) the user then indexes a document where the field is really numeric or a date.  Any existing filters or percolator queries will not work correctly until the cached filter/query is regenerated (eg after a restart).

Instead, we should support a "strict" query parsing mode which throws an error if an unmapped field is referenced.
</description><key id="36897784">6664</key><summary>Strict query parsing on percolators and alias filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T14:28:57Z</created><updated>2015-05-05T07:30:20Z</updated><resolved>2014-09-10T13:04:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-01T14:29:32Z" id="47662869">See #6572 
</comment><comment author="jpountz" created="2014-07-04T07:53:59Z" id="48017409">One option might be to make the filter evaluation lazy until fields are mapped but that would add significant complexity/overhead so I think strict query parsing would be the way to go! Maybe we could have a helper method on the context to resolve fields that would return null when strict parsing is disabled and raise an error otherwise?
</comment><comment author="clintongormley" created="2014-07-11T09:02:24Z" id="48708964">Also see #6110 - term lookup filters should verify that the doc actually exists when the filter is instantiated.
</comment><comment author="s1monw" created="2014-09-10T12:24:17Z" id="55107312">@martijnvg did this not get closed or is it only partially resolved ? Anyway can we close or push to 1.5?
</comment><comment author="martijnvg" created="2014-09-10T12:59:32Z" id="55110944">@s1monw The strict field resolution is already pushed. I forgot that there was an issue for this!

@clintongormley Regarding the terms lookup filter, the #6928 PR doesn't cover this since it only takes care of unmapped fields in the mapping. I think we should open a different issue for this? Also the geo_shape filter with is referring to an indexed shape suffers from the same problem.
</comment><comment author="s1monw" created="2014-09-10T13:01:51Z" id="55111228">@martijnvg can we close it?
</comment><comment author="martijnvg" created="2014-09-10T13:03:47Z" id="55111454">@clintongormley Ow wait, the geo_shape filters fail if an indexed shape document doesn't exist. I think we should do the same for the terms lookup filter? (right now it silently ignores it by not matching anything)
</comment><comment author="martijnvg" created="2014-09-10T13:04:29Z" id="55111538">@s1monw yes, imo we can close this.
</comment><comment author="andreasch" created="2015-04-24T23:01:07Z" id="96090846">Shouldn't alias filters at least allow referencing fields that are internal like _type, _id or _routing?
</comment><comment author="clintongormley" created="2015-04-26T17:35:27Z" id="96411875">@andreasch this sounds like it should be a new issue?
</comment><comment author="andreasch" created="2015-05-04T17:26:47Z" id="98786228">@clintongormle I believe it is a new issue and I can go create one if it makes sense. 

We have a multi-tenant implementation where the routing value is equal to the tenant name and routing is always required and used.

We also use alias filtering as described here:
http://www.elastic.co/guide/en/elasticsearch/guide/master/faking-it.html

For our alias filters we have something like this:

``` javascript
{
    "term":{
        "_routing":"tenantName"
    }
}
```

However, Elasticsearch versions 1.4 and after do not allow for such an alias filter to be created.
</comment><comment author="clintongormley" created="2015-05-05T07:30:20Z" id="98982514">@andreasch ok - I thought you meant something else.  You can create filters like that today, but you need to set the `_routing` field to be indexed.  (This will be done by default in v2).  See http://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-routing-field.html#_store_index
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Prevent usage of System Properties in the InternalTestCluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6663</link><project id="" key="" /><description>All settings should be passes as settings and the enviroment should not
influence the test cluster settings. The settings we care about ie.
`es.node.mode` and `es.logger.level` should be passed via settings.
This allows tests to override these settings if they for instance need
`network` transport to operate at all.
</description><key id="36893692">6663</key><summary>Test: Prevent usage of System Properties in the InternalTestCluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T13:46:42Z</created><updated>2014-07-16T13:05:50Z</updated><resolved>2014-07-01T16:11:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-01T14:20:53Z" id="47661740">Left a few comments @s1monw ;)
</comment><comment author="s1monw" created="2014-07-01T15:29:26Z" id="47670814">@javanna I pushed a new commit
</comment><comment author="javanna" created="2014-07-01T16:01:33Z" id="47675041">LGTM
</comment><comment author="spinscale" created="2014-07-01T16:11:23Z" id="47676261">LGTM
</comment><comment author="s1monw" created="2014-07-01T16:12:10Z" id="47676357">just in time @spinscale 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possible race condition in checksum name generator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6662</link><project id="" key="" /><description>When three threads are trying to write checksums at the same time, it's possible for all three threads to obtain the same checksum file name A. Then the first thread enters the synchronized section, creates the file with name A and exits. The second thread enters the synchronized section, checks that A exists, creates file A+1 and exits the critical section. Then it proceeds to clean up  and deletes all checksum files including A. If it happens before the third thread enters the synchronized section, it's possible for the third thread to check for A and since it no longer exists create the checksum file A the second time, which triggers "file _checksums-XXXXXXXXXXXXX was already written to" exception in MockDirectoryWrapper and fails recovery.
</description><key id="36889497">6662</key><summary>Fix possible race condition in checksum name generator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T12:55:36Z</created><updated>2015-06-07T19:35:36Z</updated><resolved>2014-07-01T14:11:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-01T13:00:06Z" id="47652517">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in timestamp-field.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6661</link><project id="" key="" /><description /><key id="36884291">6661</key><summary>Fix typo in timestamp-field.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">hanneskaeufler</reporter><labels /><created>2014-07-01T11:37:51Z</created><updated>2014-07-03T11:28:02Z</updated><resolved>2014-07-03T11:28:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-01T14:23:33Z" id="47662068">HI @hanneskaeufler 

Thanks for the PR.  Please could you sign the CLA so that I can get this merged in?
http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="hanneskaeufler" created="2014-07-01T14:27:01Z" id="47662541">Sure, done!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure `index.version.created` is consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6660</link><project id="" key="" /><description>Today `index.version.created` depends on the version of the master
node in the cluster. This is potentially causing new features to be
expected on shards that didn't exist when the index was created.
There is no notion of `where was the shard allocated first` such that
`index.version.created` can't be reliably used as a feature flag.

With this change the `index.version.created` can be reliably used to
determin the smallest nodes version at the point in time when the index
was created. This means we can safely use certain features that would
for instance require reindeing and / or would not work if not the
entire index (all shards and segments) have been created with a certain
version or newer.
</description><key id="36879945">6660</key><summary>Ensure `index.version.created` is consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-07-01T10:27:51Z</created><updated>2015-06-07T13:07:44Z</updated><resolved>2014-07-01T16:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-01T14:00:41Z" id="47659184">LGTM except for the comment on using the currentState var
</comment><comment author="s1monw" created="2014-07-01T14:19:09Z" id="47661540">pushed a new commit
</comment><comment author="s1monw" created="2014-07-01T14:25:21Z" id="47662309">and another commit now really using the `currentState`
</comment><comment author="kimchy" created="2014-07-01T14:25:50Z" id="47662387">LGTM
</comment><comment author="bleskes" created="2014-07-01T15:05:59Z" id="47667716">LGTM, left one minor comment.
</comment><comment author="s1monw" created="2014-07-01T15:31:41Z" id="47671099">@bleskes  pushed a new commit
</comment><comment author="bleskes" created="2014-07-01T15:32:52Z" id="47671264">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery from local gateway should re-introduce new mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6659</link><project id="" key="" /><description>The delayed mapping intro tests exposed a bug where if a new mapping is introduced, yet not updated on the master, and a full restart occurs, reply of the transaction log will not cause the new mapping to be re-introduced.
</description><key id="36847377">6659</key><summary>Recovery from local gateway should re-introduce new mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T23:08:55Z</created><updated>2015-06-07T19:35:59Z</updated><resolved>2014-06-30T23:54:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Test: Randomize translog implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6658</link><project id="" key="" /><description>We have two types of transaction log implementation - a simple and a buffered one. We should randomly choose one of the two during testing.
</description><key id="36831914">6658</key><summary>Test: Randomize translog implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>test</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T19:45:35Z</created><updated>2015-06-07T13:07:53Z</updated><resolved>2014-07-18T10:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T10:30:00Z" id="48453741">moving to `1.4` for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ensure selection of best terms is indeed O(n)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6657</link><project id="" key="" /><description>Previously the size of the priority queue was wrongly set to the total number
of terms. Instead, it should be set to 'maxQueryTerms'. This makes the
selection of best terms O(n), instead of O(n*log(n)).

Jira patch: https://issues.apache.org/jira/browse/LUCENE-5795
</description><key id="36829519">6657</key><summary>Ensure selection of best terms is indeed O(n)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T19:17:42Z</created><updated>2015-06-07T13:08:11Z</updated><resolved>2014-07-11T09:20:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-02T07:37:44Z" id="47745663">I left a comment on the lucene issue. 
</comment><comment author="s1monw" created="2014-07-10T16:22:29Z" id="48628074">I committed this to lucene will be in 4.10
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Document findable in query but not directly in index v1.2.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6656</link><project id="" key="" /><description>Just updated this morning on 1.2.1 et stumbled upon a huge WTF moment : 

127.0.0.1/index_alias/fiche/536643 fetches this result : 
`{
_index: "index_alias",
_type: "fiche",
_id: "536643",
found: false
}`

which seems logical until 127.0.0.1/index_alias/fiche/_search?q=_id:536643 fetches : 

```
{
  took: 14,
  timed_out: false,
  _shards: {
    total: 5,
    successful: 5,
    failed: 0
  },
  hits: {
    total: 1,
    max_score: 1,
    hits: [
      {
        _index: "index",
        _type: "fiche",
        _id: "536643",
        _score: 1,
        _source: {.....}
      }
    ]
  }
}
```

( I replaced the real index name with "index" and the alias by "index_alias" )
Any idea on what could cause this behavior?
</description><key id="36807634">6656</key><summary>Document findable in query but not directly in index v1.2.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JimminiKin</reporter><labels /><created>2014-06-30T15:23:32Z</created><updated>2014-06-30T15:30:13Z</updated><resolved>2014-06-30T15:29:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-30T15:27:56Z" id="47545829">Which version did you upgrade from? Any chance it was 1.2.0?
</comment><comment author="JimminiKin" created="2014-06-30T15:28:15Z" id="47545874">Yes it was
</comment><comment author="jpountz" created="2014-06-30T15:28:21Z" id="47545881">Are you upgrading from 1.2.0? If so, I'm afraid that you are impacted by a routing bug that was introduced in 1.2.0. See http://www.elasticsearch.org/blog/elasticsearch-1-2-1-released/ for a description of the bug and http://www.elasticsearch.org/blog/tool-help-routing-issues-elasticsearch-1-2-0/ for help to recover from it.
</comment><comment author="JimminiKin" created="2014-06-30T15:29:39Z" id="47546058">Thanks everybody, sorry about the report, should have search more thoroughly before posting.
</comment><comment author="jpountz" created="2014-06-30T15:30:13Z" id="47546136">No worries!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Histogram Aggregation key Bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6655</link><project id="" key="" /><description>Hi,

I'm working with histogram aggregation but there is something strange with keys.
For instance (cf : http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html):

If I use this request :

``` json
{
    "aggs" : {
        "prices" : {
            "histogram" : {
                "field" : "price",
                "interval" : 50
            }
        }
    }
}
```

I obtain something like this :

``` json
{
    "aggregations": {
        "prices" : {
            "buckets": [
                {
                    "key_as_string" : "0",
                    "key": 0,
                    "doc_count": 2
                },
                {
                    "key_as_string" : "50",
                    "key": 50,
                    "doc_count": 4
                },
                {
                    "key_as_string" : "150",
                    "key": 150,
                    "doc_count": 3
                }
            ]
        }
    }
}
```

Instead of :

``` json
{
    "aggregations": {
        "prices" : {
            "buckets": [
                {
                    "key": 0,
                    "doc_count": 2
                },
                {
                    "key": 50,
                    "doc_count": 4
                },
                {
                    "key": 150,
                    "doc_count": 3
                }
            ]
        }
    }
}
```

You could say, it's not important but it generates json ~1/3 bigger...
Is there a mean to disable this ???

Moreover, in Elasticsearch Java API, it could be fine to have a method to request the response as a hash instead keyed by the buckets keys (cf :http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html#_response_format)

Thanks!!!
</description><key id="36806372">6655</key><summary>Aggregations: Histogram Aggregation key Bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">rnonnon</reporter><labels><label>bug</label><label>v1.3.0</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T15:10:13Z</created><updated>2014-08-01T14:27:57Z</updated><resolved>2014-07-16T10:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-30T15:18:49Z" id="47544645">There is currently no way to disable `key_as_string`.

&gt; Moreover, in Elasticsearch Java API, it could be fine to have a method to request the response as a hash instead keyed by the buckets keys

I am not sure to get what you mean. The Java API already has hash-like access to the buckets via the `getBucketByKey` method, is it what you are looking for?
</comment><comment author="uboness" created="2014-07-01T01:14:56Z" id="47606826">this is a bug... the `key_as_string` should only be there if format is explicitly specified (like in all other aggs)
</comment><comment author="rnonnon" created="2014-07-01T08:00:03Z" id="47627649">Thanks jpountz for having format my post  ;)
Thanks both for your answers.

I meant it's not possible to ask ES for a JSON like this :

``` json
{
    "aggregations": {
        "prices": {
            "buckets": {
                "0": {
                    "key": 0,
                    "doc_count": 2
                },
                "50": {
                    "key": 50,
                    "doc_count": 4
                },
                "150": {
                    "key": 150,
                    "doc_count": 3
                }
            }
        }
    }
}
```

Yes it's possible to access the Json with getBucketByKey but the JSON is like this : 

``` json
{
    "aggregations": {
        "prices" : {
            "buckets": [
                {
                    "key_as_string" : "0",
                    "key": 0,
                    "doc_count": 2
                },
                {
                    "key_as_string" : "50",
                    "key": 50,
                    "doc_count": 4
                },
                {
                    "key_as_string" : "150",
                    "key": 150,
                    "doc_count": 3
                }
            ]
        }
    }
}
```

Furthermore if it's a bug, why close that post?
</comment><comment author="clintongormley" created="2014-07-01T14:20:33Z" id="47661705">&gt; Furthermore if it's a bug, why close that post?

Because this post is still open, and the closed post is a duplicate.
</comment><comment author="rnonnon" created="2014-07-01T14:28:53Z" id="47662795">Oups... My Bad.... :s
Sorry
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Histogram Aggregation key Bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6654</link><project id="" key="" /><description>Hi,

I'm working with histogram aggregation but there is something strange with keys.
For instance (cf : http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html):

If I use this request :
{
    "aggs" : {
        "prices" : {
            "histogram" : {
                "field" : "price",
                "interval" : 50
            }
        }
    }
}

I obtain something like this :
{
    "aggregations": {
        "prices" : {
            "buckets": [
                {
                    "key_as_string" : "0",
                    "key": 0,
                    "doc_count": 2
                },
                {
                                        "key_as_string" : "50",
                    "key": 50,
                    "doc_count": 4
                },
                {
                                        "key_as_string" : "150",
                    "key": 150,
                    "doc_count": 3
                }
            ]
        }
    }
}

Instead of :
{
    "aggregations": {
        "prices" : {
            "buckets": [
                {
                    "key": 0,
                    "doc_count": 2
                },
                {
                    "key": 50,
                    "doc_count": 4
                },
                {
                    "key": 150,
                    "doc_count": 3
                }
            ]
        }
    }
}

You could say, it's not important but it generates json ~1/3 bigger...
Is there a mean to disable this ???

Moreover, in Elasticsearch Java API, it could be fine to have a method to request the response as a hash instead keyed by the buckets keys (cf :http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html#_response_format)

Thanks!!!
</description><key id="36806293">6654</key><summary>Histogram Aggregation key Bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rnonnon</reporter><labels /><created>2014-06-30T15:09:16Z</created><updated>2014-06-30T15:11:16Z</updated><resolved>2014-06-30T15:11:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-30T15:11:16Z" id="47543614">Duplicate of https://github.com/elasticsearch/elasticsearch/issues/6655
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script engines should be aware of the script name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6653</link><project id="" key="" /><description>It would be nice if we could access the name of the script from the script engine framework so it could be logged when errors occur. This would be especially helpful for additional logging for users with dynamic scripting disabled.
</description><key id="36800377">6653</key><summary>Script engines should be aware of the script name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jdconrad/following{/other_user}', u'events_url': u'https://api.github.com/users/jdconrad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jdconrad/orgs', u'url': u'https://api.github.com/users/jdconrad', u'gists_url': u'https://api.github.com/users/jdconrad/gists{/gist_id}', u'html_url': u'https://github.com/jdconrad', u'subscriptions_url': u'https://api.github.com/users/jdconrad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/2126764?v=4', u'repos_url': u'https://api.github.com/users/jdconrad/repos', u'received_events_url': u'https://api.github.com/users/jdconrad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jdconrad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jdconrad', u'type': u'User', u'id': 2126764, u'followers_url': u'https://api.github.com/users/jdconrad/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>enhancement</label></labels><created>2014-06-30T14:07:01Z</created><updated>2015-07-11T01:28:29Z</updated><resolved>2015-07-11T01:28:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-19T18:28:41Z" id="83703737">Heya @dakrone are you working on this? If not can you elaborate on what you mean? Where should the script name be available but it isn't?
</comment><comment author="dakrone" created="2015-03-20T15:00:13Z" id="84037725">@javanna when exceptions are thrown in a script, it's nice to know in the logs which script (if it came from on disk) the error came from, I believe right now we use:

``` java
private String generateScriptName() {
  return "Script" + counter.incrementAndGet() + ".groovy";
}
```

Which works for dynamic scripts (maybe we want to rename it "dynamic-script1.groovy" or something to be extra clear. The on-disk ones should have their name clearly in the exception/logging.
</comment><comment author="jdconrad" created="2015-07-10T22:28:43Z" id="120541984">@rjernst Would you please take a look when you get a chance?  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extended_bound does not return empty buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6652</link><project id="" key="" /><description>I'm trying to get a complete histogram for the month of June, here's my query:

```
{
    "query": {
        "filtered": {
            "filter": {
                "and": [
                   [ multiple filters here including terms, range and bool ]
                ]
            }
        }
    },
    "aggs": {
        "histogram": {
            "date_histogram": {
                "field": "_timestamp",
                "interval": "1d",
                "min_doc_count": 0,
                "extended_bounds": {
                    "min": 1401573600000,
                    "max": 1404165599999
                },
                "pre_zone": 2,
                "post_zone": -2
            },
            [ multiple sub terms aggregation here ]
        }
    }
}
```

This should give me every day of the month of June but it does not, empty buckets are not returned.
</description><key id="36796182">6652</key><summary>Extended_bound does not return empty buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rocambolesque</reporter><labels /><created>2014-06-30T13:17:12Z</created><updated>2014-08-13T07:58:17Z</updated><resolved>2014-07-01T14:15:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-07-01T09:47:28Z" id="47636968">I have tested this on master and it seems to return the expected buckets.  Which version of Elasticsearch are you running?
</comment><comment author="rocambolesque" created="2014-07-01T09:54:21Z" id="47637606">I'm running 1.2.1.
The strange thing is that my ElasticSearch instance crashed running this query, I got:

```
java.lang.OutOfMemoryError: Java heap space
Dumping heap to java_pid2903.hprof ...
Heap dump file created [35656409797 bytes in 236.788 secs]
[2014-06-30 15:58:03,560][DEBUG][action.search.type       ] [Bloodshed] failed to reduce search
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce]
[....]
```

I restarted it and then the query was working properly.
</comment><comment author="colings86" created="2014-07-01T12:32:55Z" id="47649909">ok so this is most likely to do with:

&gt; [ multiple sub terms aggregation here ]

Each bucket generated by the date_histogram aggregation will contain buckets for each matched term in the sub aggregations.  Potentially generating 30x the number of buckets which would be generated for a single day (this is worst case as not all the days in June may be populated and not all terms will exist in all days). If you have further levels of bucket aggregations nested under the terms aggregation the number of buckets will grow further (and exponentially).

As the buckets are transient data stored in memory for the life of the request, generating a lot of buckets in this way can be very memory intensive and can lead to OOM Errors.  Although there are features currently under development which should help to combat these problems (detailed below), if you continue seeing these problems, you may need to either simplify your aggregation structure to reduce the number of generated buckets or increase the heap memory assigned to your ES nodes.

There is a circuit breaker currently in development which should help stop the server from actually running out of memory in these cases and instead will stop the request and pass an error back to the client. This is planned for 1.3.

There is also a feature coming in 1.3 which will add a new mode to an aggregation where the calculation of sub-aggregations can be deferred until the parent aggregation has determined the final list of buckets to keep (e.g. the top 10 terms in a terms aggregation) and will then replay the collected documents to calculate the sub-aggregations, vastly reducing the number of generated buckets).  This incurs a memory cost of keeping the collected docIds in memory but can be useful when the number of generated buckets is very large.
</comment><comment author="rocambolesque" created="2014-07-01T12:45:41Z" id="47651090">I will look into simplifying my aggregations, filling the blanks in the histogram myself or make separate queries, although it is really handy to get everything with the right timezone offset with a single query.

Thanks for the explanation, looking forward to 1.3!
</comment><comment author="colings86" created="2014-07-01T12:52:20Z" id="47651737">What is the structure of the sub aggregations under the date_histogram?  It is more likely to be that structure which is causing the issue then the date_histogram itself.  

Empty date_histogram buckets will not generate any sub-aggregation buckets since no documents will be passed to the sub-aggregations so I don't think your issue will be due to the min_doc_count or extended_bounds options you have set but more due to the sub-aggregations you are using.
</comment><comment author="rocambolesque" created="2014-07-01T13:01:35Z" id="47652680">Here's the full aggregation:

```
"aggs": {
    "histogram": {
      "date_histogram": {
        "field": "_timestamp",
        "interval": "1d",
        "min_doc_count": 0,
        "extended_bounds": {
          "min": 1401573600000,
          "max": 1404165599999
        },
        "pre_zone": 2,
        "post_zone": -2
      },
      "aggs": {
        "apps": {
          "terms": {
            "field": "field1",
            "size": 1 // this could be more but it ran out of memory with size = 1
          },
          "aggs": {
            "events": {
              "terms": {
                "field": "field2"
              },
              "aggs": {
                "formats": {
                  "terms": {
                    "field": "field3"
                  },
                  "aggs": {
                    "countries": {
                      "terms": {
                        "field": "field4",
                        "size": 50
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```
</comment><comment author="colings86" created="2014-07-01T13:31:25Z" id="47655805">&gt; "size": 1 // this could be more but it ran out of memory with size = 1

The size attribute doesn't  affect the amount of memory require to compute the aggregation since we have to track all buckets until we have processed all the documents to determine which are the top N buckets.  The amount of memory required is a function of the number of buckets that need to be created to work out the top N buckets.

For example if your query returned 1,000,000 documents which had 100 different countries, 50 different formats, 200 different events and 20 apps across even a single day,  you would potentially be creating up to 100 \* 50 \* 200 \* 20 = 20,000,000 buckets for each request.

This example is the sort of case which could benefit from the deferred sub-aggregation feature I previously mentioned as the number of potential buckets is quite a bit larger than the number of hits returned by the query.

Out of interest, how many hits does your query (without aggregations) return? 
</comment><comment author="rocambolesque" created="2014-07-01T13:46:35Z" id="47657542">The number of hits ranges from a few thousands to a few millions for a 30 day range.
Although there are only a few number of formats, events and countries, it might create a huge number of buckets indeed.
</comment><comment author="colings86" created="2014-07-01T14:15:34Z" id="47661066">Until 1.3 you will probably need to simplify your aggregations structure or increase heap memory if you can.  After 1.3 has been released you should be able to take advantage of the deferred aggregations in #6128.
</comment><comment author="colings86" created="2014-07-01T14:15:54Z" id="47661108">Closing in favour of #6128 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing coma in the example.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6651</link><project id="" key="" /><description /><key id="36795475">6651</key><summary>Missing coma in the example.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jnguyenx</reporter><labels><label>docs</label></labels><created>2014-06-30T13:07:28Z</created><updated>2014-07-03T06:17:43Z</updated><resolved>2014-07-03T06:17:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-07-01T06:29:48Z" id="47621330">Hey,

good catch. Can you sign our CLA at http://www.elasticsearch.org/contributor-agreement/ - so I can get it in? Thanks a lot!
</comment><comment author="jnguyenx" created="2014-07-01T07:26:05Z" id="47624953">Hi,

It is signed :-)

Thanks!
</comment><comment author="spinscale" created="2014-07-03T06:17:42Z" id="47870609">thx. Closed by https://github.com/elasticsearch/elasticsearch/commit/1883f74cc057d4f551e100bfef46cbbce0d1592f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Randomize the logging level between DEBUG and INFO</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6650</link><project id="" key="" /><description>we should randomize the logging level to make sure we don't hide any NPEs etc.
</description><key id="36794471">6650</key><summary>[TEST] Randomize the logging level between DEBUG and INFO</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>test</label></labels><created>2014-06-30T12:53:23Z</created><updated>2015-04-28T09:23:39Z</updated><resolved>2015-04-28T09:23:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-30T13:01:06Z" id="47528227">We found NPEs with `TRACE` as well some time ago... it might make sense to add TRACE too here and there (rarely)?
</comment><comment author="s1monw" created="2014-06-30T13:01:26Z" id="47528255">oh yeah +1
</comment><comment author="s1monw" created="2015-04-28T09:23:38Z" id="96986503">closing for now not going anywhere
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JAVA API: Fix source excludes setting if no includes were provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6649</link><project id="" key="" /><description>Due to a bogus if-check in SearchSourceBuilder.fetchSource(String include, String exclude)
the excludes only got set when the includes were not null. Fixed this and added some
basic tests.

Closes #6632
</description><key id="36789052">6649</key><summary>JAVA API: Fix source excludes setting if no includes were provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels /><created>2014-06-30T11:22:39Z</created><updated>2014-07-02T09:49:56Z</updated><resolved>2014-07-02T09:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-30T16:32:17Z" id="47553864">Left a minor comment, LGTM otherwise. 
I really like the fact that you wrote unit tests for it!
</comment><comment author="javanna" created="2014-07-01T08:15:19Z" id="47628896">LGTM
</comment><comment author="s1monw" created="2014-07-02T09:43:34Z" id="47756337">this should be ported to `1.2` :P
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: Update mapping on master in async manner</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6648</link><project id="" key="" /><description>Today, when a new mapping is introduced, the mapping is rebuilt (refreshSource) on the thread that performs the indexing request. This can become heavier and heavier if new mappings keeps on being introduced, we can move this process to another thread that will be responsible to refresh the source and then send the update mapping to the master (note, this doesn't change the semantics of new mapping introduction, since they are async anyhow).
When doing so, the thread can also try and batch as much updates as possible, this is handy especially when multiple shards for the same index exists on the same node. An internal setting that can control the time to wait for batches is also added (defaults to 0).

Testing wise, a new support method on ElasticsearchIntegrationTest#waitForConcreteMappingsOnAll to allow to wait for the concrete manifestation of mappings on all relevant nodes is added. Some tests mistakenly rely on the fact that there are no more pending tasks to mean mappings have been updated, so if we see, timing related, failures down later (all tests pass), then those will need to be fixed to wither awaitBusy on the master for the new mapping, or in the rare case, wait for the concrete mapping on all the nodes using the new method.

Note, this change also removes `action.wait_on_mapping_change`, this is an internal setting, and is not recommended to set it. It was used using the old test infrastructure to validate if the problem was due to mapping propagation, but we have a much better infra for this now.
</description><key id="36786864">6648</key><summary>Mappings: Update mapping on master in async manner</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>:Mapping</label><label>breaking</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T10:44:16Z</created><updated>2015-06-06T16:50:19Z</updated><resolved>2014-06-30T20:09:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-30T11:43:16Z" id="47521977">Left two comments, this change looks good to me. Maybe someone else can also take a look?
</comment><comment author="bleskes" created="2014-06-30T14:55:05Z" id="47541540">I went through the change. Bulk of it looks good. Left some minor comments. I also wonder if we should mark it as breaking because we removed the `action.wait_on_mapping_change` option.
</comment><comment author="kimchy" created="2014-06-30T16:02:05Z" id="47550291">@bleskes used the support method, added a note on breaking, also bit the bullet and cleaned all calls to update mapping to include doc mapper and UUID actually used 
</comment><comment author="bleskes" created="2014-06-30T19:36:17Z" id="47576242">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyzers for Vietnamese?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6647</link><project id="" key="" /><description>Any analyzers for Vietnamese on the roadmap?

Thx and cheers
</description><key id="36785774">6647</key><summary>Analyzers for Vietnamese?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nguyenchiencong</reporter><labels><label>enhancement</label><label>high hanging fruit</label></labels><created>2014-06-30T10:26:36Z</created><updated>2016-07-17T07:18:42Z</updated><resolved>2014-10-20T09:56:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-30T12:02:17Z" id="47523413">Indeed we don't have a good tokenizer for vietnamese today. Although we would like to have one, vietnamese segmentation is quite hard so I'm afraid this won't be fixed anytime soon.
</comment><comment author="nguyenchiencong" created="2014-07-02T03:04:16Z" id="47732840">Do you have by any chance a list of Vietnamese stopwords? thx
</comment><comment author="jpountz" created="2014-07-02T07:05:54Z" id="47743539">No we don't.
</comment><comment author="anhtran" created="2014-10-14T14:28:14Z" id="59053611">@jpountz How about this thing? https://github.com/CaoManhDat/VNAnalyzer
It based on the research at http://mim.hus.vnu.edu.vn/phuonglh/tools/userguide-vnTokenizer.pdf
I believe it can wrap about 80-90% cases in Vietnamese. That's good enough for searching.
</comment><comment author="jpountz" created="2014-10-14T15:42:58Z" id="59067129">vnTokenizer is under GPL, which would be an issue for inclusion in Lucene or Elasticsearch. However, elasticsearch supports plugin-in custom analyzers so you could write a plugin that would expose this analyzer, see for instance https://github.com/elasticsearch/elasticsearch-analysis-kuromoji
</comment><comment author="nguyenchiencong" created="2014-10-19T12:21:46Z" id="59648154">Thanks. For those wanting a vietnamese plugin, you guys can check out this one: https://github.com/duydo/elasticsearch-analysis-vietnamese
</comment><comment author="clintongormley" created="2014-10-19T19:18:32Z" id="59661243">@nguyenchiencong you want to submit a PR adding this to the plugins page here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#analysis-plugins ?
</comment><comment author="nguyenchiencong" created="2014-10-20T03:40:38Z" id="59680439">@duydo is the author. I think we should ask him first. @duydo it would be great if you can do it.
</comment><comment author="duydo" created="2014-10-20T06:30:38Z" id="59688639">Thanks @nguyenchiencong for mentioning the plugin.

@clintongormley It would be great if you can add the plugin to the plugins page. Thank you.
</comment><comment author="dripp1" created="2016-07-17T07:18:42Z" id="233169105">That plugin has issues with highlighting offsets, at least in version 2.2.0. 
Has anybody been able to use it or can recommend another Vietnamese plugin?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix return of wrong request type on failed updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6646</link><project id="" key="" /><description>In case an update request failed (for example when updating with a
wrongly formatted date), the returned index operation type was index
instead of update.

Closes #6630
</description><key id="36784172">6646</key><summary>Fix return of wrong request type on failed updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T10:01:12Z</created><updated>2015-06-07T19:37:01Z</updated><resolved>2014-07-02T10:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-30T16:32:29Z" id="47553889">Left a small comment, LGTM otherwise!
</comment><comment author="javanna" created="2014-07-01T08:14:02Z" id="47628790">LGTM
</comment><comment author="s1monw" created="2014-07-02T09:45:04Z" id="47756467">LGTM should go into 1.2 too
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cancel recovery if shard on the target node closes during recovery operation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6645</link><project id="" key="" /><description>On the target side if the shard closes while recovery is in progress (e.g. files being transferred), the recovery operation should be cancelled.
</description><key id="36782023">6645</key><summary>Cancel recovery if shard on the target node closes during recovery operation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T09:29:50Z</created><updated>2015-06-07T13:08:32Z</updated><resolved>2014-07-01T16:18:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-30T11:15:08Z" id="47520037">LGTM - I would change the title of the PR to indicate this is about closing the shard on the _target_ node. I was thinking how we can test it. Maybe a test that issues a relocation command, then overrides it with an allocation filtering rule. This will at least make sure the code kicks in every once in a while
</comment><comment author="martijnvg" created="2014-06-30T13:19:56Z" id="47530132">@bleskes I added a test that is likely to cancel an ongoing recovery.
</comment><comment author="bleskes" created="2014-06-30T13:28:46Z" id="47531096">@martijnvg thx. Left some comments on the commit (sorry, notice too late it's not the PR)
</comment><comment author="martijnvg" created="2014-07-01T09:23:16Z" id="47634805">@bleskes Good points, I  applied the feedback.
</comment><comment author="bleskes" created="2014-07-01T15:46:37Z" id="47673072">LGTM!
</comment><comment author="magnhaug" created="2014-07-05T00:40:29Z" id="48075495">Does this solve issue #6430 ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryState should be concurrently accessible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6644</link><project id="" key="" /><description>For ongoing recoveries we maintain a recovery state. That state is used by both the recovery mechanism as the recovery API, which reports on ongoing recoveries. This means  RecoveryState can be accessed concurrently from multiple threads. 

I run into at least one problem concerning the list of files that should be replicated:

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/recovery/RecoveryState.java#L563
</description><key id="36779097">6644</key><summary>RecoveryState should be concurrently accessible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>adoptme</label><label>bug</label><label>resiliency</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-30T08:47:55Z</created><updated>2015-02-25T16:55:55Z</updated><resolved>2015-02-25T16:55:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vinhn" created="2014-11-04T19:36:35Z" id="61700406">On a node restart, it's common that a user constantly checks the state of shard recoveries until the cluster is green.  Like to check if the UNASSIGNED shard counts are dropping.  Or, to list those shards for debugging when we find that those counts are not dropping as fast as expected.  Would these activities be affected by this bug?
</comment><comment author="vinhn" created="2014-11-04T19:40:41Z" id="61701026">Also, what ES version(s) does this bug apply to?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to search the records with locations all in a polygon or multiPolygon?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6643</link><project id="" key="" /><description>I add the mappings and insert a record with 2 locations ([13, 13], [52, 52]),
and I want to search the results with it's locations all in the polygon&#65292;not one of the locations in the polygon. would you please tell me how to search the reslut? 
# 

curl -XPOST localhost:9200/test5 -d '{
    "mappings" : {
        "gistype" : {
            "properties" : {
                "address":{
                  "properties":{
                    "location":{"type" : "geo_point"}  
                  }
                }
            }
        }
    }
}'

curl -XPUT 'http://localhost:9200/test5/gistype/1' -d '{
    "name": "Wind &amp; Wetter, Berlin, Germany",
    "address": [
      {
        "name":1,
        "location": [13, 13]
      },
      {
        "name":2,
        "location": [52, 52]
      }
    ]
# }'

I searched like this , but I want to search the record locations all in the polygon. So it's wrong .

curl -XGET 'http://localhost:9200/test5/gistype/_search?pretty=true' -d '{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "geo_polygon" : {
                "location" : {
                    "points" : [
                        {"lat" : 0, "lon" : 0},
                        {"lat" : 14, "lon" : 0},
                        {"lat" : 14, "lon" : 14},
                        {"lat" : 0, "lon" : 14}
                    ]
                }
        }
      }
    }
  }
}'
</description><key id="36774297">6643</key><summary>How to search the records with locations all in a polygon or multiPolygon?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">binque</reporter><labels /><created>2014-06-30T07:24:57Z</created><updated>2014-06-30T07:37:28Z</updated><resolved>2014-06-30T07:37:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="binque" created="2014-06-30T07:27:42Z" id="47501972">@kimchy
</comment><comment author="dadoonet" created="2014-06-30T07:37:28Z" id="47502573">Hi!

Could you post your question on the mailing list?
We could help you there.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to start shard when restarting elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6642</link><project id="" key="" /><description>This just happened as I was restarting elasticsearch (1.1.1) on one node. As it came up again it failed to start one of the shards with the following exception:
[2014-06-29 04:38:35,401][INFO ][node                     ] [es-6636e.recfut.net] started
[2014-06-29 04:38:56,268][WARN ][indices.cluster          ] [es-6636e.recfut.net] [reference_2014-04-10_1][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [reference_2014-04-10_1][2] failed recovery
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:256)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: org.elasticsearch.index.engine.FlushNotAllowedEngineException: [reference_2014-04-10_1][2] already flushing...
        at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:756)
        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryFinalization(InternalIndexShard.java:716)
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:250)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:197)
        ... 3 more
[2014-06-29 04:38:56,824][WARN ][cluster.action.shard     ] [es-6636e.recfut.net] [reference_2014-04-10_1][2] sending failed shard for [reference_2014-04-10_1][2], node[IdoOCT9rTwWUZMCwf95hcg], [P], s[INITIALIZING], indexUUID [va11FwbtRTmsGsKW97LYkA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[reference_2014-04-10_1][2] failed recovery]; nested: FlushNotAllowedEngineException[[reference_2014-04-10_1][2] already flushing...]; ]]

After being anxious for a while I did another restart, and this time all shards started. There were no writes happening to the cluster during this. But it was very worrying.
</description><key id="36739442">6642</key><summary>Failed to start shard when restarting elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">maf23</reporter><labels><label>bug</label></labels><created>2014-06-29T04:59:44Z</created><updated>2014-07-07T14:02:04Z</updated><resolved>2014-07-02T10:24:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-30T13:21:06Z" id="47530259">At the end of a recovery process (from disk or copying primaries) we do a flush in order to make sure that all recent changes are committed to disk. If there is already an ongoing flush the flush operation fails (the assumption is that the flush was caused by an external API). I looked a bit at the code and I think I found a place that could cause a background flush, even if the shard is not yet started. I will work to remove it.

Other than that - your response was correct. It's just an unlucky timing.
</comment><comment author="maf23" created="2014-06-30T13:35:04Z" id="47531686">Thank you, that is exactly the kind of response I was hoping for.

On Mon, Jun 30, 2014 at 3:21 PM, Boaz Leskes notifications@github.com
wrote:

&gt; At the end of a recovery process (from disk or copying primaries) we do a
&gt; flush in order to make sure that all recent changes are committed to disk.
&gt; If there is already an ongoing flush the flush operation fails (the
&gt; assumption is that the flush was caused by an external API). I looked a bit
&gt; at the code and I think I found a place that could cause a background
&gt; flush, even if the shard is not yet started. I will work to remove it.
&gt; 
&gt; Other than that - your response was correct. It's just an unlucky timing.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6642#issuecomment-47530259
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search templates with numeric array loops</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6641</link><project id="" key="" /><description>On the elasticsearch webpage there is an template example using a mustache loop.
If you try use a long field type, for example:

```
           "status": {
              "type": "long",
              "ignore_malformed": false
           },
```

the query: 

GET /_search/template
{
  "template": {
    "query": {
      "terms": {
        "status": [
          "{{#status}}",
          "{{.}}",
          "{{/status}}"
        ]
      }
    }
  },
  "params": {
    "status": [ 1, 16 ]
  }
}

returns:

org.elasticsearch.search.SearchParseException: [library][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"terms":{"status":["","1","","16",""]}}}]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:511)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NumberFormatException: For input string: ""
</description><key id="36722437">6641</key><summary>Search templates with numeric array loops</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nnegativ</reporter><labels /><created>2014-06-28T10:16:45Z</created><updated>2014-07-01T14:24:21Z</updated><resolved>2014-06-30T08:43:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-30T08:43:29Z" id="47507632">Please use the mailinglist for requests like this in the future - github issues is for bugs only. Maybe you can just remove the ticks in the template for the number in the loop and it works.
</comment><comment author="nnegativ" created="2014-06-30T13:29:13Z" id="47531132">Thank you for response but I already tried this. The result was:

[2014-06-30 15:25:36,552][ERROR][search                   ] [Madam Slay] Error trying to parse template:
org.elasticsearch.common.jackson.core.JsonParseException: Unexpected character ('{' (code 123)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name
 at [Source: [B@e311eb1; line: 21, column: 34]

Note also how to data was parsed into the table (redundant empty strings)
</comment><comment author="clintongormley" created="2014-07-01T14:12:05Z" id="47660633">You probably need to use a string template here, not a json template.  Mustache can be tricky...
</comment><comment author="nnegativ" created="2014-07-01T14:24:21Z" id="47662175">@clintongormley: You're probably right, although it's very inconvenient - a template a little more complicated than in the examples is totally unreadable.The solution could be to allow illegal json in request (bad idea - eventually may work for template in external file) or allow formatting as in my example. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some percolators are used only after restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6640</link><project id="" key="" /><description>scenario:
1. put some stuff in the index
2. add a few percolators
3. percolate an existing document
4. some of the percolators would not appear in the results until cluster is restarted
only some percolators behave this way, eg this works fine

```
"query": {
                  "constant_score": {
                     "filter": {
                        "and": {
                           "filters": [
                              {
                                 "and": {
                                    "filters": [
                                       {
                                          "term": {
                                             "customer_id": "customer_id"
                                          }
                                       },
                                       {
                                          "range": {
                                             "stats.logins.count": {
                                                "from": -1,
                                                "to": null,
                                                "include_lower": false,
                                                "include_upper": true
                                             }
                                          }
                                       }
                                    ]
                                 }
                              },
                              {
                                 "term": {
                                    "app_id": "53ade461bda195157e8dd2fd"
                                 }
                              }
                           ]
                        }
                     }
                  }
               }
```

but this consistently fails

```
"query": {
                  "constant_score": {
                     "filter": {
                        "and": {
                           "filters": [
                              {
                                 "and": {
                                    "filters": [
                                       {
                                          "term": {
                                             "customer_id": "customer_id"
                                          }
                                       },
                                       {
                                          "term": {
                                             "stats.logins.count": 1
                                          }
                                       }
                                    ]
                                 }
                              },
                              {
                                 "term": {
                                    "app_id": "53ade461bda195157e8dd2fd"
                                 }
                              }
                           ]
                        }
                     }
                  }
               }
```

no amount of flushing or refreshing or waiting will fix that behavior
I use es 1.2.1 but can reproduce the same behavior with 1.1.2 on windows
</description><key id="36707712">6640</key><summary>Some percolators are used only after restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">OlegYch</reporter><labels /><created>2014-06-27T22:05:41Z</created><updated>2014-07-01T15:31:16Z</updated><resolved>2014-07-01T14:20:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OlegYch" created="2014-06-27T22:16:29Z" id="47406128">i have a failing test case locally, but unfortunately i'm not able to reproduce the behavior outside of my tests
the critical point seems to be using TermFilterBuilder or RangeFilter with lte or gte, all the other percolators i use seem to be behaving as expected
</comment><comment author="OlegYch" created="2014-06-27T22:33:39Z" id="47407290">happens with both local and remote clusters on windows and linux
</comment><comment author="julianhille" created="2014-06-29T23:31:29Z" id="47483094">are you percolating on dynamic fields? is this a case for a filter on a dynamic field?
</comment><comment author="OlegYch" created="2014-06-29T23:36:53Z" id="47483401">yes those are dynamic fields, though i'm indexing documents prior to indexing percolators
</comment><comment author="OlegYch" created="2014-06-29T23:37:31Z" id="47483434">(and using refresh on index)
</comment><comment author="julianhille" created="2014-06-29T23:42:42Z" id="47483704">have a look at this: #6572 you may encounter the same bug. 
</comment><comment author="OlegYch" created="2014-06-29T23:50:25Z" id="47484102">that doesn't seem to be related as i'm indexing the document before the percolator and additionally the percolator starts working after node restart
</comment><comment author="OlegYch" created="2014-06-30T00:13:00Z" id="47485335">actually i did index a few other percolators before indexing the document and the failing percolator.. so the issues are related indeed
</comment><comment author="OlegYch" created="2014-06-30T00:20:22Z" id="47485761">if i make sure to index a document before _all_ the percolators it seems to work
so what triggers the issue for me is the following sequence:
- index a percolator using fields a and b
- index the document containing fields a b and c
- test that the first percolator works
- index another percolator using field c
- index the same document with the same id
- the percolator using fields c will not appear in results until i restart the node
- index the same document a few more times and a few other documents and percolators, some of which will exhibit the failure, but others will work
- restart the node - now all the percolators appear in the results
</comment><comment author="jpountz" created="2014-06-30T11:51:22Z" id="47522591">@OlegYch would it be possible for you to write this sequence of actions as a curl recreation?
</comment><comment author="julianhille" created="2014-07-01T13:01:45Z" id="47652696">jpountz for parrs you can use my added gists in #6572 
</comment><comment author="martijnvg" created="2014-07-01T14:20:25Z" id="47661690">@OlegYch I'm closing this issue, because it is similar to #6572, the reason that the percolator yields results after node restart is because all the queries get parsed again into native Lucene queries and at that time the mapping contains all the fields mentioned in the queries. Closing and opening an index would have yielded the same result.
</comment><comment author="OlegYch" created="2014-07-01T14:35:24Z" id="47663606">any idea why it doesn't work before restart even though the document was already indexed before the failing percolator?
</comment><comment author="martijnvg" created="2014-07-01T15:04:32Z" id="47667519">@OlegYch The #6572 issue happens when you store a percolator query that refers to a field that doesn't exist yet in the mapping. If that isn't the case in your setup then can you attach a curl recreation that shows that?
</comment><comment author="OlegYch" created="2014-07-01T15:19:37Z" id="47669509">i'm certain that i save the document containing all the fields before the failing percolator
trying to minimize it... is there a way to capture requests made through java NodeClient perhaps?
</comment><comment author="martijnvg" created="2014-07-01T15:31:16Z" id="47671054">Not really unless you have some code that does this. Are you able to isolate the issue via a test using the java api?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Results returned in wrong order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6639</link><project id="" key="" /><description>I noticed while writing tests that ordering by most fields worked properly, but ordering by the `grantee` field does not. It returns the results in the same order regardless of whether "ascending" or "descending" is selected.

I've attached a script which reproduces the issue:

```
chris@lap-x201:~/aptivate/2014/indigodata/360giving-demos/src/scripts$ ./elasticsearch_bug.sh 
{"acknowledged":true}
{"took":57,"errors":false,"items":[{"index":{"_index":"test_360giving","_type":"modelresult","_id":"data.activity.5","_version":1,"status":201}}]}
{"took":7,"errors":false,"items":[{"index":{"_index":"test_360giving","_type":"modelresult","_id":"data.activity.6","_version":1,"status":201}}]}
data.activity.5 data.activity.6 
data.activity.5 data.activity.6 
```

The very last line should say "data.activity.6 data.activity.5", because it requested the results in the opposite order compared to the previous line.

If I make the data sufficiently different (e.g. change the grantee of the second record to "H" or "F") then it works as expected.

Here is my test script:

```
#!/bin/bash

server=http://localhost:9200
index="test_$RANDOM"
type=modelresult

set -e

do_curl() {
    method=$1
    shift
    curl -s -X$method $server/"$@"
    echo ''
}

do_curl DELETE "$index" || true
do_curl PUT "$index"
do_curl PUT "$index/$type/mapping" --data-binary @- &lt;&lt;EOF
{"$type": {"_boost": {"name": "boost", "null_value": 1.0}, "properties": {"grantee": {"index": "not_analyzed", "term_vector": "with_positions_offsets", "type": "string", "analyzer": "snowball", "boost": 1.0, "store": "yes"}}}}
EOF

do_curl POST "_bulk?refresh=true" --data-binary @- &lt;&lt;EOF
{"index": {"_type": "$type", "_id": "data.activity.5", "_index": "$index"}}
{"django_ct": "data.activity", "grantee": "Grantee 1"}
EOF

do_curl POST "_bulk?refresh=true" --data-binary @- &lt;&lt;EOF
{"index": {"_type": "$type", "_id": "data.activity.6", "_index": "$index"}}
{"django_ct": "data.activity", "grantee": "Grantee 2"}
EOF

# Note: the bug is that you get [data.activity.5, data.activity.6]
# regardless of the specified sort order, as shown below. If you make
# the records sufficiently different (e.g. change the grantee of the
# second record to "H" or "F") then it works.

do_curl GET "$index/$type/_search" --data-binary '{"sort": [{"grantee": {"order": "asc"}}], "query": {"filtered": {"filter": {"fquery": {"query": {"query_string": {"query": "*"}}}}}}}' | perl -ne 'while (s/"_id":"([^"]+)"//) { print "$1 " }'; echo
do_curl GET "$index/$type/_search" --data-binary '{"sort": [{"grantee": {"order": "desc"}}], "query": {"filtered": {"filter": {"fquery": {"query": {"query_string": {"query": "*"}}}}}}}' | perl -ne 'while (s/"_id":"([^"]+)"//) { print "$1 " }'; echo
do_curl DELETE "$index"
```
</description><key id="36671330">6639</key><summary>Results returned in wrong order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qris</reporter><labels /><created>2014-06-27T14:53:58Z</created><updated>2014-07-16T10:58:41Z</updated><resolved>2014-06-27T15:06:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-27T15:06:27Z" id="47357659">The problem I can see here is that your field grantee is analyzed by default.

I don't think your test case describe an actual issue.
You should try the same script but with a mapping which set your `grantee` field as `not_analyzed`.

See also: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_sort_mode_option

Closing. Feel free to reopen if you think it's an issue.
</comment><comment author="qris" created="2014-06-27T15:19:59Z" id="47359479">OK, i modified the script to create a new index each time and to set `grantee` to `not_analyzed` as shown above. Can I reopen the issue?

It returns the second result in the wrong order about 50% of the time, so I sometimes need to rerun the script several times to demonstrate the bug.
</comment><comment author="qris" created="2014-06-27T15:32:58Z" id="47361128">Also, the[ mapping documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping.html) says:

&gt; By default, there isn&#8217;t a need to define an explicit mapping, since one is automatically created and registered when a new type or new field is introduced (with no performance overhead) and have sensible defaults. Only when the defaults need to be overridden must a mapping definition be provided.

Does "sensible defaults" really include "not reliably sortable"? That would be an interesting definition of "sensible" :)
</comment><comment author="s1monw" created="2014-06-28T07:13:27Z" id="47420472">I added a test above but I can't reproduce the issue. Can you tell us which version you are using?
</comment><comment author="qris" created="2014-06-28T20:47:38Z" id="47438127">I reproduced it by installing Ubuntu 12.04.3 (i386) from the live CD, followed by these commands:

```
sudo apt-get install curl openjdk-7-jre
sudo dpkg -i elasticsearch-1.2.1.deb
sudo /etc/init.d/elasticsearch start
cat &gt; ./elasticsearch_bug.sh &lt;&lt;EOF ... (pasted in the script above)
chmod a+x ./elasticsearch_bug.sh
./elasticsearch_bug.sh
./elasticsearch_bug.sh
```

The second time I ran the script, I got the behaviour described above: 

```
data.activity.5 data.activity.6 
data.activity.5 data.activity.6 
```

Please could you try to reproduce it this way?
</comment><comment author="qris" created="2014-07-02T14:29:43Z" id="47782983">Please could someone reopen this issue? I think there is a real bug here, as I've been able to reproduce it on a clean system.
</comment><comment author="dakrone" created="2014-07-02T15:40:19Z" id="47792961">@qris there is a typo in your script,

```
do_curl PUT "$index/$type/mapping" --data-binary @- &lt;&lt;EOF
```

should be:

```
do_curl PUT "$index/$type/_mapping" --data-binary @- &lt;&lt;EOF
```

As a result, your mapping is not being applied (instead you're indexing a document with the id of "mapping"). If I correct this the sorting works correctly.
</comment><comment author="qris" created="2014-07-16T10:58:41Z" id="49150334">Thanks @dakrone, you were right, fixing that made it work and helped me to find the problem in my code!

I would note that this behaviour is really unintuitive. I think it would be better to fail to sort on an analyzed field rather than pretend to do so, and return incorrect results.

But even better would be to make it work as expected. Since the original field value is usually stored, why can't we sort on it? Is it not indexed? Could it be?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to snapshot to file system of a single node.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6638</link><project id="" key="" /><description>Add new "local" snapshot repository type that would be similar to shared filesystem repository but will allow storing all snapshot file on a local file system of a single node.
</description><key id="36652256">6638</key><summary>Add ability to snapshot to file system of a single node.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>adoptme</label><label>feature</label></labels><created>2014-06-27T10:25:18Z</created><updated>2017-07-19T12:53:58Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2014-06-27T11:47:13Z" id="47334336">What about an alternative - an API to download a tarball of a snapshot. "GET" a snapshot allows you to download the full snapshot (from the cluster, not just one node) ? I'm OK if my proposal is rejected ;)
</comment><comment author="dadoonet" created="2014-06-27T12:21:28Z" id="47337339">@jordansissel May be this is something we could work on in the future if at one point we want to deal with "attachments", so stream binary content to a client. But I might be wrong.

My 2 cents.
</comment><comment author="saahn" created="2014-11-12T23:00:04Z" id="62811415">what is the recommendation for restoring from the current fs repository? if i specify local fs path on each node, a snapshot of each node gets created separately on each individual node. If I want to restore these snapshots to a new cluster in a different datacenter that may not have the same number of nodes, the restore process is not as simple as scp-ing a tarball to each node in the new cluster... do you have any suggestions around this? Thanks.
</comment><comment author="schmorgs" created="2014-12-24T09:59:43Z" id="68041556">It looks like since the latest release of ES (1.4.1 is where I noticed this) the location for a snapshot has to be a shared mount, and ES seems to check for this now erroring with something like this when I try to use a local filesystem :

```
RepositoryVerificationException[[kibana-int] store location [/apps/elk/data/elasticsearch/backup] is not shared between node
```

In an older version, I could put a local directory and it worked as long as the same directory existed on the nodes.

I like the idea of being able to say "snapshot an entire index to a named node", either through the existing snapshot mechanism with some additional parameters to specify this, or as a tar file, as long as the snapshot is portable to another node where I could restore through that mechanism.
</comment><comment author="clintongormley" created="2015-11-21T15:32:57Z" id="158652554">@imotov do you still think this is something that we should implement?  My inclination would be to say: just set up an NFS mount or similar.
</comment><comment author="imotov" created="2015-11-21T15:51:42Z" id="158657758">@clintongormley we should definitely implement that. It will help users in several important use cases, including simplifying sharing of marvel data with support.
</comment><comment author="ashish-kumar-goel" created="2016-01-08T21:00:33Z" id="170122461">I am facing a similar issue. I have placed my cluster nodes on EC2 instances and when I try to take a snapshot on a local FS repository, it gives an error saying that the target location for snapshot is not shared. 
Then, I made use of a S3 repository for taking snapshots. It works well, the only problem is that when it is taking snapshots, the process blocks any updates over ES. I read something regarding this which mentioned that ES needs to take into account the details of the previous snapshots taken (figuring out the segments involved) and due to this it takes some time in creating the new one. I already have a cron job setup which deletes a week old snapshots but even after that it takes some time to create the new snapshots. 
So, I was wondering if there is a way I can move out of S3, make use of a local repository. Please not that these nodes are not having any shared mount space.
</comment><comment author="t33m" created="2016-03-11T05:56:41Z" id="195208182">Additionally, encrypted snapshots to nfs share - it's more secure. If it's impossible, we have only one way - make snapshots to local server, compress and encrypt them and  then move to nfs share.

Can you add on-the-fly encryption in AES mode to nfs share or add local snapshots type?
</comment><comment author="shaharmor" created="2016-07-19T12:23:56Z" id="233615732">Is this being worked on?
</comment><comment author="imotov" created="2016-07-20T15:22:59Z" id="233983685">I am not working on it at the moment. If somebody wants to take over, I will be glad to provide any assistance. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable explicit GC by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6637</link><project id="" key="" /><description /><key id="36651098">6637</key><summary>Disable explicit GC by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-27T10:05:34Z</created><updated>2015-06-07T13:09:07Z</updated><resolved>2014-06-27T12:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-27T10:06:20Z" id="47327633">+1
</comment><comment author="kimchy" created="2014-06-27T10:07:22Z" id="47327716">LGTM, I would love to see a bit more details on why we did it in the commit message for history sake. For example, the fact that mmap dir properly unmaps files, so no need to rely on Java calling explicit GC to clean buffers that are dangling.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make a hybrid directory default using `mmapfs` / `niofs`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6636</link><project id="" key="" /><description>`mmapfs` is really good for random access but can have sideeffects if
memory maps are large depending on the operating system etc. A hybrid
solution where only selected files are actually memory mapped but others
mostly consumed sequentially brings the best of both worlds and
minimizes the memory map impact.
This commit mmaps only the `dvd` and `tim` file for fast random access
on docvalues and term dictionaries.
</description><key id="36648485">6636</key><summary>Make a hybrid directory default using `mmapfs` / `niofs`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Store</label><label>enhancement</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-27T09:25:21Z</created><updated>2015-06-07T13:09:30Z</updated><resolved>2014-07-09T22:02:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-27T09:26:45Z" id="47324340">FYI - I think the name here can be improved... I just wanted to get the code out asap.
</comment><comment author="s1monw" created="2014-07-02T07:38:40Z" id="47745737">@rmuir @kimchy any comment or better ideas for the name?
</comment><comment author="rjernst" created="2014-07-02T18:20:50Z" id="47815057">The name `file_switch` doesn't mean anything to me.  What about something like `minimal_mmap` or `mixed_mmap_nio`?
</comment><comment author="rmuir" created="2014-07-02T19:28:55Z" id="47824173">One idea is just `default`. Its whatever we think is a good default...
</comment><comment author="kimchy" created="2014-07-02T19:29:23Z" id="47824223">I like `default`
</comment><comment author="rjernst" created="2014-07-03T00:35:16Z" id="47854332">`default` is good
</comment><comment author="dakrone" created="2014-07-03T13:17:39Z" id="47927822">I don't think `default` is a good name, what happens if we make an improvement in the future and this is no longer the default? It also makes it harder to talk about in conversation:

"what directory type are you using?"
"oh, I'm using the default"
"... what version of ES are you on?"
"I'm on Elasticsearch 0.90/1.0/1.3"
"oh, then the default is actually niofs/mmapfs/mixed"
"okay, then I'm using that"
"wait, which operating system are you using?"
"windows"
"oh, then the default is mmapfs"
"I'm using a 32-bit JDK"
"oh, then the default is simplefs, you should really stop using a 32-bit windows JVM..."

I like `mixed_mmap_nio` personally.
</comment><comment author="rmuir" created="2014-07-03T13:33:56Z" id="47929802">@dakrone but that situation already exists today, no? Even if we name it `mixed_mmap_nio`, we'd have the exact same conversation.
</comment><comment author="dakrone" created="2014-07-03T13:35:44Z" id="47929993">@rmuir yep, it totally does, I do think that naming it `default` would make it worse though. It means an extra "wait, did you set it to 'default', or do you have it unset and you're using the defaults?" question all the time.
</comment><comment author="rmuir" created="2014-07-03T14:07:44Z" id="47933969">but if our default is always `default` then there is no confusion?
</comment><comment author="dakrone" created="2014-07-03T14:11:22Z" id="47934409">Yep, there is none, so `default` is a fine name for this. If it changes in the future though, we're deferring the conversation to give this a name (if/when this changes in ES version 3.4.1 or so)
</comment><comment author="dakrone" created="2014-07-04T12:50:13Z" id="48040245">Okay, after more discussion we agreed on `default` for now.
</comment><comment author="s1monw" created="2014-07-09T12:56:49Z" id="48465859">@rmuir @rjernst @dakrone @kimchy I updated this commit - would love to get another review
</comment><comment author="kimchy" created="2014-07-09T13:14:17Z" id="48467674">LGTM, some comments were already made :)
</comment><comment author="s1monw" created="2014-07-09T14:18:20Z" id="48476785">@dakrone @kimchy added another round of changes
</comment><comment author="s1monw" created="2014-07-09T14:26:53Z" id="48477950">addressed all comments...
</comment><comment author="s1monw" created="2014-07-09T19:29:26Z" id="48522757">I pushed several new commits
</comment><comment author="s1monw" created="2014-07-09T19:29:39Z" id="48522779">I think it's ready
</comment><comment author="rmuir" created="2014-07-09T21:09:16Z" id="48534465">looks good.
</comment><comment author="kimchy" created="2014-07-09T21:18:10Z" id="48535510">+1, LGTM
</comment><comment author="uschindler" created="2014-07-16T12:08:55Z" id="49156086">There is already a similar issue in Lucene: https://issues.apache.org/jira/browse/LUCENE-1743

This one was not about random access (it did not exist at that time), but the idea is the same. A file switch by file name is more natural to me.
</comment><comment author="uschindler" created="2014-07-16T12:26:28Z" id="49157711">@s1monw 
ElasticSearch has settings "index.compound_format" and "index.compound_on_flush" (see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules.html). If this is used (not many people do, but some do because they run out of file descriptors), this would make all files use NIO only, although they contain the term dictionary or docvalues.

Maybe add "cfs" to the list of extensions?
</comment><comment author="rmuir" created="2014-07-16T12:30:19Z" id="49158092">We should not add .cfs in my opinion. if such cfs options are enabled, it means we are mmaping the whole index again, which we want to avoid (purpose of this issue). By default, the only thing using .cfs are tiny segments flushed from indexwriter. Because they are small performance is not really critical there.
</comment><comment author="uschindler" created="2014-07-16T12:58:31Z" id="49161214">@s1monw @rmuir @kimchy One cool thing for the WeakRef haters: This reduces also load to GC, because we dont create so many weak refs when cloning MMapIndexinputs: Random access inputs dont really need to be cloned all the time and on every request (no state involved, as position is not needed). Stuff like posting lists are cloned all the time, but those are now not weak ref'ed because they are read using NIO.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Randomize netty worker and connection parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6635</link><project id="" key="" /><description>Try and push our system to a state where there is only a single worker, trying to expose potential deadlocks when we by mistake execute blocking operations on the worker thread
</description><key id="36647574">6635</key><summary>Test: Randomize netty worker and connection parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-27T09:10:10Z</created><updated>2014-07-16T13:11:25Z</updated><resolved>2014-06-30T12:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-06-27T09:15:37Z" id="47323444">LGTM FWIW :yum: 
</comment><comment author="s1monw" created="2014-06-30T12:54:49Z" id="47527644">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Results count depend on arguments count</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6634</link><project id="" key="" /><description>First I just indexed 2 documents:
curl -XPUT http://192.168.0.118:9200/test/company/apple -d '{data:"Apple corp"}'
curl -XPUT http://192.168.0.118:9200/test/fruit/apple -d '{data:"Just red apple"}'

According to example above we have test index with 2 documents (both with "apple" id) in different types.
When we make curl -XGET http://192.168.0.118:9200/test/_mget?pretty -d '{ids:["apple","apple"]}'
We see this:
{
  "docs" : [ {
    "_index" : "test",
    "_type" : "fruit",
    "_id" : "apple",
    "_version" : 1,
    "found" : true, "_source" : {data:"Just red apple"}
  }, {
    "_index" : "test",
    "_type" : "fruit",
    "_id" : "apple",
    "_version" : 1,
    "found" : true, "_source" : {data:"Just red apple"}
  } ]
}

As you can see documents are duplicated.
So if we request curl -XGET http://192.168.0.118:9200/test/_mget?pretty -d '{ids:["apple","apple","apple"]}' we will see 3 documents in result set and so on.

I think the right way to resolve this issue is to ignore duplicate identificators in "ids" parameter.
</description><key id="36639724">6634</key><summary>Results count depend on arguments count</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hovsep</reporter><labels /><created>2014-06-27T06:45:58Z</created><updated>2014-06-27T07:01:28Z</updated><resolved>2014-06-27T07:01:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-27T07:01:28Z" id="47313630">Duplicate of #6633 

I think we have already answered to this in the mailing list, right? 

See https://groups.google.com/d/msgid/elasticsearch/etPan.53a19057.41b71efb.5d8%40MacBook-Air-de-David.local?utm_medium=email&amp;utm_source=footer

If the answer we gave is unclear, please answer to the mailing list thread. We will happy to help there.

Copy and pasting the original answer here:

So, what is happening here.

1st request:
You set index to test
type is not set
id is apple
It "guess" the type and send you the document from fruit type.

2nd request:
It's exactly the same request as the first one. So it gives you the same response.

To have the correct results, you should ask for:

```
GET /test/_mget?pretty
{
  "docs" : [
        {
            "_type" : "company",
            "_id" : "apple"
        },
        {
            "_type" : "fruit",
            "_id" : "apple"
        }
    ]
}
```

Or you should use unique ids for the whole index.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Same id in different types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6633</link><project id="" key="" /><description>First I just indexed 2 documents:
curl -XPUT http://192.168.0.118:9200/test/company/apple -d '{data:"Apple corp"}'
curl -XPUT http://192.168.0.118:9200/test/fruit/apple -d '{data:"Just red apple"}'

As you can see, they have same id and placed in same index,but different types.
Ok, next I perform multi get request:

curl -XGET http://192.168.0.118:9200/test/_mget?pretty -d '{ids:["apple"]}'

Response:

{
  "docs" : [ {
    "_index" : "test",
    "_type" : "fruit",
    "_id" : "apple",
    "_version" : 1,
    "found" : true, "_source" : {data:"Just red apple"}
  } ]
}

As you can see only one document returned. Why not both? I find such behavior very ambiguous. I think if we request whole index, so all documents with given id should be returned regardless of type.
</description><key id="36639658">6633</key><summary>Same id in different types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hovsep</reporter><labels /><created>2014-06-27T06:44:04Z</created><updated>2014-07-01T21:57:40Z</updated><resolved>2014-06-27T07:00:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-27T07:00:56Z" id="47313605">I think we have already answered to this in the mailing list, right? 

See https://groups.google.com/d/msgid/elasticsearch/etPan.53a19057.41b71efb.5d8%40MacBook-Air-de-David.local?utm_medium=email&amp;utm_source=footer

If the answer we gave is unclear, please answer to the mailing list thread. We will happy to help there.

Copy and pasting the original answer here:

So, what is happening here.

1st request:
You set index to test
type is not set
id is apple
It "guess" the type and send you the document from fruit type.

2nd request:
It's exactly the same request as the first one. So it gives you the same response.

To have the correct results, you should ask for:

```
GET /test/_mget?pretty
{
  "docs" : [
        {
            "_type" : "company",
            "_id" : "apple"
        },
        {
            "_type" : "fruit",
            "_id" : "apple"
        }
    ]
}
```

Or you should use unique ids for the whole index.
</comment><comment author="kimchy" created="2014-06-27T09:56:24Z" id="47326752">btw, agreed on it being confusing. The trickiness here is the support for "any type" in a single get request, which returns one of the docs (this was a request raised by the users when get was introduced), and how multi get works, which is just execute each inner get request using the already built in get API (which has the mentioned behavior).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix source excludes setting if no includes were provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6632</link><project id="" key="" /><description>Line 524 uses "include == null" to check if the exclude string is null.

I don't know why this had to be a one-liner, but I don't like using ternary operators, particularly twice (!) on the same line. Makes it harder to read and can lead to bugs like this.

Affects v1.2.1

Cheers!
</description><key id="36599857">6632</key><summary>Fix source excludes setting if no includes were provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jnaous</reporter><labels><label>:Java API</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T18:11:07Z</created><updated>2015-06-07T19:36:10Z</updated><resolved>2014-07-02T09:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make sure we don't reuse arrays when sending an error back</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6631</link><project id="" key="" /><description>We want to make sure recycling will not fail for any reason while trying to send a response back that is caused by a failure, for example, if we have circuit breaker on it (at one point), sending an error back will not be affected by it.
</description><key id="36587800">6631</key><summary>Make sure we don't reuse arrays when sending an error back</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T15:39:24Z</created><updated>2015-06-07T13:09:58Z</updated><resolved>2014-06-27T09:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-27T07:22:43Z" id="47314852">LGTM. 
</comment><comment author="bleskes" created="2014-06-27T07:26:43Z" id="47315124">Do want to put this into 1.2.2?
</comment><comment author="kimchy" created="2014-06-27T07:36:43Z" id="47315854">@bleskes I am good with 1.2.2, @s1monw thoughts?
</comment><comment author="s1monw" created="2014-06-27T09:06:00Z" id="47322668">LGTM &amp; +1 for `1.2.2`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk API: Fix return of wrong request type on failed updates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6630</link><project id="" key="" /><description>Opening an issue as suggested by @dadoonet after the following discussion on the mailing list: https://groups.google.com/forum/#!topic/elasticsearch/HAA4Y8Qziqg

When a bulk update action fails, an "index" entry can be returned in the response.

Example bulk commands list with an update command failing because the second date can't be parsed:

``` json
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } } 
{ "title" : "Great Title of doc 1" } 
{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "2" } } 
{ "title" : "Great Title of doc 2" } 
{ "update" : { "_index" : "test", "_type" : "type1", "_id" : "1" } } 
{ "doc" : { "date" : "2014-04-30T23:59:57" }} 
{ "update" : { "_index" : "test", "_type" : "type1", "_id" : "2" } } 
{ "doc" : { "date" : "2014-04-31T00:00:01" }} 
{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "1" } } 
{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "2" } }
```

Here is the actual response (elasticsearch v1.1):

``` json
{
  "took" : 4, 
  "errors" : true, 
  "items" : [ { 
    "index" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "1", 
      "_version" : 8, 
      "status" : 201 
    } 
  }, { 
    "index" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "2", 
      "_version" : 5, 
      "status" : 201 
    } 
  }, { 
    "update" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "1", 
      "_version" : 9, 
      "status" : 200 
    } 
  }, { 
    "index" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "2", 
      "status" : 400, 
      "error" : "MapperParsingException[failed to parse [date]]; nested: MapperParsingException[failed to parse date field [2014-04-31T00:00:01], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalFieldValueException[Cannot parse \"2014-04-31T00:00:01\": Value 31 for dayOfMonth must be in the range [1,30]]; " 
    } 
  }, { 
    "delete" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "1", 
      "_version" : 10, 
      "status" : 200, 
      "found" : true 
    } 
  }, { 
    "delete" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "2", 
      "_version" : 6, 
      "status" : 200, 
      "found" : true 
    } 
  } ] 
}
```

The problem here concerns the bulk update response for the forth item. It's key is `index` while it should be `update`, as in the following hand edited response:

``` json
{
  "took" : 4, 
  "errors" : true, 
  "items" : [ { 
    "index" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "1", 
      "_version" : 8, 
      "status" : 201 
    } 
  }, { 
    "index" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "2", 
      "_version" : 5, 
      "status" : 201 
    } 
  }, { 
    "update" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "1", 
      "_version" : 9, 
      "status" : 200 
    } 
  }, { 
    "update" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "2", 
      "status" : 400, 
      "error" : "MapperParsingException[failed to parse [date]]; nested: MapperParsingException[failed to parse date field [2014-04-31T00:00:01], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalFieldValueException[Cannot parse \"2014-04-31T00:00:01\": Value 31 for dayOfMonth must be in the range [1,30]]; " 
    } 
  }, { 
    "delete" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "1", 
      "_version" : 10, 
      "status" : 200, 
      "found" : true 
    } 
  }, { 
    "delete" : { 
      "_index" : "test", 
      "_type" : "type1", 
      "_id" : "2", 
      "_version" : 6, 
      "status" : 200, 
      "found" : true 
    } 
  } ] 
}
```

I didn't do a lot of code reading but a possible starting point for investigations is there: https://github.com/elasticsearch/elasticsearch/blob/1.1/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java#L317
</description><key id="36571458">6630</key><summary>Bulk API: Fix return of wrong request type on failed updates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">tuxnco</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T12:36:02Z</created><updated>2015-06-07T19:36:22Z</updated><resolved>2014-07-02T10:42:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Thread pool rejection status code should be 429</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6629</link><project id="" key="" /><description>Thread rejection should return too many requests status code, and not 503, which is used to also show that the cluster is not available
 relates to #6627, but only for rejections for now
</description><key id="36570189">6629</key><summary>Thread pool rejection status code should be 429</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Exceptions</label><label>breaking</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T12:16:58Z</created><updated>2015-06-06T16:54:12Z</updated><resolved>2014-06-27T09:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-27T07:32:02Z" id="47315531">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Wrap groovy script exceptions in a serializable Exception object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6628</link><project id="" key="" /><description>Fixes #6598

It prevents ES from trying to serialize the default Groovy exceptions, which want to carry over a lot of state that doesn't  serialize properly.
</description><key id="36564078">6628</key><summary>Scripting: Wrap groovy script exceptions in a serializable Exception object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T10:37:41Z</created><updated>2015-06-07T19:37:15Z</updated><resolved>2014-06-30T14:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-30T12:54:15Z" id="47527603">@kimchy I've added logging and catching a more generic exception to this PR.

I also discovered that compilation exceptions don't serialize properly either, so I've done the same thing if an exception occurs during compilation.
</comment><comment author="kimchy" created="2014-06-30T13:16:11Z" id="47529720">@dakrone added a few more comments
</comment><comment author="dakrone" created="2014-06-30T13:40:12Z" id="47532237">@kimchy thanks! I've addressed all of the feedback except for catching Throwable, instead I used `catch (Exception|AssertionError e) { ...` (and added a test for it) so it doesn't swallow an `OutOfMemoryError`. But I'm happy to change it if you think it would be better to catch that also.
</comment><comment author="kimchy" created="2014-06-30T14:04:40Z" id="47535055">LGTM, lets do the script name (since its tricky) in another change ++
</comment><comment author="dakrone" created="2014-06-30T14:07:14Z" id="47535357">Opened #6653 for this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API: Replace error code 503 with 429 when appropriate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6627</link><project id="" key="" /><description>Right now Elasticsearch returns 503 on a couple of occasions, where the full cluster is actually not completely out of service, but a single node just has a problem, so that trying another node makes sense. On the other hand there are error messages, where telling the client to wait a bit makes more sense in order to reduce load (when the threadpool queue is full for example).

There is a HTTP status code for that, called 429 (Too many requests).
It can be found in RFC 6585 (Additional HTTP status codes), see https://tools.ietf.org/html/rfc6585

Closes #4066
</description><key id="36563823">6627</key><summary>REST API: Replace error code 503 with 429 when appropriate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T10:33:49Z</created><updated>2015-06-07T16:22:07Z</updated><resolved>2014-06-30T12:51:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-30T11:35:26Z" id="47521431">I went thought all the places that return 503, and I think we are good except for FlushNotAllowed failure, which is a transient state where flush is not allowed on a shard that is being recovered from, unsure which code we should use there. We use 503 also when there are no shards to satisfy a search request, I think its appropriate.
</comment><comment author="spinscale" created="2014-06-30T12:51:20Z" id="47527333">closing this for now, the too many requests error does not make sense for the flush allowed failure IMO
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Test that dynamically changing throttling settings works</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6626</link><project id="" key="" /><description>Not sure there is really an issue here, but maybe ...

The code goes through a number of layers when the application tries to either turn throttling on/off or change the rate, and it's not clear if dynamic updating is really working?
</description><key id="36560921">6626</key><summary>Test: Test that dynamically changing throttling settings works</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>non-issue</label></labels><created>2014-06-26T09:51:00Z</created><updated>2014-07-16T13:14:09Z</updated><resolved>2014-07-14T12:49:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-27T09:20:43Z" id="47323863">@mikemccand maybe we can start with a test :)
</comment><comment author="mikemccand" created="2014-06-27T22:39:49Z" id="47407746">@s1monw good idea :)
</comment><comment author="mikemccand" created="2014-06-28T09:02:36Z" id="47422203">Hmm this is tricky to test.  For example, with dynamic updating of throttling "working", it won't affect already opened files by a current merge, so e.g. a test that does indexing, then checks node stats for throttle time, then dynamically shuts off throttling, then checks stats for throttle time again to make sure it didn't increase, must be careful to stop all merges in between.
</comment><comment author="mikemccand" created="2014-07-14T12:49:43Z" id="48895508">Fixed with #6842
</comment><comment author="s1monw" created="2014-07-14T12:55:52Z" id="48896061">@mikemccand was there actually a bug or not? afaik there is no bug and we should re-label the issue saying `noissue`?
</comment><comment author="mikemccand" created="2014-07-14T14:22:57Z" id="48905476">Sorry, yes, there was no bug ... I'll relabel.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrap RateLimiter rather than copy RateLimitedIndexOutput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6625</link><project id="" key="" /><description>We clone RateLimitedIndexOutput from lucene just to collect pausing
statistics we can do this in a more straight forward way in a delegating
RateLimiter.
</description><key id="36559450">6625</key><summary>Wrap RateLimiter rather than copy RateLimitedIndexOutput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T09:29:20Z</created><updated>2015-06-07T13:10:13Z</updated><resolved>2014-06-27T09:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-27T09:31:52Z" id="47324734">LGTM nice cleanup
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Change es.node.mode default for tests to `local`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6624</link><project id="" key="" /><description>In order to speed up test execution we should run in local mode by
default. CI builds will still use network builds all the time.
</description><key id="36558339">6624</key><summary>Test: Change es.node.mode default for tests to `local`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T09:12:30Z</created><updated>2014-07-16T13:14:37Z</updated><resolved>2014-06-27T09:59:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-26T09:15:26Z" id="47204073">@s1monw can you update TESTING.asciidoc to mention that `local` is now the default?
</comment><comment author="s1monw" created="2014-06-27T09:32:31Z" id="47324768">@dakrone done
</comment><comment author="dakrone" created="2014-06-27T09:36:18Z" id="47325054">Left an extremely minor comment, but this LGTM regardless.
</comment><comment author="s1monw" created="2014-06-27T09:56:21Z" id="47326748">@dakrone fixed, wanna take another look?
</comment><comment author="dakrone" created="2014-06-27T09:57:53Z" id="47326927">LTGM again, thanks for adding the helper method!
</comment><comment author="s1monw" created="2014-06-27T09:58:12Z" id="47326954">YW! thx for the review
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6623</link><project id="" key="" /><description /><key id="36557663">6623</key><summary>Upgrade to Lucene 4.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Core</label><label>upgrade</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-26T09:01:42Z</created><updated>2015-10-06T09:40:00Z</updated><resolved>2014-06-26T12:25:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-26T10:46:18Z" id="47211666">I left one comment, I think it looks good otherwise
</comment><comment author="jpountz" created="2014-06-26T12:15:42Z" id="47218217">LGTM
</comment><comment author="martijnvg" created="2014-06-26T12:16:14Z" id="47218250">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move `RamDirectoryService` to the test package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6622</link><project id="" key="" /><description>We have `RamDirectoryService` in our core code that allows you to use lucene's `RAMDirectory` I think this is an awful trap and we should only use it for certain unit test.
</description><key id="36555810">6622</key><summary>Move `RamDirectoryService` to the test package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2014-06-26T08:33:50Z</created><updated>2015-03-17T22:53:46Z</updated><resolved>2015-03-17T22:53:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-09T10:57:25Z" id="48455913">labeled as `1.4` for now we can still move back if we make it
</comment><comment author="s1monw" created="2015-03-17T22:53:46Z" id="82632063">this has been removed in 2.0 already so I am just closing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename optimize to forceMerge</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6621</link><project id="" key="" /><description>Optimize is a very costly operation, and rarely necessary.  I think it's trappy because it's name is so tempting, and also because in ES the optimize is done in the background (so the request returns immediately), hiding the true cost.

There are times when it's appropriate, e.g. in the logging use case when you know a given daily index is done, but for most cases it's not a good idea to optimize.

I think we should follow Lucene here and rename it to forceMerge and deprecate optimize.
</description><key id="36545888">6621</key><summary>Rename optimize to forceMerge</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-06-26T04:58:23Z</created><updated>2014-07-16T13:15:09Z</updated><resolved>2014-06-28T08:58:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-26T08:18:23Z" id="47199267">+1 I think we should add a new API endpoint for this and deprecate the current on in 1.3. I don't necessarily think we need to follow the lucene naming. A lot of folks know `_compact` as an operation that corresponds to this. so lets think about what makes most sense..
</comment><comment author="kimchy" created="2014-06-26T09:51:58Z" id="47207329">+1, I don't like the forceMerge name, I like compact. This APIs actually does do optimizations though :), less segments means faster searches, less memory used, but it is costly to run. Note, by default, we do wait till the optimize request is done and only then the API returns.
</comment><comment author="jpountz" created="2014-06-26T12:32:32Z" id="47219595">If we don't call it either `merge` or `forge_merge` I'm afraid that we will keep on referring to it with several names, which is confusing.
</comment><comment author="mikemccand" created="2014-06-26T12:47:05Z" id="47220866">&gt; Note, by default, we do wait till the optimize request is done and only then the API returns.

Ahh OK my mistake!  That's good.

Hmm, I don't like the name "compact".  Or, rather, I think it sounds just "as good" as optimize.  I prefer a name which sounds more low level / advanced to decrease how frequently users just call it because it sounds like they should.  Rarely is it really warranted.
</comment><comment author="mikemccand" created="2014-06-28T08:58:43Z" id="47422132">I talked to Shay a bit about this, and his feeling is ES users don't generally fall into the trap of calling optimize when they shouldn't, and so there's not really a problem here to fix.  And also it looks like we can't agree on a name (naming is the hardest part!) :)

So I'm closing this w/o changing anything.  We can always reopen the issue if we see users falling into this dangerous trap.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Untarring Elasticsearch on top of running instance causes segfault</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6620</link><project id="" key="" /><description>Hey there - I was using a chef recipe to install Elasticsearch.  My logic was originally to extract the tar into my root folder, regardless of what was there (blowing away an existing installation if necessary).  Then if the untar succeeds, I try to restart the Elasticsearch service.

This was when I noticed that untarring was causing the existing Java Elasticsearch process to consistently experience an uncaught segfault.  This is with Sun Java 7u25 and ES 1.0.1.

I changed my logic to first shutdown any running service before untarring, so I got around the segfault.  But this doesn't seem like normal behavior.
</description><key id="36534375">6620</key><summary>Untarring Elasticsearch on top of running instance causes segfault</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tyler-ball</reporter><labels /><created>2014-06-26T00:12:16Z</created><updated>2014-06-30T12:26:08Z</updated><resolved>2014-06-30T12:26:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tyler-ball" created="2014-06-26T00:16:08Z" id="47174893">This is the dump that Java gave me:

```
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x000000000000973e, pid=31076, tid=140270049507072
#
# JRE version: 7.0_17-b02
# Java VM: Java HotSpot(TM) 64-Bit Server VM (23.7-b01 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# C  0x000000000000973e
[error occurred during error reporting (printing problematic frame), id 0xb]

# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try "ulimit -c unlimited" before starting Java again
#
# If you would like to submit a bug report, please visit:
#   http://bugreport.sun.com/bugreport/crash.jsp
# The crash happened outside the Java Virtual Machine in native code.
# See problematic frame for where to report the bug.
#

---------------  T H R E A D  ---------------

Current thread (0x00007f9334083000):  JavaThread "elasticsearch[stgesmas03][management][T#1]" daemon [_thread_in_native, id=31195, stack(0x00007f932a753000,0x00007f932a794000)]

siginfo:si_signo=SIGSEGV: si_errno=0, si_code=1 (SEGV_MAPERR), si_addr=0x000000000000973e

Registers:
RAX=0x00007f935871ca50, RBX=0x00007f93340831d8, RCX=0x00007f935871ca50, RDX=0x0000000000000000
RSP=0x00007f932a792128, RBP=0x00007f935871ca50, RSI=0x00007f932a792130, RDI=0x00007f93587237d0
R8 =0x00007f932a7920a0, R9 =0x00000000bc8c45e0, R10=0x0000000000000180, R11=0x00007f935d5550e8
R12=0x0000000000000000, R13=0x00000000fbf30aa8, R14=0x00007f932a7921e0, R15=0x00007f9334083000
RIP=0x000000000000973e, EFLAGS=0x0000000000010206, CSGSFS=0x6553000000000033, ERR=0x0000000000000014
  TRAPNO=0x000000000000000e

Top of Stack: (sp=0x00007f932a792128)
0x00007f932a792128:   00007f932b220d04 00000000fae1fc20
0x00007f932a792138:   0000000000000000 00007f932a792170
0x00007f932a792148:   00007f935d61d95a 00000000fbf30ab0
0x00007f932a792158:   00007f932a7921c0 0000000000000000
0x00007f932a792168:   00007f93549bdf90 0000000041800000
0x00007f932a792178:   00007f93549be4a1 00007f932a792180
0x00007f932a792188:   0000000000000000 00007f932a7921e0
0x00007f932a792198:   00000000fbf386a8 0000000000000000
0x00007f932a7921a8:   00000000fbf30ab0 0000000000000000
0x00007f932a7921b8:   00007f932a7921e0 00007f932a792230
0x00007f932a7921c8:   00007f93549b2333 0000000000000000
0x00007f932a7921d8:   00007f93549baddb 00000000bc8c45e0
0x00007f932a7921e8:   00000000b952a090 00007f932a7921f0
0x00007f932a7921f8:   00000000fbf45209 00007f932a792258
0x00007f932a792208:   00000000fc368e78 0000000000000000
0x00007f932a792218:   00000000fbf453b8 00007f932a7921e0
0x00007f932a792228:   00007f932a792258 00007f932a7922b8
0x00007f932a792238:   00007f93549b29e1 0000000000000000
0x00007f932a792248:   00000000b952a090 00000000bc8c45e0
0x00007f932a792258:   00000000bc8c6eb8 00000000bc8c6c70
0x00007f932a792268:   0000000000000000 00000000bc8c6c70
0x00007f932a792278:   00007f932a792268 00000000fbf49cb3
0x00007f932a792288:   00007f932a7922c8 00000000fc1c64f8
0x00007f932a792298:   0000000000000000 00000000fbf49cd8
0x00007f932a7922a8:   00007f932a792258 00007f932a7922c8
0x00007f932a7922b8:   00007f932a792340 00007f93549b2333
0x00007f932a7922c8:   00000000bc8c6c70 00000000b952a040
0x00007f932a7922d8:   00000146d520fc38 00007f93549badd7
0x00007f932a7922e8:   00000000b46c88d0 00000000b9529b80
0x00007f932a7922f8:   00000000b9529b80 00007f932a792300
0x00007f932a792308:   00000000fbadb031 00007f932a7923a0
0x00007f932a792318:   00000000fbfd4ce0 0000000000000000 

Instructions: (pc=0x000000000000973e)
0x000000000000971e:   
[error occurred during error reporting (printing registers, top of stack, instructions near pc), id 0xb]

Register to memory mapping:

RAX=0x00007f935871ca50 is an unknown value
RBX=0x00007f93340831d8 is an unknown value
RCX=0x00007f935871ca50 is an unknown value
RDX=0x0000000000000000 is an unknown value
RSP=0x00007f932a792128 is pointing into the stack for thread: 0x00007f9334083000
RBP=0x00007f935871ca50 is an unknown value
RSI=0x00007f932a792130 is pointing into the stack for thread: 0x00007f9334083000
RDI=0x00007f93587237d0 is an unknown value
R8 =0x00007f932a7920a0 is pointing into the stack for thread: 0x00007f9334083000
R9 =0x00000000bc8c45e0 is an oop
org.hyperic.sigar.Sigar 
 - klass: 'org/hyperic/sigar/Sigar'
R10=0x0000000000000180 is an unknown value
R11=0x00007f935d5550e8: &lt;offset 0xcfa0e8&gt; in /usr/java/jdk1.7.0_17/jre/lib/amd64/server/libjvm.so at 0x00007f935c85b000
R12=0x0000000000000000 is an unknown value
R13=0x00000000fbf30aa8 is an oop
{constMethod} 
 - klass: {other class}
 - method:       0x00000000fbf30ab0 {method} 'getLoadAverage' '()[D' in 'org/hyperic/sigar/Sigar'
 - exceptions:   0x00000000fae01d60
R14=0x00007f932a7921e0 is pointing into the stack for thread: 0x00007f9334083000
R15=0x00007f9334083000 is a thread


Stack: [0x00007f932a753000,0x00007f932a794000],  sp=0x00007f932a792128,  free space=252k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
C  0x000000000000973e
[error occurred during error reporting (printing native stack), id 0xb]

Java frames: (J=compiled Java code, j=interpreted, Vv=VM code)
j  org.hyperic.sigar.Sigar.getLoadAverage()[D+0
j  org.elasticsearch.monitor.os.SigarOsProbe.osStats()Lorg/elasticsearch/monitor/os/OsStats;+25
j  org.elasticsearch.monitor.os.OsService.stats()Lorg/elasticsearch/monitor/os/OsStats;+27
j  org.elasticsearch.node.service.NodeService.stats(Lorg/elasticsearch/action/admin/indices/stats/CommonStatsFlags;ZZZZZZZZZ)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats;+49
j  org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(Lorg/elasticsearch/action/admin/cluster/node/stats/TransportNodesStatsAction$NodeStatsRequest;)Lorg/elasticsearch/action/admin/cluster/node/stats/NodeStats;+49
j  org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(Lorg/elasticsearch/action/support/nodes/NodeOperationRequest;)Lorg/elasticsearch/action/support/nodes/NodeOperationResponse;+5
j  org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(Lorg/elasticsearch/action/support/nodes/NodeOperationRequest;Lorg/elasticsearch/transport/TransportChannel;)V+6
j  org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(Lorg/elasticsearch/transport/TransportRequest;Lorg/elasticsearch/transport/TransportChannel;)V+6
j  org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run()V+12
j  java.util.concurrent.ThreadPoolExecutor.runWorker(Ljava/util/concurrent/ThreadPoolExecutor$Worker;)V+95
j  java.util.concurrent.ThreadPoolExecutor$Worker.run()V+5
j  java.lang.Thread.run()V+11
v  ~StubRoutines::call_stub

---------------  P R O C E S S  ---------------

Java Threads: ( =&gt; current thread )
=&gt;0x00007f9334083000 JavaThread "elasticsearch[stgesmas03][management][T#1]" daemon [_thread_in_native, id=31195, stack(0x00007f932a753000,0x00007f932a794000)]
  0x00007f9358008800 JavaThread "DestroyJavaVM" [_thread_blocked, id=31085, stack(0x00007f935e11f000,0x00007f935e160000)]
  0x00007f93587bf000 JavaThread "elasticsearch[keepAlive/1.0.1]" [_thread_blocked, id=31194, stack(0x00007f932a794000,0x00007f932a7d5000)]
  0x00007f93587bc000 JavaThread "elasticsearch[stgesmas03][http_server_boss][T#1]{New I/O server boss #15}" daemon [_thread_in_native, id=31193, stack(0x00007f932a7d5000,0x00007f932a816000)]
  0x00007f93587ba000 JavaThread "elasticsearch[stgesmas03][http_server_worker][T#4]{New I/O worker #14}" daemon [_thread_in_native, id=31192, stack(0x00007f932a816000,0x00007f932a857000)]
  0x00007f93587b9000 JavaThread "elasticsearch[stgesmas03][http_server_worker][T#3]{New I/O worker #13}" daemon [_thread_in_native, id=31191, stack(0x00007f932a857000,0x00007f932a898000)]
  0x00007f9358c86800 JavaThread "elasticsearch[stgesmas03][http_server_worker][T#2]{New I/O worker #12}" daemon [_thread_in_native, id=31190, stack(0x00007f932a898000,0x00007f932a8d9000)]
  0x00007f9358c85800 JavaThread "elasticsearch[stgesmas03][http_server_worker][T#1]{New I/O worker #11}" daemon [_thread_in_native, id=31189, stack(0x00007f932aa1e000,0x00007f932aa5f000)]
  0x00007f9334081800 JavaThread "elasticsearch[stgesmas03][clusterService#updateTask][T#1]" daemon [_thread_blocked, id=31169, stack(0x00007f932a9dd000,0x00007f932aa1e000)]
  0x00007f933814d800 JavaThread "elasticsearch[stgesmas03][transport_client_timer][T#1]{Hashed wheel timer #1}" daemon [_thread_blocked, id=31163, stack(0x00007f932a95b000,0x00007f932a99c000)]
  0x00007f9358c7d000 JavaThread "elasticsearch[stgesmas03][generic][T#1]" daemon [_thread_blocked, id=31159, stack(0x00007f932aa5f000,0x00007f932aaa0000)]
  0x00007f9358c4f000 JavaThread "elasticsearch[stgesmas03][transport_server_boss][T#1]{New I/O server boss #10}" daemon [_thread_in_native, id=31158, stack(0x00007f932aaa0000,0x00007f932aae1000)]
  0x00007f9358c3d000 JavaThread "elasticsearch[stgesmas03][transport_server_worker][T#4]{New I/O worker #9}" daemon [_thread_in_native, id=31157, stack(0x00007f932aae1000,0x00007f932ab22000)]
  0x00007f9358b87800 JavaThread "elasticsearch[stgesmas03][transport_server_worker][T#3]{New I/O worker #8}" daemon [_thread_in_native, id=31156, stack(0x00007f932ab22000,0x00007f932ab63000)]
  0x00007f9358b86800 JavaThread "elasticsearch[stgesmas03][transport_server_worker][T#2]{New I/O worker #7}" daemon [_thread_in_native, id=31155, stack(0x00007f932ab63000,0x00007f932aba4000)]
  0x00007f9358ba5000 JavaThread "elasticsearch[stgesmas03][transport_server_worker][T#1]{New I/O worker #6}" daemon [_thread_in_native, id=31154, stack(0x00007f932aba4000,0x00007f932abe5000)]
  0x00007f9358b81800 JavaThread "elasticsearch[stgesmas03][transport_client_boss][T#1]{New I/O boss #5}" daemon [_thread_in_native, id=31153, stack(0x00007f932abe5000,0x00007f932ac26000)]
  0x00007f9358ad4000 JavaThread "elasticsearch[stgesmas03][transport_client_worker][T#4]{New I/O worker #4}" daemon [_thread_in_native, id=31152, stack(0x00007f932ac26000,0x00007f932ac67000)]
  0x00007f9358ad2000 JavaThread "elasticsearch[stgesmas03][transport_client_worker][T#3]{New I/O worker #3}" daemon [_thread_in_native, id=31151, stack(0x00007f932ac67000,0x00007f932aca8000)]
  0x00007f9358ad5800 JavaThread "elasticsearch[stgesmas03][transport_client_worker][T#2]{New I/O worker #2}" daemon [_thread_in_native, id=31150, stack(0x00007f932aca8000,0x00007f932ace9000)]
  0x00007f9358ad5000 JavaThread "elasticsearch[stgesmas03][transport_client_worker][T#1]{New I/O worker #1}" daemon [_thread_in_native, id=31149, stack(0x00007f932ace9000,0x00007f932ad2a000)]
  0x00007f9358ac0800 JavaThread "elasticsearch[stgesmas03][[ttl_expire]]" daemon [_thread_blocked, id=31148, stack(0x00007f932ad2a000,0x00007f932ad6b000)]
  0x00007f9358a49000 JavaThread "elasticsearch[stgesmas03][scheduler][T#1]" daemon [_thread_blocked, id=31147, stack(0x00007f932ad6b000,0x00007f932adac000)]
  0x00007f93588c1800 JavaThread "elasticsearch[stgesmas03][[timer]]" daemon [_thread_blocked, id=31125, stack(0x00007f932b1cc000,0x00007f932b20d000)]
  0x00007f93582f5800 JavaThread "RMI TCP Accept-0" daemon [_thread_in_native, id=31099, stack(0x00007f932b55b000,0x00007f932b59c000)]
  0x00007f93582df800 JavaThread "RMI TCP Accept-9201" daemon [_thread_in_native, id=31098, stack(0x00007f932b59c000,0x00007f932b5dd000)]
  0x00007f93582ba000 JavaThread "RMI TCP Accept-0" daemon [_thread_in_native, id=31097, stack(0x00007f9350012000,0x00007f9350053000)]
  0x00007f935817a800 JavaThread "Service Thread" daemon [_thread_blocked, id=31096, stack(0x00007f9350097000,0x00007f93500d8000)]
  0x00007f9358177800 JavaThread "C2 CompilerThread1" daemon [_thread_blocked, id=31095, stack(0x00007f93500d8000,0x00007f93501d9000)]
  0x00007f9358175800 JavaThread "C2 CompilerThread0" daemon [_thread_blocked, id=31094, stack(0x00007f93501d9000,0x00007f93502da000)]
  0x00007f9358173800 JavaThread "Signal Dispatcher" daemon [_thread_blocked, id=31093, stack(0x00007f93502da000,0x00007f935031b000)]
  0x00007f9358171000 JavaThread "Surrogate Locker Thread (Concurrent GC)" daemon [_thread_blocked, id=31092, stack(0x00007f935c03e000,0x00007f935c07f000)]
  0x00007f9358125800 JavaThread "Finalizer" daemon [_thread_blocked, id=31091, stack(0x00007f935c07f000,0x00007f935c0c0000)]
  0x00007f9358123800 JavaThread "Reference Handler" daemon [_thread_blocked, id=31090, stack(0x00007f935dfa9000,0x00007f935dfea000)]

Other Threads:
  0x00007f935811c000 VMThread [stack: 0x00007f935031b000,0x00007f935041c000] [id=31089]
  0x00007f93582f8000 WatcherThread [stack: 0x00007f932b45a000,0x00007f932b55b000] [id=31100]

VM state:not at safepoint (normal execution)

VM Mutex/Monitor currently owned by a thread: None

Heap
 par new generation   total 153344K, used 122088K [0x00000000b3000000, 0x00000000bd660000, 0x00000000bd660000)
  eden space 136320K,  77% used [0x00000000b3000000, 0x00000000b969a348, 0x00000000bb520000)
  from space 17024K,  99% used [0x00000000bc5c0000, 0x00000000bd65fff8, 0x00000000bd660000)
  to   space 17024K,   0% used [0x00000000bb520000, 0x00000000bb520000, 0x00000000bc5c0000)
 concurrent mark-sweep generation total 1006208K, used 14940K [0x00000000bd660000, 0x00000000fad00000, 0x00000000fae00000)
 concurrent-mark-sweep perm gen total 29120K, used 28962K [0x00000000fae00000, 0x00000000fca70000, 0x0000000100000000)

Card table byte_map: [0x00007f9354642000,0x00007f93548ab000] byte_map_base: 0x00007f93540aa000

Polling page: 0x00007f935e169000

Code Cache  [0x00007f93549ac000, 0x00007f9354c1c000, 0x00007f93579ac000)
 total_blobs=823 nmethods=448 adapters=327 free_code_cache=47662Kb largest_free_block=48765824

Compilation events (10 events):
Event: 108.794 Thread 0x00007f9358177800  450             java.util.Arrays::fill (21 bytes)
Event: 108.795 Thread 0x00007f9358177800 nmethod 450 0x00007f9354ab3cd0 code [0x00007f9354ab3e00, 0x00007f9354ab3e98]
Event: 114.456 Thread 0x00007f9358175800  451             org.elasticsearch.common.netty.handler.codec.http.DefaultHttpHeaders::hash (68 bytes)
Event: 114.461 Thread 0x00007f9358175800 nmethod 451 0x00007f9354b19710 code [0x00007f9354b19860, 0x00007f9354b19b88]
Event: 140.489 Thread 0x00007f9358177800  453             org.elasticsearch.common.jackson.core.json.UTF8JsonGenerator::_writeStringSegment (120 bytes)
Event: 140.496 Thread 0x00007f9358177800 nmethod 453 0x00007f9354afe710 code [0x00007f9354afe880, 0x00007f9354afec98]
Event: 140.664 Thread 0x00007f9358175800  454             org.elasticsearch.common.util.concurrent.jsr166y.LinkedTransferQueue::awaitMatch (197 bytes)
Event: 140.670 Thread 0x00007f9358175800 nmethod 454 0x00007f9354b1b350 code [0x00007f9354b1b500, 0x00007f9354b1b9a0]
Event: 176.251 Thread 0x00007f9358177800  455             org.elasticsearch.common.netty.util.internal.ConcurrentIdentityHashMap$HashIterator::advance (141 bytes)
Event: 176.263 Thread 0x00007f9358177800 nmethod 455 0x00007f9354afe110 code [0x00007f9354afe260, 0x00007f9354afe538]

GC Heap History (2 events):
Event: 8.707 GC heap before
{Heap before GC invocations=0 (full 0):
 par new generation   total 153344K, used 136320K [0x00000000b3000000, 0x00000000bd660000, 0x00000000bd660000)
  eden space 136320K, 100% used [0x00000000b3000000, 0x00000000bb520000, 0x00000000bb520000)
  from space 17024K,   0% used [0x00000000bb520000, 0x00000000bb520000, 0x00000000bc5c0000)
  to   space 17024K,   0% used [0x00000000bc5c0000, 0x00000000bc5c0000, 0x00000000bd660000)
 concurrent mark-sweep generation total 1006208K, used 0K [0x00000000bd660000, 0x00000000fad00000, 0x00000000fae00000)
 concurrent-mark-sweep perm gen total 26816K, used 26648K [0x00000000fae00000, 0x00000000fc830000, 0x0000000100000000)
Event: 8.760 GC heap after
Heap after GC invocations=1 (full 0):
 par new generation   total 153344K, used 17023K [0x00000000b3000000, 0x00000000bd660000, 0x00000000bd660000)
  eden space 136320K,   0% used [0x00000000b3000000, 0x00000000b3000000, 0x00000000bb520000)
  from space 17024K,  99% used [0x00000000bc5c0000, 0x00000000bd65fff8, 0x00000000bd660000)
  to   space 17024K,   0% used [0x00000000bb520000, 0x00000000bb520000, 0x00000000bc5c0000)
 concurrent mark-sweep generation total 1006208K, used 14940K [0x00000000bd660000, 0x00000000fad00000, 0x00000000fae00000)
 concurrent-mark-sweep perm gen total 26816K, used 26648K [0x00000000fae00000, 0x00000000fc830000, 0x0000000100000000)
}

Deoptimization events (10 events):
Event: 12.754 Thread 0x00007f9358b81800 Uncommon trap -42 fr.pc 0x00007f9354acfe9c
Event: 12.754 Thread 0x00007f9358b81800 Uncommon trap -42 fr.pc 0x00007f9354acfe9c
Event: 12.786 Thread 0x00007f9334081800 Uncommon trap -34 fr.pc 0x00007f9354a8a3c0
Event: 12.786 Thread 0x00007f9334081800 Uncommon trap -34 fr.pc 0x00007f9354a8a3c0
Event: 12.786 Thread 0x00007f9334081800 Uncommon trap -34 fr.pc 0x00007f9354a8a3c0
Event: 12.786 Thread 0x00007f9334081800 Uncommon trap -34 fr.pc 0x00007f9354a8a3c0
Event: 12.836 Thread 0x00007f9334081800 Uncommon trap -42 fr.pc 0x00007f9354acfe9c
Event: 12.972 Thread 0x00007f9334081800 Uncommon trap -83 fr.pc 0x00007f9354ab6528
Event: 13.319 Thread 0x00007f9358b86800 Uncommon trap -12 fr.pc 0x00007f9354ae5080
Event: 162.674 Thread 0x00007f9358c85800 Uncommon trap -83 fr.pc 0x00007f9354a8e498

Internal exceptions (10 events):
Event: 14.683 Thread 0x00007f9334083000 Threw 0x00000000b943a318 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 14.684 Thread 0x00007f9334083000 Threw 0x00000000b943ec68 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 14.701 Thread 0x00007f9334083000 Threw 0x00000000b944bc08 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 162.674 Thread 0x00007f9358c85800 Implicit null exception at 0x00007f9354a8d513 to 0x00007f9354a8e485
Event: 162.675 Thread 0x00007f9358c85800 Threw 0x00000000b8c83340 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 162.675 Thread 0x00007f9358c85800 Threw 0x00000000b8c89850 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 162.678 Thread 0x00007f9358ad2000 Threw 0x00000000b3bf87a0 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 162.678 Thread 0x00007f9358ad2000 Threw 0x00000000b3bfd1b8 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 162.679 Thread 0x00007f9358ad2000 Threw 0x00000000b3c08408 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166
Event: 162.680 Thread 0x00007f9358c7d000 Threw 0x00000000b49d3ec8 at /HUDSON/workspace/jdk7u17-2-build-linux-amd64-product/jdk7u17/hotspot/src/share/vm/prims/jvm.cpp:1166

Events (10 events):
Event: 179.482 Executing VM operation: RevokeBias
Event: 179.482 Executing VM operation: RevokeBias done
Event: 180.563 Executing VM operation: RevokeBias
Event: 180.563 Executing VM operation: RevokeBias done
Event: 182.717 Executing VM operation: RevokeBias
Event: 182.717 Executing VM operation: RevokeBias done
Event: 184.408 Executing VM operation: RevokeBias
Event: 184.408 Executing VM operation: RevokeBias done
Event: 185.533 Executing VM operation: RevokeBias
Event: 185.534 Executing VM operation: RevokeBias done


Dynamic libraries:
00400000-00401000 r-xp 00000000 fd:00 241325                             /usr/java/jdk1.7.0_17/bin/java
00600000-00601000 rw-p 00000000 fd:00 241325                             /usr/java/jdk1.7.0_17/bin/java
01940000-01961000 rw-p 00000000 00:00 0                                  [heap]
b3000000-fad00000 rw-p 00000000 00:00 0 
fad00000-fae00000 rw-p 00000000 00:00 0 
fae00000-fc2c0000 rw-p 00000000 00:00 0 
fc2c0000-fca70000 rw-p 00000000 00:00 0 
fca70000-100000000 rw-p 00000000 00:00 0 
7f9314000000-7f9314021000 rw-p 00000000 00:00 0 
7f9314021000-7f9318000000 ---p 00000000 00:00 0 
7f9318000000-7f9318038000 rw-p 00000000 00:00 0 
7f9318038000-7f931c000000 ---p 00000000 00:00 0 
7f931c000000-7f931c08a000 rw-p 00000000 00:00 0 
7f931c08a000-7f9320000000 ---p 00000000 00:00 0 
7f9320000000-7f9320021000 rw-p 00000000 00:00 0 
7f9320021000-7f9324000000 ---p 00000000 00:00 0 
7f9324000000-7f9324109000 rw-p 00000000 00:00 0 
7f9324109000-7f9328000000 ---p 00000000 00:00 0 
7f932a753000-7f932a756000 ---p 00000000 00:00 0 
7f932a756000-7f932a794000 rw-p 00000000 00:00 0 
7f932a794000-7f932a797000 ---p 00000000 00:00 0 
7f932a797000-7f932a7d5000 rw-p 00000000 00:00 0 
7f932a7d5000-7f932a7d8000 ---p 00000000 00:00 0 
7f932a7d8000-7f932a816000 rw-p 00000000 00:00 0 
7f932a816000-7f932a819000 ---p 00000000 00:00 0 
7f932a819000-7f932a857000 rw-p 00000000 00:00 0 
7f932a857000-7f932a85a000 ---p 00000000 00:00 0 
7f932a85a000-7f932a898000 rw-p 00000000 00:00 0 
7f932a898000-7f932a89b000 ---p 00000000 00:00 0 
7f932a89b000-7f932a8d9000 rw-p 00000000 00:00 0 
7f932a8d9000-7f932a8dc000 ---p 00000000 00:00 0 
7f932a8dc000-7f932a91a000 rw-p 00000000 00:00 0 
7f932a91a000-7f932a91d000 ---p 00000000 00:00 0 
7f932a91d000-7f932a95b000 rw-p 00000000 00:00 0 
7f932a95b000-7f932a95e000 ---p 00000000 00:00 0 
7f932a95e000-7f932a99c000 rw-p 00000000 00:00 0 
7f932a99c000-7f932a99f000 ---p 00000000 00:00 0 
7f932a99f000-7f932a9dd000 rw-p 00000000 00:00 0 
7f932a9dd000-7f932a9e0000 ---p 00000000 00:00 0 
7f932a9e0000-7f932aa1e000 rw-p 00000000 00:00 0 
7f932aa1e000-7f932aa21000 ---p 00000000 00:00 0 
7f932aa21000-7f932aa5f000 rw-p 00000000 00:00 0 
7f932aa5f000-7f932aa62000 ---p 00000000 00:00 0 
7f932aa62000-7f932aaa0000 rw-p 00000000 00:00 0 
7f932aaa0000-7f932aaa3000 ---p 00000000 00:00 0 
7f932aaa3000-7f932aae1000 rw-p 00000000 00:00 0 
7f932aae1000-7f932aae4000 ---p 00000000 00:00 0 
7f932aae4000-7f932ab22000 rw-p 00000000 00:00 0 
7f932ab22000-7f932ab25000 ---p 00000000 00:00 0 
7f932ab25000-7f932ab63000 rw-p 00000000 00:00 0 
7f932ab63000-7f932ab66000 ---p 00000000 00:00 0 
7f932ab66000-7f932aba4000 rw-p 00000000 00:00 0 
7f932aba4000-7f932aba7000 ---p 00000000 00:00 0 
7f932aba7000-7f932abe5000 rw-p 00000000 00:00 0 
7f932abe5000-7f932abe8000 ---p 00000000 00:00 0 
7f932abe8000-7f932ac26000 rw-p 00000000 00:00 0 
7f932ac26000-7f932ac29000 ---p 00000000 00:00 0 
7f932ac29000-7f932ac67000 rw-p 00000000 00:00 0 
7f932ac67000-7f932ac6a000 ---p 00000000 00:00 0 
7f932ac6a000-7f932aca8000 rw-p 00000000 00:00 0 
7f932aca8000-7f932acab000 ---p 00000000 00:00 0 
7f932acab000-7f932ace9000 rw-p 00000000 00:00 0 
7f932ace9000-7f932acec000 ---p 00000000 00:00 0 
7f932acec000-7f932ad2a000 rw-p 00000000 00:00 0 
7f932ad2a000-7f932ad2d000 ---p 00000000 00:00 0 
7f932ad2d000-7f932ad6b000 rw-p 00000000 00:00 0 
7f932ad6b000-7f932ad6e000 ---p 00000000 00:00 0 
7f932ad6e000-7f932adac000 rw-p 00000000 00:00 0 
7f932adac000-7f932adc2000 r-xp 00000000 fd:00 172072                     /lib64/libresolv-2.12.so
7f932adc2000-7f932afc2000 ---p 00016000 fd:00 172072                     /lib64/libresolv-2.12.so
7f932afc2000-7f932afc3000 r--p 00016000 fd:00 172072                     /lib64/libresolv-2.12.so
7f932afc3000-7f932afc4000 rw-p 00017000 fd:00 172072                     /lib64/libresolv-2.12.so
7f932afc4000-7f932afc6000 rw-p 00000000 00:00 0 
7f932afc6000-7f932afcb000 r-xp 00000000 fd:00 172060                     /lib64/libnss_dns-2.12.so
7f932afcb000-7f932b1ca000 ---p 00005000 fd:00 172060                     /lib64/libnss_dns-2.12.so
7f932b1ca000-7f932b1cb000 r--p 00004000 fd:00 172060                     /lib64/libnss_dns-2.12.so
7f932b1cb000-7f932b1cc000 rw-p 00005000 fd:00 172060                     /lib64/libnss_dns-2.12.so
7f932b1cc000-7f932b1cf000 ---p 00000000 00:00 0 
7f932b1cf000-7f932b20d000 rw-p 00000000 00:00 0 
7f932b20d000-7f932b23c000 r-xp 00000000 fd:03 229411                     /app/elasticsearch-1.0.1/lib/sigar/libsigar-amd64-linux.so
7f932b23c000-7f932b33b000 ---p 0002f000 fd:03 229411                     /app/elasticsearch-1.0.1/lib/sigar/libsigar-amd64-linux.so
7f932b33b000-7f932b341000 rw-p 0002e000 fd:03 229411                     /app/elasticsearch-1.0.1/lib/sigar/libsigar-amd64-linux.so
7f932b341000-7f932b344000 rw-p 00000000 00:00 0 
7f932b344000-7f932b359000 r-xp 00000000 fd:04 28                         /tmp/jna6130575371875995665.tmp
7f932b359000-7f932b458000 ---p 00015000 fd:04 28                         /tmp/jna6130575371875995665.tmp
7f932b458000-7f932b45a000 rw-p 00014000 fd:04 28                         /tmp/jna6130575371875995665.tmp
7f932b45a000-7f932b45b000 ---p 00000000 00:00 0 
7f932b45b000-7f932b55b000 rw-p 00000000 00:00 0 
7f932b55b000-7f932b55e000 ---p 00000000 00:00 0 
7f932b55e000-7f932b59c000 rw-p 00000000 00:00 0 
7f932b59c000-7f932b59f000 ---p 00000000 00:00 0 
7f932b59f000-7f932b5dd000 rw-p 00000000 00:00 0 
7f932b5dd000-7f932b6d0000 r--s 00aa9000 fd:03 229401                     /app/elasticsearch-1.0.1/lib/elasticsearch-1.0.1.jar
7f932b6d0000-7f932b6e0000 r-xp 00000000 fd:00 249669                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libnio.so
7f932b6e0000-7f932b8e0000 ---p 00010000 fd:00 249669                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libnio.so
7f932b8e0000-7f932b8e1000 rw-p 00010000 fd:00 249669                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libnio.so
7f932b8e1000-7f932bbe2000 rw-p 00000000 00:00 0 
7f932bbe2000-7f932bbf7000 r-xp 00000000 fd:00 249668                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libnet.so
7f932bbf7000-7f932bdf7000 ---p 00015000 fd:00 249668                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libnet.so
7f932bdf7000-7f932bdf8000 rw-p 00015000 fd:00 249668                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libnet.so
7f932bdf8000-7f932be00000 r-xp 00000000 fd:00 249666                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libmanagement.so
7f932be00000-7f932bfff000 ---p 00008000 fd:00 249666                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libmanagement.so
7f932bfff000-7f932c000000 rw-p 00007000 fd:00 249666                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libmanagement.so
7f932c000000-7f932d229000 rw-p 00000000 00:00 0 
7f932d229000-7f9330000000 ---p 00000000 00:00 0 
7f9330000000-7f933010d000 rw-p 00000000 00:00 0 
7f933010d000-7f9334000000 ---p 00000000 00:00 0 
7f9334000000-7f933408a000 rw-p 00000000 00:00 0 
7f933408a000-7f9338000000 ---p 00000000 00:00 0 
7f9338000000-7f9338a5d000 rw-p 00000000 00:00 0 
7f9338a5d000-7f933c000000 ---p 00000000 00:00 0 
7f933c000000-7f933c088000 rw-p 00000000 00:00 0 
7f933c088000-7f9340000000 ---p 00000000 00:00 0 
7f9340000000-7f934008f000 rw-p 00000000 00:00 0 
7f934008f000-7f9344000000 ---p 00000000 00:00 0 
7f9344000000-7f9344021000 rw-p 00000000 00:00 0 
7f9344021000-7f9348000000 ---p 00000000 00:00 0 
7f9348000000-7f93480c4000 rw-p 00000000 00:00 0 
7f93480c4000-7f934c000000 ---p 00000000 00:00 0 
7f934c000000-7f934c021000 rw-p 00000000 00:00 0 
7f934c021000-7f9350000000 ---p 00000000 00:00 0 
7f9350010000-7f9350011000 r--p 00000000 00:00 0 
7f9350011000-7f9350012000 rwxp 00000000 00:00 0 
7f9350012000-7f9350015000 ---p 00000000 00:00 0 
7f9350015000-7f9350053000 rw-p 00000000 00:00 0 
7f9350053000-7f9350058000 r--s 00041000 fd:00 249707                     /usr/java/jdk1.7.0_17/jre/lib/jsse.jar
7f9350058000-7f935005f000 r--s 00062000 fd:03 229418                     /app/elasticsearch-1.0.1/lib/sigar/sigar-1.6.4.jar
7f935005f000-7f9350086000 r--s 00217000 fd:03 229384                     /app/elasticsearch-1.0.1/lib/lucene-core-4.6.1.jar
7f9350086000-7f9350097000 r--s 000ac000 fd:03 229398                     /app/elasticsearch-1.0.1/lib/jts-1.12.jar
7f9350097000-7f935009a000 ---p 00000000 00:00 0 
7f935009a000-7f93500d8000 rw-p 00000000 00:00 0 
7f93500d8000-7f93500db000 ---p 00000000 00:00 0 
7f93500db000-7f93501d9000 rw-p 00000000 00:00 0 
7f93501d9000-7f93501dc000 ---p 00000000 00:00 0 
7f93501dc000-7f93502da000 rw-p 00000000 00:00 0 
7f93502da000-7f93502dd000 ---p 00000000 00:00 0 
7f93502dd000-7f935031b000 rw-p 00000000 00:00 0 
7f935031b000-7f935031c000 ---p 00000000 00:00 0 
7f935031c000-7f9350492000 rw-p 00000000 00:00 0 
7f9350492000-7f9350660000 r--s 01ac6000 fd:00 249717                     /usr/java/jdk1.7.0_17/jre/lib/rt.jar
7f9350660000-7f93506aa000 rw-p 00000000 00:00 0 
7f93506aa000-7f93506ab000 ---p 00000000 00:00 0 
7f93506ab000-7f935488c000 rw-p 00000000 00:00 0 
7f935488c000-7f9354890000 rw-p 00000000 00:00 0 
7f9354890000-7f93548aa000 rw-p 00000000 00:00 0 
7f93548aa000-7f93548ab000 rw-p 00000000 00:00 0 
7f93548ab000-7f93548ac000 ---p 00000000 00:00 0 
7f93548ac000-7f93549ac000 rw-p 00000000 00:00 0 
7f93549ac000-7f9354c1c000 rwxp 00000000 00:00 0 
7f9354c1c000-7f93579ac000 rw-p 00000000 00:00 0 
7f93579ac000-7f93579c6000 r-xp 00000000 fd:00 249681                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libzip.so
7f93579c6000-7f9357bc6000 ---p 0001a000 fd:00 249681                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libzip.so
7f9357bc6000-7f9357bc7000 rw-p 0001a000 fd:00 249681                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libzip.so
7f9357bc7000-7f9357bd3000 r-xp 00000000 fd:00 172062                     /lib64/libnss_files-2.12.so
7f9357bd3000-7f9357dd3000 ---p 0000c000 fd:00 172062                     /lib64/libnss_files-2.12.so
7f9357dd3000-7f9357dd4000 r--p 0000c000 fd:00 172062                     /lib64/libnss_files-2.12.so
7f9357dd4000-7f9357dd5000 rw-p 0000d000 fd:00 172062                     /lib64/libnss_files-2.12.so
7f9357dd5000-7f9357dfe000 r-xp 00000000 fd:00 249650                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libjava.so
7f9357dfe000-7f9357ffe000 ---p 00029000 fd:00 249650                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libjava.so
7f9357ffe000-7f9358000000 rw-p 00029000 fd:00 249650                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libjava.so
7f9358000000-7f9358d8d000 rw-p 00000000 00:00 0 
7f9358d8d000-7f935c000000 ---p 00000000 00:00 0 
7f935c000000-7f935c002000 r--s 0000e000 fd:03 229394                     /app/elasticsearch-1.0.1/lib/lucene-join-4.6.1.jar
7f935c002000-7f935c005000 r--s 0001d000 fd:03 229389                     /app/elasticsearch-1.0.1/lib/lucene-highlighter-4.6.1.jar
7f935c005000-7f935c007000 r--s 0000a000 fd:03 229391                     /app/elasticsearch-1.0.1/lib/lucene-sandbox-4.6.1.jar
7f935c007000-7f935c00b000 r--s 000d0000 fd:03 229400                     /app/elasticsearch-1.0.1/lib/jna-3.3.0.jar
7f935c00b000-7f935c00d000 r--s 00015000 fd:03 229396                     /app/elasticsearch-1.0.1/lib/lucene-spatial-4.6.1.jar
7f935c00d000-7f935c010000 r--s 00018000 fd:03 229395                     /app/elasticsearch-1.0.1/lib/lucene-grouping-4.6.1.jar
7f935c010000-7f935c017000 r--s 0004d000 fd:03 229385                     /app/elasticsearch-1.0.1/lib/lucene-codecs-4.6.1.jar
7f935c017000-7f935c020000 r--s 0006f000 fd:03 229399                     /app/elasticsearch-1.0.1/lib/log4j-1.2.17.jar
7f935c020000-7f935c023000 r--s 00015000 fd:03 229393                     /app/elasticsearch-1.0.1/lib/lucene-misc-4.6.1.jar
7f935c023000-7f935c034000 r--s 00174000 fd:03 229387                     /app/elasticsearch-1.0.1/lib/lucene-analyzers-common-4.6.1.jar
7f935c034000-7f935c03d000 r--s 00055000 fd:03 229390                     /app/elasticsearch-1.0.1/lib/lucene-queryparser-4.6.1.jar
7f935c03d000-7f935c03e000 r--s 00008000 fd:03 229388                     /app/elasticsearch-1.0.1/lib/lucene-memory-4.6.1.jar
7f935c03e000-7f935c041000 ---p 00000000 00:00 0 
7f935c041000-7f935c07f000 rw-p 00000000 00:00 0 
7f935c07f000-7f935c082000 ---p 00000000 00:00 0 
7f935c082000-7f935c0c0000 rw-p 00000000 00:00 0 
7f935c0c0000-7f935c0c1000 ---p 00000000 00:00 0 
7f935c0c1000-7f935c1c1000 rw-p 00000000 00:00 0 
7f935c1c1000-7f935c1ce000 r-xp 00000000 fd:00 249680                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libverify.so
7f935c1ce000-7f935c3cd000 ---p 0000d000 fd:00 249680                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libverify.so
7f935c3cd000-7f935c3cf000 rw-p 0000c000 fd:00 249680                     /usr/java/jdk1.7.0_17/jre/lib/amd64/libverify.so
7f935c3cf000-7f935c3d6000 r-xp 00000000 fd:00 172074                     /lib64/librt-2.12.so
7f935c3d6000-7f935c5d5000 ---p 00007000 fd:00 172074                     /lib64/librt-2.12.so
7f935c5d5000-7f935c5d6000 r--p 00006000 fd:00 172074                     /lib64/librt-2.12.so
7f935c5d6000-7f935c5d7000 rw-p 00007000 fd:00 172074                     /lib64/librt-2.12.so
7f935c5d7000-7f935c65a000 r-xp 00000000 fd:00 172054                     /lib64/libm-2.12.so
7f935c65a000-7f935c859000 ---p 00083000 fd:00 172054                     /lib64/libm-2.12.so
7f935c859000-7f935c85a000 r--p 00082000 fd:00 172054                     /lib64/libm-2.12.so
7f935c85a000-7f935c85b000 rw-p 00083000 fd:00 172054                     /lib64/libm-2.12.so
7f935c85b000-7f935d2a2000 r-xp 00000000 fd:00 257753                     /usr/java/jdk1.7.0_17/jre/lib/amd64/server/libjvm.so
7f935d2a2000-7f935d4a2000 ---p 00a47000 fd:00 257753                     /usr/java/jdk1.7.0_17/jre/lib/amd64/server/libjvm.so
7f935d4a2000-7f935d545000 rw-p 00a47000 fd:00 257753                     /usr/java/jdk1.7.0_17/jre/lib/amd64/server/libjvm.so
7f935d545000-7f935d581000 rw-p 00000000 00:00 0 
7f935d581000-7f935d70b000 r-xp 00000000 fd:00 172046                     /lib64/libc-2.12.so
7f935d70b000-7f935d90a000 ---p 0018a000 fd:00 172046                     /lib64/libc-2.12.so
7f935d90a000-7f935d90e000 r--p 00189000 fd:00 172046                     /lib64/libc-2.12.so
7f935d90e000-7f935d90f000 rw-p 0018d000 fd:00 172046                     /lib64/libc-2.12.so
7f935d90f000-7f935d914000 rw-p 00000000 00:00 0 
7f935d914000-7f935d916000 r-xp 00000000 fd:00 172052                     /lib64/libdl-2.12.so
7f935d916000-7f935db16000 ---p 00002000 fd:00 172052                     /lib64/libdl-2.12.so
7f935db16000-7f935db17000 r--p 00002000 fd:00 172052                     /lib64/libdl-2.12.so
7f935db17000-7f935db18000 rw-p 00003000 fd:00 172052                     /lib64/libdl-2.12.so
7f935db18000-7f935db2e000 r-xp 00000000 fd:00 249631                     /usr/java/jdk1.7.0_17/jre/lib/amd64/jli/libjli.so
7f935db2e000-7f935dd2e000 ---p 00016000 fd:00 249631                     /usr/java/jdk1.7.0_17/jre/lib/amd64/jli/libjli.so
7f935dd2e000-7f935dd2f000 rw-p 00016000 fd:00 249631                     /usr/java/jdk1.7.0_17/jre/lib/amd64/jli/libjli.so
7f935dd2f000-7f935dd46000 r-xp 00000000 fd:00 172070                     /lib64/libpthread-2.12.so
7f935dd46000-7f935df46000 ---p 00017000 fd:00 172070                     /lib64/libpthread-2.12.so
7f935df46000-7f935df47000 r--p 00017000 fd:00 172070                     /lib64/libpthread-2.12.so
7f935df47000-7f935df48000 rw-p 00018000 fd:00 172070                     /lib64/libpthread-2.12.so
7f935df48000-7f935df4c000 rw-p 00000000 00:00 0 
7f935df4c000-7f935df6c000 r-xp 00000000 fd:00 172039                     /lib64/ld-2.12.so
7f935df6c000-7f935dfa1000 rw-p 00000000 00:00 0 
7f935dfa1000-7f935dfa3000 r--s 0000f000 fd:03 229397                     /app/elasticsearch-1.0.1/lib/spatial4j-0.3.jar
7f935dfa3000-7f935dfa9000 r--s 0002e000 fd:03 229386                     /app/elasticsearch-1.0.1/lib/lucene-queries-4.6.1.jar
7f935dfa9000-7f935dfac000 ---p 00000000 00:00 0 
7f935dfac000-7f935e038000 rw-p 00000000 00:00 0 
7f935e038000-7f935e03c000 rw-p 00000000 00:00 0 
7f935e03c000-7f935e057000 rw-p 00000000 00:00 0 
7f935e057000-7f935e061000 rw-p 00000000 00:00 0 
7f935e061000-7f935e117000 rw-p 00000000 00:00 0 
7f935e117000-7f935e11f000 rw-s 00000000 fd:04 24578                      /tmp/hsperfdata_jetty/31076
7f935e11f000-7f935e122000 ---p 00000000 00:00 0 
7f935e122000-7f935e164000 rw-p 00000000 00:00 0 
7f935e164000-7f935e168000 r--s 0002a000 fd:03 229392                     /app/elasticsearch-1.0.1/lib/lucene-suggest-4.6.1.jar
7f935e168000-7f935e169000 rw-p 00000000 00:00 0 
7f935e169000-7f935e16a000 r--p 00000000 00:00 0 
7f935e16a000-7f935e16b000 rw-p 00000000 00:00 0 
7f935e16b000-7f935e16c000 r--p 0001f000 fd:00 172039                     /lib64/ld-2.12.so
7f935e16c000-7f935e16d000 rw-p 00020000 fd:00 172039                     /lib64/ld-2.12.so
7f935e16d000-7f935e16e000 rw-p 00000000 00:00 0 
7fffb603d000-7fffb6052000 rw-p 00000000 00:00 0                          [stack]
7fffb6087000-7fffb6088000 r-xp 00000000 00:00 0                          [vdso]
ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]

VM Arguments:
jvm_args: -Djava.net.preferIPv4Stack=true -Des.config=/app/elasticsearch-1.0.1/config/elasticsearch.yml -Xms1149m -Xmx1149m -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.port=9201 -Djava.rmi.server.hostname=0.0.0.0 -Delasticsearch -Des.pidfile=/var/run/elasticsearch/stgesmas03.pid -Des.foreground=yes -Des.path.home=/app/elasticsearch 
java_command: org.elasticsearch.bootstrap.Elasticsearch
Launcher Type: SUN_STANDARD

Environment Variables:
PATH=/sbin:/usr/sbin:/bin:/usr/bin
SHELL=/bin/bash

Signal Handlers:
SIGSEGV: [libjvm.so+0x8a5d70], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGBUS: [libjvm.so+0x8a5d70], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGFPE: [libjvm.so+0x741e50], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGPIPE: [libjvm.so+0x741e50], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGXFSZ: [libjvm.so+0x741e50], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGILL: [libjvm.so+0x741e50], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGUSR1: SIG_DFL, sa_mask[0]=0x00000000, sa_flags=0x00000000
SIGUSR2: [libjvm.so+0x7417a0], sa_mask[0]=0x00000000, sa_flags=0x10000004
SIGHUP: [libjvm.so+0x743b30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGINT: [libjvm.so+0x743b30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGTERM: [libjvm.so+0x743b30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004
SIGQUIT: [libjvm.so+0x743b30], sa_mask[0]=0x7ffbfeff, sa_flags=0x10000004


---------------  S Y S T E M  ---------------

OS:CentOS release 6.4 (Final)

uname:Linux 2.6.32-358.2.1.el6.centos.plus.x86_64 #1 SMP Wed Mar 13 02:09:07 UTC 2013 x86_64
libc:glibc 2.12 NPTL 2.12 
rlimit: STACK 10240k, CORE 0k, NPROC 1024, NOFILE 65536, AS infinity
load average:0.47 0.19 0.07

/proc/meminfo:
MemTotal:        1922416 kB
MemFree:          114868 kB
Buffers:            2424 kB
Cached:           145384 kB
SwapCached:         4864 kB
Active:           101880 kB
Inactive:         184272 kB
Active(anon):      45040 kB
Inactive(anon):   108904 kB
Active(file):      56840 kB
Inactive(file):    75368 kB
Unevictable:     1446328 kB
Mlocked:           91156 kB
SwapTotal:       2097144 kB
SwapFree:        2057692 kB
Dirty:             36180 kB
Writeback:             0 kB
AnonPages:       1582308 kB
Mapped:            27444 kB
Shmem:                16 kB
Slab:              40048 kB
SReclaimable:      12652 kB
SUnreclaim:        27396 kB
KernelStack:        1608 kB
PageTables:         9028 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:     3058352 kB
Committed_AS:    1673916 kB
VmallocTotal:   34359738367 kB
VmallocUsed:      273984 kB
VmallocChunk:   34359454280 kB
HardwareCorrupted:     0 kB
AnonHugePages:   1304576 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
DirectMap4k:        8192 kB
DirectMap2M:     2088960 kB


CPU:total 2 (1 cores per cpu, 1 threads per core) family 6 model 45 stepping 7, cmov, cx8, fxsr, mmx, sse, sse2, sse3, ssse3, sse4.1, sse4.2, popcnt, avx, tsc, tscinvbit, tscinv

/proc/cpuinfo:
processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 45
model name  : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz
stepping    : 7
cpu MHz     : 2000.000
cache size  : 20480 KB
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 cx16 pcid sse4_1 sse4_2 x2apic popcnt aes xsave avx hypervisor lahf_lm ida arat epb xsaveopt pln pts dts
bogomips    : 4000.00
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:

processor   : 1
vendor_id   : GenuineIntel
cpu family  : 6
model       : 45
model name  : Intel(R) Xeon(R) CPU E5-2650 0 @ 2.00GHz
stepping    : 7
cpu MHz     : 2000.000
cache size  : 20480 KB
fpu     : yes
fpu_exception   : yes
cpuid level : 13
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts xtopology tsc_reliable nonstop_tsc aperfmperf unfair_spinlock pni pclmulqdq ssse3 cx16 pcid sse4_1 sse4_2 x2apic popcnt aes xsave avx hypervisor lahf_lm ida arat epb xsaveopt pln pts dts
bogomips    : 4000.00
clflush size    : 64
cache_alignment : 64
address sizes   : 40 bits physical, 48 bits virtual
power management:



Memory: 4k page, physical 1922416k(114868k free), swap 2097144k(2057692k free)

vm_info: Java HotSpot(TM) 64-Bit Server VM (23.7-b01) for linux-amd64 JRE (1.7.0_17-b02), built on Mar  1 2013 02:59:06 by "java_re" with gcc 4.3.0 20080428 (Red Hat 4.3.0-8)

time: Wed Jun 25 15:24:07 2014
elapsed time: 186 seconds
```
</comment><comment author="jpountz" created="2014-06-30T12:26:08Z" id="47525171">Thanks for the detailed report. I'm not very familiar with library linking but this seems to be a expected issue: http://stackoverflow.com/questions/3855004/overwriting-library-file-causes-segmentation-fault ? I'm also afraid overwriting files this way could be an issue for Java if classes are loaded from different versions of the Elasticsearch JAR (class loading is lazy) as they could be incompatible. I think you did the right thing by stopping the process before overwriting files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't create mapping entry for dynamic templates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6619</link><project id="" key="" /><description>Today, when a new field name shows up in a document matching a dynamic template, we record that field name, its type information, etc.

But if you add many, many fields this way, the mappings become very large and serializing them into the cluster state becomes very costly.

I think we may be able to get away with not making a mapping entry and just re-matching that same field the next time it comes?  Or maybe making mapping entries only up until a limit..
</description><key id="36520833">6619</key><summary>Don't create mapping entry for dynamic templates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-06-25T20:49:47Z</created><updated>2014-07-21T13:24:47Z</updated><resolved>2014-07-05T15:44:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-06-28T09:11:18Z" id="47422361">I spent some time looking at the mapping code but I don't understand it enough to make progress here... can someone who knows ObjectMapper.java give some pointers?

I tried commenting out the putMapper(mapper) and context.setMappingsModified() in the end of parseDynamicValue, but this makes many tests angry...
</comment><comment author="kimchy" created="2014-06-30T10:49:06Z" id="47518149">This will be a rather big change, since we also need to change in each place that looks up a mapping (for search and such). I think that concrete mappings, even with dynamic templates, is very valuable, for example, Kibana can then auto suggest existing fields and such.

I think that there is a lot of improvements that we can add to ES even when it concretely creates mappings. One is this: #6648, the other is potentially to move from update on write data structures (that have a better concurrency story) to update in place concurrent data structures above a certain threshold. Based on my tests, I think we can get to a very good perf while still maintaing the concrete mappings case.

The cluster state is the place that will suffer, or when someone has 1 million fields for example. But I think that this is simply abusing the system and things will break in other places (in terms of resources used, ...), not just mappings.
</comment><comment author="kimchy" created="2014-07-03T19:06:16Z" id="47971747">update
- #6648 is in, and will be on 1.3.
- #6714 is in, will be on 1.3 (memory improvement in analysis for many mappings)
- #6707 the last one, that brings the perf to acceptable levels, but is a bit trickier in terms of code change, we are still discussing...
</comment><comment author="kimchy" created="2014-07-05T15:44:18Z" id="48089355">#6707 has been pushed as well, I think we are at a good state performance wise, so closing this for now, we can reopen a new issue if this is still a problem
</comment><comment author="kimchy" created="2014-07-21T13:24:47Z" id="49604442">#6843 another one that helps a lot with memory usage here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to predefine number of buckets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6618</link><project id="" key="" /><description>It would be great to have the ability to predefine the number of buckets on a date histogram (and possibly other types) and have the buckets be evenly divided across time based on the result set.

Currently you need to understand how your query results span time to know what interval makes sense to set on the aggregation (which is hard to know before the query is executed), because if your data spans years an interval of a minute may be too fine, and vice versa.

With this feature a sample query might look like...
"aggregations" : {
    "birth_date_time" : {
      "date_histogram" : {
        "field" : "birth_date_time",
        "bucketCount" : 100,
        "format" : "yyyy/MM/dd HH:mm:ss"
      }
    }
  }

And thus if your data spans 50 years each bucket would represent everyone who was born in approximately a 6 month window.

Note that based on implementation there may be fewer than 100 buckets in the response if some buckets have 0 results, but there should be no more than 100 buckets.

This feature would be very beneficial when plotting to a histogram on a UI, since it will give the granularity that makes sense to display.
</description><key id="36511176">6618</key><summary>Ability to predefine number of buckets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">johnrodey</reporter><labels /><created>2014-06-25T18:57:54Z</created><updated>2014-07-04T08:44:54Z</updated><resolved>2014-07-04T08:44:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-01T13:24:58Z" id="47655046">@jpountz would this be possible with bucket reducers?
</comment><comment author="jpountz" created="2014-07-01T13:35:29Z" id="47656265">I think it could. Eg. you could always use second as an interval and then reduce as necessary to reach the desired number of buckets. But then if your actual data spans several years, you will be creating tens of millions of buckets to finally reduce into a couple of them. That would have important CPU/memory implications.

I'm wondering that the best option might just be to run an additional request to figure out the optimal interval, eg. with min/max aggregations.
</comment><comment author="uboness" created="2014-07-01T13:50:27Z" id="47657989">&gt; I think it could. Eg. you could always use second as an interval and then reduce as necessary to reach the desired number of buckets. But then if your actual data spans several years, you will be creating tens of millions of buckets to finally reduce into a couple of them. That would have important CPU/memory implications.

-1... that doesn't scale as a general solution for this functionality

&gt; I'm wondering that the best option might just be to run an additional request to figure out the optimal interval, eg. with min/max aggregations.

+1.
</comment><comment author="johnrodey" created="2014-07-01T16:41:05Z" id="47679768">I think executing a second request after I find the earliest and latest dates will work for me.  I do however think it would be desirable to possibly add this feature in the future, even if it is low priority.

Thanks for the insight!
</comment><comment author="jpountz" created="2014-07-04T08:44:54Z" id="48021175">I have been thinking more about this one and I don't think we can do better than doing a first request to figure out the min/max values first and deciding on the interval based on those, which can be done from clients. So I will close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Added field data circuit breaker settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6617</link><project id="" key="" /><description>Added field data circuit breaker settings to list of update-able properties by the cluster update api.
</description><key id="36510942">6617</key><summary>Docs: Added field data circuit breaker settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">mahdeto</reporter><labels><label>docs</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-25T18:55:21Z</created><updated>2014-07-16T13:15:24Z</updated><resolved>2014-06-26T08:29:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-26T08:29:44Z" id="47200165">Pushed this to master and 1.x in e78f1ed and f2fa4dd. Thanks @mahdeto!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add relevancy to geo_shape queries based on overlap or distance</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6616</link><project id="" key="" /><description>geo_shape query results should be scored based on percent overlap with or distance from the query shape.

There are references to adding this feature (https://github.com/elasticsearch/elasticsearch/issues/2169 and http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-geo-shape-query.html#_relevancy_and_score) but I didn't find a specific issue for it. This would be a very powerful enhancement!
</description><key id="36507792">6616</key><summary>Add relevancy to geo_shape queries based on overlap or distance</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nknize/following{/other_user}', u'events_url': u'https://api.github.com/users/nknize/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nknize/orgs', u'url': u'https://api.github.com/users/nknize', u'gists_url': u'https://api.github.com/users/nknize/gists{/gist_id}', u'html_url': u'https://github.com/nknize', u'subscriptions_url': u'https://api.github.com/users/nknize/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/830187?v=4', u'repos_url': u'https://api.github.com/users/nknize/repos', u'received_events_url': u'https://api.github.com/users/nknize/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nknize/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nknize', u'type': u'User', u'id': 830187, u'followers_url': u'https://api.github.com/users/nknize/followers'}</assignee><reporter username="">shane-axiom</reporter><labels><label>:Geo</label><label>discuss</label><label>feature</label></labels><created>2014-06-25T18:19:00Z</created><updated>2017-03-03T21:16:19Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T09:46:30Z" id="48026111">Having discussed this issue, we feel that we should wait for this feature to be supported in Lucene before considering adding it to Elasticsearch.
</comment><comment author="shane-axiom" created="2014-09-10T18:51:45Z" id="55164170">Talked with @dsmiley at foss4g, and he said that this capability is now available in Lucene and implemented in Solr. Time to reopen? Note that he also said overlap relevancy is only possible with bbox queries, not with arbitrary polygons. (@dsmiley please correct me if needed).
</comment><comment author="clintongormley" created="2014-09-11T08:39:12Z" id="55235562">Great news - reopening
</comment><comment author="clintongormley" created="2014-10-24T09:45:09Z" id="60366066">@dsmiley all we can find is http://lucene.apache.org/core/4_10_0/spatial/org/apache/lucene/spatial/bbox/BBoxOverlapRatioValueSource.html

Is there anything more for arbitrary shapes, or just bounding box support?
</comment><comment author="dsmiley" created="2014-10-24T13:27:23Z" id="60386301">This is just for bounding box (rectangles). It can be used with BBoxStrategy or SerializedDVStrategy (and then converting to bbox) but it's primarily designed for BBoxStrategy.
</comment><comment author="clintongormley" created="2014-10-24T14:41:04Z" id="60396111">thanks @dsmiley 
</comment><comment author="clintongormley" created="2015-11-21T15:20:21Z" id="158651829">@nknize is this a feature you're planning on implementing?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DOC unescape regexes in Pattern Tokenizer docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6615</link><project id="" key="" /><description>Currently regexes in Pattern Tokenizer docs are escaped (it seems according to Java rules). I think it is better not to escape them because JSON escaping should be automatic in client libraries, and string escaping depends on a client language used. The default pattern is `\W+`, not `\\W+`.
</description><key id="36503002">6615</key><summary>DOC unescape regexes in Pattern Tokenizer docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kmike</reporter><labels /><created>2014-06-25T17:19:22Z</created><updated>2014-07-03T11:34:43Z</updated><resolved>2014-07-03T11:34:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-01T13:22:36Z" id="47654754">@kmike 

this makes sense.  however, it would be good to add a big note about the need to escape regexes, with an example.  would you be up for adding that?
</comment><comment author="clintongormley" created="2014-07-01T13:23:43Z" id="47654879">@kmike also, please could you sign our CLA so that I can merge your PR in?  http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="kmike" created="2014-07-01T18:25:04Z" id="47692277">@clintongormley I've signed the CLA several days ago. 

As for the escaping, the rules are the same for regexes and for all other strings that are passed to ElasticSearch; I'm not 100% sure this should be documented specifically in regex docs - it is not documented anywhere else AFAIK. But I've added a note - escaping `pattern` is a common case indeed, and there is no harm in mentioning that. 
</comment><comment author="clintongormley" created="2014-07-03T11:34:43Z" id="47895516">thanks @kmike 

merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>size:1 on a query applied to _all indexes only returns from the newest index, size:2 returns from all. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6614</link><project id="" key="" /><description>Our system is searching for the oldest record that hits a criteria.  We attempted to accomplish this with a size limited ascending range query with bool filters.  

On ES .90 and 1.1.2 the following queries were run:

 curl -XGET http://localhost:9200/_all/meta/_search -d  '{ "sort" : [ { "TimeUpdated" : { "order": "asc", "ignore_unmapped" : true } } ], query : {filtered : { filter : { bool : { must : [ { term : {"Written" : true }}  , { term :  { "LatestUpdate" : true } } , { range : { "TimeUpdated" : { gte : "2013/05/29 20:51:00" } } } ] } } } , _cache : false , from : 0,size : 1 , "fields" : [ "Session", "TimeUpdated", "TimeStart" ] } }'

{"took":616,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":42959661,"max_score":null,"hits":[{"_index":"network_2014_06_25","_type":"meta","_id":"f3bb5501-ce17-4f13-a262-48b89e175201_2","_score":null,"fields":{"TimeStart":["2014/06/25 03:03:49"],"TimeUpdated":["2014/06/25 03:25:49"],"Session":["f3bb5501-ce17-4f13-a262-48b89e175201"]},"sort":[1403666749000]}]}}

The indices are built by date, this one "network_2014_06_25" is the newest.  However adding one to the size gives a radically different result:

 curl -XGET http://localhost:9200/_all/meta/_search -d  '{ "sort" : [ { "TimeUpdated" : { "order": "asc", "ignore_unmapped" : true } } ], query : {filtered : { filter : { bool : { t : [ { term : {"Written" : true }}  , { term :  { "LatestUpdate" : true } } , { range : { "TimeUpdated" : { gte : "2013/05/29 20:51:00" } } } ] } } } , _cache : false , from : 0,size : 2 , "fields" : [ "Session", "TimeUpdated", "TimeStart" ] } }'

{"took":770,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":42973532,"max_score":null,"hits":[{"_index":"network_2014_05_25","_type":"meta","_id":"b97c12c0-b863-49d2-9355-483aee55de16_1","_score":null,"fields":{"TimeStart":["2014/05/25 01:13:05"],"TimeUpdated":["2014/05/25 01:14:04"],"Session":["b97c12c0-b863-49d2-9355-483aee55de16"]},"sort":[1400980444000]},{"_index":"network_2014_06_25","_type":"meta","_id":"bdc9d4b3-4e60-4667-b66a-db831cf2f59b_2","_score":null,"fields":{"TimeStart":["2014/06/25 03:03:49"],"TimeUpdated":["2014/06/25 03:25:49"],"Session":["bdc9d4b3-4e60-4667-b66a-db831cf2f59b"]},"sort":[1403666749000]}]}}

Further increasing the size keeps hitting the correct "b97c12c0-b863-49d2-9355-483aee55de16_1" record rather than the  certainly not the oldest "bdc9d4b3-4e60-4667-b66a-db831cf2f59b_2" record. 
</description><key id="36502163">6614</key><summary>size:1 on a query applied to _all indexes only returns from the newest index, size:2 returns from all. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">weberr13</reporter><labels><label>feedback_needed</label></labels><created>2014-06-25T17:08:43Z</created><updated>2014-10-28T22:52:42Z</updated><resolved>2014-07-23T19:09:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-01T13:19:38Z" id="47654406">Hi @weberr13 

Your request is nested incorrectly:

```
GET /_all/meta/_search
{
  "sort": [
    {
      "TimeUpdated": {
        "order": "asc",
        "ignore_unmapped": true
      }
    }
  ],
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "Written": true
              }
            },
            {
              "term": {
                "LatestUpdate": true
              }
            },
            {
              "range": {
                "TimeUpdated": {
                  "gte": "2013/05/29 20:51:00"
                }
              }
            }
          ]
        }
      }
    },
    "_cache": false,
    "from": 0,
    "size": 1,
    "fields": [
      "Session",
      "TimeUpdated",
      "TimeStart"
    ]
  }
}
```

`_cache`, `from` and `size` are at the wrong level completely.

Please could you fix the nesting and see if that resolves the issue?  If it doesn't, add a comment with the correct query and I'll take another look.
</comment><comment author="weberr13" created="2014-07-01T16:34:35Z" id="47678923">I don't understand the scope problem.  If the size is outside the "query" it is ignored, if the size is inside the "filtered" it doesn't function.  The size applies to the Query and it is scoped as such.  If the "size" was at the wrong level, wouldn't it be ignored rather than returning the wrong 1 result?   If it was at the wrong level, wouldn't the query for 2 also function the same way (only returning from the latest index, instead of returning one that is oldest and one from the latest? )  All logic seems to point to an "off by one" error when it comes to how the size parameter mixes with sorted results... 
</comment><comment author="clintongormley" created="2014-07-01T17:28:05Z" id="47685504">`from`, `size` and `fields` should all be at the same level as `query`.  `_cache` should be inside the `bool` query.

it is quite possible, especially on older versions, that Elasticsearch hits an unknown key and just stops parsing, rather than throwing an error.  
</comment><comment author="weberr13" created="2014-07-01T17:53:03Z" id="47688439">When I move size up to Query, it is ignored, as I said.  The only way to get 1 result is to write the query as above.  It isn't about unknown keys either.  All the results are valid, it is the sorting that is off.  Basically this is what is occuring:

Index 2 days ago: 2 records ( id a,b)
Index 1 days ago: 2 records ( id c,d)
Index today : 2 records ( e,f)

When I query and limit the size to 10 (default) I get

a, b, c, d, e, f

When I query and limit the size to 5 I get

a, b, c, d, e

when I limit to 4 I get

a, b, c,e

3: 

a, b, e

2: 

a, e

1: 

e

Do you see the issue? 
</comment><comment author="clintongormley" created="2014-07-01T18:10:17Z" id="47690525">@weberr13 Yes I see the issue.  What I don't see is a query that actually runs.  The query that you sent me is invalid and throws an error on 1.2.1: `ElasticsearchParseException[Expected field name but got VALUE_BOOLEAN "_cache"]`

You can see from the documentation that `from` and `size` should be at the same level as `query`http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-from-size.html

Also, that `_cache` should be applied to a particular filter: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-filters.html#caching such as the `bool` filter: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-bool-filter.html#_caching_2

So either you sent me a different query than the one you posted originally, or you are running a broken query.  

I'm not saying there isn't an issue. All I'm saying is that you haven't given me the information that I need to figure it out.  What would be awesome is if you could give me a short recreation of the problem, with `curl` statements which
- create the indices, plus mapping
- insert documents
- search, demonstrating the problem

With that in hand, it becomes a lot easier to figure out if there is a real problem or not.
</comment><comment author="weberr13" created="2014-07-01T18:52:00Z" id="47695318"> I'm running on 1.1.2 (as I said in the previous post).  If you want to test on your version, go ahead and pull the size up to the same level of query, but for my install that doesn't work (it ignores the size parameter all together).  

As for re-creation, any data that can be sorted ASC on date would work. 
</comment><comment author="clintongormley" created="2014-07-01T18:55:09Z" id="47695701">@weberr13 the `size` parameter has only ever been accepted at the same level as the `query`, right since the first release.  The only thing that has changed is whether an error is thrown or not.

Also, I've just looked at your second query which has another error: your bool query has a `t` key which is meaningless.

Please send me an actual working recreation.  For instance, from the data you've provided, I've written the following, which works exactly as it is supposed to work:

```
DELETE /_all

PUT /index_2014-07-01/meta/1
{
  "foo": "bar",
  "timeUpdated": "2014/07/01 01:00:00"
}
PUT /index_2014-07-01/meta/2
{
  "foo": "bar",
  "timeUpdated": "2014/07/01 02:00:00"
}
PUT /index_2014-07-02/meta/3
{
  "foo": "bar",
  "timeUpdated": "2014/07/02 01:00:00"
}
PUT /index_2014-07-02/meta/4
{
  "foo": "bar",
  "timeUpdated": "2014/07/02 02:00:00"
}
PUT /index_2014-07-03/meta/5
{
  "foo": "bar",
  "timeUpdated": "2014/07/03 01:00:00"
}
PUT /index_2014-07-03/meta/6
{
  "foo": "bar",
  "timeUpdated": "2014/07/03 02:00:00"
}

GET /_all/meta/_search
{
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "foo": "bar"
              }
            },
            {
              "range": {
                "timeUpdated": {
                  "gt": "2014/06/01"
                }
              }
            }
          ],
          "_cache": false
        }
      }
    }
  },
  "from": 0,
  "size": 1,
  "sort": [
    {
      "timeUpdated": {
        "order": "asc",
        "ignore_unmapped": true
      }
    }
  ]
}
```

So I am currently unable to recreate your issue.  If you are able to recreate it and can send me that recreation, that would be very useful.
</comment><comment author="weberr13" created="2014-07-01T19:06:27Z" id="47696980">I literally cut and pasted from my terminal when I submitted the bug.  I'm sorry that you cannot believe me. 
</comment><comment author="clintongormley" created="2014-07-01T19:10:05Z" id="47697368">@weberr13 It's not that I don't believe what you're seeing, but I'm telling you that what you copied and pasted is invalid.  So before we can decide if there is a bug or not, we need a valid recreation.

I demonstrated a recreation above that shows that (for this case) things work as expected. Of course there may be some other issue that does indeed have a bug, but unless you give me that information I am unable to figure out what causes it.

Have you tried my recreation? Does it work for you? Do you not believe me?
</comment><comment author="weberr13" created="2014-07-01T19:33:16Z" id="47700015">Here is what happens:

#1 the query as I originally posted it:

[weberr@probe Upgrade]$ curl -XGET http://localhost:9200/_all/meta/_search -d  '{ 
   "sort" : [ { "TimeUpdated" : { "order": "asc", "ignore_unmapped" : true } } ],
       query : {
            filtered : {
                filter : { 
                     bool : { 
                          must : [ { term : {"Written" : true }} , { term : { "LatestUpdate" : true } } , { range : { "TimeUpdated" : { gte : "2013/05/29 20:51:00" } } } ]
                     }
                }
            } , _cache : false , from : 0,size : 1 , "fields" : [ "Session", "TimeUpdated", "TimeStart" ]  
       } 
}'
{"took":54,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":1994381,"max_score":null,"hits":[{"_index":"network_2014_06_26","_type":"meta","_id":"af210e47-0bd6-4609-8f0d-aaa41f1e8aae_1","_score":null,"fields":{"TimeStart":["2014/06/26 01:05:21"],"TimeUpdated":["2014/06/26 01:06:03"],"Session":["af210e47-0bd6-4609-8f0d-aaa41f1e8aae"]},"sort":[1403744763000]}]}}

The query as you describe (where I took _cache, sort and from outside of the query bock:

curl -XGET http://localhost:9200/_all/meta/_search -d  '{ 
   "sort" : [ { "TimeUpdated" : { "order": "asc", "ignore_unmapped" : true } } ],
       query : {
            filtered : {
                filter : { 
                     bool : { 
                          must : [ { term : {"Written" : true }} , { term : { "LatestUpdate" : true } } , { range : { "TimeUpdated"
 : { gte : "2013/05/29 20:51:00" } } } ]
                     }
                }
            } , "fields" : [ "Session", "TimeUpdated", "TimeStart" ]  
       } ,  _cache : false , from : 0,size : 1 
}'

I get:

{"took":50,
"timed_out":false,
"_shards":{"total":65,
"successful":65,
"failed":0},
"hits":{"total":1994381,
"max_score":null,
"hits":[{"_index":"network_2014_06_20",
"_type":"meta",
"_id":"355a05fc-27dc-4bed-beb3-2df23afeebdb_1",
"_score":null,
 "_source" : {"TimePreviousRaw":1403253269,
"TimePrevious":"2014/06/20 08:34:29",
"Host":["usbo1pdc02.schq.secious.com"],
"QDCount":1,
"DestIP":"10.128.64.252",
"ServerAddr":"10.128.64.252",
"TTL":3600,
"TransactionID":15468,
"DestBytesDelta":670,
"SrcBytesDelta":374,
"FlowID":2529124,
"Session":"355a05fc-27dc-4bed-beb3-2df23afeebdb",
"SrcIP":"10.1.4.184",
"TimeUpdated":"2014/06/20 08:35:28",
"Protocol":17,
"SrcMAC":"00:02:55:4f:da:3a",
"ThreadID":6,
"Written":true,
"FlowSessionCount":0,
"ApplicationID":794,
"SrcPort":46025,
"StoragePath":"/pcap0/9041/355a05fc-27dc-4bed-beb3-2df23afeebdb",
"DestPort":53,
"Application":"dns",
"CnxDuration":"1.236548",
"ANCount":0,
"TimeStartRaw":1403253269,
"FamilyEnd":["Network Service"],
"MessageSize":990544,
"Query":["qa-aix61.schq.secious.com",
"qa-aix61.schq.secious.com.schq.secious.com"],
"ApplicationPath":"/udp/dns",
"TimeStart":"2014/06/20 08:34:29",
"Dev":["bond0"],
"TotalBytes":1044,
"ARCount":0,
"Duration":59,
"StartTime":"2014/06/20 08:34:29.728553",
"ChildFlowNumber":1,
"FieldCount":10,
"Captured":true,
"TotalBytesDelta":1044,
"ResponseTime":"1970/01/01 00:00:00.000138",
"PacketsDelta":8,
"LatestUpdate":true,
"TimeDelta":59,
"NSCount":1,
"DestBytes":670,
"SrcBytes":374,
"Name":["schq.secious.com"],
"TimeUpdatedRaw":1403253328,
"CreateTime":"2014/06/20 08:34:29.728553",
"ClientAddr":"10.1.4.184",
"FlowCompleted":true,
"Family":["Network Service"],
"DestMAC":"f0:f7:55:dc:a8:7f",
"SessionPktCounter":4,
"TotalPackets":8},
"sort":[1403253328000]},
{"_index":"network_2014_06_20",
"_type":"meta",
"_id":"bfefefa3-1c0e-4b97-ac16-a54c711be208_1",
"_score":null,
 "_source" : {"TimePreviousRaw":1403253084,
"TimePrevious":"2014/06/20 08:31:24",
"DestIP":"74.125.227.134",
"ServerName":["safebrowsing-cache.google.com"],
"ServerAddr":"74.125.227.134",
"SeqNum":2080575399,
"DestBytesDelta":18751,
"SrcBytesDelta":6388,
"RetransmissionBytes":412,
"FlowID":2520449,
"Session":"bfefefa3-1c0e-4b97-ac16-a54c711be208",
"SrcIP":"10.1.20.62",
... continues for 600 lines... 

Where do you suggest I put the size: parameter in order for the query to work?  Even though it returns the wrong entry, it at least works.  Even more, by moving it, the Fields block suddenly stopped working as well. I believe it doesn't work for you, but I need something from you that I can run. 
</comment><comment author="clintongormley" created="2014-07-01T19:43:26Z" id="47701134">Try this:
1. Move the `fields` parameter to the same level as `query`, `from` and `size`
2. Remove the `_cache` parameter as it is completely out of place there

The query:

```
curl -XGET "http://localhost:9200/_all/meta/_search" -d'
{
  "sort": [
    {
      "TimeUpdated": {
        "order": "asc",
        "ignore_unmapped": true
      }
    }
  ],
  "query": {
    "filtered": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "Written": true
              }
            },
            {
              "term": {
                "LatestUpdate": true
              }
            },
            {
              "range": {
                "TimeUpdated": {
                  "gte": "2013/05/29 20:51:00"
                }
              }
            }
          ]
        }
      }
    }
  },
  "fields": [
    "Session",
    "TimeUpdated",
    "TimeStart"
  ],
  "from": 0,
  "size": 1
}'
```

What does this give you?
</comment><comment author="weberr13" created="2014-07-01T19:47:29Z" id="47701583">The above reproduces the issue:

{"took":8512,"timed_out":false,"_shards":{"total":65,"successful":2,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}[weberr@probe Upgrade]$ sh foo
{"took":293,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":1994381,"max_score":null,"hits":[{"_index":"network_2014_06_26","_type":"meta","_id":"af210e47-0bd6-4609-8f0d-aaa41f1e8aae_1","_score":null,"fields":{"TimeStart":["2014/06/26 01:05:21"],"TimeUpdated":["2014/06/26 01:06:03"],"Session":["af210e47-0bd6-4609-8f0d-aaa41f1e8aae"]},"sort":[1403744763000]}]}}

with size 2:
{"took":62,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":1994381,"max_score":null,"hits":[{"_index":"network_2014_06_20","_type":"meta","_id":"355a05fc-27dc-4bed-beb3-2df23afeebdb_1","_score":null,"fields":{"TimeStart":["2014/06/20 08:34:29"],"TimeUpdated":["2014/06/20 08:35:28"],"Session":["355a05fc-27dc-4bed-beb3-2df23afeebdb"]},"sort":[1403253328000]},{"_index":"network_2014_06_26","_type":"meta","_id":"250a9777-ff40-46f4-acdf-c9d8ac341d8d_1","_score":null,"fields":{"TimeStart":["2014/06/26 01:05:17"],"TimeUpdated":["2014/06/26 01:06:04"],"Session":["250a9777-ff40-46f4-acdf-c9d8ac341d8d"]},"sort":[1403744764000]}]}}
</comment><comment author="clintongormley" created="2014-07-01T19:52:03Z" id="47702096">OK great - now we're getting somewhere.  Please could you run the same query a few times in a row and make sure that you get back the same results every time.  I'm wondering if your primary and replica shards have diverged.
</comment><comment author="weberr13" created="2014-07-01T20:07:24Z" id="47703737">I re-ran it several times with the same results.  We've actually seen this on no fewer than 8 test machines over the last month or more (I'm not sure when our tester first saw it). 
</comment><comment author="clintongormley" created="2014-07-02T09:34:19Z" id="47755497">hi @weberr13 

We've tried to replicate this on 1.1.2 with lots of randomized testing, and so far have been unable to do so.  Are you able to recreate a small test case which (when run from scratch) replicates the problem?  Alternatively, would you be able to upload all of your data somewhere so that we can replicate the problem locally?

You can send a link to the data to me personally if you'd prefer: clinton.gormley at elasticsearch.com
</comment><comment author="clintongormley" created="2014-07-02T09:48:03Z" id="47756737">You can see the test we're running here: https://github.com/elasticsearch/elasticsearch/commit/79309dcf378855e2b41a88596d46774b5bdd58a0
</comment><comment author="weberr13" created="2014-07-02T17:14:02Z" id="47806879">I'm confused, does that test successfully recreate or do you still need data from me? 
</comment><comment author="s1monw" created="2014-07-02T17:15:30Z" id="47807033">this test does pass all the time I ran it over 10k iterations with random number of shards, nodes etc. so it does NOT recreate the issue
</comment><comment author="clintongormley" created="2014-07-08T11:12:24Z" id="48323422">Hi @weberr13 

Have you managed to put together a test case for this yet? We are unable to investigate further until we can reproduce the problem. If the only way you can do this is to send me your indices, I'd be happy to take a look at them too.
</comment><comment author="weberr13" created="2014-07-08T15:22:36Z" id="48353057">I am still figuring out a way to do this.  I'd like to send you our indexes but they contain Deep Packet Inspection metadata for our engineering department and are not suitable for public consumption.  If there was a way to produce an index that only had the 

"fields": [
      "Session",
      "TimeUpdated",
      "TimeStart"
    ]

data I could send that. 
</comment><comment author="clintongormley" created="2014-07-08T20:34:56Z" id="48396096">@weberr13 Understood - reindexing would be the only way, but that may change something else which "fixes" the issue.  I'm happy to receive your indexes privately - just email me a link (but understand if you can't share them with me)..
</comment><comment author="clintongormley" created="2014-07-23T08:52:55Z" id="49848402">@weberr13 Any chance of that recreation?
</comment><comment author="weberr13" created="2014-07-23T19:09:17Z" id="49920417">At this point I guess we cannot go further.  We have a work around (we don't query for 1 anymore, always doing 2 and discarding one of them) and no way to share our indexes with you.  If I had the free cycles I would to something, but I don't. 
</comment><comment author="s1monw" created="2014-10-28T22:52:42Z" id="60845941">FYI - I think you are hitting #8226 which is now fixed in the `1.3` branch and above. I would love if you could confirm that once it's released?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Examples in language analyzers reference don't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6613</link><project id="" key="" /><description>I'm trying to create an index with an analyzer as shown here (using Python wrapper): http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer

It doesn't work as-is, raising the following exception:

```
[Lady Lark] [my-index] failed to create
org.elasticsearch.indices.IndexCreationException: [my-index] failed to create index
    at org.elasticsearch.indices.InternalIndicesService.createIndex(InternalIndicesService.java:300)
    at org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:343)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:309)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: keyword filter requires either `keywords` or `keywords_path` to be configured
```

Without "english_keywords" or with non-empty keywords it works.
</description><key id="36477949">6613</key><summary>Examples in language analyzers reference don't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">kmike</reporter><labels /><created>2014-06-25T13:16:46Z</created><updated>2016-03-21T15:19:21Z</updated><resolved>2014-07-07T08:07:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-25T13:22:10Z" id="47099502">The keywords are optional and if you don't have any keywords the whole filter should be left out.  Its a documentation problem.
</comment><comment author="clintongormley" created="2014-07-01T13:14:23Z" id="47653884">Hmm, wonder if we should change the docs, or make the keyword filter accept an empty array...
</comment><comment author="kmike" created="2014-07-04T18:31:14Z" id="48064683">Is it really fixed? Copy-paste still won't work because an example contains empty "keywords", and the note is confusing: "Words can be excluded from stemming with the stem_exclusion parameter. This filter should be removed if there are no words to exclude." - what is `stem_exclusion`, and if the filter should be removed why is it present in example?
</comment><comment author="clintongormley" created="2014-07-07T07:43:31Z" id="48149082">Yes, copy and paste won't work.  I've looked at the settings parsing code and an empty array is returned as `null`, so we can't distinguish between an empty array and an unset parameter.

I'll work on explaining this more fully
</comment><comment author="clintongormley" created="2014-07-07T08:07:26Z" id="48150830">Better?
</comment><comment author="joslinm" created="2016-03-15T21:53:53Z" id="197042141">This is happening to me from: https://www.elastic.co/guide/en/elasticsearch/guide/current/algorithmic-stemmers.html
</comment><comment author="crisu83" created="2016-03-21T15:19:04Z" id="199335277">Omitting the `keywords` definition did the job in my case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport client should not send requests to dedicated masters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6612</link><project id="" key="" /><description>When we sample discoverable nodes, we should avoid dedicated masters since their whole purpose is to have very predictable load.
</description><key id="36476078">6612</key><summary>Transport client should not send requests to dedicated masters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>non-issue</label></labels><created>2014-06-25T12:52:59Z</created><updated>2014-06-27T09:40:07Z</updated><resolved>2014-06-27T09:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-06-27T09:36:54Z" id="47325105">@kimchy I'm not sure this is actually an issue.  In my testing the `SniffNodesSampler` [finds only the data nodes](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/client/transport/TransportClientNodesService.java#L500) (which may be a separate issue that we don't get client nodes).  Can you verify what you saw the other day?
</comment><comment author="kimchy" created="2014-06-27T09:38:11Z" id="47325211">@drewr actually, yea, we only go to data nodes, so we are good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.2.1 expects computername pointing to localhost on Mac</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6611</link><project id="" key="" /><description>I downloaded version 1.2.1 on my Mac OS X 10.9.3, typed command `bin/elasticsearch`, but got `java.net.UnknownHostException: oo: oo: nodename nor servname provided, or not known` exception. `oo` is my computer name. It seems that elasticsearch expects computername pointing on `127.0.0.1`, but its not default on Mac. I was able to fix that by changing `/etc/hosts` file, but I don't think its a good default behaviour. Version 1.1 did't need that changes of `/etc/hosts`.
</description><key id="36471797">6611</key><summary>ES 1.2.1 expects computername pointing to localhost on Mac</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hypertornado</reporter><labels /><created>2014-06-25T11:50:58Z</created><updated>2014-09-21T13:22:33Z</updated><resolved>2014-07-18T10:10:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-26T07:45:27Z" id="47196794">Hey,

not aware of any OSX issues right now, so I guess you have some special sort of setup. Are you having a special IPv6 setup?
</comment><comment author="awick" created="2014-06-26T15:36:55Z" id="47241692">I have the same issue.  I'm on 10.8.5.  My hosts file doesn't have my computer name

java.net.UnknownHostException: computername: computername: nodename nor servname provided, or not known
    at java.net.InetAddress.getLocalHost(InetAddress.java:1473)
    at org.elasticsearch.common.network.NetworkUtils.&lt;clinit&gt;(NetworkUtils.java:54)

If I change /etc/hosts to have 
127.0.0.1       localhost computername

then the error goes away
</comment><comment author="hypertornado" created="2014-06-27T09:38:48Z" id="47325259">No, I don't have some special setup. My computer is quite new.
</comment><comment author="spinscale" created="2014-06-27T09:52:50Z" id="47326384">@hypertornado can you copy/paste your `/etc/hosts` and all of the output you get, when you start elasticsearch, including the full stack trace from the logs - I want to see what elasticsearch tries to bind to on your system and I agree we need to make sure, that elasticsearch starts up just fine on any system. I removed that localhost entry from my system and elasticsearch still started up fine, so I might ask a couple of more questions to you guys, so we can pinpoint the problem and hopefully solve it ASAP.
</comment><comment author="hypertornado" created="2014-06-27T10:00:12Z" id="47327142">Thanks. My stack trace when starting elasticsearch is:

```
11:55:47oo@twe&gt; ../elasticsearch-1.2.1/bin/elasticsearch
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8
[2014-06-27 11:55:52,310][INFO ][node                     ] [Razor Fist] version[1.2.1], pid[10464], build[6c95b75/2014-06-03T15:02:52Z]
[2014-06-27 11:55:52,311][INFO ][node                     ] [Razor Fist] initializing ...
[2014-06-27 11:55:52,314][INFO ][plugins                  ] [Razor Fist] loaded [], sites []
[2014-06-27 11:55:53,958][WARN ][common.network           ] failed to resolve local host, fallback to loopback
java.net.UnknownHostException: oo: oo: nodename nor servname provided, or not known
    at java.net.InetAddress.getLocalHost(InetAddress.java:1473)
    at org.elasticsearch.common.network.NetworkUtils.&lt;clinit&gt;(NetworkUtils.java:54)
    at org.elasticsearch.transport.netty.NettyTransport.&lt;init&gt;(NettyTransport.java:197)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:188)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:70)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:203)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
Caused by: java.net.UnknownHostException: oo: nodename nor servname provided, or not known
    at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
    at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:901)
    at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1293)
    at java.net.InetAddress.getLocalHost(InetAddress.java:1469)
    ... 62 more
[2014-06-27 11:55:54,467][INFO ][node                     ] [Razor Fist] initialized
[2014-06-27 11:55:54,467][INFO ][node                     ] [Razor Fist] starting ...
[2014-06-27 11:55:54,538][INFO ][transport                ] [Razor Fist] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.0.1.41:9300]}
[2014-06-27 11:55:57,567][INFO ][cluster.service          ] [Razor Fist] new_master [Razor Fist][c5a-JtA_Q7S-2rwSk7crRQ][localhost][inet[/10.0.1.41:9300]], reason: zen-disco-join (elected_as_master)
[2014-06-27 11:55:57,573][INFO ][discovery                ] [Razor Fist] ondrejElasticSearch/c5a-JtA_Q7S-2rwSk7crRQ
[2014-06-27 11:55:57,599][INFO ][http                     ] [Razor Fist] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.0.1.41:9200]}
[2014-06-27 11:55:58,166][INFO ][gateway                  ] [Razor Fist] recovered [2] indices into cluster_state
[2014-06-27 11:55:58,167][INFO ][node                     ] [Razor Fist] started
[2014-06-27 11:55:58,318][INFO ][cluster.service          ] [Razor Fist] added {[Sidewinder][6ViDpJksRimKz4ecD5a4UQ][oo][inet[/10.0.1.41:9350]]{client=true, data=false, master=false},}, reason: zen-disco-receive(join from node[[Sidewinder][6ViDpJksRimKz4ecD5a4UQ][oo][inet[/10.0.1.41:9350]]{client=true, data=false, master=false}])
```

My `/etc/hosts` is:

```
##
# Host Database
#
# localhost is used to configure the loopback interface
# when the system is booting.  Do not change this entry.
##

#127.0.0.1      oo

127.0.0.1       localhost
255.255.255.255 broadcasthost
::1             localhost
fe80::1%lo0     localhost

```

When I uncomment line with "oo", it works just fine.
</comment><comment author="GaelTadh" created="2014-06-27T10:57:14Z" id="47331148">Looks like this is caused by a java bug fixed in java 8. 
http://bugs.java.com/bugdatabase/view_bug.do?bug_id=7180557
I was able to work around by setting a long hostname : 

```
sudo hostname oo.local
```
</comment><comment author="spinscale" created="2014-06-27T14:47:59Z" id="47354016">does `hostname -l` (long) show only a single hostname without a domain (just to make sure, that all of you have the same problem)? If so, can you change that and retry?

If thats the case, it might make sense to update the docs, in case java7 does not get fixed (doesnt look like it). Luckily this will not happen on servers, if it is DHCP and Mac OSX only.
</comment><comment author="kwizzn" created="2014-07-18T09:46:11Z" id="49413592">Adding my hostname with 127.0.0.1 to /etc/hosts as explained on https://coderwall.com/p/ktbkea worked for me on the same system.
</comment><comment author="mrudult" created="2014-07-29T13:18:13Z" id="50474438">For me setting the hostname of my machine manually worked for me.
`sudo hostname &lt;machine_name&gt;`

In my case it was:

`sudo hostname mrudul@home`
</comment><comment author="mchiodo" created="2014-08-13T15:52:55Z" id="52068212">Adding this line worked for me too:

127.0.0.1 localhost [my host name]
</comment><comment author="mamachanko" created="2014-09-21T13:22:33Z" id="56298619">@awick @mchiodo, thanks for the solution!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove MVEL as a built-in scripting language</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6610</link><project id="" key="" /><description /><key id="36470109">6610</key><summary>Remove MVEL as a built-in scripting language</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>breaking</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-25T11:23:58Z</created><updated>2015-06-06T16:54:26Z</updated><resolved>2014-06-26T08:49:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-25T11:24:24Z" id="47089063">This will also be part of 1.4, to be merged when the branch is created.
</comment><comment author="s1monw" created="2014-06-26T08:17:31Z" id="47199188">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a transformer to translate constant BigDecimal to double</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6609</link><project id="" key="" /><description>This means the constant `1.42` will be compiled using a double instead of Groovy's default BigDecimal without having to add a `f` suffix.
</description><key id="36467773">6609</key><summary>Add a transformer to translate constant BigDecimal to double</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-25T10:44:05Z</created><updated>2015-06-07T13:10:42Z</updated><resolved>2014-06-26T08:54:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-26T08:21:47Z" id="47199528">cool stuff LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Fixed JSON in fielddata docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6608</link><project id="" key="" /><description /><key id="36461201">6608</key><summary>Docs: Fixed JSON in fielddata docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">bobrik</reporter><labels><label>docs</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-25T09:11:56Z</created><updated>2014-07-16T12:51:23Z</updated><resolved>2014-07-01T10:53:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-01T10:52:59Z" id="47642332">Merged, great catch, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scroll throws 403 Forbidden on read-only ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6607</link><project id="" key="" /><description>When calling scroll() method on read-only elasticsearch instance, 403 Forbidden will be returned.
The cause is the use of incorrect HTTP method POST instead of GET, although according to documentation it should be made via GET.
See:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html
</description><key id="36457260">6607</key><summary>Scroll throws 403 Forbidden on read-only ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Armagnac</reporter><labels /><created>2014-06-25T08:11:51Z</created><updated>2014-06-25T09:34:23Z</updated><resolved>2014-06-25T09:34:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="HonzaKral" created="2014-06-25T09:34:23Z" id="47079873">This was resolved in the Python client in PR elasticsearch/elasticsearch-py#98

Thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[guide]  More shard size discussion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6606</link><project id="" key="" /><description>It'd be nice to have more discussion on the shard size in the guide.  Now it says this:

```
While there is no theoretical limit to the amount of data that a primary shard can hold, there is a practical limit. What constitutes the maximum shard size depends entirely on your use case: the hardware you have, the size and complexity of your documents, how you index and query your documents, and your expected response times.
```

Somethings I've found anecdotally that would be nice to have in there if they are true:
- Having more total shards then nodes slows down queries quite a bit.
- While most things parallelize reasonably well highlighting multi term queries and the term and phrase suggester have pretty high per shard costs.
- If you have more then 500 indexes you should keep each index in fewer shards to keep the cluster state smaller.
- Shards bigger then a few gigabytes take a long time to copy.
</description><key id="36428891">6606</key><summary>[guide]  More shard size discussion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2014-06-24T21:39:32Z</created><updated>2015-11-21T15:19:32Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="govindk" created="2014-07-06T17:01:10Z" id="48117047">A simple diagram which re-iterates the impact of lot of shards per index in terms of path each actions takes and impact on resources - memory/nw. Although answer could be "it depends" but at least it becomes apparent. 
- query 
  - aggregations
  - normal queries
  - As Nik mentioned - multi-term/phrase suggester
- indexing
- operations 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Histogram Aggregation bounds offsets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6605</link><project id="" key="" /><description>Hi,

I'm working with elasticsearch and 1st, it's an amazing tool, 2nd I would suggest something about histogram aggregation with extended bounds.
Extended bounds is a good idea but it could be fine if we can force the bounds with, say, a boolean or another method...

For instance:
I have these data : [100,101,102,103,104]
I'd like to create an histogram with an interval of 3.

Elasticsearch will produce : 
[ [99,doc_count:2] , [102,doc_count:3] ]
The problem is first tick always start from 0 but if I want to start my bucket from 100, I can't...

Then I can't obtain : 
[ [100,doc_count:3] , [103,doc_count:2] ]

(The only solution is to perform a script on each data which will shift my data in the right bucket... then : creepy performances)
</description><key id="36385272">6605</key><summary>Aggregations: Histogram Aggregation bounds offsets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">rnonnon</reporter><labels><label>feature</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-24T13:51:03Z</created><updated>2014-07-29T08:37:34Z</updated><resolved>2014-07-24T13:33:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rnonnon" created="2014-06-30T15:07:50Z" id="47543174">no answer???
</comment><comment author="jpountz" created="2014-06-30T15:10:24Z" id="47543499">I think this feature makes sense. Date histograms have ways to move the origin for timezone-related reasons so I think normal histograms should have that feature too.
</comment><comment author="rnonnon" created="2014-07-01T08:01:07Z" id="47627733">Great !
Thanks for your answer ;)
</comment><comment author="zazasa" created="2014-07-21T15:21:57Z" id="49619788">Hi, i need this feature too. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java API - ClusterStateRequest lacks support for "master_node"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6604</link><project id="" key="" /><description>It appears that a flag for returning the "master_node" in the cluster state api is not provided in ClusterStateRequest. In earlier versions of ES (at least in 0.90.13), this was included by default, which does not seem to be the case anymore. 
</description><key id="36385248">6604</key><summary>Java API - ClusterStateRequest lacks support for "master_node"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nilsga</reporter><labels /><created>2014-06-24T13:50:52Z</created><updated>2014-06-25T14:34:29Z</updated><resolved>2014-06-25T12:07:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nilsga" created="2014-06-24T13:55:25Z" id="46973478">This is the code that used to work (0.90.13):

```
ClusterStateResponse clusterStateResponse = esClient.admin().cluster().state(new ClusterStateRequest().all().nodes(false)).actionGet();
DiscoveryNodes nodes = clusterStateResponse.getState().getNodes();
String master = nodes.getMasterNodeId()
```
</comment><comment author="spinscale" created="2014-06-25T12:07:05Z" id="47092310">Hey,

you are right, the API has changed. As the master node id is part of the `getNodes()` response, you need to enable the nodes part in order to get back in version 1.0. You can call `clear().nodes(true)` in order to only get the nodes data back.
</comment><comment author="nilsga" created="2014-06-25T12:23:51Z" id="47093691">Using the rest api, I can't see that the master node information is included in the nodes response?

http://localhost:9200/_cluster/state gives json back with "master_node" info. http://localhost:9200/_cluster/state/nodes only lists the node, with no indication which node is master.

Edit: It does work with setting nodes(true) though, so I guess the Java API does not directly map to the rest API.
</comment><comment author="spinscale" created="2014-06-25T14:34:29Z" id="47108915">The REST API is different, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-state.html

You can use the `master_node` parameter explicitely.

Also, you might want to use the `/_cat/master` endpoint, if you just need to find out the master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms_stats facet with _index as key not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6603</link><project id="" key="" /><description>There seems to be an issue with the terms_stats facet when _index is used as the key field.
(version number: 0.90.13).

The following works:

```
POST _all/_search?search_type=count     
{
  "query": {
    "match_all": {}
  },
  "facets": {
    "size_stats": {
      "terms_stats": {
        "key_field": "_index",
        "value_field": "_size"
      }
    }
  }
}
```

It returns the stats of the _size by type as expected.

The following _does not_ work:

```
POST _all/_search?search_type=count     
{
  "query": {
    "match_all": {}
  },
  "facets": {
    "size_stats": {
      "terms_stats": {
        "key_field": "_index",
        "value_field": "_size"
      }
    }
  }
}
```

It returns:

```
{
 "took" : 290,
 "timed_out" : false,
 "_shards" : {
  "total" : 2661,
  "successful" : 2661,
  "failed" : 0
 },
 "hits" : {
  "total" : 90103169,
  "max_score" : 0.0,
  "hits" : []
 },
 "facets" : {
  "size_stats" : {
   "_type" : "terms_stats",
   "missing" : 0,
   "terms" : []
  }
 }
}
```
</description><key id="36376482">6603</key><summary>terms_stats facet with _index as key not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels /><created>2014-06-24T11:50:32Z</created><updated>2014-06-24T14:09:01Z</updated><resolved>2014-06-24T13:48:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-24T13:26:24Z" id="46970022">I must be blind or exhausted. I can see the difference between the first query and the second one.
</comment><comment author="rore" created="2014-06-24T13:34:11Z" id="46970899">Sorry, copy/paste fluke. 
The first working one should be using _type as the key, like this:

```
{
  "query": {
    "match_all": {}
  },
  "facets": {
    "size_stats": {
      "terms_stats": {
        "key_field": "_type",
        "value_field": "_size"
      }
    }
  }
}
```
</comment><comment author="dadoonet" created="2014-06-24T13:48:08Z" id="46972569">I think it's because `_type` field in indexed by default (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-type-field.html#mapping-type-field). But `_index` is not.

I think you should rely on document content more than on technical meta data, such as `_index` field.

That being said, I think it's not actually an issue and should be discussed on the mailing list and not here.

Closing for now. Feel free to reopen if you think it's an issue.
</comment><comment author="rore" created="2014-06-24T14:09:01Z" id="46975314">OK, got it. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Finding the average value of a column</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6602</link><project id="" key="" /><description>I have a column of floating point values that I want to find the average of. I suspected this might be the best way to query ES:

``` python
params = {
    "query" : {
        "match_all" : {}
    },
    "aggs" : {
        "avg_grade" : { "avg" : { "script" : "doc['weight1'].value" } }
    }
}

resp = requests.post('http://127.0.0.1:9200/myindex/_search?q=country:united_kingdom', data=json.dumps(params))
```

Returns the following:

``` json
{
    "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed;
    shardFailures {[li87cVMZTt6PDTJj1CXtsA][myindex][1]: QueryPhaseExecutionException[[myindex][1]: query[country:united_kingdom],from[0],size[10]: Query Failed [Failed to execute main query]];
    nested: AggregationExecutionException[Unsupported script value [0.207]]; }{[li87cVMZTt6PDTJj1CXtsA][myindex][2]: QueryPhaseExecutionException[[myindex][2]: query[country:united_kingdom],from[0],size[10]: Query Failed [Failed to execute main query]]; 
    nested: AggregationExecutionException[Unsupported script value [0.106]]; }{[li87cVMZTt6PDTJj1CXtsA][myindex][0]: QueryPhaseExecutionException[[myindex][0]: query[country:united_kingdom],from[0],size[10]: Query Failed [Failed to execute main query]];
    nested: AggregationExecutionException[Unsupported script value [0.107]]; }{[li87cVMZTt6PDTJj1CXtsA][myindex][3]: QueryPhaseExecutionException[[myindex][3]: query[country:united_kingdom],from[0],size[10]: Query Failed [Failed to execute main query]];
    nested: AggregationExecutionException[Unsupported script value [0.924]]; }{[li87cVMZTt6PDTJj1CXtsA][myindex][4]: QueryPhaseExecutionException[[myindex][4]: query[country:united_kingdom],from[0],size[10]: Query Failed [Failed to execute main query]];
    nested: AggregationExecutionException[Unsupported script value [2.767]]; }]",
    "status": 500
}
```

The `AggregationExecutionException[Unsupported script value [0.106]]` makes me think I'm probably referencing `{ "avg" : { "script" : "doc['weight1'].value" } }` the wrong way but I can't see how else I could reference this in any of the documentation. Any ideas?
</description><key id="36366673">6602</key><summary>Finding the average value of a column</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marklit</reporter><labels /><created>2014-06-24T09:19:18Z</created><updated>2016-09-18T13:48:46Z</updated><resolved>2014-06-24T09:30:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-24T09:30:34Z" id="46950314">Could you ask your question on the mailing list? We can definitely help there. 

We use github issues for issues and feature requests.

Thanks!

Wondering why you are using scripts though. As you can probably compute avg directly on `weight1` without needing a script. But let's continue this discussion on the mailing list.
</comment><comment author="taf2" created="2016-09-18T13:45:44Z" id="247848576">I assume this is the related conversation https://discuss.elastic.co/t/how-to-get-only-aggregation-values-from-elasticsearch/17674/8
</comment><comment author="taf2" created="2016-09-18T13:48:46Z" id="247848746">Upgrading from 1.7 to 2.x

```
      {
        sum: {
          script: "doc['#{field}'].empty ? 0 : Float.valueOf(doc['#{field}'].value)"
        }
      }
```

The above raises an exception 

```
Caused by: groovy.lang.MissingPropertyException: No such property: Float for class: d3c10a0f498c5e77a152dc524e0fbe131cc46ca2
        at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:53)
        at org.codehaus.groovy.vmplugin.v7.IndyGuardsFiltersAndSignatures.unwrap(IndyGuardsFiltersAndSignatures.java:177)
        at org.codehaus.groovy.vmplugin.v7.IndyInterface.selectMethod(IndyInterface.java:218)
        at d3c10a0f498c5e77a152dc524e0fbe131cc46ca2.run(d3c10a0f498c5e77a152dc524e0fbe131cc46ca2:1)
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript$1.run(GroovyScriptEngineService.java:311)
        at java.security.AccessController.doPrivileged(Native Method)
        at org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:308)
        ... 23 more
```

Removing the Float.value from the expression e.g. 

```
      {
        sum: {
          script: "doc['#{field}'].empty ? 0 : doc['#{field}'].value"
        }
      }
```

Yields:

```
RemoteTransportException[[ctm.local.dev][127.0.0.1:9300][indices:data/read/search[phase/query]]]; nested: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AggregationExecutionException[Unsupported script value [25.0]];
Caused by: QueryPhaseExecutionException[Query Failed [Failed to execute main query]]; nested: AggregationExecutionException[Unsupported script value [25.0]];
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:409)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113)
        at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:364)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:376)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:368)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:365)
        at org.elasticsearch.transport.TransportService$4.doRun(TransportService.java:350)
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: AggregationExecutionException[Unsupported script value [25.0]]
        at org.elasticsearch.search.aggregations.support.values.ScriptDoubleValues.setDocument(ScriptDoubleValues.java:74)
        at org.elasticsearch.search.aggregations.metrics.sum.SumAggregator$1.collect(SumAggregator.java:79)
        at org.elasticsearch.search.aggregations.LeafBucketCollector$3.collect(LeafBucketCollector.java:73)
        at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.collectExistingBucket(BucketsAggregator.java:80)
        at org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator$2.collect(GlobalOrdinalsStringTermsAggregator.java:130)
        at org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:88)
        at org.apache.lucene.search.MultiCollector$MultiLeafCollector.collect(MultiCollector.java:173)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:218)
        at org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:169)
        at org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:821)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:535)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:384)
        ... 10 more
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch service start error ~ </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6601</link><project id="" key="" /><description>centos 6.5 
elasticsearch 1.2.1  yum install 

[root@ip-10-0-1-75 elasticsearch]# tail  /var/log/elasticsearch/elasticsearch.log
                NoClassDefFoundError[org/elasticsearch/ElasticSearchException]
                        ClassNotFoundException[org.elasticsearch.ElasticSearchException]
[2014-06-24 02:25:42,112][INFO ][node                     ] [American Eagle] version[1.2.1], pid[1573], build[6c95b75/2014-06-03T15:02:52Z]
[2014-06-24 02:25:42,113][INFO ][node                     ] [American Eagle] initializing ...
[2014-06-24 02:25:42,160][INFO ][plugins                  ] [American Eagle] loaded [transport-couchbase], sites [paramedic, head]
[2014-06-24 02:25:44,285][ERROR][bootstrap                ] {1.2.1}: Initialization Failed ...
- ExecutionError[org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchException]
      ExecutionError[java.lang.NoClassDefFoundError: org/elasticsearch/ElasticSearchException]
              NoClassDefFoundError[org/elasticsearch/ElasticSearchException]
                      ClassNotFoundException[org.elasticsearch.ElasticSearchException]

[root@ip-10-0-1-75 elasticsearch]# java -version
java version "1.7.0_55"
OpenJDK Runtime Environment (rhel-2.4.7.1.el6_5-x86_64 u55-b13)
OpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode)
</description><key id="36349403">6601</key><summary>elasticsearch service start error ~ </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JudyDBA</reporter><labels /><created>2014-06-24T02:53:01Z</created><updated>2014-06-24T10:44:59Z</updated><resolved>2014-06-24T10:44:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-24T10:44:59Z" id="46956554">The Couchbase plugin you are using is not working with Elasticsearch 1.0 (at least in the version you are using, maybe there has been an upgrade?), so you either need to upgrade or file a bug report at the home of the couchbase plugin - you can try disabling it and then elasticsearch should startup flawless. 

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>/_cat/nodes outputting negative ram percents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6600</link><project id="" key="" /><description>On SmartOS (Solaris variant), getting:

```
~  curl 'n1:9200/_cat/nodes?v'
host      ip        heap.percent ram.percent load node.role master name            
localhost 127.0.0.1          18        -189 3.15 d        -      n1      
localhost 127.0.0.1          17        -36 2.98 d        -      n2      
localhost 127.0.0.1          17        -346 3.22 d        -      n3      
localhost 127.0.0.1            9        -86 3.21 d        -      n4      
localhost 127.0.0.1            9        -273 3.10 d        -      n5      
etc
```
</description><key id="36341976">6600</key><summary>/_cat/nodes outputting negative ram percents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Stats</label><label>bug</label></labels><created>2014-06-23T23:47:55Z</created><updated>2016-04-04T17:51:39Z</updated><resolved>2016-04-04T17:51:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-23T23:48:21Z" id="46916524">This is on ES 1.2.0
</comment><comment author="clintongormley" created="2015-11-21T15:18:57Z" id="158651765">@tlrx with the changes that you made to stats, do you know if this is still relevant?
</comment><comment author="dakrone" created="2016-04-04T17:51:39Z" id="205414159">Closing this as non-relevant since it has been completely changed since 1.2
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add transform to document before index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6599</link><project id="" key="" /><description>Closes #6566
</description><key id="36340114">6599</key><summary>Add transform to document before index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>:Mapping</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-23T23:12:56Z</created><updated>2015-06-06T18:30:48Z</updated><resolved>2014-07-15T16:51:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-23T23:14:17Z" id="46914320">Still needs documentation.

I'm not super happy with how the transform is done - it turns the XContent into a Map them back into an XContent.  Not perfect at all.

Also, this has a bunch of TODOs, some of which deserve doing before merging, I imagine.
</comment><comment author="jpountz" created="2014-07-04T08:37:20Z" id="48020570">Thanks @nik9000 this looks very good! API-wise, I think it would be nice to be prepared for other ways to do transformations in the future? Although scripts are powerful, it think we could have dedicated transformers for common transformations like concatenating two string fields or suming up all values of a numeric field (which probably means we also need to support several transformers?).
</comment><comment author="clintongormley" created="2014-07-04T14:05:38Z" id="48047436">As per @jpountz 's comment, we could keep the path open for built-in transforms later in a format like this perhaps:

```
"transform": [
    { "add": { "total_price": [ "price", "tax", "markup" ]}},
    { "concat": { "full_name": [ "first", "last" ]}},
    { "script": "......", "lang": "groovy", "params": {....}}
]
```
</comment><comment author="nik9000" created="2014-07-09T14:08:23Z" id="48475528">Would it make sense to support either the

``` js
"transform": [
    { "add": { "total_price": [ "price", "tax", "markup" ]}},
    { "concat": { "full_name": [ "first", "last" ]}},
    { "script": "......", "lang": "groovy", "params": {....}}
]
```

or the

``` js
"transform":     { "script": "......", "lang": "groovy", "params": {....}}
```

syntax?  We do something like this in other places I think.
</comment><comment author="clintongormley" created="2014-07-09T14:13:37Z" id="48476175">Yes, that's what I actually meant.  Currently we don't have the short syntax, but as long as the way is open to support both in the future, that's all we need.
</comment><comment author="nik9000" created="2014-07-10T16:57:37Z" id="48632633">Almost done with @jpountz's code comment on copy-and paste-y-ness.  Going to start on api next.

It is good we're doing with with Groovy, BTW, because I tried a month ago to execute MVEL on every one of my updates and saw some real instability.  Weird error message that I couldn't get in dev when doing the same things.
</comment><comment author="nik9000" created="2014-07-10T17:18:35Z" id="48635303">I pushed a fix for the code duplication and rebased because it wasn't going to merge cleanly.  Now to fix the syntax.
</comment><comment author="nik9000" created="2014-07-10T18:21:24Z" id="48643593">Almost done with API - I've noticed the integration tests are starting much much much faster then a month ago.  Neat!
</comment><comment author="nik9000" created="2014-07-10T18:32:12Z" id="48645004">OK!  Added @clintongormley's API reformat.  I think that made it cleaner actually.  I added the infrastructure for multiple transforms of different types but only implemented the script type.
</comment><comment author="clintongormley" created="2014-07-10T18:35:49Z" id="48645462">w00t! I've tagged it for review.  thanks @nik9000 
</comment><comment author="jpountz" created="2014-07-11T08:40:04Z" id="48707184">I left two minor comments but in general this looks very good to me! This is an exciting feature.
</comment><comment author="nik9000" created="2014-07-14T06:27:28Z" id="48868968">@jpountz: updated with fixes from comments.
</comment><comment author="jpountz" created="2014-07-15T12:54:05Z" id="49026876">Thanks Nik, I'll merge it soon!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripting: Wrap groovy script exceptions in a serializable Exception object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6598</link><project id="" key="" /><description>I'm playing around with groovy and I think exceptions aren't serializing properly:

```
[2014-06-23 18:19:54,091][INFO ][org.elasticsearch.index.mapper] Action Failed
org.elasticsearch.transport.RemoteTransportException: [node_0][inet[/192.168.0.101:9300]][index]
Caused by: org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)
    at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)
    at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
    at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.InvalidClassException: failed to read class descriptor
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1603)
    at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
    at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1483)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1333)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
    at java.lang.Throwable.readObject(Throwable.java:914)
    at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
    at java.lang.Throwable.readObject(Throwable.java:914)
    at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)
    at java.lang.Throwable.readObject(Throwable.java:914)
    at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:167)
    ... 23 more
Caused by: java.lang.ClassNotFoundException: Script4
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.loadClass(ThrowableObjectInputStream.java:93)
    at org.elasticsearch.common.io.ThrowableObjectInputStream.readClassDescriptor(ThrowableObjectInputStream.java:67)
    at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1601)
    ... 62 more
```

I'm in the middle of work on #6566 so I don't have easy reproduction steps, but I'll see if I can make some soon.
</description><key id="36337116">6598</key><summary>Scripting: Wrap groovy script exceptions in a serializable Exception object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>bug</label></labels><created>2014-06-23T22:23:19Z</created><updated>2014-07-16T13:13:05Z</updated><resolved>2014-06-30T14:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-23T22:28:14Z" id="46910554">Ping @dakrone.  I'll work on it in a bit but I think it has to do with when a variable isn't found. 
</comment><comment author="dakrone" created="2014-06-23T22:29:04Z" id="46910619">@nik9000 sounds good, I definitely want to figure out what's causing this.
</comment><comment author="nik9000" created="2014-06-23T22:41:13Z" id="46911659">@dakrone, got it:  start a bunch of servers.  Enough that your request goes to more then one.  Then do this:

``` js
curl -XPOST "http://localhost:9200/test/test/1?pretty" -d '{"content": "findme"}'
curl -XPOST "http://localhost:9200/test/test/2?pretty" -d '{"title": "cat", "content": "findme"}'
curl -XPOST "http://localhost:9200/test/test/3?pretty" -d '{"title": "table", "content": "findme"}'
curl -XPOST "http://localhost:9200/test/_refresh?pretty"
curl -XPOST "http://localhost:9200/test/test/_search?pretty" -d '{
    "query": {
        "filtered": {
            "filter": {
                "script": {
                    "script": "1 == not_found",
                    "lang": "groovy"
                }
            }
        }
    }
}'
```

I don't imagine the contents of the documents matter - just that they end up on a bunch of nodes.
Here is what that spits out for me:

``` js
{
  "took" : 35,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 2,
    "failed" : 3,
    "failures" : [ {
      "index" : "test",
      "shard" : 4,
      "status" : 500,
      "reason" : "QueryPhaseExecutionException[[test][4]: query[filtered(ConstantScore(ScriptFilter(1 == not_found)))-&gt;cache(_type:test)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: MissingPropertyException[No such property: not_found for class: Script5]; "
    }, {
      "index" : "test",
      "shard" : 3,
      "status" : 500,
      "reason" : "RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: InvalidClassException[failed to read class descriptor]; nested: ClassNotFoundException[Script3]; "
    }, {
      "index" : "test",
      "shard" : 2,
      "status" : 500,
      "reason" : "RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: InvalidClassException[failed to read class descriptor]; nested: ClassNotFoundException[Script2]; "
    } ]
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```
</comment><comment author="nik9000" created="2014-06-23T22:42:03Z" id="46911734">3 servers did it for me.
</comment><comment author="kimchy" created="2014-06-24T10:55:18Z" id="46957363">I think it comes from trying to serialize the groovy exceptions objects, I would suggest we catch a script execution exception, but not have the throwable in our wrapping script exception. Users might run client side node/transport clients that don't have groovy in the class path for example.
</comment><comment author="nik9000" created="2014-06-24T13:13:15Z" id="46968648">I think @kimchy's right.  What I'm seeing is that groovy decided to name the compiled classes something different but even without that the chance that the user doesn't even have groovy in their classpath means we should transform the exception without adding the cause.  I imagine MVEL didn't have this problem because MVEL was required and was interpreted instead of compiled to byte code.
</comment><comment author="dakrone" created="2014-06-30T15:14:53Z" id="47544133">Thanks for bringing this up @nik9000!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>path.data - When a drive fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6597</link><project id="" key="" /><description>Quick question, as I can't seem to find an answer to this anywhere.  If you have multiple data paths (over 15) and a drive simply fails, what/home does ElasticSearch react to such an event?  Does the cluster encounter a performance hit? Is the data gone, but only for the data that was on that drive?  Once the drive is replaced, if the data is not lost, does ElasticSearch re-balance the data to reflect the drive?

These are all questions that I'd like some clarification on before going down this approach.

Thanks in advanced.
Drew
</description><key id="36327837">6597</key><summary>path.data - When a drive fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewrockshard</reporter><labels /><created>2014-06-23T20:26:42Z</created><updated>2014-07-01T12:11:20Z</updated><resolved>2014-07-01T12:11:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ferdynice" created="2014-06-24T08:09:19Z" id="46943109">Afaik even the newest release (1.2.1) is not able to handle it. For example when you start es and one of the drives have some i/o read-only error, it fails inmediately. When you have a lot of disks in a jbod fashion, you can use the trick I use, that is running 1 es node per disk instead per server.

Let's say you have 6 disks therefore 6 data paths, mounted on /mnt/disk?/elastic. You can start es with:

`for i in {1..6}; do elasticsearch-1.2.1/bin/elasticsearch -d --path.data=/mnt/disk$i/elastic --path.logs=elasticsearch-1.2.1/logs/$i --node.name=`hostname -s`_disk$i --node.machine=`hostname -s` --cluster.routing.allocation.awareness.attributes=machine; done`

The disks that are broken will prevent es from starting, but still you can enjoy some processing on the faulty server because of the remaining disks. The 'node.machine' property with the allocation property makes sure balancing is not messed up. Also obviously you need to reduce the heap as you are running multiple processes.

I'm not sure if this is best practise, it works for me.
</comment><comment author="clintongormley" created="2014-07-01T12:11:20Z" id="47648109">@ferdy-galema Note that you should also be setting the number of `processors` in the config file, otherwise each node on the same box will assume that it alone has access to all the cores, and will create too many threads.

@drewrockshard this is something which should be discussed on the mailing list rather than in the issues list
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Groovy sandboxing for GString-based method invocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6596</link><project id="" key="" /><description>This forbids things like:

```
obj."${'get' + 'Class'}"()."${'getDeclared' + 'Method'}"("myPrivateMethod")."${'set' + 'Accessible'}"(true)
```
</description><key id="36297130">6596</key><summary>Add Groovy sandboxing for GString-based method invocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-23T14:40:35Z</created><updated>2015-06-07T13:10:53Z</updated><resolved>2014-06-25T10:10:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-23T14:42:32Z" id="46854063">LGTM
</comment><comment author="s1monw" created="2014-06-24T10:09:20Z" id="46953761">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sum of a column multiplied by a sibling column not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6595</link><project id="" key="" /><description>I have an Elasticsearch index with a country name column and a multiplier column:

```
country | multiplier
GBR     | 1
GBR     | 2
USA     | 1
USA     | 1
USA     | 2
```

I'd like to get statistical facets where the count of each is multiplied on a per-document basis and the output would look like:

``` json
"facets": {
    "tags": {
        "_type": "terms",
        "missing": 0,
        "total": 7,
        "other": 0,
        "terms": [{
            "term": "GBR",
            "count": 3
        }, {
            "term": "USA",
            "count": 4
        }]
    }
}
```

This was my best guess at how to query elisticsearch to get this output but it's not multiplying the counts:

``` bash
curl -XGET 'http://localhost:9200/_search' -d '{
    "query": {
        "match_all": {}
    },
    "facets" : {
      "tags" : { "terms" : {"field" : "country"} },
      "script": "_count * doc['multiplier'].value"
    }
}
'
```

Results in:

``` json
...
"facets": {
    "tags": {
        "_type": "terms",
        "missing": 0,
        "total": 5,
        "other": 0,
        "terms": [{
            "term": "GBR",
            "count": 2
        }, {
            "term": "USA",
            "count": 3
        }]
    }
}
...
```

Any ideas how I could multiple each result by their respective sibling multiplier columns?
</description><key id="36296523">6595</key><summary>Sum of a column multiplied by a sibling column not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marklit</reporter><labels /><created>2014-06-23T14:34:34Z</created><updated>2014-06-30T08:26:24Z</updated><resolved>2014-06-30T08:26:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Vadi" created="2014-06-24T18:30:55Z" id="47011346">Using [Aggregations](http://bit.ly/1jLvotz) what you're trying should be possible. Anyways, Aggregations is a good facelift to Facets. If possible, you should use it -

``` json
{
  "query": {
    "match_all": {}
  },
  "aggs": {
    "group_by_country": {
      "terms": {
        "field": "country"
      },
      "aggs": {
        "sum_of_multipliers": {
          "sum": {
            "field": "multiplier"
          }
        }
      }
    }
  }
}
```

The result would be -

``` json
"aggregations": {
      "group_by_country": {
         "buckets": [
            {
               "key": "usa",
               "doc_count": 3,
               "sum_of_multipliers": {
                  "value": 4
               }
            },
            {
               "key": "gbr",
               "doc_count": 2,
               "sum_of_multipliers": {
                  "value": 3
               }
            }
         ]
      }
   }
```

Let me know if this helps
</comment><comment author="marklit" created="2014-06-30T08:26:24Z" id="47506211">It does help. Thank you Vadi.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>term,phrase suggester couldn't support double-byte word like Chinese.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6594</link><project id="" key="" /><description>I test these two suggesters.Find, they just support english,and don't support Chinese.So, I suspect they just support one-byte word like English.
</description><key id="36294232">6594</key><summary>term,phrase suggester couldn't support double-byte word like Chinese.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">LiuGangR</reporter><labels><label>:Suggesters</label></labels><created>2014-06-23T14:11:42Z</created><updated>2016-11-25T18:03:35Z</updated><resolved>2016-11-25T18:03:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-24T10:37:06Z" id="46955901">can you elaborate on this a little bit, IMO they just depend on the analyzer - can you maybe show what the problem is as an example?
</comment><comment author="LiuGangR" created="2014-06-25T08:28:24Z" id="47073823">I use term suggester like this http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters.html .

And my url is this:
http://10.20.9.50:9200/testindex/_suggest
param is : 
{
  "my-suggestion" : {
    "text" : "zhong guo",
    "term" : {
      "field" : "pinyin"
    }
  }
}

and I have 2000 word in field---"pinyin"
</comment><comment author="s1monw" created="2014-06-25T08:52:35Z" id="47075833">ok so what is the actual response what is the expected response. I can't see any problems with what you are doing here yet...
</comment><comment author="LiuGangR" created="2014-06-25T09:43:32Z" id="47080707">And the another problem is about completion suggester.
My url is :http://10.20.9.50:9200/testindex/_suggest
my param is:http:
{
    "suggest_com-suggest" : {
        "text" : "&#20013;&#22269;&#36275;&#29699;",
        "completion" : {
            "field" : "suggest",
            "fuzzy" : {
                "fuzziness" : "0.5",
                "unicode_aware":"true"
            }
        }
    }
}
And my response is:

{
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "suggest_com-suggest": [
        {
            "text": "&#20013;&#22269;&#36275;&#29699;",
            "offset": 0,
            "length": 4,
            "options": [
                {
                    "text": "&#20013;&#22269;&#20154;",
                    "score": 34,
                    "payload": {
                        "frequence": 121
                    }
                },
                {
                    "text": "&#20013;&#22269;&#20154;&#27665;",
                    "score": 34,
                    "payload": {
                        "frequence": 121
                    }
                },
                {
                    "text": "&#20013;&#22269;&#31726;&#29699;&#38431;",
                    "score": 34,
                    "payload": {
                        "frequence": 121
                    }
                },
                {
                    "text": "&#20013;&#22269;&#36275;&#29699;&#38431;",
                    "score": 34,
                    "payload": {
                        "frequence": 121
                    }
                },
                {
                    "text": "&#20013;&#22269;",
                    "score": 1,
                    "payload": {
                        "frequence": 273
                    }
                }
            ]
        }
    ]
}

How can I get the options is sorted by correlativity.For example:
When i search "&#20013;&#22269;&#36275;&#29699;", I want the "&#20013;&#22269;&#36275;&#29699;&#38431;" at the first. Because, it is the most correlativily word.
</comment><comment author="LiuGangR" created="2014-06-25T09:45:57Z" id="47080936">sorry, the respond is : 
{
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "my-suggestion": [
        {
            "text": "zhong",
            "offset": 0,
            "length": 5,
            "options": []
        },
        {
            "text": "guo",
            "offset": 6,
            "length": 3,
            "options": []
        }
    ]
}

it just token the text, and not give me any suggestion.
</comment><comment author="ymiao" created="2016-11-23T07:37:03Z" id="262449544">is this issue resolved?</comment><comment author="clintongormley" created="2016-11-25T18:03:35Z" id="263005955">This sounds like all that was needed was the `unicode_aware` parameter. I don't understand the second half</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion stats walk through all fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6593</link><project id="" key="" /><description>To compute completion stats we currently walk through all the lucene fields and all segments in our shards. This is inefficient as we only need to walk through fields used for completion. This becomes even more extreme for people not using the completion suggester.

Relevant code: 
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java#L360
</description><key id="36277760">6593</key><summary>Completion stats walk through all fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label></labels><created>2014-06-23T10:14:21Z</created><updated>2015-11-29T11:17:53Z</updated><resolved>2015-11-29T11:17:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T15:17:53Z" id="158651702">@bleskes this is still relevant in master?
</comment><comment author="bleskes" created="2015-11-22T20:29:38Z" id="158795522">the code is still the same. I don't remember anymore why this was a problem. Maybe @jpountz or @mikemccand can have a look and make suggestions? code is here these days: https://github.com/elastic/elasticsearch/blob/6b2f3a9ad2a8bd7c63d5e179560426a43ecaf9e8/core/src/main/java/org/elasticsearch/search/suggest/completion/CompletionFieldStats.java#L45
</comment><comment author="clintongormley" created="2015-11-28T14:52:55Z" id="160306968">I've seen the completion stats showing up in hotthreads output, so sounds like it is still an issue
</comment><comment author="mikemccand" created="2015-11-29T00:22:54Z" id="160348989">I've seen this in hot threads as well, but I think it's only really an issue for users with a very large number of fields ... and that will also cause other problems (heavy heap usage).

Is there an API in mappings to efficiently (less than `O(numFields)`) just ask for the completion fields?  If so we should use it; if not, I think this is just a cost of having a huge number of fields.

But, @areek, is that TODO still true?  I.e. asking for stats will cause all suggesters to load their FSTs if they were not already loaded?  I thought we fixed this before merging the new completion suggester?
</comment><comment author="bleskes" created="2015-11-29T09:55:42Z" id="160395307">&gt; Is there an API in mappings to efficiently (less than O(numFields)) just ask for the completion fields? If so we should use it; if not, I think this is just a cost of having a huge number of fields.

we currently don't have a look up by type mechanism and  I don't think it's needed - walking all your  in memory map of the mapping should be fine. I was just wondering if the Lucene code does more than just in memory map walking. If not, current code is fine with me and we can close this.
</comment><comment author="mikemccand" created="2015-11-29T10:33:13Z" id="160397673">&gt; I was just wondering if the Lucene code does more than just in memory map walking.

Lucene really doesn't do much under that `atomicReader.fields().terms(fieldName)` call: it's just 2 `TreeMap` lookups (those `Terms` instances are already loaded on init of the `SegmentReader`).  It also calls `ensureOpen` each time, which calls `AtomicInteger.get`.  I guess we could make a trivial improvement to hold onto `atomicReader.fields()` locally ... I'll do that, but it's minor (just saves the `ensureOpen` for each field).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch timestamp</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6592</link><project id="" key="" /><description>Hi! 
I have some trouble with creating records after mapping with timestamp.
At first I create mapping like this: 

curl -XPUT localhost:9200/tests -d'{
"mappings": {
"_default_": {
"_timestamp":{"enabled" : true, "store": "yes", "path": "post_date"},
 "properties": {"post_date": {"type": "date"},"text": {"type": "string"}}
}}}'

The result of mapping request: 

curl -XGET localhost:9200/tests/_mapping?pretty=true{
  "tests" : {
    "mappings" : {
      "_default_" : {
        "_timestamp" : {
          "enabled" : true,
          "store" : true,
          "path" : "post_date"
        },
        "properties" : {
          "post_date" : {
            "type" : "date",
            "format" : "dateOptionalTime"
          },
          "text" : {
            "type" : "string"
          }
        }
      }
    }
  }
}

After that I'm trying to create new record : 

curl -XPUT localhost:9200/tests/message/1 -d'{"text" : "First testing message"}'

And it returns: 

{"error":"ElasticsearchParseException[failed to parse doc to extract routing/timestamp/id]; nested: TimestampParsingException[failed to parse timestamp [null]]; ","status":400}

Can't uderstand what is the problem. I tried to make mapping only for "message" by replacing "_default_" by "message" and tried to make path like this: "path" : "message.post_date", but it is still not working.
</description><key id="36273845">6592</key><summary>elasticsearch timestamp</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">artImonkey</reporter><labels /><created>2014-06-23T09:15:28Z</created><updated>2014-07-28T12:00:37Z</updated><resolved>2014-06-23T10:20:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-06-23T10:03:58Z" id="46826044">It looks like you aren't providing the post_date field in your new record.  

Your mapping indicates that the _timestamp value should be sourced from the post_date field in each record so you will need to provide a value in each record you index.

try the following curl command instead: 

```
curl -XPUT localhost:9200/tests/message/1 -d'{
    "text" : "First testing message", 
    "post_date" : "2009-11-15T14:12:12" 
}'
```

For more information on how to use the _timestamp feature, take a look at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-timestamp-field.html
</comment><comment author="artImonkey" created="2014-06-23T10:19:59Z" id="46827252">collings86, Thanks!  I thought wrong. I just needed automatic adding date for every post and i thought that the date will be stored in "path". Now I removed path and it's working properly. 
</comment><comment author="a0s" created="2014-07-28T11:46:32Z" id="50328502">~~Could you explain how I can create a field _timestamp that automatically update with Time.now (server side) on every update operation? After PUT _mapping i don't see any _timestamp near _index, _type and _id. I dont want to update timestamp by hand from clientside.~~
_timestamp field is not returning by default until ask fields:["*"]. This fact frustrated me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Removed Sense from frontends as it is not available anymore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6591</link><project id="" key="" /><description /><key id="36273642">6591</key><summary>Removed Sense from frontends as it is not available anymore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels /><created>2014-06-23T09:12:25Z</created><updated>2014-07-01T06:32:49Z</updated><resolved>2014-07-01T06:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-30T09:14:46Z" id="47510501">Good one. Can you sign the CLA - I'll put it in: http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="fhopf" created="2014-06-30T11:07:03Z" id="47519451">Missed that, done now.
</comment><comment author="spinscale" created="2014-07-01T06:32:49Z" id="47621506">thanks!

closed by https://github.com/elasticsearch/elasticsearch/commit/c5cf28351790da434882063ccf68e0bdbc15290c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed link to native Java client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6590</link><project id="" key="" /><description /><key id="36273095">6590</key><summary>Fixed link to native Java client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels /><created>2014-06-23T09:04:27Z</created><updated>2014-07-01T12:02:10Z</updated><resolved>2014-07-01T12:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch accepts requests to write indices with bad characters that cannot be written to disk by java </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6589</link><project id="" key="" /><description>Elasticsearch 1.1.1 appears to accept requests to create an index with invalid characters that cannot be written to disk as files or directories by java. 

It should instead reject the request with invalid characters detected in index name or some similar error. Or perhaps save the disk files/directories with escaped characters and translate them back to unescaped when needed.

The request then floats around in the cluster unpersisted, unable to be written to cluster state on master nodes or data files on data nodes.

Logs show the error in two ways.

This example is an index name with ^@^@ chars on the end, (or so the logs tell me).

Master nodes complain with:

```
[2014-06-21 17:28:02,185][WARN ][gateway.local.state.meta ] [masternode1.whateverdomain] [0e5f2bd5e517b93056cd3ef7d51223c5^@^@]: failed to state
java.io.FileNotFoundException: Invalid file path
    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:215)
    at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:171)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:359)
    at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:217)
    at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:431)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
```

Data nodes complain with:

```
[2014-06-21 18:41:02,758][WARN ][indices.cluster          ] [datanode1.whateverdomain] [0e5f2bd5e517b93056cd3ef7d51223c5^@^@][6] failed to create shard
org.elasticsearch.index.shard.IndexShardCreationException: [0e5f2bd5e517b93056cd3ef7d51223c5^@^@][6] failed to create shard
    at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:342)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:628)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:546)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:178)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:425)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Invalid file path
```

Shards will continually flow around the available data nodes attempting to initialise but unable to do so, then trying on another node. This writes lots of lines to all logs and eats a chunk of cpu on all nodes involved.

The cluster goes into the red state due to being unable to allocate primary shards, even though the rest of the indices are fine.

You will be unable to delete this rogue index as it has not yet been persisted to disk for either state or data files, so there is nothing to reference to delete, it doesn't yet exist.

The only way to bring your cluster back into green state is to perform a full cluster shutdown and restart.

Is there a way to reset cluster state live, as per a full cluster shutdown without actually performing a full cluster restart of all the nodes? Via an api command that tells the cluster to reset only cluster state from a fresh state as per restarting?
</description><key id="36261976">6589</key><summary>Elasticsearch accepts requests to write indices with bad characters that cannot be written to disk by java </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels /><created>2014-06-23T04:44:50Z</created><updated>2014-08-08T13:41:17Z</updated><resolved>2014-08-08T13:41:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="geekpete" created="2014-06-23T04:52:37Z" id="46806236">Some more info:

Linux some-node-hostname 3.2.0-4-amd64 #1 SMP Debian 3.2.57-3 x86_64 GNU/Linux

Filesystem is Ext4.
</comment><comment author="spinscale" created="2014-06-24T11:56:21Z" id="46961862">Hey,

looks like some weird control characters (however we need to know which one). Can you reproduce this using curl or a shell script or sense? Also, maybe you can check the data.path how the directory is actually looking like on the filesystem in order to check those?
</comment><comment author="geekpete" created="2014-06-24T23:19:14Z" id="47043084">I've done a quick test on my mac, OSX seems to write the filename/dir out just fine, but I'll need to confirm the java version. Probably difference in filesystem allows it.

I'll try to replicate the test on the same linux/java versions if I can and let you know how that goes.
</comment><comment author="clintongormley" created="2014-08-08T13:41:17Z" id="51601463">Close in favour of #6736 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>how to query documents contain nested field's element length &gt; 0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6588</link><project id="" key="" /><description>Hi,

Here is school entity mapping

```
school:
    mappings:            
        name: ~
        rankings:
            type: "nested"
                properties:
                    id: ~
```

 I get a problem with query for finding all school contains rankings element length &gt; 0. I try use filter script but it throws error. I don't know how to fix it. Pls help me resolve this issue. Thanks.
</description><key id="36258322">6588</key><summary>how to query documents contain nested field's element length &gt; 0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hoannv</reporter><labels /><created>2014-06-23T02:23:45Z</created><updated>2014-06-23T05:00:30Z</updated><resolved>2014-06-23T05:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-23T05:00:30Z" id="46806475">You should ask this on the mailing list. You will get more readers there.
Issues are for issues and feature requests.

Also providing more details could help. See http://www.elasticsearch.org/help
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 1.2.1  - boolean multifield silently ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6587</link><project id="" key="" /><description>I'm seeing multi-fields of type boolean silently being reduced to a normal boolean field in 1.2.1 which wasn't the behavior in 0.90.9. See https://gist.github.com/Omega359/0c2a93690b4db30693a1 for an example of this.

To me it seems like it should work - the boolean field mapper seems to be calling out to multiFieldsBuilder - but I'm not versed enough in the internals of ES to know where if at all it's broken.
</description><key id="36258247">6587</key><summary>ES 1.2.1  - boolean multifield silently ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Omega359</reporter><labels><label>:Mapping</label><label>bug</label><label>discuss</label></labels><created>2014-06-23T02:20:53Z</created><updated>2015-12-24T09:21:50Z</updated><resolved>2015-12-24T09:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-24T10:38:54Z" id="46956053">@martijnvg can you take a look at this?
</comment><comment author="KlausBrunner" created="2014-10-02T09:03:48Z" id="57601531">We're currently hitting the same problem. Our mapping (which used to work fine in 0.90.x and is necessary to allow certain queries based on user input) is:

```
 {
   "template_boolean": {
    "match": "*",
    "match_mapping_type": "boolean",
    "mapping": {
     "type": "multi_field",
     "fields": {
      "{name}": {
       "index": "not_analyzed",   // or "analyzed", doesn't really matter
       "type": "boolean",
       "include_in_all": true
      },
      "untouched": {
       "type": "boolean",
       "index": "not_analyzed"
      }
     }
    }
   }
  }
```

Which is silently ignored, the result is simply a {"type":"boolean"}. For other types than boolean, the same kind of template still works fine.
</comment><comment author="KlausBrunner" created="2014-10-07T08:22:18Z" id="58151786">@martijnvg I just realised we were still using the pre-1.0 syntax for multifields. However, it doesn't really improve with the proper new syntax: https://gist.github.com/KlausBrunner/9016653829d295ae96f2

Also, existing multifield mappings work just fine - but it's seemingly impossible to create new ones.
</comment><comment author="KlausBrunner" created="2014-10-14T08:02:21Z" id="59003614">@clintongormley Not to be a nuisance, but we're a bit bothered by this bug and I don't see an easy/obvious fix from browsing the code. Could someone look into this?
</comment><comment author="clintongormley" created="2014-10-16T12:10:13Z" id="59351666">@KlausBrunner yes, currently there is no `fields` support for fields of type `boolean`.  Out of interest, what are you trying to achieve with this mapping?  The example you give just maps the same value in the same way twice.

I could imagine having a mapping like this:

```
{
  "mappings": {
    "test": {
      "properties": {
        "some_boolean_field": {
          "type": "string",
          "index": "not_analyzed",
          "fields": {
            "raw": {
              "type": "boolean"
            }
          }
        }
      }
    }
  }
}
```

which would index a not-analyzed string version and a boolean version, but the example you give doesn't make sense.
</comment><comment author="KlausBrunner" created="2014-10-16T13:30:12Z" id="59360698">@clintongormley You're right that the exact mapping in my example doesn't make a lot of sense, but in the case of booleans it doesn't matter for us anyway. We defined an additional "raw" (not_analyzed) mapping for all types of fields and rely on it to exist when we build queries. Until we switched to 1.x, this worked fine - now we need to have special treatment just for boolean fields, which is quite annoying.
</comment><comment author="Omega359" created="2014-10-16T13:32:12Z" id="59360966">I'll echo Klaus - we have exactly the same scenario and special casing boolean fields just seems broken.
</comment><comment author="clintongormley" created="2014-10-16T18:44:08Z" id="59409797">@KlausBrunner and @Omega359 so you'd rather index double the data?  That seems odd to me...
</comment><comment author="Omega359" created="2014-10-16T18:48:27Z" id="59410449">Than have custom code to handle a single type definition that doesn't behave like every other type we use? Yes.
</comment><comment author="francoisforster" created="2015-02-06T18:34:40Z" id="73288504">I need this feature to store a boolean that wasn't previously stored.
                    "type":"boolean",
                    "fields" : {
                        "stored" : {
                            "type":"boolean",
                            "index":"no",
                            "store":"yes"
                        }
                    }
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a blocking variant of close() method to BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6586</link><project id="" key="" /><description>Blocks until all bulk requests have completed.  

Updated based on feedback
Updated formatting

Closes #4158 
Closes #6314 
</description><key id="36246789">6586</key><summary>Add a blocking variant of close() method to BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">matt-preston</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-22T16:23:28Z</created><updated>2015-06-07T13:11:06Z</updated><resolved>2014-07-17T14:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-17T14:30:52Z" id="49314306">@matt-preston I wonder if you can sign the CLA so I can pull this in?
</comment><comment author="s1monw" created="2014-07-17T14:31:49Z" id="49314433">@matt-preston nevermind I overlooked your CLA - it's on file...
</comment><comment author="s1monw" created="2014-07-17T14:33:08Z" id="49314628">I merged it in - thanks...
</comment><comment author="yeroc" created="2014-09-24T15:30:03Z" id="56688074">I'm curious why the new awaitClose() method doesn't take a org.elasticsearch.common.unit.TimeValue as this is used elsewhere in the ElasticSearch API where timeouts are specified?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery API should also report ongoing relocation recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6585</link><project id="" key="" /><description>We currently only report relocation related recoveries after they are done.
</description><key id="36229329">6585</key><summary>Recovery API should also report ongoing relocation recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Stats</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-21T18:29:20Z</created><updated>2015-06-07T19:37:33Z</updated><resolved>2014-06-28T19:31:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-21T20:49:05Z" id="46764435">LGTM, except for that slowdown recovery settings question
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't acquire dirtyLock on autoid for create</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6584</link><project id="" key="" /><description>If we auto-generate the ID and know it cannot already exist then I think we can skip acquiring the dirtyLock for a small perf gain under high concurrency (I see a couple percent gain when _bulk indexing with 10 threads).
</description><key id="36228083">6584</key><summary>Don't acquire dirtyLock on autoid for create</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-21T17:17:45Z</created><updated>2015-06-07T13:11:33Z</updated><resolved>2014-07-07T15:34:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-30T13:05:18Z" id="47528622">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replication of new documents from 1.1.2 primary to 1.0.3 replica not working during upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6583</link><project id="" key="" /><description>During a rolling upgrade of our cluster from 1.0.3 to 1.1.2, we observed issues with replication of newly indexed documents from 1.0.3 data nodes holding primary shards to 1.1.2 data nodes holding their replicas.  Replication between 1.0.3 primaries and 1.0.3 replicas, between 1.1.2 primaries and 1.0.3 replicas, and 1.1.2 primaries and 1.1.2 replicas continued to work correctly throughout the upgrade.  Documents were being indexed via the _bulk API.

Logs on both 1.0.3 and 1.1.2 nodes logged three different types of unusual messages, which did not cease until every node in the cluster was upgraded:

[2014-06-18 15:29:33,057][WARN ][transport.netty          ] [T02-C01-A02] Message not fully read (request) for [2092] and action [indices/recovery/s], resetting

[2014-06-18 20:47:37,234][WARN ][transport.netty          ] [T02-C01-A02] Message not fully read (response) for [41328828] handler org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$3@1c73aab6, error [false], resetting

[2014-06-19 21:47:52,935][DEBUG][discovery.zen            ] [T02-C01-A02] received cluster state from [[ES2PROD-M01][ViFfCcmsRLKAEkoli2hfWQ][es2prod-m01][inet[/10.0.64.103:9300]]{updateDomain=3, data=false, faultDomain=1, master=true}] which is also master but with cluster name [null]

We are running our nodes in Azure on Windows 2012 R2 Datacenter using the following JVM:
            "version": "1.7.0_55",
            "vm_name": "OpenJDK 64-Bit Server VM",
            "vm_version": "24.55-b03",
            "vm_vendor": "Azul Systems, Inc."

Also, it should be noted that /_cat/shards and /_cat/indices both returned null pointer exceptions during the upgrade process.
</description><key id="36211848">6583</key><summary>Replication of new documents from 1.1.2 primary to 1.0.3 replica not working during upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">haardvark</reporter><labels><label>feedback_needed</label></labels><created>2014-06-21T00:09:37Z</created><updated>2015-01-05T11:41:03Z</updated><resolved>2015-01-05T11:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-10-24T20:31:22Z" id="60444953">@haardvark it's been a while, but I wonder if you can supply more details as to what exactly went wrong with the bulk request. The log messages you gave indicate some network level bwc issue in one of the actions inheriting from TransportBroadcastOperationAction, which are not bulk related and the other one is a misplaced log message in the ZenDiscovery class. Both are probably long fixed, but this is potentially something that was not.
</comment><comment author="clintongormley" created="2014-12-30T20:18:28Z" id="68393828">No further info provided. @bleskes OK to close?
</comment><comment author="bleskes" created="2015-01-04T20:59:22Z" id="68648715">+1 to close. Without more info we can't chase this down.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix optional default script loading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6582</link><project id="" key="" /><description>Groovy is optional as a dependency in the classpath, make sure we properly detect when its not at the right time to disable it
</description><key id="36207220">6582</key><summary>Fix optional default script loading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T22:17:32Z</created><updated>2015-06-07T13:11:45Z</updated><resolved>2014-06-20T22:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-20T22:20:47Z" id="46732790">LGTM, +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boostrap: Log startup exception to console if needed and to file as error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6581</link><project id="" key="" /><description>We ran into an issue with one of our data nodes which would quit immediately upon starting the service. Looking at every log there was nothing informative. We turned the log up to DEBUG and an exception came out:

```
[2014-06-20 14:48:00,502][DEBUG][bootstrap                ] Exception
org.elasticsearch.ElasticsearchIllegalStateException: Failed to obtain node lock, is the following location writable?: [/usr/local/var/data/elasticsearch/contacts-search-new]
    at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:114)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:150)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:68)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:201)
    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)
```

It's helpful to have all exceptions printing to `ERROR` level logging or at least `WARN`. But in the case where `Bootstrap.java` is going to quit the JVM, the reason for doing so should be at `ERROR` level so it's clear to the developer what is going wrong.
</description><key id="36175035">6581</key><summary>Boostrap: Log startup exception to console if needed and to file as error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">mgreene</reporter><labels><label>:Logging</label><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T15:05:50Z</created><updated>2015-06-07T16:50:11Z</updated><resolved>2014-10-20T18:55:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-30T09:08:08Z" id="47509899">hi,

I wonder why the error message logged here wasn't enough/didn't show up?

https://github.com/elasticsearch/elasticsearch/pull/6581/files#diff-db3acfb99fd5b696f1de71d2237583eaL243
</comment><comment author="mgreene" created="2014-06-30T14:10:43Z" id="47535830">@bleskes The error message did show up, but only if debug logging was enabled. By default, ElasticSearch does not have debug enabled and generally speaking, logging exceptions should be at `ERROR` level.
</comment><comment author="bleskes" created="2014-06-30T14:11:48Z" id="47535984">I meant line 243:

```
 logger.error(errorMessage);
```
</comment><comment author="mgreene" created="2014-06-30T14:54:00Z" id="47541396">@bleskes It looks as if the call to `buildErrorMessage` then calls `detailedMessage` which doesn't build a string with all the frames in the stack. 
</comment><comment author="bleskes" created="2014-06-30T14:59:52Z" id="47542178">I see. I have to admit that looking at your example, the stack trace is really debug info - the main message tells you all you need: `org.elasticsearch.ElasticsearchIllegalStateException: Failed to obtain node lock, is the following location writable?: [/usr/local/var/data/elasticsearch/contacts-search-new]
`
If you really feel strongly about this change, then we can iterate on the PR - i think it will log the error double now..
</comment><comment author="mgreene" created="2014-06-30T15:14:07Z" id="47544017">Your call. I would just reiterate that even the message (excluding the stack trace) should be at `ERROR` level, which it is not. 

If that were to occur, then that would certainly be an improvement over the current situation which is, by default, the operator is left without any clues as to fix their situation.
</comment><comment author="bleskes" created="2014-06-30T15:16:28Z" id="47544353">@mgreene - I'm sorry but I'm confused. This line logs the message as error? https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/bootstrap/Bootstrap.java#L243
</comment><comment author="mgreene" created="2014-06-30T15:28:44Z" id="47545932">@bleskes Sorry... The log snippet that I put in the PR description has the message coming out as `DEBUG`. There was no other error message indicating a problem...apologies for the confusion. 

I didn't thoroughly run a build of ES locally to see which code path I was hitting at the time so I'm just taking an educated guess as to why the stack trace and it's message is being hidden. I can look into this more at a later time if you'd like me to provide a thorough analysis of it.
</comment><comment author="mgreene" created="2014-06-30T15:35:04Z" id="47546814">@bleskes Also I would bring up one other point. The flow of this code block is terminal anyways as it calls `System.exit(3)`. At this point, it really doesn't make sense to not dump as much info as possible here. Most Java developers would appreciate a stack trace if the process is being killed voluntarily.
</comment><comment author="bleskes" created="2014-08-22T08:52:09Z" id="53037714">Re-thinking this, I think the right approach is to output the message to the console (if needed) and always to the log as an error:

```
             String errorMessage = buildErrorMessage(stage, e);
             if (foreground) {
                 System.err.println(errorMessage);
                 System.err.flush();
                 Loggers.disableConsoleLogging();
             }
            logger.error("Exception", e);
```

@mgreene do you agree, and if so can you update the PR?
</comment><comment author="clintongormley" created="2014-10-18T10:43:29Z" id="59606854">@mgreene Any feedback on this?
</comment><comment author="mgreene" created="2014-10-18T14:08:22Z" id="59614712">@clintongormley Certainly, I added a commit on 8/22 here: https://github.com/evertrue/elasticsearch/commit/ff5186e56777b09544d7a4fcf68be08689b5acae that followed @bleskes advice. 
</comment><comment author="clintongormley" created="2014-10-19T11:33:55Z" id="59646997">@mgreene whoops i didn't see that, thanks for pointing out.
</comment><comment author="bleskes" created="2014-10-20T06:58:36Z" id="59690573">@mgreene I completely missed it as well - Github doesn't notify when a new commit as added to a PR, just comments. 

Anyway - change LGTM thanks!. Can you please sign the CLA here: www.elasticsearch.org/contributor-agreement/ so I can pull it in?
</comment><comment author="mgreene" created="2014-10-20T12:27:58Z" id="59742461">@bleskes Good to go.
</comment><comment author="bleskes" created="2014-10-20T18:57:29Z" id="59820842">@mgreene merged. thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some interval specifications for date_histogram aggregations raise NumberFormatException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6580</link><project id="" key="" /><description>Hey guys,

whenever I try to query ES with a date_histogram aggregation that uses an arbitrary interval string it _sometimes_ responds with a NumberFormatException.

This one is totally fine and creates the histogram buckets as expected:

``` javascript
{
    "date_histogram": {
        "field": "AwesomeDateField",
        "interval": "42d",
        "format": "dd.MM.yyyy"
    }
}
```

Unfortunately, this is not (trying to bucket by half years):

``` javascript
{
    "date_histogram": {
        "field": "AwesomeDateField",
        "interval": "6M",
        "format": "dd.MM.yyyy"
    }
}
```

Response:

``` javascript
[...snip...] nested: ElasticsearchParseException[Failed to parse [6M]]; nested: NumberFormatException[For input string: "6M"]; }
```

Strings like '1M' work as well so it kind of leaves me puzzled why '6M' isn't working. Any ideas, what's up?

Thanks in advance!
Stephan
</description><key id="36171600">6580</key><summary>Some interval specifications for date_histogram aggregations raise NumberFormatException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bauneroni</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>enhancement</label></labels><created>2014-06-20T14:27:52Z</created><updated>2016-01-22T18:27:35Z</updated><resolved>2015-11-21T22:41:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-06-23T10:33:06Z" id="46828234">Reproduced this in master.  1M, 1q and 1y are prebuilt DateTimeUnits and are supported by the date histogram which is why your 1M request works.  It looks like we only support arbitrary values for time units up to weeks in the TimeValue class.  This will be because above the weeks unit, the duration of time taken is variable so as TimeValue converts the interval to a fixed number of milliseconds it cannot support intervals with variable lengths.
</comment><comment author="clintongormley" created="2014-10-24T09:31:08Z" id="60364687">Relates to #7796
</comment><comment author="clintongormley" created="2015-11-21T15:08:56Z" id="158650378">Related to https://github.com/elastic/elasticsearch/issues/14802
</comment><comment author="clintongormley" created="2015-11-21T22:41:33Z" id="158688345">Closing in favour of https://github.com/elastic/elasticsearch/issues/8939
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistant Zen Discovery Ping Timeout Documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6579</link><project id="" key="" /><description>Hello,

It appears there is some inconsistency with the setting of the ping timeout for zen discovery; some places it's referenced as `discovery.zen.ping.timeout` and in others it's `discovery.zen.ping_timeout`.

It appears a change was made to support `ping.timeout` in 9be62a06e9e1244dc726e5ee33a523d31bacfedb which also exposed the setting in the sample config file. However the [docs for zen](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html#master-election) refer to `ping_timeout` worse, it appears fault detection uses the `ping_timeout` name with no allowance for `ping.timeout` nor `ping.interval`, etc.

It seems like it would be more consistent to refer to the timeout value as `ping_timeout` in both the sample config as well as in the logs to avoid confusion.
</description><key id="36170154">6579</key><summary>Inconsistant Zen Discovery Ping Timeout Documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">xyu</reporter><labels><label>:Settings</label><label>adoptme</label><label>enhancement</label></labels><created>2014-06-20T14:10:08Z</created><updated>2015-11-20T14:12:31Z</updated><resolved>2015-09-23T17:18:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-20T19:33:49Z" id="46717893">I totally agree, it is very confusing. Actually I think `discovery.zen.ping.timeout` should be renamed to `discovery.zen.ping.gossip_window`, or something along those lines. We should revisit those settings to make sure they are more readable.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix memory leak when percolating with nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6578</link><project id="" key="" /><description>The percolator uses non segment reader impl (MemoryIndexReader), this causes associated cache entries not automatically be cleared when the reader closes and that is why the percolator removes the cache entries manually (filter cache, field data) 

However when percolating a document with nested objects a multi reader is used that wraps a MemoryIndexReader for each nested object. Cache entries use the leaves as key in filter / field data cache, but the percolator clear using the top level multi reader. This causes cache entries never to be evicted and resulting in OOM.

The memory is fixed by the following changes:
- Not allowing caching for non segment reader implementations.
- The percolator never cache anything that associated with the in-memory index where document being percolated resides in.
</description><key id="36168791">6578</key><summary>Fix memory leak when percolating with nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T13:53:11Z</created><updated>2015-06-07T19:37:45Z</updated><resolved>2014-07-01T13:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tiran" created="2014-06-21T13:18:30Z" id="46753484">Martijn's patch fixes a critical production issue that prevents us from updating from 0.90 to 1.2 We are using nested documents and percolators a lot. With 1.2.1 memory consumption on our integration system figuratively explodes to more than 8GB RSS in a matter of minutes although we ran our tests with just a small fraction of our documents.

We have cherry-picked your commits on top of ES 1.2 branch and deployed 1.2.2-SNAPSHOT on our integration server. Memory consumption is keeping steady at 2.4 GB for 900k docs and 700 MB index for more than an hour.

Thanks a lot!
</comment><comment author="julianhille" created="2014-06-23T13:04:45Z" id="46842005">is t here an ETA for this to be fixed in an official release? we have to decide if we do this on our own.
thank you for any help / information.
</comment><comment author="kimchy" created="2014-06-23T13:26:59Z" id="46844318">we still need to review it, once its in, releasing 1.2.2 is relatively simple, and will be released based on urgency of issues found, so we take your input into account!
</comment><comment author="s1monw" created="2014-06-24T10:49:02Z" id="46956888">I added some comments! Good change @martijnvg 
</comment><comment author="s1monw" created="2014-06-24T10:49:35Z" id="46956932">oh can you please label it and put `review` back once you have changes?
</comment><comment author="martijnvg" created="2014-06-25T09:09:02Z" id="47077288">@s1monw Thanks for reviewing it, I updated the PR.
</comment><comment author="s1monw" created="2014-06-26T08:27:57Z" id="47200022">left one comment, other than that looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update update.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6577</link><project id="" key="" /><description>fixed a typo :)
</description><key id="36168240">6577</key><summary>Update update.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">baldurh</reporter><labels /><created>2014-06-20T13:45:43Z</created><updated>2014-08-07T19:07:43Z</updated><resolved>2014-08-07T19:07:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T19:07:33Z" id="51517250">thanks @baldurh , merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better support for partial buffer reads/writes in translog infrastructure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6576</link><project id="" key="" /><description>Some IO api can return after writing &amp; reading only a part of the requested data. On these rare occasions, we should call the methods again to read/write the rest of the data. This has cause rare translog corruption while writing  huge documents on Windows.

Closes #6441

PS. I'll add java docs to the new Channels util class, but wanted to start the review process.
</description><key id="36167813">6576</key><summary>Better support for partial buffer reads/writes in translog infrastructure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Translog</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T13:39:42Z</created><updated>2015-06-08T19:51:22Z</updated><resolved>2014-07-01T17:17:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-23T15:14:29Z" id="46858373">@rmuir @uschindler pushed another commit based on your feedback. Thx.

I still need to do the forbidden API. 
</comment><comment author="uschindler" created="2014-06-23T15:57:57Z" id="46864397">Looks fine now. Should I retest?
</comment><comment author="bleskes" created="2014-06-23T16:01:14Z" id="46864802">Sure. Thx!!

On Mon, Jun 23, 2014 at 5:58 PM, Uwe Schindler notifications@github.com
wrote:

&gt; ## Looks fine now. Should I retest?
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/6576#issuecomment-46864397
</comment><comment author="uschindler" created="2014-06-23T16:53:10Z" id="46871163">OK, tested and works. I am still investigating about the while-loop: What happens if disk is full? Would Channel#write() then always return 0, so it will get endless loop?

As far as I remeber, the Javadocs say, that at least 1 byte is written. Have to validate! Otherwise a disk-full would produce an endless loop. At least with this patch we should also prevent translog corrumption with disk full, because it should throw exception!
</comment><comment author="uschindler" created="2014-06-23T16:58:40Z" id="46871838">I have no idea how to handle writes of 0 bytes... MAYBE you should do a test with a disk full (e.g. tmpfs with limited size).

Otherwise patch looks fine.
</comment><comment author="bleskes" created="2014-06-24T06:45:39Z" id="46937233">@uschindler the java docs say "A socket channel in non-blocking mode, for example, cannot write any more bytes than are free in the socket's output buffer." &amp; "The number of bytes written, possibly zero" 

I think this implies that an error is indicated by an exception and we should just retry upon 0 return value?
</comment><comment author="uschindler" created="2014-06-24T08:28:52Z" id="46944793">I think the main problem here is, that it is undefined. The Javadocs of FileChannel#read (more the interface docs @ http://docs.oracle.com/javase/7/docs/api/java/nio/channels/ReadableByteChannel.html) clearly state how it works: "It is guaranteed, however, that if a channel is in blocking mode and there is at least one byte remaining in the buffer then this method will block until at least one byte is read" - but nothing like that for write.

In Lucene's NIOFSDirectory we assert on exactly this behaviour when reading from the FileChannel (which is blocking).

Is the channel of the tranlog blocking or not?

I am just afraid that in some cases (especially for sockets), where nothing can be written, we wait in a spinning loop until the number of bytes is &gt; 0.
</comment><comment author="s1monw" created="2014-06-24T10:30:34Z" id="46955457">I really think the channel API needs to go into forbidden APIs ...I also left some comments on the commit.
</comment><comment author="bleskes" created="2014-06-24T10:38:21Z" id="46955997">@s1monw thx. I still have a todo for this one to add the channel write/read methods to the forbidden API.
</comment><comment author="bleskes" created="2014-06-24T11:02:36Z" id="46957878">@uschindler mostly this is a FileChannel, though there is one place where we allow arbitrary GatheringByteChannel . I looked at the NIOFSDirectory, for reads there is an assert:

```
assert i &gt; 0 : "FileChannel.read with non zero-length bb.remaining() must always read at least one byte (FileChannel is in blocking mode, see spec of ReadableByteChannel)";
```

for writes I can't find any channel write. I can add the assert but I'm not sure it will help much as as you said it's part of the contract of the channel.
</comment><comment author="uschindler" created="2014-06-24T12:04:27Z" id="46962506">@bleskes that is exactly my problem: Its not part of the spec and that makes me angry. The assert in Lucenre is just there for debugging purposes if something goes wrong (we had some problems around this code).

In Lucene we never write to channels. Everything in Lucene is simple OutputStreams (FileOutputStream).

Without a spec, that clearly states (like for read) that it writes at least one byte, it is in my opinion risky to cause a spinning loop, always getting 0 back and wasting CPU cycles. I am not sure what the best idea here is. Maybe FileChannel never writes zero bytes (unless the remaining() is not 0)? Who knows?

I give up here, I cannot solve or explain this, nor can I give a recommendation. Maybe a stresstest, as @s1monw says would be the best idea (but let it run on all OSes).
</comment><comment author="bleskes" created="2014-06-27T16:09:32Z" id="47367717">I pushed another update adding entries to the forbidden API and fixing whatever that raised. The last todo on my list is to add direct buffers to the tests
</comment><comment author="uschindler" created="2014-06-28T10:04:04Z" id="47423505">Hi, patch looks fine. Also the added random direct buffers for the testing are fine (I assume yesfs == tests) :-)
</comment><comment author="bleskes" created="2014-06-30T13:00:55Z" id="47528217">@s1monw I think this is good to go. Can you double check the tests and see if you're happy with them?
</comment><comment author="s1monw" created="2014-06-30T13:15:55Z" id="47529696">I left a bunch of comments - looks very good though.
</comment><comment author="uschindler" created="2014-06-30T14:40:57Z" id="47539629">I agree with @s1monw: java.io.Closeable requires that close() must be idempotent. Otherwise patch looks fine.
</comment><comment author="bleskes" created="2014-07-01T07:27:06Z" id="47625029">@s1monw I pushed another update.
</comment><comment author="s1monw" created="2014-07-01T07:34:58Z" id="47625644">LGTM
</comment><comment author="bleskes" created="2014-07-01T17:17:06Z" id="47684205">Pushed this. Thx all for reviewing (back porting to 1.2 run into conflicts, I'll go through them later carefully and push it as well)
</comment><comment author="uschindler" created="2014-07-01T19:41:52Z" id="47700972">Thanks @bleskes! It was a pleasure to work with you, nice work, really!
</comment><comment author="SimplyWhiteMan" created="2015-06-08T16:57:15Z" id="110072803">does this update included in the version 1.2.1?
</comment><comment author="bleskes" created="2015-06-08T19:51:22Z" id="110118885">@SimplyWhiteMan it only made into 1.2.2 (see PR labels).  Also, since 1.2.2 we have _so_ many similar issues that I urge to upgrade to the 1.5.2 (or 1.6.0 coming soonish). Going 1.2.2 will be a bad idea...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>actionGet() hangs up forever</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6575</link><project id="" key="" /><description>Hi we are testing elasticsearch in our environment.

We are experiencing a problem similar to one described in #4887 

In our case we are using a thread pool that change its size based on some load logics. this thread pool perform both search and index/unindex operations on elasticsearch.
During the execution we noticed (using VisualVM profiling) that some of these threads hangs up on actionGet() method forever and never returns neither a search nor an exceptions, simply ... wait .

As a work around we introduced a "5s" timeout on actionGet() call so these threads die automatically but is an "ugly" workaround because search or index is not phisically performed and a timeout exception is returned.

This is our environment:

4 elasticsearch data nodes with 2GB of ram and managing a total of 2M of documents 
4 elasticsearch client nodes running under tomcat 7.0.52
Both of them use java 1.6 and elastic search version 1.1.2

this is how we call elasticsearch:

```
client.prepareSearch(INDEX_NAME)
.setOperationThreading(SearchOperationThreading.NO_THREADS).addFields("score")
.setTypes(ElasticSearchNodeClient.INDEX_TYPE).setPostFilter(FilterBuilders.queryFilter(elasticQuery))
                    .setSize(PAGE_SIZE).setFrom(start + (page * PAGE_SIZE)).addSort("score", SortOrder.ASC).execute()
                    .actionGet(new TimeValue(5000)); 
```

```
client.prepareUpdate(getComposedIndexName(), ElasticSearchNodeClient.INDEX_TYPE, getAdvertIdFromId(id)).setDocAsUpsert(true)
                    .setDoc(prefixToField.get(prefix), new ArrayList&lt;String&gt;()).execute().actionGet(new TimeValue(5000)); 
```

```
client.prepareDelete(getComposedIndexName(), ElasticSearchNodeClient.INDEX_TYPE, getAdvertIdFromId(id))
                    .setOperationThreaded(false)
                    .execute().actionGet(new TimeValue(5000)); 
```

Is there something wrong in our calls that is causing this hang up behavior?

Thanks
</description><key id="36167445">6575</key><summary>actionGet() hangs up forever</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Tera</reporter><labels /><created>2014-06-20T13:34:41Z</created><updated>2014-11-14T18:17:41Z</updated><resolved>2014-11-14T18:17:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-20T13:38:06Z" id="46678588">it might be the operation threading model you set, we have removed this option in 1.2. Can you try and run it without setting anything around the operation threading model on any API?
</comment><comment author="Tera" created="2014-06-20T14:33:38Z" id="46684691">thanks for your hint, i've tryied without any threading model ( i basically removed any setOperationThreading or setOperationThreaded call ) but no notable effect, threads still hang up in waiting state . 

Could be a configuration problem? there is a check that i can perform to give you some more detailed informations?

Thanks
</comment><comment author="kimchy" created="2014-06-24T07:24:14Z" id="46939491">this is always tricky to try and find, is there a chance for a test case that reproduce it? this tends to be the simplest way to figure it out. Maybe a JVM test that starts 2 nodes, and then starts a client node to connect to it, and run your workload against it?
</comment><comment author="clintongormley" created="2014-11-14T18:17:41Z" id="63106283">Hi @Tera - haven't heard anything since June, so I'm assuming this is OK.  Please reopen if you can replicate on v1.4.0

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot backup utilizing high heap memory and cpu% of es node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6574</link><project id="" key="" /><description>## Snapshot backup utilizing high heap memory and cpu% &lt;h2&gt;

We have 3 nodes in different servers say, node1 node2 node3, and configured the snapshot repository per cluster on NFS, mounted on all the 3 servers, while executing snapshot the node which is actively taking backup utilizing high heap usage and the node is removing from cluster which let the cluster to go yellow, the same happens in all the cluster running snapshots

Size of the below index is 

size: 19.3G (38.5G)
docs: 10,214,293 (11,682,412)

| nodes | heap alloted |
| --- | --- |
| node1 | 4GB |
| node2 | 4GB |
| node2 | 4GB |

```
node 1 : 

[2014-06-09 08:12:57,447][INFO ][monitor.jvm              ] [node1] [gc][young][201419][1869] duration [725ms], collections [1]/[1.1s], total [725ms]/[7m], memory [1.6gb]-&gt;[1.5gb]/[3.9gb], all_pools {[young] [268.1mb]-&gt;[15.4mb]/[382.7mb]}{[survivor] [6.8mb]-&gt;[47.8mb]/[47.8mb]}{[old] [1.3gb]-&gt;[1.4gb]/[3.5gb]}
[2014-06-09 11:44:20,392][INFO ][monitor.jvm              ] [node1] [gc][young][214097][1980] duration [706ms], collections [1]/[1.5s], total [706ms]/[7.2m], memory [1.8gb]-&gt;[1.7gb]/[3.9gb], all_pools {[young] [303.5mb]-&gt;[9.6mb]/[382.7mb]}{[survivor] [47.8mb]-&gt;[47.8mb]/[47.8mb]}{[old] [1.5gb]-&gt;[1.6gb]/[3.5gb]}
[2014-06-09 13:39:28,102][INFO ][monitor.jvm              ] [node1] [gc][young][221002][2025] duration [808ms], collections [1]/[1.4s], total [808ms]/[7.3m], memory [1.9gb]-&gt;[1.8gb]/[3.9gb], all_pools {[young] [278.9mb]-&gt;[7.5mb]/[382.7mb]}{[survivor] [43.2mb]-&gt;[47.8mb]/[47.8mb]}{[old] [1.6gb]-&gt;[1.7gb]/[3.5gb]}
[2014-06-09 21:39:54,469][INFO ][monitor.jvm              ] [node1] [gc][young][249819][2158] duration [744ms], collections [1]/[1.3s], total [744ms]/[7.6m], memory [2gb]-&gt;[1.9gb]/[3.9gb], all_pools {[young] [252.5mb]-&gt;[1.5mb]/[382.7mb]}{[survivor] [18.2mb]-&gt;[47.8mb]/[47.8mb]}{[old] [1.8gb]-&gt;[1.9gb]/[3.5gb]}
~                                                                                                                                                                               
~

node 2 :

2014-06-08 12:17:13,044][INFO ][cluster.service          ] [node2] removed {[node1][m-lCXG-ATBGWjiDCadnriA][godavari11.sirahu.com][inet[/19.19.20.101:91201]],}, reason: zen-disco-receive(from master [[node3][0yETviCKTROJgTIUSlIRiw][godavari13.sirahu.com][inet[/19.19.20.103:91203]]])
[2014-06-08 12:18:59,955][INFO ][cluster.service          ] [node2] added {[node1][m-lCXG-ATBGWjiDCadnriA][godavari11.sirahu.com][inet[/19.19.20.101:91201]],}, reason: zen-disco-receive(from master [[node3][0yETviCKTROJgTIUSlIRiw][godavari13.sirahu.com][inet[/19.19.20.103:91203]]])
[2014-06-08 23:57:11,575][DEBUG][action.search.type       ] [node2] [incides2][1], node[m-lCXG-ATBGWjiDCadnriA], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@59ecb9f2]
org.elasticsearch.transport.RemoteTransportException: [node1][inet[/19.19.20.101:91201]][search/phase/query+fetch]
Caused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [incides2][1]: query[ConstantScore(cache(_type:59))],from[0],size[50]: Query Failed [Failed to execute main query]
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:330)

~                                                                                                                                                                               
~

node 3 : 

[2014-06-09 07:18:42,428][WARN ][monitor.jvm              ] [node3] [gc][young][198311][2132] duration [1.3s], collections [1]/[3.7s], total [1.3s]/[5.2m], memory [2.3gb]-&gt;[930.1mb]/[3.9gb], all_pools {[young] [345.9mb]-&gt;[9.9mb]/[382.7mb]}{[survivor] [17.5mb]-&gt;[0b]/[47.8mb]}{[old] [2gb]-&gt;[920.3mb]/[3.5gb]}
```

I Tried configuring the below parameters in the staging setup, backup is taking much time than usual
also cpu% in top command shows 300%  where 10 - 15% ( at normal state)

```
 curl -XPUT http://ip:port/_snapshot/es-stag -d '{
    "type": "fs",
    "settings": {
        "location": "/mountpath/to/esearch_snapshots/es-stag",
        "compress": false,
        "chunk_size": "50m",
        "max_snapshot_bytes_per_sec": "100mb"
    }
```

I want to achieve 20 mins snapshot backup without any heap or cpu utilization issue! 

Please help!

Thank you in advance

-Aswin
</description><key id="36167135">6574</key><summary>Snapshot backup utilizing high heap memory and cpu% of es node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tsaswin</reporter><labels /><created>2014-06-20T13:30:56Z</created><updated>2014-12-30T20:16:57Z</updated><resolved>2014-12-30T20:16:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-06-22T19:56:05Z" id="46790962">Could you execute [nodes stats](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html) command before and during the snapshot operation as well as [hot threads](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html#cluster-nodes-hot-threads) command 3-5 times during snapshot operation and send me the results?
</comment><comment author="clintongormley" created="2014-12-30T20:16:57Z" id="68393681">No more info provided.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match query with operator and, cutoff_frequency and stacked tokens</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6573</link><project id="" key="" /><description>If the match query with cutoff_frequency encounters stacked tokens,
like synonyms in the same position, it returns a boolean query instead
of a common terms query.  However, if the original operator was set
to "and", it was ignoring that and resetting the operator to "or".

In fact, if operator is "and" then there is little benefit in using
a common terms query as a must query is already
executed efficiently.

Fixed by https://github.com/elasticsearch/elasticsearch/commit/30c80319c05362fae49fdfe5d6422c59f942f0db
</description><key id="36157571">6573</key><summary>Match query with operator and, cutoff_frequency and stacked tokens</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T10:44:49Z</created><updated>2015-06-07T19:39:28Z</updated><resolved>2014-06-25T15:57:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-24T10:10:23Z" id="46953835">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolator inconsistency/ not hitting on dynamic fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6572</link><project id="" key="" /><description>We are using percolator and dynamic fields.

Steps to reproduce:
- create a mapping with an object
- create a percolator for a subfield of that mapping and type
- percolate a doc which has that dynamic field
- no hits

Steps to reproduce a hit:
- create a mapping with an object
- index a doc which has that dynamic field added to the idnex
- create a percolator for a subfield of that mapping and type
- percolate a doc which has that dynamic field
- hit

This is very odd, as we use percolators to generate data into the doc, before indexing it.
So we have to index the doc, then percolate with it and then change the document.

Gist for ES0.90 (tested on 0.90.13):
https://gist.github.com/julianhille/5203065480dc62bb6752

Gist for ES1.x (tested on 1.2.1):
https://gist.github.com/julianhille/bd23e8e43e28a193401e
</description><key id="36155708">6572</key><summary>percolator inconsistency/ not hitting on dynamic fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">julianhille</reporter><labels /><created>2014-06-20T10:13:03Z</created><updated>2014-07-01T14:30:30Z</updated><resolved>2014-07-01T14:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-01T14:15:07Z" id="47660998">@julianhille The reported issue is similar to #5750 and boils down to the fact the field `field.1` doesn't exist in the mapping until a doc gets indexed or percolated that has that field. Until then it assumes it is a string and decodes the value `456` as such.

Maybe you can try any of the mentioned work arounds? Fixing this isn't easy, since the query parsing that kicks in during storing a query doesn't introduce new fields in the mapping or has the capability to detect what type a field is.
</comment><comment author="clintongormley" created="2014-07-01T14:30:30Z" id="47662991">Closed in favour of #6664 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to Groovy as the default scripting language</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6571</link><project id="" key="" /><description>Considered a followup to #6233.
</description><key id="36155678">6571</key><summary>Switch to Groovy as the default scripting language</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>breaking</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T10:12:38Z</created><updated>2015-06-06T16:55:45Z</updated><resolved>2014-06-25T10:32:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-23T08:39:38Z" id="46818708">Fixed the typos, thanks @pickypg 
</comment><comment author="dakrone" created="2014-06-23T08:40:40Z" id="46818790">The table of plugins is better suited for http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html#scripting I think, rather than adding a table into this section of the documentation.
</comment><comment author="pickypg" created="2014-06-23T15:37:15Z" id="46861573">Ugh, did I comment on the commits rather than the diffs? Sorry about that. LGTM though.
</comment><comment author="s1monw" created="2014-06-24T10:12:24Z" id="46954002">IMO we should to this for `1.4` that way folks can upgrade to `1.3` switch to groovy and then move the `1.4` without pain. We can move it into master now already IMO..

other than that LGTM
</comment><comment author="dakrone" created="2014-06-25T10:12:26Z" id="47083104">@s1monw sounds good, I will merge to master now and backport to 1.4 once we've created a branch for it.
</comment><comment author="kimchy" created="2014-06-25T10:27:52Z" id="47084352">is the removal of mvel, and recommending using the mvel plugin for backward comp. (and setting the default lang for mvel), going to be in a different change?
</comment><comment author="dakrone" created="2014-06-25T10:28:41Z" id="47084420">@kimchy yes, I will submit a separate PR for that after this one has been merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Re-shade MVEL as a dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6570</link><project id="" key="" /><description>To avoid breaking backwards compatibility, MVEL should still be shaded until it is removed.
</description><key id="36152858">6570</key><summary>Re-shade MVEL as a dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-20T09:28:44Z</created><updated>2015-06-07T13:12:02Z</updated><resolved>2014-06-20T09:30:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-20T09:29:31Z" id="46660645">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException corrupts index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6569</link><project id="" key="" /><description>[2014-06-19 10:57:34,834][ERROR][index.engine.internal    ] [a3] [pl][5] failed to acquire searcher, source load_version
java.lang.NullPointerException
        at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:649)
        at org.elasticsearch.index.engine.internal.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1221)
        at org.elasticsearch.index.engine.internal.InternalEngine.innerCreate(InternalEngine.java:414)
        at org.elasticsearch.index.engine.internal.InternalEngine.create(InternalEngine.java:386)
        at org.elasticsearch.index.shard.service.InternalIndexShard.create(InternalIndexShard.java:384)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:572)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio
nOperationAction.java:249)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio
nOperationAction.java:228)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
[2014-06-19 10:57:34,838][ERROR][index.engine.internal    ] [a3] [pl][5] failed to acquire searcher, source load_version
java.lang.NullPointerException
        at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:649)
        at org.elasticsearch.index.engine.internal.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1221)
        at org.elasticsearch.index.engine.internal.InternalEngine.innerCreate(InternalEngine.java:414)
        at org.elasticsearch.index.engine.internal.InternalEngine.create(InternalEngine.java:386)
        at org.elasticsearch.index.shard.service.InternalIndexShard.create(InternalIndexShard.java:384)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:572)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio
nOperationAction.java:249)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio
nOperationAction.java:228)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
...

[2014-06-19 10:57:34,838][WARN ][indices.recovery         ] [a3] [pl][5] recovery from [[a1][VmKnyScUS968jh3MsqNh5g][a1.example.com][inet[/1.2.3.4:9301]]{datacenter=abc}] failed
org.elasticsearch.transport.RemoteTransportException: [a1][inet[/1.2.3.4:9301]][index/shard/recovery/startRecovery]
Caused by: org.elasticsearch.index.engine.RecoveryEngineException: [pl][5] Phase[2] Execution failed
        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1011)
        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:631)
        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:122)
        at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:62)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:351)
        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

Caused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=ddd229e actual=e296bd98 (resource=BufferedChecksumIndexInput(MMapIndexInput(path="/home/abc/elasticsearch_new/data/elasticsearch/nodes/0/indices/pl/5/index/_9yef_es090_0.tip")))
        at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:211)
        at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)
        at org.apache.lucene.codecs.BlockTreeTermsReader.&lt;init&gt;(BlockTreeTermsReader.java:140)
        at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:441)
        at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.&lt;init&gt;(BloomFilterPostingsFormat.java:133)
        at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:104)
        at org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat.fieldsProducer(Elasticsearch090PostingsFormat.java:79)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.&lt;init&gt;(PerFieldPostingsFormat.java:195)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:251)
        at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:116)
        at org.apache.lucene.index.SegmentReader.&lt;init&gt;(SegmentReader.java:101)
        at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:142)
        at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:236)
        at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:99)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:385)
        at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:112)
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:89)
        at org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1364)
        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:291)
        ... 7 more
</description><key id="36149920">6569</key><summary>NullPointerException corrupts index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">Rzulf</reporter><labels /><created>2014-06-20T08:40:38Z</created><updated>2014-12-30T20:16:17Z</updated><resolved>2014-12-30T20:16:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Rzulf" created="2014-06-20T08:41:26Z" id="46656827">Version 1.2.0
Ubuntu Server 14.04
</comment><comment author="Rzulf" created="2014-06-20T09:59:08Z" id="46662868">I am experiencing this bug with another issue #6441 

Both things started to happen after I run out of space on disk :/
</comment><comment author="elvarb" created="2014-10-21T22:33:34Z" id="60009953">Having the same problems. Ran out of disk space and 4 out of 5 shards cant be initialized. Using Kopf to view the status I can see the size of the Index jump up and down depending on what host is trying to initialize the shard. Possibly because each of them contain either a perimary or a replica of the corrupted shard and they dont match.
</comment><comment author="elvarb" created="2014-10-21T22:44:50Z" id="60011182">Could be happening to begin with when the node that holds the primary shard has space left but the backup one runs out of space. ES accepts and commits the incoming transactions even though the backup shards cant verify the transaction.

When the initialization starts again they refuse to start because of the size mis match.

A better behavior would be for ES to stop accepting new transactions if one data node runs out of space.
</comment><comment author="bleskes" created="2014-10-22T06:55:37Z" id="60043886">@elvarb what version are you on? 

When we assign a replica to a node and that node already has local files, we try to reuse them instead of copying the same files over. Perhaps something goes wrong in trying to read the local files after they were corrupted by the out of disk space issue. We are already supposed to deal with errors there but that become tricky and we might have missed some edge case.

Can you share the error stack trace you see?
</comment><comment author="elvarb" created="2014-10-22T10:13:43Z" id="60063419">@bleskes Running version 1.3.2 on Windows

Here are a few entries from the log file. I had to delete the Index to get it working so the log files are all I have at the moment. This is from one of three nodes in the cluster.

```
[2014-10-21 22:33:31,897][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][3] received shard failed for [logstash-2014.10.21][3], node[vHC7m1pKSkS7byI9iWZBCA], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.10.21][3] failed to recover shard]; nested: ElasticsearchIllegalArgumentException[No version type match [115]]; ]]
[2014-10-21 22:33:31,944][WARN ][indices.cluster          ] [S1-Elastic-P1] [logstash-2014.10.21][2] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [logstash-2014.10.21][2] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:269)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [46]
    at org.elasticsearch.index.VersionType.fromValue(VersionType.java:307)
    at org.elasticsearch.index.translog.Translog$Create.readFrom(Translog.java:364)
    at org.elasticsearch.index.translog.TranslogStreams.readTranslogOperation(TranslogStreams.java:52)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:241)
    ... 4 more
[2014-10-21 22:33:31,975][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][2] sending failed shard for [logstash-2014.10.21][2], node[llsky8FITxyp1R2H3Wjr4w], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.10.21][2] failed to recover shard]; nested: ElasticsearchIllegalArgumentException[No version type match [46]]; ]]
[2014-10-21 22:33:31,975][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][2] received shard failed for [logstash-2014.10.21][2], node[llsky8FITxyp1R2H3Wjr4w], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.10.21][2] failed to recover shard]; nested: ElasticsearchIllegalArgumentException[No version type match [46]]; ]]
[2014-10-21 22:33:32,365][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][1] received shard failed for [logstash-2014.10.21][1], node[vHC7m1pKSkS7byI9iWZBCA], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.10.21][1] failed to recover shard]; nested: ElasticsearchIllegalArgumentException[No version type match [48]]; ]]
[2014-10-21 22:33:32,365][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][2] received shard failed for [logstash-2014.10.21][2], node[vHC7m1pKSkS7byI9iWZBCA], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.10.21][2] failed recovery]; nested: EngineCreationFailureException[[logstash-2014.10.21][2] failed to create engine]; nested: FileNotFoundException[No such file [_bgp.cfs]]; ]]
[2014-10-21 22:33:32,381][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][2] received shard failed for [logstash-2014.10.21][2], node[vHC7m1pKSkS7byI9iWZBCA], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [master [S1-Elastic-P1][llsky8FITxyp1R2H3Wjr4w][S1-Elastic-P1][inet[/172.25.127.183:9300]]{rack=S1, master=true} marked shard as initializing, but shard is marked as failed, resend shard failure]
[2014-10-21 22:33:32,474][WARN ][cluster.action.shard     ] [S1-Elastic-P1] [logstash-2014.10.21][0] received shard failed for [logstash-2014.10.21][0], node[vHC7m1pKSkS7byI9iWZBCA], [P], s[INITIALIZING], indexUUID [1bMPcr0kTSa0h6sa0IHu9g], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logstash-2014.10.21][0] failed to recover shard]; nested: ElasticsearchIllegalArgumentException[No version type match [49]]; ]]

```

In Kopf I could see the shards pop in as "recovering" in one node, then disappearing and reappearing on a different node, then disappearing again to show up on the first node. Each time this happened the total size for the Index would jump up and down. For me that indicates that when doing a total index size it adds up the total size for all shards and the shards are showing up on each nodes with different sizes.
</comment><comment author="s1monw" created="2014-10-22T14:42:44Z" id="60095536">It seems like the original issue has been fixed in #7455  I think it can be closed @bleskes but I'd like to have @dakrone to look at the translog problem here
</comment><comment author="dakrone" created="2014-10-22T15:01:51Z" id="60098793">The `ElasticsearchIllegalArgumentException: No version type match [46]` prior to the 1.4 release (which has translog checksums) indicates a corrupted translog. If the disk did fill up it's possible that the translog that was written got corrupted.

In this situation the translog for that shard should be moved out of the way (so local recovery can resume), then any operations inside of the translog can be extracted manually and resubmitted to ensure they were not lost. The translog is a file named `translog-123456` (it will be a different number instead of 123456) inside the data directory for a shard, for instance, if my cluster is named "elasticsearch" and the index is called "test", it would look like:

```
&lt;data location&gt;/elasticsearch/nodes/0/indices/test/&lt;shardnum&gt;/translog/translog-1413798960374
```

In your case the index is `logstash-2014.10.21` and the shardnum is 2.
</comment><comment author="clintongormley" created="2014-12-30T20:16:17Z" id="68393615">Sounds like this can be closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update nested-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6568</link><project id="" key="" /><description>Fix whitespace to standardize on spaces for indents on both code examples for readability.
</description><key id="36101651">6568</key><summary>Update nested-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">xyu</reporter><labels /><created>2014-06-19T17:52:08Z</created><updated>2014-08-18T11:09:26Z</updated><resolved>2014-08-18T11:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T19:05:58Z" id="51517057">Hi @xyu 

Sorry I'm taking some time on this one.  I'm thinking that it'd be better to replace the facets examples completely, given that they're deprecated.  If you'd like to do that, that'd be great, otherwise i'll add it to my list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compute term vectors on the fly if not stored in index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6567</link><project id="" key="" /><description>...index.

Adds an option called `generate` to the term vectors and multi-termvectors API,
which generates the term vectors for some chosen fields, if they have not been
explicitely stored in the index.

Relates to #5184
</description><key id="36089051">6567</key><summary>Compute term vectors on the fly if not stored in index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:Term Vectors</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-19T15:28:02Z</created><updated>2015-06-07T10:36:58Z</updated><resolved>2014-07-17T21:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-19T17:05:57Z" id="46588280">This looks good. I'm wondering if we actually need a `generate` option? Should we rather just generate when vectors are not stored without providing any option?
</comment><comment author="alexksikes" created="2014-06-19T17:09:06Z" id="46588680">Thanks for the throughout comments. I was wondering that too, perhaps because there could be a cost in performance in computing term vectors?
</comment><comment author="jpountz" created="2014-06-19T17:24:41Z" id="46590672">There would indeed. But I tend to think that the fewer options an API has, the better it is. :-)
</comment><comment author="s1monw" created="2014-06-26T08:48:17Z" id="47201695">I left some small comments - looks good otherwise :+1: 
</comment><comment author="s1monw" created="2014-07-02T10:37:14Z" id="47760822">I really like the feature a lot but I think we need way more tests. I'd love to see boundary &amp; error cases where TV do not exists but the field doesn't either etc. Some weird combinations and we shoudl make sure the right errors are coming back. I also like duel test. For instance you create two indices and enable TV on one but not on the other index random documents and fetch the TV API from both. They should be identical. 
</comment><comment author="s1monw" created="2014-07-17T13:36:46Z" id="49307115">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mappings: Add transform to document before index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6566</link><project id="" key="" /><description>**Edit for posterity**: this is being implemented as source transforms rather than conditional copy_to in #6599.  See comments below for why.

It'd be useful to be able to condition whether or not something is copied to another field based on some field in the document.  Something like:

``` js
{
  "foo": {
    "type": "string",
    "copy_to": {
      "script": "
if _source['bar'] in [0, 100, 132]
  return 'suggest';
end
"
    }
  },
  "suggest": {
    "type": "string",
    "analyzer": "suggest"
  },
  "bar": {
    "type": "integer"
  }
}
```

I could do this when I build the document but that is inconvenient because for me building the document about two orders of magnitude slower then rebuilding the index by copying data from Elasticsearch to itself.  It'd also end up adding text to the _source twice which is less then ideal. 
</description><key id="36084111">6566</key><summary>Mappings: Add transform to document before index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-06-19T14:37:40Z</created><updated>2014-07-16T09:46:42Z</updated><resolved>2014-07-15T16:50:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-19T14:42:40Z" id="46568934">Maybe a more general-purpose alternative would be to allow to pass the `_source` through a script before parsing? The `_source` field would still store the original document, but the parsing of individual fields would be applied to the result of the script.
</comment><comment author="nik9000" created="2014-06-19T14:48:45Z" id="46569857">@jpountz that's a great idea!  I hate to do scripting work while #6233 is still in flight but I like the idea.  I'm going to be on a train a few hours later today, maybe I'll be able to put something together.
</comment><comment author="clintongormley" created="2014-06-19T15:28:31Z" id="46575502">@jpountz i like the idea...  actually i've heard the request from others to be able to change the `_source` that is stored as well.  Perhaps support both?
</comment><comment author="jpountz" created="2014-06-19T15:31:26Z" id="46575928">I'm not sure that is a good idea. If you index into `index1` and then scan `index1` to index into `index2`, your indices might be different even if they share the same mappings.
</comment><comment author="clintongormley" created="2014-06-19T15:35:29Z" id="46576501">sure
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FilterBuilds and QueryBuilders should have accessor methods</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6565</link><project id="" key="" /><description>I'd like to have accessor methods on all FilterBuilders and QueryBuilders.  This will facilitate distributed query construction as well as automatic query optimization code.  Right now I'm working on the latter and can't proceed without these accessor methods.
</description><key id="36080256">6565</key><summary>FilterBuilds and QueryBuilders should have accessor methods</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shadow000fire</reporter><labels><label>:Java API</label><label>feedback_needed</label></labels><created>2014-06-19T13:55:20Z</created><updated>2015-11-21T15:07:58Z</updated><resolved>2015-11-21T15:07:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-02-20T13:59:32Z" id="75241539">&gt;  This will facilitate distributed query construction as well as automatic query optimization code.

More information about this would be useful. Also, if there are optimizations that can be automatically applied, I think elasticsearch should do it by itself?
</comment><comment author="javanna" created="2015-10-15T17:12:30Z" id="148461637">If this is about adding the missing getters for each instance member in our query builder objects, this has been done as part of the query-refactoring which is now in the master branch. @shadow000fire can you please confirm that is what you meant?
</comment><comment author="clintongormley" created="2015-11-21T15:07:58Z" id="158650340">No feedback in a year. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add shards filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6564</link><project id="" key="" /><description>I'd like to have a shards_filter that acts very similar to the indices filter (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-indices-filter.html) where a different filter is executed on different shards.  Instead of specifying index names, you would specify a shard number or routing value.  

When specifying shard numbers, the shard would only execute the filter if its own shard # is in the list.  When specifying routing values, the shard would only execute the filter if one of the values in the list would be routed to this shard.  

Also, the node that receives the initial SearchRequest should route the request based on the "shards" key.

An example request that uses shard numbers
{
    "indices" : {
        "shards" : [0,1],
        "filter" : {
            "term" : { "tag" : "wow" }
        },
        "no_match_filter" : {
            "term" : { "tag" : "kow" }
        }
    }
}

An example request that uses routing values
{
    "indices" : {
        "routing" : ["CompanyA"],
        "filter" : {
            "term" : { "tag" : "wow" }
        },
        "no_match_filter" : {
            "term" : { "tag" : "kow" }
        }
    }
}
</description><key id="36079332">6564</key><summary>Add shards filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shadow000fire</reporter><labels><label>discuss</label></labels><created>2014-06-19T13:44:10Z</created><updated>2015-11-21T15:07:35Z</updated><resolved>2015-11-21T15:07:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-19T15:02:07Z" id="46571870">@shadow000fire what's the use case for this?

Either way, I think the numbered shards idea won't fly. And with routing, I'm not sure how much you gain here as the filters will be cached anyway. If a term doesn't exist on one shard, the cached filter will tell it to skip all segments and no further work will be done.

This feels like overcomplicating matters, but maybe I'm missing something?
</comment><comment author="shadow000fire" created="2014-06-19T16:40:22Z" id="46585124">So here's our case, but maybe I'm getting something wrong.  We use a terms filter with the terms lookup mechanism that has around 50,000 terms. The execution mode is left to plain as fielddata had worse performance.  It seems that performance is directly related to the number of terms in that filter.  So i thought if i could break that big list into say 10 pieces (10 shards) ahead of time, and the execute 10 different filters, each only on the shard it applies to.  

Currently our index is 20 shards, but will likely go higher, so I'm trying to avoid hitting every shard for every request.
</comment><comment author="clintongormley" created="2015-11-21T15:07:35Z" id="158650316">Revisiting this discussion, I see it hasn't garnered any interest in the previous year and a half.  Given that search execution has changed dramatically in 2.x, I'm going to close this issue for now.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Client intermediate interfaces removal follow-up</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6563</link><project id="" key="" /><description>After #6517 we ended up registering all of the actions (included admin ones) to the NodeClient. 
Made sure that only the proper type of `Action` instances are registered to each client type.
Also fixed some compiler warnings: unused members, imports and non matching generic types.
</description><key id="36077863">6563</key><summary>Client intermediate interfaces removal follow-up</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-19T13:26:54Z</created><updated>2015-06-07T13:12:13Z</updated><resolved>2014-06-19T16:01:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-19T15:12:40Z" id="46573311">LGTM
</comment><comment author="jpountz" created="2014-06-19T15:28:29Z" id="46575500">LGTM, safety first!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Raise proper failure if not fully reading translog entry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6562</link><project id="" key="" /><description>When reading a translog entry, raise a proper error when not reading it fully.
</description><key id="36071618">6562</key><summary>Raise proper failure if not fully reading translog entry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Translog</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-19T11:54:00Z</created><updated>2015-06-07T13:12:25Z</updated><resolved>2014-06-24T13:03:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-24T10:25:24Z" id="46955067">is there a chance to put the read method into forbidden APIs and have a helper that is allowed to read it that also checks for completeness?
</comment><comment author="bleskes" created="2014-06-24T12:43:13Z" id="46965696">I think this PR is superseded by #6576 and can be closed no?
</comment><comment author="kimchy" created="2014-06-24T13:03:24Z" id="46967610">@bleskes yes it can!, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Infrastructure for changing easily the significance terms heuristic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6561</link><project id="" key="" /><description>...euristic

This commit adds the infrastructure to allow pluging in different
measures for computing the significance of a term.
Significance measures can be provided externally by overriding
- SignificanceHeuristic
- SignificanceHeuristicBuilder
- SignificanceHeuristicParser

and registering Parser and Heuristic at the SignificantTermsHeuristicModule.

As a proof of concept, this commit also adds a second heuristic to the
already existing one (MutualInformation).
</description><key id="36066828">6561</key><summary>Infrastructure for changing easily the significance terms heuristic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-19T10:35:04Z</created><updated>2015-06-06T18:31:13Z</updated><resolved>2014-07-14T08:59:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-06-20T13:24:27Z" id="46677334">Looks great @brwe !  I have the Log Likelihood Ratio code from [Mahout](http://code.ohloh.net/file?fid=8dWpci-eq-DukJ0Y-jd2_M70UpQ&amp;cid=EMMy1BiXN54&amp;s=%22LogLikelihood%22&amp;fp=1398&amp;mp,=1&amp;projSelected=true&amp;mp=1&amp;ml=1&amp;me=1&amp;md=1#L31) if you want to bundle that too? 
I made a couple of tweaks with Ted's guidance as part of our tests.
</comment><comment author="brwe" created="2014-06-20T13:46:28Z" id="46679450">@markharwood I thought I should make a second pull request that adds that and also Chi square and all that? It is lots of code already
</comment><comment author="jpountz" created="2014-06-21T13:32:19Z" id="46753767">+1 to split into several pull requests
</comment><comment author="brwe" created="2014-06-23T10:57:16Z" id="46830015">I added two commits to add the deprecated names checking but only for the significant terms heuristics [here](https://github.com/brwe/elasticsearch/commit/ea97ba3809607e36a4253ec907d942a3fbf492e5#diff-80fa30028fc58a84b4d47f535018906eR64). It seems to me that deprecated names are never checked in aggregations anywhere unless I am missing something. I am now wondering if would make more sense to add that to aggregations in a separate commit.
</comment><comment author="brwe" created="2014-06-23T14:09:53Z" id="46849530">&gt;  I am now wondering if would make more sense to add that to aggregations in a separate commit.

Removed the strict parsing flag check again, seems to make more sense to do that consistently in a different pull request.
</comment><comment author="s1monw" created="2014-06-26T08:31:02Z" id="47200266">I only looked briefly at this but can we add extensive unittests for the individual heuristics, I think we should add those for these!
</comment><comment author="brwe" created="2014-07-01T07:55:38Z" id="47627287">Updated with new commits. @s1monw : I added unit tests in [SignificantTermsUnitTests](https://github.com/brwe/elasticsearch/blob/pluggable-significant-metrics/src/test/java/org/elasticsearch/search/aggregations/bucket/significant/SignificantTermsUnitTests.java), is that extensive enough?
@markharwood: I added assertions to the score computation and had to change one of the tests (see commits "test score assertions and score" and "check for shard failures...") - is the new behavior OK?
</comment><comment author="brwe" created="2014-07-02T11:35:40Z" id="47765283">@markharwood About the assertions in the scoring function: I agree, we might not always want to rely on the strict superset property. However, for mutual information we sort of rely on the fact that it is strict, else the computations do not make sense. 

Mutual information compares two sets and not so much foreground against background. I assumed that the two sets are the subset and the background without the subset. It therefore relies on knowing the frequency in the subset but also the frequency in the background set without the subset. Because currently I only get the background frequency, I have to do a subtraction of background frequency and foreground frequency to figure out how many are in the other set. 

Now an example:
Background contains 3 documents, but foreground contains 2 because the strict superset property was violated or because the two sets are completely independent. Now, if the function gets passed foreground freg = 3 and background freq=2 I know that one set contains 3 but I have no means to determine how many documents are actually in the other set as I do not know the overlap of the two sets. Subtraction of background frequency and foreground frequency is clearly wrong - I get a negative number and the computed value will have no meaning. Hence all the strict checks.

I will remove the assertions from the default score and only leave them in mutual information. Actually I am thinking I should replace the asserts by exceptions to make sure users are aware that whatever is computed is wrong...
</comment><comment author="markharwood" created="2014-07-02T11:47:16Z" id="47766173">I find practical uses of these significance algos on free text are vastly improved if the foreground sample is devoid of the sorts of duplicate text introduced by retweets, email replies, copyright notices etc. we find in typical content. This is the area I am working on at the moment to efficiently strip out repetitions and this will only add to the fuzziness of the numbers presented (e.g. I count only half of the text in documents in a result set). This will mean the foreground sample under-reports word frequencies and any significance algos shouldn't be too thrown off by that.
I'm closing to issuing a PR for this so it may be useful to try some of these alternative significant algos in this context.
</comment><comment author="brwe" created="2014-07-02T12:17:46Z" id="47768575">@markharwood the problem does not arise from under reports of word frequencies but from the inability to clearly distinguish what the frequencies in the two sets are that are compared. 
The current heuristic compares one set vs a background and the counts can be fuzzy I agree. But mutual information compares two distinct sets and the significance cannot be determined if the frequencies in each of the sets cannot be computed. 

&gt; This will mean the foreground sample under-reports word frequencies and any significance algos shouldn't be too thrown off by that.

Maybe I am missing something, but I do not see how I should compare two sets if I cannot determine the frequencies within them?
</comment><comment author="brwe" created="2014-07-02T12:28:40Z" id="47769461">Maybe we could let the user give a hint for mutual information if or if not the background is actually a strict superset or defines a completely different set. Something like

```
 "significant_terms": {
    "field": ...,
    ...
    "mutual_information": {
      "background_is_superset": true/false
    }
  }
```

and then derive the different frequencies depending on that? This way, the user would have all the flexibility.
</comment><comment author="markharwood" created="2014-07-02T12:43:03Z" id="47770677">&gt; Maybe I am missing something, but I do not see how I should compare two sets if I cannot determine the frequencies within them?

We potentially have a number of useful tools at our disposal in producing sample sets (background_filters, de-duping and doc "slicing") and they can all introduce some oddities into the numbers presented. Maybe it is a mistake to use words in the code like "subset" and "superset" to describe the numbers if certain algos expect that strict behaviour? Maybe foreground/background-sample are less charged words and your flag "is superset" helps clarify the position.
</comment><comment author="brwe" created="2014-07-02T12:45:39Z" id="47770893">Ok, to summaize: I will
1. remove the assertions completely from the default heuristic
2. Add the "is_superset" flag to the mutual information and derfive frequencies accordingly
3. Throw exception in mutual information if "is_superset":true and one of the currently checked properties is violated

@markharwood Do you agree?
</comment><comment author="markharwood" created="2014-07-02T12:54:19Z" id="47771661">Agreed on 1).

What is the behaviour if the user does not pass "is_superset" (in both the case where background is and isn't a superset)?
</comment><comment author="brwe" created="2014-07-02T13:02:00Z" id="47772442">I would set "is_superset": true per default. Then:

If the conditions are violated (any of the current assertions triggered) and "is_superset": true -&gt; exception
If the conditions are violated and "is_superset": false -&gt; assume two distinct sets and derive frequencies accordingly
If the conditions are not violated and "is_superset": true -&gt; derive frequencies as before
If the conditions are not violated and "is_superset": false -&gt; assume two distinct sets and derive frequencies accordingly
</comment><comment author="markharwood" created="2014-07-02T13:43:23Z" id="47777097">OK, makes sense
</comment><comment author="brwe" created="2014-07-03T07:14:54Z" id="47874191">I implemented all review comments I got so far, ready for the next review round.
</comment><comment author="markharwood" created="2014-07-03T10:16:39Z" id="47889433">I just wanted to register the concern that this may well become a scoring function with custom params in future. Shouldn't be too hard to refactor if we choose to add this later.
</comment><comment author="s1monw" created="2014-07-08T07:48:03Z" id="48281767">left a tiny comment - if you fix this you can push ie LGTM
</comment><comment author="markharwood" created="2014-07-08T18:10:16Z" id="48377748">Left a couple of small comments but otherwise looks great.
</comment><comment author="brwe" created="2014-07-11T14:00:19Z" id="48733355">implemented latest review comments
</comment><comment author="s1monw" created="2014-07-14T07:59:47Z" id="48873905">LGTM +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to use ordering in "Stats" aggregator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6560</link><project id="" key="" /><description>The new elasticsearch aggs functionality has stats aggregation. The facet aggregation earlier supported order by min,max,count etc. But the new aggs one doesn't. Can we expect it in the near future or I just don't know how to use it.
</description><key id="36064973">6560</key><summary>How to use ordering in "Stats" aggregator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">raghavj1991</reporter><labels /><created>2014-06-19T10:05:42Z</created><updated>2014-06-19T10:16:22Z</updated><resolved>2014-06-19T10:13:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-19T10:13:05Z" id="46543951">This feature is supported, see the 4th example on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_order

In the future, if you have a doubt on whether something is supported, a bug, etc., please ask on the mailing list first before opening an issue.
</comment><comment author="raghavj1991" created="2014-06-19T10:16:22Z" id="46544188">I will remember that thank you for the prompt response. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactored AckedClusterStateUpdateTask &amp; co. to remove code repetitions in subclasses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6559</link><project id="" key="" /><description>Made `AckedClusterStateUpdateTask` an abstract class instead of an interface, which contains the common methods.
Also introduced the `AckedRequest` interface to mark both `AcknowledgedRequest` &amp; `ClusterStateUpdateRequest` so that the different ways of updating the cluster state (with or without a `MetaData*Service`) can share the same code.
Removed `ClusterStateUpdateListener` as we can just use its base class `ActionListener` instead.
</description><key id="36059394">6559</key><summary>Refactored AckedClusterStateUpdateTask &amp; co. to remove code repetitions in subclasses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-19T08:41:58Z</created><updated>2015-06-07T13:12:48Z</updated><resolved>2014-06-20T18:19:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-06-19T21:00:27Z" id="46615963">Nice stats! LGTM.
</comment><comment author="jpountz" created="2014-06-20T09:10:08Z" id="46659113">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms facets count top 10,size:500 not valid</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6558</link><project id="" key="" /><description>Hi:
I'm using elasticsearch for the top 10 with count 500 item,But not valid.
would be glad to let me know if you have any knowledge about this, thanks.

```
{
            "facets": {
              "terms": {
                "terms": {
                  "field": "agent",
                  "size": 10,
                  "order": "count",
                  "exclude": []
                },
                "facet_filter": {
                  "fquery": {
                    "query": {
                      "filtered": {
                        "query": {
                          "bool": {
                            "should": [
                              {
                                "query_string": {
                                  "query": "*"
                                }
                              }
                            ]
                          }
                        },
                        "filter": {
                          "bool": {
                            "must": [
                              {
                                "range": {
                                  "@timestamp": {
                                    "from": null,
                                    "to": null
                                  }
                                }
                              }
                            ]
                          }
                        }
                      }
                    }
                  }
                }
              }
            },
            "size": 500
          }
```
</description><key id="36056093">6558</key><summary>Terms facets count top 10,size:500 not valid</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chickenlove</reporter><labels /><created>2014-06-19T07:44:11Z</created><updated>2014-06-19T12:11:54Z</updated><resolved>2014-06-19T12:11:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-19T12:11:54Z" id="46552754">please ask user questions like this on the google group (to leave github issues for bugs) and make sure you provide a fully fledged example including indexing/mapping/configuration, see http://elasticsearch.org/help

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_Export  command returning 52 : Empty reply from server. while using Knapsack</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6557</link><project id="" key="" /><description /><key id="36054685">6557</key><summary>_Export  command returning 52 : Empty reply from server. while using Knapsack</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Avneets</reporter><labels /><created>2014-06-19T07:16:12Z</created><updated>2014-06-19T12:07:34Z</updated><resolved>2014-06-19T12:07:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-19T12:07:34Z" id="46552411">Please file this issue against the knapsack repository, as you used features from an external plugin, which is not part of elasticsearch itself. Thanks a lot!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-word match query is slow when there are many words</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6556</link><project id="" key="" /><description>According to the documentation, a multi-word match query is converted into a boolean query that consists of one term query for each clause. When the the multi-match query contains many clauses (1000+), the query performance is significantly slower (5x-10x) than querying Lucene directly with a BooleanQuery object. Is it possible to add an option to search with a Lucene BooleanQuery instead of converting into many term queries?  
</description><key id="36039524">6556</key><summary>Multi-word match query is slow when there are many words</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mphung</reporter><labels /><created>2014-06-19T00:11:07Z</created><updated>2015-09-16T13:34:29Z</updated><resolved>2014-10-17T06:56:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-19T14:39:08Z" id="46568418">@mphung I don't understand what you mean.  A Lucene BooleanQuery doesn't query words directly.  Any way that you do it, you're having to do term queries in lucene, and combine their scores with a boolean or dis_max query...

I'm not surprised that 1000+ words on multiple fields is slow.  That's a lot of term lookups that it has to do.  Perhaps provide an example of what you're doing in Elasticsearch and what you are doing in Lucene?
</comment><comment author="mphung" created="2014-06-19T16:55:03Z" id="46587012">Hi Clinton,
We're just looking to query one field with many words. For example:

String queryString = "word1 word2 word3 word998, word999, word1000";
TopDocsCollector&lt;ScoreDoc&gt; collector = TopScoreDocCollector.create(results_size, true);
QueryParser queryParser = new QueryParser(Version.LUCENE_44, "words_field", new WhitespaceAnalyzer(Version.LUCENE_44));
queryParser.setDefaultOperator(QueryParser.OR_OPERATOR);
Query query = queryParser.parse(queryString); 
indexSearcher.search(query, collector);

I'm using this query in ElasticSearch:
{
  "query": {
    "match": {
      "words_field": {
        "query": "word1 word2 word3 word998 word999 word1000",
        "analyzer": "whitespace",
        "operator": "or"
      }
    }
  }
}
My understanding is that this will get converted to many term queries.

When we search against Lucene directly, our latencies are around 70ms. When we search on ElasticSearch, it takes about 400ms - 500ms.

Thanks
</comment><comment author="clintongormley" created="2014-06-19T17:06:03Z" id="46588289">@mphung 

Both of those queries do the same thing under the covers.  I think the differences that you are seeing have to do with (a) the fact that with the lucene example you're hitting a single Lucene index while with ES you're hitting probably 5 shards (lucene indices), and (b) you probably haven't configured Elasticsearch, eg ES_HEAP_SPACE etc.

I'd ask for more information on the forum - this isn't a bug.
</comment><comment author="mphung" created="2014-06-19T17:14:19Z" id="46589331">I've configured the ES_HEAP_SPACE and ElasticSearch is just running on one machine with one shard. I figured this would be most similar to one Lucene index running on one machine.  In haven't had much luck on the forums or IRC but I'll keep trying. Thanks
</comment><comment author="clintongormley" created="2014-06-19T17:29:45Z" id="46591232">Sorry, should have been ES_HEAP_SIZE... not SPACE. And you're right, one index with one shard should be similar.  Are you getting the same results?

The exact equivalent of what you're doing in Lucene would be:

```
{
  "query_string": {
    "query": "word1...",
    "fields": ["words_field"],
    "analyzer": "whitespace",
    "default_operator": "or"
  }
}
```

but that should translate into the same thing under the covers as the `match` query that you showed.
</comment><comment author="mphung" created="2014-06-19T17:45:00Z" id="46593013">Yes, the ElasticSearch queries are still significantly slower. I have the same 1 million documents in both Lucene and ElasticSearch. 
</comment><comment author="clintongormley" created="2014-06-19T17:45:12Z" id="46593035">Hi @mphung 

@jpountz (one of the Lucene committers) has just had a look and says:

&gt; one difference between the two queries is that ES would go fetch some hits (even though I would be surprised that it accounts for 330ms). Otherwise I just checked that the same rewrite method would be used in both cases, and the same scorer since ES doesn't force in-order scoring.

So essentially there appears to be a difference in setup between ES and Lucene.
</comment><comment author="jpountz" created="2014-06-19T17:56:33Z" id="46594407">@mphung Could you verify that both indices contain as many documents, that queries return the same hit count, the same top hits and that scores are identical?
</comment><comment author="mphung" created="2014-06-24T16:13:11Z" id="46992799">@jpountz 
I've verified that the indices are the same and they return the same results with the same scores. When we search with a smaller number of clauses, about 100 or less, the queries are much faster with most completing in less than 20ms. We only see the increase latency with the large number of clauses.    
</comment><comment author="clintongormley" created="2014-07-01T12:44:07Z" id="47650911">Would be interesting to get to the bottom of this.  @mphung could you provide as much information as possible to allow us to reproduce your test?
</comment><comment author="mphung" created="2014-07-01T17:07:10Z" id="47683100">We're currently running ElasticSearch 1.1.1 on RHEL5.

Each document contains 1 to 1000 unique words but most documents have 800 to 1000 words. Each word has a maximum of 15 characters. The words are separated by a space so we use the whitespace analyzer.

We just run a Boolean query with "or" as the operator with around 1,000 clauses.  I tested with 1 million of these documents on ElasticSearch and directly against Lucene (Version 4.4).  Please let me know if you need any more information.
</comment><comment author="jpountz" created="2014-07-04T08:52:44Z" id="48021828">Since you have lots of terms, those queries are presumably quite terms-dictionary-intensive, do you have similar numbers of segments in both indices? (On lucene just count the number of leaves and on elasticsearch you can use the [segments API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-segments.html) to figure it out.)
</comment><comment author="mphung" created="2014-07-04T10:28:56Z" id="48029197">@jpountz 
I "optimized" both indices before running my tests. The Lucene index has one leaf. I got this number by calling leaves() on a IndexReader object. The Elastic Search index has 24 segments.

"num_committed_segments" : 24,
"num_search_segments" : 24,

I thought an optimize would combine all segments into one. Do you know why there is more than one segment?
</comment><comment author="clintongormley" created="2014-07-04T15:16:54Z" id="48054863">Did you do an optimize like this?

```
POST /myindex/_optimize?max_num_segments=1
```
</comment><comment author="mphung" created="2014-07-08T16:47:37Z" id="48366613">I was using the optimize command from the "Head" UI. I set the maximum number of segments to 1 but now I see that this does not work through the UI. After using the command that Clinton suggested, the ElasticSearch index now has 1 segment and the search is much faster. Thanks @clintongormley and @jpountz  for all the help. One last question. Is setting  "index.merge.policy.segments_per_tier" to a low number the best way to achieve a low number of segments?  
</comment><comment author="clintongormley" created="2014-07-08T20:38:53Z" id="48396497">@mphung good to hear. 

@mikemccand any opinion about the segments_per_tier setting?
</comment><comment author="mikemccand" created="2014-07-18T12:24:40Z" id="49424722">Woops, I missed this...

You should lower both index.merge.policy.segments_per_tier (controls how many segments are allowed to be in the index) and index.merge.policy.max_merge_at_once (controls how many segments are merged at a time), assuming you are using tiered merge policy.
</comment><comment author="vorce" created="2015-09-16T13:34:29Z" id="140744015">Very interesting. We decreased the time for some nasty queries by almost 20x after optimizing one of our indices down to 1 segment. Thanks for the insight!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unassigned shards: Failed to start shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6555</link><project id="" key="" /><description>Hi All,

I've recently been struggling with some intermittent issues with unassigned shards. Once a day or two I get into a red state with a handful of unassigned shards and have to use the reroute api to get them assigned.

I was able to find the following line from the logs on the master node:

[2014-06-18 20:19:10,644][WARN ][cluster.action.shard     ] [prod-sidekiq2] [1815_2014_18][1] received shard failed for [1815_2014_18][1], node[eMYIqhQTR0S1m4UeRJlLxg], [P], s[INITIALIZING], indexUUID [uHW84Y49Qg2KTZnI9dmLJw], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[1815_2014_18][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[1815_2014_18][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: [segments.gen, _checksums-1399837226100]]; nested: FileNotFoundException[segments_8]; ]]

I found issue #4674, which sounds very similar as I am using multi data paths, but I'm running 1.0.0 so I guess the fix for this got into that release.

```
curl localhost:9200
{
  "status" : 200,
  "name" : "prod-es3",
  "version" : {
    "number" : "1.0.0",
    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",
    "build_timestamp" : "2014-02-12T16:18:34Z",
    "build_snapshot" : false,
    "lucene_version" : "4.6"
  },
  "tagline" : "You Know, for Search"
}
```

Any one have any suggestions as to how this might be diagnosed/fixed?

Would this issue be resolved with an upgrade to 1.0.3 or 1.2.?
</description><key id="36027800">6555</key><summary>Unassigned shards: Failed to start shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nedcampion</reporter><labels /><created>2014-06-18T21:08:25Z</created><updated>2015-04-28T09:23:00Z</updated><resolved>2015-04-28T09:22:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-07-23T15:04:07Z" id="49886406">@nedcampion did you upgrade in the meanwhile and did this occur again?
</comment><comment author="nedcampion" created="2014-07-25T13:57:44Z" id="50152415">I did upgrade to 1.2.1.  We still get unassigned shards occasionally, though it does seem less often. Below is it just happening earlier today. It seems it may be related to opening a closed index. Not sure if we are provoking a situation where a segment file would go missing or if this is a bug. We'll be doing the 1.3 upgrade in the near future so I can post back after that upgrade and some time has elapsed to further observe.

Here's the example (following  `1625_2014_15`). I see this line:

```
[2014-07-25 13:26:04,941][INFO ][cluster.metadata         ] [prod-app1] opening indices [[1625_2014_15, 1625_2014_13, 1625_2014_12, 1625_2014_08, 1625_2014_07, 1625_2014_09, 1625_2014_11, 1625_2014_10]]
```

And then shortly after:

```
[2014-07-25 13:26:06,091][WARN ][cluster.action.shard     ] [prod-app1] [1625_2014_15][1] received shard failed for [1625_2014_15][1], node[yfzRHgBKS2mNEdWAjfK13g], [P], s[INITIALIZING], indexUUID [iK73ROyYQbOyoiMHTvHMMQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[1625_2014_15][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[1625_2014_15][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_checksums-1398718632101, segments.gen]]; nested: FileNotFoundException[segments_4]; ]]
```
</comment><comment author="s1monw" created="2015-04-28T09:22:59Z" id="96986378">long time no feedback and I assume this is fixed with shard locks in newer version... closing for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog entry checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6554</link><project id="" key="" /><description>It's possible that a transaction log can be corrupted on disk, causing it to throw exceptions like:

```
Exception in thread "main" org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [108]
    at org.elasticsearch.index.VersionType.fromValue(VersionType.java:307)
    at org.elasticsearch.index.translog.Translog$Index.readFrom(Translog.java:506)
    at org.elasticsearch.index.translog.TranslogStreams.readTranslogOperation(TranslogStreams.java:52)
    at org.writequit.Main.main(Main.java:22)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
```

It would be nice if we had checksums for each entry in the translog, to determine corruption in a more-friendly manner than throwing an exception with "No version type match".
</description><key id="36026293">6554</key><summary>Translog entry checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>blocker</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-18T20:50:38Z</created><updated>2014-09-10T07:46:02Z</updated><resolved>2014-09-10T07:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-18T21:56:38Z" id="46499719">To clarify this, we should add checksums first, and then optionally discuss markers and entry skipping.
</comment><comment author="s1monw" created="2014-09-10T07:34:56Z" id="55080757">ping @dakrone what is the status here?
</comment><comment author="dakrone" created="2014-09-10T07:46:02Z" id="55081679">opened #7668 for the marking feature and resolving this since the translog checksums work has been merged to master and 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OOM on percolators</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6553</link><project id="" key="" /><description>steps to reproduce:
- start 1.2.1 without any index
- create index and add percolators
- start percolating
- see ram usages raising and raising.

Facts:
- We only have in this test environment around 143 percolators.
- We percolate around 200.000 items against the percolator.

[2014-06-18 16:06:32,277][DEBUG][action.percolate         ] [Black Widow] [searching_06be3f3d970eb86180aee114ea873838][0], node[KxkxW_-
5S4Sj36x8144e9w], [P], s[STARTED]: failed to executed [org.elasticsearch.action.percolate.PercolateRequest@1ad4f508]
org.elasticsearch.percolator.PercolateException: failed to percolate
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:198)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOp
erationAction.java:170)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
        at org.apache.lucene.util.RecyclingIntBlockAllocator.getIntBlock(RecyclingIntBlockAllocator.java:82)
        at org.apache.lucene.util.IntBlockPool.nextBuffer(IntBlockPool.java:155)
        at org.apache.lucene.util.IntBlockPool.newSlice(IntBlockPool.java:168)
        at org.apache.lucene.util.IntBlockPool.access$200(IntBlockPool.java:26)
        at org.apache.lucene.util.IntBlockPool$SliceWriter.startNewSlice(IntBlockPool.java:274)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:471)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:395)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:370)
        at org.elasticsearch.percolator.MultiDocumentPercolatorIndex.indexDoc(MultiDocumentPercolatorIndex.java:88)
        at org.elasticsearch.percolator.MultiDocumentPercolatorIndex.prepare(MultiDocumentPercolatorIndex.java:68)
        at org.elasticsearch.percolator.PercolatorService.percolate(PercolatorService.java:232)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:194)
        ... 5 more

[2014-06-18 16:07:57,493][DEBUG][action.percolate         ] [Black Widow] [searching_06be3f3d970eb86180aee114ea873838][0], node[KxkxW_-5S4Sj36x8144e9w], [P], s[STARTED]: failed to executed [org.elasticsearch.action.percolate.PercolateRequest@4c2ffcd2]
org.elasticsearch.percolator.PercolateException: failed to percolate
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:198)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
[2014-06-18 16:08:31,930][DEBUG][action.percolate         ] [Black Widow] [searching_06be3f3d970eb86180aee114ea873838][0], node[KxkxW_-5S4Sj36x8144e9w], [P], s[STARTED]: failed to executed [org.elasticsearch.action.percolate.PercolateRequest@3efcd38f]
org.elasticsearch.percolator.PercolateException: failed to percolate
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:198)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.OutOfMemoryError: Java heap space
</description><key id="36015870">6553</key><summary>OOM on percolators</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">julianhille</reporter><labels /><created>2014-06-18T18:51:50Z</created><updated>2014-07-02T07:41:37Z</updated><resolved>2014-07-01T13:07:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-18T19:12:35Z" id="46480192">Do the mapping of the documents being percolated contain nested object types?
</comment><comment author="julianhille" created="2014-06-18T19:14:06Z" id="46480370">yes they do.
</comment><comment author="martijnvg" created="2014-06-18T19:22:55Z" id="46481460">Do you rely on the nested query/filter in your percolator queries? Before 1.1.x these queries silently failed, and just didn't match anything and since 1.1.x the nested support has been added to the percolator.

Each nested object has its own memory index that later on gets wrapped by a composite reader to simulate a single index, this too support the nested query/filter in percolate api. How many nested objects do your document being percolate more or less have?

Just to verify: If you add a new mapping with the type set to `object` instead of `nested` on all your object fields, does the oom still happen?
</comment><comment author="julianhille" created="2014-06-18T19:26:29Z" id="46481912">Every document as around 2-20 nested objects, and we have around 200.000 docs.
we totally rely on that. It does not seem to have failed before as we relied on them before. it may have returned wrong values / results. 

Btw. we upgraded from 0.90.1x

i can test the object filtering later if you want me to.
</comment><comment author="martijnvg" created="2014-06-19T06:40:53Z" id="46528284">No need for that, I found that this issue is relatively easily reproducible. 
</comment><comment author="martijnvg" created="2014-06-19T12:59:53Z" id="46556702">This issue here is that the filter cache doesn't clean cache immediately after the in-memory index has been destroyed. It keeps a reference this index as it is the cache key and cleans it after 60 seconds.

Can you try setting: `indices.cache.filter.clean_interval` setting to `1s` in the elasticsearch.yml file to verify if this prevents the OOM in your app as well?

For the percolator nothing should be cached (filters, fielddata) at all, this should be the right fix.
</comment><comment author="martijnvg" created="2014-06-19T13:57:39Z" id="46563152">&gt; Can you try setting: indices.cache.filter.clean_interval setting to 1s in the elasticsearch.yml file to verify if this prevents the OOM in your app as well?

Sorry, no need to check this. Just this only won't fix it. The percolator doesn't close the sub memory readers properly, which is the first reason why the issue you reported arrises.
</comment><comment author="tiran" created="2014-06-19T14:05:47Z" id="46564142">Hi Martijn!

I'm a co-worker of Julian. You are right, the workaround doesn't fix the issue. Even 1ms doesn't make any difference.
</comment><comment author="martijnvg" created="2014-06-20T14:13:59Z" id="46682335">@tiran @julianhille If you like you can try out the PR that addresses this bug: https://github.com/elasticsearch/elasticsearch/pull/6578
</comment><comment author="julianhille" created="2014-06-20T20:36:11Z" id="46723722">currently testing.
</comment><comment author="julianhille" created="2014-06-20T21:32:20Z" id="46729018">first feedback is, that it seems to work, i dont see the ram usage raising that fast anymore. Will run now some more tests, if we see the same behavior as before.
</comment><comment author="julianhille" created="2014-06-21T06:13:25Z" id="46745821">havent found any other issue so far.
</comment><comment author="martijnvg" created="2014-07-01T13:09:49Z" id="47653435">@julianhille @tiran Thanks for test driving the PR #6578!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get field mapping api doesn't honor pretty flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6552</link><project id="" key="" /><description>The problem is due to the use of `XContentHelper#writeRawField` in [`GetFieldMappingsResponse`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/mapping/get/GetFieldMappingsResponse.java#L118) which is great  when dealing with the `_source` field as we never want to prettify it, but in this case the output should be prettified if requested.

Steps to reproduce:

```
curl -XPUT localhost:9200/index/type/1 -d '{
   "foo":"bar"
 }
 '

curl localhost:9200/_mapping/field/foo?pretty

{
  "index" : {
    "mappings" : {
      "type" : {
        "foo" : {
          "full_name" : "foo",
          "mapping":{"foo":{"type":"string"}}
        }
      }
    }
  }
}
```
</description><key id="36007678">6552</key><summary>Get field mapping api doesn't honor pretty flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels><label>bug</label></labels><created>2014-06-18T17:21:09Z</created><updated>2014-12-12T10:35:16Z</updated><resolved>2014-12-11T14:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>BulkRequest#add(Iterable) to support UpdateRequests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6551</link><project id="" key="" /><description>The BulkRequest#add(Iterable) method doesn't support updates, probably some leftover.
</description><key id="36000858">6551</key><summary>BulkRequest#add(Iterable) to support UpdateRequests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-18T16:04:11Z</created><updated>2015-06-07T19:39:44Z</updated><resolved>2014-06-19T08:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-18T18:30:16Z" id="46474824">LGTM should we backport this as well to `1.2.2` ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix possibility of losing meta configuration on field mapping update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6550</link><project id="" key="" /><description>The TTL, size, timestamp and index meta properties could be lost on an
update of a single field mapping due to a wrong comparison in the
merge method (which was caused by a wrong initialization, which marked
an update as explicitely disabled instead of unset.

Closes #5053
</description><key id="35992661">6550</key><summary>Fix possibility of losing meta configuration on field mapping update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-18T14:46:47Z</created><updated>2015-06-07T23:27:56Z</updated><resolved>2014-06-19T07:13:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-18T18:32:22Z" id="46475082">LGTM this should also go into `1.2.2`?
</comment><comment author="spinscale" created="2014-06-19T07:22:13Z" id="46530773">pushed into master, 1.x and  1.2 (labels are on the issue)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix 'top-tags' and 'terms' ordering in example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6549</link><project id="" key="" /><description>aggregation_name is switched with aggregation_type.
</description><key id="35985763">6549</key><summary>Fix 'top-tags' and 'terms' ordering in example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">crutch</reporter><labels><label>docs</label></labels><created>2014-06-18T13:43:29Z</created><updated>2014-08-18T10:31:20Z</updated><resolved>2014-08-18T10:31:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T19:04:34Z" id="51516889">Hi @crutch 

Thanks for the fix - i left one comment about a typo. Please could I ask you to sign our CLA so that I can get this merged in?

http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="clintongormley" created="2014-08-18T10:31:20Z" id="52475469">Already fixed by https://github.com/elasticsearch/elasticsearch/commit/3b2b57e7f73c0636b8b864cff447d3be55220a1c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[STORE]: Make use of Lucene build-in checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6548</link><project id="" key="" /><description>Since Lucene version 4.8 each file has a checksum written as it's
footer. We used to calculate the checksums for all files transparently
on the filesystem layer (Directory / Store) which is now not necessary
anymore. This commit makes use of the new checksums in a backwards
compatible way such that files written with the old checksum mechanism
are still compared against the corresponding Alder32 checksum while
newer files are compared against the Lucene build in CRC32 checksum.

Since now every written file is checksummed by default this commit
also verifies the checksum for files during recovery and restore if
applicable.

Closes #5924

Note: This PR still has a bunch of nocommits and I need to crank up some more bwcompat tests but I wanted to get the word out... all tests pass
</description><key id="35974685">6548</key><summary>[STORE]: Make use of Lucene build-in checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-06-18T11:13:09Z</created><updated>2014-07-08T08:40:39Z</updated><resolved>2014-07-08T08:40:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-18T12:15:59Z" id="46427819">this looks great!, very clean. left some minor comments.
</comment><comment author="s1monw" created="2014-06-23T14:50:53Z" id="46855184">@kimchy @imotov @rmuir I think I am ready for serious reviews now... lots of problems and possible bugs fixed here... I will update the description soon too.
</comment><comment author="imotov" created="2014-07-02T02:38:18Z" id="47731642">@s1monw, I did a cursory review of the changes and left a few nitpicking comments. The only significant problem that I found is that `SharedClusterSnapshotRestoreTests.dataFileFailureDuringRestoreTest` is failing on my machine. So far, I am not able to figure out what's causing this failures.
</comment><comment author="s1monw" created="2014-07-04T15:28:44Z" id="48056248">@kimchy @imotov I pushed a new commit taking a new approach. Instead of renaming the files we write a `corrupted_${uuid}` file with the exception in it that caused the problem. This will also prevent replica allocation on top of this store if a primary is "reelected" after failure in the case we have replicas and we prevent recovering from gateway. I think this approach is cleaner and gives the user the ability to simple wipe the file if they want to have the shard back?
</comment><comment author="kimchy" created="2014-07-07T10:40:56Z" id="48163740">@s1monw did another round, left cosmetic comment, good to go on my end
</comment><comment author="imotov" created="2014-07-07T20:40:34Z" id="48237618">@s1monw I did another review, left a couple of cosmetic comments. StoreModule still has some strings that can be replaced with constants. Otherwise, looks good to me.
</comment><comment author="imotov" created="2014-07-08T02:14:13Z" id="48264637">@s1monw I spoke too soon. It still fails in dataFileFailureDuringRestoreTest from time to time with a [slightly different error](https://gist.github.com/imotov/8a8b3c10eff270ddf969) though. From one side I think we should [clean files that failed to restore](https://github.com/imotov/elasticsearch/commit/db798905e00f6f866820a5e0b497409fcc3eefb6) to avoid issues like this. From the other side, it seems that it seems to bring the cluster into some condition when a corrupted index cannot be restored. 
</comment><comment author="s1monw" created="2014-07-08T08:39:06Z" id="48285822">&gt; @s1monw I spoke too soon. It still fails in dataFileFailureDuringRestoreTest from time to time with a slightly different error though. From one side I think we should clean files that failed to restore to avoid issues like this. From the other side, it seems that it seems to bring the cluster into some condition when a corrupted index cannot be restored.

I started a branch on the shard github repo hooked into CI where I just fixed this issue. Have a look at the latest [commit](https://github.com/elasticsearch/elasticsearch/commit/f264284892e4eebfb601e4253594eb1d45262c71). I will close this PR in favour of the new [branch](https://github.com/elasticsearch/elasticsearch/tree/feature/checksums). we can discuss it there and also work on in on that branch. What  IMO is still missing is a lot of testing when snapshots get corrupted I could totally use some help here. I also plan to add BWC tests for snapshots to ensure we work with old indices there just fine. 
</comment><comment author="s1monw" created="2014-07-08T08:40:39Z" id="48285960">closing this - there is a new PR here https://github.com/elasticsearch/elasticsearch/pull/6776
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Add tests to ensure sort order is correct for metrics aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6547</link><project id="" key="" /><description>Could do with having some tests that, for each metric aggregation, ensure the sort order and the sort keys are correct when using the aggregation for sorting
</description><key id="35971324">6547</key><summary>[TEST] Add tests to ensure sort order is correct for metrics aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>test</label></labels><created>2014-06-18T10:17:56Z</created><updated>2015-11-28T15:09:59Z</updated><resolved>2015-11-28T15:09:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-18T18:23:11Z" id="46473862">+1
</comment><comment author="clintongormley" created="2015-11-21T15:04:26Z" id="158650179">@colings86 is this still needed?
</comment><comment author="colings86" created="2015-11-23T08:56:44Z" id="158879551">@clintongormley we don't have those tests still, but I think this actually isn't needed as the numeric metric aggregations all output a double value which is easy to sort. We have tests to make sure sorting works on a sub-aggregation so maybe that's enough? The only change that could be worth making is to randomise the metric used in the sorted for that test, but that might be tricky as the sort order of the buckets would change too so it might be complex to calculate the correct sort order in the tests based on a random option
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins: Removing plugin does not fail when plugin dir is read only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6546</link><project id="" key="" /><description>If you try to remove a plugin in read only dir, you get a successful result:

```
$ bin/plugin --remove marvel
-&gt; Removing marvel 
Removed marvel
```

But actually the plugin has not been removed.

When installing, if fails properly:

```
$ bin/plugin -i elasticsearch/marvel/latest
-&gt; Installing elasticsearch/marvel/latest...

Failed to install elasticsearch/marvel/latest, reason: plugin directory /usr/local/elasticsearch/plugins is read only
```
</description><key id="35968960">6546</key><summary>Plugins: Removing plugin does not fail when plugin dir is read only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-18T09:42:56Z</created><updated>2016-09-01T05:47:13Z</updated><resolved>2014-07-04T15:29:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lety4ent" created="2016-01-27T20:12:40Z" id="175831789">You could list your plugins with "plugin list".
</comment><comment author="SrinivasThodupunuri" created="2016-09-01T05:47:13Z" id="243982197">To uninstall Marvel:
Shut down Elasticsearch and Kibana.
Remove the Marvel agent plugin from Elasticsearch: bin/plugin remove marvel-agent.
Remove the Marvel Kibana app: bin/kibana plugin --remove marvel.
Restart Elasticsearch and Kibana.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Also send Refresh and Flush actions to relocation targets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6545</link><project id="" key="" /><description>Currently we send relocation &amp; flush actions based on all assigned ShardRoutings. During the final stage of relocation, we may miss to refresh/flush a shard if the coordinating node has not yet processed the cluster state update indicating that a relocation is completed _and_ the relocation target node has already processed it (i.e., started the shard and has accepted new indexing requests).
</description><key id="35967562">6545</key><summary>Also send Refresh and Flush actions to relocation targets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>regression</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-18T09:23:06Z</created><updated>2015-06-07T19:42:04Z</updated><resolved>2014-06-18T14:15:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-18T09:30:21Z" id="46414078">LGTM except of the test comment
</comment><comment author="bleskes" created="2014-06-18T09:31:15Z" id="46414160">Thx. Repeat removed.
</comment><comment author="s1monw" created="2014-06-18T11:14:19Z" id="46422583">LGTM
</comment><comment author="bleskes" created="2014-06-18T11:20:07Z" id="46423027">@kimchy  I've pushed another commit, with a modified approach based on your feedback /cc @s1monw 
</comment><comment author="kimchy" created="2014-06-18T11:24:00Z" id="46423357">LGTM, left a comment to try and centralize the creation of initializing relocating shard, we do it in enough places now that I think its worth it
</comment><comment author="bleskes" created="2014-06-18T12:48:32Z" id="46430611">@kimchy pushed new commit with a centralized relocation ShardRouting creation.
</comment><comment author="kimchy" created="2014-06-18T12:52:25Z" id="46430992">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix handling of nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6544</link><project id="" key="" /><description>Nested documents were indexed as separate documents, but it was never checked
if the hits represent nested documents or not. Therefore, nested objects could
match not nested queries and nested queries could also match not nested documents.

Examples are in issue #6540 .

closes #6540
</description><key id="35961146">6544</key><summary>Fix handling of nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-18T07:40:19Z</created><updated>2015-06-07T19:42:46Z</updated><resolved>2014-06-18T10:26:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-18T08:01:57Z" id="46406419">Thanks for fixing this! LGTM
</comment><comment author="brwe" created="2014-06-18T09:30:07Z" id="46414053">Added a commit for fixing the percolate query and add more options for testing. @martijnvg can you check if [this](https://github.com/brwe/elasticsearch/commit/64d9fa4028009bcffbe1bb1a70d0549e208e22a7#diff-6bfb6ce1807c4d812228a3b4b3a91b88R1977) is what you meant with the percolate count api?
</comment><comment author="martijnvg" created="2014-06-18T10:14:44Z" id="46418020">@brwe Yes, that is what I meant. +1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>indices.memory.index_buffer_size optimization</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6543</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/memory/IndexingMemoryController.java#L238

Current, a shard will get a percentage of the indexing buffer on a node allocated unless it is  fully closed.  A potentially useful optimization would be to prevent allocating an indexing buffer unless the shard is actually writable.  Namely

index.blocks.read_only

Should not be set to true for the given shard in question.  This may be quite beneficial in environments where there are a large number of shards per node.
</description><key id="35955614">6543</key><summary>indices.memory.index_buffer_size optimization</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EricMCornelius</reporter><labels><label>:Core</label><label>adoptme</label><label>enhancement</label></labels><created>2014-06-18T05:27:40Z</created><updated>2015-11-21T15:03:53Z</updated><resolved>2015-11-21T15:03:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-18T09:52:34Z" id="46416079">aye, this makes sense. Note, that it already detects a shard that was not written to for a period of time, and de-allocates the buffer from it to spread it around.
</comment><comment author="clintongormley" created="2015-11-21T15:03:53Z" id="158650158">Closing in favour of https://github.com/elastic/elasticsearch/pull/14121
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Queue Capacity issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6542</link><project id="" key="" /><description>We are regularly getting the following message in version 1.2.1, even though we've changed no settings. According to the documentation here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html the default is either -1 (unbounded) or 200.  Either the documentation needs to be updated or the defaults in code need to be updated

&gt; {"error":"RemoteTransportException[[ev-web01][inet[/1.1.1.1:9300]][index]];  nested: EsRejectedExecutionException[rejected execution (queue capacity 200) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1@4fdfb29c]; ","status":503}
</description><key id="35952515">6542</key><summary>Queue Capacity issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sallgeud</reporter><labels /><created>2014-06-18T03:48:19Z</created><updated>2014-06-19T11:56:08Z</updated><resolved>2014-06-19T11:56:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-19T11:56:08Z" id="46551481">The documentation states (at least that is, what it intends to say), that it in the case of `queue_size` not being set, it defaults to `-1`/`unbounded`, if the type is fixed. The index thread pool is of type fixed, but has a `queue_size` configured (`200` as mentioned in the listing of thread pools), so it is not unbounded.

You can potentially set it to unbounded, but this may just postpone a performance problem with your cluster, as you then simply put all the pending requests into a memory queue instead of rejecting them, posing the thread og going out of memory.

Please explain your problem more detailed on the mailinglist, as github issues should only be used for bugs and issues. Thanks a lot!

If you think the paragraph wording could be improved, please create a PR by all means!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>New entry/front-end</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6541</link><project id="" key="" /><description>Adding, Calaca, a simple search client for ElasticSearch.
</description><key id="35941177">6541</key><summary>New entry/front-end</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">romansanchez</reporter><labels /><created>2014-06-17T23:14:37Z</created><updated>2014-07-07T12:49:29Z</updated><resolved>2014-07-07T08:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-07T08:17:22Z" id="48151551">Hi @romansanchez 

Please could you fix ElasticSearch -&gt; Elasticsearch and sign our CLA so that we can get your commit merged in. http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-07-07T08:22:09Z" id="48151926">Closed in favour of #6754 
</comment><comment author="romansanchez" created="2014-07-07T12:49:29Z" id="48173665">Thanks, I've signed CLA.

-Roman

On Monday, July 7, 2014, Clinton Gormley notifications@github.com wrote:

&gt; Closed #6541 https://github.com/elasticsearch/elasticsearch/pull/6541.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/6541#event-138634304
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator: Fix handling of nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6540</link><project id="" key="" /><description>The percolator appears to have some strange matching behavior with nested objects defined in the document mapping. Steps to reproduce:

curl -XDELETE localhost:9200/test

curl -XPUT localhost:9200/test -d '{
  "settings": {
    "index.number_of_shards": 1,
    "index.number_of_replicas": 0
  },
  "mappings": {
    "doc": {
      "properties": {
        "persons": {
          "type": "nested"
        }
      }
    }
  }
}'

curl -XPUT localhost:9200/test/.percolator/1 -d '{
  "query": {
    "bool": {
      "must": {
        "match": {
          "name": "obama"
        }
      }
    }
  }
}'

curl -XPUT localhost:9200/test/.percolator/2 -d '{
  "query": {
    "bool": {
      "must_not": {
        "match": {
          "name": "obama"
        }
      }
    }
  }
}'

curl -XPUT localhost:9200/test/.percolator/3 -d '{
  "query": {
    "bool": {
      "must": {
        "match": {
          "persons.foo": "bar"
        }
      }
    }
  }
}'

curl -XPUT localhost:9200/test/.percolator/4 -d '{
  "query": {
    "bool": {
      "must_not": {
        "match": {
          "persons.foo": "bar"
        }
      }
    }
  }
}'

curl -XPUT localhost:9200/test/.percolator/5 -d '{
  "query": {
    "bool": {
      "must": {
        "nested": {
          "path": "persons",
          "query": {
            "match": {
              "persons.foo": "bar"
            }
          }
        }
      }
    }
  }
}'

curl -XPUT localhost:9200/test/.percolator/6 -d '{
  "query": {
    "bool": {
      "must_not": {
        "nested": {
          "path": "persons",
          "query": {
            "match": {
              "persons.foo": "bar"
            }
          }
        }
      }
    }
  }
}'

curl -XPOST localhost:9200/_refresh

Thi should only match 1 and 5, but instead it matches 1,2,3,4,5,6

curl -XPOST localhost:9200/test/doc/_percolate?pretty -d '{
  "doc": {
    "name": "obama",
    "persons": [
      {
        "foo": "bar"
      }
    ]
  }
}'
</description><key id="35939935">6540</key><summary>Percolator: Fix handling of nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bly2k</reporter><labels><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-17T22:54:20Z</created><updated>2014-09-02T15:00:09Z</updated><resolved>2014-06-18T10:26:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tribe nodes joining two clusters with the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6539</link><project id="" key="" /><description>A customer has requested the ability for a tribe node to span two clusters with the same name.  This particular request is for use in an environment with a legacy cluster and a new cluster being brought online to replace it, but I could see the same feature being used to span two different datacenters in the future.
</description><key id="35932544">6539</key><summary>Tribe nodes joining two clusters with the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">seang-es</reporter><labels><label>:Tribe Node</label><label>enhancement</label><label>feedback_needed</label></labels><created>2014-06-17T21:14:35Z</created><updated>2015-04-26T19:56:46Z</updated><resolved>2015-04-26T19:56:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-03-20T12:46:56Z" id="84005216">This is not possible with multicast discovery, cause the tribe node has to have access to both clusters and needs to be able to distinguish between the two. 

That said, it works with unicast discovery out of the box. Let's say the two clusters are on separate networks, the tribe node has access to both and points to them this way:

```
tribe.t1.discovery.zen.ping.unicast: tribe1:9300
tribe.t2.discovery.zen.ping.unicast: tribe2:9300
```

there is not even need to specify the cluster name for the tribe node configuration, but only the unicast addresses, the tribes can have whatever node, even the same.

@seang-es could you please let me know if my answer addresses your concerns?
</comment><comment author="javanna" created="2015-04-02T10:55:42Z" id="88864967">ping @seang-es could you please have a look at my answer?
</comment><comment author="clintongormley" created="2015-04-26T19:56:45Z" id="96428569">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>java.lang.AssertionError when scan &amp; scroll</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6538</link><project id="" key="" /><description>I am getting the following error when performing a scroll request.

I am running this in a test (ElasticSearchIntegrationTest) - I dont know if that is important or not. Oddly, I have never got this error when running normally.

```
java.lang.AssertionError
    at org.elasticsearch.common.util.BigArrays$LongArrayWrapper.get(BigArrays.java:200)
    at org.elasticsearch.test.cache.recycler.MockBigArrays$LongArrayWrapper.get(MockBigArrays.java:374)
    at org.elasticsearch.common.util.BytesRefHash.get(BytesRefHash.java:66)
    at org.elasticsearch.common.util.BytesRefHash.set(BytesRefHash.java:101)
    at org.elasticsearch.common.util.BytesRefHash.add(BytesRefHash.java:145)
    at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$WithOrdinals.collect(StringTermsAggregator.java:299)
    at org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:164)
    at org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)
    at org.apache.lucene.search.Scorer.score(Scorer.java:65)
    at org.apache.lucene.search.AssertingScorer.score(AssertingScorer.java:136)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:282)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:270)
    at org.elasticsearch.action.search.type.TransportSearchScrollQueryThenFetchAction$AsyncAction.executeQueryPhase(TransportSearchScrollQueryThenFetchAction.java:200)
    at org.elasticsearch.action.search.type.TransportSearchScrollQueryThenFetchAction$AsyncAction.access$600(TransportSearchScrollQueryThenFetchAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchScrollQueryThenFetchAction$AsyncAction$2.run(TransportSearchScrollQueryThenFetchAction.java:184)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)

```

Looks like a bug to me. I will try write a simple test that reproduces it.
</description><key id="35901509">6538</key><summary>java.lang.AssertionError when scan &amp; scroll</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-06-17T15:34:54Z</created><updated>2014-07-08T19:53:28Z</updated><resolved>2014-07-08T19:53:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-17T15:39:28Z" id="46324398">Thanks!  What version of Elasticsearch are you using? 
</comment><comment author="nickminutello" created="2014-06-17T16:11:33Z" id="46328968">Sorry, I was busy trying to get a simple test case to reproduce the error. 

This is elastic version 1.1.1.

For completeness this is on:
**jdk1.7.0_60**
java version "1.7.0_60"
Java(TM) SE Runtime Environment (build 1.7.0_60-b19)
Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)

**Windows 7 Enterprise**
 (Version 6.1 (Build7601: Service Pack 1)

**Processor**
AMD64
</comment><comment author="nickminutello" created="2014-06-17T16:35:34Z" id="46332142">Here is a unit test that _should_ reproduce it, but as much as I try, I cant seem to get it to reproduce the failure.
In my system test, it fails all the time...
Not sure what is materially different between this test and what my system test is ultimately executing.

That said, I do always get a failure during teardown: "java.lang.RuntimeException: Unclosed Searchers instance for shards: [[bugindex][3],[bugindex][6],[bugindex][1],[bugindex][4],[bugindex][5],[bugindex][7],[bugindex][2],[bugindex][0],]"

``` java
package indexing.bugs;

import java.io.IOException;
import java.util.Map;

import org.elasticsearch.action.search.SearchRequestBuilder;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.action.search.SearchType;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.Test;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.joda.JodaModule;

import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;
import static org.elasticsearch.test.ElasticsearchIntegrationTest.Scope.SUITE;


@ElasticsearchIntegrationTest.ClusterScope(scope = SUITE, numNodes = 1)
public class ElasticBugs extends ElasticsearchIntegrationTest
{
    private static final String INDEX = "bugindex";
    private final ObjectMapper json = new ObjectMapper();

    public ElasticBugs()
    {
        json.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);
        json.configure(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN, true);
        json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
        json.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);
        json.registerModule(new JodaModule());
    }

    @Override
    protected Settings nodeSettings(int nodeOrdinal)
    {
        return settingsBuilder()
            .put("path.data", "target/elastic-test-data")
            .build();
    }

    private void index(String id, String type, String content) throws IOException
    {
        //noinspection unchecked
        index(INDEX, type, id, json.readValue(content, Map.class));
    }

    @Test
    public void scrollBug() throws IOException
    {
        admin()
            .indices()
            .prepareCreate(INDEX)
            .execute()
            .actionGet();

        String propertiesSource =
            "{\"properties\":{\"contractualLocationCity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"contractualLocationCountry\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isAllocationNotInvoiced\":{\"type\":\"boolean\"},\"isFullyAllocated\":{\"type\":\"boolean\"},\"allocatedQuantity\":{\"type\":\"double\"},\"contractualLocation\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"isInvoicedFinal\":{\"type\":\"boolean\"},\"isZeroAllocated\":{\"type\":\"boolean\"},\"quotaId\":{\"index\":\"not_analyzed\",\"type\":\"long\"},\"counterParty\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"allocationStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantity\":{\"type\":\"double\"},\"allocatedQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"tradeRef\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantity\":{\"type\":\"double\"},\"contract\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractualLocationCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"shape\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"isActive\":{\"type\":\"boolean\"},\"shipmentDate\":{\"format\":\"date\",\"type\":\"date\"},\"completedDate\":{\"format\":\"date\",\"type\":\"date\"},\"groupCompany\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceQuantity\":{\"type\":\"double\"},\"resolvedIds\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"uninvoicedQuantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"expectedSalesMonth\":{\"format\":\"date\",\"type\":\"date\"},\"expectedSalesLocationCity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isCancelled\":{\"type\":\"boolean\"},\"neptuneQuotaId\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"uninvoicedQuantityNormalised\":{\"type\":\"double\"},\"contractDutyPaid\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantityNormalised\":{\"type\":\"double\"},\"contractIncotermsCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"groupCompanyCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"grade\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"unallocatedQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"counterPartyCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isPurchase\":{\"type\":\"boolean\"},\"isCompleted\":{\"type\":\"boolean\"},\"trafficOperatorFullName\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"groupCompanyId\":{\"include_in_all\":false,\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocation\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"intentIncotermsCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"trafficOperatorSid\":{\"include_in_all\":false,\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityNormalised\":{\"type\":\"double\"},\"commodity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractIncoterms\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocationCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"brand\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaType\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocationCountry\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaRef\":{\"index\":\"not_analyzed\",\"boost\":2.0,\"type\":\"string\"},\"comments\":{\"type\":\"string\"},\"intentIncoterms\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}}";
        admin()
            .indices()
            .preparePutMapping(INDEX)
            .setType("quota")
            .setSource(propertiesSource)
            .execute()
            .actionGet();

        index("659819037", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFA4D\\u670A\\uD9CE\\uDCD6\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyId\":\"93C5DE5E03024B6D9237D4E030A37B4C\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":true,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"isZeroAllocated\":false,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quotaId\":\"659819037\",\"quotaRef\":\"3597.2\",\"quotaStatus\":\"CANCELLED\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"CF91B74013EC4050A975342BEF6E70DB\",\"93C5DE5E03024B6D9237D4E030A37B4C\",\"355B0197BA754686ABA1DAF2FF8527FC\"],\"shape\":null,\"shipmentDate\":\"2014-07-02\",\"tradeRef\":\"edm-trade-id\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"unallocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\"}");
        index("381617768", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFA4D\\u670A\\uD9CE\\uDCD6\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyId\":\"93C5DE5E03024B6D9237D4E030A37B4C\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"isZeroAllocated\":false,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quotaId\":\"381617768\",\"quotaRef\":\"3597.1\",\"quotaStatus\":\"OPEN\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"CF91B74013EC4050A975342BEF6E70DB\",\"93C5DE5E03024B6D9237D4E030A37B4C\",\"355B0197BA754686ABA1DAF2FF8527FC\"],\"shape\":null,\"shipmentDate\":\"2014-07-02\",\"tradeRef\":\"edm-trade-id\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"unallocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\"}");
        index("1707279120", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFA4D\\u670A\\uD9CE\\uDCD6\",\"completedDate\":\"2013-06-17\",\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyId\":\"93C5DE5E03024B6D9237D4E030A37B4C\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":true,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"isZeroAllocated\":false,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quotaId\":\"1707279120\",\"quotaRef\":\"3597.5\",\"quotaStatus\":\"COMPLETED\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"CF91B74013EC4050A975342BEF6E70DB\",\"93C5DE5E03024B6D9237D4E030A37B4C\",\"355B0197BA754686ABA1DAF2FF8527FC\"],\"shape\":null,\"shipmentDate\":\"2014-07-02\",\"tradeRef\":\"edm-trade-id\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"unallocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\"}");
        index("1331247902", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFA4D\\u670A\\uD9CE\\uDCD6\",\"completedDate\":\"2014-05-17\",\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyId\":\"93C5DE5E03024B6D9237D4E030A37B4C\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":true,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"isZeroAllocated\":false,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quotaId\":\"1331247902\",\"quotaRef\":\"3597.4\",\"quotaStatus\":\"COMPLETED\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"CF91B74013EC4050A975342BEF6E70DB\",\"93C5DE5E03024B6D9237D4E030A37B4C\",\"355B0197BA754686ABA1DAF2FF8527FC\"],\"shape\":null,\"shipmentDate\":\"2014-07-02\",\"tradeRef\":\"edm-trade-id\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"unallocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\"}");
        index("831568342", "quota", "{\"allocatedQuantity\":1000.0000,\"allocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFA4D\\u670A\\uD9CE\\uDCD6\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-17\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\u7A06\\uE7D7\\u3B9B\\uEC08\\u0010\",\"groupCompanyId\":\"93C5DE5E03024B6D9237D4E030A37B4C\",\"invoiceQuantity\":1000.0000,\"invoiceQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":false,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":true,\"isZeroAllocated\":false,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"quotaId\":\"831568342\",\"quotaRef\":\"3597.3\",\"quotaStatus\":\"PENDINGCANCELLATION\",\"quotaType\":\"PURCHASE\",\"resolvedIds\":[\"CF91B74013EC4050A975342BEF6E70DB\",\"93C5DE5E03024B6D9237D4E030A37B4C\",\"355B0197BA754686ABA1DAF2FF8527FC\"],\"shape\":null,\"shipmentDate\":\"2014-07-02\",\"tradeRef\":\"edm-trade-id\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\",\"unallocatedQuantityUnitOfMeasure\":\"7\\u88DB\\uDA27\\uDD16:\\u03AF6\\u4D6F\\uDB84\\uDC82\"}");

        String searchSource = "{\"from\":0,\"size\":500,\"query\":{\"bool\":{\"must\":[{\"term\":{\"_type\":\"quota\"}},{\"terms\":{\"groupCompanyId\":[\"93C5DE5E03024B6D9237D4E030A37B4C\"]}},{\"bool\":{\"must\":{\"prefix\":{\"_all\":{\"prefix\":\"cancelled\"}}}}}]}},\"aggregations\":{\"aggregated_quantity\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"suggestion_commodity\":{\"terms\":{\"field\":\"commodity\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_commodity\":{\"global\":{},\"aggregations\":{\"suggestion_commodity\":{\"terms\":{\"field\":\"commodity\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_groupCompany\":{\"terms\":{\"field\":\"groupCompany\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_groupCompany\":{\"global\":{},\"aggregations\":{\"suggestion_groupCompany\":{\"terms\":{\"field\":\"groupCompany\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_counterParty\":{\"terms\":{\"field\":\"counterParty\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_counterParty\":{\"global\":{},\"aggregations\":{\"suggestion_counterParty\":{\"terms\":{\"field\":\"counterParty\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_trafficOperatorSid\":{\"terms\":{\"field\":\"trafficOperatorSid\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_trafficOperatorSid\":{\"global\":{},\"aggregations\":{\"suggestion_trafficOperatorSid\":{\"terms\":{\"field\":\"trafficOperatorSid\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_brand\":{\"terms\":{\"field\":\"brand\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_brand\":{\"global\":{},\"aggregations\":{\"suggestion_brand\":{\"terms\":{\"field\":\"brand\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_grade\":{\"terms\":{\"field\":\"grade\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_grade\":{\"global\":{},\"aggregations\":{\"suggestion_grade\":{\"terms\":{\"field\":\"grade\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_shape\":{\"terms\":{\"field\":\"shape\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_shape\":{\"global\":{},\"aggregations\":{\"suggestion_shape\":{\"terms\":{\"field\":\"shape\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}},\"suggestion_quotaStatus\":{\"terms\":{\"field\":\"quotaStatus\",\"size\":500,\"order\":{\"_count\":\"desc\"}}},\"global_suggestion_quotaStatus\":{\"global\":{},\"aggregations\":{\"suggestion_quotaStatus\":{\"terms\":{\"field\":\"quotaStatus\",\"size\":500,\"order\":{\"_count\":\"desc\"}}}}}}}";

        String keepAlive = "30s";
        SearchRequestBuilder request = client()
            .prepareSearch(INDEX)
            .setSource(searchSource)
            .setSearchType(SearchType.DEFAULT)
            .setScroll(keepAlive);

//        System.out.println("request = " + prettyPrint(searchSource));

        SearchResponse searchResponse = request
            .execute()
            .actionGet();

        System.out.println("search response = " + searchResponse);

        String scrollId = searchResponse.getScrollId();

        while (true) {
            SearchResponse scrollResponse = client()
                .prepareSearchScroll(scrollId)
                .setScroll(keepAlive)
                .execute()
                .actionGet();
            System.out.println("=====================================================");
            System.out.println("scroll response = " + scrollResponse);

            assertEquals(0, scrollResponse.getFailedShards());

            if (scrollResponse.getHits().getHits().length == 0) {
                break;
            }
        }
    }
}

```
</comment><comment author="nickminutello" created="2014-06-17T16:50:01Z" id="46333954">In my system test, the error seems to be related to the aggregations. If I remove the aggregations, I dont get this assertion error resulting in the shard failure.
</comment><comment author="s1monw" created="2014-06-17T20:14:38Z" id="46359068">that is actually expected. the test-framework asserts after the test that no resources like open searchers or `BigArray` instances are left unreleased etc. If you do scrolling we keep state on the shard level and keep the searcher open as well as bigarrays potentially. If you forget to clean this up we will clear it after a while (5 min by default) but the test will fail unless you clear the scroll. I makes sure you don't leave any state behind. 

does this make sense? If so please close the issue.
</comment><comment author="nickminutello" created="2014-06-18T09:54:14Z" id="46416233">The original issue is the shard failure due to the AssertionError. Thats still open as far as I can tell.

I have removed references to the test cleanup to avoid cluttering this issue.
</comment><comment author="nickminutello" created="2014-06-23T13:44:17Z" id="46846379">bump?
</comment><comment author="jpountz" created="2014-07-04T09:29:02Z" id="48024749">@nickminutello The error is in the aggregations, it seems that the hash table that we use to compute counts gets corrupted. I think there is high probability that it is due to https://github.com/elasticsearch/elasticsearch/issues/1642 since on subsequent rounds, aggregations are operating on content that has been released at the end of the first round (which is a bug).
</comment><comment author="clintongormley" created="2014-07-08T19:53:28Z" id="48391517">Closing in favour of #1642
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PHP API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6537</link><project id="" key="" /><description>Hi.  First time posting an issue.  Please let me know if I can improve.

Found an issue in the SnapshotNamespace.php file on line 212.  

As written:
$endpoint = $endpointBuilder('Snapshot\Get');

Should be:
$endpoint = $endpointBuilder('Snapshot\Restore');

Cheers,

Brian
</description><key id="35901092">6537</key><summary>PHP API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cre8nkos</reporter><labels /><created>2014-06-17T15:30:54Z</created><updated>2014-06-17T15:51:04Z</updated><resolved>2014-06-17T15:51:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-17T15:51:04Z" id="46326041">hi @cre8nkos 

Please open this issue here instead: https://github.com/elasticsearch/elasticsearch-php/issues

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs for creating plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6536</link><project id="" key="" /><description>It would be extremely helpful to have docs and commented examples for creating the various types of Elasticsearch plugins (river, new mapper type, new query type, etc.). Currently the only way to create a plugin is to cargo-cult from an existing plugin and hope that it has code to do what you need.

For example, I'm running into an issue where I would like to create a new mapper type that needs to index sub-objects and nested objects; however, I can only find mapper type plugins that use simple fields. I have no idea how to index more complex types due to no Javadoc of the API, comments in the plugins, etc.
</description><key id="35898954">6536</key><summary>Docs for creating plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tanadeau</reporter><labels /><created>2014-06-17T15:11:31Z</created><updated>2014-12-30T20:11:47Z</updated><resolved>2014-12-30T20:11:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-17T15:27:38Z" id="46322718">Creating a plugin now requires that you have the Elasticsearch source code checked out and open for reference.  There isn't really a clean separation between interfaces plugins should use and those that Elasticsearch uses internally.  This is unfortunate but isn't really uncommon - I also maintain a MediaWiki plugin and it has the same problem only its older and there are hundreds or thousands of plugins floating around rather then tens of plugins.
</comment><comment author="tanadeau" created="2014-06-17T16:12:27Z" id="46329087">Looking at the Elasticsearch source code has been helpful and got me fairly far; however, I still am not sure how to index complicated/nested objects from within a new mapper type. Do you know of a plugin that does this?
</comment><comment author="nik9000" created="2014-06-17T16:16:39Z" id="46329644">No, sorry.

On Tue, Jun 17, 2014 at 12:12 PM, tanadeau notifications@github.com wrote:

&gt; Looking at the Elasticsearch source code has been helpful and got me
&gt; fairly far; however, I still am not sure how to index complicated/nested
&gt; objects from within a new mapper type. Do you know of a plugin that does
&gt; this?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6536#issuecomment-46329087
&gt; .
</comment><comment author="clintongormley" created="2014-12-30T20:11:47Z" id="68393222">We have no plans to provide detailed documentation for creating custom plugins.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix JSON response for significant terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6535</link><project id="" key="" /><description>Commit fbd7c9aa5d6c1c introduced a regression that caused
the min_doc_count to be equal to the number of documents in the
background set. As a result no buckets were built when the
response for significant terms was created.
This only affected the final XContent response.
</description><key id="35897069">6535</key><summary>Fix JSON response for significant terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Aggregations</label><label>bug</label><label>regression</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-17T14:54:40Z</created><updated>2015-06-07T19:43:02Z</updated><resolved>2014-06-18T16:53:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-17T23:52:13Z" id="46380665">Good catch! +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolated and query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6534</link><project id="" key="" /><description>i have 100 queries  and index with data { name: string }
So, i try to use percolated to find data math this query, but nothing match.

What i did:

```
PUT localhost:9200/test
PUT localhost:9200/test/data/_mapping 
{
  "data" : {
    "properties": {
      "name": { "type" : "string" }
    }
  }
}
```

```
PUT localhost:9200/test/data/1 
{
  "name": "test"
}
PUT localhost:9200/test/data/2
{
  "name": "dev"
}
```

```
PUT localhost:9200/test/.percolate/simply-search
{
  "query": {
    "simple_query_string" : {
        "query": "test -dev",
        "analyzer": "snowball",
        "fields": ["_all"],
        "default_operator": "and"
    }
  }
}
```

After:

```
GET localhost:9200/test/data/1/_percolate
GET localhost:9200/test/data/2/_percolate
```

Both result

```
{
   "took":2,
   "_shards":{
      "total":5,
      "successful":5,
      "failed":0
   },
   "total":0,
  "matches":[ ]
}
```

Where mistake? Or its bug?
I use Ubuntu 12.04 and 

```
{
   "status":200,
   "name":"Commander Kraken",
   "version":{
      "number":"1.2.1",
      "build_hash":"6c95b759f9e7ef0f8e17f77d850da43ce8a4b364",
      "build_timestamp":"2014-06-03T15:02:52Z",
      "build_snapshot":false,
      "lucene_version":"4.8"
   },
   "tagline":"You Know, for Search"
}
```

P.S. On my site i have thousands of items and i need put every item in specifying category. Every category have field in query format like: '"(ror|ruby) book"~5 -PHP'
</description><key id="35896579">6534</key><summary>percolated and query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">One-art</reporter><labels /><created>2014-06-17T14:50:19Z</created><updated>2014-06-18T09:09:24Z</updated><resolved>2014-06-18T09:09:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="One-art" created="2014-06-18T06:06:54Z" id="46399212">Maybe someone can say in which direction i should search?
</comment><comment author="clintongormley" created="2014-06-18T09:09:24Z" id="46412223">@One-art Please ask questions like this in the forum, not on the issues list.

the problem is that you are using `.percolate`, instead of `.percolator`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added SLES11 Support to Initscript</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6533</link><project id="" key="" /><description>Added SLES11 Support to Initscript
</description><key id="35886854">6533</key><summary>Added SLES11 Support to Initscript</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">winpat</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2014-06-17T13:10:49Z</created><updated>2016-02-14T23:19:48Z</updated><resolved>2015-01-22T15:03:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-06-17T16:45:20Z" id="46333350">@spinscale can you review this as well?
</comment><comment author="electrical" created="2014-07-09T14:08:34Z" id="48475547">Hi,

Did some testing on SLES 11 and getting the following error when using this init script.

```
# /etc/init.d/init.d/elasticsearch start
Starting elasticsearch: startproc:  cannot open /var/log/elasticsearch: Is a directory
```

Seems the -l option in startproc needs to be a file and not a dir.
</comment><comment author="clintongormley" created="2014-10-20T13:31:30Z" id="59753720">Hi @winpat 

Any chance of fixing the issues in this PR so that we can get it merged in?

thanks
</comment><comment author="winpat" created="2014-11-10T13:00:36Z" id="62380872">Hi Clinton Gormley

SLES 12 just got released 1 or 2 weeks ago.
They&#8217;re using systemd now instead of the old sysv init scripts,
so there&#8217;s no need for this PR anymore.

I will test the OpenSUSE RPM in the next couple of days&#8230;

&gt; Am 20.10.2014 um 15:32 schrieb Clinton Gormley notifications@github.com:
&gt; 
&gt; Hi @winpat https://github.com/winpat
&gt; Any chance of fixing the issues in this PR so that we can get it merged in?
&gt; 
&gt; thanks
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub https://github.com/elasticsearch/elasticsearch/pull/6533#issuecomment-59753720.
</comment><comment author="nocturnal3d" created="2015-01-13T16:39:50Z" id="69774673">Hi guys, please continue with PR - we are still using SLES11 and would benefit from this change.
</comment><comment author="electrical" created="2015-01-19T10:38:50Z" id="70474066">I'll do some work on this today since allot of people still use SLES 11 and see if i can have this merged soon.
</comment><comment author="electrical" created="2015-01-22T15:03:36Z" id="71033208">Closing if favour of #9366
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Initscript: SLES11 SP3 Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6532</link><project id="" key="" /><description>Tested on Suse Enterprise Server 11 ServicePack 3
</description><key id="35885023">6532</key><summary>Initscript: SLES11 SP3 Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">winpat</reporter><labels /><created>2014-06-17T12:46:39Z</created><updated>2014-06-17T12:46:52Z</updated><resolved>2014-06-17T12:46:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mapping: Fix delete mapping race condition.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6531</link><project id="" key="" /><description>If multiple clients attempted to delete a mapping, or a single client attempted to delete a mapping as an index is being
created a NPE could be observed in the MetaDataMappingService. This fix makes sure we don't try to access a null indexMetaData object.
Also add a test to spawn multiple create and delete threads to provoke this race condition.

Closes #5997
</description><key id="35879652">6531</key><summary>Mapping: Fix delete mapping race condition.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/GaelTadh/following{/other_user}', u'events_url': u'https://api.github.com/users/GaelTadh/events{/privacy}', u'organizations_url': u'https://api.github.com/users/GaelTadh/orgs', u'url': u'https://api.github.com/users/GaelTadh', u'gists_url': u'https://api.github.com/users/GaelTadh/gists{/gist_id}', u'html_url': u'https://github.com/GaelTadh', u'subscriptions_url': u'https://api.github.com/users/GaelTadh/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/5190064?v=4', u'repos_url': u'https://api.github.com/users/GaelTadh/repos', u'received_events_url': u'https://api.github.com/users/GaelTadh/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/GaelTadh/starred{/owner}{/repo}', u'site_admin': False, u'login': u'GaelTadh', u'type': u'User', u'id': 5190064, u'followers_url': u'https://api.github.com/users/GaelTadh/followers'}</assignee><reporter username="">GaelTadh</reporter><labels /><created>2014-06-17T11:24:16Z</created><updated>2014-10-18T10:42:41Z</updated><resolved>2014-10-18T10:42:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-17T11:28:24Z" id="46295269">hi @GaelTadh 

I don't think this fixes #5997 completely. it's only one part of the problem.  the other part is the fact that delete mapping on partially allocated indices just hangs until it eventually times out.
</comment><comment author="clintongormley" created="2014-08-07T19:03:01Z" id="51516703">Hi @GaelTadh 

Any progress on this one?
</comment><comment author="clintongormley" created="2014-10-18T10:42:41Z" id="59606831">Fixed by #7744
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>regarding the Chinese translation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6530</link><project id="" key="" /><description>We are going to translate the official reference into Chinese&#65292;there was a old copy&#65288;https://github.com/elasticsearch-cn/elasticsearch-cn.github.com&#65289;&#65292;but not completed&#65292;and we are going to restart the project&#65292;and that already translated pages will be migrated into the new repo&#65306;https://github.com/elasticsearch-cn/elasticsearch-reference&#65292;and will using AsciiDoc as well&#65292;is it possible to add a little link after we finishe that?
</description><key id="35874441">6530</key><summary>regarding the Chinese translation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2014-06-17T10:03:50Z</created><updated>2014-12-30T20:10:19Z</updated><resolved>2014-12-30T20:10:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T20:10:19Z" id="68393081">Hi @medcl 

If you get the translation done, we can certainly add a link

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Moved BucketsAggregator#docCounts field to IntArray</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6529</link><project id="" key="" /><description /><key id="35871150">6529</key><summary>Moved BucketsAggregator#docCounts field to IntArray</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-17T09:18:39Z</created><updated>2015-06-07T13:12:58Z</updated><resolved>2014-06-17T11:36:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-17T09:23:11Z" id="46285129">LGTM
Thanks @martijnvg for taking care of this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>-d64 parameter not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6528</link><project id="" key="" /><description>I'm using ES 1.1.2 , java 1.7.0_51

After upgrading ES I get trouble passing -d64 as a parameter when starting elasticsearch.

I start with the standard elasticsearch script and pass in -d64 in JAVA_OPTS

This is logged in my error log:
getopt: invalid option -- '6'
getopt: invalid option -- '4'

I believe the problem might be the new -d parameter conflicts with the -d64 parameter?

relevant sections in the elasticsearch script:

#    -d: daemonize, start in background
...
# Parse any command line options.
...
        -d)
            daemonized="yes"
            shift
        ;;

Is the -d64 java opt still reccomended to use? How to pass it to ES?
</description><key id="35870692">6528</key><summary>-d64 parameter not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ingebrigt</reporter><labels /><created>2014-06-17T09:12:00Z</created><updated>2014-12-30T21:12:25Z</updated><resolved>2014-12-30T20:08:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T20:08:34Z" id="68392849">Hi @ingebrigt 

The `-d64` option is not required. See http://stackoverflow.com/a/24133656/819598
</comment><comment author="ingebrigt" created="2014-12-30T21:12:25Z" id="68398964">thanx @clintongormley 

I'm no longer working on the project I used ES for, but will let the people that took over know.

btw, found someone else that seem to have the same problem:
http://elasticsearch-users.115913.n3.nabble.com/Java-d64-Option-td3206638.html#a4068321
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>version 1.2.1 has_child query with function_score can not access child's numeric field value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6527</link><project id="" key="" /><description>index script as:
# !/bin/sh
# assume the index products doesn't exist
# otherwise uncomment the following line
# curl -XDELETE localhost:9200/products
# echo ""

curl -XPOST 'localhost:9200/products/' -d '{
    "index" : {
        "number_of_shards" : 4,
        "number_of_replicas" : 1
    }
}'
echo ""

curl -XPOST localhost:9200/products/product/_mapping -d '{
  "product":{
    "properties" : {
      "property1" : {"type" : "string"}
    }
  }
}'
echo ""

curl -XPOST localhost:9200/products/offer/_mapping -d '{
  "offer":{
    "_parent": {"type": "product"},
    "properties" : {
      "color" : {"type" : "string"},
      "size" : {"type" : "integer"},
      "price" : {"type" : "float"}
    }
  }
}'
echo ""

curl -XPUT localhost:9200/products/product/1 -d'{
  "property1": "value1"
}'
echo ""

curl -XPUT localhost:9200/products/product/2 -d'{
  "property1": "value2"
}'
echo ""

curl -XPOST localhost:9200/products/offer/1?parent=1 -d '{
   "color": "blue",
   "size": 1,
   "price": 99.4
}'
echo ""

curl -XPOST localhost:9200/products/offer/2?parent=1 -d '{
   "color": "red",
   "size": 2,
   "price": 100.5
}'
echo ""

curl -XPOST localhost:9200/products/offer/3?parent=2 -d '{
   "color": "blue",
   "size": 3,
   "price": 100.7
}'
echo ""

search script as:
curl -s -XPOST 'localhost:9200/products/product,offer/_search?pretty=true' -d '{
  "query" : {
    "has_child" : {
      "type" : "offer",
      "score_mode" : "max",
      "query" : {
        "function_score" : {
          "boost_mode" : "replace",
          "query" : {
            "bool" : {
              "must" : [
                { "term" : { "color" : "blue" } }
              ]
            }
          },
          "script_score" : {
            "script" : "doc['offer.price'].value"
          }
        }
      }
    }
  }
}'
echo ""

get exception:
nested: PropertyAccessException[[Error: could not access: offer; in class: org.elasticsearch.search.lookup.DocLookup]\n[Near : {... doc[offer.price].value ....}

Can function_score get child's numeric field as parent's score like version 0.90.x custom_score did?
</description><key id="35866611">6527</key><summary>version 1.2.1 has_child query with function_score can not access child's numeric field value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fiefdx</reporter><labels /><created>2014-06-17T08:10:10Z</created><updated>2014-06-17T08:14:13Z</updated><resolved>2014-06-17T08:14:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-17T08:14:13Z" id="46279056">Could you please ask your question on the mailing list? We can definitely help you there.
We keep github issues only for issues and feature requests after discussion on the mailing list.

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Discovery] when master is gone, flush all pending cluster states</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6526</link><project id="" key="" /><description>If the master FD flags master as gone while there are still pending cluster states, the processing of those cluster states we re-instate that node a master again.

**NOTE:** this PR is for the `feature/improve_zen` branch
</description><key id="35866434">6526</key><summary>[Discovery] when master is gone, flush all pending cluster states</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-06-17T08:06:41Z</created><updated>2014-06-17T12:29:05Z</updated><resolved>2014-06-17T12:29:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-17T12:27:13Z" id="46299880">LGTM
</comment><comment author="bleskes" created="2014-06-17T12:29:05Z" id="46300045">merged with: b680b784308327cb3011088093e53b555e9c47c7
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>es 1.2.1 in has_child query can not use custom_score and function_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6525</link><project id="" key="" /><description>use as:

``` sh
#!/bin/sh

# assume the index products doesn't exist
# otherwise uncomment the following line
# curl -XDELETE localhost:9200/products

curl -XPUT localhost:9200/products/product/1 -d'{
  "property1": "value1"
}'

curl -XPUT localhost:9200/products/product/2 -d'{
  "property1": "value2"
}'

curl -XPOST localhost:9200/products/offer/_mapping -d '{
  "book":{
    "_parent": {"type": "product"}
  }
}'

curl -XPOST localhost:9200/products/offer/1?parent=1 -d '{
   "color": "blue",
   "size": 1,
   "price": 99.4
}'

curl -XPOST localhost:9200/products/offer/2?parent=1 -d '{
   "color": "red",
   "size": 2,
   "price": 100.5
}'

curl -XPOST localhost:9200/products/offer/3?parent=2 -d '{
   "color": "blue",
   "size": 3,
   "price": 100.7
}'
echo ""

use custom_score query as:
curl -s -XPOST 'localhost:9200/products/product/_search?pretty=true' -d '{
  "query" : {
    "has_child" : {
      "type" : "offer",
      "score_mode" : "max",
      "query" : {
        "custom_score" : {
          "script" : "-doc[\"offer.price\"].value",
          "query": {
            "term" : {
              "color" : "blue"        
            }
          }
        }
      }
    }
  },
  "facets" : {
      "size" : {
          "terms" : {
              "field" : "size"
          },
          "global" : true,
          "facet_filter" : {
            "bool" : {
              "must" : [
                { "term" : { "color" : "blue" } }
              ]
            }
          }
      }
  }
}'
```

get exception in version 1.2.1:
nested: QueryParsingException[[products] No query registered for [custom_score]]; 
in version 0.90.9 runs fine.

use function_score as:

```
curl -s -XPOST 'localhost:9200/products/product/_search?pretty=true' -d '{
  "query" : {
    "has_child" : {
      "type" : "offer",
      "score_mode" : "max",
      "query" : {
        "function_score" : {
          "boost_mode" : "replace",
          "query" : {
            "term" : {
              "color" : "blue"        
            }
          },
          "script_score" : {
            "script" : "doc['offer.price'].value"
          }
        }
      }
    }
  }
}'
```

get exception:
nested: QueryParsingException[[products] script_score the script could not be loaded]; nested: ScriptException[dynamic scripting disabled];
</description><key id="35861221">6525</key><summary>es 1.2.1 in has_child query can not use custom_score and function_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fiefdx</reporter><labels /><created>2014-06-17T06:23:19Z</created><updated>2014-08-20T00:47:27Z</updated><resolved>2014-06-17T06:27:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-17T06:27:20Z" id="46271576">Not an issue.

See: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_deprecations.html
And http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-scripting.html#_enabling_dynamic_scripting

Closing
</comment><comment author="clintongormley" created="2014-06-17T11:18:48Z" id="46294539">@fiefdx for some reason your previous comment disappeared:

&gt; I add "script.disable_dynamic: false" in the elasticsearch.yml, use version 1.2.1, restart elsaticsearch, and search use has_child query with function_score,
&gt; 
&gt; and get exception:
&gt; nested: PropertyAccessException[[Error: could not access: offer; in class:  org.elasticsearch.search.lookup.DocLookup]\n[Near : {... doc[offer.price].value ....}]
&gt; 
&gt; function_score in version 1.2.1 can get child's numeric field as score like version 0.90.x custom_score did?

The problem lies with your update mapping command:
    curl -XPOST localhost:9200/products/offer/_mapping -d '{
      "book":{
        "_parent": {"type": "product"}
      }
    }'

You've specified two different types: `offer` in the URL and `book` in the body.  Change the body to `offer` (and with dynamic scripting enabled) and your `function_score` query works correctly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make BytesValues.WithOrdinals more similar to Lucene's SortedSetDocValues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6524</link><project id="" key="" /><description>Mid-term we should switch from `BytesValues` to Lucene's doc values APIs, in
particular the `SortedSetDocValues` class. While `BytesValues.WithOrdinals` and
SortedSetDocValues expose the same functionality, `BytesValues.WithOrdinals`
exposes its ordinals via a different `Ordinals.Docs` object while
`SortedSetDocValues` exposes them on the same object as the one that holds the
values. This commit merges ordinals into `BytesValues.WithOrdinals` in order to
make both classes even closer.

Global ordinals were a bit tricky to migrate to I just changed them to use
Lucene's OrdinalMap that will soon (LUCENE-5767, scheduled for 4.9) have the
same optimizations as our global ordinals.
</description><key id="35848499">6524</key><summary>Make BytesValues.WithOrdinals more similar to Lucene's SortedSetDocValues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-17T00:28:30Z</created><updated>2015-06-07T13:13:11Z</updated><resolved>2014-06-19T10:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-18T11:59:49Z" id="46426590">I pushed new commits that remove `copyShared` and move `getTermsEnum` to `BytesValues.WithOrdinals` instead of `AtomicFieldData`.

This way, `BytesValues.WithOrdinals`is now exactly the same thing as lucene's sorted set doc values, up to method names. That means that we will be able to replace BytesValues.WithOrdinals with SortedSetDocValues when lucene 4.9 is out so that we won't need an adapter for doc values (that currently slows them down) anymore.
</comment><comment author="jpountz" created="2014-06-18T12:16:41Z" id="46427869">To be more specific, I plan to use `RandomAccessOrds` (that is a specialization of `SortedSetDocValues`) and then we will just need to:
- split `int setDocument(int doc)` into `void setDocument(int doc)` and `int cardinality()`.
- change `getOrd(int doc)` to return a `SortedDocValues` instance somehow (this method is only used in the single-valued case anyway).

In addition to speeding up doc values, this will also help switch to other Lucene APIs like comparators, etc.
</comment><comment author="rmuir" created="2014-06-18T13:02:31Z" id="46432025">&gt; change getOrd(int doc) to return a SortedDocValues instance somehow (this method is only used in the single-valued case anyway).

This is easy enough: I think we should just use the lucene API? In other words, stuff that cares about this will call DocValues.unwrapSingleton(SortedSet). So if fielddata has a representation for single-valued stuff, it should simply implement the SortedDocValues API.
</comment><comment author="jpountz" created="2014-06-18T13:03:13Z" id="46432109">Sounds good!
</comment><comment author="s1monw" created="2014-06-18T13:59:05Z" id="46438711">honestly I don't need to look long at this to love it. It drops soo much code and removes the confusing `Docs` abstraction! this LGTM ( I only left one small comment )
</comment><comment author="jpountz" created="2014-06-18T15:22:59Z" id="46450378">The shared scratch was problematic for wrappers that needed to copy into the shared scratch instead of returning the value directly. I just pushed a commit that removes this shared scratch so that impls can decide on whether they need one.
</comment><comment author="s1monw" created="2014-06-18T18:22:10Z" id="46473719">cool thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow _version to use `disk` as a doc values format.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6523</link><project id="" key="" /><description>VersionFieldMapper.defaultDocValuesFormat claims that the default is `disk`
(which is wrong, it's using Lucene's defaults).
This is not used to choose the DV format in the index but for mappings
serialization in order to know when the _version doc values format is
different from the default format. This made it impossible to use the `disk`
doc values format since mappings would never retain that information at
serialization time.
</description><key id="35847059">6523</key><summary>Allow _version to use `disk` as a doc values format.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Mapping</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T23:56:41Z</created><updated>2015-06-07T19:43:19Z</updated><resolved>2014-06-19T10:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-17T08:27:38Z" id="46280287">LGTM
</comment><comment author="s1monw" created="2014-06-18T18:33:31Z" id="46475238">LGTM should also go into `1.2.2`?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index prioritization during recovery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6522</link><project id="" key="" /><description>ES currently does not have the ability to prioritize indices during recovery.  It would be useful for the node to recover certain shards first, using an algorithm chosen by the user.  Some possibilities could be 'smaller indices first' or 'larger indices first' or 'most recently created indices first'.  That would allow for fast recovery of a current day's log index (for example) to get that index green and ready for ingestion as soon as possible.
</description><key id="35831405">6522</key><summary>Index prioritization during recovery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>:Recovery</label><label>adoptme</label><label>enhancement</label></labels><created>2014-06-16T20:09:05Z</created><updated>2015-08-13T13:48:59Z</updated><resolved>2015-08-05T11:13:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="faxm0dem" created="2014-06-27T10:45:42Z" id="47330420">hell yes please let us have this!
</comment><comment author="garyelephant" created="2015-01-07T12:32:45Z" id="69015892">+10 for this
</comment><comment author="simwood-simon" created="2015-07-31T12:33:59Z" id="126676291">+1
</comment><comment author="faxm0dem" created="2015-07-31T13:19:30Z" id="126689680">A feature which addresses this at least partially has been [released in 1.7](https://www.elastic.co/guide/en/elasticsearch/reference/1.7/recovery-prioritization.html)
</comment><comment author="clintongormley" created="2015-08-05T11:13:39Z" id="127957193">I think that users now have sufficient control over index recovery prioritization, so I'm going to close this one
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master node took a long time to remove the problematic data nodes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6521</link><project id="" key="" /><description>We have an ElasticSearch cluster (ver 1.1.1) that has a dedicated master node (=es-diag-19) and a few data nodes. We noticed that when one of the data nodes (=es-diag-16) was put under high index an search requests pressure, and it then experienced very long gc time. During its long gc period, the master node got many timeouts to ping the data node for more than an hour. 

As we already set discovery.zen.ping.timeout: 25s, I suppose that master should automatically remove the data node when it saw frequent timeouts when connecting the data node, but seems that the master took a long time to have this timeout before finally removed the problematic data node. Is this bug?

Here are the log from the master and data nodes : 
## master (es-diag-19)

2014-06-16 07:09:02,883][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [45538ms] ago, timed out [15530ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7295820]
[2014-06-16 07:10:25,321][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [55224ms] ago, timed out [25211ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7297000]
[2014-06-16 07:11:24,406][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [55815ms] ago, timed out [25804ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7297931]
[2014-06-16 07:12:07,292][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [40825ms] ago, timed out [10820ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data]
......
[2014-06-16 08:33:01,184][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [83298ms] ago, timed out [53288ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7362676]
[2014-06-16 08:33:01,184][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [53288ms] ago, timed out [23275ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7363037]
[2014-06-16 08:34:00,401][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [58194ms] ago, timed out [28190ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7363693]
[2014-06-16 08:34:36,793][WARN ][transport                ] [ES-DIAG-19-master] Received response for a request that has timed out, sent [35385ms] ago, timed out [5374ms] ago, action [discovery/zen/fd/ping], node [[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}], id [7364398]
[2014-06-16 08:36:08,944][INFO ][cluster.service          ] [ES-DIAG-19-master] removed {[ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{AvailabilitySet=As-Data-3, master=false},}, reason: zen-disco-node_failed([ES-DIAG-16-data][SWuDp-k4QaC-Lh8X4P3Rfw][es-diag-16][inet[/10.1.0.14:9300]]{master=false}), reason failed to ping, tried [3] times, each with maximum [30s] timeout
## data(es-diag-16)

[2014-06-16 07:09:02,027][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165282][71] duration [46s], collections [1]/[46.5s], total [46s]/[1.2m], memory [6.8gb]-&gt;[6.3gb]/[6.9gb], all_pools {[young] [520.6mb]-&gt;[61.2mb]/[532.5mb]}{[survivor] [55.4mb]-&gt;[0b]/[66.5mb]}{[old] [6.2gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 07:10:24,498][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165309][73] duration [55.1s], collections [1]/[55.8s], total [55.1s]/[2.1m], memory [6.9gb]-&gt;[6.4gb]/[6.9gb], all_pools {[young] [528.1mb]-&gt;[142.8mb]/[532.5mb]}{[survivor] [58.3mb]-&gt;[0b]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 07:11:23,514][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165312][74] duration [56.6s], collections [1]/[56.8s], total [56.6s]/[3m], memory [6.9gb]-&gt;[6.4gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[64.4mb]/[532.5mb]}{[survivor] [53.6mb]-&gt;[0b]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
......
[2014-06-16 08:33:00,308][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165449][170] duration [45.7s], collections [1]/[45.8s], total [45.7s]/[1.3h], memory [6.9gb]-&gt;[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [58.9mb]-&gt;[59mb]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:34:35,917][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165450][173] duration [2.2m], collections [3]/[2.2m], total [2.2m]/[1.4h], memory [6.9gb]-&gt;[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [59mb]-&gt;[61.6mb]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:37:24,322][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165451][177] duration [2.8m], collections [4]/[2.8m], total [2.8m]/[1.4h], memory [6.9gb]-&gt;[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [61.6mb]-&gt;[64.9mb]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:40:05,384][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165452][179] duration [1.8m], collections [2]/[1.8m], total [1.8m]/[1.4h], memory [6.9gb]-&gt;[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [64.9mb]-&gt;[64.5mb]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:41:02,775][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165453][181] duration [1.7m], collections [2]/[1.7m], total [1.7m]/[1.5h], memory [6.9gb]-&gt;[6.9gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [64.5mb]-&gt;[63.3mb]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:45:35,336][WARN ][discovery.zen.ping.unicast] [ES-DIAG-16-data] failed to send ping to [[#zen_unicast_2#][es-diag-16][inet[es-diag-20/10.1.0.34:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es-diag-20/10.1.0.34:9300]][discovery/zen/unicast] request_id [5391981] timed out after [59031ms]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:356)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-06-16 08:46:33,539][WARN ][discovery.zen            ] [ES-DIAG-16-data] failed to connect to master [[ES-DIAG-19-master][J7p7Z9h2QCGvBSeGRbfaSw][es-diag-19][inet[/10.1.0.8:9300]]{data=false, AvailabilitySet=As-Master, master=true}], retrying...
org.elasticsearch.transport.ConnectTransportException: [ES-DIAG-19-master][inet[/10.1.0.8:9300]] connect_timeout[30s]
    at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:718)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:647)
    at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:615)
    at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:129)
    at org.elasticsearch.discovery.zen.ZenDiscovery.innerJoinCluster(ZenDiscovery.java:338)
    at org.elasticsearch.discovery.zen.ZenDiscovery.access$500(ZenDiscovery.java:79)
    at org.elasticsearch.discovery.zen.ZenDiscovery$1.run(ZenDiscovery.java:286)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-06-16 08:45:35,351][WARN ][discovery.zen.ping.unicast] [ES-DIAG-16-data] failed to send ping to [[#zen_unicast_3#][es-diag-16][inet[es-diag-21/10.1.0.35:9300]]]
org.elasticsearch.transport.ReceiveTimeoutTransportException: [][inet[es-diag-21/10.1.0.35:9300]][discovery/zen/unicast] request_id [5391982] timed out after [59047ms]
    at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:356)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-06-16 08:46:33,664][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165458][187] duration [58.6s], collections [1]/[58.9s], total [58.6s]/[1.6h], memory [6.8gb]-&gt;[6.8gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[532.5mb]/[532.5mb]}{[survivor] [26.4mb]-&gt;[31.1mb]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:46:34,101][DEBUG][action.bulk              ] [ES-DIAG-16-data] [perf-searchindexer-2014-06-16][1], node[SWuDp-k4QaC-Lh8X4P3Rfw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@3f536b50]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:139)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:76)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
[2014-06-16 08:46:34,149][WARN ][transport                ] [ES-DIAG-16-data] Transport response handler not found of id [5391984]
[2014-06-16 08:47:32,289][WARN ][monitor.jvm              ] [ES-DIAG-16-data] [gc][old][165459][188] duration [58.4s], collections [1]/[58.6s], total [58.4s]/[1.6h], memory [6.8gb]-&gt;[6.8gb]/[6.9gb], all_pools {[young] [532.5mb]-&gt;[470.6mb]/[532.5mb]}{[survivor] [31.1mb]-&gt;[0b]/[66.5mb]}{[old] [6.3gb]-&gt;[6.3gb]/[6.3gb]}
[2014-06-16 08:47:32,842][DEBUG][action.bulk              ] [ES-DIAG-16-data] [logs-sep-2014-06-16][1], node[SWuDp-k4QaC-Lh8X4P3Rfw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.bulk.BulkShardRequest@450272fe]
java.lang.NullPointerException
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:139)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shards(TransportShardBulkAction.java:76)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performReplicas(TransportShardReplicationOperationAction.java:610)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:557)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
</description><key id="35827102">6521</key><summary>Master node took a long time to remove the problematic data nodes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JeffreyZZ</reporter><labels /><created>2014-06-16T19:16:54Z</created><updated>2014-06-17T06:44:24Z</updated><resolved>2014-06-17T06:44:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-17T06:44:24Z" id="46272497">Hey,

it takes three failed attempts to ping a node in order to remove it from the cluster, so if one of the many GCs ended inbetween and one successful ping is coming through, the node is not removed - which I think will contribute to your instability.

You can change those values (number of tries and number of seconds IIRC), but you should really either add more memory or offload your data node in this example as everything else is just moving the problem instead of solving it.

Also, please use the mailinglist for questions like this, as github is intended to be used as a bug tracker.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use ordinals for comparison in GlobalOrdinalsStringTermsAggregator.build...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6520</link><project id="" key="" /><description>...Aggregation. Closes #6518

I used Martijn's benchmark to measure terms agg at different cardinality, I'm sure its not a perfect benchmark but it demonstrates the issue, in the current code the term lookups start to dominate the total time:

| cardinality | avg ms (master) | avg ms (patch) |
| --- | --- | --- |
| 1M | 4751ms | 4380ms |
| 5M | 5803ms | 5030ms |
| 10M | 6421ms | 5079ms |
| 25M | 8725ms | 5992ms |
| 50M | 11829ms | 6179ms |
| 100M | 20266ms | 7064ms |
</description><key id="35824688">6520</key><summary>Use ordinals for comparison in GlobalOrdinalsStringTermsAggregator.build...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-06-16T18:48:53Z</created><updated>2014-06-19T12:11:57Z</updated><resolved>2014-06-19T12:11:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-16T20:30:37Z" id="46232352">LGTM
</comment><comment author="martijnvg" created="2014-06-16T20:51:11Z" id="46235356">LGTM

I think we also do this with GlobalOrdinalsSignificantTermsAggregator?
</comment><comment author="rmuir" created="2014-06-16T21:45:07Z" id="46242193">We should improve that one, but its different and more complicated, and it uses the term bytes today to lookup index statistics.
</comment><comment author="martijnvg" created="2014-06-17T07:29:35Z" id="46275646">yes, that makes it more difficult to improve
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete By Query Statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6519</link><project id="" key="" /><description>Would it be possible to add some stats to the response from a DeleteByQuery giving information on how my objects were deleted?
</description><key id="35818266">6519</key><summary>Delete By Query Statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshblum</reporter><labels /><created>2014-06-16T17:34:34Z</created><updated>2014-06-17T18:51:12Z</updated><resolved>2014-06-17T18:51:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="usmanm" created="2014-06-17T03:05:54Z" id="46262780">+1
</comment><comment author="spinscale" created="2014-06-17T06:52:18Z" id="46273013">Took a quick look at Elasticsearch's `InternalEngine` which reveals that Lucene does not expose this information when deleting documents by query, so elasticsearch cannot pass anything to the user at the moment. So your current (and possibly returning wrong counts as time passes between those two operations) solution would be to run a `count` operation first and then execute the delete-by-query one.
</comment><comment author="joshblum" created="2014-06-17T18:51:12Z" id="46349149">Gotcha, this was my tmp fix. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GlobalOrdinalsStringTermsAggregator is inefficient for high-cardinality fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6518</link><project id="" key="" /><description>After doing some profiling and seeing surprising results, and looking at the code, and I can easily be reading it wrong...

buildAggregation() has the following loop:

```
for (long globalTermOrd = Ordinals.MIN_ORDINAL; globalTermOrd &lt; globalOrdinals.getMaxOrd(); ++globalTermOrd) {
                ...
                copy(globalValues.getValueByOrd(globalTermOrd), spare.termBytes);  
}
```

This is very costly for high-cardinality fields, because it means we lookup ord-&gt;term for every single one. Instead, can we use a PriorityQueue of "OrdAndSortValue", find the top-N, and then only at the end, lookup ord-&gt;term for the top-N? E.g. in solr faceting this OrdAndSortValue is just a long (32 bits ord and 32 bits count) but the representation is less important here.
</description><key id="35807639">6518</key><summary>GlobalOrdinalsStringTermsAggregator is inefficient for high-cardinality fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/rmuir/following{/other_user}', u'events_url': u'https://api.github.com/users/rmuir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/rmuir/orgs', u'url': u'https://api.github.com/users/rmuir', u'gists_url': u'https://api.github.com/users/rmuir/gists{/gist_id}', u'html_url': u'https://github.com/rmuir', u'subscriptions_url': u'https://api.github.com/users/rmuir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/504194?v=4', u'repos_url': u'https://api.github.com/users/rmuir/repos', u'received_events_url': u'https://api.github.com/users/rmuir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/rmuir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'rmuir', u'type': u'User', u'id': 504194, u'followers_url': u'https://api.github.com/users/rmuir/followers'}</assignee><reporter username="">rmuir</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T15:33:01Z</created><updated>2015-06-07T13:13:25Z</updated><resolved>2014-06-16T22:25:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-06-16T16:27:07Z" id="46200904">I experimented some more, this is only one perf problem. The other one is the massive amount of Bucket objects created (maxOrd).

We should do this more like a lucene collector: use the PQ more efficiently so we only create queue_size Bucket objects. With high cardinality fields, this method is just as much of a hotspot as collecting ordinals so we should optimize it as such: at least as a step avoid creating Bucket objects if it can't compete with the PQ, but maybe later try to optimize the loop with sentinels and stuff.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary intermediate interfaces</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6517</link><project id="" key="" /><description>`Client`, `ClusterAdminClient` and `IndicesAdminClient` had corresponding intermediate `internal` interfaces that are unnecessary and cause a lot of casting. This commit removes the intermediate interfaces and uses the super interfaces directly.

This PR also  simplifys `FilterClient` by subclassing `AbstractClient`

Closes #4355
</description><key id="35804935">6517</key><summary>Remove unnecessary intermediate interfaces</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Java API</label><label>breaking</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T15:05:17Z</created><updated>2015-06-07T16:20:41Z</updated><resolved>2014-06-17T10:28:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-16T15:16:27Z" id="46191656">LGTM, very nice improvement
</comment><comment author="uboness" created="2014-06-16T15:23:21Z" id="46192549">can we also consider having one base interface for all clients... the all have the same `execute` methods (only the request types is different). If we do this, it'll also clean up https://github.com/elasticsearch/elasticsearch/pull/6513 as we'll only need a single copy headers filter client (I think)
</comment><comment author="s1monw" created="2014-06-16T15:25:08Z" id="46192808">@uboness I am working on makeing things more generic as we speak I think having a single execute method would make sense. Lemme see how it goes
</comment><comment author="uboness" created="2014-06-16T15:25:47Z" id="46192895">awesome! thx
</comment><comment author="s1monw" created="2014-06-17T08:41:29Z" id="46281510">@uboness here are some updates I think we are close?
</comment><comment author="jpountz" created="2014-06-17T09:05:08Z" id="46283589">This looks great. We might want to make clients extend `Releasable` instead of `Closeable` to not have the `IOException` to deal with?
</comment><comment author="uboness" created="2014-06-17T09:08:31Z" id="46283867">I LOVE IT!!! left a small comment on naming... but other than that LGTM
</comment><comment author="s1monw" created="2014-06-17T09:22:52Z" id="46285102">I pushed the fixes for the comments and added some more javadocs. I think it's ready! @javanna I will include the issue in the final commit message
</comment><comment author="javanna" created="2014-06-17T10:04:48Z" id="46288756">I left a few comments on generics types missing, other than that LGTM!!!
</comment><comment author="s1monw" created="2014-06-17T10:34:38Z" id="46291069">I mark this as breaking since it changes class hierarchies... yet, I don't think users will be affected since they only use the final classes rather than internal once... thanks for the reviews...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Verify all threads created by node and client have the node name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6516</link><project id="" key="" /><description /><key id="35802513">6516</key><summary>Test: Verify all threads created by node and client have the node name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T14:40:54Z</created><updated>2014-07-16T12:58:56Z</updated><resolved>2014-06-16T19:51:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-16T14:49:57Z" id="46188075">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No Node Available</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6515</link><project id="" key="" /><description>Hi guys,

I'm having some problems with NoNodeAvailable. When the cluster is idle during some time the problem occurs (between 0:00 and 8:00 am). 

I saw the sockets log in the cluster and there is a lot of sockets opened (like 700 in one of them). And because of that I load-balancer node is throwing out of memory.

I'm using TranportClient to connect to cluster. My environment is:
- 15 ES nodes with 7 shards and 2 replicas (7 data nodes (16Gb each one), 4 load balancers(4Gb each one), 4 masters(4Gb each one)); 
- 8 clients pointing to the 4 load-balancer nodes.

When the cluster is being used (during the day), this problem does not happen, but every first connection of the day catch this error.

I created a discussion on Google Groups as follows:
https://groups.google.com/forum/#!searchin/elasticsearch/marcelopaesrech/elasticsearch/ZqypeTOguoM/8qunqmONggoJ

Best regards.
</description><key id="35802266">6515</key><summary>No Node Available</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcelopaesrech</reporter><labels><label>discuss</label></labels><created>2014-06-16T14:38:39Z</created><updated>2015-11-21T15:00:54Z</updated><resolved>2015-11-21T15:00:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T15:00:54Z" id="158650040">Fixed by https://github.com/elastic/elasticsearch/pull/10189
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing: Using _id as object doesnt work for future queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6514</link><project id="" key="" /><description>Hey,

had to describe the issue in one line, here is the possibiltiy to reproduce

```
DELETE campaigns
PUT campaigns
{
   "mappings" : {
      "campaign" : {
         "properties" : {
            "location" : {
                "type": "geo_shape",
                "tree": "quadtree"
            }
         }
      }
   }
}

POST /campaigns/campaign/1
{
  "location" : {
      "type" : "circle",
      "coordinates" : [45.01, 2.26],
      "radius" : "9000m"
  }
}

POST campaigns/campaign/
{
  "_id": { "c_id": "1891"},
  "location" : {
      "type" : "circle",
      "coordinates" : [45.01, 2.26],
      "radius" : "9000m"
  }
}

GET campaigns/campaign/_search

GET campaigns/campaign/_search
{
    "query":{
        "filtered": {
            "query": {
                "match_all": {}
            },
            "filter": {
                "geo_shape": {
                    "location": {
                   "relation": "intersects",
                        "shape": {
                            "type": "circle",
                            "coordinates" : [45.01001, 2.26],
                            "radius":"1m"
                        }
                    }
                }
            }
        }
    }
}
```

You can see the difference between match all and the geo shape query, even though the points are the same. If you remove the `_id` field from the document, everything works, so maybe the IdFieldMapper throws an Exception (rightfully, as an id cannot be an object) and then the rest of the document is not indexed.

If you replace the `_id` object with a string, you get an exception, that the id values dont match as one ID is autogenerated. Not sure, if this is the desired behaviour.
</description><key id="35796436">6514</key><summary>Indexing: Using _id as object doesnt work for future queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-06-16T13:36:15Z</created><updated>2014-12-30T20:04:30Z</updated><resolved>2014-12-30T20:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T20:04:30Z" id="68392405">We should not try to parse the `_id` field inside the document. Duplicate of #6730
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Copy the headers from REST requests to the corresponding TransportRequest(s)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6513</link><project id="" key="" /><description>Introduced the use of the `FilterClient` in all of the REST actions, which delegates all of the operations to the internal `Client`, but makes sure that the headers are properly copied if needed from REST requests to `TransportRequest`(s) when it comes to executing them.  Added new abstract handleRequest method to `BaseRestHandler` with additional `Client` argument and made private the client instance member (was protected before) to force the use of the client received as argument.  The list of headers to be copied over is by default empty but can be extended via plugins.

Replaces #6464 as it provides a better and safer way to copy headers from REST layer to transport layer.
</description><key id="35794210">6513</key><summary>Copy the headers from REST requests to the corresponding TransportRequest(s)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T13:07:28Z</created><updated>2015-06-07T13:13:40Z</updated><resolved>2014-06-19T17:27:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-17T15:06:48Z" id="46319709">I updated the PR after #6517 was committed, ready for review again...
</comment><comment author="s1monw" created="2014-06-18T09:35:12Z" id="46414527">I think this looks awesome. I am hesitating with the BW break here, IMO it would be nicer to deprecate the `BaseRestHandler#client` and add a method `BaseRestHandler#client(RestRequest)` instead of adding a new abstract method. We can then make the member private in master and drop the deprecation?
</comment><comment author="kimchy" created="2014-06-18T09:50:41Z" id="46415907">does it make sense when we copy over the headers, to not copy over common known HTTP headers that are not really used, like Content-Type, Content-Length, ...
</comment><comment author="uboness" created="2014-06-18T09:56:02Z" id="46416410">@s1monw I think it's actually better to pass in the `client` as an arg to `handleRequest` as it "encourage" handler implementations to use the client. Nothing really prevents handler impl. from injecting transport actions directly and bypass the client all together. The only bwc breaking we have here, relates to plugins, and we're breaking bwc for plugins almost every other release (e.g. when we upgrade lucene version).. I think this change is important enough to justify it.

@kimchy yea, it makes sense +1
</comment><comment author="s1monw" created="2014-06-18T11:13:56Z" id="46422557">fair enough... LGTM
</comment><comment author="javanna" created="2014-06-18T13:50:25Z" id="46437609">Pushed a new commit containing a static list of REST headers that need to be copied over to the transport.
</comment><comment author="javanna" created="2014-06-18T15:24:34Z" id="46450599">Pushed a couple of new commits that address @uboness feedback, it should be close.
</comment><comment author="uboness" created="2014-06-19T15:57:32Z" id="46579650">LGTM
</comment><comment author="javanna" created="2014-06-19T17:31:21Z" id="46591402">As a side note, this change is breaking for plugins that add their own REST handlers by extending `BaseRestHandler`. They can easily adapt just by adding an additional `Client` argument to their existing `handleRequest` method and use that `Client` received as argument to execute the request instead of `this.client`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Context Suggester: Context does not work with non strings in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6512</link><project id="" key="" /><description>Hey,

the context suggester only works with strings in the mapping if you specify a field, see this example

```
DELETE /services
PUT /services
PUT /services/service/_mapping
{
    "service": {
        "properties": {
            "network_id": {
                "type" : "long"
            },
            "suggest_field": {
                "type": "completion",
                "context": {
                    "network": { 
                        "type": "category",
                        "path": "network_id"
                    }
                }
            },
            "suggest_field2": {
                "type": "completion",
                "context": {
                    "network": { 
                        "type": "category"
                    }
                }
            }
        }
    }
}

PUT /services/service/1
{
    "name": "knapsack",
    "network_id": "1",
    "suggest_field": {
        "input": ["knacksack", "backpack", "daypack"]
    },
    "suggest_field2": {
        "input": ["knacksack", "backpack", "daypack"],
        "context" : {
          "network" : "1"
        }
    }
}

POST services/_suggest?pretty'
{
    "suggest" : {
        "text" : "k",
        "completion" : {
            "field" : "suggest_field",
            "size": 10,
            "context": {
                "network": "1"
            }
        }
    },
    "suggest2" : {
        "text" : "k",
        "completion" : {
            "field" : "suggest_field2",
            "size": 10,
            "context": {
                "network": "1"
            }
        }
    }
}
```

Solution 1: Call `toString()` for everything
Solution 2: Reject contexts for anything else than strings
</description><key id="35793834">6512</key><summary>Context Suggester: Context does not work with non strings in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">spinscale</reporter><labels /><created>2014-06-16T13:02:18Z</created><updated>2015-11-21T14:57:50Z</updated><resolved>2015-11-21T14:57:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T14:57:50Z" id="158649909">This has been fixed in #11740
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Refactoring parent/child to improve memory consumption and query execution.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6511</link><project id="" key="" /><description>Parent/child refactoring:
- Cut the `has_child` and `has_parent` queries over to use Lucene's query time global ordinal join. The main benefit of this change is that parent/child queries can now efficiently execute if parent/child queries are wrapped in a bigger boolean query. If the rest of the query only hit a few documents both has_child and has_parent queries don't need to evaluate all parent or child documents any more.
- Cut the `_parent` field over to use doc values. This significantly reduces the on heap memory footprint of parent/child, because the parent id values are never loaded into memory.

Breaking changes:
- The `type` option on the `_parent` field can only point to a parent type that doesn't exist yet, so this means that an existing type/mapping can't become a parent type any longer.
- Unless #10485 get fixed, the `has_child` and `has_parent` queries can't be use in alias filters. Actually  today parent/child queries in aliases filters are broken as is reported in #10135.

All these changes, improvements and breaks in compatibility only apply for indices created with ES version 2.0 or higher. For indices creates with ES &lt;= 2.0 the older implementation is used.

It is highly recommended to re-index all your indices with parent and child documents to benefit from all the improvements that come with this refactoring. The easiest way to achieve this is by using the scan and bulk apis using a simple script.

PR for #6107 and #8134
</description><key id="35792428">6511</key><summary>Refactoring parent/child to improve memory consumption and query execution.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Parent/Child</label><label>breaking</label><label>enhancement</label><label>release highlight</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T12:42:21Z</created><updated>2015-10-06T01:26:49Z</updated><resolved>2015-05-29T19:57:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-17T08:51:50Z" id="46282428">This looks good. Maybe we can randomly enable doc values on `_parent` in `ElasticsearchIntegrationTest.randomIndexTemplate`?
</comment><comment author="martijnvg" created="2014-08-11T10:18:35Z" id="51762976">@jpountz I updated the PR and took a different approach.

Instead of controlling per `_parent` field if doc values are enabled, doc values field for the `_parent` field is always stored on indices created on or after 1.4.0.

Added an index level option `index._parent.doc_values` which controls whether doc values based parent/child field data is used or on the fly un-inverted parent/child field data is used. This option defaults to true, meaning to on indices created on or after 1.4.0 doc values based parent/child is used by default.

Also due to rebase conflicts I removed the old commit in favour on a new commit.
</comment><comment author="jpountz" created="2014-08-12T07:35:28Z" id="51882565">This looks good to me in general but I'm not sure I understand the need for the `index._parent.doc_values` setting. I think it should rather be managed by an index template?
</comment><comment author="jpountz" created="2014-08-12T16:24:23Z" id="51938830">I just discussed with @martijnvg and there is an issue with the current state of this PR in case a child type is introduced after documents have already been added to the parent type. We're not sure how to address this issue yet...
</comment><comment author="snmaynard" created="2015-01-08T03:21:07Z" id="69130766">Do you have any plans to release this in the near term or do you think it will be a while before this is ready? Keeping all the parent document ids in memory is using a fair bit of memory for us so I'm wondering if I should work around the issue or wait.

Thanks!
</comment><comment author="martijnvg" created="2015-03-25T10:00:31Z" id="85966375">I'm reviving this PR with the work that is going on in: https://issues.apache.org/jira/browse/LUCENE-6352
For now I forked the patch in this PR to move ES over to use the new Lucene join.

This going to be a breaking change:
1) A parent type will now also need a _parent field that marks the type as a parent. (by setting: `parent=true`). This is needed because the parent document needs to write its id in the dedicated join docvalues field.
2) Because of the above change a parent type can't be lazily introduced. If a type needs to be a parent then it should be configured as such.
3) Cutting over `top_children` isn't worth the effort, since this change will speed up `has_child` and reduce jvm heap memory significantly. The `top_children` wasn't always faster and isn't accurate. This change will ban the usage of `top_children` on indices created on 2.0 and above.

Regarding bwc in general for the new p/c docvalues based impl, the existing impl. will remain to run on indices created before 2.0, from 2.0 and onwards the new impl will be used. So to gain from the improvements a reindex needs to happen.

This is a work in progress, certain tests fail due to the fact that the Lucene join module doesn't have a min score mode yet and min/max children still needs to be implemented for the doc values join.

On top of this because Lucene's JoinUtil is going to be used also #8134 gets fixed. The non leaf reader nature which has caused many bugs in the past is gone, because of the fact that the join translates into two seperate queries instead of one query which actually holds two queries. Also we can now gain from  the two phase iterator added recently to Lucene that will significantly speedup has_child and has_parent if those queries are part of a bigger query: https://issues.apache.org/jira/browse/LUCENE-6198
</comment><comment author="clintongormley" created="2015-04-05T11:21:21Z" id="89755438">&gt; 2) Because of the above change a parent type can't be lazily introduced. If a type needs to be a parent then it should be configured as such.

Does this mean that you can add a new type to an existing index, but that it needs to be configured as a parent type from the moment it is added, or that you can only add parent types at the moment you create the index?
</comment><comment author="martijnvg" created="2015-04-05T12:13:32Z" id="89757692">The first option, so you can add a type to an existing index, but you need
set the `parent` option to `true` if you want it to be a parent type.
(maybe there is a better to mark a type as parent?)

On 5 April 2015 at 13:21, Clinton Gormley notifications@github.com wrote:

&gt; 2) Because of the above change a parent type can't be lazily introduced.
&gt; If a type needs to be a parent then it should be configured as such.
&gt; 
&gt; Does this mean that you can add a new type to an existing index, but that
&gt; it needs to be configured as a parent type from the moment it is added, or
&gt; that you can only add parent types at the moment you create the index?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/pull/6511#issuecomment-89755438
&gt; .

## 

Met vriendelijke groet,

Martijn van Groningen
</comment><comment author="clintongormley" created="2015-04-05T19:45:07Z" id="89839095">@martijnvg we should probably just keep the current API (ie you specify the _parent type for a child) but fail if the _parent type already exists and isn't marked as a parent.  That also means that if you create the two types concurrently, you need to be able to cope with the fact that the parent type may be parsed before the child type...
</comment><comment author="martijnvg" created="2015-04-05T20:35:51Z" id="89847826">@clintongormley +1 we need that validation. The concurrent types thingy is I guess tricky, but needs to work, since this is going to happen (for example during an index create api call).
</comment><comment author="martijnvg" created="2015-05-11T00:12:08Z" id="100720151">The changes in the Lucene join module have been committed (via LUCENE-6352) and are available in the master branch.

The only missing feature in the Lucene join is the `min_children` and `max_children` feature. This will be available to ES when LUCENE-6472 gets committed. 

In the mean time I have updated this PR with the latest changes in master. I think it is in a state now that it can be reviewed.
</comment><comment author="rjernst" created="2015-05-20T22:12:34Z" id="104058102">I really don't like having this new `_parent.parent` setting, which must be aligned with a setting in another type. I discussed with @martijnvg and I propose the following:
1. Collect all the parent types in the `MapperService` on mapping updates. This can be checked when indexing a doc (to see "is my type a parent of something?").
2. Fail any mapping update that creates a type, with `_parent.type` set to an existing type. This is a little less flexible than the current PR because now you cannot create the parent first, and children later. But it is simpler for a user to deal with, setting up the relationship in just one place. And the "lateness" of the error message is no different, since right now you would get an error with an existing type that doesn't have `_parent.parent` set, which can't be changed anyways, so requires creating a new type.
</comment><comment author="martijnvg" created="2015-05-20T22:14:42Z" id="104058471">@rjernst +1 I I'll adopt these ideas.
</comment><comment author="martijnvg" created="2015-05-21T17:07:31Z" id="104357139">@rjernst I implemented your ideas:
- Removed the `_parent.parent` option.
- Keep track of the parent types in the MapperService and during indexing this is used to determine if a document is a parent.
- Added the validation when a new mapping is added that the `_parent.type` can't point to an existing type. I'm not super happy where I place the validation now. The validation is only performed on the elected master node if a new mapping is added via the put mapping api and I think that is the only time we need to validate this. For example mappings as part of a create index call don't need to be checked (since everything is new) and put mapping call to an existing mapping also don't need this check, because `_parent.type` is immutable. 
</comment><comment author="clintongormley" created="2015-05-25T16:09:10Z" id="105257782">@martijnvg Please could you expand the description (https://github.com/elastic/elasticsearch/pull/6511#issue-35792428) for this PR, and also add documentation (and breaking changes) that explain that existing types cannot be promoted to be parents after the fact.
</comment><comment author="rjernst" created="2015-05-26T07:03:50Z" id="105420197">I left some comments about the mappings part of this. It would be good if @jpountz looked at the fielddata and p/c part.
</comment><comment author="martijnvg" created="2015-05-26T12:12:00Z" id="105502126">@rjernst Thanks for the feedback! I updated the PR.
@clintongormley I updated the docs too.

I also forbid parent/child self referencing. We allowed this before, but this is actually broken and resulted in incorrect results. I added validation for this to be upfront about this. We can properly support this later in a separate issue.
</comment><comment author="rjernst" created="2015-05-27T06:07:25Z" id="105768535">LGTM, I just left a couple very minor follow up comments.
</comment><comment author="jpountz" created="2015-05-29T14:39:57Z" id="106830555">LGTM

Given that the _parent field can have a high cardinality, global ordinals can be heavy to build. However now that we build the join field at index time, I'm wondering if we could remove the dependency on global ordinals and just build one bit set (+ scores array if scores are needed) per segment (as a follow-up of course).
</comment><comment author="jpountz" created="2015-05-29T14:51:47Z" id="106836757">I just realized it still can't work because some parents might have children on another segment. Nevermind. :)
</comment><comment author="andreasch" created="2015-10-06T01:26:49Z" id="145718109">How feasible would be to back-port these changes to a 1.3.x or any other 1.x version of Elasticsearch? I started merging the changes into 1.3.2 but there are quite a few conflicts. Besides the numerous code changes that occurred it seems to me that it would also require upgrading the version of Lucene to 5.x. Is that case? Do these changes require a Lucene version other than 4.9.x?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Fix Test - Cluster naming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6510</link><project id="" key="" /><description>Today we have `ImmutableCluster`, `ExternalTestCluster` and `TestCluster` which is not quite what they represent. IMO the basic class should be `TestCluster` and the we should rename `TestCluster` to `InternalTestCluster` that way the naming is consistent with where the nodes run (within or outside of the executing JVM.
</description><key id="35790538">6510</key><summary>Test: Fix Test - Cluster naming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T12:12:56Z</created><updated>2014-07-16T12:59:41Z</updated><resolved>2014-06-16T13:15:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-16T12:15:43Z" id="46170890">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Add test for accessing _score in scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6509</link><project id="" key="" /><description>We don't have any tests that use `_score` or `_score.&lt;type&gt;Value()` in a script, so this adds a test with a simple call to make sure it always works.
</description><key id="35790134">6509</key><summary>Test: Add test for accessing _score in scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T12:06:36Z</created><updated>2014-09-09T13:53:18Z</updated><resolved>2014-06-16T12:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-16T12:07:30Z" id="46170203">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added tables for directory locations for deb+rpm and zip+tar.gz installs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6508</link><project id="" key="" /><description>I raised an issue for this but Clint mentioned it's not required, please let me know though if you want me to sign the contributing agreement.

Per the title, I've written down the specific directories being used by various install methods, hopefully it'll be of use to newer users. I wasn't able to find and indication of a work directory for the zip/tar.gz install though, not sure if that is by design.
</description><key id="35786272">6508</key><summary>Added tables for directory locations for deb+rpm and zip+tar.gz installs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">markwalkom</reporter><labels><label>docs</label></labels><created>2014-06-16T11:00:49Z</created><updated>2014-08-18T10:22:07Z</updated><resolved>2014-08-18T10:22:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T19:01:51Z" id="51516566">Hi @markwalkom 

thanks for the PR. sorry it has taken a while to get to it.  The `work` directory is no longer relevant and can be removed. it's a hangover from the gateway days.

Also, have you already signed our CLA? I don't see you in the list: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="markwalkom" created="2014-08-08T01:48:29Z" id="51554789">I've now signed the CLA, I didn't as you mentioned in another issue that it wasn't needed for docs :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Retry if initial contact fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6507</link><project id="" key="" /><description>We have a 5-node cluster. As part of a full application redeploy, we are restarting every node in sequence.
Here the rebooted 2nd node ("Moondark", new node, logs attached) tries to reach the old 3rd node ("Volstagg", old master) right as it shuts down as part of the restart. The new 3rd node to replace Volstagg has not yet come up.
Moondark then figures out that the master is unreachable, and _gives up_.

Shouldn't a new node trigger another round of pings and a master connect attempt, in the case that the old master dies during initial connect?

Like this, the new node never joins the cluster. I let the node live for an hour just to confirm.

```
2014-06-16 10:18:21,114 INFO  node                                      - [Moondark] version[1.1.1], pid[3867], build[f1585f0/2014-04-16T14:27:12Z]
2014-06-16 10:18:21,118 INFO  node                                      - [Moondark] initializing ...
2014-06-16 10:18:21,125 INFO  plugins                                   - [Moondark] loaded [], sites [head, bigdesk, elasticsearch_hq, kibana]
2014-06-16 10:18:23,008 DEBUG discovery.zen.ping.unicast                - [Moondark] using initial hosts [tsl0mag12:12500, tsl0mag14:12500, tsl0mag15:12500, tsl0mag16:12500, tsl0mag17:12500], with concurrent_connects [10]
2014-06-16 10:18:23,022 DEBUG discovery.zen                             - [Moondark] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]
2014-06-16 10:18:23,025 DEBUG discovery.zen.elect                       - [Moondark] using minimum_master_nodes [3]
2014-06-16 10:18:23,026 DEBUG discovery.zen.fd                          - [Moondark] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
2014-06-16 10:18:23,045 DEBUG discovery.zen.fd                          - [Moondark] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
2014-06-16 10:18:24,003 INFO  node                                      - [Moondark] initialized
2014-06-16 10:18:24,003 INFO  node                                      - [Moondark] starting ...
2014-06-16 10:18:24,097 INFO  transport                                 - [Moondark] bound_address {inet[/0:0:0:0:0:0:0:0:12500]}, publish_address {inet[/151.187.99.214:12500]}
2014-06-16 10:18:24,119 TRACE discovery                                 - [Moondark] waiting for 30s for the initial state to be set by the discovery
2014-06-16 10:18:24,162 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]
2014-06-16 10:18:24,163 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]
2014-06-16 10:18:24,162 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]
2014-06-16 10:18:24,167 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]
2014-06-16 10:18:24,169 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]
2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]
2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]
2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]
2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]
2014-06-16 10:18:24,224 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]
2014-06-16 10:18:24,227 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]
2014-06-16 10:18:24,227 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]
2014-06-16 10:18:24,229 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]
2014-06-16 10:18:24,228 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_2#][tsl0mag14.skead.no][inet[tsl0mag14/151.187.99.214:12500]]
2014-06-16 10:18:24,230 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]
2014-06-16 10:18:24,274 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1
cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH
_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:24,275 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1
cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg
][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:24,274 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:24,274 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:24,280 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:25,664 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]
2014-06-16 10:18:25,665 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]
2014-06-16 10:18:25,666 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]
2014-06-16 10:18:25,667 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]
2014-06-16 10:18:25,668 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]
2014-06-16 10:18:25,672 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:25,673 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:25,674 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:25,674 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:25,674 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:27,175 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]
2014-06-16 10:18:27,176 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]
2014-06-16 10:18:27,177 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]
2014-06-16 10:18:27,178 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]
2014-06-16 10:18:27,178 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]
2014-06-16 10:18:27,180 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:27,180 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:27,182 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:27,182 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:27,184 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}]
2014-06-16 10:18:27,185 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]
2014-06-16 10:18:27,194 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]
2014-06-16 10:18:27,195 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]
2014-06-16 10:18:27,223 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]
2014-06-16 10:18:27,223 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]
2014-06-16 10:18:27,226 TRACE discovery.zen                             - [Moondark] full ping responses:
        --&gt; target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
        --&gt; target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
        --&gt; target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
        --&gt; target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
2014-06-16 10:18:27,227 DEBUG discovery.zen                             - [Moondark] filtered ping responses: (filter_client[true], filter_data[false])
        --&gt; target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
        --&gt; target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
        --&gt; target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
        --&gt; target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]
2014-06-16 10:18:27,472 DEBUG discovery.zen.fd                          - [Moondark] [master] starting fault detection against master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], reason [initial_join]
2014-06-16 10:18:27,483 DEBUG discovery.zen.fd                          - [Moondark] [master] stopping fault detection against master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], reason [master failure, failed to perform initial connect [[Volstagg][inet[/151.187.99.215:12500]] connect_timeout[30s]]]
2014-06-16 10:18:27,483 INFO  discovery.zen                             - [Moondark] master_left [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], reason [failed to perform initial connect [[Volstagg][inet[/151.187.99.215:12500]] connect_timeout[30s]]]
2014-06-16 10:18:27,489 TRACE discovery                                 - [Moondark] initial state set from discovery
2014-06-16 10:18:27,489 INFO  discovery                                 - [Moondark] IRIS-GP2-N-S/lJ5VZ7IGQ5C1cG38rxoSZA
2014-06-16 10:18:27,498 INFO  http                                      - [Moondark] bound_address {inet[/0:0:0:0:0:0:0:0:12400]}, publish_address {inet[/151.187.99.214:12400]}
2014-06-16 10:18:27,510 INFO  node                                      - [Moondark] started
```
</description><key id="35785252">6507</key><summary>Retry if initial contact fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">magnhaug</reporter><labels /><created>2014-06-16T10:43:20Z</created><updated>2015-01-26T08:53:24Z</updated><resolved>2015-01-26T08:53:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-18T07:32:50Z" id="46404253">To make sure I understand: when Moondark didn't rejoin the cluster upon master failure - did it elect it self as master? (you can tell by calling `_cat/master` on it).
</comment><comment author="magnhaug" created="2014-06-18T09:41:01Z" id="46415042">Unfortunately this cluster is now restarted, and not in a failed/ambiguous state any more. I'll remember to call `_cat/master` the next time this issue arises.

_But:_ If it selected itself as a master, would you not see a log entry in this format?

```
&lt;date&gt; INFO   cluster.service   -  [Moondark] master {new [Moondark][&lt;hash&gt;][&lt;hostname&gt;][&lt;inet_addr&gt;],  reason: zen-disco-join (elected_as_master)}
```
</comment><comment author="henrikno" created="2014-09-05T07:32:07Z" id="54593648">Version 1.1.2. _cat/master against the node that did not connect:

```
{"error":"MasterNotDiscoveredException[waited for [30s]]","status":503}
```
</comment><comment author="bleskes" created="2015-01-26T08:53:24Z" id="71429836">With 1.4, we did a lot of work to improve failure modes in the Zen Discovery. I'm going to close this for now. If this re-occurs, please do re-open.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: NPE in ES when shutdown happens in the middle of snapshotting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6506</link><project id="" key="" /><description>the process has to be killed forecefully to come out of this state.. here is the stack trace - 

[2014-06-16 07:41:36,568][WARN ][snapshots                ] [Quentin Quire] Fail
ed to update snapshot state
java.lang.NullPointerException
        at org.elasticsearch.snapshots.SnapshotsService.processIndexShardSnapsho
ts(SnapshotsService.java:644)
        at org.elasticsearch.snapshots.SnapshotsService.clusterChanged(Snapshots
Service.java:508)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:430)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
</description><key id="35775549">6506</key><summary>Snapshot/Restore: NPE in ES when shutdown happens in the middle of snapshotting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">tvinod</reporter><labels><label>bug</label><label>v1.3.3</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-16T08:12:11Z</created><updated>2014-09-08T15:12:24Z</updated><resolved>2014-08-20T00:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-06-23T09:13:44Z" id="46821721">Which version of elasticsearch was it? Did you shutdown entire cluster or just the master node? 
</comment><comment author="tvinod" created="2014-06-23T20:04:40Z" id="46894557">1.1.0

i was shutting down the cluster node by node.. on couple of the nodes, the
shutdown was graceful. and then i hit this exception on 2 other nodes.

thanks

On Mon, Jun 23, 2014 at 2:14 AM, Igor Motov notifications@github.com
wrote:

&gt; Which version of elasticsearch was it? Did you shutdown entire cluster or
&gt; just the master node?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6506#issuecomment-46821721
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added ServiceDisruptionScheme(s) and testAckedIndexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6505</link><project id="" key="" /><description>This commit adds the notion of ServiceDisruptionScheme allowing for introducing disruptions in our test cluster. This abstraction as used in a couple of wrappers around the functionality offered by MockTransportService to simulate various network partitions. There is also one implementation for causing a node to be slow in processing cluster state updates.

This new mechanism is integrated into existing tests DiscoveryWithNetworkFailuresTests.

A new test called testAckedIndexing is added to verify retrieval of documents whose indexing was acked during various disruptions.

**NOTE:** this PR is against the feature/improve_zen branch
</description><key id="35771142">6505</key><summary>Added ServiceDisruptionScheme(s) and testAckedIndexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2014-06-16T06:37:49Z</created><updated>2014-06-17T12:47:49Z</updated><resolved>2014-06-17T12:47:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-16T12:02:05Z" id="46169790">LGTM
</comment><comment author="bleskes" created="2014-06-17T12:47:49Z" id="46301698">closed via: 3dd4b9488280c98f677a0eedacb71a0a827f24d1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better default size for global index -&gt; alias map</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6504</link><project id="" key="" /><description>The alias -&gt; (index -&gt; alias) map, specifically the index -&gt; alias one, typically just hold one entry, yet we eagerly initialize it to the number of indices. When there are many indices, each with many aliases, this is a very large overhead per alias...
</description><key id="35760068">6504</key><summary>Better default size for global index -&gt; alias map</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.1.3</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-15T22:52:19Z</created><updated>2015-06-07T13:13:59Z</updated><resolved>2014-06-15T22:53:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-15T22:53:00Z" id="46130703">Good catch! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Created and updated a section "Misc ElasticSearch Clients"(see bottom)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6503</link><project id="" key="" /><description>Kafka Standalone Consumer will read the messages from Kafka, processes and index them in ElasticSearch.

More details of this project can be found here: https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer/wiki
</description><key id="35745076">6503</key><summary>Created and updated a section "Misc ElasticSearch Clients"(see bottom)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">reachkrishnaraj</reporter><labels><label>docs</label></labels><created>2014-06-15T08:33:55Z</created><updated>2014-08-18T10:12:41Z</updated><resolved>2014-08-18T10:12:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T18:59:13Z" id="51516234">Hi @reachkrishnaraj 

Thanks for this PR, but I think a more appropriate place for it is here:
http://www.elasticsearch.org/guide/en/elasticsearch/client/community/current/integrations.html

could you update the PR please?

Also, we'll need you to sign the CLA before we can merge this in.  
http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="clintongormley" created="2014-08-18T10:12:41Z" id="52473760">Closed in favour of #7193
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>externalValueSet should be true when externalValue has been set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6502</link><project id="" key="" /><description>In context of mapper attachment and other mapper plugins, when dealing with multi fields, sub fields never get the `externalValue` although it was set.

This patch consider that an external value is set when `externalValue` is different than `null`.

Here is a full script which reproduce the issue when used with mapper attachment plugin:

```
DELETE /test

PUT /test
{
    "mappings": {
        "test": {
            "properties": {
                "f": {
                    "type": "attachment",
                    "fields": {
                        "f": {
                            "analyzer": "english",
                            "fields": {
                                "no_stemming": {
                                    "type": "string",
                                    "store": "yes",
                                    "analyzer": "standard"
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}

PUT /test/test/1
{
    "f": "VGhlIHF1aWNrIGJyb3duIGZveGVz"
}

GET /test/_search
{
    "query": {
        "match": {
           "f": "quick"
        }
    }
}

GET /test/_search
{
    "query": {
        "match": {
           "f.no_stemming": "quick"
        }
    }
}

GET /test/test/1?fields=f.no_stemming
```

Related to https://github.com/elasticsearch/elasticsearch-mapper-attachments/issues/57
</description><key id="35730809">6502</key><summary>externalValueSet should be true when externalValue has been set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-06-14T15:50:22Z</created><updated>2014-07-11T10:44:29Z</updated><resolved>2014-07-04T14:27:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-14T15:52:14Z" id="46091540">@jpountz This is the PR for the issue you got last week. Let me know.

I tested it against the GIST you sent me (see commit message) and it fixes it.
</comment><comment author="jpountz" created="2014-06-19T16:17:15Z" id="46582184">@dadoonet I played with your PR to try to see if we could fix the bug by cloning the context, which seems to work fine: https://github.com/jpountz/elasticsearch/commit/b03bab2b5e034e3f4453de89a19c1217e7f5941e What do you think? cc @s1monw 

The diff is unfortunately large due to some spaghetti code, but the interesting part is

``` diff
-    public void externalValue(Object externalValue) {
-        this.externalValueSet = true;
-        this.externalValue = externalValue;
+    /**
+     * Return a new context that will have the external value set.
+     */
+    public final ParseContext externalValue(final Object externalValue) {
+        return new FilterParseContext(this) {
+            @Override
+            public boolean externalValueSet() {
+                return true;
+            }
+            @Override
+            public Object externalValue() {
+                return externalValue;
+            }
+        };
     }
```

So intead of performing modifications in-place, it returns a wrapper that overrides the external value for the sub field mappers.
</comment><comment author="dadoonet" created="2014-07-04T14:27:03Z" id="48050144">Started some new work about this PR with @jpountz recently.
The change is completely different. Closing this one for now and will reopen a new PR based on our findings so far.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch - Conflict in retrieval of document saved with CouchbaseLite database?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6501</link><project id="" key="" /><description>I am using ElasticSearch on server side (.NET C#) that is sitting on the CouchDB database for indexing. And I am facing this issue with the documents which are updated with CouchbaseLite. If document is updated using CBLite then it throws exception. If document is saved using CouchDB then it works fine.

Lets say that this revision is saved using CBLite and synced to CouchDB server.

```
{
"_id":"d2f09b05-96f4-4fc0-aa94-73902fb4d469",
"_rev":"1-48eed54757c9fb2f8422a61bc460ed2d"
}
```

then elastic-search will throw an exception. Exception trace is given below.

```
Input string was not in a correct format.
When converting a string to DateTime, parse the string to take the data before putting each variable into the DateTime Object
```

Now, Lets say that this document is updated on CouchDB side from futon interface and its revision is update from `"_rev":"1-48eed54757c9fb2f8422a61bc460ed2d"` to `"_rev":"2-7b7d5ec4b5df62e12edb3f284a3702a7"`

Then document becomes.

```
{
"_id":"d2f09b05-96f4-4fc0-aa94-73902fb4d469",
"_rev":"2-7b7d5ec4b5df62e12edb3f284a3702a7"
}
```

Now elastic search will be accessing it successfully without throwing any exception.

If again, you update this document using CBL then elastic-search will keep on throwing above exception on server side. 

```
{
"_id":"d2f09b05-96f4-4fc0-aa94-73902fb4d469",
"_rev":"3-1321fc8139f8f87f8271efc6d3d0ca55"
}
```

And the change in the document is only this `_rev` field.
</description><key id="35722005">6501</key><summary>ElasticSearch - Conflict in retrieval of document saved with CouchbaseLite database?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">PankaJJakhar</reporter><labels /><created>2014-06-14T05:53:03Z</created><updated>2014-06-16T10:07:49Z</updated><resolved>2014-06-16T10:07:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="PankaJJakhar" created="2014-06-16T10:07:49Z" id="46161197">Issue was because of Gson library that I was using to convert my Java class to `Map&lt;String, Object&gt;`. It converts `Integer` value to `Float` which creates problem on ElasticSearch side. Elastic Search expects Integer value but it finds Float instead and Exception was thrown.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't expose hashes in Fielddata anymore.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6500</link><project id="" key="" /><description>Our field data currently exposes hashes of the bytes values. That takes roughly
4 bytes per unique value, which is definitely not negligible on high-cardinality
fields.

These hashes have been used for 3 different purposes:
- term-based aggregations,
- parent/child queries,
- the percolator _id -&gt; Query cache.

Both aggregations and parent/child queries have been moved to ordinals which
provide a greater speedup and lower memory usage. In the case of the percolator
it is used in conjunction with HashedBytesRef to not recompute the hash value
when getting resolving a query given its ID. However, removing this has no
impact on PercolatorStressBenchmark.
</description><key id="35687941">6500</key><summary>Don't expose hashes in Fielddata anymore.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Fielddata</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-13T16:46:51Z</created><updated>2015-06-07T13:14:12Z</updated><resolved>2014-06-13T21:18:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-13T18:37:12Z" id="46045560">this looks awesome! +1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove `ordinals` execution hint.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6499</link><project id="" key="" /><description>This was how terms aggregations managed to not be too slow initially by caching
reads into the terms dictionary using ordinals. However, this doesn't behave
nicely on high-cardinality fields since the reads into the terms dict are
random and this execution mode loads all unique terms into memory.

The `global_ordinals` execution mode (default since 1.2) is expected to be
better in all cases.
</description><key id="35670744">6499</key><summary>Remove `ordinals` execution hint.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-13T13:22:59Z</created><updated>2015-06-07T13:14:25Z</updated><resolved>2014-06-13T21:18:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-13T16:15:55Z" id="46030374">+1! LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RPM repository broken on RHEL5: &#171;Error performing checksum&#187;</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6498</link><project id="" key="" /><description>Yum complains about wrong checksums when the RPM repository is used in RHEL 5:

&gt;    $ yum list
&gt;    Loaded plugins: rhnplugin, security
&gt;    This system is receiving updates from RHN Classic or RHN Satellite.
&gt;    elasticsearch-0.90/primary_db                            | 9.1 kB     00:00
&gt;    http://packages.elasticsearch.org/elasticsearch/0.90/centos/repodata/primary.sqlite.bz2: [Errno -3] Error performing checksum
&gt;    Trying other mirror.
&gt;    Error: failure: repodata/primary.sqlite.bz2 from elasticsearch-0.90: [Errno 256] No more mirrors to try.

This problem has been dissected in this blog post: http://prefetch.net/blog/index.php/2009/11/26/dealing-with-yum-checksum-errors/.

The solution is to create the repositores using `createrepo` with the `-s sha1` option that enables the creation of SHA1 checksums instead of SHA256.
</description><key id="35666053">6498</key><summary>RPM repository broken on RHEL5: &#171;Error performing checksum&#187;</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">gioele</reporter><labels><label>:Packaging</label></labels><created>2014-06-13T12:06:16Z</created><updated>2015-05-07T06:20:58Z</updated><resolved>2015-05-07T06:20:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-06-13T12:19:32Z" id="46004431">Ahh, very interesting. i wasn't aware of this.
An other issue that CentOS 5 users have reported is that we sign with a to newer version of the signature ( version 4 versus version 3 what centos5 supports )
That might be the next issue you might encounter.
I haven't been able to solve the signature issue yet but i think we can solve the SHA issue.
I'll do some tests and update the issue when i find something.
</comment><comment author="sergei-maertens" created="2015-04-30T19:02:58Z" id="97928464">what's the status of this? I'm having the same problem on CentOS 5.11 and would prefer to install/update via yum instead of sources.
</comment><comment author="spinscale" created="2015-05-04T13:19:21Z" id="98702852">Hey

I just checked this, and @electrical if right. Even though we change the `createrepo` call, we still call `gpg` with the wrong arguments. I would like to incorporate this fully into our built, so we need the possibility to configure this via `mvn` - I will ask the maven-rpm-plugin folks to support this feature, by allowing to configure the `%__gpg_sign_cmd` call.

In the meantime I will try to add this to the build by manually adding this to the `~/.rpmmacros`.

@sergei-maertens @gioele Do you know, if there is any disadvantage in using the old signatures or the sha1 hashing in `crearerepo`? After all there might be a reason for the change? (sorry I am not a packaging guru) Especially, will all the newer RPM based distros work with the old repo?
</comment><comment author="spinscale" created="2015-05-04T14:15:59Z" id="98724216">So, from digging around a bit more there are two things. First, we need to have a custom GPG key, because the old rpm only seems to accept certain sized keys (no big ones), second we seem to need an own repository, which is only for those older versions. Not sure, how to integrate this in our build infra.
</comment><comment author="spinscale" created="2015-05-06T08:05:57Z" id="99366544">Hey,

so we talked this through and decided to not support this for now, as it would make our automated build and release process far more complex (creating the RPM twice and then processing it differently, signing it with another key and have two different repositories). That said this does not mean it's set in stone for eternity.

A couple of workarounds are available if you are willing to create your own repo.
- Built elasticsearch with `mvn rpm:rpm` and it to your own repo (you can check the `build_repositories.sh` script in the elasticsearch for some help
- Grab the zip file and maybe create your own package using [fpm](https://github.com/jordansissel/fpm), if you do not care to much about rpm standards
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Resiliency: Add basic Backwards Compatibility Tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6497</link><project id="" key="" /><description>This commit add a basic infrastructure as well as primitive tests
to ensure version backwards compatibility between the current
development trunk and an arbitrary previous version. The compatibility
tests are simple unit tests derived from a base class that starts
and manages nodes from a provided elasticsearch release package.

Use the following commandline executes all backwards compatiblity tests
in isolation:

```
mvn test -Dtests.bwc=true -Dtests.bwc.version=1.2.1 -Dtests.class=org.elasticsearch.bwcompat.*
```

These tests run basic checks like rolling upgrades and
routing/searching/get etc. against the specified version. The version
must be present in the `./backwards` folder as
`./backwards/elasticsearch-x.y.z`

Note: the tests currently fail if run with `-Des.node.mode=local`
</description><key id="35660006">6497</key><summary>Resiliency: Add basic Backwards Compatibility Tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>release highlight</label><label>resiliency</label><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-13T10:20:40Z</created><updated>2014-07-16T12:38:03Z</updated><resolved>2014-06-16T10:48:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-13T11:12:56Z" id="45999853">It looks great!
</comment><comment author="martijnvg" created="2014-06-13T11:50:45Z" id="46002342">+1 this looks very good
</comment><comment author="s1monw" created="2014-06-13T14:22:46Z" id="46016901">I added another test - basic test for analyzers and some cleanups I think this is ready
</comment><comment author="s1monw" created="2014-06-14T20:30:03Z" id="46098799">@nik9000 I pushed a bunch of cleanups + javadocs. Can you take a look if things make sense to you?
</comment><comment author="nik9000" created="2014-06-16T12:26:35Z" id="46171718">@s1monw just got to read it - makes sense.
</comment><comment author="s1monw" created="2014-06-16T12:27:05Z" id="46171761">@nik9000 thanks for looking at it!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Named queries (_name) are inconsistent and the documentation is wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6496</link><project id="" key="" /><description>See http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-request-named-queries-and-filters.html, it says:

&gt; Each filter and query can accept a `_name` in its top level definition.

However, not all queries/filters can accept a `_name`, e.g.

``` json
{
  "query": {
    "simple_query_string": {
      "query": "bungalow",
      "fields": [
        "building_type"
      ],
      "_name": "blah"
    }
  }
}
```

returns you a `QueryParsingException[[index] [simple_query_string] unsupported field [_name]]`

---

Some queries let you put the `_name` in the top level definition (as the documentation suggests), while some insist that the `_name` is **not** in its "top level definition", but inside the field object, e.g.

valid:

``` json
{
  "query": {
    "term": {
      "building_type": {
        "value": "bungalow",
        "_name": "blah"
      }
    }
  }
}
```

invalid:

``` json
{
  "query": {
    "term": {
      "building_type": "bungalow",
      "_name": "blah"
    }
  }
}
```

It would make more sense to me if the `_name` was alongside the field name, rather than within it.

---

Some types are even inconsistent within themselves, e.g.

range query

``` json
{
  "query": {
    "range": {
      "field1": {
        "gt": 10,
        "lt": 20,
        "_name": "range1"
      }
    }
  }
}
```

range filter

``` json
{
  "query": {
    "constant_score": {
      "filter": {
        "range": {
          "field1": {
            "gt": 10,
            "lt": 20
          },
          "_name": "range1"
        }
      }
    }
  }
}
```

---

The documentation also says that when filters match the list of matches will be returned in an element called `matched_filters`, however it appears that they just show up in the `matched_queries` list. This even applies to filters supplied as part of the `filter` (as opposed to `query`) element of the search request body.

Note I'm testing these on 1.0.2 currently.
</description><key id="35658954">6496</key><summary>Named queries (_name) are inconsistent and the documentation is wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label></labels><created>2014-06-13T10:02:32Z</created><updated>2015-10-15T17:15:41Z</updated><resolved>2015-10-15T17:15:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2015-10-15T17:15:41Z" id="148462375">This has been discussed and fixed in #11744 as part of the query-refactoring effort which is now in the master branch (future 3.0 release).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make sure afterBulk is always called in BulkProcessor</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6495</link><project id="" key="" /><description>Java API: Make sure afterBulk is always called in BulkProcessor.

Also strenghtened BulkProcessorTests by adding randomizations to existing tests and new tests for concurrent request.

Closes #5038
</description><key id="35658727">6495</key><summary>Make sure afterBulk is always called in BulkProcessor</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Java API</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-13T09:58:49Z</created><updated>2015-06-07T23:30:00Z</updated><resolved>2014-06-13T15:54:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-13T10:00:13Z" id="45994623">added a small comment... otherwise lgtm
</comment><comment author="javanna" created="2014-06-13T13:00:58Z" id="46007979">Updated according to review, also added a commit to make sure we don't call `afterBulk` twice in the blocking case.
</comment><comment author="kimchy" created="2014-06-13T13:44:58Z" id="46012468">LGTM
</comment><comment author="spinscale" created="2014-06-13T15:06:27Z" id="46022274">LGTM
</comment><comment author="javanna" created="2014-06-13T15:54:36Z" id="46028060">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wildcard expansion on field names should not try to match on `name`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6494</link><project id="" key="" /><description>If you have two fields `bar` and `foo.bar`, `b*` would match both since the regexp is applied to the index name, the full name and the name. And for `foo.bar`, these are:
- index name: `foo.bar`
- full name: `foo.bar`
- name: `bar`

I don't think the regexp should try to match the `name`.

Related to: https://github.com/elasticsearch/elasticsearch/issues/4081
</description><key id="35655441">6494</key><summary>Wildcard expansion on field names should not try to match on `name`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-06-13T09:06:18Z</created><updated>2014-12-30T19:57:46Z</updated><resolved>2014-12-30T19:57:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-13T11:13:37Z" id="45999891">I agree.  If wildcards are used, then they should match the full path, not the short version.  Additional thought: it would be nice to allow `*` to match just one step in the path, and `**` to match across dots, so, given fields:
- `name.first`
- `name.first.raw`
- `name.last`
- `name.last.raw`

This pattern `name.*` would match only `name.first` and `name.last`, while `name.**` would match all four fields.
</comment><comment author="clintongormley" created="2014-12-30T19:57:46Z" id="68391467">Closing in favour of #8870
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possibility to access search templates via Rest-API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6493</link><project id="" key="" /><description>There should be a possibility to access the search templates and get the parameters.Because there is no way you know which search templates are possibile, if you dont have access to the file system and parse the mustache files.
</description><key id="35654621">6493</key><summary>Possibility to access search templates via Rest-API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Greevee</reporter><labels /><created>2014-06-13T08:53:05Z</created><updated>2014-12-30T19:54:30Z</updated><resolved>2014-12-30T19:54:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-13T09:09:05Z" id="45990612">I think you are looking for this: #5637, right?
</comment><comment author="Greevee" created="2014-06-13T09:40:24Z" id="45993109">ya right, sorry, didnt notice. Important part is to get access to the available templates, without knowing their name!
</comment><comment author="dadoonet" created="2014-06-13T09:49:20Z" id="45993793">So a

```
GET _search/template
```

that is?

I updated the issue. May be we can close this one?
</comment><comment author="Greevee" created="2014-06-13T10:08:18Z" id="45995263">sorry, ill have to ask again, maybe im just confused : )

My Problem: i have some .mustache files in the "config\scripts" directory. I know how to access them via rest, thats no a problem! but i dont know the name of the templates, thats my problem, is there a way to access the available templates? in my case there are template1.mustache and template2.mustache and i want to get the 2 avaiable templates via rest, without knowing they are names template1 and template2.

if this is possible, sure, this ticket is obsolet and can be closed! :)
</comment><comment author="clintongormley" created="2014-12-30T19:54:30Z" id="68391016">Hi @Greevee 

We don't support returning node-level config file info via the REST API.  I suggest you use indexed search templates instead, which can then be entirely managed by the REST API. 
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-template.html#pre-registered-templates
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Valid geo_shape incorrectly throws InvalidShapeException: Too few distinct points in geometry component</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6492</link><project id="" key="" /><description>This shape is generated from PostGIS using `st_asgeojson(st_makevalid(geom))` to ensure it's a valid geometry. The SRID is 4326. Many [other geometries](https://gist.githubusercontent.com/nicerobot/10c8daf5273017b8b1c7/raw/5c1f689fa1510897ae17f229ad852f1017fdffbd/works.geojson) using the same technique are loaded just fine.

Here's a [gist with the geojson and exception](https://gist.github.com/nicerobot/10c8daf5273017b8b1c7) (and [the raw geojson](https://gist.githubusercontent.com/nicerobot/10c8daf5273017b8b1c7/raw/00e7a6fbb37797856aa6aa95a7720ca9bde27e17/boston.geojson)).

```
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [shape_]
  at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:249)
  at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:538)
  at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:480)
  at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)
  at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)
  at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:394)
  at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:413)
  at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:155)
  at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:534)
  at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:433)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744)
Caused by: com.spatial4j.core.exception.InvalidShapeException: Too few distinct points in geometry component at or near point (-71.1651, 42.1832, NaN)
  at com.spatial4j.core.shape.jts.JtsGeometry.validate(JtsGeometry.java:125)
  at org.elasticsearch.common.geo.builders.ShapeBuilder.jtsGeometry(ShapeBuilder.java:87)
  at org.elasticsearch.common.geo.builders.MultiPolygonBuilder.build(MultiPolygonBuilder.java:76)
  at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:234)
  ... 12 more
```
</description><key id="35640183">6492</key><summary>Valid geo_shape incorrectly throws InvalidShapeException: Too few distinct points in geometry component</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicerobot</reporter><labels /><created>2014-06-13T02:30:05Z</created><updated>2014-08-15T10:18:23Z</updated><resolved>2014-08-15T10:18:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-15T10:18:23Z" id="52292532">Duplicate. Closing in favour of #3909 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>query chaining and conditional break</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6491</link><project id="" key="" /><description>Hi
This mabbe a feature request as I cudnt find out anyway to do this.
I want to run multiple queries on an index q1,q2,q3 in sequence. But in a way that if i get results from q1 then i dont want q2,q3 to be executed.
If there is a way in current setup do let me know
</description><key id="35633549">6491</key><summary>query chaining and conditional break</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hitchhikar</reporter><labels /><created>2014-06-12T23:42:57Z</created><updated>2016-06-09T19:04:20Z</updated><resolved>2014-12-30T19:51:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:51:54Z" id="68390717">Hi @hitchhikar 

No there isn't a way to do this.  You will have to execute multiple queries serially via the API instead.
</comment><comment author="ebuildy" created="2016-06-09T19:04:20Z" id="224994673">That will be a brillant feature, I always run several queries one after one like a "fallback chain" :/ 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Factor parameter in Date Histogram aggregation not accepted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6490</link><project id="" key="" /><description>According to documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#_factor_2) date histogram aggregation should accept _factor_ parameter. But using it results in 

```
Parse Failure [Unknown key for a VALUE_NUMBER in [agg]: [factor].]
```

Query example:

``` javascript
"aggs" : { 
    "registration": {
        "date_histogram": {
            "field": "registration_date",
            "interval": "day", 
            "format": "dd-MM-yyyy",
            "min_doc_count": 0,
            "extended_bounds": {
                "min": "now/d-30d",
                "max": "now/d"
            },
            "factor": 1000  
        }
    }
}
```

Without _factor_, query works just fine.

Using version 1.1.2, but looking to source of master (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java#L124), _factor_ is not used there neither. 
</description><key id="35632402">6490</key><summary>Factor parameter in Date Histogram aggregation not accepted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">junckritter</reporter><labels /><created>2014-06-12T23:20:20Z</created><updated>2014-08-18T09:52:32Z</updated><resolved>2014-08-18T09:52:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="RyanNielson" created="2014-07-10T18:40:58Z" id="48646155">I'm running into the same issue. Any word on this as it's causing some parsing errors on my end which means I have to use facets for now.
</comment><comment author="clintongormley" created="2014-07-10T18:43:18Z" id="48646460">@uboness it looks like `factor` isn't supported in date histo aggs, even though it is documented: 
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#_factor_2
</comment><comment author="clintongormley" created="2014-08-18T08:48:04Z" id="52466153">@jpountz are there plans to support `factor` or in date_histos or should we drop it?  I'd be in favour of dropping it.  date histos should be run on `date` fields.
</comment><comment author="jpountz" created="2014-08-18T08:52:41Z" id="52466563">@clintongormley I would be +1 on dropping it as well and requiring the field to be either a date or unmapped in order for a date aggregation to succeed.
</comment><comment author="clintongormley" created="2014-08-18T09:52:32Z" id="52471876">Agreed - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>I found a typo. It was written fielddata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6489</link><project id="" key="" /><description>I replaced with field data
</description><key id="35618237">6489</key><summary>I found a typo. It was written fielddata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fabiofumarola</reporter><labels /><created>2014-06-12T20:03:22Z</created><updated>2014-06-13T10:58:24Z</updated><resolved>2014-06-13T10:58:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-13T10:55:41Z" id="45998668">hi @fabiofumarola 

Actually, we use "fielddata" in most places and all params which refer to fielddata.  Unfortunately,in a lot of places we also talk about "field data". In fact we often have both versions in the same sentence!

In this case, it is referring to a parameter name, so I think we should display it as code.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added link to "native" ES client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6488</link><project id="" key="" /><description /><key id="35605737">6488</key><summary>Added link to "native" ES client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">winterstein</reporter><labels /><created>2014-06-12T17:30:12Z</created><updated>2014-06-13T10:50:04Z</updated><resolved>2014-06-13T10:50:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-13T10:50:04Z" id="45998274">Thanks, merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added missing comma in suggester example</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6487</link><project id="" key="" /><description /><key id="35601746">6487</key><summary>Added missing comma in suggester example</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">stephlag</reporter><labels /><created>2014-06-12T16:42:15Z</created><updated>2014-06-13T14:01:12Z</updated><resolved>2014-06-13T14:01:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-13T10:46:29Z" id="45998036">Hi @stephlag 

Thanks for the fix. Please could I ask you to sign our CLA so that I can merge your PR in?  http://elasticsearch.org/contributor-agreement

thanks
</comment><comment author="stephlag" created="2014-06-13T13:42:29Z" id="46012201">Already signed
</comment><comment author="clintongormley" created="2014-06-13T14:01:12Z" id="46014353">thanks @stephlag 

merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to serialize negative thread pool sizes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6486</link><project id="" key="" /><description>As a SizeValue is used for serializing the thread pool size, a negative number
resulted in throwing an exception when deserializing (using -ea an assertionerror
was thrown).

This fixes a check for changing the serialization logic, so that negative numbers are read correctly.

Closes #6325
Closes #5357
</description><key id="35591761">6486</key><summary>Allow to serialize negative thread pool sizes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T14:57:12Z</created><updated>2015-06-07T19:43:37Z</updated><resolved>2014-07-16T13:27:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-12T14:57:27Z" id="45902443">**Note**: There are several solutions to this problem. My first approach was to allow -1 in `SizeValue`, but I think a size value should always be positive and serializing this using writeLong instead of writeVLong seemed wasteful. This is why I opted for the ThreadPool.Info to either be a sizevalue or -1, as this setting is implementation specific to the threadpool.

Also, the bwc layer could potentially be removed for master.

Maybe someone has a better idea
</comment><comment author="s1monw" created="2014-07-09T22:34:11Z" id="48544014">@spinscale can you pick this up any time soon or should somebody else pick it up?
</comment><comment author="spinscale" created="2014-07-14T16:20:30Z" id="48921436">@s1monw updated the PR, feel free to take another look. `SizeValue` now does not accept a `-1` value, but when parsing from a string it automatically sets it to `unbounded`.. added test to ensure that reading `-1` as setting still works as expected as well as serialization
</comment><comment author="s1monw" created="2014-07-15T13:14:30Z" id="49029183">I left comments on the latest push
</comment><comment author="spinscale" created="2014-07-15T18:13:01Z" id="49070983">added the `UNBOUNDED` size value to `ThreadPool`.. allowed SizeValue to be negative again and added some unit tests for it
</comment><comment author="s1monw" created="2014-07-15T19:41:41Z" id="49082241">ok so now I am a bit lost... I don't get why we add unittests for SizeValue  with negative values? didn't we fix this before to make sure you can't pass a negative value?
</comment><comment author="spinscale" created="2014-07-16T08:09:39Z" id="49135445">oh boy, thats what happens when you push things before hopping on a plane after giving a workshop all day, sorry for that! back to throwing an exception and and incorporated your review comment
</comment><comment author="s1monw" created="2014-07-16T13:09:35Z" id="49162443">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Facets deprecated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6485</link><project id="" key="" /><description>Users are encouraged to move to the new aggregation framework that was
introduced in Elasticsearch 1.0.
</description><key id="35590426">6485</key><summary>Docs: Facets deprecated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>docs</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T14:44:28Z</created><updated>2014-07-16T12:39:05Z</updated><resolved>2014-06-13T11:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T15:21:52Z" id="45905811">LGTM
</comment><comment author="roytmana" created="2014-06-12T23:06:29Z" id="45959043">Please do not deprecate facets until aggtegation supports _missing and _other buckets like facets do
</comment><comment author="jpountz" created="2014-06-12T23:22:48Z" id="45960100">@roytmana there are a `missing` and a `value_count` aggregations that can be used to compute exactly the same thing as facets' `_missing` and `_other`.
</comment><comment author="roytmana" created="2014-06-13T03:57:51Z" id="45973658">@jpountz i am aware of them but it is not the same as explicitly supporting buckets of such kinds. The issue is really in the usability. When you have couple of levels of sub aggregations and forced repeating them on missing and other aggs on each level in addition to defining them in say terms agg plus you have few metrics aggs on all these levels it gets rather complicated to achieve the same results. When you consider that people often need to build generic frameworks or UIs around it, the need to translate concept of missing/other buckets into bloated config with missing/other aggs and then translate results back into consistent bucket structure becomes rather unpleasant experience. I did a similar thing (composite facets classes such as multi-filter facets or _other supports for stats which decomposed my extended facet onto several supported facets and then transformed results of multiple facets into unified bucket-like  format) for facets but that was fairly easy because of single level nature of their aggregation. With aggregations it would be lot more headache to do. And performance will probably suffer for _other aggregation when done separately

For more discussion please see #5324 and #6273 

Thank you,
Alex
</comment><comment author="uboness" created="2014-06-13T07:21:16Z" id="45982723">@roytmana we're well aware of your concerns. With that, on our side we're ready to deprecate facets as development on them stopped and all our efforts focus on aggs (to the point where at the moment we need to carry redundant constructs in the code base that we'd love to simply get rid of). Please note that we're talking about deprecation here, not an immediate removal. Hopefully by the time we remove them, the aggs will support the definition of default values for missing, which (as previously discussed) will serve as a viable alternative to the _missing functionality you're looking for... In any case, we don't see the verbosity of the API in the specific scenario you're dealing with as a viable reason to put the facets deprecation on hold
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix reducing of range aggregations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6484</link><project id="" key="" /><description>Under some rare circumstances:
- local transport,
- the range aggregation has both a parent and a child aggregation,
- the range aggregation got no documents on one shard or more and several
  documents on one shard or more.

the range aggregation could return incorrect counts and sub aggregations.

The root cause is that since the reduce happens in-place and since the range
aggregation uses the same instance for all sub-aggregation in case of an
empty bucket, sometimes non-empty buckets would have been reduced into this
shared instance.

In order to avoid similar bugs in the future, aggregations have been updated
to return a new instance when reducing instead of doing it in-place.

Close #6435
</description><key id="35587311">6484</key><summary>Fix reducing of range aggregations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T14:13:19Z</created><updated>2015-06-07T19:43:52Z</updated><resolved>2014-06-13T21:19:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Admin: Expose IndexWriter and versionMap RAM usage in stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6483</link><project id="" key="" /><description>#6443 added RAM accounting to InternalEngine's versionMap.  In this issue we should make this available; Boaz suggested ShardStats and it's family and the indices cat api.
</description><key id="35583878">6483</key><summary>Admin: Expose IndexWriter and versionMap RAM usage in stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>low hanging fruit</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T13:34:06Z</created><updated>2014-08-25T18:00:44Z</updated><resolved>2014-07-15T00:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>write consistency levels -- quorum of two is two</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6482</link><project id="" key="" /><description>In `TransportShardReplicationOperationAction`'s verification of the write consistency level where it checks before executing the write if sufficient shards are available,

``` java
                    int requiredNumber = 1;
                    if (consistencyLevel == WriteConsistencyLevel.QUORUM &amp;&amp; shardIt.size() &gt; 2) {
                        // only for more than 2 in the number of shardIt it makes sense, otherwise its 1 shard with 1 replica, quorum is 1 (which is what it is initialized to)
                        requiredNumber = (shardIt.size() / 2) + 1;
                    } else if (consistencyLevel == WriteConsistencyLevel.ALL) {
                        requiredNumber = shardIt.size();
                    }
```

the `(n / 2) + 1)` should apply even with just 2 shards, giving a `requiredNumber = 2`. This is the commonly accepted definition.
</description><key id="35577898">6482</key><summary>write consistency levels -- quorum of two is two</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shikhar</reporter><labels><label>:CRUD</label><label>discuss</label><label>resiliency</label></labels><created>2014-06-12T12:08:20Z</created><updated>2016-09-27T13:03:42Z</updated><resolved>2016-09-27T12:46:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-12T12:09:44Z" id="45883436">It would stop Elasticsearch from working out of the box, as you'd have to run at least two nodes before you could index anything.
</comment><comment author="shikhar" created="2014-06-12T12:13:28Z" id="45883721">@clintongormley is the default `number_of_replicas` not 0? then that's the problem :)
</comment><comment author="shikhar" created="2014-06-12T12:14:23Z" id="45883794">note that with 1 shard, in java `(1/2)+1)` results in the right thing == 1
</comment><comment author="shikhar" created="2014-06-12T12:24:43Z" id="45884637">I see that the default `number_of_replicas` is 1. So to have an index on just one node and keep cluster health green, you anyway have to set this to 0.
</comment><comment author="clintongormley" created="2014-06-12T14:51:02Z" id="45901558">Having a yellow cluster in dev is just fine.  Having a good default for `number_of_replicas` (ie `1`) is good.  Not being able to develop on Elasticsearch without running two instances is bad.  

What's the worst that could happen?  The primary accepts a write where there isn't a replica available.  It's not like a minority of replicas is accepting writes.
</comment><comment author="shikhar" created="2014-06-12T15:05:14Z" id="45903459">&gt; Having a yellow cluster in dev is just fine. Having a good default for number_of_replicas (ie 1) is good. Not being able to develop on Elasticsearch without running two instances is bad.

Going to production, having two instances, `number_of_replicas = 1` (the default), using the write consistency level of quorum (also the default), and seeing data loss because one node went down -- is way worse.

&gt; What's the worst that could happen? The primary accepts a write where there isn't a replica available. It's not like a minority of replicas is accepting writes.

1 is not a quorum of 2. The point of a write consistency level of quorum is (or should be) to have redundancy if one node was to go down. It's misleading.
</comment><comment author="kimchy" created="2014-06-12T15:31:41Z" id="45907270">I disagree, this default is good out of the box, otherwise ES will not work on a single node case with default settings. We can potentially document it better, that this is what happens when there is 1 replica (so total of 2 copies of the data).
</comment><comment author="shikhar" created="2014-06-12T15:52:00Z" id="45910142">The out of the box experience can be addressed by changing the default to `number_of_replicas = 0`. It's really not a big deal to have to change that setting for production. In case users want a green cluster health with the current default, they have to setup discovery for the additional node in any case.

`WriteConsistencyLevel.QUORUM` is really `WriteConsistencyLevel.QUORUM_BUT_NOT_ALWAYS`. IMO it's not really something that should be papered over with a warning in the documentation.
</comment><comment author="kimchy" created="2014-06-12T16:03:33Z" id="45911772">I think that having the default as 1 for number of replicas is a better out of the box solution. Seems like we are at an impasse and rehash the same argument, this is the decision we went with at Elasticsearch, and changing it have too many downsides, with no real upsides.
</comment><comment author="shikhar" created="2014-06-12T16:10:09Z" id="45912588">Impasse acknowledged :) I will open a separate issue for the _real_ problem, which is that acknowledged writes may be lost since the current consistency-level check only happens before the write is issued (based on my current understanding anyway, will verify...)
</comment><comment author="kimchy" created="2014-06-17T12:38:13Z" id="46300860">FYI, I updated the docs to reflect this case, we missed it on our end.

I will add, that the number 2 is just a tricky number when it comes to distributed systems. I would argue that either quorum in this case set to 2, or it being set to 1 can be debatable..., since 2 in this case also means all. The reason we went with the default mentioned is because many times people run Elasticsearch using 1 node, or 2 (as a search platform for their database), on top of the just getting started aspect, and they are ok with potentially needing to reindex the data with the downsides that come with 1 or 2 nodes.

Its similar in nature to deciding what should be the default number for minimum_master_nodes when running with 2 nodes. Any value you choose for it has implications, 1 means you might loose data and hit split brain, 2 means that once a single node is down, you can't do anything with the cluster (in our improve zen branch, we added the ability where they can opt to still being able to search on it, which will help the decision of which value to choose). For people running Elasticsearch with only 2 nodes, they need to decided and live with the implications that come with it.

The same applies to running Elasticsearch with 2 copies of the data (number of replicas set to 1). If people care more about their data, then they will configure it to run with 2 replicas (3 copies), and then the default quorum does the right thing. But many are ok with running with 2 copies of the data (for the scenario explained above), and not have to allocate more resources to sustain a cluster with 3 copies of the data. Its a matter of tradeoffs, and the number 2 just makes it tricky....

Maybe the name quorum is misleading when it comes to 2 copies of the data, but to be honest, I haven't seen users being confused by our default behavior, and they understand what it means. So it doesn't seem beneficial to change the current behavior, especially when it can be easily configured (config or per API)
</comment><comment author="Wilfred" created="2014-06-27T15:01:24Z" id="47356465">FWIW, as a user, I was confused by this issue: https://github.com/mobz/elasticsearch-head/issues/134
</comment><comment author="kimchy" created="2014-06-27T15:03:38Z" id="47357107">@Wilfred I don't think that its the same type confusion, this one is more of "why my cluster is yellow" if I have one node, which is because ES defaults to 1 additional replica. If it defaults to 2 additional replicas, then the cluster would not be writable with 1 node.
</comment><comment author="shikhar" created="2014-10-23T19:59:51Z" id="60299543">A scenario where this led to actual trouble for a user: https://groups.google.com/d/msg/elasticsearch/M17mgdZnikk/N6q9iGWRxncJ
</comment><comment author="shikhar" created="2015-09-09T03:06:16Z" id="138763649">v2.0 seems like a good opportunity to fix this :-)

The main argument I have heard here is the out-of-the-box experience, which can be addressed by making the default `number_of_replicas` 0.
</comment><comment author="bleskes" created="2015-09-09T06:49:18Z" id="138806248">@shikhar the one node experience is one argument. The other is being fault tolerant when loosing a node (and it's replicas). If we remove the exception of quorum for the two data copies case, (i.e., primary and replica) people won't be able to index when they loose a node, until the replica is re-constructed on another node - which may take a long time depending on shard size. To avoid this, one would need to keep 3 copies of everything by default. This is all about making the right tradeoffs between things and we feel that asking people to have 3 copies of their data (potentially many terabytes if not more) by default  goes too far. The alternative of having only one copy by default also doesn't seem appealing as it is a risky default. Of course, people can make another choice and change the setting - and we should document that clearly. 
</comment><comment author="dakrone" created="2016-09-27T12:46:39Z" id="249854283">It's been a year of discussion on this since the last comment, I don't think we are planning on changing the default number of replicas to 0, or changing the quorum value, so I'm going to close this, we can re-open if people still want to discuss it.
</comment><comment author="bleskes" created="2016-09-27T13:03:17Z" id="249858107">Thx @dakrone for bubbling this up. I think this is covered by #19454, which removes the quorum terminology, so we can consider it properly closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update of setup docs to include directory structure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6481</link><project id="" key="" /><description>I'm in the process of forking the repo to include a table with directory structures for the rpm/deb and tar.gz/zip files in the docs/reference/setup/dir-layout.asciidoc file, making it a little easier to understand where things are.

Just wanted to follow procedure and raise this before issuing a pull request, incase there is anything I may need to know beyond the contributing doc.
</description><key id="35575874">6481</key><summary>Update of setup docs to include directory structure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markwalkom</reporter><labels /><created>2014-06-12T11:32:50Z</created><updated>2014-06-12T21:23:27Z</updated><resolved>2014-06-12T21:23:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-12T12:08:17Z" id="45883332">For doc PRs, no need for an issue - you can just describe what you're doing in the PR itself.  Close this issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wait till node is part of cluster state for join process</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6480</link><project id="" key="" /><description>When a node sends a join request to the master, only send back the response after it has been added to the master cluster state and published.
This will fix the rare cases where today, a join request can return, and the master, since its under load, have not yet added the node to its cluster state, and the node that joined will start a fault detect against the master, failing since its not part of the cluster state.
Since now the join request is longer, also increase the join request timeout default.
</description><key id="35573697">6480</key><summary>Wait till node is part of cluster state for join process</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T10:54:08Z</created><updated>2015-06-07T13:14:36Z</updated><resolved>2014-06-12T16:16:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-12T11:07:02Z" id="45878787">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Typo in histogram-facet.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6479</link><project id="" key="" /><description>Spotted a typo, which I've fixed.
</description><key id="35570341">6479</key><summary>Typo in histogram-facet.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">MRdNk</reporter><labels><label>docs</label></labels><created>2014-06-12T10:30:37Z</created><updated>2014-07-01T08:50:41Z</updated><resolved>2014-07-01T08:50:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-12T10:44:57Z" id="45877199">Hi @MRdNk 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge your commit in?  http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="MRdNk" created="2014-06-15T18:42:44Z" id="46123931">Done.
</comment><comment author="MRdNk" created="2014-06-15T19:59:32Z" id="46126012">No problem, done.

Date: Thu, 12 Jun 2014 03:45:30 -0700
From: notifications@github.com
To: elasticsearch@noreply.github.com
CC: duncanwilkie@hotmail.com
Subject: Re: [elasticsearch] Typo in histogram-facet.asciidoc (#6479)

Hi @MRdNk 

Thanks for the PR.  Please could I ask you to sign the CLA so that I can merge your commit in?  http://www.elasticsearch.org/contributor-agreement/

thanks

&#8212;
Reply to this email directly or view it on GitHub.                    
</comment><comment author="clintongormley" created="2014-07-01T08:50:41Z" id="47631966">many thank, merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added caching support to geohash_filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6478</link><project id="" key="" /><description /><key id="35566234">6478</key><summary>Added caching support to geohash_filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Geo</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T10:10:35Z</created><updated>2015-06-07T13:14:58Z</updated><resolved>2014-06-12T20:21:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T10:13:16Z" id="45851379">I don't think we should cache this by default. It's very unlikely that you have cache hits here. In-fact this filter should rather not be cached at all and never be turned into a big bitset since this fitlter is likely going to be very fast (it's really just 9 terms after all?) I am not sure if we should do this?
</comment><comment author="jpountz" created="2014-06-12T10:14:51Z" id="45851485">+1 to add the ability to cache it
-1 to cache it by default
</comment><comment author="bleskes" created="2014-06-12T10:23:41Z" id="45860827">I'm a bit on the fence regarding the default - I was triggered to add it by someone using the geohash_cell filter as the fastest way to limit the result to a certain area. They were using level 4 with neighbours which translates to 120KM by 60KM box and put it under a bool filter which causes it to generate a bit set anyway. Maybe cache by default on level 4 or up?

I don't have a good grip on the different usages of the geohash_cell filter, so if the people fill it shouldn't be cached, I'm good with that. Let me know.
</comment><comment author="s1monw" created="2014-06-12T12:25:52Z" id="45884727">yeah I'd let the user decide ie. follow @jpountz suggestions.
</comment><comment author="bleskes" created="2014-06-12T12:55:23Z" id="45887336">kk. Pushed another commit with the cache turned off + added cache/cachekey params to the java builder.
</comment><comment author="bleskes" created="2014-06-12T19:05:13Z" id="45934434">@s1monw pushed another commit with an extra test
</comment><comment author="s1monw" created="2014-06-12T19:18:00Z" id="45935773">LGTM ;)
</comment><comment author="bleskes" created="2014-06-12T20:24:36Z" id="45943196">merged. Thx @s1monw , @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delegation of nextReader calls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6477</link><project id="" key="" /><description>Currently aggregations subscribe to setNextReader calls in AggregationContext.  When a new reader is used all reader aware objects are notified of the new reader.  With deferred aggregations children aggregations of a breath-first aggregation do not require to be notified of reader changes until the replay stage. Also, at the replay stage only the child aggregations of the breath-first aggregation need to be notified, we should not be notifying all the other aggregations of the new reader.

To solve this the calls for setNextReader are now handled by each aggregation so it can notify its child aggregations and any other ReaderContextAware objects (e.g. ValueSources) of the new reader at the relevant time.

The same idea has also been applied to the setScorer and setTopReader calls for Aggregators and ValueSources.
</description><key id="35566123">6477</key><summary>Delegation of nextReader calls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T10:08:45Z</created><updated>2015-06-07T13:15:06Z</updated><resolved>2014-06-13T10:10:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T12:56:52Z" id="45887479">@colings86 I stopped half way through and I wonder given the number of added simple delegate method if we should add an interface that we can implement like `SegmentAware` that has the two methods such that we can add util methods to iterate over arrays or list. Another example would be to have a 

``` Java
public static class FilteredValueSource extends ValueSource {
   private final SegmentAware delegate;

   public FilteredValueSource(SegmentAware delegate) {
      this.delegate = delegate;
   }

   public void setNextReader(IndexReaderContext ctx) {
      delegate.setNextReader(ctx);
   }

   //....
} 
```

just to clean this up a bit and remove some of the boilerplate code?
</comment><comment author="uboness" created="2014-06-12T13:45:21Z" id="45892810">@colings86 did you benchmark this change? In the past, calling `nextReader` on every aggregator was a bottleneck... perhaps it's not anymore in breadth first mode, but in depth first it still might be. In any case, we need to benchmark this change and compare it to calling `nexReader` on the agg context.
</comment><comment author="colings86" created="2014-06-12T13:47:00Z" id="45892983">@uboness no I haven't, but I will do now
</comment><comment author="colings86" created="2014-06-12T20:21:05Z" id="45942798">@uboness your concerns are well founded.  The performance of calling nextReader is about the same for single aggregations and sibling aggregations but deteriorates when aggregations are put on multiple levels. I'll have a quick look to see if there is any fixable bottleneck but if not then maybe the original problem could be solved by providing a different context for the replay stage of the deferred aggregations so that the nextReader is not called multiple times?
</comment><comment author="jpountz" created="2014-06-12T20:54:27Z" id="45946658">+1 to a dedicated context if calling setNextReader from the tree of aggregators proves to be too slow
</comment><comment author="uboness" created="2014-06-12T21:35:14Z" id="45951152">@colings86 sounds like a plan, +1 on dedicated context
</comment><comment author="colings86" created="2014-06-13T10:10:29Z" id="45995421">Performance of nextReader calls delegated through the aggregation tree is going to be too slow.  I will go with the using a different context for the replay stage of deferred aggregations.  This is going to be easier to implement from a new PR off master so I will close this PR and work on a new one for that change.
</comment><comment author="jpountz" created="2014-11-18T22:20:59Z" id="63556469">I was looking at how to integrate [per-segment collection](https://issues.apache.org/jira/browse/LUCENE-5527) into the aggregations framework but the fact that `AggregationContext` rules all calls to `setNextReader` makes it a nightmare, so it would be nice if we could somehow revamp this PR in a way that it still fast. Otherwise I have the feeling that it is going to cause more and more issues as we move forward.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issues with OutOfMemoryError on org.elasticsearch.index.engine.EngineClosedException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6476</link><project id="" key="" /><description>Hello,
I migrated a 4 nodes cluster to ES 1.2.0 from 0.90.9.
I started from scratch with the data and I reused most of the config I made for ES 0.90.9 (more or less the default one, just some index templates).
I am facing a lot of OutOfMemoryError: Map failed for "slightly bigger indexes", but they were not occurring on 0.90.0.

Backtrace
https://gist.github.com/lucabelluccini/9308b76234b0bea0c33a

JVM 7 55, 4
4 Machines with Intel(R) Xeon(R) CPU X5670 @ 2.93GHz, 50GB RAM and 24 Cores, SLES 11 SP1

At the beginning, I gave 24GB as JAVA HEAP.
Then I tried to lower the JAVA HEAP to 16GB...

Indexes are aroung 10GB each, ~10M documents, 8 Shards 1 Replica

Indices: 72(15 of them are 10GB, 10 are of 500MB and others ~KB)
Shards: 773
Data: 89.77 GB
CPU: ~3%
Memory: 9.52 GB / 49.56 GB
</description><key id="35564026">6476</key><summary>Issues with OutOfMemoryError on org.elasticsearch.index.engine.EngineClosedException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lucabelluccini</reporter><labels /><created>2014-06-12T09:39:09Z</created><updated>2014-12-30T19:50:19Z</updated><resolved>2014-12-30T19:50:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-12T10:09:32Z" id="45851071">the store type in ES changed to match the Lucene defaults, which means that now its `mmapfs` by default. You are probably running into the OS limits of mmap counts, you might need to run something like this: `sysctl -q -w vm.max_map_count=262144`. See more about mmap here: http://blog.thetaphi.de/2012/07/use-lucenes-mmapdirectory-on-64bit.html.

You can always revert to the `niofs` store type by setting `index.store.type` to `niofs` in the elasticsearch config.
</comment><comment author="lucabelluccini" created="2014-06-12T12:07:15Z" id="45883259">Thank you @kimchy I will test it as soon as I can ask the ops to set this setting on all the clusters.
So I could also reassign something like 20GB to the JAVA HEAP, I imagine.
</comment><comment author="clintongormley" created="2014-12-30T19:50:18Z" id="68390555">Hi @lucabelluccini 

I'm assuming you managed to resolve this problem, and the ticket can be closed. Please reopen if there are still issues.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `ignore_unavailable` option should also ignore indices that are closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6475</link><project id="" key="" /><description>The `ignore_unavailable` option should also ignore indices that are closed, but it doesn't at the moment. The PR makes sure that `ignore_unavailable` also ignores specified indices that are closed.

PR for #6471
</description><key id="35561787">6475</key><summary>The `ignore_unavailable` option should also ignore indices that are closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T09:06:32Z</created><updated>2015-06-07T19:45:53Z</updated><resolved>2014-07-01T11:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-17T09:17:09Z" id="46284643">@javanna I updated the PR and moved the 'canFailClosed' to IgnoreIndices.
</comment><comment author="javanna" created="2014-06-17T15:59:49Z" id="46327287">Left a few comments, looks good though
</comment><comment author="martijnvg" created="2014-06-20T21:02:54Z" id="46726273">Thanks @javanna, I updated the PR.
</comment><comment author="javanna" created="2014-06-30T18:49:24Z" id="47570478">Left a small comment, LGTM otherwise
</comment><comment author="martijnvg" created="2014-07-01T10:09:16Z" id="47638876">@javanna Thanks for reviewing it!
</comment><comment author="javanna" created="2014-07-10T12:25:29Z" id="48597488">Side note: this change makes it possible to ignore closed indices when using the `ignore_unavailable` option, whereas we would previously return a `ClusterBlockException`. It also changes the error returned when searching against closed indices without `ignore_unavailable` set, which is not a `ClusterBlockException` anymore but the newly introduced `IndexClosedException`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Check for tabs and nocommits in the code on validate</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6474</link><project id="" key="" /><description>This commit adds checks for nocommit and tabs in the source code.
The task is executed during the validate phase and can be disabled via
`-Dvalidate.skip`
</description><key id="35559158">6474</key><summary>Build: Check for tabs and nocommits in the code on validate</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>build</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-12T08:25:21Z</created><updated>2015-06-07T13:15:16Z</updated><resolved>2014-06-12T09:31:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T08:56:40Z" id="45844938">&gt; Will this fail if the source contains \t inside a string constant? Or maybe we just disallow that too...

no it will not fail - we use that often like in the CAT API
</comment><comment author="s1monw" created="2014-06-12T08:57:09Z" id="45844991">&gt; Can we also check e.g. .py, .xml, .pl, .txt (others?)? Or maybe invert the check to exclude known false positives...

I added a new commit
</comment><comment author="mikemccand" created="2014-06-12T08:58:59Z" id="45845154">Thanks Simon, LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Per-field boosting of the _all field doesn't work for multi-type queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6473</link><project id="" key="" /><description>```
# Remove old data
curl -XDELETE "http://localhost:9200/test"

# Create index
curl -XPUT "http://localhost:9200/test/"

# Define mapping
curl -XPOST "http://localhost:9200/test/vendor/_mappings" -d '
{
  "vendor": {
    "properties": {
      "name": {
        "type": "string",
        "boost": 1
      }
    }
  }  
}
'
curl -XPOST "http://localhost:9200/test/client/_mappings" -d '
{
  "client": {
    "properties": {
      "name": {
        "type": "string",
        "boost": 2
      }
    }
  }  
}
'

# Create Documents
curl -XPOST "http://localhost:9200/test/vendor/" -d '
{
  "name": "Mr John Doe"
}
'

curl -XPOST "http://localhost:9200/test/client/" -d '
{
  "name": "Mr John Doe"
}
'

# Search
curl -XPOST "http://localhost:9200/test/_search?pretty=true" -d '
{
  "query": {
    "query_string": {
      "query": "Mr John Doe"
    }
  }
}
'
```

Expected result: client record score to be higher than vendor's
Actual result: boost is ignored, scores are the same
</description><key id="35539221">6473</key><summary>Per-field boosting of the _all field doesn't work for multi-type queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alebo</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label></labels><created>2014-06-11T23:51:51Z</created><updated>2016-01-22T18:31:18Z</updated><resolved>2015-11-21T14:51:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2014-06-12T16:56:44Z" id="45918453">Yes, it looks like a bug. Because the first type has default boost the boosting mechanism doesn't seem to kick in. 

There are two possible workarounds here. First, you can make sure that type with non-default boost is processed first: 

```
curl -XPOST "http://localhost:9200/test/client,vendor/_search?pretty=true" -d '
{
  "query": {
    "query_string": {
      "query": "Mr John Doe"
    }
  }
}
'
```

The second workaround is to specify something other then 1.0 for the boost of the vendor. Something as simple as this should fix it:

```
curl -XPOST "http://localhost:9200/test/vendor/_mappings" -d '
{
  "vendor": {
    "properties": {
      "name": {
        "type": "string",
        "boost": 1.000001
      }
    }
  }  
}
'
```
</comment><comment author="brwe" created="2014-08-21T08:44:43Z" id="52893620">This seems to be a problem with `query_string`. If I query with `match` boosting works:

```
POST test/_search
{
  "query": {
    "match": {
      "name": "Mr John Doe"
    }
  }
}
```
</comment><comment author="clintongormley" created="2015-11-21T14:51:54Z" id="158649669">Closing this as it is no longer possible to specify different boosts on the same field name in two different types.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>File-based in-memory index support in ES.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6472</link><project id="" key="" /><description>We used to use Lucene directly generate index and query the index. The use case is that we first use MmapDirectory the generate a index in a batch manner. After the index is generated and optimized, we deploy this index to a query fleet which use read-only MMapDirectory backed RAMDirectory (new RAMDirectory(new MMapDirectory(dir))) to serve the queries to optimize the query latency.

Our index size (on one shard) is relatively small - at 1-3GB. We have observed significant (or very noticable) performance gains by using MMAPDirectory based RAMDirectory  instead of MMAPDirectory when query the index, the performance gain is 20% or much more in certain cases. And using G1 garbarge collector we are not seeing any issue with long pauses caused by GC.

I've learned that ElasticSearch's 'memory' index is based on RAMDirectory but not reading the index from a file. And any update to this in-memory index is non-persistent.

I think it's a worthwhile use case to support for ES. Use file-based store at batch indexing time, and switch to (optionally read-only) file-based memory store later at index querying time to optimize the query latency. I am actually wondering who would use a pure memory-based index store with the risk that you have to re-index every time the system crashed or restarted. This won't be practical especially when the index size is relatively large and takes time to re-index. There will be a loss of availability at time of re-indexing.

Could ES to either slightly change the implementation of 'memory' store to read the index from the file-system initially when this index is initialized, it can keep the behavior of not persisting the updates thereafter? Or a new index type of 'file-based-memory'?
</description><key id="35521356">6472</key><summary>File-based in-memory index support in ES.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markzlu</reporter><labels /><created>2014-06-11T20:08:28Z</created><updated>2014-06-12T07:45:43Z</updated><resolved>2014-06-12T07:45:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markzlu" created="2014-06-11T21:52:08Z" id="45805041">The JVM flag we used is "-server -d64 -XX:+UseG1GC -XX:MaxGCPauseMillis=30 -Xmx25000m". As long as the GC pause is tractable, we don't really care a little bit increase in CPU load.

We have 8 shards of lucene indices (on average 1.5G) managed by one JVM, We query these 8 shards parallelly then aggregate the result upon each search requests. The OS is RHEL 5 64-bit. JVM max heap is 25G, It's a VM (AWS cc2.8xlarge machine type). Java version is JDK 1.7.0_51. Lucene v 4.5. We run basic lucene queries, no facetting. We rarely see long GC pauses, we get very good latency and throughput number out of this system.
</comment><comment author="s1monw" created="2014-06-12T07:45:43Z" id="45838980">The Lucene RAM directory was never intended to be used in production while it can help in some edge cases. The performance of MMAP has been improved in the past and is the first choice today for several reasons. We removed the Ram based (Direct) directory from Elasticsearch in `1.0` and only support disk based directories. If you want to load certain parts of the system into memory (heap that is) you should rather look into using a ram based postings format. We don't plan to add the HEAP based directory implementations back in the future.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `ignore_unavailable=true` parameter seems to be ignored in REST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6471</link><project id="" key="" /><description>When trying to search against a closed index via curl with the `ignore_unavailable` parameter, an error response is returned:

``` bash
curl -X DELETE 'http://localhost:9200/index_1?pretty'

#2014-06-11T21:20:10+02:00 [200] (0.107s)
#
# {
#   "acknowledged":true
# }

curl -X DELETE 'http://localhost:9200/index_2?pretty'

#2014-06-11T21:20:12+02:00 [200] (0.022s)
#
# {
#   "acknowledged":true
# }

curl -X PUT 'http://localhost:9200/index_1?pretty'

#2014-06-11T21:20:25+02:00 [200] (0.218s)
#
# {
#   "acknowledged":true
# }

curl -X PUT 'http://localhost:9200/index_2?pretty'

#2014-06-11T21:20:29+02:00 [200] (0.112s)
#
# {
#   "acknowledged":true
# }

curl -X POST 'http://localhost:9200/index_2/_close?pretty'

#2014-06-11T21:20:37+02:00 [200] (0.024s)
#
# {
#   "acknowledged":true
# }

curl -X GET 'http://localhost:9200/index_1,index_2/_search?pretty&amp;ignore_unavailable=true'

#2014-06-11T21:20:57+02:00 [403] (0.005s)
#
# {"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}
```
</description><key id="35517597">6471</key><summary>The `ignore_unavailable=true` parameter seems to be ignored in REST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">karmi</reporter><labels /><created>2014-06-11T19:27:34Z</created><updated>2014-07-02T07:48:11Z</updated><resolved>2014-07-01T11:09:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Disable searching across all indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6470</link><project id="" key="" /><description>Hitting `/_search` causes elasticsearch to search all types, all indices.
With very large indices this is a problem.
We currently have ~30Tb of data across multiple logstash indices.

If someone talks to ES (eg, using sense or a other method) and doesn't specify the index name, the cluster effectively grinds to a half until we restart it.

Can we have an option to require specifying at least one index name to the search endpoint?

Thanks!
</description><key id="35517587">6470</key><summary>Disable searching across all indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>:Search</label><label>adoptme</label><label>enhancement</label></labels><created>2014-06-11T19:27:27Z</created><updated>2017-07-05T09:10:03Z</updated><resolved>2016-08-25T10:07:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-12T09:08:17Z" id="45845945">The setting 'allow_no_indices' requires at least one available index to be specified:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/multi-index.html
</comment><comment author="martijnvg" created="2014-06-12T12:14:57Z" id="45883840">The above comment doesn't make sense. The index expansion should be disabled in this your case. However there're only two values for 'expand_wildcards', either 'open' for open indices and 'closed' for closed indices. If you don't have closed indices setting this to 'close' will make specifying no index (and \* and _all as index)  not execute the search request. I think a third option 'none' should be added, which disables index expansion.
</comment><comment author="avleen" created="2014-06-12T15:33:14Z" id="45907484">Doesn't 'allow_no_indices' have to be set in the query?
We want a servers side setting to prevent this problem where people don't
use 'allow_no_indices' with their query.

This can too easily happen by accident right now.
On Jun 12, 2014 5:08 AM, "Martijn van Groningen" notifications@github.com
wrote:

&gt; The setting 'allow_no_indices' requires at least one available index to be
&gt; specified:
&gt; 
&gt; http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/multi-index.html
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6470#issuecomment-45845945
&gt; .
</comment><comment author="martijnvg" created="2014-06-12T16:57:11Z" id="45918514">Yes, the 'allow_no_indices' and other multi index settings have to be set in the search request.
</comment><comment author="avleen" created="2014-06-12T19:16:34Z" id="45935633">Right, that's exactly the problem :-)
Here's an analogy to the problem I face, where I have many people who will
want to query Elasticsearch directly:

Imagine if the `rm` command, when run with no arguments, defaulted to
defaulted to wiping your hard drive.
I can tell people to always use arguments, I can remind them constantly,
but it only takes one person one time to run `rm` without any arguments,
expecting everything to be safe.. Oops.

Without server-side enforcement here to protect the cluster, we have to
assume that everyone know and will always be able to use the right
arguments, which isn't possible.

On Thu, Jun 12, 2014 at 12:57 PM, Martijn van Groningen &lt;
notifications@github.com&gt; wrote:

&gt; Yes, the 'allow_no_indices' and other multi index settings have to be set
&gt; in the search request.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6470#issuecomment-45918514
&gt; .
</comment><comment author="martijnvg" created="2014-06-13T07:50:59Z" id="45984598">Right, I understand :-)  a server side setting disallowing a search without index or type specified makes sense.
</comment><comment author="tomryanx" created="2015-05-28T11:32:01Z" id="106285081">I'm also looking to enforce well formed queries by disabling _all and wildcard expansion on the index name.  I've disabled creation of _all on the index level, but over-eager queries are still able to search every index (when they should be restricted by date range and specify all relevant indices).  Any update on this one?
</comment><comment author="clintongormley" created="2015-05-28T19:17:51Z" id="106570378">I'm on the fence about this one. It seems simple to add, but then everybody has a different policy that they want to enforce, so a tiny feature becomes complex.  This is already easily solved by putting a proxy like nginx in front of ES and specifying the exact rules that you want to implement.  I'm not sure that we should go down this road...
</comment><comment author="nik9000" created="2015-05-28T19:26:25Z" id="106572409">&gt; It seems simple to add, but then everybody has a different policy that they want to enforce, so a tiny feature becomes complex.

I'm not sure the slippery slope argument applies here - its probably a bad idea to query _all_ indexes in a decently large cluster and if that is simple to make an option then its probably worth it. Anyone with any more complex set of requirements can break out nginx.

OTOH if it isn't a simple option to implement just telling people to break out nginx if they want any filtering isn't that bad. Its not like Elasticsearch _needs_ to be the one to enforce this. It'd just be convenient if it did.
</comment><comment author="avleen" created="2015-05-29T00:06:19Z" id="106634968">I would agree with Nik.
This is a pain point currently, and even with nginx in place, it is _easy_
to directly access elasticsearch (eg on localhost) and accidentally query
all indices.

Note that I'm not talking about malicious actors, but actual accidents and
those are otherwise hard to protect. It would be most appropriate I think
for this single toggle to be in elasticsearch.

On Thu, May 28, 2015, 12:26 Nik Everett notifications@github.com wrote:

&gt; It seems simple to add, but then everybody has a different policy that
&gt; they want to enforce, so a tiny feature becomes complex.
&gt; 
&gt; I'm not sure the slippery slope argument applies here - its probably a bad
&gt; idea to query _all_ indexes in a decently large cluster and if that is
&gt; simple to make an option then its probably worth it. Anyone with any more
&gt; complex set of requirements can break out nginx.
&gt; 
&gt; OTOH if it isn't a simple option to implement just telling people to break
&gt; out nginx if they want any filtering isn't that bad. Its not like
&gt; Elasticsearch _needs_ to be the one to enforce this. It'd just be
&gt; convenient if it did.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/6470#issuecomment-106572409
&gt; .
</comment><comment author="tomryanx" created="2015-05-29T09:23:26Z" id="106755209">I find it sickening that it's not possible to limit ES to _only_ being reachable through nginx (or something else which can control access in complex ways), even on localhost, but that being the case I have to say I find the answer "use nginx for that", even for something as simple as preventing _all indices being queried, to be consistent and therefore satisfactory.

Side note: if there is a way to make ES require a specific header (/token/anything), to make it more difficult or cumbersome to access without eg nginx in the middle, I'd love to hear about it.
</comment><comment author="jpountz" created="2016-08-24T15:22:57Z" id="242103972">Could it be considered superseded by the soft limit on the number of queried shards at once? #17396
</comment><comment author="clintongormley" created="2016-08-25T10:07:49Z" id="242339002">@jpountz i think the soft limit does the job better than disabling querying on all indices - it get to the root of the problem
</comment><comment author="aides" created="2017-07-04T13:51:15Z" id="312883424">Hello, 

I have the following case for disabling querying on all indices. I have a multi-tenant ElasticSearch cluster, where tenant1 data is in "index1" and tenant2 data is in "index2". There are two instances of client applications for tenant1 and tenant2 respectively. 

It is strictly forbidden to mix search results for these tenants, i.e. if people of tenant1 see some data that doesn't belong them and find out it is some other tenant data they will go crazy about that.

So right now all search queries are written so they specify the exact index the need to search against. But it happened a couple of times that someone forgot to specify index and some queries would return mixed data. Luckily this was caught early enough. 

So I'm thinking having some switch that just prevents searching against all indices makes a lot of sense due to the above scenario. 

P.S. If there is another way of tenant bulletproof isolation within a cluster I would really like to know it. Thanks!</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure to recover from 1-way network partition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6469</link><project id="" key="" /><description>After a 1-way network partition scenario, two nodes might be stuck in different opinions of which nodes are currently in the cluster.

I.e. with 5 nodes:
Node 1 sees node 1, 3 and 5. This is a RED cluster. 
Node 2 sees node 1, 2, 4 and 5. This is a GREEN cluster.
All of this according to the `nodes` endpoint.

The cluster is _stuck in this state_, even when every node could reach every other node (verified with curl on the es http port and binary port).

Some of these nodes give an NPE for the `_status` endpoint:
`{"error":"NullPointerException[null]","status":500}`

All nodes are configured to discover all the others through unicast.
Discovered on ES 0.90.13 on RHEL 6.5.
</description><key id="35500683">6469</key><summary>Failure to recover from 1-way network partition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">magnhaug</reporter><labels /><created>2014-06-11T16:22:02Z</created><updated>2015-01-26T08:54:59Z</updated><resolved>2015-01-26T08:54:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-16T09:43:11Z" id="46159135">it sounds like a similar issue to issue #2488, which we are working on.  I assume this one is different due to the asymmetric nature of the network partition. But in essence it is the same as you have two masters in your cluster (correct?).

&gt; The cluster is stuck in this state, even when every node could reach every other node (verified with curl on the es http port and binary port).

Once a cluster has formed, ES will verify the all the nodes of the cluster are active (via pinging) but it will not consistently look for new nodes. The assumption is that if a new node comes up, it will actively join the cluster. The down side is that if you end up with two stable "sub" cluster - they will not discover each other.

&gt; Some of these nodes give an NPE for the _status endpoint:
&gt; {"error":"NullPointerException[null]","status":500}

Do you have a stack trace in your ES logs?
</comment><comment author="magnhaug" created="2014-06-16T10:26:34Z" id="46162727">Yes, the issue looks similar to #2488, it might be wise to use this as another test case on the improved_zen branch.
As far as I remember, we saw two master nodes in the two different clusters. I cannot verify, as we had to reboot the cluster to continue other tests.

&gt; Do you have a stack trace in your ES logs?

No, unfortunately the logs were completely silent when _status failed with a NPE.
I tried to re-configure the logger to log everything from TRACE and up, but the node seemed to reload itself when I curled a new root logger level to it(?), which cleared up the issue (it joined the other master).
</comment><comment author="SeanTAllen" created="2014-06-19T17:54:13Z" id="46594084">We've seen this multiple times. We've since moved to dedicated master nodes. Since that time, we haven't seen any but I'm sure the possibility still exists as a possibility.
</comment><comment author="AeroNotix" created="2014-07-09T13:33:47Z" id="48469751">Using dedicated master nodes _is not a fix!!!!!!111!!!_

What dedicated master nodes does is lighten the load for those nodes which are the masters and therefore makes the likelihood of a split _less likely_.
</comment><comment author="bleskes" created="2015-01-26T08:54:59Z" id="71430000">I'm going to close this assuming it is indeed a duplicate of #2488. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Silently hangs when not able to connect to node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6468</link><project id="" key="" /><description>If you create a client which connects to one good node and a few bad nodes it will take a very long time to connect. Our production application server was taking several minutes to come up every time it was restarted and we couldn't figure out why for quite some time. It would be extremely helpful to log that one of the nodes is unreachable so that the issue can be addressed.

One thing that's odd is that is seems to take exponentially longer the more nodes you add.

Example for reproducing the issue:

```
TransportClient transportClient = new TransportClient(settings.build());
transportClient.addTransportAddress(new InetSocketTransportAddress("127.0.0.1", 9300));
transportClient.addTransportAddress(new InetSocketTransportAddress("11.22.33.44", 9300));
transportClient.addTransportAddress(new InetSocketTransportAddress("22.33.44.55", 9300));
transportClient.addTransportAddress(new InetSocketTransportAddress("33.44.55.66", 9300));
```

I'm using the following client:

```
"org.elasticsearch"             % "elasticsearch"             % "1.0.1"
```
</description><key id="35496932">6468</key><summary>Silently hangs when not able to connect to node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benmccann</reporter><labels><label>:Logging</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-06-11T15:43:40Z</created><updated>2016-11-26T12:18:26Z</updated><resolved>2016-11-26T12:18:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-11-21T13:37:07Z" id="158641279">@bleskes do you have any thoughts about this?
</comment><comment author="bleskes" created="2015-11-22T20:39:35Z" id="158797255">we people add nodes to the list of a client we try to connect to them and validate that they are there. The connection timeout is 45s by default. Maybe the connection hangs?

At the moment we log this under debug, because we use the standard sniffing mechanism to validate the new nodes and that one runs frequently so you don't want to spam the logs:

```
 logger.debug("failed to connect to node [{}], removed from nodes list", e, listedNode);
```

However, if we do succeed to connect/are already connected but fail to get a response from our (recently added) liveness ping, we log it in info:

```
logger.info("failed to get node info for {}, disconnecting...", e, listedNode);
```

Looks to me like we should report on a connection failure in info as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>uax_url_email analyzer incorrectly parses domain: ".local"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6467</link><project id="" key="" /><description>uax_url_email analyzer appears unable to recognize the .local TLD. Bug can be reproduced by

curl -XGET "ADDRESS/INDEX/_analyze?text=First%20Last%20lname@section.mycorp.local&amp;pretty&amp;analyzer=uax_url_email"

will parse "lname@section.my" and "corp.local" as separate tokens, as opposed to

curl -XGET "ADDRESS/INDEX/_analyze?text=First%20Last%20lname@section.mycorp.org&amp;pretty&amp;analyzer=uax_url_email"

which will recognize  "lname@section.mycorp.org".
</description><key id="35491272">6467</key><summary>uax_url_email analyzer incorrectly parses domain: ".local"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stevemer</reporter><labels /><created>2014-06-11T14:50:08Z</created><updated>2015-11-21T13:36:07Z</updated><resolved>2015-11-21T13:36:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="stevemer" created="2014-06-11T18:07:50Z" id="45777953">Which raises the question - what domain lists is this tool utilizing, and what other TLDs may be missing? Either way, it's causing problems on my end, so I'd appreciate if someone could look into it.
</comment><comment author="clintongormley" created="2014-06-11T18:15:56Z" id="45779015">It uses the TLDs from http://www.internic.net/zones/root.zone - last updated Dec 2013.

This is actually in Lucene, not Elasticsearch, so I would advise opening an issue here: https://issues.apache.org/jira/browse/LUCENE
</comment><comment author="stevemer" created="2014-06-11T19:59:04Z" id="45791644">Thank you!
</comment><comment author="clintongormley" created="2014-06-11T20:00:42Z" id="45791817">let's leave this issue open.  add the lucene jira when you open it, then we can track when it is resolved here.
</comment><comment author="stevemer" created="2014-06-11T21:31:54Z" id="45802879">https://issues.apache.org/jira/browse/LUCENE-5753?filter=-4
</comment><comment author="clintongormley" created="2015-11-21T13:36:07Z" id="158641208">No point in keeping this open any longer - we'll get the update when it is addressed in Lucene
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not use versions to optimize cluster state copying for a first update from a new master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6466</link><project id="" key="" /><description>We have an optimization which compares routing/meta data version of cluster states and tries to reuse the current object if the versions are equal. This can cause rare failures during recovery from a minimum_master_node breach when using the "new light rejoin" mechanism and simulated network disconnects. This happens where the current master updates it's state, doesn't manage to broadcast it to other nodes due to the disconnect and then steps down. The new master will start with a previous version and continue to update it. When the old master rejoins, the versions of it's state can equal but the content is different.

**NOTE:** this a PR for the feature/improve_zen branch
</description><key id="35488577">6466</key><summary>Do not use versions to optimize cluster state copying for a first update from a new master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-11T14:24:55Z</created><updated>2015-06-07T13:15:33Z</updated><resolved>2014-06-12T09:05:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-11T18:27:21Z" id="45780489">LGTM
</comment><comment author="bleskes" created="2014-06-12T09:05:10Z" id="45845684">Pushed in with a2ca26e1ace9a3c3e8f2946a1d45e49ea027caae
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Aggregations] Added _meta construct to inject metadata</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6465</link><project id="" key="" /><description>This PR adds the ability to associate a bit of state with each individual aggregation.

Use cases:
- Typed clients can inject some state so that the deserialization
  process has enough information to deserialize the aggregations into
  the correct type. Facets return `_type` to signal what kind of facet the
  response corresponds with.
- UI state, since aggregations can be nested at arbitrary depths
  injecting pieces of UI state can greatly simplify programming with
  aggregations since there are no two tree data-structures to walk anymore only the aggregation data.

thank you @martijnvg for the help reading the `byte[]` representation of _meta properly
- this PR also fixes the response format of the new `geo_bounds` aggregation to wrap the result in a `"aggregationname" : {}`

[Here's a gist of a sense session](https://gist.github.com/Mpdreamz/785a3347d4415e1fe6bb#file-aggs-metada) you can use to review this branch

Example request:

``` json
{
"aggs": {
    "name": {
      "terms": {
        "field": "title"
      },
      "meta": 2,
      "aggs": {
        "viewport" : {
          "meta" : {
               "complex" : "object"
          },
          "geo_bounds" : {
            "field" : "location"
          }
        },
        "empty_agg" : {
          "meta" : "should still be returned",
          "sum": {
            "field": "i_do_not_exist"
          }
        },
```

Example response:

``` json
{
   "aggregations": {
      "name": {
         "meta": 2,
         "buckets": [
            {
               "key": "banner",
               "doc_count": 1,
               "viewport": {
                  "meta": {
                    "complex": "object"
                  },
                  "bounds": {
                     "top_left": {
                        "lat": 28,
                        "lon": 21
                     },
                     "bottom_right": {
                        "lat": 28,
                        "lon": 21
                     }
                  }
               },
               "nested_terms": {
                  "meta": {
                     "complex": "object",
                     "nuber": 1
                  },
                  "buckets": [
                     {
                        "key": "a",
                        "doc_count": 1,
                        "counter": {
                           "meta": 4.1,
                           "value": 40
                        }
                     },
```
</description><key id="35483764">6465</key><summary>[Aggregations] Added _meta construct to inject metadata</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/Mpdreamz/following{/other_user}', u'events_url': u'https://api.github.com/users/Mpdreamz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/Mpdreamz/orgs', u'url': u'https://api.github.com/users/Mpdreamz', u'gists_url': u'https://api.github.com/users/Mpdreamz/gists{/gist_id}', u'html_url': u'https://github.com/Mpdreamz', u'subscriptions_url': u'https://api.github.com/users/Mpdreamz/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/245275?v=4', u'repos_url': u'https://api.github.com/users/Mpdreamz/repos', u'received_events_url': u'https://api.github.com/users/Mpdreamz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/Mpdreamz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'Mpdreamz', u'type': u'User', u'id': 245275, u'followers_url': u'https://api.github.com/users/Mpdreamz/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2014-06-11T13:38:52Z</created><updated>2015-03-19T10:19:04Z</updated><resolved>2014-10-29T21:08:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2014-06-12T18:28:50Z" id="45929733">Thanks for review @uboness ! I wrapped the `create()` because I wasn't sure if updating all the Aggregation implementation constructors would break elasticsearch's public API. 

I opted for `_meta` because adding it in the response might break existing code that uses `meta` as aggregation name. 

Will update both property names to `meta` and pass metaData in the constructors of `Aggregation` implementations.
</comment><comment author="uboness" created="2014-06-12T18:32:27Z" id="45930199">why would it break? it's under the agg name, isn't it? in any case... still... not good enough reason to introduce inconsistency in the api. That is, if we feel that something will break apis, better break them and target for a later release (one that enables breaking changes) rather than introduce inconsistency in the api (something we'll need to cary later for a long time)
</comment><comment author="Mpdreamz" created="2014-06-12T19:12:24Z" id="45935207">@uboness I've updated the PR with all your feedback I only have one question regarding the AggregationBuilder.java changes.
</comment><comment author="clintongormley" created="2014-08-22T10:16:24Z" id="53044924">@Mpdreamz where are you on this one?
</comment><comment author="Mpdreamz" created="2014-10-29T21:08:05Z" id="61005272">closing in favour of #8279
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Transport: copied over REST headers from RestRequest to TransportRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6464</link><project id="" key="" /><description>Made sure that the REST headers are copied over from each `RestRequest` to the related `TransportRequest`(s).

Introduced new base class (`BaseActionRequestRestHandler`) for all the REST actions that map to a single `TransportRequest` (most of them). Split the existing `handleRequest` method in two phases: 1) request creation 2) request execution. Between the two we can automatically copy the headers to the transport request in the base class so we don't have to do it manually all the time.

Also added a utility method to copy the headers in `BaseRestHandler`, to be used by all the actions that don't extend the newly introduced `BaseActionRequestRestHandler` (e.g. cat api as each `RestRequest` maps to multiple `TransportRequest`s).

The following changes have been made as well:
 1) split benchmark actions into different RestAction classes (submit, abort, status)
 2) modified a couple of actions that were doing some validation during transport request creation to throw `ActionRequestValidationException` instead of directly sending the error through the channel (`RestGetSourceAction` and `RestIndexAction`)
</description><key id="35479279">6464</key><summary>Transport: copied over REST headers from RestRequest to TransportRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2014-06-11T12:46:24Z</created><updated>2014-06-16T13:08:26Z</updated><resolved>2014-06-16T13:08:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T08:46:57Z" id="45844069">I looked at this and it feels wrong to me to rely on the implementer to pass on these parameters. IMO there should be no way to work around this. One clean and entirely transparent way of doing this would be wrapping the Client we are using with a `FilteredClient` and have a method on the `BaseRestHandler` or even on the interface like:

``` Java
protected Client client(RestRequest request) {
    return new HeaderCopyClient(this.client, request);
} 
```

and then override the execute methods and pass the headers on there? That way you have the headers everywhere?
</comment><comment author="javanna" created="2014-06-16T13:08:26Z" id="46175483">Agreed @s1monw , closing as this PR is replaced by #6513 which uses the approach you suggested.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory pre-allocation with size attribute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6463</link><project id="" key="" /><description>When I do a filter or query, and set the size valeu to a huge value (like 10000000), the memory consuption is increased a lot even though the final result be 10 documents.

If I do the same thing with a huge amount of users the memory is increased so fast that the nodes would become bottlenecks (sometimes a node shutdowns itself).

I solved that changing the size attribute to a low value, but I think it would be better if the ES deal with that in a better way, avoiding bad programmers to shutdown the server.

Regards.
</description><key id="35478131">6463</key><summary>Memory pre-allocation with size attribute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">marcelopaesrech</reporter><labels /><created>2014-06-11T12:31:18Z</created><updated>2014-10-17T06:51:48Z</updated><resolved>2014-10-17T06:51:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marcelopaesrech" created="2014-06-11T12:50:14Z" id="45736332">See the discussion in google groups:
https://groups.google.com/forum/?utm_medium=email&amp;utm_source=footer#!msg/elasticsearch/udDQcpN-e_Y/8qT9i5jeYo0J
</comment><comment author="jpountz" created="2014-06-13T21:25:31Z" id="46061682">Would it work for you if we allowed to configure a maximum size?
</comment><comment author="marcelopaesrech" created="2014-06-16T14:27:45Z" id="46185040">Hi @jpountz,

Thanks for your reply.

I don't know if that would be the best solution. There is already a Size parameter for query/filter.

The problem is the programmer is responsible to do that, and some times he will not understand what big size parameter in query/filter would cause in production environment.

A maximum size in the entire cluster or index would help avoiding cluster shutdown. But I think it would be better count how many documents are available and then cut off pre-allocated memory that would not be used.

Regards.
</comment><comment author="clintongormley" created="2014-10-17T06:51:48Z" id="59473017">Closing in favour of #8080 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Naming (date-)ranges within a range aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6462</link><project id="" key="" /><description>It would be really nice to have the option to **name a range**.

This is a kind of "internal name" which can be used in the UI-layer to give the range a meaningful (and localizable) title/name/description.

It is way easier to identify the range by this **name** than by the key that gets created (eg. 06-2014-05-2014; *-03-2014).

``` json
"date_range": {
  "field": "news.created",
  "format": "MM-yyyy",
  "ranges": [
    {
      "to": "now",
      "from": "now-1M",
      "name": "last_month" /* naming of the range */
    },
    {
      "to": "now-1M",
      "from": "now-2M",
      "name": "prev_month" /* naming of the range */
    },
    {
      "to": "now-3M",
      "name": "older" /* naming of the range */
    }
  ]
}
```
</description><key id="35477062">6462</key><summary>Naming (date-)ranges within a range aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geek0r</reporter><labels /><created>2014-06-11T12:15:19Z</created><updated>2014-06-11T16:46:58Z</updated><resolved>2014-06-11T16:46:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-06-11T15:18:33Z" id="45755710">you can already do that (just replace your `name` field to `key`)
</comment><comment author="geek0r" created="2014-06-11T16:46:58Z" id="45767771">Awesome! Thank you ... And now that I know this works I even can find it in the documentation ... :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Getting the error on autosuggest field </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6461</link><project id="" key="" /><description>I am creating new table with name product_search .. Inside that table i want one field as auto suggested one ( I have taken the reference of http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-completion.html one) ... I have executed the following syntax ...

// Creating the database with name db1
1) curl -XPUT 'localhost:9200/db1'

// Here you will find title_suggest field is defined as type completion.
2) curl -XPUT 'localhost:9200/db1/product_search/_mapping' -d '{"product_search":{"dynamic":"true","properties": {"_delete":{"type": "string"},"title":  {"type": "string"},"image":  {"type": "string"},"title_suggest": {"type": "completion","index_analyzer": "simple","search_analyzer": "simple","payloads": true},"is_category":{"type":"boolean","default":"false"},"description":{"type": "string"},"url":{"type": "string"},"sku":{"type": "string"},"status":{"type": "string"},"categories":  {"type":"object","dynamic":true},"attributes":  {"type":"object","dynamic":true},"price":{"type": "float"}}}}'

And also I have inserted proper data with input ,output , payload etc. But while executing the auto suggest query 

curl -X POST 'localhost:9200/db1/_suggest?pretty' -d '{"title-suggest" : {"text" : "n","completion" : {"field" : "title_suggest"}}}'

I am getting the error of
# ERROR : [Field [title_suggest] is not a completion suggest field]

I would like to know where is my mistake .. Hence I have followed right syntax from given above link.
</description><key id="35471972">6461</key><summary>Getting the error on autosuggest field </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ozataral</reporter><labels /><created>2014-06-11T10:56:01Z</created><updated>2014-06-11T11:26:08Z</updated><resolved>2014-06-11T11:26:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-11T11:26:08Z" id="45729500">Please ask questions like this in the forum.  The github issues list is for bugs and feature requests.  If you follow the steps you listed above, then it works.  I'm guessing you have another `title_suggest` field somewhere in `db1`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alternative to #6446</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6460</link><project id="" key="" /><description>While discussion pull request #6446, @javanna suggested making `HighlightQuery`'s constructor protected instead of the proposed solution. This is an implementation of that suggestion, though I prefer the original solution.
</description><key id="35466473">6460</key><summary>Alternative to #6446</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">itsadok</reporter><labels /><created>2014-06-11T09:35:00Z</created><updated>2014-06-16T06:27:52Z</updated><resolved>2014-06-11T12:29:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-11T09:44:11Z" id="45721624">Hey @itsadok thanks for sending this!
I think you just need to make `HighlightQuery` public and its constructor protected, don't think the other changes you made are needed. Makes sense?
</comment><comment author="javanna" created="2014-06-11T09:44:55Z" id="45721675">BTW you can just keep pushing to the same PR branch to update your PR, no need to send another one ;)
</comment><comment author="itsadok" created="2014-06-11T10:08:18Z" id="45723539">1. If we make the constructor `protected` without further changes, `HighlightPhase` will not compile, because it calls that constructor in lines [112 and 114](https://github.com/elasticsearch/elasticsearch/blob/4aa59aff0058b05e822592431eda4a469a6b9eef/src/main/java/org/elasticsearch/search/highlight/HighlightPhase.java#L112), so I have to make these other changes.
2. Coincidentally, these changes also solve my problem, because now nobody needs to instantiate a `HighlightQuery` object other than `HighlighterContext`. 
3. I kept the other pull request, because I'm still hopeful you'll accept that one and reject this one.

Hope that makes sense :)
</comment><comment author="javanna" created="2014-06-11T10:20:55Z" id="45724523">`protected` visibility means it will be visible from subclasses plus classes in the same package. `HighlightPhase` is in the same package, thus it should compile as far as I can see...

Looks like moving the creation of `HighlightQuery` to `HighlighterContext` would  not require its constructor to be `protected` anymore though, it could even be private if I'm not mistaken.

We have two options then:
1) make `HighlightQuery` public and its constructor `protected`, that's it
2) move the `HighlightQuery` creation to `HighlighterContext` as you did here, make its constructor `private` and add the needed parameters to the constructor

I slightly prefer option 1 as the HighlighterContext constructor has less parameters and it's clearer to me, but not a big deal...can you please pick the option that you prefer and update the PR accordingly?
</comment><comment author="itsadok" created="2014-06-11T10:44:22Z" id="45726371">Seems a made a bit of a fool out of myself... definitely should have checked before making such strong claims. 
I'll go with option 1. Before I make another blunder, should I update the PR with a `git commit --amend` and then `push -f`, or should I just add another commit? Seems silly to have two commits for this tiny change.
</comment><comment author="javanna" created="2014-06-11T11:00:10Z" id="45727485">no worries! push -f is fine in this case ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic mapping not identifying floats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6459</link><project id="" key="" /><description>It saves them as type string.. I have several values with content of:
0.000017643

But the mapping (ES 1.1.2) becomes:
"time_firstbyte":{"type":"string"
,"index":"not_analyzed"},

IMHO dynamic mapping should identify them as floats..
This means that when I try to use that field in a histrogram, I get the error: ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData
</description><key id="35462040">6459</key><summary>dynamic mapping not identifying floats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KlavsKlavsen</reporter><labels /><created>2014-06-11T08:30:19Z</created><updated>2014-06-11T08:36:10Z</updated><resolved>2014-06-11T08:36:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-11T08:36:10Z" id="45715589">```
DELETE /t 

PUT /t/t/1
{
  "foo": "1.234",
  "bar": 1.234
}

GET /t/_mapping/t
```

Gives you:

```
    {
       "t": {
          "mappings": {
             "t": {
                "properties": {
                   "bar": {
                      "type": "double"
                   },
                   "foo": {
                      "type": "string"
                   }
                }
             }
          }
       }
    }
```

I'm guessing that you're using Logstash? If so, then you need to use the `mutate` filter to convert strings to numbers: http://logstash.net/docs/1.4.1/filters/mutate#convert
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Feature Request] Allow Tiered Storage/Multiple Data Paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6458</link><project id="" key="" /><description>Hi Guys,

regarded to https://groups.google.com/forum/#!topic/logstash-users/DJgSfEbl1bQ and 
https://groups.google.com/forum/#!topic/elasticsearch/tJGJRx2xWyM there is a need for bigger environments to have the ability to store indices on different types of storage (i.e SSD and HDD paths) without the nuisance to create multiple instances of ES on 1 server (which also consumes ressources).

Cheers
</description><key id="35459800">6458</key><summary>[Feature Request] Allow Tiered Storage/Multiple Data Paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">german23</reporter><labels /><created>2014-06-11T07:53:29Z</created><updated>2014-12-30T19:42:14Z</updated><resolved>2014-12-30T19:42:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:42:14Z" id="68389810">Fixed by #9033
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It should be possible to restore an index without restoring its aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6457</link><project id="" key="" /><description /><key id="35419885">6457</key><summary>It should be possible to restore an index without restoring its aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-10T20:09:02Z</created><updated>2015-06-07T13:15:44Z</updated><resolved>2014-07-13T08:57:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-11T09:09:50Z" id="48709557">@imotov are you going to be able to get this in for 1.3?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.cache.filter.expire cannot be dynamically set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6456</link><project id="" key="" /><description>On Elasticsearch 1.2.1, running the command

```
PUT /vd-rsa-2014-06-10-17/_settings
{
    "index" : {
        "cache.filter.expire" : "5m"
    }
}
```

returns:

```
{
   "error": "ElasticsearchIllegalArgumentException[Can't update non dynamic settings[[index.cache.filter.expire]] for open indices[[vd-rsa-2014-06-10-17]]]",
   "status": 400
}
```

This should have worked, according to the documentation on the site and #874
</description><key id="35408598">6456</key><summary>index.cache.filter.expire cannot be dynamically set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ludovicc</reporter><labels /><created>2014-06-10T18:05:08Z</created><updated>2014-06-10T18:15:45Z</updated><resolved>2014-06-10T18:15:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-10T18:15:45Z" id="45650975">duplicate of #6455, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.cache.filter.expire cannot be dynamically set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6455</link><project id="" key="" /><description>On Elasticsearch 1.2.1, running the command

PUT /vd-rsa-2014-06-10-17/_settings
{
    "index" : {
        "cache.filter.expire" : "5m"
    }
}

gives mes:

{
   "error": "ElasticsearchIllegalArgumentException[Can't update non dynamic settings[[index.cache.filter.expire]] for open indices[[vd-rsa-2014-06-10-17]]]",
   "status": 400
}

This should have worked, according to the documentation on the site and #874
</description><key id="35408502">6455</key><summary>index.cache.filter.expire cannot be dynamically set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ludovicc</reporter><labels /><created>2014-06-10T18:02:29Z</created><updated>2014-07-04T12:26:39Z</updated><resolved>2014-07-04T12:26:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-10T18:15:05Z" id="45650901">actually, we no longer have the option to set only an index (or shard) level filter cache, we should fix the docs. The node level filter cache is the way to go, since it sets bounds on the node level and not per shard. This setting can be changed dynamically, but using the cluster update settings API. Side note, setting expiration on filter cache is probably not worth it, size based eviction tends to be enough.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed is/if typo in Api Conventions doc.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6454</link><project id="" key="" /><description /><key id="35403151">6454</key><summary>Fixed is/if typo in Api Conventions doc.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mattjanssen</reporter><labels><label>docs</label></labels><created>2014-06-10T17:01:17Z</created><updated>2014-06-16T16:06:30Z</updated><resolved>2014-06-16T13:46:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-16T10:12:06Z" id="46161589">Thanks for the fix @mattjanssen, may I ask you to sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) if you haven't done it yet, so we can merge this in?
</comment><comment author="mattjanssen" created="2014-06-16T13:16:50Z" id="46176312">@javanna Signed and ready.
</comment><comment author="javanna" created="2014-06-16T13:46:03Z" id="46179651">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve deletion of corrupted snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6453</link><project id="" key="" /><description>Makes it possible to delete snapshots that are missing some of the metadata files. This can happen if snapshot creation failed because repository drive ran out of disk space.

Closes #6383
</description><key id="35397117">6453</key><summary>Improve deletion of corrupted snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2014-06-10T15:59:28Z</created><updated>2014-07-01T02:14:03Z</updated><resolved>2014-07-01T02:14:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-18T19:24:23Z" id="46481639">I left some comments
</comment><comment author="imotov" created="2014-06-19T21:59:36Z" id="46622525">@s1monw thanks! I pushed an update.
</comment><comment author="s1monw" created="2014-06-27T10:55:11Z" id="47331025">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improvements to StemmerTokenFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6452</link><project id="" key="" /><description>The StemmerTokenFilter had a number of issues:
- `english` returned the slow snowball English stemmer
- `porter2` returned the snowball Porter stemmer (v1)
- `portuguese` was used twice, preventing the second version from working

Changes:
- `english` now returns the fast PorterStemmer (for indices created from v1.3.0 onwards)
- `porter2` now returns the snowball English stemmer (for indices created from v1.3.0 onwards)
- `light_english` now returns the `kstem` stemmer (`kstem` still works)
- `portuguese_rslp` returns the PortugueseStemmer
- `dutch_kp` is a synonym for `kp` 

Tests and docs updated
</description><key id="35392559">6452</key><summary>Improvements to StemmerTokenFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>breaking</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-10T15:18:04Z</created><updated>2015-06-07T10:38:38Z</updated><resolved>2014-06-11T10:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-10T15:19:52Z" id="45628011">Fixes #6345 
Fixes #6213 
Fixes #6330 
</comment><comment author="rmuir" created="2014-06-10T15:27:36Z" id="45629177">This looks great: I especially like the cleanup to the code organization and the doc links.
</comment><comment author="clintongormley" created="2014-06-10T15:50:01Z" id="45632364">@jpountz Please could you check that I'm using the index versions correctly here
</comment><comment author="jpountz" created="2014-06-11T08:59:38Z" id="45717723">@clintongormley It looks good to me. I really like the fact that we now recommend on a stemmer!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermVector GET on invalid id returns 500</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6451</link><project id="" key="" /><description>Elasticssearch 1.2.1

```
GET /index/type/thisiddoesnotexist/_termvector?fields=name
```

returns:

``` json
{
  "error" : "JsonGenerationException[Current context not an object but ROOT]",
  "status" : 500
}
```

while in `1.1.0` it returned:

``` json
{
  "_index" : "nest_test_data-8136",
  "_type" : "elasticsearchprojects",
  "_id" : "thisiddoesnotexist",
  "_version" : 0,
  "found" : false
}
```
</description><key id="35390581">6451</key><summary>TermVector GET on invalid id returns 500</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2014-06-10T14:59:18Z</created><updated>2014-08-21T08:53:01Z</updated><resolved>2014-08-21T08:53:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2014-08-21T08:53:01Z" id="52894366">Fixed by #7124
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No way of finding out which date/number/geo field matched</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6450</link><project id="" key="" /><description>If I have two date fields (date1 or date2) and do a search including both (something like date1=abc OR date2=xyz), I have no way to find out which one matched. Normally, you could use highlighting to find out which fields (and which bits of those fields) matched your query for each document. But in the case of numbers and dates, highlighting doesn't work.

It would be useful to add a way to find out which number/date fields were involved in matching your query, whether it's through the highlighting mechanism or some other way. A new element in the response that listed the fields that matched would be one way.
</description><key id="35379267">6450</key><summary>No way of finding out which date/number/geo field matched</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tstibbs</reporter><labels /><created>2014-06-10T13:02:58Z</created><updated>2015-12-07T06:23:42Z</updated><resolved>2014-06-10T16:05:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-10T16:05:47Z" id="45634546">You can assign a name to any query or filter: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html#_named_queries
</comment><comment author="tstibbs" created="2014-06-13T10:10:27Z" id="45995419">Hmm, I suppose that is one way of doing it. For every query (which could be entered by a user) I have to go through inserting the `_name` elements. Then I've got to extract from each one which fields it is that the query operates on. Then when the results come back I've got to do the lookup based on the `matched_queries` to work out which fields actually ended up matching.

And this breaks down if any of the date/number/geo queries ever change to search over multiple fields, because the fact that a particular query matched wouldn't actually tell you which field matched.
</comment><comment author="tstibbs" created="2014-06-16T08:40:32Z" id="46153642">Actually, this is really tricky. I'm not sure that the suggestion of using named queries really solves the problem.

My use case is:
- Clients (could be human users or remote systems using my webservice) send an elasticsearch query to my application
- My app queries elasticsearch
- looks at the highlights in the response, and does a whole bunch of processing to enrich the results
- Returns the enriched results to the users in a more domain-appropriate format than elasticsearch's response format

So, my application will need to look at the list of matched queries to enrich the results in the same way as it currently does for highlights. Therefore, I can't expect the clients to insert the names of the queries appropriately, my application needs to do it.

But inserting the names into a query is hard. Below are two example queries that show why:

```
{
  "query": {
    "range": {   &lt;-- this is the query name
      "match": {   &lt;-- this is the field name
        "gt": 4,
        "lt": 5
      }
    }
  }
}
```

```
{
  "query": {
    "match": {   &lt;-- this is the query name
      "range": {   &lt;-- this is the field name
        "query": "blah"
      }
    }
  }
}
```

In both cases it's going to be hard to spot where the query is, especially as it could be nested deep within many layers of bool, boosting, constant score or nested queries/filters. You can't simply look for the string "range", you've got to know more about the structure. You'll end up coding a bunch of code that knows about what queries can be nested in which other queries, and understands the format of each query. At which point you're building a query parser (which elasticsearch already has...)
</comment><comment author="tstibbs" created="2014-07-15T13:38:54Z" id="49032258">The proposed solution here is further complicated by this issue: https://github.com/elasticsearch/elasticsearch/issues/6871
</comment><comment author="asldevi" created="2015-12-07T06:23:42Z" id="162422467">This has hit me too. Not sure why this is closed. 
@tstibbs, could you find a work around other than using named queries? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Minimium_should_match doesn't work with simple_query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6449</link><project id="" key="" /><description>When I add a minimum_should_match field to my simple_query_string object, it returns an error about an unsupported field.

The error:

```
QueryParsingException[[owlin-production] [simple_query_string] unsupported field [minimum_should_match]]; }]","status":400}
```

The query:

```
{
    simple_query_string: {
        fields: [
            'header',
            'description'
        ],
        query: 'foo',
        minimum_should_match: 2,
        default_operator: 'OR'
    }
}
```
</description><key id="35376314">6449</key><summary>Minimium_should_match doesn't work with simple_query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">riichard</reporter><labels /><created>2014-06-10T12:23:20Z</created><updated>2015-02-25T19:05:09Z</updated><resolved>2015-02-25T19:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Coming -&gt; Added in doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6448</link><project id="" key="" /><description /><key id="35374448">6448</key><summary>Coming -&gt; Added in doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephlag</reporter><labels /><created>2014-06-10T11:55:26Z</created><updated>2014-06-13T14:01:46Z</updated><resolved>2014-06-13T14:01:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T07:52:42Z" id="45839466">thanks for raising this can you sign the CLA otherwise I can't pull it in :)
</comment><comment author="stephlag" created="2014-06-12T10:25:14Z" id="45862373">I already signed it...
</comment><comment author="clintongormley" created="2014-06-13T14:01:46Z" id="46014437">thanks @stephlag 

merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update nested-aggregation.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6447</link><project id="" key="" /><description>Just a typo in the json request, coma missing.
</description><key id="35369801">6447</key><summary>Update nested-aggregation.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">acerb</reporter><labels><label>docs</label></labels><created>2014-06-10T10:41:03Z</created><updated>2014-06-18T17:38:33Z</updated><resolved>2014-06-18T17:38:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-16T10:11:30Z" id="46161535">Thanks for the fix @acerb , may I ask you to sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) if you haven't done it yet, so we can merge this in?
</comment><comment author="acerb" created="2014-06-18T16:25:45Z" id="46459001">Hi Luca,

I just did it.
</comment><comment author="javanna" created="2014-06-18T17:38:33Z" id="46468047">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the HighlightQuery class public</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6446</link><project id="" key="" /><description>We're using the highlighter directly in our code, and the introduction of the package-scoped HighlightQuery in 4aa59aff0058b05e822592431eda4a469a6b9eef has made this impossible without building our own little fork.
</description><key id="35362591">6446</key><summary>Make the HighlightQuery class public</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">itsadok</reporter><labels><label>:Highlighting</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-10T08:56:25Z</created><updated>2015-06-07T13:15:55Z</updated><resolved>2014-06-11T15:32:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-11T15:32:14Z" id="45757558">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Build: Generate source jars for tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6445</link><project id="" key="" /><description>Closes #6125
</description><key id="35337646">6445</key><summary>Build: Generate source jars for tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>build</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-10T00:02:27Z</created><updated>2015-06-07T13:16:05Z</updated><resolved>2014-06-12T10:07:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-12T10:07:00Z" id="45850867">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>context suggester not working in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6444</link><project id="" key="" /><description>running the example given in the documentation is throwing an error:

{
   "_shards": {
      "total": 5,
      "successful": 4,
      "failed": 1,
      "failures": [
         {
            "index": "services",
            "shard": 2,
            "reason": "BroadcastShardOperationFailedException[[services][2] ]; nested: ElasticsearchException[failed to execute suggest]; nested: NullPointerException; "
         }
      ]
   }
}

here's the code example given:

PUT /services

PUT /services/service/_mapping
{
    "service": {
        "properties": {
            "name": {
                "type" : "string"
            },
            "tag": {
                "type" : "string"
            },
            "suggest_field": {
                "type": "completion",
                "context": {
                    "color": { 
                        "type": "category",
                        "path": "color_field",
                        "default": ["red", "green", "blue"]
                    },
                    "location": { 
                        "type": "geo",
                        "precision": "5m",
                        "neighbors": true,
                        "default": "u33"
                    }
                }
            }
        }
    }
}

PUT services/service/1
{
    "name": "knapsack",
    "suggest_field": {
        "input": ["knacksack", "backpack", "daypack"],
        "context": {
            "color": ["red", "yellow"]
        }
    }
}

POST services/_suggest?pretty'
{
    "suggest" : {
        "text" : "m",
        "completion" : {
            "field" : "suggest_field",
            "size": 10,
            "context": {
                "color": "red"
            }
        }
    }
}
</description><key id="35329122">6444</key><summary>context suggester not working in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">bigerock</reporter><labels /><created>2014-06-09T22:00:11Z</created><updated>2014-12-11T20:36:03Z</updated><resolved>2014-12-11T20:36:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-10T09:42:43Z" id="45593527">This code https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java#L300 expects to receive a query for each context, but doesn't have a default value if a context is missing, or doesn't throw an error if a context is missing.
</comment><comment author="clintongormley" created="2014-06-10T09:43:35Z" id="45593597">I assume that contexts are required? No way to handle the "all" case for a particular context? 
</comment><comment author="bigerock" created="2014-06-10T15:46:54Z" id="45631939">are you saying it needs a query? i'm not following.
</comment><comment author="spinscale" created="2014-06-16T06:23:20Z" id="46144506">This looks like a bug indeed, as the mapping allows you to specify defaults, you should be able to so specify something like `"location": {}` in your query to make sure it uses the default location in the mapping. Alternatively an empty context in a query should fallback to the defaults and not raise NPEs

```
POST services/_suggest?pretty'
{
"suggest" : {
"text" : "m",
"completion" : {
"field" : "suggest_field",
"size": 10,
"context": {
}
}
}
}
```
</comment><comment author="benjamin-smith" created="2014-06-17T22:18:17Z" id="46373139">I had a very similar issue, and passing in an empty object for any context items that I did not want to specify got rid of the NPE error.
</comment><comment author="shoteff" created="2014-08-06T08:31:19Z" id="51307147">I also think this should be considered a bug.
According to the ES documentation for the category query:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/suggester-context.html#_category_query

&gt; A query within a category works similar to the configuration. If the value is null the mappings default categories will be used. Otherwise the suggestion takes place for all documents that have at least one category in common with the query.

It works if the context value is explicitly set to _null_, but it doesn't work if the context value is not provided at all (even if there is default value in the mapping)

This works:

```
{
  "suggest": {
    "text": "a",
    "completion": {
      "field": "my_autocomplete",
      "size": 10,
      "context": {
        "_type" : "tweet",
        "additional_name": null
      }
    }
  }
}
```

And this doesn't:

```
{
  "suggest": {
    "text": "a",
    "completion": {
      "field": "my_autocomplete",
      "size": 10,
      "context": {
        "_type" : "tweet"
      }
    }
  }
}
```
</comment><comment author="missinglink" created="2014-11-25T14:43:06Z" id="64408912">FWIW I agree that this functionality should be changed / fixed.

### background

ES allows you to define [completion suggesters](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters-completion.html), defined by `"type": "completion"` in your mapping, which contain [context suggesters](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/suggester-context.html) which can be either 'category' or 'geo'. These provide a means of reducing the documents returned by the suggester for the given input text.

You can optionally provide a `default` value for each context suggester, which is only used; at index time; when the document you send to be indexed is missing a value for that context.

### the problem

**All** of the defined contexts **must** be specified when sending a query to `/_suggest`. Failing to do so will result in the cryptic error `ElasticsearchException[failed to execute suggest]; nested: NullPointerException;`. ..as per @spinscale comment

Some people above commented about the `default` value, which has nothing to do with the query, don't confuse this with **omitting the context in the query** which is what I believe everyone really wants.

@shoteff if you read that passage again it refers to index time, not to query time.

### use-cases

I'll give you my use-case to give some context about how I would like to use it.

We are indexing 50M places of interest around the globe, and for each I would like to have 1x 'category' context for its 'class' (think restaurant/bar/cafe) in addition to the 1x 'geo' context used to target geo-relevant documents.

Who doesn't want autocomplete for all the bars near their house? am I right?

Problem is.. once I introduce the 'class' context I can no longer query for 'all' documents in the UK.
I am now forced to specify a 'class' on every single request, or worse, send a list of _every_ class with every query. Sending 'key:null' as per previous comments does not remedy this issue.

FWIW: The problem defined above also applies to using the 'geo' context on it's own and not specifying a lat/lon at query time, which means you can never query at `precision:0`, and can never have autocomplete for the whole globe.

## test-case / game

I put together a testcase for you to explain what's going on, it's the game 'Guess Who?' which you should all remember from your youth. I hope you enjoy @clintongormley, @markharwood  ;)

``` bash
#!/bin/bash

################################################
# Let's play Guess Who? in Elasticsearch!!
################################################

ES='localhost:9200';

# drop index
curl -XDELETE "$ES/guesswho?pretty=true" 2&gt;/dev/null;

# create index
curl -XPUT "$ES/guesswho?pretty=true" -d'
{ 
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0
    }
  },
  "mappings": {
    "character": {
      "properties": {
        "name": { "type": "string" },
        "sex": { "type": "string" },
        "hair": { "type": "string" },
        "eyes": { "type": "string" },
        "wears": { "type": "string" },
        "guess": {
          "type": "completion",
          "payloads": "false",
          "preserve_separators": "false",
          "preserve_position_increments": "false",
          "max_input_length": "50",
          "context": {
            "sex": {
              "type": "category",
              "path": "sex",
              "default": "unknown"
            },
            "hair": {
              "type": "category",
              "path": "hair",
              "default": "unknown"
            },
            "eyes": {
              "type": "category",
              "path": "eyes",
              "default": "unknown"
            },
            "wears": {
              "type": "category",
              "path": "wears",
              "default": "unknown"
            }
          }
        }
      }
    }
  }
}';

# Index some characters
index_character() {
  curl -XPOST "$ES/guesswho/character/$1?pretty=true" -d'
  {
    "name":"'"$1"'",
    "sex":"'"$2"'",
    "hair":"'"$3"'",
    "eyes":"'"$4"'",
    "wears":"'"$5"'",
    "guess": {
      "input": [ "'"$1"'" ],
      "output": "'"$1 is $2 with $3 hair, $4 eyes and wears $5"'"
    }
  }';
}

# src: http://www.buzzfeed.com/kenblom/the-game-of-guess-who-and-their-celebrity-dopplgangers
index_character "bernard" "male" "red" "red" "a fedora";
index_character "anne" "female" "black" "red" "earrings";
index_character "eric" "male" "blond" "yellow" "a sailor hat";
index_character "george" "male" "grey" "red" "a detective hat";
index_character "herman" "male" "blond" "red" "nothing";
index_character "alfred" "male" "blond" "blue" "frown";
index_character "mystery" "unknown" "unknown" "unknown" "unknown";

# Refresh index
curl -XPOST "$ES/guesswho/_refresh";

# The fun is trying to guess when you play Guess Who?
guess_character_without_context() {
  curl -XPOST "$ES/guesswho/_suggest?pretty=true" -d'
  {
    "guesswho": {
      "text":"'"$1"'",
      "completion": {
        "size": "10",
        "field": "guess"
      }
    }
  }';
}

guess_character_by_sex() {
  curl -XPOST "$ES/guesswho/_suggest?pretty=true" -d'
  {
    "guesswho": {
      "text":"'"$1"'",
      "completion": {
        "size": "10",
        "field": "guess",
        "context": {
          "sex":"'"$2"'"
        }
      }
    }
  }';
}

guess_character_by_specifying_everything() {
  curl -XPOST "$ES/guesswho/_suggest?pretty=true" -d'
  {
    "guesswho": {
      "text":"'"$1"'",
      "completion": {
        "size": "10",
        "field": "guess",
        "context": {
          "sex":"'"$2"'",
          "hair":"'"$3"'",
          "eyes":"'"$4"'",
          "wears":"'"$5"'"
        }
      }
    }
  }';
}

guess_character_without_context "a";
# ElasticsearchIllegalArgumentException[suggester [completion] requires context to be setup]

guess_character_by_sex "a" "male";
# ElasticsearchException[failed to execute suggest]; nested: NullPointerException;

guess_character_by_specifying_everything "a" "male" "blond" "blue" "frown";
# I win, play again?
```
</comment><comment author="shoteff" created="2014-11-25T15:13:04Z" id="64413957">@missinglink thank you, I think I messed up the index and query time stuff, but fortunately you got the idea :)
</comment><comment author="missinglink" created="2014-11-26T08:28:57Z" id="64529567">cc/ @chilling @areek
</comment><comment author="sschuerz" created="2014-11-27T14:54:46Z" id="64799312">So what is the value that I have to set if I _don't_ want restrict the context for a particular field in "context"? Using null only gives me the documents that really have null for this field. Using an empty object results in a NullPointerException. Does anyone have a solution for this problem?
I use elasticsearch 1.4.
</comment><comment author="missinglink" created="2014-11-27T15:24:26Z" id="64802783">Hey @sschuerz there are currently no 'solutions' for this issue, it will require a change to the codebase to fix.

Two absolute hacks come to mind; which I feel dirty for mentioning, but I will anyway.
1. add `'*'` (or any other token) to _every_ 'category' context for _every_ record and then use that as your catchall.
2. somehow collect a list of _all_ possible values of a context and send the entire list with _every_ request to `/_suggest`.

They are both awful hacks but should work, I am currently considering whether to consider consideration of them myself.

If you are using the 'geo' context then you're unfortunately out of luck, I don't think there is a hack to allow you to query the whole planet.
</comment><comment author="shoteff" created="2014-11-27T15:30:23Z" id="64803682">Actually I'm using the 1-st scenario, using a script I'm adding '*' to the list of the categories, so I can always have it as catchall
</comment><comment author="missinglink" created="2014-11-27T15:34:26Z" id="64804121">This is not an acceptable solution to this issue, but I appreciate that we both came up with the same hacky workaround :)
</comment><comment author="sschuerz" created="2014-11-27T15:46:38Z" id="64805533">Thank you for the responses, @missinglink and @shoteff. I think I'll use the first scenario because collecting a list is very unhandy in my context.
@shoteff: Could you please provide an example for such a script?
</comment><comment author="missinglink" created="2014-11-27T16:11:31Z" id="64808177">I think what @shoteff is referring to is below, which I would like to re-iterate is a hack for the following 3 reasons:
- still requires you to know and specify all contexts in every query, plus know the magic token.
- extra overhead at index time, prone to error
- potentially fills the FST with bloat (may be negligible due to the data-structure)

``` bash
index_character() {
  curl -XPOST "$ES/guesswho/character/$1?pretty=true" -d'
  {
    "name":["*","'"$1"'"],
    "sex":["*","'"$2"'"],
    "hair":["*","'"$3"'"],
    "eyes":["*","'"$4"'"],
    "wears":["*","'"$5"'"],
    "guess": {
      "input": [ "'"$1"'" ],
      "output": "'"$1 is $2 with $3 hair, $4 eyes and wears $5"'"
    }
  }';
}
```

``` bash
guess_character_by_hack() {
  curl -XPOST "$ES/guesswho/_suggest?pretty=true" -d'
  {
    "guesswho": {
      "text":"'"$1"'",
      "completion": {
        "size": "10",
        "field": "guess",
        "context": {
          "sex":"'"$2"'",
          "hair":"*",
          "eyes":"*",
          "wears":"*"
        }
      }
    }
  }';
}
```
</comment><comment author="missinglink" created="2014-11-27T16:46:02Z" id="64811836">My java is terrible, the `java.lang.NullPointerException` seems to be caused here: https://github.com/elasticsearch/elasticsearch/blob/8aebb9656b63148190076bd8cdc5be0e87a9923d/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java#L286

more specifically..
https://github.com/elasticsearch/elasticsearch/blob/8aebb9656b63148190076bd8cdc5be0e87a9923d/src/main/java/org/elasticsearch/search/suggest/context/ContextMapping.java#L303-L306

more input from someone who knows more would be very welcome, it seems like an easy fix @spinscale?

``` bash
[2014-11-27 17:42:57,161][DEBUG][action.suggest           ] [Nobilus] [guesswho][0], node[pPxp4aPKSyW9GDmX39YF3Q], [P], s[STARTED]: failed to executed [[[guesswho]], suggestSource[
  {
    "guesswho": {
      "text":"a",
      "completion": {
        "size": "10",
        "field": "guess",
        "context": {
          "sex":"male"
        }
      }
    }
  }]]
org.elasticsearch.ElasticsearchException: failed to execute suggest
    at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:166)
    at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:61)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.search.suggest.context.ContextMapping$ContextQuery.toAutomaton(ContextMapping.java:260)
    at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$2.getLookup(AnalyzingCompletionLookupProvider.java:270)
    at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$CompletionTerms.getLookup(Completion090PostingsFormat.java:297)
    at org.elasticsearch.search.suggest.completion.CompletionSuggester.innerExecute(CompletionSuggester.java:70)
    at org.elasticsearch.search.suggest.completion.CompletionSuggester.innerExecute(CompletionSuggester.java:45)
    at org.elasticsearch.search.suggest.Suggester.execute(Suggester.java:42)
    at org.elasticsearch.search.suggest.SuggestPhase.execute(SuggestPhase.java:85)
    at org.elasticsearch.action.suggest.TransportSuggestAction.shardOperation(TransportSuggestAction.java:161)
    ... 5 more
```
</comment><comment author="shoteff" created="2014-11-27T17:03:43Z" id="64813600">I'm using the following groovy script to add "*" to all the fields that are contexts:

``` javascript
if (![ctx._source.contextFieldName].contains('*')) ctx._source.contextFieldName = 
[ctx._source.contextFieldName] + '*'
```

Replace _contextFieldName_ with your context field
As an additional hack I'm also storing all the context field names in the __meta_ special field, so I can easily read them programatically (in my case the end user should be able to define them when creating the mapping)
</comment><comment author="sschuerz" created="2014-11-28T08:32:12Z" id="64867846">Thank you very much @shoteff, the script works for me. However, this is a hacky solution and fixing the underlying problem in elasticsearch would be highly appreciated.
</comment><comment author="areek" created="2014-12-11T20:36:03Z" id="66684203">closing in favour of https://github.com/elasticsearch/elasticsearch/issues/8909
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Force refresh when versionMap is using too much RAM</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6443</link><project id="" key="" /><description>If the user sets a high refresh interval, the versionMap can use
unbounded RAM.  I fixed LiveVersionMap to track its RAM used, and
trigger refresh if it's &gt; 25% of IW's RAM buffer.  (We could add
another setting for this but we have so many settings already?).

I also fixed deletes to prune every index.gc_deletes/4 msec, and I
only save a delete tombstone if index.gc_deletes &gt; 0.

I think we could expose the RAM used by versionMap somewhere
(Marvel?  _cat?), but we can do that separately ... I put a TODO.

Closes #6378
</description><key id="35325702">6443</key><summary>Force refresh when versionMap is using too much RAM</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-09T21:19:48Z</created><updated>2015-06-07T19:46:16Z</updated><resolved>2014-07-08T16:07:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-06-10T16:58:47Z" id="45641509">OK I folded in all the feedback here (thank you!), and added two new
tests.

I reworked how deletes are handled, so that they are now included in
the versionMap.current/old but also added to a separate tombstones
map so that we can prune that map separately from using refresh to
free up RAM.  I think the logic is simpler now.
</comment><comment author="s1monw" created="2014-06-12T12:30:49Z" id="45885155">+1 to expose the RAM usage via an API. Can you please open an issue to do that? We might think further here and see how much RAM IW is using per shard as well, the DWFlushControl expose this to the FlushPolicy already so we might want to expose that via the IW API?
</comment><comment author="mikemccand" created="2014-06-12T13:34:57Z" id="45891575">I opened #6483 to expose the RAM usage via ShardStats and indices cat API...
</comment><comment author="bleskes" created="2014-06-18T12:34:25Z" id="46429337">I made another round - it looks good! 

I think we should also add a test to validate behave when enableGcDeletes is false - maybe have index.gc_deletes set to 0ms and disable enableGcDeletes and see delete stick around..
</comment><comment author="mikemccand" created="2014-06-19T13:40:09Z" id="46561043">OK I folded in all feedback I think!

However I still have concerns about how we calculate the RAM usage ... I put two nocommits about it, but these are relatively minor issues and shouldn't hold up committing this if we can't think of a simple way to address them.

Also, I want to do some performance testing here in the updates case: net/net this will put somewhat more pressure on the bloom filters / terms dict for the PK lookups since the version map acts like a cache now since the last flush, whereas with this change it only has updates since the last refresh.  Maybe not a big deal in practice ... only for updates to very recently indexed docs.
</comment><comment author="mikemccand" created="2014-06-19T13:58:59Z" id="46563309">OK performance looks fine; I ran a test doing 75% adds and 25% updates (though, not biased for recency) using random UUID and there was no clear change...
</comment><comment author="mikemccand" created="2014-06-20T16:20:01Z" id="46697517">OK I resolved the two nocommits about ramBytesUsed; I think this is ready now.
</comment><comment author="mikemccand" created="2014-06-22T10:15:48Z" id="46777154">OK I pushed another iteration, trying to improve the RAM accounting using an idea from @bleskes to shift accounting of BytesRef/VersionValue back from the tombstones to current when a tombstone is removed.  I also moved the forced pruning of tombtones to commit, and now call maybePruneDeletedTombstones from refresh.
</comment><comment author="s1monw" created="2014-06-26T08:15:57Z" id="47199059">LGTM
</comment><comment author="mikemccand" created="2014-06-28T09:18:15Z" id="47422507">I think along with this, we can go back to Integer.MAX_VALUE default for index.translog.flush_threshold_ops.... I'll commit that.
</comment><comment author="s1monw" created="2014-06-29T10:53:11Z" id="47451283">@mikemccand can we make the move to `INT_MAX` a separate issue?
</comment><comment author="s1monw" created="2014-07-02T07:39:41Z" id="47745814">I think this is ready, mike if you want another review put the review label back pls
</comment><comment author="mikemccand" created="2014-07-04T10:11:23Z" id="48027962">Thanks Simon, I think it's ready too.  I put xlog flushing back to 5000 ops ... I'll commit this soon.
</comment><comment author="s1monw" created="2014-07-04T10:25:12Z" id="48028956">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Null geopoint should not throw an exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6442</link><project id="" key="" /><description>Refers to: https://github.com/elasticsearch/elasticsearch/issues/5390 and https://gist.github.com/hkorte/9936192

While upgrading to 1.2.1 we have run into this issue.

We use model objects that we serialize into JSON. One of the fields is a geopoint. Sometimes we don't have the lat/lon yet, so we were leaving the field null until we do have the data, then we update the document. 

I can understand that a location that is missing lat or missing lon would be invalid; this warrants an exception. But null or empty is not invalid, nor is it reason to throw an exception. 

Since, according to the Gist above there's is no longer a way to leave the geopoint null you are forcing me to write a custom JSON serializer to exclude the field when the geo data is not yet available. This is considerably more work/maintenance for myself and everyone else who encounters this. I don't see a good reason for this.

I suggest that Elasticsearch should handle null and empty gracefully by ignoring/skipping the field instead of throwing an exception. Null and empty aren't invalid values. They simply mean there's no value for this field at this time.

---

Update: Later on after writing this I realized I can use `@JsonInclude(JsonInclude.Include.NON_NULL)` to exclude null fields from my JSON. However, I still think my above points are valid. 

Thanks, Matt
</description><key id="35296791">6442</key><summary>Null geopoint should not throw an exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">MattFriedmanAfilias</reporter><labels><label>bug</label></labels><created>2014-06-09T15:55:33Z</created><updated>2014-12-17T11:39:57Z</updated><resolved>2014-09-06T15:05:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-02T11:06:16Z" id="54136798">I have tested this on both master and 1.x branches and both exhibit the same behaviour:
- Trying to index a document with `"location": null` does not error correctly indexes the document.
-  Trying to index a document with "location":{} errors with:

```
{
   "error": "MapperParsingException[failed to parse]; nested: ElasticsearchParseException[field [lat] missing]; ",
   "status": 400
}
```

I think this is ok behaviour as we gracefully deal with the null case and interpret it as 'there is no data yet' which I think is as a user expects.  The empty object case feels to me to be an error case and should throw as error as it does now still I think its correctly interpreted as 'there is some data to index but it is malformed'.

@clintongormley thoughts?
</comment><comment author="clintongormley" created="2014-09-06T14:59:26Z" id="54714598">@colings86 i agree
</comment><comment author="clintongormley" created="2014-09-06T15:05:21Z" id="54714751">It looks like there was a bug in null handling in geopoints in 1.1.0 which has subsequently been fixed, so I'm going to close this issue.
</comment><comment author="goooooouwa" created="2014-12-15T05:47:43Z" id="66952404">Hi,
I'm just wandering how ES will treat document with `"location": null` when sorting by distance?
It returns a giant number as sort value in my case(  I'm using ES 1.3.2). I'm wandering if this is desired.

``` javascript
// example of sort by distance on document with "location": null
// search request body

{
  "query": {
    "match_all": {}
  },
    "sort" : [
    {
      "_geo_distance" : {
        "location" : "-70, 40",
        "order" : "asc",
        "unit" : "km",
        "mode" : "min",
        "distance_type" : "sloppy_arc"
      }
    }
  ]
}

// response
{
  "_index": ...,
    "_type": ...,
    "_id": "1",
    "_score": null,
    "_source": {
      "id": 1,
      "name": ...,
    },  
    "sort": [
      1.7976931348623157e+308
      ]   
}
```
</comment><comment author="clintongormley" created="2014-12-15T16:47:00Z" id="67024252">Hi @goooooouwa - you can control whether a missing value is sorted first or last: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_missing_values
</comment><comment author="goooooouwa" created="2014-12-16T03:08:32Z" id="67106132">Hi @clintongormley , thanks for reply.
I'm not sure if I use "missing value" correctly, but when I search like this:

``` javascript
// example of sort by distance on document with "location": null and setting "missing" as "_last"
// search request body

{
  "query": {
    "match_all": {}
  },
    "sort" : [
    {
      "_geo_distance" : {
        "missing": "_last",
        "location" : "-70, 40",
        "order" : "asc",
        "unit" : "km",
        "mode" : "min",
        "distance_type" : "sloppy_arc"
      }
    }
  ]
}
```

I get the following error

``` javascript
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[fpzi7BmbQGC1HZBaNQK8iQ][constr_base-companies][3]: SearchParseException[[constr_base-companies][3]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"match_all\": {}\n  },\n    \"sort\" : [\n    {\n      \"_geo_distance\" : {\n          \"missing\": \"_last\",\n        \"lat_lon\" : \"-40,70\",\n        \"order\" : \"asc\",\n        \"unit\" : \"km\",\n        \"mode\" : \"min\",\n        \"distance_type\" : \"sloppy_arc\"\n      }\n    }\n  ]\n}\n]]]; nested: ElasticsearchIllegalArgumentException[the character '_' is not a valid geohash character]; }{[fpzi7BmbQGC1HZBaNQK8iQ][constr_base-companies][4]: SearchParseException[[constr_base-companies][4]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"match_all\": {}\n  },\n    \"sort\" : [\n    {\n      \"_geo_distance\" : {\n          \"missing\": \"_last\",\n        \"lat_lon\" : \"-40,70\",\n        \"order\" : \"asc\",\n        \"unit\" : \"km\",\n        \"mode\" : \"min\",\n        \"distance_type\" : \"sloppy_arc\"\n      }\n    }\n  ]\n}\n]]]; nested: ElasticsearchIllegalArgumentException[the character '_' is not a valid geohash character]; }{[fpzi7BmbQGC1HZBaNQK8iQ][constr_base-companies][1]: SearchParseException[[constr_base-companies][1]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"match_all\": {}\n  },\n    \"sort\" : [\n    {\n      \"_geo_distance\" : {\n          \"missing\": \"_last\",\n        \"lat_lon\" : \"-40,70\",\n        \"order\" : \"asc\",\n        \"unit\" : \"km\",\n        \"mode\" : \"min\",\n        \"distance_type\" : \"sloppy_arc\"\n      }\n    }\n  ]\n}\n]]]; nested: ElasticsearchIllegalArgumentException[the character '_' is not a valid geohash character]; }{[fpzi7BmbQGC1HZBaNQK8iQ][constr_base-companies][2]: SearchParseException[[constr_base-companies][2]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"match_all\": {}\n  },\n    \"sort\" : [\n    {\n      \"_geo_distance\" : {\n          \"missing\": \"_last\",\n        \"lat_lon\" : \"-40,70\",\n        \"order\" : \"asc\",\n        \"unit\" : \"km\",\n        \"mode\" : \"min\",\n        \"distance_type\" : \"sloppy_arc\"\n      }\n    }\n  ]\n}\n]]]; nested: ElasticsearchIllegalArgumentException[the character '_' is not a valid geohash character]; }{[fpzi7BmbQGC1HZBaNQK8iQ][constr_base-companies][0]: SearchParseException[[constr_base-companies][0]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n  \"query\": {\n    \"match_all\": {}\n  },\n    \"sort\" : [\n    {\n      \"_geo_distance\" : {\n          \"missing\": \"_last\",\n        \"lat_lon\" : \"-40,70\",\n        \"order\" : \"asc\",\n        \"unit\" : \"km\",\n        \"mode\" : \"min\",\n        \"distance_type\" : \"sloppy_arc\"\n      }\n    }\n  ]\n}\n]]]; nested: ElasticsearchIllegalArgumentException[the character '_' is not a valid geohash character]; }]",
   "status": 400
}
```

basically it says:  `ElasticsearchIllegalArgumentException[the character '_' is not a valid geohash character]`

any idea?( I'm using ES 1.3.2)

Thanks
</comment><comment author="clintongormley" created="2014-12-17T11:39:57Z" id="67310908">@goooooouwa looks like it doesn't support `missing`.  Please open a new issue for that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticsearchIllegalArgumentException No version type match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6441</link><project id="" key="" /><description>I'm using es 1.2.1 in with settings like this

```
      NodeBuilder.nodeBuilder().local(true).data(true).settings(
ImmutableSettings.settingsBuilder().
        put("path.data", dataPath).
        put("node.http.enabled", false).
        put("http.enabled", false).
        put("index.number_of_shards", 1).
        put("index.number_of_replicas", 1).
        put("discovery.zen.ping.unicast.hosts", "localhost").
        put("discovery.zen.ping.multicast.enabled", false).
        build())
```

when i start two jvms and start inserting stuff in one of them after restart i get 

```
[info] 2014-06-09 16:56:50,257 - o.e.node - [Taskmaster] version[1.2.1], pid[7220], build[6c95b75/2014-06-03T15:02:52Z]
[info] 2014-06-09 16:56:50,258 - o.e.node - [Taskmaster] initializing ...
[info] 2014-06-09 16:56:50,262 - o.e.plugins - [Taskmaster] loaded [], sites []
[info] 2014-06-09 16:56:52,392 - o.e.node - [Taskmaster] initialized
[info] 2014-06-09 16:56:52,392 - o.e.node - [Taskmaster] starting ...
[info] 2014-06-09 16:56:54,643 - o.e.transport - [Taskmaster] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[apc/192.168.9.233:9300]}
[info] 2014-06-09 16:56:57,681 - o.e.c.service - [Taskmaster] new_master [Taskmaster][4PROogcjRSSUjP-He9HNGA][apc][inet[apc/192.168.9.233:9300]]{http.enabled=false, local=false}, reason: zen-disco-join (elected_as_master)
[info] 2014-06-09 16:56:57,689 - o.e.discovery - [Taskmaster] elasticsearch/4PROogcjRSSUjP-He9HNGA
[info] 2014-06-09 16:56:58,544 - o.e.gateway - [Taskmaster] recovered [2] indices into cluster_state
[info] 2014-06-09 16:56:58,545 - o.e.node - [Taskmaster] started
[warn] 2014-06-09 16:56:59,204 - o.e.i.cluster - [Taskmaster] [users_idx3][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [users_idx3][0] failed to recover shard
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:241) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132) ~[elasticsearch-1.2.1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
    at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [49]
    at org.elasticsearch.index.VersionType.fromValue(VersionType.java:307) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.translog.Translog$Index.readFrom(Translog.java:506) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.translog.TranslogStreams.readTranslogOperation(TranslogStreams.java:52) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:218) ~[elasticsearch-1.2.1.jar:na]
    ... 4 common frames omitted

```

i have to delete the index after this
this is on windows
</description><key id="35286910">6441</key><summary>ElasticsearchIllegalArgumentException No version type match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">OlegYch</reporter><labels /><created>2014-06-09T14:04:21Z</created><updated>2014-07-01T17:13:30Z</updated><resolved>2014-07-01T17:13:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="OlegYch" created="2014-06-09T14:10:38Z" id="45494236">actually just one jvm and hard restart is enough
i also occasionally get even before restart

```
java.lang.IllegalArgumentException: No type mapped for [0]
    at org.elasticsearch.index.translog.Translog$Operation$Type.fromId(Translog.java:223) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.translog.TranslogStreams.readSource(TranslogStreams.java:59) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.engine.internal.InternalEngine.get(InternalEngine.java:340) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:469) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:195) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:106) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:109) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43) ~[elasticsearch-1.2.1.jar:na]
    at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:163) ~[elasticsearch-1.2.1.jar:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_51]
    at java.lang.Thread.run(Thread.java:744) ~[na:1.7.0_51]
```
</comment><comment author="OlegYch" created="2014-06-09T14:40:31Z" id="45497724">the same test works fine with 1.1.2
</comment><comment author="OlegYch" created="2014-06-11T13:06:03Z" id="45737940">actually the problem also exists on linux
</comment><comment author="uschindler" created="2014-06-14T23:17:13Z" id="46102169">Hi the same happens for me in Windows and Linux (Ubuntu 12.04, DEB package by Elasticsearch).

It happens mostly after heavy indexing (in my case I am inexing out of one index into another one with some transformations). The error is easily visible after restarting the node, in that case you get the given errors ASAP.

Before the actual problem shows after restart, the following can be seen on my index, too: While reading one document from the original index with a realtime get, to be indexed in the other index, it starts to complain about some broken mapping, while reading the field data (source/stored field):

```
[2014-06-15 00:01:04,147][DEBUG][action.get               ] [PANGAEA Elasticsearch] [panfmp_v2][3]: failed to execute [[panfmp_v2][panfmp_meta][badc]: routing [null]]
java.lang.IllegalArgumentException: No type mapped for [121]
        at org.elasticsearch.index.translog.Translog$Operation$Type.fromId(Translog.java:223)
        at org.elasticsearch.index.translog.TranslogStreams.readSource(TranslogStreams.java:59)
        at org.elasticsearch.index.engine.internal.InternalEngine.get(InternalEngine.java:340)
        at org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:469)
        at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:195)
        at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:106)
        at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:109)
        at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43)
        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:163)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
```

When this error happened for the first time, you can be sure that the index is corrupt and will produce the messages reported in this issue on startup.

Oracle Java 7u60 !
</comment><comment author="uschindler" created="2014-06-14T23:24:45Z" id="46102286">To work around the problem, you need to trigger a flush() before the realtime get, so it looks like the problem is related to the transaction log.
</comment><comment author="uschindler" created="2014-06-15T10:37:45Z" id="46112396">I tried this out with a variety of combinations of mappings (the affected mapping is always one with solely some key-value-pairs, which are never indexed, just some "metadata" stored as key-value pairs in the index). It does not work if stored=true for the fields and source is disabled, but the same if source is enabled.

It looks like Elasticsearch has a problem in version 1.2.1 to load documents from the realtime transaction log, because it has problems with its mapping.

The mapping it has problems with is:

``` javascript
    "metadata" : {
        "dynamic_templates" : [ {
          "kv_pairs" : {
            "mapping" : {
              "type" : "string",
              "index" : "no",
              "store" : false
            },
            "match" : "*"
          }
        } ],
        "_all" : {
          "enabled" : false
        },
        "properties" : {
          "lastHarvested" : {
            "type" : "string",
            "index" : "no"
          }
        }
   }
```

As said before, if you flush before getting the realtime documents, it works. But this is a bad idea for performance and couteracts realtime.
</comment><comment author="uschindler" created="2014-06-15T11:42:23Z" id="46113616">I have the feeling, this has to do wth the changes in #6149, although my code does not use external versions. To me it looks like reading and writing version numbers to the tranlog f*cks up, and the readByte() for looking up the transaction type then gets an invalid value, because we are reading something different (filepointer at wrong position). I want to mention, its done with a completely fresh index, so its not a backwards problem.

Maybe @bleskes should have look!
Uwe
</comment><comment author="bleskes" created="2014-06-16T06:48:29Z" id="46145806">@uschindler yeah, I was already keeping an eye on this thread :) I will try to reproduce this / look into the code. If you already some code you can share that reproduces it, it will be very much appreciated.
</comment><comment author="uschindler" created="2014-06-16T08:40:29Z" id="46153636">Hi @bleskes: I will try to write an examle TransportClient-main() program that reproduvces the issue. I am not sure if this can be reproduced in a conventional test. In addition, for me its easier to create just a client that does exactly the same what mine does (same mapping).

Please note: The mapping I used has no indexed fields, and only a dynamic field that assigns &lt;String,String&gt; key value pairs (unstored). I use this just for storing some "state" in the index using one single docid. This docid is updated quite often and read via realtime get.
</comment><comment author="bleskes" created="2014-06-16T08:44:42Z" id="46154025">@uschindler that would be awesome. Just clarifying - you can reproduce without indexing lots of documents? (via a scan / index operation).
</comment><comment author="uschindler" created="2014-06-16T08:49:33Z" id="46154471">I will try out, I just said what the pattern of the usage of the realtime get was: one single document with state information. The other stuff was going on in parallel (scan and reindex to new index). I will try to reproduce during the day.
</comment><comment author="ikks" created="2014-06-17T20:37:25Z" id="46361779">Yesterday I had an issue like this one, also with space problems, the index was broken, so removing it let me restart elasticsearch.
</comment><comment author="bleskes" created="2014-06-18T06:45:26Z" id="46401313">@ikks yeah, the issue indicates (I think) a corrupted transaction log. During restart we replay the translog to make sure all indexed documents that were not committed to disk by lucene (== flush in ES terminology) are re-indexed. Running out of disk space can corrupt the tranlog and break this replay process. For what it's worth - it's enough to delete the translog file is enough (with the potential loss of data) to bring back the index.
</comment><comment author="uschindler" created="2014-06-18T08:39:25Z" id="46409700">@ikks, @bleskes yes thats a different issue. I am at the moment again working on reproducing. I found out up to now: I modified org.elasticsearch.stresstest.get.GetStressTest to have 2 indexes and 2 mappings and indexing random documents with various field combinations and getting random ones of the already indexed. This showed:
- The mapping itsself is not the bad guy
- Changing/updating the mapping while indexing is not the issue (we do this, so it worth to try)
- There is no out of disk space here :-)

What I have not yet tested is one thing: The scan-search with copying documents from one index to the other. This is the only one from my code that is not in my modified test.

Once I have something, I will post.

To me the issue looks like not really introduced in ES 1.2, it looks more like some other changes (like when we flush based on memory) now caused the issue to happen. I have seen references to corrupt transaction log several times in the past, so this might be all related.
</comment><comment author="bleskes" created="2014-06-18T08:41:59Z" id="46409893">@uschindler agreed it's not necessarily a 1.2 issue, though @OlegYch was very specific about that. Thx for the hard work. Hoping you will find it.
</comment><comment author="uschindler" created="2014-06-18T08:44:06Z" id="46410063">I have the following until now, which still passes: https://github.com/uschindler/elasticsearch/commit/5b72037ed60fbb387bbe16e6defa950d68a6aabd

Just run from eclipse with right-click -&gt; execute.
</comment><comment author="uschindler" created="2014-06-18T08:46:24Z" id="46410265">@bleskes I also sometimes to bulk indexing requests in my test, because this was also something i did locally. As said before, no corrumption until now :-)
</comment><comment author="bleskes" created="2014-06-18T08:47:52Z" id="46410383">@uschindler yeah, I spent half of bbuzz with a modified version this test doing crazy stuff to no avail.. @OlegYch said he hard restarted a node? maybe that's a clue?
</comment><comment author="uschindler" created="2014-06-18T08:51:40Z" id="46410706">@bleskes restarting node is not the primary problem. The problem shows before restarting the node. If you get errors while realtime gets and restart the node after that, it is no longer able to reapply transactions. But this is just a followup, the original issue happened earlier.
</comment><comment author="bleskes" created="2014-06-18T08:54:46Z" id="46410970">@uschindler if you had without restarting, then yeah, I agree. I was thinking maybe the restart cause the translog not to be written to disk correctly.
</comment><comment author="OlegYch" created="2014-06-18T09:29:46Z" id="46414022">registering a few percolators into clean index is enough to trigger No version type match [56] after restart
the code is like this

```
      new IndexRequestBuilder(null).setIndex(index)
        .setType(PercolatorService.TYPE_NAME).setId(id).setRefresh(true)
        .setSource(source).request
```
</comment><comment author="uschindler" created="2014-06-18T11:25:56Z" id="46423538">@OlegYch maybe you can post your code?
</comment><comment author="OlegYch" created="2014-06-18T12:06:21Z" id="46427105">ok let me try to minimize it
</comment><comment author="uschindler" created="2014-06-18T13:00:35Z" id="46431839">I also have the code actually in open access. so its easy to reproduce the whole thing. If you are willing to checkout some SVN code and the patch away the "explicit flush" i added to workaround the problem, you can easily reproduce...
</comment><comment author="uschindler" created="2014-06-18T13:08:55Z" id="46432722">Maybe the whole problem is one of the good old "Oracle JDK bugs"!
</comment><comment author="uschindler" created="2014-06-18T14:00:05Z" id="46438841">Here is how to reproduce:
- Checkout panFMP source code, revision 600: svn co -r 600 https://svn.code.sf.net/p/panfmp/code/main/trunk
- Patch this:

```
Index: src/de/pangaea/metadataportal/processor/ElasticsearchConnection.java
===================================================================
--- src/de/pangaea/metadataportal/processor/ElasticsearchConnection.java        (revision 600)
+++ src/de/pangaea/metadataportal/processor/ElasticsearchConnection.java        (working copy)
@@ -200,8 +200,8 @@

     final IndicesAdminClient indicesAdmin = client.admin().indices();

-    log.info("Flushing data...");
-    indicesAdmin.prepareFlush(realIndexName).get();
+    // log.info("Flushing data...");
+    // indicesAdmin.prepareFlush(realIndexName).get();

     final String aliasedIndex = getAliasedIndex(ticonf);
     if (cleanShutdown &amp;&amp; !realIndexName.equals(aliasedIndex)) {
```
- Start Elasticsearch (as downloaded), version 1.2.1
- Run as a first step to fill the index with XML metadata from PANGAEA and some US datacenters: ant run-harvester (if this fails for first time with NoNodeAvailableException, retry - this requires that ES is running on localhost, port 9300). 

After the data is harvested (index panfmp_v1), rebuild the index: ant run-rebuilder

While running the rebuilder, it hits exceptions on realtime get. If you restart the ES instance it will break while trying to reapply tranlog.

I used Oracle JDK 7u60... The bug reproduces every time on Windows 7! Did not try Linux.
</comment><comment author="bleskes" created="2014-06-18T14:12:09Z" id="46440345">@uschindler thx! I will definitely dig into this. 

@OlegYch I will also try the percolator route. If you have running reproduction it will be awesome.
</comment><comment author="uschindler" created="2014-06-18T15:53:13Z" id="46454725">Just for completeness: Here is my other tester that also does not find the bug :(: https://github.com/uschindler/elasticsearch/commit/8301432f74bdb3231a7a9f0f21cbd09543749895
</comment><comment author="OlegYch" created="2014-06-18T17:15:31Z" id="46465219">here is a repro https://github.com/OlegYch/es-test
basically 1.2.1 fails when i insert several somewhat large percolators (perhaps other documents as well)
</comment><comment author="OlegYch" created="2014-06-18T17:22:48Z" id="46466101">it seems to fail only on windows though
 using same jdk on both windows and linux
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unsupported Java major.minor version 51.0 w/ OS X 10.10 Yosemite</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6440</link><project id="" key="" /><description>System:

```
$ sw_vers
ProductName:    Mac OS X
ProductVersion: 10.10
BuildVersion:   14A238x
```

Installed Java:
ref: http://support.apple.com/kb/DL1572

```
$ java -version
java version "1.6.0_65"
Java(TM) SE Runtime Environment (build 1.6.0_65-b14-466.1-11M4716)
Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-466.1, mixed mode)
```

Error:

```
$ elasticsearch
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
  at java.lang.ClassLoader.defineClass1(Native Method)
  at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
  at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
  at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
  at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
  at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
  at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
```
</description><key id="35283352">6440</key><summary>Unsupported Java major.minor version 51.0 w/ OS X 10.10 Yosemite</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">takahiro47</reporter><labels /><created>2014-06-09T13:16:26Z</created><updated>2015-08-11T17:27:30Z</updated><resolved>2014-06-09T14:00:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-06-09T13:18:41Z" id="45489306">Hi,

starting from ES 1.2 we stopped supporting Java 1.6
Its best to upgrade your java to 1.7
</comment><comment author="johtani" created="2014-06-09T13:21:55Z" id="45489553">Hi Takahiro,

What do you use the version of elasticsearch?
1.2.1?

Elasticsearch 1.2.1 is required Java 7.
Does not work Java6.
You should install Java 7.

Regards ,
Jun
2014/06/09 22:16 "Takahiro Tsuchiya" notifications@github.com:

&gt; Installed Java:
&gt; ref: http://support.apple.com/kb/DL1572
&gt; 
&gt; $ java -version
&gt; java version "1.6.0_65"
&gt; Java(TM) SE Runtime Environment (build 1.6.0_65-b14-466.1-11M4716)
&gt; Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-466.1, mixed mode)
&gt; 
&gt; Error:
&gt; 
&gt; $ elasticsearch
&gt; Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
&gt;   at java.lang.ClassLoader.defineClass1(Native Method)
&gt;   at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
&gt;   at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
&gt;   at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
&gt;   at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
&gt;   at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
&gt;   at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
&gt;   at java.security.AccessController.doPrivileged(Native Method)
&gt;   at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
&gt;   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
&gt;   at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
&gt;   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6440.
</comment><comment author="takahiro47" created="2014-06-09T13:38:17Z" id="45490985">Thank you for the reply both of you!

I installed the version 1.2.1.

electricsearch:

```
$ brew info elasticsearch
elasticsearch: stable 1.2.1, HEAD
http://www.elasticsearch.org
/usr/local/Cellar/elasticsearch/1.2.1 (31 files, 21M) *
  Built from source
From: https://github.com/Homebrew/homebrew/commits/master/Library/Formula/elasticsearch.rb
...
```

To be ashamed, I did not know that elasticsearch does not support Java 6 already.

Now, OS X 10.10 Yosemite does not have to support the Java7 still, that only able to install only Java6.
I decided to try waiting a little more coverage for the Java7.

Thank you. m(_ _)m
</comment><comment author="takahiro47" created="2014-06-09T14:25:09Z" id="45495943">Hi,

Although it is nothing to do with this product, 
Since I was able to run on OS X 10.10 Yosemite the Java7, and reports for reference.

---

### How to install
1. Rewrite "ProductVersion" 10.10 to 10.9 in /System/Library/CoreServices/SystemVerion.plist
2. Install Java 7
   (ref: https://jdk7.java.net/download.html)
3. Write back the rewritten "ProductVersion" in /System/Library/CoreServices/SystemVerion.plist

The reason you were unable to install, UNIX seems to be because you have recognized as 10.1 10.10.

---

### After install

elasticsearch (via Homebrew):

```
$ elasticsearch -v
Version: 1.2.1, Build: 6c95b75/2014-06-03T15:02:52Z, JVM: 1.7.0_60-ea
```

Java:

```
$ java -version
java version "1.7.0_60-ea"
Java(TM) SE Runtime Environment (build 1.7.0_60-ea-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)
```

run:

```
$ elasticsearch
[2014-06-09 23:11:27,400][INFO ][node                     ] [Stature] version[1.2.1], pid[16835], build[6c95b75/2014-06-03T15:02:52Z]
[2014-06-09 23:11:27,400][INFO ][node                     ] [Stature] initializing ...
[2014-06-09 23:11:27,403][INFO ][plugins                  ] [Stature] loaded [], sites []
[2014-06-09 23:11:34,081][WARN ][common.network           ] failed to resolve local host, fallback to loopback
[2014-06-09 23:11:34,483][INFO ][node                     ] [Stature] initialized
[2014-06-09 23:11:34,484][INFO ][node                     ] [Stature] starting ...
[2014-06-09 23:11:34,572][INFO ][transport                ] [Stature] bound_address {inet[/127.0.0.1:9301]}, publish_address {inet[/127.0.0.1:9301]}
[2014-06-09 23:11:37,604][INFO ][cluster.service          ] [Stature] new_master [Stature][GFpcWkinQRmtitPv0eUH8Q][localhost][inet[/127.0.0.1:9301]], reason: zen-disco-join (elected_as_master)
[2014-06-09 23:11:37,636][INFO ][discovery                ] [Stature] elasticsearch_takahiro/GFpcWkinQRmtitPv0eUH8Q
[2014-06-09 23:11:37,655][INFO ][http                     ] [Stature] bound_address {inet[/127.0.0.1:9201]}, publish_address {inet[/127.0.0.1:9201]}
[2014-06-09 23:11:37,672][INFO ][gateway                  ] [Stature] recovered [0] indices into cluster_state
[2014-06-09 23:11:37,673][INFO ][node                     ] [Stature] started
```

---

elasticsearch worked perfectly, even with OS X 10.10 Yosemite!

Amazing! Thanks! XD
</comment><comment author="electrical" created="2014-06-09T14:34:31Z" id="45497045">@takahiro47 thank you very much for writing this up and adding it to the issue.
This will certainly help users who will be hitting the same issue.
Thanks!
</comment><comment author="flocks" created="2014-07-08T08:40:53Z" id="48285989">Many thanks @takahiro47 :)
</comment><comment author="searchandanalytics" created="2015-08-11T17:27:29Z" id="129979411">Amazing! Thank you so much @takahiro47 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>when using the scrolling search should set refresh_interval to -1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6439</link><project id="" key="" /><description>when using the scrolling search shoud we set the refresh_interval to -1,close the updating the index? could we ignore the refresh_interval?
</description><key id="35254644">6439</key><summary>when using the scrolling search should set refresh_interval to -1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JasonBian</reporter><labels /><created>2014-06-09T02:06:19Z</created><updated>2014-06-09T09:38:17Z</updated><resolved>2014-06-09T09:38:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-09T09:38:16Z" id="45475010">Hi @JasonBian 

The issues list is for bugs and feature requests. Please ask questions like this on the forum instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing ampersands</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6438</link><project id="" key="" /><description>I have a situation where I need to keep ampersands as a token in my index, and ideally, they should be converted to "and". I'm using a char filter to do this. It's working fine on searches, as when I run a search through the Validate API that includes an ampersand, I can see "and" as a token. But for some reason it's not working on the index side.

In one case in particular, I have an item titled "Terms &amp; Conditions" and I want it to match searches for "terms and conditions." (Searches are a multi_match with boolean AND.) I've included my index definition below (it's in PHP format, not JSON, because I'm using the Elastica library.)

```
        'analysis' =&gt; array(
            "char_filter"  =&gt; array(
                // Collapses "H.M.S." or "H. M. S." into HMS
                "abbreviations" =&gt; array(
                    "type" =&gt; "pattern_replace",
                    "pattern" =&gt; "\b([A-Z])[\.\, ]+(?=[A-Z]\b)",
                    "replacement" =&gt; "$1"
                ),
                "punctuation" =&gt; array(
                    "type" =&gt; "pattern_replace",
                    "pattern" =&gt; "[^a-zA-Z\d\s]",
                    "replacement" =&gt; ""
                ),
                "&amp;_to_and" =&gt; array(
                    "type" =&gt;  "mapping",
                    "mappings" =&gt; [ "&amp;=&gt;and"]
                )
            ),
            "filter"  =&gt; array(
                [ ... filters ... ]
            ),
            'analyzer' =&gt; array(
                'default_index' =&gt; array(
                    'type' =&gt; 'custom',
                    'tokenizer' =&gt; 'standard',
                    "char_filter" =&gt; array('&amp;_to_and', "abbreviations", 'punctuation'),
                    'filter' =&gt; array('standard', 'lowercase', 'elision', 'kstem', 'shingle_filter')
                ),
                'default_search' =&gt; array(
                    'type' =&gt; 'custom',
                    'tokenizer' =&gt; 'standard',
                    "char_filter" =&gt; array('&amp;_to_and', "abbreviations", 'punctuation'),
                    'filter' =&gt; array('standard', 'lowercase', 'elision', 'word_synonym_filter', 'kstem')
                ),
                [ ... other analyzers ... ]
                'variants' =&gt; array(
                    'type' =&gt; 'custom',
                    'tokenizer' =&gt; 'standard',
                    "char_filter" =&gt; array('&amp;_to_and', "abbreviations", 'punctuation'),
                    'filter' =&gt; array('standard', 'lowercase', 'elision', 'name_synonym_filter', 'word_synonym_filter')
                ),
                [ ... other analyzers ... ]
            )
        )
```

This should, to my understanding, cause instances of "&amp;" to become "and" in the index for any field that doesn't specify an analyzer, or one that uses the "variants" analyzer. But when I specifically index a document with an ampersand, then query it, it still shows an ampersand.

Where am I going wrong here? I'm using ElasticSearch 1.1.2. 
</description><key id="35250993">6438</key><summary>Indexing ampersands</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">shaneiseminger</reporter><labels /><created>2014-06-08T23:45:26Z</created><updated>2014-06-09T08:54:58Z</updated><resolved>2014-06-09T08:54:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-09T08:54:58Z" id="45472225">HI @shaneiseminger 

Please ask these questions in the forum instead.  The issues list is for bugs.  The reason you're not seeing `and` in your index is because you're using the `kstem` token filter, which is removing them.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update index_.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6437</link><project id="" key="" /><description>The version types other than `internal` and `external` are only supported in Elasticsearch 1.1 and up
</description><key id="35245729">6437</key><summary>Update index_.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ranrub</reporter><labels><label>docs</label></labels><created>2014-06-08T20:15:03Z</created><updated>2014-08-07T18:55:28Z</updated><resolved>2014-08-07T18:55:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-07T18:55:28Z" id="51515716">We're moving to versioned docs, so I'm going to close this one. thanks anyway
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk request which try and fail to create multiple indices may never return</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6436</link><project id="" key="" /><description>This is caused by an NPE in the error handling code. All is well if only 1 index fails (or none).
</description><key id="35178567">6436</key><summary>Bulk request which try and fail to create multiple indices may never return</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Bulk</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-06T20:14:25Z</created><updated>2015-06-07T19:46:40Z</updated><resolved>2014-06-06T21:16:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-06T20:55:26Z" id="45383349">@jpountz pushed another commit.
</comment><comment author="jpountz" created="2014-06-06T20:56:18Z" id="45383428">LGTM
</comment><comment author="bleskes" created="2014-06-06T21:18:26Z" id="45385432">merged. Thx @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Intermittent DateRange aggregation bug.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6435</link><project id="" key="" /><description>Am seeing a intermittent bug whereby we see incorrect and internally inconsistent aggregation results (ie where the subaggregations have more items that the parent aggregation).

``` javascript
{
    "sales_quotas": {
        "doc_count": 5,
        "shipmentDate": {
            "buckets": [
                {
                    "key": "Overdue",
                    "to": 1.3989024E12,
                    "to_as_string": "2014-05-01",
                    "doc_count": 0,
                    "commodity": {
                        "buckets": [
                            {
                                "key": "&#64174;&#12505;&#518487;&#34127;&#603;",
                                "doc_count": 5,
                                "quantityNormalised": {
                                    "value": 5000.0
                                },
                                "unallocatedQuantityNormalised": {
                                    "value": 5000.0
                                }
                            }
                        ]
                    },
                    "nothingAllocated": {
                        "doc_count": 5,
                        "ME": {
                            "doc_count": 3
                        },
                        "NOT_ME": {
                            "doc_count": 2
                        }
                    },
                    "notFullyAllocated": {
                        "doc_count": 0,
                        "ME": {
                            "doc_count": 0
                        },
                        "NOT_ME": {
                            "doc_count": 0
                        }
                    }
                }
            ]
        }
    }
}
```

I have written a test-case which (if you run it enough times) will reproduce it.

Here is the test class:

_AggregationBugs.java_

``` java
package elastic.bugs;

import java.io.IOException;
import java.util.Collection;
import java.util.Map;

import org.elasticsearch.action.search.SearchRequestBuilder;
import org.elasticsearch.action.search.SearchResponse;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.search.aggregations.Aggregations;
import org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;
import org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;
import org.elasticsearch.search.aggregations.bucket.range.date.InternalDateRange;
import org.elasticsearch.search.aggregations.bucket.terms.StringTerms;
import org.elasticsearch.search.aggregations.bucket.terms.Terms;
import org.elasticsearch.search.aggregations.metrics.sum.InternalSum;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.Test;

import com.fasterxml.jackson.core.JsonGenerator;
import com.fasterxml.jackson.databind.DeserializationFeature;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.joda.JodaModule;

import static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;
import static org.elasticsearch.test.ElasticsearchIntegrationTest.Scope.SUITE;

@ElasticsearchIntegrationTest.ClusterScope(scope = SUITE, numNodes = 1)
public class AggregationBugs extends ElasticsearchIntegrationTest
{
    private static final String INDEX = "bugindex";
    private final ObjectMapper json = new ObjectMapper();

    public AggregationBugs()
    {
        json.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);
        json.configure(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN, true);
        json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);
        json.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);
        json.registerModule(new JodaModule());
    }

    @Override
    protected Settings nodeSettings(int nodeOrdinal)
    {
        return settingsBuilder()
            .put("path.data", "target/elastic-test-data")
            .build();
    }

    private void index(String id, String type, String content) throws IOException
    {
        index(INDEX, type, id, json.readValue(content, Map.class));
    }

    @Test
    public void subAggregationBug() throws IOException
    {
        admin()
            .indices()
            .prepareCreate(INDEX)
            .execute()
            .actionGet();

        String propertiesSource = "{\"properties\":{\"contractualLocationCity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"contractualLocationCountry\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isAllocationNotInvoiced\":{\"type\":\"boolean\"},\"isFullyAllocated\":{\"type\":\"boolean\"},\"allocatedQuantity\":{\"type\":\"double\"},\"contractualLocation\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"isInvoicedFinal\":{\"type\":\"boolean\"},\"isZeroAllocated\":{\"type\":\"boolean\"},\"quotaId\":{\"index\":\"not_analyzed\",\"type\":\"long\"},\"counterParty\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"allocationStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantity\":{\"type\":\"double\"},\"allocatedQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"tradeRef\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantity\":{\"type\":\"double\"},\"contract\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractualLocationCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"shape\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceStatus\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"isActive\":{\"type\":\"boolean\"},\"shipmentDate\":{\"format\":\"date\",\"type\":\"date\"},\"completedDate\":{\"format\":\"date\",\"type\":\"date\"},\"groupCompany\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceQuantity\":{\"type\":\"double\"},\"resolvedIds\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"uninvoicedQuantityNormalisedUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"expectedSalesMonth\":{\"format\":\"date\",\"type\":\"date\"},\"expectedSalesLocationCity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isCancelled\":{\"type\":\"boolean\"},\"neptuneQuotaId\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"uninvoicedQuantityNormalised\":{\"type\":\"double\"},\"contractDutyPaid\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"unallocatedQuantityNormalised\":{\"type\":\"double\"},\"contractIncotermsCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"groupCompanyCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"grade\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"invoiceQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"unallocatedQuantityUnitOfMeasure\":{\"index\":\"no\",\"type\":\"string\"},\"counterPartyCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"isPurchase\":{\"type\":\"boolean\"},\"isCompleted\":{\"type\":\"boolean\"},\"trafficOperatorFullName\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"groupCompanyId\":{\"include_in_all\":false,\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocation\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"intentIncotermsCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"trafficOperatorSid\":{\"include_in_all\":false,\"index\":\"not_analyzed\",\"type\":\"string\"},\"quantityNormalised\":{\"type\":\"double\"},\"commodity\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"contractIncoterms\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocationCode\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"brand\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaType\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"expectedSalesLocationCountry\":{\"index\":\"not_analyzed\",\"type\":\"string\"},\"quotaRef\":{\"index\":\"not_analyzed\",\"boost\":2.0,\"type\":\"string\"},\"comments\":{\"type\":\"string\"},\"intentIncoterms\":{\"index\":\"not_analyzed\",\"type\":\"string\"}}}";
        admin()
            .indices()
            .preparePutMapping(INDEX)
            .setType("quota")
            .setSource(propertiesSource)
            .execute()
            .actionGet();

        index("1581354603", "quota", "{\"allocatedQuantity\":0.0000,\"allocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFAAE\\u30D9\\uD9BA\\uDD57\\u854F\\u025B\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-06\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyId\":\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"invoiceQuantity\":0.0000,\"invoiceQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":false,\"isZeroAllocated\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quotaId\":\"1581354603\",\"quotaRef\":\"2723.4\",\"quotaStatus\":\"OPEN\",\"quotaType\":\"SALES\",\"resolvedIds\":[\"5D2E1DF59DB54303819A7A1320DF6053\",\"D9B2681C5CEB4FC1AB34280B32FB1C0E\",\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"18F42D923B444C3FB074D1A05548BE2B\"],\"shape\":null,\"shipmentDate\":\"2014-05-30\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"James Brown\",\"trafficOperatorSid\":\"SID2\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"unallocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"uninvoicedQuantityNormalised\":0.0000,\"uninvoicedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\"}");
        index("839452955", "quota", "{\"allocatedQuantity\":0.0000,\"allocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFAAE\\u30D9\\uD9BA\\uDD57\\u854F\\u025B\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-06\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyId\":\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"invoiceQuantity\":0.0000,\"invoiceQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":false,\"isZeroAllocated\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quotaId\":\"839452955\",\"quotaRef\":\"2723.1\",\"quotaStatus\":\"OPEN\",\"quotaType\":\"SALES\",\"resolvedIds\":[\"CE280D70F7CD4E16B871983420E7A136\",\"D9B2681C5CEB4FC1AB34280B32FB1C0E\",\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"18F42D923B444C3FB074D1A05548BE2B\"],\"shape\":null,\"shipmentDate\":\"2014-05-30\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Current User \\u054C\\u0561\\u0581\\u058B\\u0545\\u0576\\u0565\\u054C\\u0550\\u0582\",\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"unallocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"uninvoicedQuantityNormalised\":0.0000,\"uninvoicedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\"}");
        index("679144004", "quota", "{\"allocatedQuantity\":0.0000,\"allocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFAAE\\u30D9\\uD9BA\\uDD57\\u854F\\u025B\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-06\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyId\":\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"invoiceQuantity\":0.0000,\"invoiceQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":false,\"isZeroAllocated\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quotaId\":\"679144004\",\"quotaRef\":\"2723.5\",\"quotaStatus\":\"OPEN\",\"quotaType\":\"SALES\",\"resolvedIds\":[\"CE280D70F7CD4E16B871983420E7A136\",\"D9B2681C5CEB4FC1AB34280B32FB1C0E\",\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"18F42D923B444C3FB074D1A05548BE2B\"],\"shape\":null,\"shipmentDate\":\"2014-05-30\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Current User \\u054C\\u0561\\u0581\\u058B\\u0545\\u0576\\u0565\\u054C\\u0550\\u0582\",\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"unallocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"uninvoicedQuantityNormalised\":0.0000,\"uninvoicedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\"}");
        index("516594663", "quota", "{\"allocatedQuantity\":0.0000,\"allocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFAAE\\u30D9\\uD9BA\\uDD57\\u854F\\u025B\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-06\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyId\":\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"invoiceQuantity\":0.0000,\"invoiceQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":false,\"isZeroAllocated\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quotaId\":\"516594663\",\"quotaRef\":\"2723.3\",\"quotaStatus\":\"OPEN\",\"quotaType\":\"SALES\",\"resolvedIds\":[\"CE280D70F7CD4E16B871983420E7A136\",\"D9B2681C5CEB4FC1AB34280B32FB1C0E\",\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"18F42D923B444C3FB074D1A05548BE2B\"],\"shape\":null,\"shipmentDate\":\"2014-05-30\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Current User \\u054C\\u0561\\u0581\\u058B\\u0545\\u0576\\u0565\\u054C\\u0550\\u0582\",\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"unallocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"uninvoicedQuantityNormalised\":0.0000,\"uninvoicedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\"}");
        index("1677219499", "quota", "{\"allocatedQuantity\":0.0000,\"allocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"allocationStatus\":\"UNALLOCATED\",\"brand\":null,\"comments\":\"comments...\",\"commodity\":\"\\uFAAE\\u30D9\\uD9BA\\uDD57\\u854F\\u025B\",\"completedDate\":null,\"contract\":\"contract\",\"contractDutyPaid\":null,\"expectedSalesMonth\":\"2014-06-06\",\"grade\":null,\"groupCompany\":\"GROUP-COMPANY-NAME-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyCode\":\"GROUP-COMPANY-CODE-\\uE815\\uD866\\uDD82d\\u549D\\uD8D3\\uDEF4\\uE9C3\\uD9A8\\uDDA5\\u716Fqd\\uB1B7\",\"groupCompanyId\":\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"invoiceQuantity\":0.0000,\"invoiceQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"invoiceStatus\":\"NOTFINALINVOICED\",\"isActive\":true,\"isAllocationNotInvoiced\":false,\"isCancelled\":false,\"isCompleted\":false,\"isFullyAllocated\":false,\"isInvoicedFinal\":false,\"isPurchase\":false,\"isZeroAllocated\":true,\"neptuneQuotaId\":\"neptune-quota-id\",\"quantity\":1000.0000,\"quantityNormalised\":1000.0000,\"quantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"quotaId\":\"1677219499\",\"quotaRef\":\"2723.2\",\"quotaStatus\":\"OPEN\",\"quotaType\":\"SALES\",\"resolvedIds\":[\"D9B2681C5CEB4FC1AB34280B32FB1C0E\",\"DBF8E7284EC7423BBE1FCE8C0FDE9039\",\"18F42D923B444C3FB074D1A05548BE2B\",\"D41031D6EF9E4F829913E856FE2B8E89\"],\"shape\":null,\"shipmentDate\":\"2014-05-30\",\"tradeRef\":\"edm-trade-id\",\"trafficOperatorFullName\":\"Mary\",\"trafficOperatorSid\":\"SID1\",\"unallocatedQuantity\":1000.0000,\"unallocatedQuantityNormalised\":1000.0000,\"unallocatedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"unallocatedQuantityUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\",\"uninvoicedQuantityNormalised\":0.0000,\"uninvoicedQuantityNormalisedUnitOfMeasure\":\"\\uF5FC\\u03FE\\u0105r\\u0440Z\"}");
        refresh();

        String aggregationSource = "{\"size\":0,\"aggregations\":{\"sales_quotas\":{\"filter\":{\"and\":{\"filters\":[{\"range\":{\"shipmentDate\":{\"from\":null,\"to\":\"2014-06-30\",\"include_lower\":true,\"include_upper\":true}}},{\"terms\":{\"groupCompanyId\":[\"DBF8E7284EC7423BBE1FCE8C0FDE9039\"]}},{\"type\":{\"value\":\"quota\"}},{\"term\":{\"isPurchase\":false}},{\"terms\":{\"quotaStatus\":[\"OPEN\"]}}]}},\"aggregations\":{\"shipmentDate\":{\"date_range\":{\"field\":\"shipmentDate\",\"ranges\":[{\"key\":\"Overdue\",\"to\":\"2014-05-01\"},{\"key\":\"May\",\"from\":\"2014-05-01\",\"to\":\"2014-06-01\"},{\"key\":\"June\",\"from\":\"2014-06-01\",\"to\":\"2014-07-01\"}]},\"aggregations\":{\"nothingAllocated\":{\"filter\":{\"term\":{\"isZeroAllocated\":true}},\"aggregations\":{\"ME\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}},\"NOT_ME\":{\"filter\":{\"not\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}}}}}},\"notFullyAllocated\":{\"filter\":{\"and\":{\"filters\":[{\"term\":{\"isZeroAllocated\":false}},{\"term\":{\"isFullyAllocated\":false}}]}},\"aggregations\":{\"ME\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}},\"NOT_ME\":{\"filter\":{\"not\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}}}}}},\"allocationNotFullyInvoiced\":{\"filter\":{\"term\":{\"isAllocationNotInvoiced\":true}},\"aggregations\":{\"ME\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}},\"NOT_ME\":{\"filter\":{\"not\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}}}}}},\"allocationNotFinalInvoiced\":{\"filter\":{\"and\":{\"filters\":[{\"term\":{\"isZeroAllocated\":false}},{\"term\":{\"isInvoicedFinal\":false}}]}},\"aggregations\":{\"ME\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}},\"NOT_ME\":{\"filter\":{\"not\":{\"filter\":{\"term\":{\"trafficOperatorSid\":\"SID-IhNqzHDayeRxFVJHzoRy\"}}}}}}},\"commodity\":{\"terms\":{\"field\":\"commodity\"},\"aggregations\":{\"quantityNormalised\":{\"sum\":{\"field\":\"quantityNormalised\"}},\"unallocatedQuantityNormalised\":{\"sum\":{\"field\":\"unallocatedQuantityNormalised\"}}}}}}}}}}";
        SearchRequestBuilder request = client()
            .prepareSearch(INDEX)
            .setSource(aggregationSource);

        System.out.println("request = " + prettyPrint(aggregationSource));

        SearchResponse response = request
            .execute()
            .actionGet();

        System.out.println("response = " + response);

        SingleBucketAggregation salesQuotas = response.getAggregations().get("sales_quotas");
        InternalDateRange dateRangeAggregation = salesQuotas.getAggregations().get("shipmentDate");

        assertAllZero(dateRangeAggregation, "Overdue");
        assertExpectedValues(dateRangeAggregation, "May");
        assertAllZero(dateRangeAggregation, "June");
    }

    private void assertExpectedValues(InternalDateRange dateRangeAggregation, String name)
    {
        InternalDateRange.Bucket bucket = dateRangeAggregation.getBucketByKey(name);
        assertFilter(bucket, name, "nothingAllocated", 5, 3, 2);
        assertFilter(bucket, name, "notFullyAllocated", 0, 0, 0);
        assertFilter(bucket, name, "allocationNotFullyInvoiced", 0, 0, 0);
        assertFilter(bucket, name, "allocationNotFinalInvoiced", 0, 0, 0);

        StringTerms commodity = bucket.getAggregations().get("commodity");
        Collection&lt;Terms.Bucket&gt; buckets = commodity.getBuckets();
        assertEquals("commodity buckets", 1, buckets.size());
        assertSum(buckets, "quantityNormalised", 5000.0);
        assertSum(buckets, "unallocatedQuantityNormalised", 5000.0);
    }

    private void assertSum(Collection&lt;Terms.Bucket&gt; buckets, String name, double expectedValue)
    {
        InternalSum quantityNormalised = buckets.iterator().next().getAggregations().get(name);
        assertEquals(name, expectedValue, quantityNormalised.getValue(), 0.1);
    }

    private void assertAllZero(InternalDateRange dateRangeAggregation, String name)
    {
        InternalDateRange.Bucket bucket = dateRangeAggregation.getBucketByKey(name);
        assertFilter(bucket, name, "nothingAllocated", 0, 0, 0);
        assertFilter(bucket, name, "notFullyAllocated", 0, 0, 0);
        assertFilter(bucket, name, "allocationNotFullyInvoiced", 0, 0, 0);
        assertFilter(bucket, name, "allocationNotFinalInvoiced", 0, 0, 0);

        Aggregations aggregations = bucket.getAggregations();
        StringTerms commodity = aggregations.get("commodity");
        assertEquals(name + " commodity buckets should be zero", 0, commodity.getBuckets().size());
    }

    private void assertFilter(InternalDateRange.Bucket bucket, String bucketName, String filterName, int expectedFilterCount, int expectedMeCount, int expectedNotMeCount)
    {
        String message = bucketName + ":" + filterName;

        Aggregations aggregations = bucket.getAggregations();

        InternalFilter filter = aggregations.get(filterName);
        assertEquals(message, expectedFilterCount, filter.getDocCount());

        assertBreakdowns(message, filter.getAggregations(), expectedMeCount, expectedNotMeCount);
    }

    private void assertBreakdowns(String message, Aggregations aggregations, int expectedMeCount, int expectedNotMeCount)
    {
        assertBreakdown(aggregations, message, "ME", expectedMeCount);
        assertBreakdown(aggregations, message, "NOT_ME", expectedNotMeCount);
    }

    private void assertBreakdown(Aggregations aggregations, String message, String subFilterName, int expectedCount)
    {
        InternalFilter filter = aggregations.get(subFilterName);
        assertEquals(message + ":" + subFilterName, expectedCount, filter.getDocCount());
    }

    private String prettyPrint(String aggregationSource) throws IOException
    {
        return json.writerWithDefaultPrettyPrinter().writeValueAsString(json.readValue(aggregationSource, Map.class));
    }
}

```

I hope this helps.
</description><key id="35172360">6435</key><summary>Intermittent DateRange aggregation bug.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nickminutello</reporter><labels /><created>2014-06-06T18:53:10Z</created><updated>2014-07-08T14:39:35Z</updated><resolved>2014-06-13T21:18:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nickminutello" created="2014-06-06T19:01:03Z" id="45372676">For completeness this is on:
**jdk1.7.0_60**
java version "1.7.0_60"
Java(TM) SE Runtime Environment (build 1.7.0_60-b19)
Java HotSpot(TM) 64-Bit Server VM (build 24.60-b09, mixed mode)

**Windows 7 Enterprise**
 (Version 6.1 (Build7601: Service Pack 1)

**Processor**
AMD64
</comment><comment author="jpountz" created="2014-06-07T00:31:25Z" id="45396331">Thanks for this, I haven't dug through what the test does so far but I ran it 10 times and got one failure. I will look into it soon...
</comment><comment author="nickminutello" created="2014-06-07T22:26:52Z" id="45422880">Great. Thanks.
</comment><comment author="nickminutello" created="2014-06-09T08:40:21Z" id="45471326">If it helps, when I have seen this issue on live data, the aggregation consistently returned the wrong results.
</comment><comment author="jpountz" created="2014-06-09T19:42:55Z" id="45534737">The failure is 100% reproducible with some seeds, I've spent some time debugging it today and the issue is in the reduce logic. For some reason sometimes two distinct aggs from the tree reduce into the same aggregation, making the counts wrong. I'll be busy on the next two days but will have more time to look into it at the end of the week. I marked this issue as a blocker so that we don't release before this is fixed.
</comment><comment author="nickminutello" created="2014-06-10T08:58:57Z" id="45589617">Excellent. Thanks.
</comment><comment author="jpountz" created="2014-06-12T09:30:16Z" id="45847841">OK, I found the (sneaky) issue, which only occurs when using the local transport and some buckets are empty:
- RangeAggregator.buildEmptyAggregation uses the same empty aggregation instances for all buckets
- Then buckets get reduced in-place, which is going to modify these sub aggs, that are shared by all DateRange buckets.

This is a one-line fix but I'm afraid other aggregations might have a similar bug so I'll see if I can try to fix it in a general way, eg. by making aggregations immutable...
</comment><comment author="nickminutello" created="2014-06-12T09:34:05Z" id="45848172">Excellent. Can you let me know the one-line change so I can patch locally? (Will the making aggregations immutable take long?)
</comment><comment author="jpountz" created="2014-06-12T09:37:03Z" id="45848386">Hopefully it won't take too long but here is the one-line fix I was talking about:

``` patch
diff --git a/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java b/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
index d9c8af6..57bb1c7 100644
--- a/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
+++ b/src/main/java/org/elasticsearch/search/aggregations/bucket/range/RangeAggregator.java
@@ -208,10 +208,10 @@ public class RangeAggregator extends BucketsAggregator {

     @Override
     public InternalAggregation buildEmptyAggregation() {
-        InternalAggregations subAggs = buildEmptySubAggregations();
         List&lt;org.elasticsearch.search.aggregations.bucket.range.Range.Bucket&gt; buckets = Lists.newArrayListWithCapacity(ranges.length);
         for (int i = 0; i &lt; ranges.length; i++) {
             Range range = ranges[i];
+            InternalAggregations subAggs = buildEmptySubAggregations();
             org.elasticsearch.search.aggregations.bucket.range.Range.Bucket bucket =
                     rangeFactory.createBucket(range.key, range.from, range.to, 0, subAggs, formatter);
             buckets.add(bucket);
```
</comment><comment author="nickminutello" created="2014-06-12T09:51:21Z" id="45849554">Thanks
</comment><comment author="nickminutello" created="2014-06-17T09:10:08Z" id="46284017">Thanks! What release is this going into? 
</comment><comment author="jpountz" created="2014-06-17T14:02:52Z" id="46310409">This will be in 1.2.2 and 1.3.0.
</comment><comment author="nickminutello" created="2014-07-08T14:39:08Z" id="48345039">When is 1.2.2 scheduled for? (roughly)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the top_hits to be a metric aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6434</link><project id="" key="" /><description>Now that there is a distinction between metric aggregation and numeric aggregation, the top_hits aggregation can be a metric aggregation instead of a bucket aggregation that can't hold sub aggregations.
</description><key id="35171029">6434</key><summary>Change the top_hits to be a metric aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-06-06T18:35:56Z</created><updated>2015-05-18T23:31:32Z</updated><resolved>2014-06-10T07:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-06T23:46:54Z" id="45394634">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invoking analyzer at query time returning a parse exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6433</link><project id="" key="" /><description>We recently upgraded from 1.1.1 to 1.2.1 in our dev installation and have encountered an issue with calling analyzers at query time.

In 1.1.1 the following query:

```
GET /doc/_search
{
    "query" : {
        "match" : {
            "title" : "colorado"
        },
            "analyzer" : "simple"
    }
}
```

Worked fine. I could swap in any of the default analyzers and they all seemed to function as expected.

In 1.2.1. we're getting the following error:

```
{
   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[fk7ZFddlSwKduZNP32kjoQ][im][4]: SearchParseException[[im][4]: query[title:colorado],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\" : {\n        \"match\" : {\n            \"title\" : \"colorado\"\n        },\n            \"analyzer\" : \"simple\"\n    }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got VALUE_STRING \"analyzer\"]; }{[fk7ZFddlSwKduZNP32kjoQ][im][3]: SearchParseException[[im][3]: query[title:colorado],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\" : {\n        \"match\" : {\n            \"title\" : \"colorado\"\n        },\n            \"analyzer\" : \"simple\"\n    }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got VALUE_STRING \"analyzer\"]; }{[fk7ZFddlSwKduZNP32kjoQ][im][2]: SearchParseException[[im][2]: query[title:colorado],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\" : {\n        \"match\" : {\n            \"title\" : \"colorado\"\n        },\n            \"analyzer\" : \"simple\"\n    }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got VALUE_STRING \"analyzer\"]; }{[fk7ZFddlSwKduZNP32kjoQ][im][1]: SearchParseException[[im][1]: query[title:colorado],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\" : {\n        \"match\" : {\n            \"title\" : \"colorado\"\n        },\n            \"analyzer\" : \"simple\"\n    }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got VALUE_STRING \"analyzer\"]; }{[fk7ZFddlSwKduZNP32kjoQ][im][0]: SearchParseException[[im][0]: query[title:colorado],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n    \"query\" : {\n        \"match\" : {\n            \"title\" : \"colorado\"\n        },\n            \"analyzer\" : \"simple\"\n    }\n}\n]]]; nested: ElasticsearchParseException[Expected field name but got VALUE_STRING \"analyzer\"]; }]",
   "status": 400
}
```

(Sorry for the length).

We did not make any changes to our settings or mappings, I even installed 1.2.1 on a different machine with no settings or mappings and just loaded our content set and get the same error. A clean install of 1.1.1 works as expected. 

Any help would be greatly appreciate.
</description><key id="35162214">6433</key><summary>Invoking analyzer at query time returning a parse exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zrdunlap</reporter><labels /><created>2014-06-06T16:44:25Z</created><updated>2014-09-08T00:25:08Z</updated><resolved>2014-06-06T16:49:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-06T16:49:51Z" id="45358884">Actually, in 1.1.1 it didn't work as expected. It just ignored the invalid parameter, which is why we made query parsing stricter.  The correct syntax is:

```
GET /_search
{
  "query": {
    "match": {
      "title": {
        "query": "colorado",
        "analyzer": "simple"
      }
    }
  }
}
```
</comment><comment author="zrdunlap" created="2014-06-06T16:56:18Z" id="45359544">Thank you very much for the speedy response! So glad it works now!

I had tried this:

```
GET /doc/_search
{
    "query" : {
        "match" : {
            "title" : "colorado",
            "analyzer" : "simple"
        }
    }
}
```

But clearly I was missing some brackets. (And should have said clearly the structure is different as you pointed out below) They need a voting mechanism on GitHub like they have on StackOverflow. +1 to you sir.
</comment><comment author="clintongormley" created="2014-06-06T16:59:40Z" id="45359913">... and the change in structure.  note the addition level with `query`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added percentile rank aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6432</link><project id="" key="" /><description>Percentile Rank Aggregation is the reverse of the Percentiles aggregation.  It determines the percentile rank (the proportion of values less than a given value) of the provided array of values.

Closes #6386
</description><key id="35150128">6432</key><summary>Added percentile rank aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>feature</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-06T14:28:18Z</created><updated>2015-06-06T18:38:40Z</updated><resolved>2014-06-18T11:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-06-06T14:29:04Z" id="45342582">Not completely sure on the documentation here as it duplicates some of the stuff in the percentiles aggregation docs.  Might be a better way of doing this?
</comment><comment author="jpountz" created="2014-06-07T00:07:20Z" id="45395523">I left some minor comments but it looks very good to me in general. I like how you managed to share code with the percentiles aggregation.
</comment><comment author="jpountz" created="2014-06-11T18:38:43Z" id="45781938">This looks good to me.
@uboness would you mind giving it a look before we merge it in?
</comment><comment author="uboness" created="2014-06-17T09:47:14Z" id="46287215">left a few comments/questions... looks good in general
</comment><comment author="uboness" created="2014-06-18T10:25:01Z" id="46418807">LGTM
</comment><comment author="kimchy" created="2014-06-18T10:25:41Z" id="46418856">nice one!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check-and-set for aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6431</link><project id="" key="" /><description>The atomic update of aliases is very useful. However, if you have aggregate aliases that point to a lot of indexes, and these indexes are managed by a different threads/processes, you risk running into race conditions on updates.

I'd like this application flow:
- Read which aliases to update
- Issue the correct deletes and adds, _given the current situation is as expected_
- Full _rollback_, and the ability to retry if the situation had changed from what we asserted

This would remove the need to synchronize complex alias operations in a multi-thread environment.

In the Java API I'd wish for something like:
- .addAlias(index, alias)
- .removeAlias(index, alias)
- _.assertAlias(index, alias)_  or similar
</description><key id="35148313">6431</key><summary>Check-and-set for aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnhaug</reporter><labels><label>:Aliases</label><label>discuss</label></labels><created>2014-06-06T14:06:56Z</created><updated>2015-11-21T13:34:46Z</updated><resolved>2015-11-21T13:34:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:40:04Z" id="68389618">Hi @magnhaug 

Sorry it's taken a while to get to this.  What specific situations are you trying to avoid here?  An alias can be added to multiple indices, and index can have multiple aliases... exactly what differences would cause an assertion failure?
</comment><comment author="magnhaug" created="2015-01-04T01:00:58Z" id="68616258">@clintongormley Ok, I'll try to describe a quick example. Let's say:
- A new datestamped index is created every day by process 1. The "current data"-alias is set up with a few alias filters.
- Another process (process 2) periodically lists out the active aliases and writes them back with updated alias filters, when new sensitivity restrictions on the data should apply.

If these two processes work at the same time, process 2 might read soon-to-be-outdated aliases, and thusly re-binding the alias to an index that was deprecated by process 1.
Given that process 2 could atomically assert that the aliases it is updating are still valid, it could nicely retry instead of harmfully adding aliases pointing to old/stale data.

Our real world example is a bit more involved, but the application flow is similar to this example.
Our workaround has been to let "process 1" and "process 2" compete for a distributed lock, but this introduces the need for new components and synchronization points in the architecture.
</comment><comment author="clintongormley" created="2015-01-05T11:37:30Z" id="68697358">Hi @magnhaug 

Thanks for the info.  I'll put this issue up for discussion, but my feeling is that it is the wrong approach.  Once you start with assertions like this, then where does it end?  Should we also assert on routing values, or alias filters?  What about other indices that an alias might point to?  It starts getting really messy.

Typically updating aliases is a very light-weight operation, so serialising requests application side is pretty easy.  Also, I'm working on a proposal for a more event-driven framework which will allow you to specify actions to be taken on certain cluster events (eg the creation of a new index).  i think this will be a cleaner solution than the assertions.
</comment><comment author="magnhaug" created="2015-01-05T11:54:01Z" id="68698721">Valid points. I agree that specifying actions to be taken in order sounds cleaner from a client-perspective. Especially if they can be more fine-grained, like replacing a named alias filter or changing a routing value on an alias.
</comment><comment author="clintongormley" created="2015-11-21T13:34:46Z" id="158641109">This ticket hasn't garnered any further interest, so I'm going to close it with the advice to serialize alias changes client side, where custom logic can easily be controlled.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stuck on shard recovery, NPE in _recovery API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6430</link><project id="" key="" /><description>I'm running a cluster of 5 nodes. After a normal reboot, recovery of certain shards started as normally. However, one of the replica shards doing recovery stopped when it had reached a shard size of about 233MB, out of the 1.4GB total shard size on the primary shard.

Here's what I've found out so far:
- Shards might stop mid-recovery on any size. 0 bytes, a few KB, or several GB.
- Shards stopped mid-recovery might even report a higher size_in_bytes than the primary shard.
- Shards of any size might experience this (smalles index: 2 very small documents, biggest: millions of large documents).
- The translog was empty for all shards on the sick node.
- Accessing the `_recovery` API on any node in the cluster produces a `{"error":"NullPointerException[null]","status":500}` (verified multiple times). No NPE in a healthy cluster.
- Even with a log level of TRACE there's no info in the logs, even after the NPE.
- Waiting for hours does not seem to fix anything.
- Rebooting the node with the shard stuck on recovery fixes it (another node tries, and most often succeeds).
- Observed on ElasticSearch 1.1.1 and 1.1.2 on RHEL 6.5 with Sun Java 1.7.0_55. Not observed in the same cluster configuration when running 0.90.x or 0.20.x.

Output from `_cat/recovery/` on my index:

```
curl "tsl0mag19:2500/_cat/recovery/meta"
meta 0 2332847 replica init n/a                tsl0mag15.skead.no n/a n/a 0 0.0%   0    0.0%
meta 0 17687   replica done tsl0mag16.skead.no tsl0mag18.skead.no n/a n/a 1 100.0% 7286 100.0%
meta 0 3634    replica done tsl0mag19.skead.no tsl0mag16.skead.no n/a n/a 9 100.0% 7286 100.0%
```

The segments have not yet been created/registered:

```
curl "tsl0mag19:2500/_cat/segments/meta"
meta 0 r 151.187.99.218 _7lhu 354450 1 3 3.6kb 443 true  true  4.7 true
meta 0 r 151.187.99.218 _ajjy 491902 4 3 3.4kb   0 true  false 4.7 true
meta 0 r 151.187.99.218 _ajp5 492089 1 2 3.3kb 442 false true  4.7 true
meta 0 p 151.187.99.216 _7lhu 354450 1 3 3.6kb 443 true  true  4.7 true
meta 0 p 151.187.99.216 _ajk4 491908 1 0 2.9kb   0 true  false 4.7 true
meta 0 p 151.187.99.216 _ajpm 492106 1 0 2.9kb 442 false true  4.7 true
```

Here is the stack trace from the ElasticSearch process that should be recovering this shard:
https://gist.github.com/magnhaug/11fa5750fe76a6adca4b

Here are the contents from the `indices` folder:
https://dl.dropboxusercontent.com/u/233260280/unhealthy_shard.tar.gz (problematic shard)
https://dl.dropboxusercontent.com/u/233260280/healthy_shard.tar.gz (primary shard, for reference)

This is how a sample stuck shard looks in HEAD:

```
{
  routing: {
    state: INITIALIZING
    primary: false
    node: 85EJGc_1RjyZMBuc2QluCw
    relocating_node: null
    shard: 0
  i  ndex: meta
}
  state: RECOVERING
  index: {
    size_in_bytes: 3566
  }
}
```

This was recovering from the following primary shard:

```
{
  routing: {
    state: STARTED
    primary: true
    node: ZrYUKHXZSFaftExxueYo3w
    relocating_node: null
    shard: 0
    index: meta
}
  state: STARTED
  index: {
    size_in_bytes: 10489
  }
  translog: {
    id: 1401785020764
    operations: 805
  }
  docs: {
    num_docs: 2
    max_doc: 5
    deleted_docs: 3
  }
  merges: {
    current: 0
    current_docs: 0
    current_size_in_bytes: 0
    total: 0
    total_time_in_millis: 0
    total_docs: 0
    total_size_in_bytes: 0
  }
  refresh: {
    total: 263
    total_time_in_millis: 2145
  }
  flush: {
    total: 0
    total_time_in_millis: 0
  }
}
```
</description><key id="35146871">6430</key><summary>Stuck on shard recovery, NPE in _recovery API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">magnhaug</reporter><labels><label>feedback_needed</label></labels><created>2014-06-06T13:47:55Z</created><updated>2014-10-15T11:39:19Z</updated><resolved>2014-10-15T11:39:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-07-07T09:49:45Z" id="48159474">I don't know why the shard recovery got stuck here. The NPE you saw in the recovery api is likely to get fixed via: https://github.com/elasticsearch/elasticsearch/pull/6190
</comment><comment author="magnhaug" created="2014-09-01T17:39:49Z" id="54081297">@s1monw  I'm guessing this is fixed through #6808?
The only difference I see between this bug report and #6808 is that I've seen this happen mid-recovery, when the replica shard has reached a certain size.
</comment><comment author="s1monw" created="2014-09-01T18:43:35Z" id="54084724">@magnhaug I could totally buy that this is fixed by #6808 it's at least the same symptoms. I'd vote for closing this! 
</comment><comment author="magnhaug" created="2014-10-06T06:18:53Z" id="57978171">I'll close this when we've run 1.3.x for a while and not seen any problems with recovery.
</comment><comment author="clintongormley" created="2014-10-15T11:39:19Z" id="59192849">Closing - please reopen is you see the problem recur.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unified PUT/POST behaviour in relation to create parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6429</link><project id="" key="" /><description>The put index template api supports the `create` parameter (defaults to false), which tells whether the template can replace an existing one with same name or not. When invoked through REST using the POST method, the `create` is currently forced to `true`, which is not consistent with our other apis.

Unified its behaviour between PUT and POST method. Also added create parameter to the rest spec (was missing before) and a REST test for create true scenario.
</description><key id="35145753">6429</key><summary>Unified PUT/POST behaviour in relation to create parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index Templates</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-06T13:34:15Z</created><updated>2015-06-07T13:16:29Z</updated><resolved>2014-06-06T14:20:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-06T13:39:18Z" id="45337224">@javanna 

Looks good, except for the 409.  See discussion here #6264.  I'm still undecided about whether to use 409s for things other than doc versions...
</comment><comment author="javanna" created="2014-06-06T13:39:56Z" id="45337281">Cool, makes sense, are you ok with the original 400 then? Or can you come up with a better one?
</comment><comment author="clintongormley" created="2014-06-06T13:45:51Z" id="45337872">I can't come up with a better one, so I'd stick with 400 (at least until we make a decision on #6264)
</comment><comment author="javanna" created="2014-06-06T13:51:24Z" id="45338477">Ok, updated the PR, reverted to use 400 as status code in case of duplicates.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripts should have a "cache" they can work with</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6428</link><project id="" key="" /><description>It'd be useful if scripts could dump arbitrary values into a string keyed map that is reused across executions of the script.  In the case of function scoring you'd want it reused for all the scored documents on the same shard.  For script filters you'd want it to reused for all filtered documents on the shard.
</description><key id="35145600">6428</key><summary>Scripts should have a "cache" they can work with</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Scripting</label><label>discuss</label></labels><created>2014-06-06T13:32:15Z</created><updated>2015-11-21T13:32:28Z</updated><resolved>2015-11-21T13:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-06T13:33:19Z" id="45336627">@brwe, we talked about this with regards to hand rolled scoring a few weeks ago.  I just encountered a use for it myself with costly (to set up) script filters.
</comment><comment author="nik9000" created="2014-06-11T20:11:40Z" id="45793119">So it turns out that, at  least in MVEL, scripts do have a cache they can work with.  This is me using it to cache a compiled regex that I want to run against the source:

``` js
         "filter": {
            "script": {
               "script": "import org.apache.lucene.util.automaton.*;source_text = _source.get(\"source_text\");if (source_text == null) {false} else {if (automaton == null) {automaton = new CharacterRunAutomaton(new RegExp(pattern).toAutomaton());}automaton.run(source_text)}",
               "params": {
                  "pattern": ".*(catapult).*",
                  "automaton": null      &lt;--------- This creates a _writable_ slot 
               }
            }
         }
```

Its not fast because I'm loading source everywhere.  But it is 10x faster then compiling the regex on every script execution.

@dakrone, would a horrible hack like this survive the groovy switch?
</comment><comment author="dakrone" created="2014-06-27T09:44:27Z" id="47325658">@nik9000 just want you to know I've seen this, once we've hammered out some of the initial Groovy issues I'll do some testing with this to see whether it is possible/feasible.
</comment><comment author="alexsv" created="2014-08-20T12:47:15Z" id="52772393">Currently I'm solving some groovy issue - it's good to provide some sort of 'initialization' of the script, for example the place to put class definition, import, object creation and in 'script' only call this object method. This could solve many problems. Also It's good to support _scripts index as well.
</comment><comment author="shadow000fire" created="2014-10-30T19:19:37Z" id="61154121">Would this be specific to non-Java scripts a guess? I assume a native script could simple keep a static variable that would serve the purpose of existing across executions.
</comment><comment author="javanna" created="2015-03-19T18:58:09Z" id="83718299">@dakrone did you make any progress on this? I am curious on what you had in mind, and whether it is still doable or not.
</comment><comment author="nik9000" created="2015-03-19T19:09:42Z" id="83720641">@shadow000fire, a static variable would work pretty poorly given that it could be run on multiple requests across multiple threads.  A ThreadLocal would have leakage issues, I think.  OTOH a native script might just be able to use a class variable - I haven't checked but I imagine the class is built once per request.  There _should_ be something for native scripts though.

@javanna, the writable slot trick in groovy _works_ but its a little bit horrible.  I imagine expression language wouldn't be able to do things like assign variables.....
</comment><comment author="dakrone" created="2015-03-19T21:37:45Z" id="83769145">@javanna I haven't made any progress on this yet, feel free to grab it :)
</comment><comment author="javanna" created="2015-03-20T08:04:25Z" id="83945872">I marked this for discussion, I am personally not sure we should support something like this, it sounds scary, but maybe it's just me. Also, if we do we need to find a way to make it work for all of the different engines.
</comment><comment author="nik9000" created="2015-03-20T15:49:45Z" id="84053640">&gt; I marked this for discussion, I am personally not sure we should support something like this, it sounds scary, but maybe it's just me.
&gt; I suppose it'd give you some way to consume a ton of memory.  I was just using it for holding precocmpiled automata.  It only had to work across a single request.  Its the same sort of attack vector as creating a huge collection in groovy in a while true loop.
&gt; 
&gt; Also, if we do we need to find a way to make it work for all of the different engines.
&gt; Maybe its only a feature in one.  I dunno.  We're probably going to abandon scripts pretty soon, especially if we can get fast rolling restarts from index sealing.  In that case it doesn't make a huge difference for me.

This, btw, is why I stopped submitting patches for the builtin highlighters - it because a huge pain to implement everything three times with three different mindsets.
</comment><comment author="s1monw" created="2015-03-20T15:50:58Z" id="84053876">&gt; This, btw, is why I stopped submitting patches for the builtin highlighters - it because a huge pain to implement everything three times with three different mindsets.

you really read my mind... the highlighter situation is a nightmare and we have to fix this one way or the other!
</comment><comment author="clintongormley" created="2015-11-21T13:32:28Z" id="158640935">This is too risky to expose with generic script engines.  If you want a cache in scripts then I think a native script is the only way to go here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Flickering on "Events over time" histogram</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6427</link><project id="" key="" /><description>On the default Logstash dashboard "Events Over Time" panel, say I have it showing events from the last 2 days, when I set auto-refresh on I get this weird flickering each time the histogram refreshes. At the actual point it refreshes, it very briefly will show only the data from the current day, and then instantly goes back to shown all the data (i.e. the current day and the previous day), until it refreshes again.

If I delete all indexes other than the current day, the issue goes away. Until tomorrow when I'm trying to plot data from more than one index on the histogram!
</description><key id="35140158">6427</key><summary>Flickering on "Events over time" histogram</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpendle</reporter><labels /><created>2014-06-06T12:07:39Z</created><updated>2014-12-30T19:34:59Z</updated><resolved>2014-12-30T19:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:34:59Z" id="68389125">Hi @jpendle 

This sounds like an issue for Kibana, rathern than Elasticsearch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Keep getting VersionConflictEngineException with the latest version [1.2.1]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6426</link><project id="" key="" /><description>I have a quite simple index of product, I was using the older version 1.1.2 but decided to swtich to 1.2.1 and then starting to get the following issues everything I try to index new items:

```
 index: /myindex/product/base2_1 caused VersionConflictEngineException[[myindex][1] [product][base2_1]: version conflict, current [2], provided [-3]]
 index: /myindex/product/base2_3 caused VersionConflictEngineException[[myindex][3] [product][base2_3]: version conflict, current [2], provided [-3]]
 index: /myindex/product/base2_6 caused VersionConflictEngineException[[myindex][1] [product][base2_6]: version conflict, current [2], provided [-3]]
 index: /myindex/product/base2_8 caused VersionConflictEngineException[[myindex][3] [product][base2_8]: version conflict, current [2], provided [-3]]
 index: /myindex/product/base2_10 caused VersionConflictEngineException[[myindex][3] [product][base2_10]: version conflict, current [2], provided [-3]]
```

My mapping:

https://gist.github.com/yellow1912/b272ca6b6c88bdfb243c

My data index:

https://gist.github.com/yellow1912/a1ea00137e0b7c0106fa

The same data above works fine in older version.
</description><key id="35136894">6426</key><summary>Keep getting VersionConflictEngineException with the latest version [1.2.1]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">yellow1912</reporter><labels /><created>2014-06-06T11:04:52Z</created><updated>2014-10-06T12:22:46Z</updated><resolved>2014-10-06T12:22:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-06T11:09:37Z" id="45325755">Hiya

I just tried using your mapping and indexing those documents and it all worked just fine.  There is something happening that does not appear in your examples.  For some reason, you're setting a version number of -3 somewhere.
</comment><comment author="yellow1912" created="2014-06-06T13:36:50Z" id="45336986">Very very weird, the same code but I turned on the old version of ES and it goes in fine. The 1.2.1 version I uses is compiled from code if that makes any difference? 

Also, I'm using FOSElastica bundle (symfony), but I think this should not affect anything because otherwise the old version should not work? I don't set version anywhere. The data I pasted above is the dump of the data directly before I post to ES. 

Is there anyway to debug what ES receives on its end?
</comment><comment author="clintongormley" created="2014-06-06T17:02:54Z" id="45360234">&gt; The 1.2.1 version I uses is compiled from code if that makes any difference?

it shouldn't do, but who knows - maybe try the released version instead?

&gt; Also, I'm using FOSElastica bundle (symfony), but I think this should not affect anything because otherwise the old version should not work? I don't set version anywhere. 

I had a look at the FOSElastica code and I don't see them setting versions anywhere either, yet the error message clearly says that you specified version 2. (And no idea where it got version -3 from!)

&gt; Is there anyway to debug what ES receives on its end?

No, but perhaps if you turn on debugging in FOSElastica? It seems to indicate that it'll do query logging then.

This does sound extremely weird - who knows, it may well be some issue with the compiled version.
</comment><comment author="bleskes" created="2014-06-11T07:38:05Z" id="45710907">Hi there,

This smells of something that has to do with https://github.com/elasticsearch/elasticsearch/pull/6149  (-3 is the new value for MATCH_ANY, which is the default and should not conflict with anything). Can it be that that one of your nodes is on an older ES version? 

To help investigating further: can you elaborate more about your setup? are you using client nodes? is all of your cluster on the same ES version? I assume the gists are just a reproduction of the query generated by FOSElastica, are you sure they are accurate?

Cheers,
Boaz
</comment><comment author="yellow1912" created="2014-10-06T12:22:46Z" id="58009219">I no longer get this with latest version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FileSystem: Use XNativeFSLockFactory instead of the buggy Lucene 4.8.1 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6425</link><project id="" key="" /><description>There is a pretty nasty bug in the lock factory we use that can cause
nodes to use the same data dir wiping each others data. Luckily this is
unlikely to happen if the nodes are running in different JVM which they
do unless they are embedded.

See LUCENE-5738

Closes #6424
</description><key id="35131907">6425</key><summary>FileSystem: Use XNativeFSLockFactory instead of the buggy Lucene 4.8.1 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-06-06T09:39:36Z</created><updated>2014-06-12T10:59:36Z</updated><resolved>2014-06-06T09:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use XNativeFSLockFactory instead of the buggy Lucene 4.8.1 version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6424</link><project id="" key="" /><description>There is a pretty nasty [bug](https://issues.apache.org/jira/browse/LUCENE-5738) in the lock factory we use that can cause nodes to use the same data dir wiping each others data. Luckily this is unlikely to happen if the nodes are running in different JVM which they do unless they are embedded. We should place the fix for this as an X-Class until the fix is released.

yet this might have other side-effects that we haven't uncovered yet so I mark it critical
</description><key id="35131728">6424</key><summary>Use XNativeFSLockFactory instead of the buggy Lucene 4.8.1 version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>critical</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-06T09:36:45Z</created><updated>2015-06-07T19:47:05Z</updated><resolved>2014-06-06T09:57:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-06-06T09:44:55Z" id="45319886">+1, looks great
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Suppress "Unsafe is an internal proprietary API..." compilation warnings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6423</link><project id="" key="" /><description>We use Unsafe in a number of places, and #6400 (pulling in CHMV8) just made it worse.

It looks like it's possible to suppress this warning: http://stackoverflow.com/questions/19553336/suppress-java-warning-unsafe-is-internal-proprietary-api-in-groovy-project-bui
</description><key id="35131144">6423</key><summary>Internal: Suppress "Unsafe is an internal proprietary API..." compilation warnings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>build</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-06T09:27:13Z</created><updated>2015-06-07T13:18:23Z</updated><resolved>2014-06-06T09:46:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-06T09:32:09Z" id="45318992">+1
</comment><comment author="s1monw" created="2014-06-06T09:32:50Z" id="45319056">this should do it:

``` Diff
diff --git a/pom.xml b/pom.xml
index c93c01a..3388ed9 100644
--- a/pom.xml
+++ b/pom.xml
@@ -398,6 +398,9 @@
                          be fixed in version &gt; 3.1
                      --&gt;
                     &lt;useIncrementalCompilation&gt;false&lt;/useIncrementalCompilation&gt;
+                    &lt;compilerArgs&gt;
+                        &lt;arg&gt;-XDignore.symbol.file&lt;/arg&gt;
+                    &lt;/compilerArgs&gt;
                 &lt;/configuration&gt;
             &lt;/plugin&gt;
             &lt;plugin&gt;
```

can you make the change?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>config file discrepencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6422</link><project id="" key="" /><description>Why state 1g max, then set the size to 2g below?

 elasticsearch / src / rpm / sysconfig / elasticsearch 

```
# Heap Size (defaults to 256m min, 1g max)
#ES_HEAP_SIZE=2g
```
</description><key id="35119625">6422</key><summary>config file discrepencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drocsid</reporter><labels /><created>2014-06-06T05:19:19Z</created><updated>2014-06-06T06:18:56Z</updated><resolved>2014-06-06T06:18:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-06T06:18:24Z" id="45306257">The mailing list is definitely a better place to ask for questions.
We use this space for issues and feature requests.
See help for details: http://www.elasticsearch.org/help/

That said, this comment line means that by default, the heap size will be set to 256m / 1g min/max heap size. But you you can set it to any other value (recommended) and for example, you can start the JVM with 2g min/max heap size.

Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update post-filter.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6421</link><project id="" key="" /><description>Remove 'be' where it is not needed
</description><key id="35115053">6421</key><summary>Update post-filter.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lfender6445</reporter><labels><label>docs</label></labels><created>2014-06-06T02:46:21Z</created><updated>2014-06-12T10:09:41Z</updated><resolved>2014-06-12T10:09:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-06T07:54:52Z" id="45311776">Makes sense, thanks @lfender6445 ! Can you please sign our CLA so we can merge this in? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="lfender6445" created="2014-06-06T14:11:27Z" id="45340606">Updated
</comment><comment author="javanna" created="2014-06-12T10:09:41Z" id="45851087">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>highlighting doesn't work for nested queries, even with _source enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6420</link><project id="" key="" /><description>Even with _source available, highlighting doesn't work for nested queries. Here, the first two search requests fail to produce highlights, although interestingly, the last one succeeds:

https://gist.github.com/jtibshirani/3f6baeb7f306e92ca44e

It seems the nested fields are correctly extracted from the _source, but when the highlight_query is run against them, nothing matches. Un-nesting the highlight_query actually fixes the issue, which suggests there's something wrong with how the nested query is being parsed.

This issue reproduced on ES 1.2.1.
</description><key id="35108998">6420</key><summary>highlighting doesn't work for nested queries, even with _source enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jtibshirani</reporter><labels><label>:Highlighting</label></labels><created>2014-06-06T00:01:46Z</created><updated>2014-11-24T21:43:45Z</updated><resolved>2014-11-24T21:43:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-09-06T13:19:55Z" id="54712086">Also see #5245
</comment><comment author="jtibshirani" created="2014-11-07T19:51:45Z" id="62201948">This turned out to be an issue within Lucene, fixed here:

https://issues.apache.org/jira/browse/LUCENE-5929
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support minimum value in cardinality aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6419</link><project id="" key="" /><description>Initially suggested at http://elasticsearch-users.115913.n3.nabble.com/filtering-avec-cardinality-td4055450.html

Something like:

```
         "aggregations": {
                "agg_distinct_ip":{
                    "cardinality": {
                        "field" : "ip_client",
                        "min_value": 5 
                    },
                },
            }
```
</description><key id="35086797">6419</key><summary>Support minimum value in cardinality aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">loren</reporter><labels /><created>2014-06-05T19:02:21Z</created><updated>2014-11-17T16:51:00Z</updated><resolved>2014-11-17T16:51:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-11-17T16:51:00Z" id="63335252">This is something that could not be addressed at the shard level since we would need to know how many unique ips a particular term got on other shards in order to decide on whether or not to include a term. So this could only happen during the reduce phase and could be addressed by #8110 .

But I think a better way to address your needs might be to sort terms based on the value returned by the cardinality aggregation in order to get users who have the highest number of unique ips first? http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#search-aggregations-bucket-terms-aggregation-order
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow fine-grained script settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6418</link><project id="" key="" /><description>Would be good to globally disallow scripting based on the type of operation being done. 

In particular I want to prevent scripting on queries but allow scripting in update/index operations.

Or maybe just separate this by which thread pool it is running in?

Related to #5943 
</description><key id="35084595">6418</key><summary>Allow fine-grained script settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gibrown</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-05T18:36:43Z</created><updated>2015-03-26T18:57:39Z</updated><resolved>2015-03-26T18:57:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:34:08Z" id="68389055">Hi @gibrown 

What's your use case for this?
</comment><comment author="gibrown" created="2014-12-30T19:47:31Z" id="68390289">We don't allow (or want) any scripting on queries. We have a lot of internal and external users creating queries, and I don't trust running scripts in any of the queries. We try and prevent that in our client code, but given how easily you could bring down a cluster with a script I'd prefer them to be completely disabled for queries.

But for update operations scripting is extremely necessary. The update API is mostly useless with scripting turned off, and using the update API is necessary to scale our indexing.

So I think it would beneficial to individually configure which endpoints can accept scripts.
</comment><comment author="clintongormley" created="2014-12-31T12:01:56Z" id="68438158">@gibrown thanks for the info.  These script settings also relate to the script source ie #7147, and sandboxed vs enabled vs disabled.

For instance, you may want to allow sandboxed dynamic scripting for updates, but only predefined indexed scripts for search (but without sandboxing). Wondering if we need to support general settings (for ease of use) and more specific settings (for flexible control).  There are also differences between scripting engines that make some safer than others, regardless of sandboxing. For instance, even though groovy is sandboxed, a simple `while (true) {}` will run forever.  Expressions, on the other hand, doesn't have loops and so doesn't suffer from that issue.

The settings need to cover the following combinations
- mode:  enabled, disabled, sandboxed
- source:  indexed, dynamic, file
- engine: groovy, expressions, mustache, etc
- operation: update, search, aggs

I propose the following specific settings, which is what the code will use to decide whether to allow a script or not:

```
script.engine.groovy.indexed.update:    sandbox/enable/disable
script.engine.groovy.indexed.search:    sandbox/enable/disable
script.engine.groovy.indexed.aggs:      sandbox/enable/disable
script.engine.groovy.dynamic.update:    sandbox/enable/disable
script.engine.groovy.dynamic.search:    sandbox/enable/disable
script.engine.groovy.dynamic.aggs:      sandbox/enable/disable
script.engine.groovy.file.update:       sandbox/enable/disable
script.engine.groovy.file.search:       sandbox/enable/disable
script.engine.groovy.file.aggs:         sandbox/enable/disable
```

For ease of use, we should also support the following more generic settings:

```
script.all:     sandbox/enable/disable  (default: `sandbox`)

script.indexed: sandbox/enable/disable
script.dynamic: sandbox/enable/disable
script.file:    sandbox/enable/disable

script.update:  sandbox/enable/disable
script.search:  sandbox/enable/disable
script.aggs:    sandbox/enable/disable
```

These will be used to calculate the more specific settings, using the stricter setting of each combination, eg:

```
script.all:                             sandbox
script.dynamic:                         disable
script.search:                          disable
script.engine.groovy.file.search:       enable
```

will result in the following specific settings:

```
script.engine.groovy.indexed.update:    sandbox
script.engine.groovy.indexed.search:    disable
script.engine.groovy.indexed.aggs:      sandbox
script.engine.groovy.dynamic.update:    disable
script.engine.groovy.dynamic.search:    disable
script.engine.groovy.dynamic.aggs:      disable
script.engine.groovy.file.update:       sandbox
script.engine.groovy.file.search:       enable
script.engine.groovy.file.aggs:         sandbox
```
</comment><comment author="gibrown" created="2014-12-31T21:05:52Z" id="68469181">Those sound great to me. Would definitely work for my case.
</comment><comment author="s1monw" created="2015-02-27T12:04:55Z" id="76384887">I removed the blocker label here - it's not really a blocker...
</comment><comment author="javanna" created="2015-02-27T14:11:42Z" id="76399739">I am looking into this. I think that the default should not just be `script.all: sandbox` otherwise groovy file scripts cannot be run by default, which is kinda annoying and breaks bw comp. To be honest I am leaning towards removing the `script.all` setting for simplicity in favour of explicit configuration of the three generic settings, which would have the following defaults:

```
script.file: enable
script.indexed: sandbox
script.dynamic: sandbox
```

Thoughts?
</comment><comment author="clintongormley" created="2015-03-03T12:51:47Z" id="76939601">+1
</comment><comment author="javanna" created="2015-03-03T13:05:39Z" id="76941514">As for the more generic settings, I am wondering about conflicts. Of course more specific settings take precedence. But which one takes precedence between conflicting script_type based setting and operation based setting?

Let's say we have:

```
script.dynamic: enable
script.search: disable
```

Should it get expanded to the following?

```
script.engine.groovy.indexed.update:  sandbox
script.engine.groovy.indexed.search: disable
script.engine.groovy.indexed.aggs: sandbox
script.engine.groovy.dynamic.update: enable
script.engine.groovy.dynamic.search: disable
script.engine.groovy.dynamic.aggs: enable
script.engine.groovy.file.update: sandbox
script.engine.groovy.file.search: disable
script.engine.groovy.file.aggs: sandbox
```

If so, it means that operation based settings take precedence over script_type based settings. Maybe it's obvious, but I thought it was worth to double check what folks think.
</comment><comment author="clintongormley" created="2015-03-03T18:54:36Z" id="77009489">Yes, I think the order should be: lang &gt; source &gt; operation
</comment><comment author="javanna" created="2015-03-05T18:04:01Z" id="77415498">Just noticed that `mustache` and `native` are treated as special scripting engines since they are considered safer than others. Turns out that currently we always considered them both enabled, even if dynamic scripting is completely off.

That means that our default settings should be different only for `mustache` and `native`, they would have everything enabled by default.

We could have a problem though with generic settings as they would end up affecting `mustache` and `native` too. Think of `script.dynamic` set to `false`. Should we still allow `mustache` and `native` scripts anyways, which would mean not applying those generic settings to them? But maybe make it possible to set their settings only through engine specific settings? Thoughts?
</comment><comment author="rjernst" created="2015-03-05T19:03:51Z" id="77428007">`native` scripts are by definition not dynamic.  Why would these settings ever affect them?
</comment><comment author="clintongormley" created="2015-03-05T19:04:08Z" id="77428107">&gt; Think of script.dynamic set to false. Should we still allow mustache and native scripts anyways, which would mean not applying those generic settings to them? But maybe make it possible to set their settings only through engine specific settings? Thoughts?

Hmm... good point. Native scripts can't be defined dynamically anyway, so it's really just templating.  The default will be that dynamic mustache and expressions are on, but everything else is off.  So I think there is no real need to set `script.dynamic: false`.  So if somebody sets it, we'd do what they say and disable mustache and expressions too.
</comment><comment author="dakrone" created="2015-03-05T19:34:25Z" id="77434888">If we do that, we also need to be extra sure to document that search templates will not work if dynamic scripting is disabled (just a reminder)
</comment><comment author="javanna" created="2015-03-06T09:07:09Z" id="77527478">&gt; native scripts are by definition not dynamic. Why would these settings ever affect them?

@rjernst good point. Weird, but the fact that native scripts are not dynamic is an implementation detail of the native engine. You execute the script as it was dynamic (providing its name as `script: my_script`), then the engine knows that what you provided is not a script but the identifier of a script that is already compiled somewhere...

&gt; Native scripts can't be defined dynamically anyway, so it's really just templating.

@clintongormley right but as I described above native scripts end up being considered dynamic although they aren't. That means that unless we treat them as special scripts they won't be executable with defaults settings, as their engine is not declared "sandboxed". I think the notion of `file`, `indexed` and `dynamic` makes little sense in the context of `native` scripts, and we should have a special case for them, like we currently have [here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/script/ScriptService.java#L511). I wonder if these fine grained settings should even apply to native scripts. By default you have no native scripts available, and if you install any via plugin, you just want to be able to execute them. Does it ever make sense to disable native scripts, especially given that these settings are static? I am leaning towards taking them out of this loop and making them always enabled regardless of settings.

As for `mustache` scripts, they are a little bit different as their engine is declared "sandboxed", thus they would run with default settings. That said they would be affected by generic settings applied, which is acceptable as @clintongormley said, we will document it as @dakrone suggested. The only concern is that this behaviour breaks backwards compatibility as at the moment you can always execute mustache scripts...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6417</link><project id="" key="" /><description /><key id="35076780">6417</key><summary>Add Javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">stephlag</reporter><labels><label>docs</label></labels><created>2014-06-05T17:02:29Z</created><updated>2014-06-22T00:20:21Z</updated><resolved>2014-06-05T17:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-05T17:18:06Z" id="45247944">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change logging level without restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6416</link><project id="" key="" /><description>It'd be nice to be able to change the logging levels without a restart just by editing the file.
</description><key id="35070498">6416</key><summary>Change logging level without restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-06-05T15:53:09Z</created><updated>2014-12-31T11:01:49Z</updated><resolved>2014-12-30T19:33:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-22T18:25:25Z" id="53101973">@nik9000 I think this covers this issue: #2517
</comment><comment author="clintongormley" created="2014-12-30T19:33:05Z" id="68388938">Closing - you can change logging levels on the fly using the API
</comment><comment author="nik9000" created="2014-12-30T19:43:00Z" id="68389887">Sorry, yeah, you can change it on the fly using the API.  I'd have preferred it rescan the file which is pretty common for other Java servers but I'll take what I can get. :)
</comment><comment author="clintongormley" created="2014-12-31T11:01:49Z" id="68435768">@nik9000 Your new mantra: "API Good.  Config Bad" :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException on previously valid mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6415</link><project id="" key="" /><description>I'm running 1.2.1 and getting the same error without include_in_all:

curl -XPUT stealth:9200/filters?pretty
{
  "error" : "MapperParsingException[mapping [pseudo_doc]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [content : {dynamic=false, properties={content={type=string}, title={type=string}, urls={type=string}, author_name={type=string}, author_id={type=string}, destination_url={type=string}, publisher={type=string}}, type=object}]]; ",
  "status" : 400
}

The only mapping is in config/mappings/_default/pseudo_doc.json:

```
{
    "content" : {
        "dynamic" : false,
        "type" : "object",
        "properties" : {
            "author_id" : {
                "type" : "string"
            },
            "author_name" : {
                "type" : "string"
            },
            "content" : {
                "type" : "string"
            },
            "title" : {
                "type" : "string"
            },
            "urls" : {
                "type" : "string"
            },
            "destination_url" : {
                "type" : "string"
            },
            "publisher" : {
                "type" : "string"
            }
        }
    }
}
```
</description><key id="35068536">6415</key><summary>MapperParsingException on previously valid mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kwloafman</reporter><labels /><created>2014-06-05T15:33:27Z</created><updated>2014-06-06T10:13:31Z</updated><resolved>2014-06-05T15:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-06T10:13:31Z" id="45322080">See https://github.com/elasticsearch/elasticsearch/issues/6304#issuecomment-45322027
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException on valid mapping file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6414</link><project id="" key="" /><description>I'm running 1.2.1 and getting a MapperParsingException with no real explanation of what's wrong.  What gives with that?

curl -XPUT stealth:9200/filters?pretty
{
  "error" : "MapperParsingException[mapping [pseudo_doc]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [content : {dynamic=false, properties={content={type=string}, title={type=string}, urls={type=string}, author_name={type=string}, author_id={type=string}, destination_url={type=string}, publisher={type=string}}, type=object}]]; ",
  "status" : 400
}

```
The only mapping is in config/mappings/_default/pseudo_doc.json:
{
    "content" : {
        "dynamic" : false,
        "type" : "object",
        "properties" : {
            "author_id" : {
                "type" : "string"
            },
            "author_name" : {
                "type" : "string"
            },
            "content" : {
                "type" : "string"
            },
            "title" : {
                "type" : "string"
            },
            "urls" : {
                "type" : "string"
            },
            "destination_url" : {
                "type" : "string"
            },
            "publisher" : {
                "type" : "string"
            }
        }
    }
}
```
</description><key id="35067791">6414</key><summary>MapperParsingException on valid mapping file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kwloafman</reporter><labels /><created>2014-06-05T15:26:05Z</created><updated>2014-07-28T20:22:27Z</updated><resolved>2014-06-05T15:47:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="josegonzalez" created="2014-06-11T20:59:43Z" id="45799230">@kwloafman did you find a solution to this problem? I'm getting similar errors on a mapping that should work on 1.2.1. It definitely works on 0.90.13, and as far as we can tell, the usage should be the same.
</comment><comment author="clintongormley" created="2014-06-12T08:53:13Z" id="45844662">@josegonzalez See https://github.com/elasticsearch/elasticsearch/issues/6304#issuecomment-45322027
</comment><comment author="AndresOsinski" created="2014-07-17T22:50:51Z" id="49376812">I'm having a similar issue: a perfectly valid type mapping is not working.

This is making ES literally useless for me.
</comment><comment author="megastef" created="2014-07-28T19:36:01Z" id="50387517">+1
</comment><comment author="clintongormley" created="2014-07-28T19:50:07Z" id="50389352">@megastef @AndresOsinski This issue is closed, and there is a solution.  If you don't provide more info (or try the solution) then we can't figure out if there is a problem or if you are doing something incorrect.
</comment><comment author="megastef" created="2014-07-28T20:16:17Z" id="50393016">Hi in my case I got some old entries in _templates (from older installation I guess). And other definitions in templates directory.
After deleteting the templates using _templates endpoint and restarting ES I was  again able to create an Index. So my issue got solved.
</comment><comment author="megastef" created="2014-07-28T20:22:27Z" id="50394050">Sorry for not specifying the errors - as I was still looking into it. Just comming back from a customer where it went wrong -  still in firefighting mode.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix cardinality aggregation when doc values field is empty</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6413</link><project id="" key="" /><description>In case a doc values field doesn't have values all the time the cardinality aggregation can fail with a ArrayIndexOutOfBoundsException.
</description><key id="35065286">6413</key><summary>Fix cardinality aggregation when doc values field is empty</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-05T15:00:53Z</created><updated>2015-06-07T19:47:20Z</updated><resolved>2014-06-10T07:16:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-06T09:44:23Z" id="45319849">LGTM
</comment><comment author="jpountz" created="2014-06-09T07:25:04Z" id="45467314">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Options context in ContextSuggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6412</link><project id="" key="" /><description>I have been using the ContextSuggester and its working fine. Is there a way to make the context optional?
</description><key id="35046793">6412</key><summary>Options context in ContextSuggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">andyofb</reporter><labels /><created>2014-06-05T11:03:04Z</created><updated>2015-11-21T13:30:27Z</updated><resolved>2015-11-21T13:30:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-16T06:26:27Z" id="46144651">Can you provide more information and give an example? Do you want to fallback to the defaults then? What should happen in this case? Why is an error message bad here?
</comment><comment author="clintongormley" created="2015-11-21T13:30:27Z" id="158640519">No more feedback. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tribe node fails on index close.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6411</link><project id="" key="" /><description>Hi, 
I have 1 tribe node, on 1 cluster with 1 master and 2 data nodes.
Here the tribe config:

```
tribe.t1.cluster.name: appliance_bu-01
tribe.t1.discovery.zen.ping.unicast.hosts: ["bu-01"]
```

The tribe operates normally when I create indices and making query requests. 
But if I close an index I will se a null pointer exception in the log file, and it stops working properly (e.g. I can see the cluster state, but some query fails).
Here a simple test:

on the master node:

```
curl -XPUT "http://localhost:9200/movies/movie/2" -d'
{
    "title": "The Godfather",
    "director": "Francis Ford Coppola",
    "year": 1972
}'
{"_index":"movies","_type":"movie","_id":"2","_version":1,"created":true}

curl -XPOST "http://localhost:9200/movies/_close"
{"acknowledged":true}
```

on the tribe node:

```
tail -f /var/log/elasticsearch/elasticsearch.log

[2014-06-05 10:53:09,539][INFO ][tribe                    ] [Lord Pumpkin] [t1] adding index [movies]
[2014-06-05 10:54:33,140][WARN ][tribe                    ] [Lord Pumpkin] failed to process [cluster event from t1, zen-disco-receive(from master [[Cornelius van Lunt][ZYRaI1BMQ0avMdyxCSMfAg][bu-01][inet[/*myipaddress*:9300]]{data=false, master=true}])]
java.lang.NullPointerException
    at org.elasticsearch.cluster.routing.RoutingTable$Builder.add(RoutingTable.java:400)
    at org.elasticsearch.tribe.TribeService$TribeClusterStateListener$1.execute(TribeService.java:297)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:309)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)

```

my ES version is the same on each node:

```
{
  "status" : 200,
  "name" : "Lord Pumpkin",
  "version" : {
    "number" : "1.2.0",
    "build_hash" : "c82387f290c21505f781c695f365d0ef4098b272",
    "build_timestamp" : "2014-05-22T12:49:13Z",
    "build_snapshot" : false,
    "lucene_version" : "4.8"
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="35039598">6411</key><summary>Tribe node fails on index close.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">zazasa</reporter><labels><label>:Tribe Node</label><label>bug</label></labels><created>2014-06-05T09:09:32Z</created><updated>2015-01-20T09:16:52Z</updated><resolved>2015-01-19T16:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sakalajuraj" created="2014-08-16T23:25:42Z" id="52408805">I have same problem on 1.3.1
</comment><comment author="smorovic" created="2014-11-11T12:24:38Z" id="62539775">+1
</comment><comment author="smorovic" created="2015-01-08T10:42:08Z" id="69163070">I managed to fix this in 1.4.2 codebase (this is unchanged in current master).
Tribe needs to check index close state and do the same as if it was deleted:

``` diff
--- bak/elasticsearch-1.4.2/src/main/java/org/elasticsearch/tribe/TribeService.java 2014-12-16 15:10:48.000000000 +0100
+++ ./elasticsearch-1.4.2/src/main/java/org/elasticsearch/tribe/TribeService.java   2015-01-07 18:15:57.000000000 +0100
@@ -259,6 +259,9 @@
                             if (tribeIndex == null) {
                                 logger.info("[{}] removing index [{}]", tribeName, index.index());
                                 removeIndex(blocks, metaData, routingTable, index);
+                            } else if (tribeIndex.state() == IndexMetaData.State.CLOSE) {
+                                logger.info("[{}] removing closed index [{}]", tribeName, index.index());
+                                removeIndex(blocks, metaData, routingTable, index);
                             } else {
                                 // always make sure to update the metadata and routing table, in case
                                 // there are changes in them (new mapping, shards moving from initializing to started)
```
</comment><comment author="clintongormley" created="2015-01-13T19:46:44Z" id="69806847">Hi @smorovic 

Thanks for looking into this.  Would you like to send a PR with your fix? Please could you also add some tests.

thanks
</comment><comment author="smorovic" created="2015-01-19T17:35:00Z" id="70531638">I was going to make a PR with a test case for this fix, but you were faster in getting it done.
Thanks for taking care of it!
</comment><comment author="javanna" created="2015-01-20T09:04:35Z" id="70623418">Hi @smorovic indeed your fix looked good, it was just a matter of testing it. Thanks anyway and sorry to hear you were working on this, I didn't get that.
</comment><comment author="smorovic" created="2015-01-20T09:16:52Z" id="70624757">Hi @javanna, no worries, I didn't do much there. I was adding a couple of lines with "prepareClose() and prepareOpen() calls in the test. Your test cases are certainly more extensive than what I was doing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk request against multiple indices fails on missing index instead of failing individual actions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6410</link><project id="" key="" /><description>Consider the following repro use case:

``` json
PUT /bulkindex1
PUT /bulkindex2
POST /_bulk
{"index":{"_id":"1","_type":"index1_type","_index":"bulkindex1"}}
{"text": "hallo1" }
{"index":{"_id":"1","_type":"index2_type","_index":"bulkindex2"}}
{"text": "hallo2" }
GET /bulkindex*/_search
POST /bulkindex2/_close
POST /_bulk
{"index":{"_id":"1","_type":"index1_type","_index":"bulkindex1"}}
{"text": "hallo1-update" }
{"index":{"_id":"1","_type":"index2_type","_index":"bulkindex2"}}
{"text": "hallo2" }
```

The 2nd bulk action will certainly fail since bulkindex2 is closed.    However, when the _bulk request is submitted, ES fails the entire _bulk request:

``` json
{
   "error": "IndexMissingException[[bulkindex2] missing]",
   "status": 404
}
```

Expected behavior in this case is for ES to still process the request against the other index that is available and report the 404 as part of the response for the action against bulkindex2 like the following:

``` json
{
   "took": 14,
   "errors": true,
   "items": [
      {
         "index": {
            "_index": "bulkindex1",
            "_type": "index1_type",
            "_id": "1",
            "_version": 3,
            "status": 200
         }
      },
      {
         "index": {
            "_index": "bulkindex2",
            "_type": "index2_type",
            "_id": "1",
            "status": 404,
   "error": "IndexMissingException[[bulkindex2] missing]",
         }
      }
   ]
}
```
</description><key id="35012282">6410</key><summary>Bulk request against multiple indices fails on missing index instead of failing individual actions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">ppf2</reporter><labels><label>blocker</label><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-04T22:46:59Z</created><updated>2014-09-19T08:57:18Z</updated><resolved>2014-09-19T08:57:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tmkujala" created="2014-06-12T17:00:13Z" id="45918900">This issue was giving me some trouble as well, saw it was fixed in the 1.1 and 1.2 branches, see #4987.
</comment><comment author="TheLudd" created="2014-07-08T10:02:23Z" id="48298353">@tmkujala This bug is not fixed. It is not the same as #4987. That one tackles _invalid_ index names. This one talks about performing bulk operations against indices that does not exits and where the response would be 404. This of course only occurs if you have the option `action.auto_create_index` set to false. But I experience this error as well.
</comment><comment author="clintongormley" created="2014-07-08T10:11:19Z" id="48309664">@spinscale I've confirmed that this bug still exists in master.
</comment><comment author="TheLudd" created="2014-07-09T07:32:02Z" id="48438343">@clintongormley @spinscale 
I have seen a few similar issues like #4987 which had the same effect but was caused by a different error. Is there any reason why there are checks for specific errors in the bulk items and the errors that do not match those cause the full request to fail?

Right now I index my documents in elasticsearch one by one but I am planing on queuing up our incoming data in redis and then bulk it in to optimize performance. The errors that could occur when those bulks are indexed cannot be known but I was hoping that if one index fails the others will not. Can I trust this to be the case?
</comment><comment author="bytenik" created="2014-08-04T14:01:20Z" id="51063359">Has any progress been made on this? Its a huge performance show-stopper for my company.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Search Cluster becomes unresponsive </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6409</link><project id="" key="" /><description>Hi,

I have been facing this issue for couple of days. Here are the details -

I have a cluster of 7 nodes, with 3 master eligible with no data and 4 only data nodes. I have set minimum master node property to 2. I have used KOPF/ElasticHQ/Head for monitoring. I have set up the cluster for testing so there is hardly any querying to going on. So it remains idle most of the time. After few hours of inactivity, the plugins cannot show the any details except for the green status (no other details like shard etc shown).  If I make any search query, it does not work. 

e.g. http://ironman:9200/_search?q=chrome

I have to restart the master's elasticsearch service to make it responsive again. I checked the  $ tail -f /var/log/elasticsearch/avengers.log I don't see any failure/exceptions. I had elastic search 1.1 and updated to 1.2 as well. But I am still having this issue. 

Please let me know if you need more details.

Thanks,
Nilesh.
</description><key id="34991747">6409</key><summary>Elastic Search Cluster becomes unresponsive </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nimahajan</reporter><labels><label>feedback_needed</label></labels><created>2014-06-04T19:04:28Z</created><updated>2014-10-29T14:09:48Z</updated><resolved>2014-10-29T14:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T12:46:52Z" id="51595770">Hi @nimahajan 

Are you still experiencing this problem? What version?  There must be something going on in the logs.  Without more info it's difficult to help.
</comment><comment author="psych0d0g" created="2014-08-20T08:17:21Z" id="52746594">I have the same issue of a cluster becoming unresposive after a random amount of time,
Im running with 4 nodes (all of them are master eligable and data nodes) in a Datacenter Aware Setup with 5 shards/index and 1 Replica in the other Datacenter.
My logfiles are quiet aswell when this happens, ES just stops logging at all at the same time the cluster is getting unresponsive.
Last logline i get is from yesterday:

2014-08-19 16:48:48,390][INFO ][cluster.service          ] [csliveeubs-elasticsearch01.server.lan] removed {[csliveeubap-logparser01.server.lan][81xwRvieT4ytrMO2PdJXZg][csliveeubap-logparser01.server.lan][inet[/**.**.**.**:9300]]{client=true, data=false},}, added {[csliveeubap-logparser01.server.lan][I1t_-leETwqdSH_Xt-0Ymg][csliveeubap-logparser01.server.lan][inet[/**.**.**.**:9300]]{client=true, data=false},}, reason: zen-disco-receive(from master [[csliveeubap-elasticsearch02.server.lan][TgTzuqrkRYay6fOyDIP66w][csliveeubap-elasticsearch02.server.lan][inet[/**.**.**.**:9300]]{datacenter_location=2, master=true}])

Im running ES 1.3.1 on debian wheezy x64 on the following Hardware Setup without the use of any plugins:

2x Intel(R) Xeon(R) CPU E5-2430 v2 @ 2.50GHz
24GB DDR3 ecc ram
2.7T raid 5 with 3x600GB 10k sas HDDs
</comment><comment author="clintongormley" created="2014-08-20T08:58:14Z" id="52750366">@psych0d0g that is your client shutting down and being replaced with another client.  Also, we strongly advise against using a single cluster across two data centres. It will produce instability and could well be responsible for what you are seeing.

What is in the logs of your other nodes?
</comment><comment author="psych0d0g" created="2014-08-20T09:13:01Z" id="52751713">Exactly the same for all data-node logs.
Our datacenters are linked with a dedicated connection via fibre.
The ping time between them is about 0.9ms
Additionally here is my ES Configuration:

cluster.name:                                                   cslive-elasticsearch
node.name:                                                      "csliveeubs-elasticsearch01.server.lan"
node.datacenter_location:                                       1
node.master:                                                    true
node.data:                                                      true
path.data:                                                      /mnt/storage
cluster.routing.allocation.awareness.attributes:                datacenter_location
cluster.routing.allocation.cluster_concurrent_rebalance:        12
cluster.routing.allocation.node_concurrent_recoveries:          15
indices.recovery.concurrent_streams:                            12
cluster.routing.allocation.node_initial_primaries_recoveries:   5
#Node in the other datacenter, diffrent vlan:
discovery.zen.ping.unicast.hosts:                               **.**.**.** 
discovery.zen.minimum_master_nodes:                             2
index.number_of_replicas:                                       1
index.recovery.initial_shards:                                  2
indices.memory.index_buffer_size:                               50%
index.translog.flush_threshold_ops:                             50000
replication:                                                    async
index.compound_on_flush:                                        false
index.compound_format:                                          false
indices.recovery.concurrent_streams:                            12
indices.recovery.max_bytes_per_sec:                             512mb
indices.store.throttle.max_bytes_per_sec:                       512mb
action.disable_delete_all_indices:                              true

# Search thread pool

threadpool.search.type:                                         fixed
threadpool.search.size:                                         20
threadpool.search.queue_size:                                   100

# Index thread pool

threadpool.index.type:                                          fixed
threadpool.index.size:                                          600
threadpool.index.queue_size:                                    2000
</comment><comment author="clintongormley" created="2014-08-20T09:20:40Z" id="52752431">Some notes on your settings:

```
index.translog.flush_threshold_ops: 50000
```

In v1.3.x we set `flush_threshold_ops` to unbounded.  You should do the same.

```
replication: async
```

This stops Elasticsearch from exerting backpressure if it is struggling. Use `sync` replication.

```
index.compound_on_flush: false
index.compound_format: false
```

Why are you disabling the compound format? This should not be necessary, especially if you have an unbounded `flush_threshold_ops`

```
threadpool.index.size: 600
```

You're going to overwhelm Elasticsearch with so many indexing threads.  Use the default settings instead.

And back to your original issue - you say the cluster becomes unresponsive, but what do you mean by that?  Do no nodes respond at all, even to a simple `GET /` request?
</comment><comment author="clintongormley" created="2014-08-20T09:21:25Z" id="52752496">There is nothing else at all in any of your other logs?
</comment><comment author="psych0d0g" created="2014-08-20T09:23:04Z" id="52752638">the logs arent telling me anything for the time when the cluster becomes "unresposive".
When im using curl to for eg:

&gt; ```
&gt;    curl -XPUT localhost:9200/_cluster/settings -d '{
&gt;             "persistent" : {
&gt;             "cluster.routing.allocation.disable_allocation" : true
&gt;             }
&gt;     }'
&gt; ```

The cluster responds with a 503 timeout

{"error":"RemoteTransportException[[csliveeubs-elasticsearch01.server.lan][inet[/_._._._:9300]][cluster/settings/update]]; nested: ProcessClusterEventTimeoutException[failed to process cluster event (cluster_update_settings) within 30s]; ","status":503}
</comment><comment author="clintongormley" created="2014-08-20T09:26:12Z" id="52752893">How big is your cluster state? What's the output of:

```
curl 'localhost:9200/_cat/pending_tasks?v'
```
</comment><comment author="psych0d0g" created="2014-08-20T09:27:28Z" id="52753005">at this moment its freshly restartet to bring it back to a working state asapst because our queue are filling up pretty fast, we need to get them empty again, i will have a look when this happens again.
</comment><comment author="psych0d0g" created="2014-08-25T15:50:51Z" id="53282757">okay, here is the pending tasks, zen-disco-recive piled up like hell:

https://gist.github.com/psych0d0g/8e4e0d140b9d95946df4
</comment><comment author="clintongormley" created="2014-08-25T16:04:26Z" id="53284728">@psych0d0g have you tried using the elasticsearch-http connector in logstash? 
</comment><comment author="psych0d0g" created="2014-08-25T16:05:50Z" id="53284912">not yet since my thought was that its not as performant as the node protocol
additionally with the node protocol i dont have to care about distributing the load myself accross the cluster
</comment><comment author="clintongormley" created="2014-08-25T16:14:45Z" id="53286072">So when a client node joins the cluster, it has to go through "discovery" which can take a little time. The idea is that the client node will be part of the cluster for an extended period.  It looks like, for whatever reason, you are either creating lots and lots of connections or they are very short lived.

You probably want to ask about your logstash config on the logstash forums, but I'd definitely try out the http connector, which is the recommended connector these days.
</comment><comment author="psych0d0g" created="2014-08-25T16:17:58Z" id="53286495">mhm, i just did, and it IS slow,
with the node protocoll i managed to index about 20k loglines/s
with the http api im down to 12k/s
</comment><comment author="psych0d0g" created="2014-08-25T16:19:33Z" id="53286695">and the problem might be a combination of both, i use 24 workes for the ES output and i restart the logstashes constantly when introducing new grok filter.
That might be the root cause indeed
</comment><comment author="clintongormley" created="2014-08-25T16:20:42Z" id="53286857">Perhaps the logstash guys can give you some tuning advice.  Btw, with the work that is happening in the improve_zen branch on Elasticsearch, discovery will become much quicker than it is today.
</comment><comment author="psych0d0g" created="2014-08-27T09:06:41Z" id="53544038">okay, now the issue is back again, even the /_cat/ request seems to timeout right now
so i cant check whether my pending tasks increased
at least i get a bunch of stacktrace's in my logfiles now:
https://gist.github.com/psych0d0g/73a52ca35405236c9406
</comment><comment author="psych0d0g" created="2014-08-27T09:18:49Z" id="53545226">Here are some more, from yesterday at around the time the last loglines where indexed:

[2014-08-26 20:17:37,892][DEBUG][action.admin.cluster.node.stats] [csliveeubs-elasticsearch01.server.lan] failed to execute on node [MM0GwoWYTSqBWD3VeFiaiw]
org.elasticsearch.transport.RemoteTransportException: [csliveeubap-elasticsearch01.server.lan][inet[/10.76.30.11:9300]][cluster/nodes/stats/n]
Caused by: org.elasticsearch.index.engine.EngineException: [logstash-live-2014.08.26][4] failed to acquire searcher, source doc_stats
        at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:713)
        at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)
        at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)
        at org.elasticsearch.index.shard.service.InternalIndexShard.docStats(InternalIndexShard.java:498)
        at org.elasticsearch.action.admin.indices.stats.CommonStats.&lt;init&gt;(CommonStats.java:127)
        at org.elasticsearch.action.admin.indices.stats.ShardStats.&lt;init&gt;(ShardStats.java:49)
        at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:209)
        at org.elasticsearch.node.service.NodeService.stats(NodeService.java:156)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:95)
        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:43)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:277)
        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:268)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed
        at org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)
        at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:700)
        ... 15 more
[2014-08-26 20:19:26,109][WARN ][cluster.action.shard     ] [csliveeubs-elasticsearch01.server.lan] [logstash-live-2014.08.26][4] received shard failed for [logstash-live-2014.08.26][4], node[MM0GwoWYTSqBWD3VeFiaiw], [R], s[STARTED], indexUUID [V5aS0Y7ZRveaReA_huF-KA], reason [engine failure, message [merge exception][MergeException[java.lang.OutOfMemoryError: Java heap space]; nested: OutOfMemoryError[Java heap space]; ]]

ES has 12GB of heap space, how the heck can it eat all that precious RAM up and demand even more for its heap?
</comment><comment author="clintongormley" created="2014-09-02T15:50:21Z" id="54172832">@psych0d0g I'm guessing that you are sorting or aggregating on some fields which are taking up a lot of memory?  Have a look at the output of:

```
GET '/_nodes/stats/indices/fielddata?human&amp;fields=*'
```
</comment><comment author="psych0d0g" created="2014-09-03T08:39:05Z" id="54267196">{"cluster_name":"cslive-elasticsearch","nodes":{"vCAZom1MQF2XZGNqqtqtew":{"timestamp":1409733531784,"name":"csliveeubs-elasticsearch02.server.lan","transport_address":"inet[/10.79.192.12:9300]","host":"csliveeubs-elasticsearch02.server.lan","ip":["inet[/10.79.192.12:9300]","NONE"],"attributes":{"datacenter_location":"bs","master":"true"},"indices":{"fielddata":{"memory_size":"103.7mb","memory_size_in_bytes":108769547,"evictions":0,"fields":{"tags":{"memory_size":"74.6kb","memory_size_in_bytes":76485},"@timestamp":{"memory_size":"103.5mb","memory_size_in_bytes":108570376},"_uid":{"memory_size":"119.8kb","memory_size_in_bytes":122686}}}}},"eMjtqKXER4ilutu9UPwwmw":{"timestamp":1409733531779,"name":"csliveeubap-elasticsearch02.server.lan","transport_address":"inet[/10.76.30.12:9300]","host":"csliveeubap-elasticsearch02.server.lan","ip":["inet[/10.76.30.12:9300]","NONE"],"attributes":{"datacenter_location":"bap","master":"true"},"indices":{"fielddata":{"memory_size":"194.5kb","memory_size_in_bytes":199171,"evictions":0,"fields":{"_uid":{"memory_size":"119.8kb","memory_size_in_bytes":122686},"tags":{"memory_size":"74.6kb","memory_size_in_bytes":76485}}}}},"g2nl3aZJSeqhRZMaUdpbsA":{"timestamp":1409733531783,"name":"csqaeubs-logparser02.server.lan","transport_address":"inet[/10.88.7.52:9300]","host":"csqaeubs-logparser02.server.lan","ip":["inet[/10.88.7.52:9300]","NONE"],"attributes":{"client":"true","data":"false"},"indices":{"fielddata":{"memory_size":"0b","memory_size_in_bytes":0,"evictions":0}}},"2NuW6V_LQLieGaRLlFXipQ":{"timestamp":1409733532930,"name":"csliveeubap-elasticsearch01.server.lan","transport_address":"inet[/10.76.30.11:9300]","host":"csliveeubap-elasticsearch01.server.lan","ip":["inet[/10.76.30.11:9300]","NONE"],"attributes":{"datacenter_location":"bap","master":"true"},"indices":{"fielddata":{"memory_size":"96.7mb","memory_size_in_bytes":101478287,"evictions":0,"fields":{"tags":{"memory_size":"149.3kb","memory_size_in_bytes":152936},"@timestamp":{"memory_size":"96.5mb","memory_size_in_bytes":101220616},"_uid":{"memory_size":"102.2kb","memory_size_in_bytes":104735}}}}},"fQIrgXboSWyZ6IgZ19_BPw":{"timestamp":1409733531783,"name":"csqaeubs-logparser01.server.lan","transport_address":"inet[/10.88.7.51:9300]","host":"csqaeubs-logparser01.server.lan","ip":["inet[/10.88.7.51:9300]","NONE"],"attributes":{"client":"true","data":"false"},"indices":{"fielddata":{"memory_size":"0b","memory_size_in_bytes":0,"evictions":0}}},"U8TV4j2ETYCuf4Y8HW4zYQ":{"timestamp":1409733531780,"name":"csqaeubap-logparser01.server.lan","transport_address":"inet[/10.88.32.51:9300]","host":"csqaeubap-logparser01.server.lan","ip":["inet[/10.88.32.51:9300]","NONE"],"attributes":{"client":"true","data":"false"},"indices":{"fielddata":{"memory_size":"0b","memory_size_in_bytes":0,"evictions":0}}},"XiaE-n4_Rp6ZvZ8mj74D1w":{"timestamp":1409733531780,"name":"csqaeubap-logparser02.server.lan","transport_address":"inet[/10.88.32.52:9300]","host":"csqaeubap-logparser02.server.lan","ip":["inet[/10.88.32.52:9300]","NONE"],"attributes":{"client":"true","data":"false"},"indices":{"fielddata":{"memory_size":"0b","memory_size_in_bytes":0,"evictions":0}}},"krlSSP8JSneA6Lqu4xdR-A":{"timestamp":1409733531783,"name":"csliveeubs-elasticsearch01.server.lan","transport_address":"inet[/10.79.192.11:9300]","host":"csliveeubs-elasticsearch01.server.lan","ip":["inet[/10.79.192.11:9300]","NONE"],"attributes":{"datacenter_location":"bs","master":"true"},"indices":{"fielddata":{"memory_size":"235.8mb","memory_size_in_bytes":247358263,"evictions":0,"fields":{"tags":{"memory_size":"149.3kb","memory_size_in_bytes":152936},"@timestamp":{"memory_size":"235.6mb","memory_size_in_bytes":247100592},"_uid":{"memory_size":"102.2kb","memory_size_in_bytes":104735}}}}}}}
</comment><comment author="clintongormley" created="2014-09-03T13:10:56Z" id="54293908">OK, so it doesn't look like your fielddata, as that is not using much memory (or did you restart nodes in the interim).

How big are your bulk batches?  Are you still using so many indexing threads?  Please add the full output of these two commands (in a separate gist):

```
GET /_nodes
GET /_nodes/stats
```
</comment><comment author="psych0d0g" created="2014-09-03T14:41:45Z" id="54307364">It took the cluster several minutes to respond with the requested data

GET /_nodes: 
https://gist.github.com/psych0d0g/9a19380a5d5241002f55

GET /_nodes/stats: 
https://gist.github.com/psych0d0g/524c8279e4bc5de161c4

another GET '/_nodes/stats/indices/fielddata?human&amp;fields=*':
https://gist.github.com/psych0d0g/1119212c262ef2026ba7

my batches are: 1000
my indexing threads have ben removed from the config so im back to default,
im restarting the cluster daily atm to get it back to performance
</comment><comment author="clintongormley" created="2014-09-06T16:24:17Z" id="54718230">Hi @psych0d0g 

The only things that show up as abnormal are (1) slow garbage collections and (2) the amount of RAM being used for your segments.  See `indices.segments.memory_in_bytes` in the stats output.

I suggest disabling bloom filters on old indices, and disabling norms (or making fields not_analyzed) on any string field which is not used for full text search.

See:
- http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#norms
- http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-update-settings.html#codec-bloom-load
- https://github.com/elasticsearch/curator/wiki/Disable-Bloom-Filter-Cache
</comment><comment author="clintongormley" created="2014-10-29T14:09:48Z" id="60929491">I'm assuming that this advice worked, as we haven't heard any more, so I'll close this ticket
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a `interval_format` parameter to `date_histogram` aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6408</link><project id="" key="" /><description>It would be very cool if we could give a format to create the interval instead of the simple multi value `interval` parameter.

Let's give some examples... A `days` interval could be the same as `'Y-M-d'` interval format.

So, what is this new parameter allowing? More complex aggregations! Like if I want to aggregate each element depending on which week day (`"interval_format": "e"`) it was created on. Or which month of the year (`"interval_format": "M"`).

I would love to have this! Right now I have to write ugly scripts and I don't even know how to handle timezones properly in those.
</description><key id="34971740">6408</key><summary>Add a `interval_format` parameter to `date_histogram` aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">qraynaud</reporter><labels><label>:Aggregations</label><label>discuss</label></labels><created>2014-06-04T15:47:28Z</created><updated>2015-08-26T15:04:32Z</updated><resolved>2015-08-26T15:04:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-06-04T21:31:06Z" id="45154459">you can get this by suing a `terms` filter with scripts (where the script extracts the month/day/week from the docs as the "terms")
</comment><comment author="qraynaud" created="2014-06-05T06:04:58Z" id="45183900">Yes, this is what I'm doing right now. But I need to have some timezone support in them and it makes for very bad looking scripts.

Moreover we don't want to use dynamic scripting in production which makes for a more complex software codebase too: we need to use the script names in production &amp; the real script in dev.

I hoped this modification would make sense since we are really keeping the original purpose of date_histogram but only making it more powerful.
</comment><comment author="g00fy-" created="2014-09-01T18:38:51Z" id="54084487">+1
</comment><comment author="jpountz" created="2015-08-26T15:04:32Z" id="135052912">Closing in favour of https://github.com/elastic/elasticsearch/issues/6580
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6407</link><project id="" key="" /><description /><key id="34968748">6407</key><summary>Add Javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">stephlag</reporter><labels><label>docs</label></labels><created>2014-06-04T15:18:31Z</created><updated>2014-06-14T18:01:52Z</updated><resolved>2014-06-05T08:32:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-05T08:34:29Z" id="45194176">Merged, thanks!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exposed _uid, _id and _type fields as stored fields (_fields notation)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6406</link><project id="" key="" /><description>The `_uid` field wasn't available in a script despite it's always stored. Made it available and made available also `_id` and `_type` fields that are deducted from it.
</description><key id="34955618">6406</key><summary>Exposed _uid, _id and _type fields as stored fields (_fields notation)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Scripting</label><label>enhancement</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-04T12:53:15Z</created><updated>2015-06-07T13:18:34Z</updated><resolved>2014-06-05T15:18:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-06-05T08:43:43Z" id="45194956">LGTM
</comment><comment author="jpountz" created="2014-06-05T12:02:20Z" id="45210874">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>date_histogram extendedBounds(DateTime,DateTime) failed when using format option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6405</link><project id="" key="" /><description>This code was working in elasticsearch 1.1.1:

``` java
SearchResponse response = esClient.prepareSearch("person")
  .addAggregation(
    AggregationBuilders.dateHistogram("by_year")
      .field("dateOfBirth")
      .minDocCount(0)
      .interval(DateHistogram.Interval.YEAR)
      .extendedBounds(DateTime.parse("1940"), DateTime.parse("2009"))
      .format("YYYY")
  )
  .execute().actionGet();
```

From elasticsearch 1.2.0, it fails with:

```
[2014-06-04 12:54:38,337][DEBUG][action.search.type       ] [Karla Sofen] [person][0], node[wTCLjroMSk-3LPUx3dIdlA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@300f4bad] lastShard [true]
org.elasticsearch.ElasticsearchParseException: failed to parse date field [1940-01-01T00:00:00.000Z], tried both date format [YYYY], and timestamp number
    at org.elasticsearch.common.joda.DateMathParser.parseStringValue(DateMathParser.java:226)
    at org.elasticsearch.common.joda.DateMathParser.parse(DateMathParser.java:68)
    at org.elasticsearch.common.joda.DateMathParser.parse(DateMathParser.java:42)
    at org.elasticsearch.search.aggregations.support.format.ValueParser$DateMath.parseLong(ValueParser.java:96)
    at org.elasticsearch.search.aggregations.bucket.histogram.ExtendedBounds.processAndValidate(ExtendedBounds.java:52)
    at org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator$Factory.create(HistogramAggregator.java:178)
    at org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator$Factory.create(HistogramAggregator.java:137)
    at org.elasticsearch.search.aggregations.support.ValuesSourceAggregatorFactory.create(ValuesSourceAggregatorFactory.java:54)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createAndRegisterContextAware(AggregatorFactories.java:52)
    at org.elasticsearch.search.aggregations.AggregatorFactories.createTopLevelAggregators(AggregatorFactories.java:145)
    at org.elasticsearch.search.aggregations.AggregationPhase.preProcess(AggregationPhase.java:79)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)
    at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IllegalArgumentException: Invalid format: "1940-01-01T00:00:00.000Z" is malformed at "-01-01T00:00:00.000Z"
    at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:754)
    at org.elasticsearch.common.joda.DateMathParser.parseStringValue(DateMathParser.java:220)
    ... 18 more
```

Of course, it works nicely when using `extendedBounds(String, String)` instead of `extendedBounds(DateTime, DateTime)`:

``` java
SearchResponse response = esClient.prepareSearch("person")
  .addAggregation(
    AggregationBuilders.dateHistogram("by_year")
      .field("dateOfBirth")
      .minDocCount(0)
      .interval(DateHistogram.Interval.YEAR)
      .extendedBounds("1940", "2009")
      .format("YYYY")
  )
  .execute().actionGet();
```

As it was working in 1.1.1, should we consider this as a regression or is it by design?
</description><key id="34949727">6405</key><summary>date_histogram extendedBounds(DateTime,DateTime) failed when using format option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels><label>:Aggregations</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-06-04T11:16:45Z</created><updated>2016-07-26T13:08:39Z</updated><resolved>2016-07-26T13:08:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bakura10" created="2015-02-02T14:15:52Z" id="72463891">I'm having this issue to. Using a timestamp for extended bounds also make the trick.
</comment><comment author="clintongormley" created="2015-11-21T13:29:51Z" id="158640489">@colings86 could take a look at this please
</comment><comment author="dadoonet" created="2016-07-26T13:08:39Z" id="235261752">Reading the existing code in master (5.0), `extendedBounds` method only supports now a [`ExtendedBounds` object](https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/ExtendedBounds.java#L43-L43).

ExtendedBounds only supports the following CTORs:

``` java
public ExtendedBounds(Long min, Long max);
public ExtendedBounds(String minAsStr, String maxAsStr);
```

So we can't hit this issue anymore in 5.0.

Closing but feel free to reopen if you think closing it was a mistake.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create only one MLT query per field for all queried items</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6404</link><project id="" key="" /><description>Previously, one MLT query per field was created for each item. One issue with
this method is that the maximum number of selected terms was equal to the
number of items times 'max_query_terms'. Instead, users should have direct control
over the maximum number of selected terms allowed, regardless of the number of
queried items.

Another issue related to the previous method is that it could lead to the
selection of rather uninteresting terms, that because they were found in a
particular queried item. Instead, this new procedure enforces the selection of
interesting terms across ALL items, not within each item. This could lead to
search results where the best matching items share commonalities amongst the
best characteristics of all the items.
</description><key id="34947237">6404</key><summary>Create only one MLT query per field for all queried items</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-04T10:32:45Z</created><updated>2015-06-07T13:18:49Z</updated><resolved>2014-06-12T12:20:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-05T12:27:51Z" id="45212965">Could we try to keep LikeText immutable? It doesn't feel right to me to modify in-place the instances returned by `fetchService.fetch`.

Otherwise I think it is a nice change!
</comment><comment author="s1monw" created="2014-06-12T09:02:12Z" id="45845440">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Marvel plugin broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6403</link><project id="" key="" /><description>If elasticsearch 1.2 is not listening on the ipv4 localhost the Marvel plugin does not work. Tried it in my dev environment on two machines.

Elasticsearch is bind to a ipv6 address. With 1.0 everything worked well.
</description><key id="34943597">6403</key><summary>Marvel plugin broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">DominicBoettger</reporter><labels><label>feedback_needed</label></labels><created>2014-06-04T09:37:06Z</created><updated>2014-10-24T11:21:41Z</updated><resolved>2014-10-24T11:21:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-04T09:40:03Z" id="45070815">Hi Dominic,

That's interesting. Can you give the your elasticsearch.yml file (or command line parameters)? will make it easier to reproduce. As a workaround, did you try setting the `marvel.agent.exporter.es.hosts` setting pointing it at the right direction?
</comment><comment author="DominicBoettger" created="2014-06-04T11:32:14Z" id="45079688">I tried the marvel.agent.exporter.es.hosts setting, but it said also not reachable in the logfiles.

```
##################### Elasticsearch Configuration Example #####################

# This file contains an overview of various configuration settings,
# targeted at operations staff. Application developers should
# consult the guide at &lt;http://elasticsearch.org/guide&gt;.
#
# The installation procedure is covered at
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html&gt;.
#
# Elasticsearch comes with reasonable defaults for most settings,
# so you can try it out without bothering with configuration.
#
# Most of the time, these defaults are just fine for running a production
# cluster. If you're fine-tuning your cluster, or wondering about the
# effect of certain configuration option, please _do ask_ on the
# mailing list or IRC channel [http://elasticsearch.org/community].

# Any element in the configuration can be replaced with environment variables
# by placing them in ${...} notation. For example:
#
#node.rack: ${RACK_ENV_VAR}

# For information on supported formats and syntax for the config file, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html&gt;


################################### Cluster ###################################

# Cluster name identifies your cluster for auto-discovery. If you're running
# multiple clusters on the same network, make sure you're using unique names.
#
#cluster.name: elasticsearch
cluster.name: dobby1

#################################### Node #####################################

# Node names are generated dynamically on startup, so you're relieved
# from configuring them manually. You can tie this node to a specific name:
#
#node.name: "Franz Kafka"

# Every node can be configured to allow or deny being eligible as the master,
# and to allow or deny to store the data.
#
# Allow this node to be eligible as a master node (enabled by default):
#
#node.master: true
#
# Allow this node to store data (enabled by default):
#
#node.data: true

# You can exploit these settings to design advanced cluster topologies.
#
# 1. You want this node to never become a master node, only to hold data.
#    This will be the "workhorse" of your cluster.
#
#node.master: false
#node.data: true
#
# 2. You want this node to only serve as a master: to not store any data and
#    to have free resources. This will be the "coordinator" of your cluster.
#
#node.master: true
#node.data: false
#
# 3. You want this node to be neither master nor data node, but
#    to act as a "search load balancer" (fetching data from nodes,
#    aggregating results, etc.)
#
#node.master: false
#node.data: false

# Use the Cluster Health API [http://localhost:9200/_cluster/health], the
# Node Info API [http://localhost:9200/_nodes] or GUI tools
# such as &lt;http://www.elasticsearch.org/overview/marvel/&gt;,
# &lt;http://github.com/karmi/elasticsearch-paramedic&gt;,
# &lt;http://github.com/lukas-vlcek/bigdesk&gt; and
# &lt;http://mobz.github.com/elasticsearch-head&gt; to inspect the cluster state.

# A node can have generic attributes associated with it, which can later be used
# for customized shard allocation filtering, or allocation awareness. An attribute
# is a simple key value pair, similar to node.key: value, here is an example:
#
#node.rack: rack314

# By default, multiple nodes are allowed to start from the same installation location
# to disable it, set the following:
#node.max_local_storage_nodes: 1


#################################### Index ####################################

# You can set a number of options (such as shard/replica options, mapping
# or analyzer definitions, translog settings, ...) for indices globally,
# in this file.
#
# Note, that it makes more sense to configure index settings specifically for
# a certain index, either when creating it or by using the index templates API.
#
# See &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules.html&gt; and
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/indices-create-index.html&gt;
# for more information.

# Set the number of shards (splits) of an index (5 by default):
#
#index.number_of_shards: 5

# Set the number of replicas (additional copies) of an index (1 by default):
#
#index.number_of_replicas: 1

# Note, that for development on a local machine, with small indices, it usually
# makes sense to "disable" the distributed features:
#
#index.number_of_shards: 1
#index.number_of_replicas: 0

# These settings directly affect the performance of index and search operations
# in your cluster. Assuming you have enough machines to hold shards and
# replicas, the rule of thumb is:
#
# 1. Having more *shards* enhances the _indexing_ performance and allows to
#    _distribute_ a big index across machines.
# 2. Having more *replicas* enhances the _search_ performance and improves the
#    cluster _availability_.
#
# The "number_of_shards" is a one-time setting for an index.
#
# The "number_of_replicas" can be increased or decreased anytime,
# by using the Index Update Settings API.
#
# Elasticsearch takes care about load balancing, relocating, gathering the
# results from nodes, etc. Experiment with different settings to fine-tune
# your setup.

# Use the Index Status API (&lt;http://localhost:9200/A/_status&gt;) to inspect
# the index status.


#################################### Paths ####################################

# Path to directory containing configuration (this file and logging.yml):
#
#path.conf: /path/to/conf

# Path to directory where to store index data allocated for this node.
#
#path.data: /path/to/data
#
# Can optionally include more than one location, causing data to be striped across
# the locations (a la RAID 0) on a file level, favouring locations with most free
# space on creation. For example:
#
#path.data: /path/to/data1,/path/to/data2

# Path to temporary files:
#
#path.work: /path/to/work

# Path to log files:
#
#path.logs: /path/to/logs

# Path to where plugins are installed:
#
#path.plugins: /path/to/plugins


#################################### Plugin ###################################

# If a plugin listed here is not installed for current node, the node will not start.
#
#plugin.mandatory: mapper-attachments,lang-groovy


################################### Memory ####################################

# Elasticsearch performs poorly when JVM starts swapping: you should ensure that
# it _never_ swaps.
#
# Set this property to true to lock the memory:
#
#bootstrap.mlockall: true

# Make sure that the ES_MIN_MEM and ES_MAX_MEM environment variables are set
# to the same value, and that the machine has enough memory to allocate
# for Elasticsearch, leaving enough memory for the operating system itself.
#
# You should also make sure that the Elasticsearch process is allowed to lock
# the memory, eg. by using `ulimit -l unlimited`.


############################## Network And HTTP ###############################

# Elasticsearch, by default, binds itself to the 0.0.0.0 address, and listens
# on port [9200-9300] for HTTP traffic and on port [9300-9400] for node-to-node
# communication. (the range means that if the port is busy, it will automatically
# try the next port).

# Set the bind address specifically (IPv4 or IPv6):
#
#network.bind_host: 127.0.0.1

# Set the address other nodes will use to communicate with this node. If not
# set, it is automatically derived. It must point to an actual IP address.
#
#network.publish_host: 192.168.0.1

# Set both 'bind_host' and 'publish_host':
#
network.host: ::1
#network.host: 127.0.0.1

# Set a custom port for the node to node communication (9300 by default):
#
#transport.tcp.port: 9300

# Enable compression for all communication between nodes (disabled by default):
#
#transport.tcp.compress: true

# Set a custom port to listen for HTTP traffic:
#
#http.port: 9200

# Set a custom allowed content length:
#
#http.max_content_length: 100mb

# Disable HTTP completely:
#
#http.enabled: false


################################### Gateway ###################################

# The gateway allows for persisting the cluster state between full cluster
# restarts. Every change to the state (such as adding an index) will be stored
# in the gateway, and when the cluster starts up for the first time,
# it will read its state from the gateway.

# There are several types of gateway implementations. For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-gateway.html&gt;.

# The default gateway type is the "local" gateway (recommended):
#
#gateway.type: local

# Settings below control how and when to start the initial recovery process on
# a full cluster restart (to reuse as much local data as possible when using shared
# gateway).

# Allow recovery process after N nodes in a cluster are up:
#
#gateway.recover_after_nodes: 1

# Set the timeout to initiate the recovery process, once the N nodes
# from previous setting are up (accepts time value):
#
#gateway.recover_after_time: 5m

# Set how many nodes are expected in this cluster. Once these N nodes
# are up (and recover_after_nodes is met), begin recovery process immediately
# (without waiting for recover_after_time to expire):
#
#gateway.expected_nodes: 2


############################# Recovery Throttling #############################

# These settings allow to control the process of shards allocation between
# nodes during initial recovery, replica allocation, rebalancing,
# or when adding and removing nodes.

# Set the number of concurrent recoveries happening on a node:
#
# 1. During the initial recovery
#
#cluster.routing.allocation.node_initial_primaries_recoveries: 4
#
# 2. During adding/removing nodes, rebalancing, etc
#
#cluster.routing.allocation.node_concurrent_recoveries: 2

# Set to throttle throughput when recovering (eg. 100mb, by default 20mb):
#
#indices.recovery.max_bytes_per_sec: 20mb

# Set to limit the number of open concurrent streams when
# recovering a shard from a peer:
#
#indices.recovery.concurrent_streams: 5


################################## Discovery ##################################

# Discovery infrastructure ensures nodes can be found within a cluster
# and master node is elected. Multicast discovery is the default.

# Set to ensure a node sees N other master eligible nodes to be considered
# operational within the cluster. Its recommended to set it to a higher value
# than 1 when running more than 2 nodes in the cluster.
#
#discovery.zen.minimum_master_nodes: 1

# Set the time to wait for ping responses from other nodes when discovering.
# Set this option to a higher value on a slow or congested network
# to minimize discovery failures:
#
#discovery.zen.ping.timeout: 3s

# For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-zen.html&gt;

# Unicast discovery allows to explicitly control which nodes will be used
# to discover the cluster. It can be used when multicast is not present,
# or to restrict the cluster communication-wise.
#
# 1. Disable multicast discovery (enabled by default):
#
#discovery.zen.ping.multicast.enabled: false
#
# 2. Configure an initial list of master nodes in the cluster
#    to perform discovery when new nodes (master or data) are started:
#
#discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]

# EC2 discovery allows to use AWS EC2 API in order to perform discovery.
#
# You have to install the cloud-aws plugin for enabling the EC2 discovery.
#
# For more information, see
# &lt;http://elasticsearch.org/guide/en/elasticsearch/reference/current/modules-discovery-ec2.html&gt;
#
# See &lt;http://elasticsearch.org/tutorials/elasticsearch-on-ec2/&gt;
# for a step-by-step tutorial.

# GCE discovery allows to use Google Compute Engine API in order to perform discovery.
#
# You have to install the cloud-gce plugin for enabling the GCE discovery.
#
# For more information, see &lt;https://github.com/elasticsearch/elasticsearch-cloud-gce&gt;.

# Azure discovery allows to use Azure API in order to perform discovery.
#
# You have to install the cloud-azure plugin for enabling the Azure discovery.
#
# For more information, see &lt;https://github.com/elasticsearch/elasticsearch-cloud-azure&gt;.

################################## Slow Log ##################################

# Shard level query and fetch threshold logging.

#index.search.slowlog.threshold.query.warn: 10s
#index.search.slowlog.threshold.query.info: 5s
#index.search.slowlog.threshold.query.debug: 2s
#index.search.slowlog.threshold.query.trace: 500ms

#index.search.slowlog.threshold.fetch.warn: 1s
#index.search.slowlog.threshold.fetch.info: 800ms
#index.search.slowlog.threshold.fetch.debug: 500ms
#index.search.slowlog.threshold.fetch.trace: 200ms

#index.indexing.slowlog.threshold.index.warn: 10s
#index.indexing.slowlog.threshold.index.info: 5s
#index.indexing.slowlog.threshold.index.debug: 2s
#index.indexing.slowlog.threshold.index.trace: 500ms

################################## GC Logging ################################

#monitor.jvm.gc.young.warn: 1000ms
#monitor.jvm.gc.young.info: 700ms
#monitor.jvm.gc.young.debug: 400ms

#monitor.jvm.gc.old.warn: 10s
#monitor.jvm.gc.old.info: 5s
#monitor.jvm.gc.old.debug: 2s
```
</comment><comment author="bleskes" created="2014-06-06T11:07:02Z" id="45325591">Can you try setting this in your .yml file, as a temporary workaround:

```
marvel.agent.exporter.es.hosts: ["[::1]:9200"]
```
</comment><comment author="DominicBoettger" created="2014-06-10T07:49:48Z" id="45583479">This did not help at all. Did you try it on your local installation?
</comment><comment author="bleskes" created="2014-06-16T13:23:42Z" id="46177056">hmm. Yeah, I did and it works just fine. Can you share the your ES logs reporting where ES is bound to  (similar to):

```
[2014-06-16 15:22:13,254][INFO ][transport                ] [Agamotto] bound_address {inet[/0:0:0:0:0:0:0:0%0:10300]}, publish_address {inet[/10.20.100.101:10300]}
```

and also the marvel's export settings (outputted in debug logging):

```
[2014-06-16 15:22:13,151][DEBUG][marvel.agent.exporter    ] [Agamotto] initialized with targets: [localhost:10000], index prefix [.marvel], index time format [YYYY.MM.dd]
```
</comment><comment author="clintongormley" created="2014-08-18T09:07:06Z" id="52467886">@DominicBoettger any chance of the above info?
</comment><comment author="DominicBoettger" created="2014-08-18T09:15:58Z" id="52468726">[2014-08-18 11:14:15,350][INFO ][transport                ] [N'astirh] bound_address {inet[/2a01:4f8:123:1234:0:0:0:2:9300]}, publish_address {inet[/2a01:4f8:123:1234:0:0:0:2:9300]}

I don't have the other entry. How can i activate it?
</comment><comment author="bleskes" created="2014-08-18T09:30:58Z" id="52470079">@DominicBoettger thx. To get the marvel entry you need to enable debug logging for it. In the logging.yml file, add a `marvel: DEBUG` jus under the `logger:` line:

```
logger:
   marvel: DEBUG
```
</comment><comment author="bleskes" created="2014-10-24T11:21:41Z" id="60374544">Closing this due to lack of feedback.  Please re-open if things are still not working.. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[BUILD] Promote artifacts from strings to their own type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6402</link><project id="" key="" /><description>@s1monw Figured I would make this a formal PR.  Not sure what other errors you were getting during the release, but I've tested this end-to-end and it WFM.
</description><key id="34911157">6402</key><summary>[BUILD] Promote artifacts from strings to their own type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">drewr</reporter><labels><label>adoptme</label></labels><created>2014-06-03T22:34:25Z</created><updated>2015-08-08T07:18:00Z</updated><resolved>2014-11-11T19:06:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T13:22:39Z" id="59752507">@s1monw could you take a look please?
</comment><comment author="s1monw" created="2014-10-22T12:45:56Z" id="60078666">grr this is out of sync - @drewr should we close or will you update?
</comment><comment author="clintongormley" created="2014-11-11T19:06:20Z" id="62598808">No more news - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Guard the auto expand replicas setting against improper values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6401</link><project id="" key="" /><description>This is the PR for Issue #5752

It updates the documentation to be more precise, and adds guards in the code to ensure the values are recognized, and if not it emits a log message and throws an exception with a more helpful message.
</description><key id="34898656">6401</key><summary>Guard the auto expand replicas setting against improper values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mdaniel</reporter><labels /><created>2014-06-03T19:59:19Z</created><updated>2014-06-23T04:50:38Z</updated><resolved>2014-06-06T23:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-05T13:13:33Z" id="45217059">Thanks @mdaniel this is a nice cleanup! I left one comment inline, let me know what you think.
</comment><comment author="mdaniel" created="2014-06-05T18:08:26Z" id="45253969">I have updated the PR according to your comments, and hopefully added some additional sanity along the way. In my initial effort, I was trying not to touch parts of the code that were not directly related to my issue. So I hope I didn't go too far the other way this time. :-)

Also, while we're discussing this, if one provides a bogus `auto_expand_replicas` value, the server replies with `{"acknowledged":true}` which I feel is a bit misleading.

Is there a mechanism through which we can surface the fact that their setting was _not_ enacted?
</comment><comment author="jpountz" created="2014-06-06T17:49:12Z" id="45365102">Agreed that it is misleading. This method is called in a listener that is not supposed to throw exceptions, so I guess the validation would need to be performed up-front... May I propose to try to tackle it in another change, I think your current PR is already a great improvement!
</comment><comment author="mdaniel" created="2014-06-06T22:18:13Z" id="45389895">I can agree with that; is there anything else you need from me for this one?
</comment><comment author="nik9000" created="2014-06-06T22:37:34Z" id="45391026">Thanks for fixing this!  I can't be sure but I imagine @jpountz'll want to make sure you've signed the CLA and he _might_ want you to squash the commit.   I believe he'll merge it squashed but I can't be sure whether it's best if you squash it or if he does.
</comment><comment author="jpountz" created="2014-06-06T23:23:04Z" id="45393537">CLA is already signed, so all is good! I'll handle the squashing myself.
</comment><comment author="jpountz" created="2014-06-06T23:40:26Z" id="45394365">I merged the commits into b0a85f6ca38ff741f3d9d89e7cac8be38d970ed8 and took the liberty to make `ALL_NODES_VALUE` a constant as suggested by your comments. Thanks @mdaniel and @nik9000 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use ConcurrentHashMapV8 for lower memory overhead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6400</link><project id="" key="" /><description>In Java8, ConcurrentHashMap has lower memory overhead.  We are on Java 7 but can cherry-pick the Java8 impls from Doug Lea's jsr166e work.  In addition to CHMV8 I had to pull in some other deps from jsr166e ... I'm not certain they are all required (i.e., they may work with the Java 7 versions?) but I wanted to be safe.

I also fixed a couple places to use ConcurrentCollections to create a new CHM rather then instantiate on their own.
</description><key id="34895434">6400</key><summary>Use ConcurrentHashMapV8 for lower memory overhead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-03T19:19:55Z</created><updated>2015-06-07T13:19:01Z</updated><resolved>2014-06-05T14:48:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-05T13:16:53Z" id="45217387">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion mapping type throws a misleading error on null value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6399</link><project id="" key="" /><description>When a field is null, the completion mapping parser throws an error but it refers to the next field in the document, not the field with the completion type.

Another user had the same issue and reported it on your google group, but I don't see it here in the bug tracker. Here's the link he provided to reproduce the issue: 

https://gist.github.com/glade-at-gigwell/6408e0e4b69ddf2e8856

And the stack trace:

```
 {"acknowledged":true}[2014-05-18 13:40:24,150][INFO ][cluster.metadata         ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth] creating index, cause [api], shards [1]/[0], mappings []
 {"acknowledged":true}[2014-05-18 13:40:24,224][INFO ][cluster.metadata         ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth] create_mapping [object]
 {"acknowledged":true}[2014-05-18 13:40:24,245][INFO ][cluster.metadata         ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth] update_mapping [object] (dynamic)
 {"_index":"completion_type_cant_handle_the_null_truth","_type":"object","_id":"1","_version":1,"created":true}[2014-05-18 13:40:24,265][DEBUG][action.index             ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth][0], node[k4lbsgzYSlWzynQkVGqMaw], [P], s[STARTED]: Failed to execute [index      {[completion_type_cant_handle_the_null_truth][object][2], source[{"field1" : null,"field2" : "nulls make me sad"}]}]
 org.elasticsearch.index.mapper.MapperParsingException: failed to parse
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:540)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)
at      org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:384)
at      org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)
at      org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
at      org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
 Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: Unknown field name[field2], must be one of [payload, input, weight, output]
at      org.elasticsearch.index.mapper.core.CompletionFieldMapper.parse(CompletionFieldMapper.java:237)
at      org.elasticsearch.index.mapper.object.ObjectMapper.serializeNullValue(ObjectMapper.java:505)
at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:465)
at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)
... 8 more
```
</description><key id="34892023">6399</key><summary>Completion mapping type throws a misleading error on null value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">metysj</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-03T18:39:27Z</created><updated>2014-08-01T19:26:08Z</updated><resolved>2014-08-01T19:26:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update warmers.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6398</link><project id="" key="" /><description>Facets are slowly being deprecated
(http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html). Documents should move to providing examples with aggregations instead of facets.
</description><key id="34877407">6398</key><summary>Update warmers.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">plcstevens</reporter><labels><label>docs</label></labels><created>2014-06-03T15:56:37Z</created><updated>2014-06-18T10:59:29Z</updated><resolved>2014-06-05T17:23:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-04T07:52:33Z" id="45061042">Hi @plcstevens this makes a lot of sense, could  you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this in?
</comment><comment author="plcstevens" created="2014-06-05T11:05:05Z" id="45206459">Hi @javanna I've now signed the CLA. :)
</comment><comment author="javanna" created="2014-06-05T17:23:08Z" id="45248534">Thanks, merged!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Download on website is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6397</link><project id="" key="" /><description>I can't seem to find a valid link to download a current elasticsearch distribution. The download page links to

https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.0.zip

which throws an error back:

``` xml
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;Error&gt;
  &lt;Code&gt;NoSuchKey&lt;/Code&gt;
  &lt;Message&gt;The specified key does not exist.&lt;/Message&gt;
  &lt;Key&gt;elasticsearch/elasticsearch/elasticsearch-1.2.0.zip&lt;/Key&gt;
  &lt;RequestId&gt;FA2BF98075C3A1F4&lt;/RequestId&gt;
  &lt;HostId&gt;eJzrsGTmIY20zUfntuJlTuYYmAmppNgz4XCVeTC4RIqKwdNzLxEOfgvyp/kyQsPX&lt;/HostId&gt;
&lt;/Error&gt;
```

**EDIT**: I got it, the website links to an old release, changing the version number to 1.2.1 works: https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.1.zip
</description><key id="34876977">6397</key><summary>Download on website is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">niclashoyer</reporter><labels /><created>2014-06-03T15:52:27Z</created><updated>2014-06-04T07:55:18Z</updated><resolved>2014-06-03T15:54:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-04T07:54:48Z" id="45061212">Hi @niclashoyer we removed 1.2.0 from the downloads and released 1.2.1, full details [here](http://www.elasticsearch.org/blog/elasticsearch-1-2-1-released/).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Duplicate Documents In All Queries Following Upgrade to 1.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6396</link><project id="" key="" /><description>I'm not sure if this is a bug or corruption experienced during the upgrade but as of 1.2.0, we're now experiencing duplicate results in all queries:

{
took: 0
timed_out: false
_shards: {
total: 5
successful: 5
failed: 0
}
hits: {
total: 2
max_score: 13.58639
hits: [
{
_index: media6
_type: Media
_id: 534918829017389526_260014085,
_score: 13.58639,
_source: {}
},
{
_index: media6
_type: Media
_id: 534918829017389526_260014085,
_score: 12.815866,
_source: {}
}]
}}

There have been no changes to indexes, types, or other mappings. My understanding was that a unique ID really depends on index/type/id/routing.  The routing, by 
default, is derived from the ID. No parent-child relationships are in use here.

Cross-posted but unrelated to sort as initially suspected:
http://elasticsearch-users.115913.n3.nabble.com/Duplicate-Results-Following-Upgrade-to-1-2-0-amp-SortScript-td4056672.html
</description><key id="34876786">6396</key><summary>Duplicate Documents In All Queries Following Upgrade to 1.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nariman-haghighi</reporter><labels /><created>2014-06-03T15:50:40Z</created><updated>2014-06-04T08:05:53Z</updated><resolved>2014-06-04T08:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-04T08:05:53Z" id="45062118">Hi @nariman-haghighi, have a look at #6393. We released elasticsearch 1.2.1 with a fix for this, full details [here](http://www.elasticsearch.org/blog/elasticsearch-1-2-1-released/).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Should refactor top_hits aggregation to extend MetricsAggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6395</link><project id="" key="" /><description>Now that MetricsAggregations can support more than just doubles, we should refactor the top_hits aggregation to be a metrics aggregation since it is providing a metrics and not providing buckets to sub-aggregations.
</description><key id="34872717">6395</key><summary>Aggregations: Should refactor top_hits aggregation to extend MetricsAggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-06-03T15:10:21Z</created><updated>2014-06-10T07:11:37Z</updated><resolved>2014-06-10T07:11:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-03T15:11:35Z" id="44977242">+1
</comment><comment author="martijnvg" created="2014-06-06T14:52:01Z" id="45345317">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Result Set Format For Nested Aggregations</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6394</link><project id="" key="" /><description>Is it possible to request just the result provided by the last of a set of nest aggreagations only?
</description><key id="34869907">6394</key><summary>Result Set Format For Nested Aggregations</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">richardjennings</reporter><labels /><created>2014-06-03T14:42:47Z</created><updated>2014-12-30T19:29:18Z</updated><resolved>2014-12-30T19:29:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T12:22:33Z" id="49866184">Hi @richardjennings 

I'm afraid I don't understand the question at all.  Wondering if it might be related to the top-hits aggregation?
</comment><comment author="clintongormley" created="2014-12-30T19:29:18Z" id="68388584">No more info. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restore shard routing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6393</link><project id="" key="" /><description>Routing has been inadvertly changed in #5562 resulting in documents going to
different shards in 1.2. This is a terrible bug because an indexing request
would not necessarily go to the same shard anymore, potentially leading to
duplicates.

Close #6391
</description><key id="34866849">6393</key><summary>Restore shard routing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Core</label><label>bug</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-03T14:12:31Z</created><updated>2015-06-07T19:47:33Z</updated><resolved>2014-06-03T16:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-03T14:15:04Z" id="44969579">LGTM
</comment><comment author="jpountz" created="2014-06-03T14:27:11Z" id="44971186">Thanks @colings86 I updated the path and will push shortly...
</comment><comment author="andrassy" created="2014-06-03T15:39:36Z" id="44981099">If data is indexed with the broken 1.2 and then a fixed version is released then won't that, again, lead to data corruption. Really nasty bug :(
</comment><comment author="s1monw" created="2014-06-04T08:09:29Z" id="45062407">@andrassy Agreed, indexed data with 1.2.0 will not be compatible with 1.2.1. Yet, we are working on tooling / help for the problem. I also agree that this bug is really nasty.
</comment><comment author="micpalmia" created="2014-06-04T10:18:45Z" id="45074125">We had 0.90.7 installed, with no routing in place. We updated to 1.2.0 and reindexed everything with custom routing, that is now enforced using aliases (we don't explicitly route documents in the queries).

Does this mean that we might store (or might have stored) documents in the wrong shards, putting them in some kind of endless limbo? Any advice on how we should proceed?
</comment><comment author="bleskes" created="2014-06-04T11:01:48Z" id="45077364">@micpalmia sadly roughly 50% of the documents indexed with 1.2.0 is affected by this issue - regardless of whether custom routing is used or not. We're still evaluating the options for creating a tool to help identify (and hopefully solve) the problematic documents but in your case it will likely be half of your data. Since you've reindexed once I think your quickest option will be to reindex again once you upgrade to 1.2.1. Do note that 1.2.0 is consistent with it self - so if you indexed all data with 1.2.0 you're good to access it with 1.2.0. This may help with timing the reindexing.
</comment><comment author="micpalmia" created="2014-06-04T13:12:01Z" id="45088094">Thank you @bleskes, should we look somewhere specific for future updates about this issue?
</comment><comment author="nariman-haghighi" created="2014-06-04T16:41:29Z" id="45115371">Any updates here? This is about as ugly as it could get. It's shocking something like this made it out. Not only does it impact us, it impacts our customers and our SLAs. Is there a manual workaround at the moment? Deleting duplicates through a query and re-indexing? Re-indexing alone won't do it.
</comment><comment author="kimchy" created="2014-06-04T17:46:49Z" id="45125112">@nariman-haghighi we are working on a tool that will allow to go over the data and fix it, should be out in a day or 2.
</comment><comment author="s1monw" created="2014-06-04T18:53:03Z" id="45134967">@nariman-haghighi one manual workaround is to move to `1.2.1` and reindex into a new index. The new index will have everything in the right place. That said, it's not always feasible and we try to come up with a solution that is less painful.
</comment><comment author="nariman-haghighi" created="2014-06-06T19:12:09Z" id="45373732">The workaround of re-indexing into a new index worked for 2/3 indexed but something peculiar happens with the 3rd index. It has 454 documents (all unique Ids), but the attempt to re-index with the bulk API results in 1 document in the destination index. No changes to mappings. May be a NEST issue? /cc: @Mpdreamz 
</comment><comment author="jpountz" created="2014-06-06T23:44:42Z" id="45394527">@nariman-haghighi Have you checked if the bulk response contained errors? Additionally, http://www.elasticsearch.org/blog/tool-help-routing-issues-elasticsearch-1-2-0/ might help get your indices back to normal.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve ram efficiency of InternalEngine.versionMap</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6392</link><project id="" key="" /><description>When refreshes are infrequent and docs are small the versionMap will consume lots of RAM; we should improve its RAM efficiency, maybe parallel arrays ...
</description><key id="34865928">6392</key><summary>Improve ram efficiency of InternalEngine.versionMap</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label></labels><created>2014-06-03T14:02:35Z</created><updated>2015-03-19T15:42:47Z</updated><resolved>2014-12-30T21:44:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:28:55Z" id="68388548">@mikemccand is this still relevant?
</comment><comment author="mikemccand" created="2014-12-30T21:44:18Z" id="68401876">This is much less urgent, now that we clear the version map on refresh (not flush) and now that we trigger a refresh if version map is using too much RAM.  I think we should just keep the code simple (use the straightforward map that we use today)...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Two documents with the same _id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6391</link><project id="" key="" /><description>Hi there,
with the last version (1.2.0) I'm having documents sharing the same "_id".

Regards,
Giorgio
![es](https://cloud.githubusercontent.com/assets/527325/3160638/3dc2e6de-eb23-11e3-9bb8-d6f0c36046ef.png)
</description><key id="34862986">6391</key><summary>Two documents with the same _id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mindflayer</reporter><labels /><created>2014-06-03T13:30:39Z</created><updated>2014-06-04T07:56:37Z</updated><resolved>2014-06-03T14:41:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-03T13:32:30Z" id="44964415">Thank you for opening this issue. We are trying to resolve the problem as "we speak" and I will come back to you soon. It seems like there is a problem with 1.2 at this point.
</comment><comment author="mindflayer" created="2014-06-03T15:09:10Z" id="44976937">@jpountz: what if I'll upgrade my cluster with some indexes corrupted in this way?
</comment><comment author="stephlag" created="2014-06-04T07:56:37Z" id="45061385">It seems that it does not fix your indexes (see http://www.elasticsearch.org/blog/elasticsearch-1-2-1-released/)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices stats options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6390</link><project id="" key="" /><description>This PR fixes two bugs in indices stats:
-  `groups` and `types` were being ignored
- `completion_fields` with wildcards were not resolving the real field names

It also allows `groups` and `types` to accept wildcards, like `fields`, `completion_fields` and `fielddata_fields`

Added YAML tests for indices stats for: indices, metrics, level, fields, types, groups
</description><key id="34858568">6390</key><summary>Indices stats options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Stats</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-03T12:30:43Z</created><updated>2015-06-07T19:48:00Z</updated><resolved>2014-06-10T15:36:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-06-03T12:35:24Z" id="44958456">I've added YAML tests rather than Java test - are Java tests still required, even if they just duplicate the YAML tests?
</comment><comment author="bleskes" created="2014-06-04T09:31:30Z" id="45070123">Looking good! Left some comments
</comment><comment author="clintongormley" created="2014-06-04T10:28:17Z" id="45074843">@bleskes I've updated the PR in response to your comments. 
</comment><comment author="bleskes" created="2014-06-04T10:56:49Z" id="45076951">@clintongormley changes look good. What about the number of shards tests?
</comment><comment author="clintongormley" created="2014-06-05T10:31:04Z" id="45203873">@bleskes pushed changes to make the number of shards predetermined.
</comment><comment author="bleskes" created="2014-06-05T13:06:59Z" id="45216406">LGTM!
</comment><comment author="javanna" created="2014-06-05T13:27:38Z" id="45218500">My two cents on testing: the REST bug (groups and types were being ignored) can only be tested from a REST test. Beyond that, I'd love to see some Java test for these changes, although I do see that we have a very good coverage in terms of yaml tests. The reason is simply that we should not rely too much on the REST tests which are a bit of an afterthought and don't necessarily use all the randomizations that we use in our integration tests. Ideally we should be able to test the core without client tests as much as possible, and have a good coverage through Java tests.
</comment><comment author="s1monw" created="2014-06-05T13:30:11Z" id="45218786">+1 to what luca said! I think we should push towards java tests. Can we add those before pushing?
</comment><comment author="clintongormley" created="2014-06-06T15:49:28Z" id="45352108">Found a bug in the YAML tests where, thanks to randomization, primaries may not reflect, eg positive query count. Fixed that and added Java tests
</comment><comment author="clintongormley" created="2014-06-10T15:26:02Z" id="45628941">@javanna thanks for looking at the tests.  I've pushed a new commit which implements your changes.
</comment><comment author="javanna" created="2014-06-10T15:28:19Z" id="45629277">Great work @clintongormley !!!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reads better with a comma before the "but".</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6389</link><project id="" key="" /><description /><key id="34849765">6389</key><summary>Reads better with a comma before the "but".</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">violuke</reporter><labels><label>docs</label></labels><created>2014-06-03T10:18:35Z</created><updated>2014-07-16T21:44:19Z</updated><resolved>2014-06-03T12:23:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T12:23:59Z" id="44957399">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improved grammer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6388</link><project id="" key="" /><description /><key id="34845287">6388</key><summary>Improved grammer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">violuke</reporter><labels><label>docs</label></labels><created>2014-06-03T09:12:27Z</created><updated>2014-07-16T21:44:21Z</updated><resolved>2014-06-03T09:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T09:51:22Z" id="44944602">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[Security] Filter CRLF and semicolon characters in the `callback` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6387</link><project id="" key="" /><description>This could be a security issue in some specific contexts. At the very least one does not expect a callback parameter to be able to inject arbitrary javascript/HTML.

A request like the following

```
fitblip:~ fitblip$ curl '127.0.0.1:9200/?callback=alert("hrmmm");%0a%0dtest'
```

Results in the following JSONP response

```
alert("hrmmm");
test({
  "status" : 200,
  "name" : "Indra",
  "version" : {
    "number" : "2.0.0",
    "build_hash" : "cc63d795f3afd06107280c6ff431bd1a079ea278",
    "build_timestamp" : "2014-06-03T08:21:46Z",
    "build_snapshot" : true,
    "lucene_version" : "4.8"
  },
  "tagline" : "You Know, for Search"
}
);
```
</description><key id="34842822">6387</key><summary>[Security] Filter CRLF and semicolon characters in the `callback` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fitblip</reporter><labels /><created>2014-06-03T08:34:40Z</created><updated>2014-12-30T19:27:35Z</updated><resolved>2014-12-30T19:27:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Fitblip" created="2014-06-03T08:38:43Z" id="44938386">Actually, on second thought - a CRLF isn't even necessary to inject arbitrary JS, by separating things with a semi-colon a similar affect can be achieved. The right fix is to just strip out any CRLF and semicolons (since JS can have basically anything but those be a function name :D).

```
fitblip:~ fitblip$ curl '127.0.0.1:9200/?callback=alert("hrmmm");test'
```

```
alert("hrmmm");test({
  "status" : 200,
  "name" : "Indra",
  "version" : {
    "number" : "2.0.0",
    "build_hash" : "cc63d795f3afd06107280c6ff431bd1a079ea278",
    "build_timestamp" : "2014-06-03T08:21:46Z",
    "build_snapshot" : true,
    "lucene_version" : "4.8"
  },
  "tagline" : "You Know, for Search"
}
);
```
</comment><comment author="Fitblip" created="2014-06-03T09:00:02Z" id="44940135">As far as getting this fixed, we need to figure out how to hit #6164, as that may affect the actual impl of the filtering. 
</comment><comment author="clintongormley" created="2014-06-03T09:10:14Z" id="44940986">Well, identifying a valid JS function name is a bit more complex, eg:  http://www.geekality.net/2011/08/03/valid-javascript-identifier/

As in the solution given above, I'd ignore the need to support unicode escape sequences.  We could even make it more restrictive and just allow: letters, numbers, $, _ and - 
</comment><comment author="Fitblip" created="2014-06-03T19:51:30Z" id="45011866">Yeah, lots of different symbols can be used for JS function names (probably even un-printable Unicode characters), but since ES does all the logic to make sure it groks unicode, we shouldn't restrict that down per se. 

I think the real fix here is to just filter semi-colons and CRLF. If people want to put crazy characters into the callback parameter they can either get erroneous js out or valid js, but at least it won't have a security impact. 
</comment><comment author="clintongormley" created="2014-12-30T19:27:35Z" id="68388414">JSONP is now disabled by default and scheduled for removal #9108, which resolves this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the percentiles_rank aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6386</link><project id="" key="" /><description>The percentiles aggregation currently allows to return a percentile given a percentage. However, the data-structure that we are using under the hoods is also able to do the [reverse operation](https://github.com/tdunning/t-digest/blob/master/src/main/java/com/tdunning/math/stats/TDigest.java#L112): given a value, what percentage of the values from my dataset are below it?

For example, if you have a dataset of response times and provided it with a response time of 134 (ms), it would be able to tell you that this is the 72th percentile.
</description><key id="34839205">6386</key><summary>Added the percentiles_rank aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-03T07:50:20Z</created><updated>2015-06-06T18:31:24Z</updated><resolved>2014-06-18T11:16:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-06-03T12:59:38Z" id="44960852">@jpountz Do you see this as a separate aggregation from the percentiles aggregation, a mode on the percentiles aggregation which can be switched on with a parameter (changing the role of the current percents parameter), or something that can be run alongside the percents parameter (so the user can specify percents and cdf values)?
</comment><comment author="jpountz" created="2014-06-03T15:59:32Z" id="44983848">I think it would be nice to be able to run it alongside the percents parameter?
</comment><comment author="uboness" created="2014-06-03T16:37:09Z" id="44988790">+1

Re adding it to the existing `percentiles` agg, @jpountz, it's possible, but that would most definitely mean breaking the API (the response format at the minimum). Personally, I don't mind that, in fact, if we're already creating the data structure, better reuse it for both purposes.... just something to think about.
</comment><comment author="jpountz" created="2014-06-03T20:52:49Z" id="45018810">@uboness Good point. Another thing I'm wondering is how sorting would work if we expose the two metrics in a single aggregation (ie. what would keys look like?)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DOC:Added field data circuit breaker settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6385</link><project id="" key="" /><description>Added field data circuit breaker settings to list of update-able properties by the cluster update api.
</description><key id="34836385">6385</key><summary>DOC:Added field data circuit breaker settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahdeto</reporter><labels /><created>2014-06-03T06:53:05Z</created><updated>2014-06-25T18:40:04Z</updated><resolved>2014-06-25T18:40:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-06-03T08:03:16Z" id="44930653">Left a comment, but this looks good once that has been added, could you also please sign the [CLA](http://www.elasticsearch.org/contributor-agreement/) also so I can merge this in?
</comment><comment author="mahdeto" created="2014-06-08T15:01:40Z" id="45438857">Added the tag, signed the CLA
</comment><comment author="dakrone" created="2014-06-25T09:43:01Z" id="47080677">@mahdeto I see the `[float]` tag, but it looks like it's in the wrong place. Can you move to to the line before `=== Field data circuit breaker` please? If you can squash these changes into a single commit I will merge it in as well, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Data lose after downgrade from ES 1.2.0 to ES 1.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6384</link><project id="" key="" /><description>We have a test Elasticsearch cluster of version 1.1.1, and we upgraded to 1.2.0 recently. However because of ES 1.2.0 regression bug #6304, we decided to downgrade back to ES 1.1.1.    

During the downgrade process, our Elasticsearch cluster went red and log indicated that Lucene data format of ES 1.2.0 cannot be handled by ES 1.1.1. 
# Error message

[2014-06-03 01:39:30,526][WARN ][cluster.action.shard     ] [ES-DIAG-6-master] [logs-searchindexer-2014-06-03][4] received shard failed for [logs-searchindexer-2014-06-03][4], node[3joAufOMTieJkNXJ35Fg9Q], [P], s[INITIALIZING], indexUUID [BsSY5qHqSvOl-fQ8RXej0A], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[logs-searchindexer-2014-06-03][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[logs-searchindexer-2014-06-03][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: [segments_1, _checksums-1401755632980, write.lock]]; nested: IndexFormatTooNewException[Format version is not supported (resource: ChecksumIndexInput(MMapIndexInput(path="J:\IndexStorage\Data\ES-Cluster-CiDiag\nodes\0\indices\logs-searchindexer-2014-06-03\4\index\segments_1"))): 2 (needs to be between 0 and 1)]; ]]
</description><key id="34826248">6384</key><summary>Data lose after downgrade from ES 1.2.0 to ES 1.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JeffreyZZ</reporter><labels /><created>2014-06-03T01:53:53Z</created><updated>2015-02-05T12:41:51Z</updated><resolved>2014-06-03T05:40:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-03T05:40:18Z" id="44922018">You can not downgrade an elasticsearch version.
If you think you'll need to do that, you should in version 1.1.1 snapshot your indices using snapshot API, then upgrade to 1.2 and then if you need to downgrade, just restore your saved copy.

Closing. Feel free to reopen if you think it's an issue.
</comment><comment author="cvasii" created="2015-02-05T09:49:30Z" id="73019998">This is not cool at all. You guys should respect semantic versioning in the releases numbers. If it's incompatible, then increase the major in the version.
</comment><comment author="nik9000" created="2015-02-05T12:41:51Z" id="73039974">Elasticsearch mostly follows the letter of the semver requirements because
semver only talks about public API. So far as I can tell application
private data isn't covered even though it maybe should be.
 On Feb 5, 2015 4:49 AM, "cvasii" notifications@github.com wrote:

&gt; This is not cool at all. You guys should respect semantic versioning in
&gt; the releases numbers. If it's incompatible, then increase the major in the
&gt; version.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6384#issuecomment-73019998
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Allow deleting of interrupted snapshot</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6383</link><project id="" key="" /><description>If a snapshot is interrupted by running of disk space or connection loss to the repository folder, it might be not possible to delete such snapshot.
</description><key id="34823297">6383</key><summary>Snapshot/Restore: Allow deleting of interrupted snapshot</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v1.2.2</label><label>v1.3.0</label></labels><created>2014-06-03T00:35:47Z</created><updated>2014-09-30T08:13:45Z</updated><resolved>2014-07-01T02:14:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Can't start embedded ES after upgrade to 1.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6382</link><project id="" key="" /><description>Can not start embedded Elastic Search process after upgrading to 1.2.0.

Error says: 
A binding to org.elasticsearch.indices.analysis.IcuIndicesAnalysis was already configured at _unknown_.

I am starting embedded elastic search server from storm worker process. I have updated the plugins for version 1.2.0. Saw somewhere that this is due to older version of plugins in classpath but that is not the case here.

On startup, it gives me error.

https://gist.github.com/mkathuria/76a6883c16bb55b8b7fd

I logged the classpath (of the spawning process) while starting the embedded node.. This contains only storm jars in classpath.

https://gist.github.com/mkathuria/9476bb7bbd3f2a3c4101
</description><key id="34805871">6382</key><summary>Can't start embedded ES after upgrade to 1.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkathuria</reporter><labels><label>feedback_needed</label></labels><created>2014-06-02T20:04:16Z</created><updated>2015-05-04T14:20:16Z</updated><resolved>2015-02-28T05:15:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-06-02T20:08:32Z" id="44884539">Did you install ICU plugin version 2.2.0?

``` sh
bin/plugin -install elasticsearch/elasticsearch-analysis-icu/2.2.0
```
</comment><comment author="mkathuria" created="2014-06-03T09:24:27Z" id="44942264">It is happening in embedded node. I have already copied the icu installation jars from my production set up to the directory from where embedded node gets started.

Probably, the issue is that carrot2 is getting started even if it is not mentioned in the list of mandatory plugins.
</comment><comment author="clintongormley" created="2014-12-30T19:24:00Z" id="68388063">Hi @mkathuria 

Did you manage to figure this out?  Can this issue be closed?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Disable circuit breaking for BigArrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6381</link><project id="" key="" /><description>The BigArrays limit is currently shared by the translog, netty, http and some
queries/aggregations. If any of these consumers starts taking a lot of memory,
then other ones might fail to allocate memory, which could have bad
consequences, eg. if ping requests can't be sent. The plan is to come up with
a better solution in 1.3.

Close #6332
</description><key id="34792609">6381</key><summary>Disable circuit breaking for BigArrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Circuit Breakers</label><label>bug</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-02T17:11:47Z</created><updated>2015-06-07T19:48:09Z</updated><resolved>2014-06-03T13:36:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-02T17:55:47Z" id="44869937">Is this a "don't upgrade to 1.20" kind of thing?
</comment><comment author="s1monw" created="2014-06-02T19:15:01Z" id="44878752">LGTM

@nik9000 we will likely do a bugfix release soonish so this can cause problems but the workaround is straight forward setting this to unlimited
</comment><comment author="s1monw" created="2014-06-03T13:36:26Z" id="44964837">closing...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CORS: Allowed to configure allow-credentials header to work via SSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6380</link><project id="" key="" /><description>Elasticsearch supports CORS and Basic Authentication but not together. A browser sending a cross-origin request will omit credentials unless the XHR `withCredentials` field is set to `true`. If `withCredentials = true` there are two additional checks the browser must perform on the preflight response before sending the real request.
- Preflight response `Access-Control-Allow-Origin` must be the Origin (host), not `*`
- Preflight response must have header `Access-Control-Allow-Credentials: true`

The W3C spec [http://www.w3.org/TR/access-control/#resource-sharing-check-0](http://www.w3.org/TR/access-control/#resource-sharing-check-0) explains the CORS withCredentials check.

These headers can be added with 2 lines of code in `org.elasticsearch.http.netty.NettyHttpChannel.java` at line 95.

``` java
// Add support for cross-origin Ajax requests (CORS)
resp.headers().add("Access-Control-Allow-Origin", transport.settings().get("http.cors.allow-origin", HttpHeaders.getHeader(nettyRequest, "Origin", "*")));
resp.headers().add("Access-Control-Allow-Credentials", transport.settings().get("http.cors.allow-credentials", "true"));
```

This would make the `Access-Control-Allow-Origin` header fall back to the `Origin` header before falling back to `*`. It would also add the new header `Access-Control-Allow-Credentials: true`.
## Reproduce steps
1. Start an elasticsearch instance on `localhost:9200`
2. Open a browser window with any domain other than `localhost:9200`. I'm using an empty Chrome tab in my example.
3. Run this JavaScript to send a request with basic authentication

``` js
var xhr = new XMLHttpRequest();
xhr.open('POST', 'http://user:passwd@localhost:9200', true);
xhr.withCredentials = true;
xhr.setRequestHeader('Content-Type', 'application/json');
xhr.send('{"query":{"match_all":{}}}');
```

Preflight Request

```
OPTIONS http://localhost:9200/ HTTP/1.1
Host: localhost:9200
Connection: keep-alive
Cache-Control: no-cache
Authorization: Basic YWRtaW46d2VzdA==
Access-Control-Request-Method: POST
Pragma: no-cache
Origin: https://www.google.com
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.116 Safari/537.36
Access-Control-Request-Headers: content-type
Accept: */*
Accept-Encoding: gzip,deflate,sdch
Accept-Language: en-US,en;q=0.8,et;q=0.6
```

Preflight Response

```
HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Access-Control-Max-Age: 3
Access-Control-Allow-Methods: OPTIONS, HEAD, GET, POST, PUT, DELETE
Access-Control-Allow-Headers: X-Requested-With, Content-Type, Content-Length
Content-Type: text/plain; charset=UTF-8
Content-Length: 0
```

The browser throws an error at this point saying the `Access-Control-Allow-Origin` and `Access-Control-Allow-Credentials` headers are bad.
### Notes

After you make the code changes the browser will start to cache preflight requests. When this happens you will only see the real request in Fiddler. To clear the cache, close the window and open a new one.

The XHR must be a [non-simple request](http://www.w3.org/TR/access-control/#simple-method). This is a rather vague spec but a POST with a content type of application/json should do it.

IE skips most of these checks so don't bother testing in IE.

The example preflight request header `Authorization: Basic YWRtaW46d2VzdA==` is a bug in Chrome and shouldn't be included until the real request. There is a bug open for this here https://code.google.com/p/chromium/issues/detail?id=377541. Firefox works correctly and doesn't include the credentials on the preflight.
</description><key id="34787144">6380</key><summary>CORS: Allowed to configure allow-credentials header to work via SSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">erikringsmuth</reporter><labels><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-02T16:06:04Z</created><updated>2016-05-06T08:46:22Z</updated><resolved>2014-08-05T15:34:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-21T10:34:54Z" id="49591221">/cc @spinscale @rashidkpc 
</comment><comment author="spinscale" created="2014-07-25T11:34:55Z" id="50137923">Hey,

first, thanks a lot for such a long and already very well described issue, makes it easy for me to followup.

Regarding sending the Origin headers, I just added https://github.com/elasticsearch/elasticsearch/pull/6923 to the master and 1.4 branch. I think this should already suffice you, right? If you wanted to allow everything, you could simply use the new regex feature and allow all origin headers.

Your second request is not yet solved, as far as I read it at https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS?#Requests_with_credentials - there is nothing more to do, than providing this with another configuration option, right (so basically incorporating your suggestion)?

I do get it correctly, that there is no real possibility on the server side, that credentials are sent, right? .There _might_ be a cookie and there _might_ be an auth header. As this is not my area of expertise, feel free to correct and I would be happy to get feedback on the above PR and if it solves your first problem, which I think it should.
</comment><comment author="erikringsmuth" created="2014-07-25T18:32:32Z" id="50187071">Hey @spinscale,

The regex change works great for the Access-Control-Allow-Origin header! I tested it out and it resolves to the caller's host. I then get the next CORS warning about credentials. Adding this code gets it completely working.

src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java line 100

``` java
if (transport.settings().getAsBoolean("http.cors.enabled", true)) {
    /// ... existing checks

    // new check starting at line 115
    if (transport.settings().getAsBoolean("http.cors.allow-credentials", false)) {
        resp.headers().add(ACCESS_CONTROL_ALLOW_CREDENTIALS, "true");
    }
}
```

Then I set up my `elasticsearch.yml` like this.

```
http.cors.allow-origin: "/.*/"
http.cors.allow-credentials: true
```

Now running this JavaScript succeeds. It actually returns a 404 but this is the correct behavior since a `POST http://localhost:9200/ HTTP/1.1` is invalid. The good news is it doesn't give any CORS warnings at this point.

``` js
var xhr = new XMLHttpRequest();
xhr.open('POST', 'http://user:passwd@localhost:9200', true);
xhr.withCredentials = true;
xhr.setRequestHeader('Content-Type', 'application/json');
xhr.send('{"query":{"match_all":{}}}');
```
</comment><comment author="digitalpacman" created="2016-04-19T16:19:01Z" id="212002380">+1

Spent a whole day thinking it was my fault on how I setup cors on Grafana.

Could you please release this ASAP. 
</comment><comment author="rachellji" created="2016-05-05T21:53:58Z" id="217290967">happened to me too. I'm using 2.0. It was working fine. today I couldn't search and got "Request header field X-CSRF-TOKEN is not allowed by Access-Control-Allow-Headers in preflight response"
</comment><comment author="clintongormley" created="2016-05-06T08:46:21Z" id="217386124">@rachellji you just need to update your config to list that header
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing: Clear versionMap on refresh not flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6379</link><project id="" key="" /><description>Initial pull request; tests pass, but I have some nocommits.

I use 3 separate maps.  2 are for adds (current + old), and current becomes old when refresh begins, and old is cleared once refresh finishes (like Lucene's LiveFieldValues).  The other map separately tracks / expires deletes.
</description><key id="34785422">6379</key><summary>Indexing: Clear versionMap on refresh not flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label></labels><created>2014-06-02T15:48:09Z</created><updated>2014-07-16T12:24:09Z</updated><resolved>2014-06-04T09:37:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-06-03T00:42:49Z" id="44908239">I think this is ready; I ran a quick perf test and it's basically the same as master, within noise.  But the test had a big enough heap that there was no GC pressure; if there were GC pressure I think the patch would do much better.
</comment><comment author="s1monw" created="2014-06-03T09:03:55Z" id="44940456">left some small comments - I think it looks good
</comment><comment author="mikemccand" created="2014-06-03T09:13:49Z" id="44941289">Also, I think the deleteByQuery method is broken today: it will delete entries from the versionMap even when the query didn't match that UID, and then an update to that UID with a stale version will succeed when it should have failed.  With this change, it's still broken, just in a different way, because we no longer drop the added UIDs except on refresh.
</comment><comment author="mikemccand" created="2014-06-03T11:34:16Z" id="44952922">Actually I'm wrong about deleteByQuery: we force a refresh in refreshVersioningTable which ensures all the added UIDs are visible in the searcher, so it's safe to prune them at that point.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing: If versionMap is too large we should trigger refresh</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6378</link><project id="" key="" /><description>In #6363 we are fixing versionMap to clear on refresh not flush, but even once we fix that, if the user sets a big refresh (or turns it off), and big xlog flush triggers, then the versionMap can grow unbounded.  I think we should trigger refresh once versionMap is too large?
</description><key id="34780825">6378</key><summary>Indexing: If versionMap is too large we should trigger refresh</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-02T15:00:56Z</created><updated>2014-07-16T12:24:32Z</updated><resolved>2014-07-08T16:15:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-07-08T16:15:11Z" id="48362088">Fixed with #6443
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lower the translog flush triggers to workaround #6363</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6377</link><project id="" key="" /><description>If we don't do #6363 for 1.2.1 then we should consider lowering the translog flush triggers (which we increased in 1.2 to prevent too-frequent commits), because the versionMap is only cleared on flush not on refresh.  It can easily tie up a lot of RAM, put GC pressure, etc.; when Full GC does run it takes a looong time w/ that map.
</description><key id="34780483">6377</key><summary>Lower the translog flush triggers to workaround #6363</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>:Translog</label><label>bug</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-02T14:57:37Z</created><updated>2015-06-07T19:48:25Z</updated><resolved>2014-06-03T14:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>TransportClient: Improve logging, fix minor issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6376</link><project id="" key="" /><description>In order to return more information to the client, in case a TransportClient
can not connect to the cluster, this commit adds logging and also returns the
configured nodes in the NoNodeAvailableException

Also a minor bug has been fixed, which propagated exceptions wrong, so that an
invalid request was actually tried on every node, if a regular connection failure
on the first tried node had happened.
</description><key id="34766089">6376</key><summary>TransportClient: Improve logging, fix minor issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Java API</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-06-02T11:57:44Z</created><updated>2015-06-07T13:19:07Z</updated><resolved>2014-06-10T11:17:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-06-02T14:39:41Z" id="44845029">Few notes:
- One behavior that was removed is the part where we take a "reference" of the nodes class variable, and only work on it from then on. Its important since it might be replaces with a new "sniff/ping" operation
- can we have the last exception in the NoNodes exception?
- if we log warn, it can completely fill the log for the user, maybe debug?
</comment><comment author="spinscale" created="2014-06-03T15:45:01Z" id="44981842">updated the PR. Not sure what you meant with including the last exception, there was only one occurence where this was possible to add, where I added it
</comment><comment author="kimchy" created="2014-06-08T23:58:00Z" id="45452496">`ensureNodesAreAvailable` should get a reference to nodes, that we sample at the start, and use thought the method call, otherwise, we might work on a changed nodes variable (becoming empty after check). Same for the exception message that we build, all should work with a sampled nodes variable.
</comment><comment author="spinscale" created="2014-06-10T08:33:38Z" id="45587286">fell back to the old behaviour, so that changing the `nodes` variable will not have any effect on the exception message content or the nodes to try out 
</comment><comment author="kimchy" created="2014-06-10T08:40:21Z" id="45587898">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory management: Raise the default max memory usage for BigArrays to 50%.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6375</link><project id="" key="" /><description>The value had been chosen by taking into account the fact that eg. the filter
cache takes 10% and field data 60%. However, if you either don't need field
data or if you use doc values to store data on disk, then this would just leave
a lot of memory unused on your node. We will fortunately have a better story
for this once able to share memory accounting across several breakers.
</description><key id="34759523">6375</key><summary>Memory management: Raise the default max memory usage for BigArrays to 50%.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2014-06-02T10:03:32Z</created><updated>2014-06-17T12:23:47Z</updated><resolved>2014-06-02T17:14:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-02T10:18:23Z" id="44821452">should we really change defaults in minor releases?
</comment><comment author="jpountz" created="2014-06-02T10:26:08Z" id="44822028">I guess it depends on whether you consider it a bugfix as well. Since this one is not too bad and can be worked-around by updating an node setting, I removed the `1.2.1` label.
</comment><comment author="dakrone" created="2014-06-02T11:17:50Z" id="44825572">LGTM, +1 to increase until we have shared memory accounting
</comment><comment author="dakrone" created="2014-06-02T12:03:12Z" id="44828919">@s1monw spoke with @jpountz about this, I think we should increase the BigArrays limit as a temporary bugfix until it can use shared circuit breakers, so this would be a bugfix for 1.2.1.
</comment><comment author="jpountz" created="2014-06-02T17:14:14Z" id="44865101">Closed in favor of https://github.com/elasticsearch/elasticsearch/pull/6381
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory management: do not enforce the BigArrays limit on the network layer and the tranlog.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6374</link><project id="" key="" /><description>This commit disables memory circuit breaking in BigArrays on FsTranslog,
NettyHttpServerTransport and NettyTransport so that these components are not
impacted by heavy search requests.

Close #6332
</description><key id="34758944">6374</key><summary>Memory management: do not enforce the BigArrays limit on the network layer and the tranlog.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-06-02T09:53:50Z</created><updated>2014-06-13T10:24:18Z</updated><resolved>2014-06-02T17:13:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-06-02T10:24:10Z" id="44821864">The commit looks good but I worry about the scenario that transmitting a large cluster state (for example), still counts towards all other requests because of the shared counter. This will cause all other requests to fail until the bytes are released. I wonder if we should just introduce a dedicated buffer for the cluster state/translog/critical memory users that shouldn't be blocked.
</comment><comment author="jpountz" created="2014-06-02T10:33:41Z" id="44822547">&gt; This will cause all other requests to fail until the bytes are released.

I think this is a good property? Otherwise the risk is that the node would go out-of-memory which is worse? On a related note, the case that you just described was my motivation to open #6375 and try to make sure we do not set a limit that is too low as @kevinkluge mentioned on #6332.

Maybe the right fix for now would be to just completely back out this feature until we have a proper way to share memory across breakers.
</comment><comment author="bleskes" created="2014-06-02T11:20:20Z" id="44825728">what worries me is that if you have a constant load of request that just works, and suddenly there comes a big cluster state update that pushes the buffer above a limit, you only see it in the requests and will not have a way of knowing what happened. This will be hard to debug because cluster state publishing is internal to ES. One alternative is  log a warning in the log for that case? (letting X through, but limit has been breached). Another is to say the CS and translog is using their own unlimitted BigArrays as the PR doesn't limit it anyway. 
</comment><comment author="dakrone" created="2014-06-02T11:48:17Z" id="44827785">I think the safest thing to do is to leave the breaker as-is, but increase it to a very high amount until we're able to share memory across breakers. I think OOME and rejecting CS/translog are both bad, so I think I'd rather wait until we had it using a breaker that had the potential to shrink (since BigArray pages don't currently ever shrink), since otherwise we could end up overloading a cluster that could potentially recover while remaining under the memory limit.
</comment><comment author="jpountz" created="2014-06-02T17:13:28Z" id="44865001">After more discussions with @bleskes @dakrone and others, we decided to postpone it to 1.3. Closed in favor of https://github.com/elasticsearch/elasticsearch/pull/6381.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dataloss after timedout fixindex on master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6373</link><project id="" key="" /><description>We had one shard which couldn't be started because of a corrupt index.

We added the setting to the node to check for corrupt indexes and restarted. As the node was one of our back nodes, with more data, checking the indexes took more time.

An exception turned up on the master:

[2014-05-28 01:39:36,779][DEBUG][cluster.service          ] [server5N16] processing [shard-failed ([2014.04.20.0000_000][1], node[pDQlF6xWSW2X4npbfqKEoQ], [R], s[INITIALIZING]), reason [Failed to start shard, message [RecoveryFailedException[[2014.04.20.0000_000][1]: Recovery failed from [I61N3][y0f8jdjkTDe4fyeaY1BwOw][i61n3][inet[/10.20.39.3:9300]]{_cluster=HR61, service=searchdata, _scluster=SEARCH1, max_local_storage_nodes=1, master=false, river=_none_} into [I59N7][pDQlF6xWSW2X4npbfqKEoQ][i59n7][inet[/10.20.34.7:9300]]{_cluster=HR59, service=searchdata, _scluster=SEARCH1, max_local_storage_nodes=1, master=false, river=_none_}]; nested: RemoteTransportException[[I61N3][inet[/10.20.39.3:9300]][index/shard/recovery/startRecovery]]; nested: RecoveryEngineException[[2014.04.20.0000_000][1] Phase[2] Execution failed]; nested: ReceiveTimeoutTransportException[[I59N7][inet[/10.20.34.7:9300]][index/shard/recovery/prepareTranslog] request_id [2828908] timed out after [900000ms]]; ]]]: done applying updated cluster_state (version: 2220) {elasticsearch[server5N16][clusterService#updateTask][T#1]}

I think we tried to open/close the index a few times afterwards, and then restarted the node 2 times.We didn't restart the master node afterwards.

The second restart lead the node to delete the shard (deleting unneeded shard message), causing the shard to be completely lost.

Maybe the master node thought that no node would have the shard anymore because of the exception above?

Unfortunately I don't have more information on this.
</description><key id="34756898">6373</key><summary>Dataloss after timedout fixindex on master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bluelu</reporter><labels /><created>2014-06-02T09:21:21Z</created><updated>2014-06-02T17:04:25Z</updated><resolved>2014-06-02T17:04:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bluelu" created="2014-06-02T17:04:25Z" id="44863644">After reopening the index today, the shard reappeared (from another node I guess). Issue can be closed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When starting up, recovery of shards takes up to 50 minutes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6372</link><project id="" key="" /><description>We have about 1000 indexes, 20k shards and over 400 nodes in our cluster.

When we restart the cluster, it takes about 50 minutes to reach yellow state. All nodes (including the master node) seem to be idling.

It doesn't seem to be traffic related (the clsuter state is only sent more often later when yellow state is being reached).

The master node seems to only request shard recovery every 11 minutes (but not for all shards), causing the long wait.

[2014-05-27 13:41:53,300][DEBUG][index.gateway            ] [server5N5] [2013.06.24.0000_000][4] starting recovery from local ... {elasticsearch[server5N5][generic][T#1]}
[2014-05-27 13:41:53,325][DEBUG][index.gateway            ] [server5N5] [2014.03.13.0000_000][1] starting recovery from local ... {elasticsearch[server5N5][generic][T#4]}
[2014-05-27 13:41:53,359][DEBUG][index.gateway            ] [server5N5] [2012.07.06.0000_000][5] starting recovery from local ... {elasticsearch[server5N5][generic][T#5]}
[2014-05-27 13:41:53,367][DEBUG][index.gateway            ] [server5N5] [2013.03.30.0000_000][4] starting recovery from local ... {elasticsearch[server5N5][generic][T#6]}
[2014-05-27 13:41:53,374][DEBUG][index.gateway            ] [server5N5] [2012.05.27.0000_000][0] starting recovery from local ... {elasticsearch[server5N5][generic][T#7]}
[2014-05-27 13:41:53,462][DEBUG][index.gateway            ] [server5N5] [2012.09.05.0000_000][0] starting recovery from local ... {elasticsearch[server5N5][generic][T#8]}
[2014-05-27 13:41:53,469][DEBUG][index.gateway            ] [server5N5] [2013.06.17.0000_000][2] starting recovery from local ... {elasticsearch[server5N5][generic][T#9]}
[2014-05-27 13:41:53,552][DEBUG][index.gateway            ] [server5N5] [2012.05.05.0000_000][9] starting recovery from local ... {elasticsearch[server5N5][generic][T#10]}
[2014-05-27 13:41:53,635][DEBUG][index.gateway            ] [server5N5] [2012.05.22.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#10]}
[2014-05-27 13:41:53,642][DEBUG][index.gateway            ] [server5N5] [2014.01.26.0000_000][0] starting recovery from local ... {elasticsearch[server5N5][generic][T#11]}
[2014-05-27 13:41:53,648][DEBUG][index.gateway            ] [server5N5] [2012.11.09.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#12]}
[2014-05-27 13:41:53,678][DEBUG][index.gateway            ] [server5N5] [2013.12.26.0000_000][1] starting recovery from local ... {elasticsearch[server5N5][generic][T#13]}
[2014-05-27 13:41:53,685][DEBUG][index.gateway            ] [server5N5] [2014.01.11.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#14]}
[2014-05-27 13:41:53,693][DEBUG][index.gateway            ] [server5N5] [2013.11.04.0000_000][1] starting recovery from local ... {elasticsearch[server5N5][generic][T#15]}
[2014-05-27 13:41:53,715][DEBUG][index.gateway            ] [server5N5] [2014.02.18.0000_000][3] starting recovery from local ... {elasticsearch[server5N5][generic][T#16]}
[2014-05-27 13:41:53,739][DEBUG][index.gateway            ] [server5N5] [2012.06.23.0000_000][4] starting recovery from local ... {elasticsearch[server5N5][generic][T#17]}
[2014-05-27 13:41:53,747][DEBUG][index.gateway            ] [server5N5] [2013.07.07.0000_000][8] starting recovery from local ... {elasticsearch[server5N5][generic][T#18]}
[2014-05-27 13:41:53,754][DEBUG][index.gateway            ] [server5N5] [2013.05.12.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#19]}
[2014-05-27 13:41:53,804][DEBUG][index.gateway            ] [server5N5] [2013.02.15.0000_000][7] starting recovery from local ... {elasticsearch[server5N5][generic][T#20]}
[2014-05-27 13:41:53,995][DEBUG][index.gateway            ] [server5N5] [2013.09.14.0000_000][4] starting recovery from local ... {elasticsearch[server5N5][generic][T#21]}
[2014-05-27 13:41:54,001][DEBUG][index.gateway            ] [server5N5] [2012.11.04.0000_000][5] starting recovery from local ... {elasticsearch[server5N5][generic][T#22]}
[2014-05-27 13:41:54,007][DEBUG][index.gateway            ] [server5N5] [2013.11.17.0000_000][3] starting recovery from local ... {elasticsearch[server5N5][generic][T#23]}
[2014-05-27 13:41:54,014][DEBUG][index.gateway            ] [server5N5] [2014.03.15.0000_000][9] starting recovery from local ... {elasticsearch[server5N5][generic][T#24]}
[2014-05-27 13:41:54,088][DEBUG][index.gateway            ] [server5N5] [2014.02.17.0000_000][7] starting recovery from local ... {elasticsearch[server5N5][generic][T#25]}
[2014-05-27 13:41:54,095][DEBUG][index.gateway            ] [server5N5] [2013.08.05.0000_000][9] starting recovery from local ... {elasticsearch[server5N5][generic][T#26]}
[2014-05-27 13:41:54,500][DEBUG][index.gateway            ] [server5N5] [2013.07.29.0000_000][0] starting recovery from local ... {elasticsearch[server5N5][generic][T#27]}
[2014-05-27 13:41:54,507][DEBUG][index.gateway            ] [server5N5] [2014.04.02.0000_000][4] starting recovery from local ... {elasticsearch[server5N5][generic][T#28]}
[2014-05-27 13:41:54,514][DEBUG][index.gateway            ] [server5N5] [2013.11.23.0000_000][7] starting recovery from local ... {elasticsearch[server5N5][generic][T#29]}
[2014-05-27 13:41:54,520][DEBUG][index.gateway            ] [server5N5] [2013.01.04.0000_000][5] starting recovery from local ... {elasticsearch[server5N5][generic][T#30]}
[2014-05-27 13:41:54,526][DEBUG][index.gateway            ] [server5N5] [2013.05.15.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#31]}
[2014-05-27 13:53:33,567][DEBUG][index.gateway            ] [server5N5] [2012.12.13.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#307]}
[2014-05-27 13:53:33,645][DEBUG][index.gateway            ] [server5N5] [2014.01.19.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#296]}
[2014-05-27 13:53:33,732][DEBUG][index.gateway            ] [server5N5] [2014.02.06.0000_000][4] starting recovery from local ... {elasticsearch[server5N5][generic][T#295]}
[2014-05-27 13:53:33,760][DEBUG][index.gateway            ] [server5N5] [2012.05.12.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#294]}
[2014-05-27 13:53:33,816][DEBUG][index.gateway            ] [server5N5] [2013.01.09.0000_000][0] starting recovery from local ... {elasticsearch[server5N5][generic][T#313]}
[2014-05-27 13:53:34,005][DEBUG][index.gateway            ] [server5N5] [2013.06.05.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#288]}
[2014-05-27 13:53:34,099][DEBUG][index.gateway            ] [server5N5] [2012.11.25.0000_000][2] starting recovery from local ... {elasticsearch[server5N5][generic][T#257]}
[2014-05-27 13:53:34,257][DEBUG][index.gateway            ] [server5N5] [2012.09.23.0000_000][3] starting recovery from local ... {elasticsearch[server5N5][generic][T#303]}
[2014-05-27 13:53:34,821][DEBUG][index.gateway            ] [server5N5] [2012.06.21.0000_000][6] starting recovery from local ... {elasticsearch[server5N5][generic][T#304]}
</description><key id="34756328">6372</key><summary>When starting up, recovery of shards takes up to 50 minutes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bluelu</reporter><labels><label>:Allocation</label></labels><created>2014-06-02T09:11:51Z</created><updated>2015-12-28T19:08:10Z</updated><resolved>2015-12-28T19:08:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-02T09:18:57Z" id="44816799">hey,

can you give us some more info about what version of ES you are running?
</comment><comment author="bluelu" created="2014-06-02T09:26:02Z" id="44817404">Hi, we use elasticsearch-1.0.2

Please keep in mind that his might also be related to
https://github.com/elasticsearch/elasticsearch/issues/6295
https://github.com/elasticsearch/elasticsearch/issues/5232

We set discovery.zen.publish_timeout:0 in order to start the cluster. Otherwise nodes wouldn't even join at first before the master node starts the recovery.
</comment><comment author="s1monw" created="2014-06-02T09:27:52Z" id="44817581">it seems like you are setting a lot of things on the cluster. can you provide the settings you are using on the cluster aside of the defaults.
</comment><comment author="bluelu" created="2014-06-02T09:44:07Z" id="44818859">Here it is:

Our river + master nodes have identical configuration, except that data node and master have datadir/master flags switched.

We only use one master in order to not have the split brain issue.

```
# Configuration Template for ES
# used by es data nodes (river is disabled)


# Set publish timeout to 0, otherwise cluster startup takes long time
# See https://github.com/elasticsearch/elasticsearch/issues/5232
discovery.zen.publish_timeout: 0


#lock memory
bootstrap.mlockall: true

# Cluster Settings
cluster:
  name: xxx

node:
  name: xxx
  t_cluster: xxx
  t_scluster: xxx
  master: false
  data: true
  max_local_storage_nodes: 1
  service: xxx

  # disable river allocation to data node
  river: _none_

#disabled cluster awareness -&gt; allocation settings are now being set on index level.
#cluster.routing.allocation.awareness.attributes: rack_id

cluster.routing.allocation.node_initial_primaries_recoveries: 30
cluster.routing.allocation.node_concurrent_recoveries: 8
indices.recovery.max_bytes_per_sec: 100mb

path:
  logs: /tmp/searchdata_log/
  work: /elasticsearch_work/
  data: /ed1/,/ed2/

index:
  number_of_replicas: 1
  number_of_shards: 1
  query:
    bool:
      max_clause_count: 10240

# When changing the analysis settings, don't forget to also update
# com.xxx.modules.searchclient.analysis.SpecialNGramAnalyzer and
# com.xxx.modules.searchclient.analysis.xxxDefaultAnalyzer
  analysis:
    analyzer:
      xxx:
        type: custom
        tokenizer: uax_url_email
        filter: [standard, lowercase, elision_split, asciifolding]
      xxx-exact:
        type: custom
        tokenizer: whitespace
        filter: [lowercase, exact-match]
      xxx-exact-cs:
        type: custom
        tokenizer: whitespace
        filter: [exact-match]
      patternnewline:
        type: pattern
        pattern: "[\n\r]+"
        lowercase: false

# increase cache filter size to 30% (default is 20%)
indices.cache.filter.size: 30%

# Gateway Settings
gateway:
  recover_after_data_nodes: 425
  recover_after_time: 2m
  expected_data_nodes: 430
#gateway.local.compress: false
#gateway.local.pretty: true

# Use fixed ports, prevents es from starting multiple instances
# Should also already be prevented by max_local_storage_nodes set to 1
http.port: 9200
transport.tcp.port: 9300

discovery:
  zen.ping_timeout: 5s
  zen.minimum_master_nodes: 1
  zen.ping:
    multicast:
      enabled: false
    unicast:
      hosts: masternode[9300]
  zen.fd.ping_interval: 3s
  zen.fd.ping_timeout: 60s

script.native.relevance.type: com.xxx.modules.search.es.script.RelevanceScore
script.native.orelevance.type: com.xxx.modules.search.es.script.OldRelevanceScore
# script.native.sortvalue.type: com.xxx.modules.search.es.script.SortValue
script.native.dynamicdata.type: com.xxx.modules.search.es.script.DynamicDataScore
script.native.updateproject.type: com.xxx.modules.search.es.script.UpdateScript
script.native.projectcopies.type: com.xxx.modules.search.es.script.ProjectCopiesScript
script.native.propagatescript.type: com.xxx.modules.search.es.script.PropagateScript

# disable deleting of indices in a single API call
action.disable_delete_all_indices: true
# disable automatic index creation during indexation
action.auto_create_index: false

# since _all is disabled for our documents, we have to define another default_field
# edit: commented out, as the default_field has to be specified anyway during searching (content+title)
#       so we can leave the _all field enabled for auxiliary indices
# index.query.default_field: content

# disable automatic index creation for indices
action.auto_create_index: -dsearch_*,-nsearch_*,-tw_*,-twindex_*,-twproj_*,-twprojindex_*,-tw-*,+*

################################## Slow Log ##################################
# Shard level query and fetch threshold logging.

index.search.slowlog.level: TRACE
index.search.slowlog.threshold.query.warn: 30s
index.search.slowlog.threshold.query.info: 15s
index.search.slowlog.threshold.query.debug: 5s
index.search.slowlog.threshold.query.trace: 1s

index.search.slowlog.threshold.fetch.warn: 15s
index.search.slowlog.threshold.fetch.info: 10s
index.search.slowlog.threshold.fetch.debug: 5s
index.search.slowlog.threshold.fetch.trace: 1s

################################## GC Logging ################################

monitor.jvm.gc.ParNew.warn: 1000ms
monitor.jvm.gc.ParNew.info: 700ms
monitor.jvm.gc.ParNew.debug: 400ms

monitor.jvm.gc.ConcurrentMarkSweep.warn: 10s
monitor.jvm.gc.ConcurrentMarkSweep.info: 5s
monitor.jvm.gc.ConcurrentMarkSweep.debug: 2s
```
</comment><comment author="bluelu" created="2014-06-05T09:15:39Z" id="45197714">Please let me know if you need any further information. thanks
</comment><comment author="miccon" created="2014-09-15T08:23:34Z" id="55563354">While restarting with 1000 indexes, 20k shards and 500 nodes, the master node took 15 minutes to get to the initial allocation.  We are still running elasticsearch 1.0.2.
Commit 6af80d501797903e3a3b627cd7cc331e6806bc38 does some optimizations about the allocation, but it seems that the optimization is only after getting the shard stores, although our nodes are mostly waiting for the responses from the cluster. The load on the master node is low (network and cpu wise) during the whole time:

```
"elasticsearch[I51N16][clusterService#updateTask][T#1]" daemon prio=10 tid=0x00007fc420005000 nid=0x2234 waiting on condition [0x00007fc4d5bc9000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  &lt;0x00000006ca9c15e8&gt; (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.buildShardStores(LocalGatewayAllocator.java:441)
        at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:279)
        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:74)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:216)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:159)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:145)
        at org.elasticsearch.cluster.routing.RoutingService$1.execute(RoutingService.java:144)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:308)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
```

It seems that the requests are being done sequentially, so maybe its possible to speed this up by running some requests in parallel / caching more information.
</comment><comment author="bleskes" created="2014-09-29T11:54:33Z" id="57149991">@miccon in those 15m, does the master node report the correct number of nodes? I wonder if it is the time it takes for the 500 nodes to join the master. We improved the latter considerably in 1.4 (yet to be released): https://github.com/elasticsearch/elasticsearch/pull/7493 (see batch joining bullet point)
</comment><comment author="miccon" created="2014-09-30T12:28:26Z" id="57305651">@bleskes I agree that the batch joining of the nodes will indeed help when the cluster starts, so it should help with issue #5232. In order to get the nodes to join quickly in 1.0 we have to set discovery.zen.publish_timeout to 0 as described in the other issue.

Here, these 15m are after the nodes have joined, so yes the master reports the correct number of nodes. It might be related to how the master queries the nodes for which shards are available / when it calculates the allocation.
</comment><comment author="bluelu" created="2014-09-30T12:34:31Z" id="57306320">@bleskes I added more logs here at the end regarding to the slow recoery issue (miccon and I work together):
https://github.com/elasticsearch/elasticsearch/issues/6295
</comment><comment author="bleskes" created="2014-09-30T19:06:10Z" id="57365373">&gt; The master node seems to only request shard recovery every 11 minutes (but not for all shards), causing the long wait.

Before recovering indices from disk, the master asks all the nodes about what they have on disk. To do so the nodes need some information that's part of the cluster state and if the don't have it they respond with "I don't know yet". The problem is that you have set `discovery.zen.publish_timeout: 0`, which means the master doesn't wait on the nodes and continues processing join events. It does not speed the processing on the node side. I suspect that's way recovering shards from disk takes long.

That said, do you know what happens in the 11 minutes? how big are the shards? 

Last, with the cluster of your size, I would really recommend you upgrade as soon as you can . We have had so many optimizations that will help you (.batched joins , memory signature ... and many more)
</comment><comment author="shikhar" created="2014-10-09T07:33:18Z" id="58473088">I have noticed cluster initialization 'hung' on the same cluster update task, like @miccon :

```
"elasticsearch[blabla-es0][clusterService#updateTask][T#1]" #79 daemon prio=5 os_prio=0 tid=0x00007fd16988d000 nid=0x6e01 waiting on condition [0x00007fd0bc279000]
   java.lang.Thread.State: WAITING (parking)
    at sun.misc.Unsafe.park(Native Method)
    - parking to wait for  &lt;0x0000000614a22508&gt; (a org.elasticsearch.common.util.concurrent.BaseFuture$Sync)
    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
    at org.elasticsearch.gateway.local.LocalGatewayAllocator.buildShardStores(LocalGatewayAllocator.java:443)
    at org.elasticsearch.gateway.local.LocalGatewayAllocator.allocateUnassigned(LocalGatewayAllocator.java:281)
    at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:74)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)
    at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)
    at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:278)
    at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:328)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

Notes:
- This is on a cluster using [eskka discovery](https://github.com/shikhar/eskka) which might be reason to discount this report. However AFAICT I don't think it is doing anything wrong. eskka does publishing asynchronously, same as the effect of `discovery.zen.publish_timeout: 0` with Zen -- so that seems to be a common feature here
- The tdump is from the master node, and it was aware of all cluster nodes at this point and had published a few cluster state versions prior to this happening 
- ES version 1.3.4, `Nodes: 20   Indices: 13   Shards: 1122   Data: 748.24 GB`
- This has happened a few times during bounce and gets resolved by bouncing the cluster again. Seems like some sort of race condition. Normally going from start to green only takes a couple of minutes.
</comment><comment author="bleskes" created="2014-10-09T08:23:18Z" id="58477444">@shikhar when the master assigns replicas, it first asks all the nodes what files they have for that shard on disk. The idea is to assign it to the nodes that has most files already available. The stack trace you're seeing is master waiting on a node to answer this question. I can see optimizations we can do here, but but this requests should be relatively quick. 
- How long does the master wait? 
- do you see high cpu on it? 
- If you stack trace it a couple of times, does the lock pointer change? In your example it's - 0x0000000614a22508 . This will indicate the master is issuing multiple requests as opposed to hang on one.
</comment><comment author="shikhar" created="2014-10-09T09:22:32Z" id="58483589">For now I can only answer

&gt; - How long does the master wait?

As opposed to the normal cluster init time of a couple of minutes, it seems to be taking over 10-15 mins by which time alerts fire so we re-bounce it

As for

&gt; - do you see high cpu on it?
&gt; - If you stack trace it a couple of times, does the lock pointer change? In your example it's - 0x0000000614a22508 . This will indicate the master is issuing multiple requests as opposed to hang on one.

I will be sure to check these out next time. Thanks @bleskes!
</comment><comment author="bluelu" created="2014-10-24T09:10:19Z" id="60362740">Hi,
Just a short update on our tests:
We created a 200 node empty cluster on 1.3.4, and the joining process took more than 45 minutes.
It worked fine (only 1-3 minutes) in 1.4.0 beta, so we will wait for the 1.4.0 version until we upgrade.

After 1.4.0 is release, we will then also run some tests with production indexes, to see if the long shard initialisation phase has also improved.
</comment><comment author="bleskes" created="2014-10-24T09:32:10Z" id="60364787">@bluelu great news. Indeed 1.4 massively improved the time it takes to form large clusters by batching join requests. 

W.r.t shard initialization time, let's see how it goes. We still need to reach out to all the nodes and ask them for information about what shards they have on disk before primaries can be allocated and cluster becomes yellow.
</comment><comment author="bluelu" created="2014-10-24T10:08:54Z" id="60368365">Just an idea there (Don't know how it's done at the moment, but I guess it's like that at the moment also in 1.4.0):
I suppose that asking all nodes for their shards, also means that the shards need to do an integrity check (e.g. the new checksum checks, etc...) on their shards before they report back to the master node, which in our case could take some time on some servers (e.g. non ssd disks) with multiple shards, as it involves reading a lot of TBs of data per node.
If this process could be splitted in two, first list available shards (no integrety check, just check what is on the filesystem, should be very fast) and return, and start the current allocation phase. But during that phase, you could already allocate the primaries as soon as all nodes have finished the initialisation phases which contain a specific shard, as you know in advance already that no other nodes contain that shard.
</comment><comment author="shikhar" created="2014-12-03T12:56:30Z" id="65402668">@bleskes 

&gt; do you see high cpu on it?

master cpu is pretty low

&gt; If you stack trace it a couple of times, does the lock pointer change? In your example it's - 0x0000000614a22508 . This will indicate the master is issuing multiple requests as opposed to hang on one.

it does change

I mainly wanted to update that in our case the problem might possibly be due to using JDK8u5. I was able to capture some more diagnostics when bouncing the nodes following one such event of super-slow cluster init. We have some automated thread-dumping if a node takes too long to go down nicely and kill-9 needs to be used. The thread dump on this (non-master) node revealed a bunch of threads executing code relevant to the RPC's issued by the master:

```
"elasticsearch[blabla][generic][T#43]" #160 daemon prio=5 os_prio=0 tid=0x00007f79180c5800 nid=0x3d1f in Object.wait() [0x00007f79d9289000]
   java.lang.Thread.State: RUNNABLE
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:359)
    at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:912)
    at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:758)
    at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)
    at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)
    at org.elasticsearch.index.store.Store.readSegmentsInfo(Store.java:126)
    at org.elasticsearch.index.store.Store.access$300(Store.java:76)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.buildMetadata(Store.java:465)
    at org.elasticsearch.index.store.Store$MetadataSnapshot.&lt;init&gt;(Store.java:456)
    at org.elasticsearch.index.store.Store.readMetadataSnapshot(Store.java:281)
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.listStoreMetaData(TransportNodesListShardStoreMetaData.java:186)
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:140)
    at org.elasticsearch.indices.store.TransportNodesListShardStoreMetaData.nodeOperation(TransportNodesListShardStoreMetaData.java:61)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:277)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:268)
    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
```

The really weird thing is that these threads are reported to be RUNNABLE although they are supposedly in `Object.wait()`, and I can't even spot any relevant synchronization in `SegmentInfos.read()`.

Anyway we have seen this weirdness a couple of times in some non-ES usage as well. We plan to upgrade our JDK8 version and hopefully this occasional issue will stop happening altogether.

**UPDATE May 14, 15** https://issues.apache.org/jira/browse/LUCENE-6482 - unrelated to JDK version
</comment><comment author="bluelu" created="2014-12-06T10:21:03Z" id="65892182">@bleskes 
We upgraded now to 1.4.1:

Cluster join has increased much, that's great.

Still the allocation will take a lot of time (it's now more than 30 minutes since cluster start and it hasn't much progressed, see below status it's still hanging at   "initializing_shards" : 8390). 
The master node runs one Thread at 100% cpu. It's not stuck as the stack progresses.

elasticsearch[I51N16][clusterService#updateTask][T#1]" #52 daemon
prio=5 os_prio=0 tid=0x00007f1ad25d9000 nid=0x127f runnable
[0x00007f1a22171000]
   java.lang.Thread.State: RUNNABLE
        at org.elasticsearch.common.collect.UnmodifiableListIterator.&lt;init&gt;(UnmodifiableListIterator.java:34)
        at org.elasticsearch.common.collect.AbstractIndexedListIterator.&lt;init&gt;(AbstractIndexedListIterator.java:68)
        at org.elasticsearch.common.collect.Iterators$11.&lt;init&gt;(Iterators.java:1058)
        at org.elasticsearch.common.collect.Iterators.forArray(Iterators.java:1058)
        at org.elasticsearch.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106)
        at org.elasticsearch.common.collect.ImmutableList.listIterator(ImmutableList.java:344)
        at org.elasticsearch.common.collect.ImmutableList.iterator(ImmutableList.java:340)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:173)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:46)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:552)
        at org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)
        at org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)
        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)
        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate(DiskThresholdDecider.java:288)
        at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.tryRelocateShard(BalancedShardsAllocator.java:799)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.balance(BalancedShardsAllocator.java:426)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.rebalance(BalancedShardsAllocator.java:124)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned(BalancedShardsAllocator.java:118)
        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:75)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)
        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:281)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)

Cluster health:

{
  "cluster_name" : "cluster",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 651,
  "number_of_data_nodes" : 650,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 8390,
  "unassigned_shards" : 16108
}
</comment><comment author="bleskes" created="2014-12-06T10:28:28Z" id="65892353">@thibaut Thx for the update. I'll read more carefully later but can you run&#160;http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-nodes-hot-threads.html&#160;on the master a couple of times? I wonder what it does.

On Sat, Dec 6, 2014 at 11:21 AM, Thibaut notifications@github.com wrote:

&gt; @bleskes 
&gt; We upgraded now to 1.4.1:
&gt; Cluster join has increased much, that's great.
&gt; Still the allocation will take a lot of time (it's now more than 30 minutes since cluster start and it hasn't much progressed, see below status it's still hanging at   "initializing_shards" : 8390). 
&gt; The master node runs one Thread at 100% cpu. It's not stuck as the stack progresses.
&gt; elasticsearch[I51N16][clusterService#updateTask][T#1]" #52 daemon
&gt; prio=5 os_prio=0 tid=0x00007f1ad25d9000 nid=0x127f runnable
&gt; [0x00007f1a22171000]
&gt;    java.lang.Thread.State: RUNNABLE
&gt;         at org.elasticsearch.common.collect.UnmodifiableListIterator.&lt;init&gt;(UnmodifiableListIterator.java:34)
&gt;         at org.elasticsearch.common.collect.AbstractIndexedListIterator.&lt;init&gt;(AbstractIndexedListIterator.java:68)
&gt;         at org.elasticsearch.common.collect.Iterators$11.&lt;init&gt;(Iterators.java:1058)
&gt;         at org.elasticsearch.common.collect.Iterators.forArray(Iterators.java:1058)
&gt;         at org.elasticsearch.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106)
&gt;         at org.elasticsearch.common.collect.ImmutableList.listIterator(ImmutableList.java:344)
&gt;         at org.elasticsearch.common.collect.ImmutableList.iterator(ImmutableList.java:340)
&gt;         at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:173)
&gt;         at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:46)
&gt;         at org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:552)
&gt;         at org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)
&gt;         at org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)
&gt;         at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)
&gt;         at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate(DiskThresholdDecider.java:288)
&gt;         at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)
&gt;         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.tryRelocateShard(BalancedShardsAllocator.java:799)
&gt;         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.balance(BalancedShardsAllocator.java:426)
&gt;         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.rebalance(BalancedShardsAllocator.java:124)
&gt;         at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned(BalancedShardsAllocator.java:118)
&gt;         at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:75)
&gt;         at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)
&gt;         at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)
&gt;         at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:281)
&gt;         at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
&gt;         at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
&gt;         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
&gt;         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
&gt;         at java.lang.Thread.run(Thread.java:745)
&gt; Cluster health:
&gt; {
&gt;   "cluster_name" : "cluster",
&gt;   "status" : "red",
&gt;   "timed_out" : false,
&gt;   "number_of_nodes" : 651,
&gt;   "number_of_data_nodes" : 650,
&gt;   "active_primary_shards" : 0,
&gt;   "active_shards" : 0,
&gt;   "relocating_shards" : 0,
&gt;   "initializing_shards" : 8390,
&gt;   "unassigned_shards" : 16108
&gt; 
&gt; ## }
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/6372#issuecomment-65892182
</comment><comment author="bluelu" created="2014-12-06T10:42:35Z" id="65892658">@bleskes 
I uploaded 4 hothreads outputs of the master node to pastebin:
http://pastebin.com/H0mQcAFE

In the meantime, the allocation advanced a little bit:

{
  "cluster_name" : "cluster",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 651,
  "number_of_data_nodes" : 650,
  "active_primary_shards" : 3468,
  "active_shards" : 3468,
  "relocating_shards" : 1,
  "initializing_shards" : 7402,
  "unassigned_shards" : 13628
}

What we normally do (or did before) was that we only added a few nodes first (main indexes, 200-300 nodes), waited for it to come up, and then added the other nodes 1 by 1 (since we could afford to have some indexes red in the beginning)  afterwards, while having the allocating/balancing disabled. 
</comment><comment author="bleskes" created="2014-12-06T11:17:23Z" id="65893470">I see. The disk threshold allocator decider, in charge of making sure a node is not overloaded with shards is calculating the size of relocating shards walking the shards list again and again. We'd have to make it more efficient. A simple work around is to temporally set

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.disk.include_relocations" : false
    }
}'
```

and enable it while start up is done. Can you try?
</comment><comment author="bluelu" created="2014-12-06T12:06:00Z" id="65894654">@bleskes 
Thanks for this suggestion :-)

We couldn't apply this change while it was running (timeouts setting the value when we tried multiple times), so we had to restart. I can confirm that the allocation works now at excellent speed (just a few minutes) :-). We will not reenable this feature for now and rather limit the allocation per node to 1 at most at the same time.

Only remaining issue is that it still takes 10 minutes until the allocation starts. But for us this is acceptable...

Are you going to open up an issue for the include_relocations performance issue?
</comment><comment author="bluelu" created="2014-12-06T16:09:11Z" id="65903459">Just a further update on the blocking issue:

We enabled allocation of non primaries now, which caused the master node to block for another 20 minutes at the stacktrace below (local gateway recovery), not updating cluster health at all, nor sending any cluster updates. During that time no commands can be executed on the cluster (everything will timeout)

The more and more shards are allocated, the more and more the "blocking" time gets reduced and updates progress.
prio=5 os_prio=0 tid=0x00007f1ad25d9000 nid=0x127f runnable
[0x00007f1a22171000]
   java.lang.Thread.State: RUNNABLE
        at org.elasticsearch.common.collect.UnmodifiableListIterator.&lt;init&gt;(UnmodifiableListIterator.java:34)
        at org.elasticsearch.common.collect.AbstractIndexedListIterator.&lt;init&gt;(AbstractIndexedListIterator.java:68)
        at org.elasticsearch.common.collect.Iterators$11.&lt;init&gt;(Iterators.java:1058)
        at org.elasticsearch.common.collect.Iterators.forArray(Iterators.java:1058)
        at org.elasticsearch.common.collect.RegularImmutableList.listIterator(RegularImmutableList.java:106)
        at org.elasticsearch.common.collect.ImmutableList.listIterator(ImmutableList.java:344)
        at org.elasticsearch.common.collect.ImmutableList.iterator(ImmutableList.java:340)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:173)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.iterator(IndexShardRoutingTable.java:46)
        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.shardsWithState(IndexShardRoutingTable.java:552)
        at org.elasticsearch.cluster.routing.IndexRoutingTable.shardsWithState(IndexRoutingTable.java:268)
        at org.elasticsearch.cluster.routing.RoutingTable.shardsWithState(RoutingTable.java:114)
        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.sizeOfRelocatingShards(DiskThresholdDecider.java:225)
        at org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider.canAllocate(DiskThresholdDecider.java:288)
        at org.elasticsearch.cluster.routing.allocation.decider.AllocationDeciders.canAllocate(AllocationDeciders.java:74)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.tryRelocateShard(BalancedShardsAllocator.java:799)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator$Balancer.balance(BalancedShardsAllocator.java:426)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.rebalance(BalancedShardsAllocator.java:124)
        at org.elasticsearch.cluster.routing.allocation.allocator.BalancedShardsAllocator.allocateUnassigned(BalancedShardsAllocator.java:118)
        at org.elasticsearch.cluster.routing.allocation.allocator.ShardsAllocators.allocateUnassigned(ShardsAllocators.java:75)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.reroute(AllocationService.java:217)
        at org.elasticsearch.cluster.routing.allocation.AllocationService.applyStartedShards(AllocationService.java:86)
        at org.elasticsearch.cluster.action.shard.ShardStateAction$4.execute(ShardStateAction.java:281)
        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:329)
        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
</comment><comment author="bleskes" created="2014-12-06T16:22:22Z" id="65903946">@bluelu it's the same stack trace as before. Are you sure you had that settings I mentioned disabled? It is valid for any allocation, both primary and replicas
</comment><comment author="bleskes" created="2014-12-06T16:38:43Z" id="65904550">Btw, even if the settings update request times out , the master will get to it once it finishes the current task
</comment><comment author="bluelu" created="2014-12-06T16:58:03Z" id="65905197">Yes, we have  "cluster.routing.allocation.disk.include_relocations" : false:

{
  "persistent" : {
    "action" : {
      "destructive_requires_name" : "true"
    },
    "cluster" : {
      "routing" : {
        "allocation" : {
          "node_concurrent_recoveries" : "1",
          "disk" : {
            "threshold_enabled" : "true",
            "watermark" : {
              "low" : "0.9",
              "high" : "0.95"
            },
            "include_relocations" : "false",
            "reroute_interval" : "60000s"
          },
          "enable" : "all"
        }
      }
    },
    "indices" : {
      "recovery" : {
        "concurrent_streams" : "3"
      }
    },
    "threadpool" : {
      "search" : {
        "type" : "fixed",
        "size" : "12"
      }
    },
    "logger" : {
      "cluster" : {
        "service" : "DEBUG"
      }
    }
  },
  "transient" : { }
}
</comment><comment author="bluelu" created="2014-12-07T10:03:26Z" id="65932317">@bleskes 
It seems that both the reallocate and rebalance functions are just too slow if you have too many nodes and shards. We also use the Shard allocation filtering.

What do you need in order to be able to debug this and reproduce the performance issue? Cluster state, settings and configuration? I can send you this in private?
</comment><comment author="bleskes" created="2014-12-07T12:00:03Z" id="65935081">@Thibaut I can't look at things in details right now. It is surprising that disabling include relocation didn't kick in. Being able to reroute quickly is important for the operations of the master. As a temporary work around (as I see from the tickets that it causes other problems as well) try disabling the disk threshold allocator all together.&#160;

Ps - Simon alread fixed that slowness we saw in your hot threads: #8803

On Sun, Dec 7, 2014 at 11:03 AM, Thibaut notifications@github.com wrote:

&gt; @bleskes 
&gt; It seems that both the reallocate and rebalance functions are just too slow if you have too many nodes and shards. We also use the Shard allocation filtering.
&gt; 
&gt; ## What do you need in order to be able to debug this and reproduce the performance issue? Cluster state, settings and configuration? I can send you this in private?
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/issues/6372#issuecomment-65932317
</comment><comment author="bleskes" created="2014-12-08T08:20:42Z" id="66036431">@bluelu it seems that the include_relocations settings were backported to 1.4, but the dynamic update code didn't make it (although it is registered for dynamic updates, see https://github.com/elasticsearch/elasticsearch/commit/4e5264c8dcab392fd94554f5d036573b085b6450 ). Which misses https://github.com/elasticsearch/elasticsearch/commit/4185566e9344091a3ddad4090435fc3609fab208#diff-1b8dca987fbcfb8d8e452d7e29c4d058R92 ). I'm sorry for sending you down a wrong path. @dakrone  I think it makes sense to add the dynamic update logic for 1.4.2, right?
</comment><comment author="dakrone" created="2014-12-08T09:20:31Z" id="66041815">@bleskes yes, most likely caused by a bad backport, I'll fix in the 1.4 branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent usage pattern of SearchSourceBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6371</link><project id="" key="" /><description>The usage pattern for constructing some of the subsections of a search request using the SearchSourceBuilder is different, e.g. query, postFilter, facets, aggregations can be build by passing in the binary content or a Map. For highlighting and suggestions there are only methods to retrieve the current builder and use it to customize the query.

The clojure client Elastisch relies on the methods that accept Maps. That is why there doesn't seem to be support for highlighting and suggestions using the native client in Elastisch, only for the REST API.

Besides the problem with the clojure client I also think it would be nice to have a consistent interface.

Related issue in Elastisch:
https://github.com/clojurewerkz/elastisch/issues/82
</description><key id="34750877">6371</key><summary>Inconsistent usage pattern of SearchSourceBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fhopf</reporter><labels><label>:Java API</label></labels><created>2014-06-02T07:28:26Z</created><updated>2015-10-15T17:06:40Z</updated><resolved>2015-10-15T17:06:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaelklishin" created="2014-06-02T07:44:12Z" id="44809645">In general, method overloads that accept `source` as a map make it much more convenient to use ES Java client from Clojure.
</comment><comment author="javanna" created="2015-10-15T17:06:35Z" id="148459004">With #13859 we made a big step towards parsing the search request on the coordinating node. Effectively if you use the java api you can't specify a string or bytes array or map anymore when building a search request, as you need a structured java object instead, which is now natively streamable for communication between the nodes. This is why we removed all of the set methods that accepted string, bytes array, maps etc. from `SearchSourceBuilder` and `SearchRequest`. Those ones could only work knowing that parsing is delayed to each data node, which is not the case anymore in the master branch (future 3.0 release). That said I am closing this as won't fix, as the set methods that accept maps have all been removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixing small typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6370</link><project id="" key="" /><description /><key id="34749594">6370</key><summary>fixing small typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">suchkultur</reporter><labels><label>docs</label></labels><created>2014-06-02T06:57:19Z</created><updated>2014-08-01T10:41:03Z</updated><resolved>2014-08-01T10:41:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T10:02:44Z" id="44945589">Hi @suchkultur thanks for the PR, may I ask you to sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so that we can merge it in?
</comment><comment author="javanna" created="2014-08-01T10:41:03Z" id="50871232">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding Kopf plugin to plugins list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6369</link><project id="" key="" /><description /><key id="34739849">6369</key><summary>Adding Kopf plugin to plugins list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">geekpete</reporter><labels /><created>2014-06-01T23:58:15Z</created><updated>2014-08-08T06:39:33Z</updated><resolved>2014-08-07T18:54:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-02T06:49:27Z" id="44806358">Isnt it here already? 

https://github.com/elasticsearch/elasticsearch/blob/master/docs/community/monitoring.asciidoc
https://github.com/elasticsearch/elasticsearch/blob/1.x/docs/community/monitoring.asciidoc
</comment><comment author="geekpete" created="2014-06-03T01:21:55Z" id="44910375">Just thought it should have equal footing on the page with the other plugins. This page is ahead of the one where kopf is already listed, so people are probably going to see this page first. I use this plugin daily, quite useful.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Snapshot/Restore: Add ability to restore partial snapshots</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6368</link><project id="" key="" /><description>Closes #5742
</description><key id="34738096">6368</key><summary>Snapshot/Restore: Add ability to restore partial snapshots</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-06-01T22:12:29Z</created><updated>2015-06-07T14:32:52Z</updated><resolved>2014-07-01T01:02:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T13:34:26Z" id="45891501">added some comments... seems close
</comment><comment author="imotov" created="2014-06-12T19:38:31Z" id="45938171">@s1monw - makes sense. Thanks! Fixed.
</comment><comment author="s1monw" created="2014-06-27T10:54:01Z" id="47330953">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Queries, filters and match_all</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6367</link><project id="" key="" /><description>Hello. Help me please, I'm confused. As far as I remember, there was the only way to pass filters to search query - via filtered query. But currently there is a top-level `filter` part of the query. However, top-level `filter` affects `query` only and doesn't affect i.e. `facets`. But filtered query filter affects both of the `query` and `facets` facilities. Also, I remember there was a time I need to add `match_all` query to filtered `query` section if query was empty and filters only was present. Otherwise returned empty set of documents. Since I'm trying to create high-level Ruby library could you please answer following questions:

1) Which way is preferred now and in future: filtered top-level query or top-level filter with top-level query?
2) How do you plan to resolve such an API inconsistency when filtered query filter affects outside statements and top-level filter doesn't affect some parts of request?
3) Why do I remember about `match_all` feature and when did requests started to return  all the documents with empty query section in filtered query? I'm checking it right now on 1.2.0 and I don't need to use `match_all`, or `constant_score` it just returns all the docs for me.

Thanks in advance.
</description><key id="34730716">6367</key><summary>Queries, filters and match_all</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pyromaniac</reporter><labels /><created>2014-06-01T15:43:42Z</created><updated>2014-06-03T12:30:15Z</updated><resolved>2014-06-03T12:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T12:19:39Z" id="44957023">Hi @pyromaniac I think this type of questions would get the proper attention on our [mailing list](https://groups.google.com/forum/#!forum/elasticsearch). May I ask you to post it there please?

As for api consistency, we renamed the top-level filter to `post-filter` in `1.0` to clarify what it does. It was called `filter` before. In most of the cases users need a filtered query though, whose filter applies to both search results and aggregations.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Docs: Special attributes for allocation filtering</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6366</link><project id="" key="" /><description /><key id="34726702">6366</key><summary>Docs: Special attributes for allocation filtering</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">itsadok</reporter><labels><label>docs</label></labels><created>2014-06-01T11:29:02Z</created><updated>2014-06-22T00:43:04Z</updated><resolved>2014-06-05T08:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-06-01T12:50:42Z" id="44777008">Lgtm
</comment><comment author="javanna" created="2014-06-03T12:20:56Z" id="44957116">Hi @itsadok could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this in?
</comment><comment author="itsadok" created="2014-06-05T06:25:41Z" id="45184926">Done
</comment><comment author="javanna" created="2014-06-05T08:38:36Z" id="45194527">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add option to disable version logic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6365</link><project id="" key="" /><description>For append only use cases (e.g. one log entry per document) we spend unnecessary CPU/IO adding auto-ids, adding versions to the index and to the versionMap, etc.  Since these docs are typically tiny, that added cost could be a non-trivial cost.

Maybe we could even make adding _uid/_id optional?  Or are there too many places that expect every document to have an id?  Seems silly to add it when the app will never use it.
</description><key id="34715460">6365</key><summary>Add option to disable version logic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-05-31T21:22:52Z</created><updated>2014-09-10T19:13:43Z</updated><resolved>2014-07-11T14:37:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-31T22:36:39Z" id="44762117">you should sync with @s1monw on that, he had a branch for an append only engine that (AFAIR) skipped versioning
</comment><comment author="mikemccand" created="2014-05-31T23:49:49Z" id="44763566">Thanks Uri, will do.
</comment><comment author="kimchy" created="2014-06-01T00:04:52Z" id="44763837">note, we rely on versioning also during replication, where during a shard movement, we play the existing actions also on the relocated shards. Thats why for example we can only do the auto generate id optimization when relocation doesn't happen.

Ids are used in many places in ES, and are a core concept (get by id, search returning ids, ...), I don't think we should have a feature to remove it, we should further optimize when they exists though.
</comment><comment author="mikemccand" created="2014-07-11T14:37:22Z" id="48738087">I'm closing this; it sounds too dangerous/tricky to allow turning off these low level ES capabilities...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Switch to ConcurrentHashMapV8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6364</link><project id="" key="" /><description>ConcurrentHashMapV8 from jsr166e has much better memory footprint and GC behavior.  We create large ConcurrentHashMaps with versionMap, so this should help.
</description><key id="34715411">6364</key><summary>Internal: Switch to ConcurrentHashMapV8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label></labels><created>2014-05-31T21:19:49Z</created><updated>2014-07-16T13:20:07Z</updated><resolved>2014-06-05T14:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ijuma" created="2014-05-31T22:58:31Z" id="44762623">Maybe this is obvious, but Java 8 has the improved version.
</comment><comment author="mikemccand" created="2014-05-31T23:48:42Z" id="44763539">We don't require Java 8 to run Elasticsearch ... so we pull in some of the Java 8 concurrent classes "ahead of time" under src/main/java/jsr166e/*.
</comment><comment author="nik9000" created="2014-06-01T00:55:27Z" id="44764569">I know it'd be more work to evaluate, but what about the `ConcurrentMap`s you can get from Guava's `MapMaker`?
</comment><comment author="kimchy" created="2014-06-01T01:15:46Z" id="44764863">@nik9000 Guava MapMaker concurrent map when customized, i.e. by having listeners, computing, or soft/weak values works very similar to how CHM works in Java 7, Java 8 CHM is very different in how it works. Non customized maps simply result in returning CHM.
</comment><comment author="nik9000" created="2014-06-01T10:24:58Z" id="44774097">Thanks for setting me strait. Suggestion withdrawn. 

Sent from my iPhone

&gt; On May 31, 2014, at 9:15 PM, Shay Banon notifications@github.com wrote:
&gt; 
&gt; @nik9000 Guava MapMaker concurrent map when customized, i.e. by having listeners, computing, or soft/weak values works very similar to how CHM works in Java 7, Java 8 CHM is very different in how it works. Non customized maps simply result in returning CHM.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing: Clear versionMap on refresh not flush</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6363</link><project id="" key="" /><description>Today we clear only on flush (Lucene commit) but this is dangerous because the in-memory versionMap can grow very large e.g. if the translog flush size/ops was increased from the defaults.
</description><key id="34715367">6363</key><summary>Indexing: Clear versionMap on refresh not flush</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-31T21:17:20Z</created><updated>2014-07-16T12:26:49Z</updated><resolved>2014-06-04T09:42:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-06-04T09:42:56Z" id="45071044">Fixed with #6379
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>removed slowest on single query benchmark requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6362</link><project id="" key="" /><description>according to https://github.com/elasticsearch/elasticsearch/issues/5904 this shouldn't be present anymore.
</description><key id="34714398">6362</key><summary>removed slowest on single query benchmark requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>docs</label></labels><created>2014-05-31T20:24:56Z</created><updated>2014-07-05T17:06:10Z</updated><resolved>2014-06-03T09:47:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T09:47:55Z" id="44944289">Good point, merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update request-body.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6361</link><project id="" key="" /><description>There is no unit defined for timeout parameter.
</description><key id="34708557">6361</key><summary>Update request-body.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">csonuryilmaz</reporter><labels><label>docs</label></labels><created>2014-05-31T15:26:20Z</created><updated>2014-07-28T09:09:22Z</updated><resolved>2014-07-28T09:09:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-05T08:29:16Z" id="45193736">Hi @csonuryilmaz thanks for the fix, could you please sign our CLA so that we can go ahead and merge it in? http://www.elasticsearch.org/contributor-agreement/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: Make "groups" and "types" accept wildcards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6360</link><project id="" key="" /><description>The "fields" parameter to indices stats accepts wildcards, but
the "groups" and "types" parameters don't.
</description><key id="34707291">6360</key><summary>Stats: Make "groups" and "types" accept wildcards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-05-31T14:23:14Z</created><updated>2014-07-02T13:44:31Z</updated><resolved>2014-06-03T12:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-05-31T16:39:38Z" id="44752947">LGTM, but needs a test.
</comment><comment author="clintongormley" created="2014-06-03T12:32:18Z" id="44958179">Closed in favor of #6390
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stats: Indices stats ignoring "groups" and "types"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6359</link><project id="" key="" /><description>The "groups" and "types" parameters to indices stats were being set too
early, so could end up being cleared.
</description><key id="34707236">6359</key><summary>Stats: Indices stats ignoring "groups" and "types"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2014-05-31T14:20:03Z</created><updated>2014-06-21T01:40:45Z</updated><resolved>2014-06-03T12:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-05-31T16:34:26Z" id="44752781">LGTM. This also means a hole in our test coverage. Care to add a test?
</comment><comment author="clintongormley" created="2014-06-03T12:32:24Z" id="44958185">Closed in favor of #6390
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update script - unable to resolve method: java.util.LinkedHashMap.distance(java.lang.Double, java.lang.Double)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6358</link><project id="" key="" /><description>when having a document with a property like 

```
"last_location" : {
          "type" : "geo_point"
        }
```

i am not able to use the `.distance(lat, lon)` function in a update (Java API)

```
"ctx._source.duration = ctx._source.last_location.distance(41.12, -71.34); "
```

the exception mentions the cause:

```
org.elasticsearch.common.mvel2.PropertyAccessException: [Error: unable to resolve method: java.util.LinkedHashMap.distance(java.lang.Double, java.lang.Double) [arglength=2]]
```
</description><key id="34657147">6358</key><summary>Update script - unable to resolve method: java.util.LinkedHashMap.distance(java.lang.Double, java.lang.Double)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">albertocsm</reporter><labels><label>v0.90.13</label></labels><created>2014-05-30T15:50:25Z</created><updated>2014-05-30T18:59:56Z</updated><resolved>2014-05-30T18:30:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="albertocsm" created="2014-05-30T17:07:38Z" id="44674787">btw, im using 0.90.13. does it make any difference  upgrading to the latest?
</comment><comment author="drewr" created="2014-05-30T18:30:58Z" id="44683993">It's not possible during an `_update` to use `distance()` on a `geo_point`.  You'd have to pull the values out of the resulting ArrayList and call `GeoDistance.PLANE.calculate`.  Tested with `master`:

```
curl -s -XPOST localhost:9200/twitter/status/472425208746221568/_update -d'---
script: "
import org.elasticsearch.common.geo.GeoDistance;   
import org.elasticsearch.common.unit.DistanceUnit; 
coord = ctx._source.coordinates.coordinates;
lon = coord[0];
lat = coord[1];
distance = GeoDistance.PLANE.calculate(41.12, -71.34, lat, lon, DistanceUnit.MILES);
ctx._source.duration = distance;
"
'
```
</comment><comment author="albertocsm" created="2014-05-30T18:59:56Z" id="44687182">that worked just fine! 
ty
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Random shards do not recover if one index has faulty synonym filter defined</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6357</link><project id="" key="" /><description>I use a synonym filter with the synonyms defined in a text file. 1 node, several indexes with several shards, only one of them uses the synonym filter. When I remove the textfile that contains the synonyms many indexes do not recover when I restart the node. This is random, affected shards and indexes vary.

To reproduce:
1. start node and create some indexes with documents
2. place synonym file in correct folder, create index that uses synonym filter with synonyms from file and index documents (see example below for details)
3. stop node
4. remove the synonym file
5. start node again

I would have expected only the affected index shards not to recover but instead this is completely random.

Example index:

```
DELETE synonymindex

PUT synonymindex
{
   "settings": {
      "index": {
         "analysis": {
            "analyzer": {
               "synonym": {
                  "tokenizer": "whitespace",
                  "filter": [
                     "synonym"
                  ]
               }
            },
            "filter": {
               "synonym": {
                  "type": "synonym",
                  "synonyms_path": "analysis/synonym.txt"
               }
            }
         }
      }
   },
   "mappings": {
      "sometype": {
         "properties": {
            "text": {
               "type": "string",
               "analyzer": "synonym"
            }
         }
      }
   }
}

POST synonymindex/sometype
{
    "text": "foo"
}
```

the file analysis/synonym.txt contains only one line:

```
foo, bar
```
</description><key id="34656506">6357</key><summary>Random shards do not recover if one index has faulty synonym filter defined</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>bug</label></labels><created>2014-05-30T15:43:06Z</created><updated>2015-05-22T13:38:24Z</updated><resolved>2015-05-22T13:15:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2015-05-22T13:15:18Z" id="104656881">cannot reproduce anymore
</comment><comment author="kimchy" created="2015-05-22T13:38:24Z" id="104662554">@brwe to close the loop, this fixed it: https://github.com/elastic/elasticsearch/pull/10283
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failing unit test</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6356</link><project id="" key="" /><description>1&gt; [2014-05-30 14:49:45,472][ERROR][test                     ] FAILURE  : testPagination(org.elasticsearch.search.aggregations.bucket.TopHitsTests)
  1&gt; REPRODUCE WITH  : mvn test -Dtests.seed=13F6C7DD97BC621A -Dtests.class=org.elasticsearch.search.aggregations.bucket.TopHitsTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="testPagination" -Des.logger.level=INFO -Des.node.local=true -Dtests.heap.size=512m -Dtests.processors=8
  1&gt; Throwable:
  1&gt; java.lang.AssertionError: 
  1&gt; Expected: "0"
  1&gt;      got: "9"
  1&gt; 
  1&gt;     __randomizedtesting.SeedInfo.seed([13F6C7DD97BC621A:7BD60EF0EB7ECB73]:0)
  1&gt;     [...org.junit._]
  1&gt;     org.elasticsearch.search.aggregations.bucket.TopHitsTests.testPagination(TopHitsTests.java:212)
  1&gt;     [...sun._, com.carrotsearch.randomizedtesting._, java.lang.reflect._]
  1&gt;     org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  1&gt;     org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  1&gt;     [...com.carrotsearch.randomizedtesting._]
  1&gt;     java.lang.Thread.run(Thread.java:745)
  1&gt; 
  1&gt; [2014-05-30 14:49:45,490][INFO ][test                     ] Test testPagination(org.elasticsearch.search.aggregations.bucket.TopHitsTests) finished
</description><key id="34651803">6356</key><summary>Failing unit test</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels /><created>2014-05-30T14:49:44Z</created><updated>2014-05-30T14:56:56Z</updated><resolved>2014-05-30T14:51:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-30T14:51:10Z" id="44659499">This has been fixed via: https://github.com/elasticsearch/elasticsearch/commit/760cee7c24cc953889df1934b6b8e1042c2bc516
</comment><comment author="peschlowp" created="2014-05-30T14:56:56Z" id="44660184">Thanks, that was quick :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing: DocumentMissingException is uncaught if thrown during retry of update request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6355</link><project id="" key="" /><description>This issue is about the same behavior as described in #4663 but identifies a server bug as the root cause and suggests a possible fix. It replaces #4663 because the title of that issue is misleading as it points to client-side logic. Here is the scenario:

An update request always checks if the document exists. If the document does not exist, a DocumentMissingException is thrown ([UpdateHelper.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/update/UpdateHelper.java):91). Everything is fine if this happens on the first try of the request: The exception is caught ([TransportInstanceSingleOperationAction.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/support/single/instance/TransportInstanceSingleOperationAction.java):194) and an HTTP response is sent accordingly.

However, if the update request is retried because of a version conflict, and DocumentMissingException is thrown only then, the exception will not be caught because there is no exception handling in the Runnable executing the retry ([TransportUpdateAction.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java):246). The exception will just show up in the server log of the Elasticsearch node and no further action is taken. In particular, no HTTP response is sent to the client. In my view, a response should be sent to the client in this situation as well, indicating that the document is missing.

We discovered this behavior in integration tests that hung forever because the test client was blocking on a future waiting for the response that was not to happen. A possible scenario how this can happen (in fact, how it happened to us in our tests) is as follows:
- A delete request for document A is made.
- At roughly the same time, an update request for document A is made.
- The timing is such that both requests reach their respective methods in [InternalEngine.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/engine/internal/InternalEngine.java) concurrently, i.e., the update call reaches InternalEngine.innerIndex() and the delete call reaches InternalEngine.innerDelete().
- Let the delete call happen first. It will delete the document and update its version.
- After the delete call has finished, the update call proceeds. It then notices that a version conflict exists and retries.
- The retry will detect that the document is missing, throwing the exception that will not be caught.

[This gist](https://gist.github.com/peschlowp/df952b5900ff9d1279e3) is a self-contained java program that will provoke the behavior with some probability. The test program inserts 100 documents into an index, then starts two threads which concurrently try to update or delete these 100 documents. When the bug manifests, the exception will show in the server log and the updating thread will block forever. On my local machine, the bug manifests every time I run the program.

Alternatively, you can reproduce the behavior simply by performing a concurrent delete and update request for the same document and setting breakpoints in InternalEngine to force the timing as sketched above.

A possible fix for this scenario would be to wrap the call in [TransportUpdateAction.java](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/update/TransportUpdateAction.java):246 in a try-catch block like this. If you feel this solution is adequate, I have already tested the fix and can provide a pull request:

```
try {
    shardOperation(request, listener, retryCount + 1);
} catch (DocumentMissingException e){
    listener.onFailure(e);
}
```

On the other hand, if there are similar cases maybe a more generic solution is more appropriate?
</description><key id="34649622">6355</key><summary>Indexing: DocumentMissingException is uncaught if thrown during retry of update request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peschlowp</reporter><labels><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-30T14:24:18Z</created><updated>2014-07-16T12:27:27Z</updated><resolved>2014-07-15T12:57:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add low level Lucene tool to see which types/fields are using the most space in the index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6354</link><project id="" key="" /><description>At the Hackfest after Berlin Buzzwords we created a simple diagnostic
tool to see low-level index statistics (bytes used, total postings
ints) per-type and per-field.

It's just a standalone command-line (static main) tool for now, and it
takes some time to run (it visits all postings, docs only).  It
produces output like this:

&lt;pre&gt;
Reader maxDoc=537472 delDocs=0 (0%)

stored fields bytes by _type &amp; field:
  foo: 100.00%, 6665.5 MB
    _source
      98.62%, 6573.2 MB
    _uid
      1.38%, 92.3 MB

postings terms bytes:
  foo: 100.00%, 142.7 MB
    _all
      38.58%, 55.1 MB
    message
      38.31%, 54.7 MB
    _uid
      9.34%, 13.3 MB
    request
      7.31%, 10.4 MB
    ...

postings total ints:
  foo: 100.00%, 446.1 M
    _all
      norms,DOCS_AND_FREQS_AND_POSITIONS
      46.01%, 205.2 M
    message
      norms,DOCS_AND_FREQS_AND_POSITIONS
      38.55%, 172.0 M
    agent
      norms,DOCS_AND_FREQS_AND_POSITIONS
      4.07%, 18.1 M
    ...
&lt;/pre&gt;


It works by first visiting all _uid terms to make a mapping from docID
-&gt; _type, and then visits stored fields &amp; postings to gather up stats
by type and field.

It's just a start, but it should be useful when you want some
low-level details about which types are contributing to which field's
index bytes usage, how fields were indexed, etc.
</description><key id="34645230">6354</key><summary>Add low level Lucene tool to see which types/fields are using the most space in the index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-05-30T13:36:53Z</created><updated>2014-10-20T13:43:24Z</updated><resolved>2014-10-20T13:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-10-20T13:21:15Z" id="59752224">@mikemccand any plans for this one?
</comment><comment author="mikemccand" created="2014-10-20T13:43:24Z" id="59755447">Sorry I don't plan to commit this; we can leave it here as a [closed] PR and if somehow people find it helpful in the future we can revisit.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Object and Type parsing: Fix include_in_all in type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6353</link><project id="" key="" /><description>include_in_all can also be set on type level (root object).
This fixes a regression introduced  in #6093

closes #6304
</description><key id="34644429">6353</key><summary>Object and Type parsing: Fix include_in_all in type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Mapping</label><label>regression</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-30T13:26:02Z</created><updated>2015-06-07T19:58:03Z</updated><resolved>2014-06-02T15:57:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-02T09:21:54Z" id="44817038">the fix looks good but I wonder if you want add a test for the other cases we can set this?
</comment><comment author="brwe" created="2014-06-02T12:13:09Z" id="44829748">SimpleAllMapperTests already has some tests that cover that, for example testMultiField_includeInAllSetToFalse(). Do you still think we need another one?
</comment><comment author="s1monw" created="2014-06-02T15:39:31Z" id="44853190">no in that case I think I am good.... LGTM
</comment><comment author="brwe" created="2014-06-02T15:57:33Z" id="44855693">ok, pushed to master (125e0c16cddb7), 1.2 (34d62003d5f) and 1.x (dafba2e19057e5)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using environment variable for square brackets is not documented, not obvious</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6352</link><project id="" key="" /><description>The example config in elasticsearch.yml is:

`#discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]`

Using environment variables, I would assume to replace everything in square brackets:
`export ES_HOSTS="[host1, host2:port]"
discovery.zen.ping.unicast.hosts: $ES_HOSTS`

This does not parse correctly though.  The only way I could get it to work was omit the brackets:
`export ES_HOSTS="host1, host2:port"
discovery.zen.ping.unicast.hosts: $ES_HOSTS`

Given that this is not documented, not obvious, and also parses fine as:
`#discovery.zen.ping.unicast.hosts: "host1", "host2:port"`

I propose we change the default config file.  There is also no other use of square brackets in the config file.
</description><key id="34644351">6352</key><summary>Using environment variable for square brackets is not documented, not obvious</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/debadair/following{/other_user}', u'events_url': u'https://api.github.com/users/debadair/events{/privacy}', u'organizations_url': u'https://api.github.com/users/debadair/orgs', u'url': u'https://api.github.com/users/debadair', u'gists_url': u'https://api.github.com/users/debadair/gists{/gist_id}', u'html_url': u'https://github.com/debadair', u'subscriptions_url': u'https://api.github.com/users/debadair/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/362578?v=4', u'repos_url': u'https://api.github.com/users/debadair/repos', u'received_events_url': u'https://api.github.com/users/debadair/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/debadair/starred{/owner}{/repo}', u'site_admin': False, u'login': u'debadair', u'type': u'User', u'id': 362578, u'followers_url': u'https://api.github.com/users/debadair/followers'}</assignee><reporter username="">matthughes</reporter><labels><label>adoptme</label><label>docs</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-05-30T13:24:49Z</created><updated>2016-01-15T12:41:20Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="matthughes" created="2014-05-30T13:25:16Z" id="44649889">https://groups.google.com/forum/#!topic/elasticsearch/RlyeFhnZjlI
</comment><comment author="clintongormley" created="2014-12-30T19:20:59Z" id="68387759">Hi @matthughes 

Sorry it has taken a while to look at this.  The config file is just YAML, and arrays are perfectly acceptable in YAML, so I don't think it is worth preventing arrays from working.

I agree though that the documentation could be improved, specifically here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html#styles
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Difference in deb/rpm init scripts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6351</link><project id="" key="" /><description>After checking the init scripts there are allot of differences between them.
This is the result after an initial scan of the files.

RPM doesn't have:
- id = root check ( validation of the user trying to start is the root user )
- default values ( currently sysconfig file is leading while default values should be in init script and sysconfig file should override those )
- Debian script has a nice DAEMON and DAEMON_OPTS variables to keep things clean and simple.
- Check for different JDK directories to use.
- -Des.default.config=$CONF_FILE option when launching ES.
- Uses lower and upper case variable names ( nicer to use upper case everywhere. )

DEB doesn't have:
- Daemon check ( does the program exist )
- Does the config file exist?
- mkdir/chown for general directories
- if MAX_LOCKED_MEMORY is set, make sure that ES_HEAP_SIZE  is set as well.
</description><key id="34641753">6351</key><summary>Difference in deb/rpm init scripts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">electrical</reporter><labels><label>:Packaging</label><label>adoptme</label></labels><created>2014-05-30T12:42:54Z</created><updated>2015-10-14T16:40:00Z</updated><resolved>2015-10-14T16:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T16:40:00Z" id="148110939">I believe these scripts have been sync'ed up now, closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove java-6 directories from debian init script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6350</link><project id="" key="" /><description>As we only support java 7 from 1.2 on, we should backport this down into 1.2.
</description><key id="34638659">6350</key><summary>Remove java-6 directories from debian init script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Packaging</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-30T11:44:45Z</created><updated>2015-06-07T13:19:19Z</updated><resolved>2014-06-13T11:49:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="electrical" created="2014-05-30T11:56:07Z" id="44642927">LGTM
</comment><comment author="nik9000" created="2014-05-30T13:22:05Z" id="44649555">+1
</comment><comment author="s1monw" created="2014-06-12T09:05:34Z" id="45845716">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Turn off bloom filters on _uid by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6349</link><project id="" key="" /><description>In #6298, reusing Lucene's TermsEnums gets back much of the performance that bloom filters buy us, at least in an initial test; I think we can turn them off by default and save some RAM.

We can simply change the default index.codec.bloom.load to false, so we still index bloom filters, and then if a user has trouble, they can turn them back on.

I plan to run some more tests to see if the results hold up; I'll post here.
</description><key id="34637767">6349</key><summary>Turn off bloom filters on _uid by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-05-30T11:28:02Z</created><updated>2014-08-12T18:29:11Z</updated><resolved>2014-07-23T10:03:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-30T15:13:31Z" id="44662039">Would it be possible to count the average number of disk seeks per version lookup in both cases. I think this would give a pretty good idea of how performance would compare in case of an index that is cold or much larger than RAM?
</comment><comment author="mikemccand" created="2014-05-30T15:37:44Z" id="44665020">I did exactly that when working on the UUID blog post.  Random IDs (which ES is using for auto-id) are the worst case: they cause a seek per segment, once the index is large enough.  Predictable IDs give much less seeking.

I didn't test seek count with the bloom filters; I can do that.  It should be much less, though I suspect even in a cold case (overall index bigger than free RAM), the OS would keep the _uid terms dict blocks warmish in the update case, as long as ongoing indexing is fast enough, because ES is doing a lookup per indexed doc.  Especially if the lookups are biased towards recently indexed docs.

In the append-only case I think none of this matters much, because we are never doing a lookup by ID.
</comment><comment author="mikemccand" created="2014-07-21T15:45:26Z" id="49623242">I ran another test here, indexing 50M small (lines from web access logs) docs.  I pass my own ID (so ES must do the ID lookup), and 25% of the time the ID does exist and so the doc is replaced.

It was a worst case test: I used fully random UUIDs, docs are tiny, I left terms index at its default settings (i.e., did not let it use the ~10 bits of RAM per UUID that blooms got to use), and indexing performance was ~10% slower.  This used to be much, much worse before #6298 ...

I suspect the apps that do pass their own ID and update docs are "typically" indexing larger docs than the common "append only tiny docs" case, and so that 10% would be lower because more time is spent actually indexing.

Net/net I think we should disable blooms today: I think the added RAM usage at search time is dangerous and not worth the minor indexing gains.  We could do this in a low-risk way, just by changing the default  index.codec.bloom.load to false.  This way the bloom filters are still computed at indexing time, but not loaded at search time.  Apps that "need them" can just flip that boolean to true.

Or we can stop computing them at indexing time too; this means apps that want them back would have to re-index.
</comment><comment author="mikemccand" created="2014-07-23T10:03:58Z" id="49854966">Closed via #6959
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Made base64 decode parsing to detect more errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6348</link><project id="" key="" /><description>The base64 did not completely check, if there were other characters
after the equals `=` sign. This PR adds some small additional checks.

Closes #6334
</description><key id="34634962">6348</key><summary>Made base64 decode parsing to detect more errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-30T10:38:51Z</created><updated>2015-06-07T13:21:04Z</updated><resolved>2014-06-24T11:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T09:07:23Z" id="45845862">this looks great, can we maybe have some more random tests that encode / decode random strings?
</comment><comment author="spinscale" created="2014-06-13T13:12:20Z" id="46009047">good point. completely switched to randomized strings now in the test
</comment><comment author="s1monw" created="2014-06-24T10:50:59Z" id="46957039">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UnsupportedClassVersion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6347</link><project id="" key="" /><description>I keep having this problem:

https://github.com/elasticsearch/elasticsearch/issues/6313

Meaning I get : 

java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0

When I execute elastic search.

I am running Java Version 7 Update 60 (build 1.7.0_60-b19) as I can see in my Java Control Panel.

Any suggestions, on how to handle it? Maybe there is another JRE which might be running and being used by elastic search?

Thanks!
</description><key id="34629614">6347</key><summary>UnsupportedClassVersion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lamakun</reporter><labels /><created>2014-05-30T09:20:53Z</created><updated>2014-05-30T09:40:43Z</updated><resolved>2014-05-30T09:40:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lamakun" created="2014-05-30T09:40:43Z" id="44633080">Problem solved. Meanwhile I had Java 7 version installed the terminal was showing Java 6 ($&gt;java -version). To solve it I needed to instal JDK and not only JRE.

Now it all works properly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix top_hits reduce error in case of empty shard results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6346</link><project id="" key="" /><description>The top_hits aggregation returned an empty InternalTopHits instance with no fields set when there were no result, causing reduce and serialization errors down the road. This is fixed by setting all required fields when a there are no results.

This bug does't affect any released versions, since the the top_hits aggregation is part of the upcoming and unreleased 1.3.0 release.
</description><key id="34629250">6346</key><summary>Fix top_hits reduce error in case of empty shard results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2014-05-30T09:15:52Z</created><updated>2015-05-18T23:31:34Z</updated><resolved>2014-05-30T09:42:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-30T09:27:48Z" id="44630848">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping API: Added portuguese stem token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6345</link><project id="" key="" /><description>The stem token filter was cloaked, due to a wrong if-else chain and could
never be loaded. Added it as `rslp_portuguese`, the existing `portuguese` token filter
is the one based on snowball in order to retain backwards compatibility.

Closes #6330
</description><key id="34626846">6345</key><summary>Mapping API: Added portuguese stem token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2014-05-30T08:37:32Z</created><updated>2014-06-13T16:43:39Z</updated><resolved>2014-06-11T10:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-30T08:42:07Z" id="44627514">LGTM. +1 to push.
</comment><comment author="clintongormley" created="2014-06-10T15:53:43Z" id="45632870">@spinscale I've done a bigger cleanup of the `stemmer` token filter here: #6452
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>docs-delete-by-query is not working with elasticsearch 1.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6344</link><project id="" key="" /><description>i think do that wrote according to guideline.
https://github.com/elasticsearch/elasticsearch/blob/master/CONTRIBUTING.md

from elasticsearch version 0.90.7, was used delete-by-query-string query . 
at the one time. deleted approximately 250 doc. 
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html 

and i did upgrade version to 1.2.0.
after that, i did meet up error screen. 
please help me .. 
thanks you.
# create index

curl -XPOST 'http://10.99.196.141:22000/delete_by_query_string_query_test1'
# create mapping

curl -XPUT 'http://10.99.196.141:22000/delete_by_query_string_query_test1/1/_mapping' -d '{
    "properties":{
        "playId":{
            "type":"string",
            "index":"not_analyzed",
            "store":"true"
        }
    }
}'
# make doc

curl -XPOST 'http://10.99.196.141:22000/delete_by_query_string_query_test1/1' -d '{
  "playId": "1395395f-1543-44f7-ba11-fd8eff137afe"
}'
# delete by query string query

curl -XDELETE 'http://10.99.196.141:22000/delete_by_query_string_query_test1/1/_query?pretty=true' -d '
{
    "query_string" : {
        "default_field" : "playId",
         "query" : "1395395f-1543-44f7-ba11-fd8eff137afe"  
    }
}
'
# response occured error -

{
  "_indices" : {
    "delete_by_query_string_query_test1" : {
      "_shards" : {
        "total" : 2,
        "successful" : 0,
        "failed" : 2,
        "failures" : [ {
          "index" : "delete_by_query_string_query_test1",
          "shard" : 1,
          "reason" : "RemoteTransportException[[10.99.196.141_21001][inet[/10.99.196.141:21001]][deleteByQuery/shard]]; nested: QueryParsingException[[delete_by_query_string_query_test1] request does not support [query_string]]; "
        }, {
          "index" : "delete_by_query_string_query_test1",
          "shard" : 0,
          "reason" : "RemoteTransportException[[10.101.63.182_21000][inet[/10.101.63.182:21000]][deleteByQuery/shard]]; nested: QueryParsingException[[delete_by_query_string_query_test1] request does not support [query_string]]; "
        } ]
      }
    }
  }
}
</description><key id="34618245">6344</key><summary>docs-delete-by-query is not working with elasticsearch 1.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">happyprg</reporter><labels /><created>2014-05-30T05:06:56Z</created><updated>2014-05-30T05:14:15Z</updated><resolved>2014-05-30T05:14:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>make boost_factor consistent with other functions in function_score query DSL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6343</link><project id="" key="" /><description>boost_factor is currently the only function that doesn't accept hash but only a single value, this might make it more difficult to work with for some clients (currently dealing with it in the Python DSL).

Could we add support for also doing `{"boost_factor": {"value": 3}}`? Or maybe `{"boost_factor": {"boost": 3}}`?

Thanks!
</description><key id="34604282">6343</key><summary>make boost_factor consistent with other functions in function_score query DSL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HonzaKral</reporter><labels /><created>2014-05-29T23:36:52Z</created><updated>2014-12-30T19:15:39Z</updated><resolved>2014-12-30T19:15:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-29T23:46:13Z" id="44599621">+1 for the sake of consistency and future proofness

+1 on `{ "boost_factor" : { "boost" : 3 } }`
</comment><comment author="clintongormley" created="2014-12-30T19:15:39Z" id="68387223">`boost_factor` has been removed, and replaced with `weight` (a parameter which can be passed to any function)

closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Have a dedicated join timeout that is higher than ping.timeout for join</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6342</link><project id="" key="" /><description>Using ping.timeout, which defaults to 3s, to use as a timeout value on the join request a node makes to the master once its discovered can be too small, specifically when there is a large cluster state involved (and by definition, all the buffers and such on the nio layer will be "cold"). Introduce a dedicated join.timeout setting, that by default is 10x the ping.timeout (so 30s by default).
</description><key id="34601735">6342</key><summary>Have a dedicated join timeout that is higher than ping.timeout for join</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Discovery</label><label>enhancement</label><label>resiliency</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-29T22:50:31Z</created><updated>2015-06-07T13:19:32Z</updated><resolved>2014-05-30T10:42:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-30T08:17:27Z" id="44625701">LGTM with the exception of documentation about the new parameter, we should mention, that it is bound to the ping timeout by default. Also the behaviour is different, as `ping_timeout` and `ping.timeout` are supported, but only `join_timeout` (which I think is ok).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting tags do not surround special characters at end of search string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6341</link><project id="" key="" /><description>Elastic Search Version 1.1
Java API

Issue: We are using elastic search 1.1 and performing highlighting on the our results. We starting noticing a particularly strange occurence where the tags where being inserted in regards to single quotes.

If I were to search the exact sting (with single quotes): 'de
I am returned this string as one of my results (this works):
&lt;b&gt;'de&lt;/b&gt;termine output from
However if I perform this search: de'
Now the tags do no honor the single quote anymore and this string is returned:
'Add co&lt;b&gt;de&lt;/b&gt;' button

So the issue here is that when it comes to highlighting, es is not honoring trailing single quotes in the highlighting. But if the quote comes first in the search string it will honor it in the highlighting. Elastic search should honor wrapping the highlighting tags around the entirely found search string when it ends with a single quote.

Update:

This issue also occurs with any special character " / \ | and more when they are the last item in the search string, they don't get highlighted.
</description><key id="34579753">6341</key><summary>Highlighting tags do not surround special characters at end of search string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roblangenfeld</reporter><labels><label>:Highlighting</label></labels><created>2014-05-29T18:30:54Z</created><updated>2016-11-24T18:47:21Z</updated><resolved>2016-11-24T18:47:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mdomans" created="2014-10-31T21:35:16Z" id="61334879">I have the same problem with 1.3
</comment><comment author="clintongormley" created="2016-11-24T18:47:21Z" id="262831346">No recreation provided, but this looks like a simple problem with analysis which removes punctuation.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failure in saving percolate query with date type function score </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6340</link><project id="" key="" /><description>When I try to save a query in percolate which has function score on a date field, it fails with error Failed to parse NullPointerException. If function score is on another data type field like number, it works fine. 

```
PUT testindex

PUT testindex/document/1
{
  "actionDate" : "2014-05-01",
  "score" : 5
}

PUT testindex/.percolator/1
{
  "query" : {
    "function_score" : {
      "functions" : [
        {
          "exp" :{ 
              "actionDate" : {
                "origin" : "now",
                "scale" : "10d"
              }
          }
        }
      ],
      "query" : {
        "match_all": {}
      }
    }
  }
}

{
   "error": "RemoteTransportException[[nylxdev1.node][inet[/172.17.9.175:9301]][index]]; nested: PercolatorException[[testindex] failed to parse query [1]]; nested: QueryParsingException[[testindex] Failed to parse]; nested: NullPointerException; ",
   "status": 500
}
```
</description><key id="34572349">6340</key><summary>Failure in saving percolate query with date type function score </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">ajhalani</reporter><labels><label>:Percolator</label><label>adoptme</label><label>bug</label></labels><created>2014-05-29T16:59:34Z</created><updated>2015-10-26T05:25:59Z</updated><resolved>2015-10-26T05:25:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-30T09:33:56Z" id="44631326">Thanks for reporting this @ajhalani!

Stacktrace:

```
org.elasticsearch.index.percolator.PercolatorException: [testindex] failed to parse query [1]
    at org.elasticsearch.index.percolator.PercolatorQueriesRegistry.parsePercolatorDocument(PercolatorQueriesRegistry.java:182)
    at org.elasticsearch.index.percolator.PercolatorQueriesRegistry$RealTimePercolatorOperationListener.preIndex(PercolatorQueriesRegistry.java:306)
    at org.elasticsearch.index.indexing.ShardIndexingService.preIndex(ShardIndexingService.java:132)
    at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:401)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:198)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:534)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:433)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.elasticsearch.index.query.QueryParsingException: [testindex] Failed to parse
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:234)
    at org.elasticsearch.index.percolator.PercolatorQueriesRegistry.parseQuery(PercolatorQueriesRegistry.java:195)
    at org.elasticsearch.index.percolator.PercolatorQueriesRegistry.parsePercolatorDocument(PercolatorQueriesRegistry.java:180)
    ... 9 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.query.functionscore.DecayFunctionParser.parseDateVariable(DecayFunctionParser.java:268)
    at org.elasticsearch.index.query.functionscore.DecayFunctionParser.parseVariable(DecayFunctionParser.java:166)
    at org.elasticsearch.index.query.functionscore.DecayFunctionParser.parse(DecayFunctionParser.java:148)
    at org.elasticsearch.index.query.functionscore.FunctionScoreQueryParser.parseFiltersAndFunctions(FunctionScoreQueryParser.java:190)
    at org.elasticsearch.index.query.functionscore.FunctionScoreQueryParser.parse(FunctionScoreQueryParser.java:116)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:230)
    ... 11 more
```

The search context isn't set during parsing of the percolator query, causing the above stacktrace.
</comment><comment author="martijnvg" created="2015-01-06T22:07:20Z" id="68942985">Fixing the NPE that is occurring is relatively easy, but there is a bigger problem with this percolator query.

The `now` usage in this function score function is problematic, because it is resolved at percolate query register time and after that for each percolate request the now time will stay the same. This is similar to date ranges issue in percolator queries and alias filters, which was fixed only recently: #8534

Fixing this will add additional logic similar as was added via #8534, do we want this extra complexity? 
</comment><comment author="clintongormley" created="2015-01-13T14:49:06Z" id="69754674">@martijnvg i think so - it makes sense for the same reasons as #8534
</comment><comment author="clintongormley" created="2015-10-14T16:39:20Z" id="148110572">With the new query parsing coming in 3.0, it should be possible to handle dates correctly.

@martijnvg there's nothing to do before 3.0, is there?
</comment><comment author="martijnvg" created="2015-10-26T05:25:59Z" id="151027923">This has been fixed some time ago, I think I just forgot to close this issue:
https://github.com/elastic/elasticsearch/commit/7cc2bc8a145456297e7e33a2c22f5ea4b778e04e

If `now` is used in range queries we resolve that at lucene query rewrite time. In case of the percolator that happens at percolate time and the current time should therefor not be stale any more.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6339</link><project id="" key="" /><description /><key id="34561102">6339</key><summary>fixed typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">MungoH</reporter><labels><label>docs</label></labels><created>2014-05-29T14:51:38Z</created><updated>2014-07-16T21:44:28Z</updated><resolved>2014-06-03T10:06:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T10:06:09Z" id="44945867">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: Added GeoBounds Aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6338</link><project id="" key="" /><description>The GeoBounds Aggregation is a new single bucket aggregation which outputs the coordinates of a bounding box containing all the points from all the documents passed to the aggregation as well as the doc count.

Closes #5634
</description><key id="34544085">6338</key><summary>Aggregations: Added GeoBounds Aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels /><created>2014-05-29T10:27:35Z</created><updated>2014-08-21T15:07:53Z</updated><resolved>2014-06-03T15:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-02T22:39:20Z" id="44900249">I just did another round but I think it's very close!
</comment><comment author="jpountz" created="2014-06-03T09:30:37Z" id="44942783">Thanks @colings86 it looks good. I left minor comments about formatting but other than that it looks good to me. Something else that would be nice before pushing would be to mark this aggregation as experimental as we did for eg. percentiles (just add a warning to the docs) so that we don't commit on bw compat: this will allow us to completely change the API in case we find a better way to expose the feature. We'll just remove the warning in a few releases if we're happy with it (I'm currently thinking of removing this warning for percentiles).
</comment><comment author="jpountz" created="2014-06-03T14:17:14Z" id="44969854">LGTM
</comment><comment author="colings86" created="2014-06-03T15:04:16Z" id="44976234">Closing PR as it has been merged but was not automatically closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Hide some system fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6337</link><project id="" key="" /><description>Hi all!
I use elasticsearch in a high-load project where an urgent need to save traffic.
I have a some queries like this:

``` bash
curl -XGET 'http://localhost:9200/testindex/testmapping/_search?pretty&amp;scroll=5m' -d '{"from":0, "size":1000}'

{
  "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7MjszMjp6TmdjNmxkM1NtV1NOeTl5X3dab1FnOzMxOnpOZ2M2bGQzU21XU055OXlfd1pvUWc7MDs=",
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 2,
    "successful" : 2,
    "failed" : 0
  },
  "hits" : {
    "total" : 15457332,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "testindex",
      "_type" : "testmapping",
      "_id" : "mo7vQrWUTquBRowjq2AVkw",
      "_score" : 1.0, "_source" : {"reffer_id":"","date":"2013-05-31T00:00:00","source":5,"user_id":"2fdfdf0fbbce603cf24c0eee7dabf28c"}
    }, ....]
  }
}
```

Can I exclude some system fields (like _shards.*, hits._index, hits._type, hits._id, hits._score)? I found how exclude source fields, but not system.
Also I need to get _timestamp field in _source rows. It generated from 'date' field:

``` javascript
'_timestamp' =&gt; array(
    'enabled' =&gt; true,
    'path' =&gt; 'date',
    'format' =&gt; "YYYY-MM-dd'T'HH:mm:ss"
)
```
</description><key id="34537050">6337</key><summary>Hide some system fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Sergspm</reporter><labels /><created>2014-05-29T08:26:42Z</created><updated>2014-05-29T08:35:14Z</updated><resolved>2014-05-29T08:35:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-29T08:35:14Z" id="44507852">We'll be happy to help you on the mailing list. Note that github issues are only for issues and feature requests.

Closing. Feel Free to reopen if you think it's an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Title in Batch file (Windows)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6336</link><project id="" key="" /><description>Please add a title to the batch file in windows version (and possibly in linux version as well) so that we can differentiate between a regular cmd window and elasticsearch also which version of elasticsearch is running. - The title could be the elasticsearch jar name itself!
</description><key id="34531694">6336</key><summary>Title in Batch file (Windows)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">wodash</reporter><labels /><created>2014-05-29T06:25:06Z</created><updated>2014-07-09T16:18:19Z</updated><resolved>2014-07-09T16:18:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>docs-delete-by-query is not working with elasticsearch 1.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6335</link><project id="" key="" /><description>http://elasticsearch-users.115913.n3.nabble.com/docs-delete-by-query-is-not-working-with-elasticsearch-1-2-0-td4056615.html

docs-delete-by-query is not working with elasticsearch 1.2.0

i did version up  elasticsearch from 0.90.7  to 1.2.0 
and i did regressionTest. 

after that. doc-delete-by-query is not working. 

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-delete-by-query.html
- request 
  org.apache.http.wire[72] - http-outgoing-5 &gt;&gt; "DELETE /test_10.70.16.183_708246/temp/_query HTTP/1.1[\r][\n]" 

e.g) 
{ 
    "query_string": { 
        "default_field": "playId", 
        "query": "9eaee1fb-c8ab-4ef3-9e37-5c3f68ba21e1 OR 6b6352c1-209e-4d0c-be85-c7048453ad03ORdbe9e6b3-895f-4bd4-8943-b50ddd8712 .. OR ... more than 200 docu" 
    } 
} 
- response 

{ 
    "_indices":{ 
        "test_10.70.16.183_708246":{ 
            "_shards":{ 
                "total":2, 
                "successful":0, 
                "failed":2, 
                "failures":[ 
                    { 
                        "index":"test_10.70.16.183_708246", 
                        "shard":0, 
                        "reason":"RemoteTransportException[ 
                            [ 
                                10.99.198.150_21000 
                            ][ 
                                inet[ 
                                    /10.99.198.150:21000 
                                ] 
                            ][ 
                                deleteByQuery/shard 
                            ] 
                        ]; nested: QueryParsingException[ 
                            [ 
                                test_10.70.16.183_708246 
                            ] request does not support [ 
                                simple_query_string 
                            ] 
                        ]; " 
                    }, 
                    { 
                        "index":"test_10.70.16.183_708246", 
                        "shard":1, 
                        "reason":"RemoteTransportException[ 
                            [ 
                                10.99.198.150_21001 
                            ][ 
                                inet[ 
                                    /10.99.198.150:21001 
                                ] 
                            ][ 
                                deleteByQuery/shard 
                            ] 
                        ]; nested: QueryParsingException[ 
                            [ 
                                test_10.70.16.183_708246 
                            ] request does not support [ 
                                simple_query_string 
                            ] 
                        ]; " 
                    } 
                ] 
            } 
        } 
    } 
} 

what's worng? 
plz somebody help me. 
</description><key id="34531339">6335</key><summary>docs-delete-by-query is not working with elasticsearch 1.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">happyprg</reporter><labels /><created>2014-05-29T06:15:10Z</created><updated>2014-05-30T05:11:54Z</updated><resolved>2014-05-30T05:11:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-29T07:38:45Z" id="44502699">The mailing list is definitely the best place to start discussing it.
If you did not get an answer there, it's probably because your question is incomplete.

You should read http://www.elasticsearch.org/help/

If you can provide a full SENSE or curl recreation, that would help understanding what you are doing.
</comment><comment author="happyprg" created="2014-05-29T07:50:24Z" id="44503848">ok isee.
i will prepare  question  according to guideline
wait plz.
thanks you^^;
</comment><comment author="happyprg" created="2014-05-30T03:30:19Z" id="44611765">i think do that wrote according to guideline.
https://github.com/elasticsearch/elasticsearch/blob/master/CONTRIBUTING.md

from elasticsearch version 0.90.7, was used delete-by-query-string query . 
at the one time. deleted approximately 250 doc. 
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-query-string-query.html 

and i did upgrade version to 1.2.0.
after that, i did meet up error screen. 
please help me .. 
thanks you.

# create index

curl -XPOST 'http://10.99.196.141:22000/delete_by_query_string_query_test1'

# create mapping

curl -XPUT 'http://10.99.196.141:22000/delete_by_query_string_query_test1/1/_mapping' -d '{
    "properties":{
        "playId":{
            "type":"string",
            "index":"not_analyzed",
            "store":"true"
        }
    }
}'

# make doc

curl -XPOST 'http://10.99.196.141:22000/delete_by_query_string_query_test1/1' -d '{
  "playId": "1395395f-1543-44f7-ba11-fd8eff137afe"
}'

# delete by query string query

curl -XDELETE 'http://10.99.196.141:22000/delete_by_query_string_query_test1/1/_query?pretty=true' -d '
{
    "query_string" : {
        "default_field" : "playId",
         "query" : "1395395f-1543-44f7-ba11-fd8eff137afe"  
    }
}
'

# response occured error -

{
  "_indices" : {
    "delete_by_query_string_query_test1" : {
      "_shards" : {
        "total" : 2,
        "successful" : 0,
        "failed" : 2,
        "failures" : [ {
          "index" : "delete_by_query_string_query_test1",
          "shard" : 1,
          "reason" : "RemoteTransportException[[10.99.196.141_21001][inet[/10.99.196.141:21001]][deleteByQuery/shard]]; nested: QueryParsingException[[delete_by_query_string_query_test1] request does not support [query_string]]; "
        }, {
          "index" : "delete_by_query_string_query_test1",
          "shard" : 0,
          "reason" : "RemoteTransportException[[10.101.63.182_21000][inet[/10.101.63.182:21000]][deleteByQuery/shard]]; nested: QueryParsingException[[delete_by_query_string_query_test1] request does not support [query_string]]; "
        } ]
      }
    }
  }
}
</comment><comment author="dadoonet" created="2014-05-30T05:11:54Z" id="44615675">As I said, you should read http://www.elasticsearch.org/help/ and send this to the mailing list.
It's not an issue here. You just have an incorrect request here.

This should be wrap in a query from 1.0 as explained in doc: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-delete-by-query.html#docs-delete-by-query
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Base64 decode parsing detects more errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6334</link><project id="" key="" /><description>I found a bug in the `Base64.decode()` function. It will decode incorrectly formatted base 64 strings. For example, `"user:password"` encodes to `"dXNlcjpwYXNzd29yZA=="`. Both `"dXNlcjpwYXNzd29yZA==123"` and `"dXNlcjpwYXNzd29yZA=5"` decode to "user:password" although neither of them are valid base 64 strings. You can add or replace characters after the first padding character.

I wrote a quick test to show the bug.

``` java
@Test
public void testBase64DecodeWithExtraCharactersAfterPadding() {
    // "user:password" encodes to "dXNlcjpwYXNzd29yZA=="
    String correctlyEncoded = "";
    String extraCharacters = "";
    String replacePaddingCharacter = "";
    try {
        correctlyEncoded = new String(Base64.decode("dXNlcjpwYXNzd29yZA=="));
        extraCharacters = new String(Base64.decode("dXNlcjpwYXNzd29yZA==123"));
        replacePaddingCharacter = new String(Base64.decode("dXNlcjpwYXNzd29yZA=5"));
    } catch (IOException e) {
    }
    assertEquals("user:password", correctlyEncoded);
    assertNotEquals("user:password", extraCharacters); // this assertion fails
    assertNotEquals("user:password", replacePaddingCharacter); // this assertion fails
}
```

I think this line is the culprit.
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/Base64.java#L1219

It breaks early when it should really throw an `IOException` like the code 8 lines down.
</description><key id="34503992">6334</key><summary>Internal: Base64 decode parsing detects more errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikringsmuth</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-28T20:33:09Z</created><updated>2015-06-07T13:19:54Z</updated><resolved>2014-06-24T11:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Tribe node with Amazon public DNS in a multi-region setup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6333</link><project id="" key="" /><description>Trying to configure a tribe node on a multi-region clusters setup, I am running into an issue trying to configure this setup with Amazon Public DNS.

discovery.ec2.host_type: public_dns
network.publish_host: _ec2:publicDns_

In that case, the remote tribe node... discover the cluster but then try to connect to the remote node with his local.ip
With public_Ip, it seems to be working but its not acceptable in our situation since we do not want the internal cluster to talk internally with Public I.p

Normally using public_dns go around  that issue.. since it tries to connect  using internal i.p when it makes sense.
</description><key id="34482875">6333</key><summary>Tribe node with Amazon public DNS in a multi-region setup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Dchamard</reporter><labels><label>:Plugin Discovery EC2</label><label>:Tribe Node</label><label>discuss</label><label>feedback_needed</label></labels><created>2014-05-28T16:23:54Z</created><updated>2016-11-25T17:58:56Z</updated><resolved>2016-11-25T17:58:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:12:14Z" id="68386883">Hi @Dchamard 

Sorry it has taken a while to get to this.  I got a bit lost in your description.  Did you manage to get it working satisfactorily?
</comment><comment author="rparkhunovsky" created="2015-02-12T18:13:17Z" id="74121637">Hi @Dchamard
Interested too!!
</comment><comment author="dadoonet" created="2015-02-12T18:23:44Z" id="74123460">I think it's related to this issue: https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/76
</comment><comment author="rparkhunovsky" created="2015-02-16T13:44:06Z" id="74509820">The bug has been around for almost a year which makes using such a useful feature complicated. I'm eager to provide details to test and fix it!
</comment><comment author="Pryz" created="2015-10-06T11:41:14Z" id="145830926">Any news or feedback on that ?

I'm facing the same issue with elasticsearch 1.7.1
</comment><comment author="clintongormley" created="2016-11-25T17:58:56Z" id="263005426">Closing in favour of #21473</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Memory management: do not enforce the BigArrays limit on the network layer and the tranlog.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6332</link><project id="" key="" /><description>BigArrays byte accounting (https://github.com/elasticsearch/elasticsearch/pull/6050) applies all the time. Yet, we might want to disable it for cluster-management-related operations so that they would not be impacted eg. in case of heavy search requests.
</description><key id="34472400">6332</key><summary>Memory management: do not enforce the BigArrays limit on the network layer and the tranlog.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-28T14:38:51Z</created><updated>2014-06-03T14:31:21Z</updated><resolved>2014-06-03T09:44:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kevinkluge" created="2014-05-28T15:08:08Z" id="44419121"> would this significantly reduce the memory usage monitored by this breaker?  

I am concerned that our default of 20% on the BigArray breaker is too low.  I'm not sure if this is enough to change that or if we need to separately change the default.
</comment><comment author="jpountz" created="2014-05-28T15:19:16Z" id="44420697">I'm not sure about how much the monitored memory usage would be reduced but I agree on the fact that 20% might be too low. To me the issue is that is if you have very large field data, this would actually make sense to make sure you never go out of memory but on the other hand if you rely on doc values, this makes you waste memory. I heard @dakrone is working on a way to share memory across several breakers, I think it would help for that issue?
</comment><comment author="dakrone" created="2014-05-28T15:40:33Z" id="44423836">Yes, currently I have a POC I'm working on for a CircuitBreakerService that has child circuit breakers (for example, one for fielddata and one for BigArrays/requests) and when a circuit break happens on one breaker, it can "borrow" space from another breaker if there is memory available within configurable minimums and maximums.
</comment><comment author="jpountz" created="2014-06-02T10:05:15Z" id="44820521">@kevinkluge I opened a separate pull request to deal with the default breaker value: https://github.com/elasticsearch/elasticsearch/pull/6375
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to netty 3.9.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6331</link><project id="" key="" /><description /><key id="34462229">6331</key><summary>Upgrade to netty 3.9.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>:Network</label><label>upgrade</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-28T12:41:05Z</created><updated>2015-08-25T13:25:43Z</updated><resolved>2014-05-29T22:21:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-28T20:45:12Z" id="44461998">LGTM, changelog/issues look good, see https://github.com/netty/netty/issues?milestone=81&amp;page=1&amp;state=closed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StemmerTokenFilterFactory has two entries for language="portuguese"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6330</link><project id="" key="" /><description>There are two checks for Portuguese; one returns a PortugueseStemFilter and one returns a SnowballFilter. The one returning a SnowballFilter comes first so there's no way to get a PortugueseStemFilter.
</description><key id="34461409">6330</key><summary>StemmerTokenFilterFactory has two entries for language="portuguese"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">hughbiquitous</reporter><labels /><created>2014-05-28T12:28:47Z</created><updated>2014-06-11T10:31:18Z</updated><resolved>2014-06-11T10:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add the ability to specify the analyzer used for each Field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6329</link><project id="" key="" /><description>When using multiple items, the user may want to specify which analyzer to use
for each field. Previously, either the analyzer specified by 'analyzer' would
be used for all the fields, or if not set, the analyzer associated with the
field would be chosen. This commit provides the ability to fine grain which
analyzer should be used for each field by providing a new 'fields_analyzer'
parameter to the More Like This Query.
</description><key id="34458639">6329</key><summary>Add the ability to specify the analyzer used for each Field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-28T11:43:25Z</created><updated>2015-06-07T10:37:03Z</updated><resolved>2014-08-23T17:32:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T12:39:33Z" id="45885869">left a small comment - it's close
</comment><comment author="s1monw" created="2014-06-18T18:47:28Z" id="46476977">LGTM
</comment><comment author="alexksikes" created="2014-08-23T17:32:22Z" id="53159671">This should be integrated to the term vector APIs? Closing for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw parsing errors on unknown keys in warmers configuration json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6328</link><project id="" key="" /><description> The Index Warmers code is too lenient w.r.t unknown configuration options. Accepting them lead to other parsing problems, for example wrongfully setting the source to null.
</description><key id="34457812">6328</key><summary>Throw parsing errors on unknown keys in warmers configuration json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Warmers</label><label>adoptme</label><label>enhancement</label></labels><created>2014-05-28T11:30:10Z</created><updated>2016-03-14T13:06:55Z</updated><resolved>2016-03-08T14:31:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-28T11:39:25Z" id="44395514">Maybe a `SearchParseException` would be more appropriate as an exception?
</comment><comment author="spinscale" created="2014-05-28T11:42:37Z" id="44395776">that test could be a unit test possibly or did you intentionally create an integration test?
</comment><comment author="s1monw" created="2015-03-20T21:46:57Z" id="84161737">+1 to @spinscale suggestion - @bleskes will you pick this up at any point or should I find somebody?
</comment><comment author="bleskes" created="2015-03-23T21:19:10Z" id="85202117">@s1monw - I'll give it another shot..
</comment><comment author="nik9000" created="2015-07-27T17:40:24Z" id="125283373">Is this intentionally now `adoptme`?  @bleskes if you don't want this you can assign it to me and I'll finish it up.
</comment><comment author="bleskes" created="2015-08-13T11:52:58Z" id="130637870">@nik9000 erhm - this is so long ago I don't remember .. my bad for not getting to it. Do feel free to pick it up 
</comment><comment author="clintongormley" created="2016-03-08T14:31:29Z" id="193804495">Warmers have been removed so this is no longer needed.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Store documentation doesn't explain how to change storage type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6327</link><project id="" key="" /><description>The [store documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html) describes the different storage types but not how to switch them.
</description><key id="34455887">6327</key><summary>[docs] Store documentation doesn't explain how to change storage type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-05-28T11:01:16Z</created><updated>2014-06-12T11:56:35Z</updated><resolved>2014-06-12T11:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Ensure internal scope extrators are always operating on a Map</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6326</link><project id="" key="" /><description>Mustache extracts the key/value pairs for parameter substitution from
objects and maps but it's decided on the first execution. We need to
make sure if the params are null we pass an empty map to ensure we
bind the map based extractor

Closes #6318
</description><key id="34453475">6326</key><summary>Ensure internal scope extrators are always operating on a Map</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search Templates</label><label>bug</label><label>v1.1.3</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-28T10:23:15Z</created><updated>2015-06-07T19:56:38Z</updated><resolved>2014-05-28T11:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Serialization of queue size is broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6325</link><project id="" key="" /><description>This issue exists in both in the 1.1 branch and head.

I have set the threadpool.get.queue_size to -1 using the REST API.
I am running the elasticsearch server with assertions enable (-ea).
If then a TransportClient with "sniff=false" connects and tries to send a query it gets a org.elasticsearch.client.transport.NoNodeAvailableException
The underlying exception is:
Caused by: java.lang.AssertionError
    at org.elasticsearch.common.io.stream.StreamOutput.writeVLong(StreamOutput.java:176)
    at org.elasticsearch.common.io.stream.AdapterStreamOutput.writeVLong(AdapterStreamOutput.java:126)
    at org.elasticsearch.common.unit.SizeValue.writeTo(SizeValue.java:211)
    at org.elasticsearch.threadpool.ThreadPool$Info.writeTo(ThreadPool.java:643)
    at org.elasticsearch.threadpool.ThreadPoolInfo.writeTo(ThreadPoolInfo.java:74)
    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:291)
    at org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse.writeTo(NodesInfoResponse.java:68)
    at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:246)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:241)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.finishHim(TransportNodesOperationAction.java:227)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onOperation(TransportNodesOperationAction.java:202)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$900(TransportNodesOperationAction.java:102)
    at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:146)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:724)

That is the server tries to use OutputStream.writeVLong on a negative number.

Please let me know if you need more information.
</description><key id="34451204">6325</key><summary>Serialization of queue size is broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">maf23</reporter><labels /><created>2014-05-28T09:49:51Z</created><updated>2014-07-16T13:27:15Z</updated><resolved>2014-07-16T13:27:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-06-12T06:46:14Z" id="45835213">I think I found the root cause of this... the `queue_size` variable is serialized as a `SizeValue`, which does not support negative values in `ThreadPool.Info.writeTo()`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Added plugins to .gitignore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6324</link><project id="" key="" /><description>Since plugins should never be committed to the core codebase and it is useful to be able to add plugins to the development environment adding plugins folder to the .gitignore file will stop it from appearing in the unstaged changes
</description><key id="34446583">6324</key><summary>Internal: Added plugins to .gitignore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>build</label><label>non-issue</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-28T08:42:55Z</created><updated>2015-06-07T13:21:36Z</updated><resolved>2014-05-28T08:50:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-28T08:43:11Z" id="44379528">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Update documentation on pull request policy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6323</link><project id="" key="" /><description>After talking to some Elasticsearch folks they mentioned that they now like pull requests to be made up of multiple commits rather then amending the same commit.  Before the pull request is merged it'd be squashed but during review it'd keep growing.

It'd be cool to document this on CONTRIBTUING.md and http://www.elasticsearch.org/contributing-to-elasticsearch/ .

If Elasticsearch grows pull request guidelines then can we add:  always make the subject of your pull request descriptive of what it does rather then what issue it closes.  These subjects turn into email subjects and it is a lot easier to filter what you need to read if the subject is descriptive like "Splort the sort sprocket to make the sort faster" or "Fix overflow in the foo" then "Closes #1234".
</description><key id="34441692">6323</key><summary>[docs] Update documentation on pull request policy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2014-05-28T07:22:15Z</created><updated>2014-06-12T11:07:36Z</updated><resolved>2014-06-12T11:07:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[docs] contributing page doesn't match CONTRIBUTIING.md</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6322</link><project id="" key="" /><description>The [contributing page](http://www.elasticsearch.org/contributing-to-elasticsearch/) doesn't match CONTRIBUTING.md.  I figured the page would be built from the file but I don't believe that is the case?

This is the section that is missing from the page but in the file

```
* Don't worry too much about imports.  Try not to change the order but don't worry about fighting your IDE to stop it from switching from * imports to specific imports or from specific to * imports.
```
</description><key id="34441220">6322</key><summary>[docs] contributing page doesn't match CONTRIBUTIING.md</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>docs</label></labels><created>2014-05-28T07:12:40Z</created><updated>2014-06-03T10:19:02Z</updated><resolved>2014-06-03T10:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T10:19:02Z" id="44946863">Fixed, thanks for pointing this out @nik9000 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix github download link when using specific version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6321</link><project id="" key="" /><description>Removes the extraneous 'v' character which could have been used previously for Github links.

Does not include an updated test case since the PluginManagerTests class skips tests with invalid downloads (try-catch block in singlePluginInstallAndRemove). In addition, the PluginManager logs to System.out, which is removed during tests.
</description><key id="34429145">6321</key><summary>Fix github download link when using specific version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">brusic</reporter><labels><label>:Plugins</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-28T02:01:05Z</created><updated>2015-06-07T19:55:35Z</updated><resolved>2014-07-03T17:54:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-07-03T17:54:01Z" id="47962878">Thanks @brusic 

Actually the `v` was used by elasticsearch when github was the download repo.
I agree that it does not make sense anymore.

Pushed in 1.2, 1.x and master branches.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the default type of the page recycler to CONCURRENT instead of SOFT_CONCURRENT.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6320</link><project id="" key="" /><description>This default type has been inherited from its ancestor, the (non-paged) recycler whose memory
usage was unbounded and required soft references to make sure it could release memory eventually.
On the contrary, the page cache recycler memory usage is bounded so we could remove soft
references in order to remove load on the garbage collector.

Note: the cache type is already randomized in integration tests.
</description><key id="34397511">6320</key><summary>Change the default type of the page recycler to CONCURRENT instead of SOFT_CONCURRENT.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-27T18:05:48Z</created><updated>2015-06-07T13:21:59Z</updated><resolved>2014-05-28T13:25:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-28T12:17:41Z" id="44398672">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmarks: Move classes into sub-packages to reflect namespace.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6319</link><project id="" key="" /><description>Refactoring benchmark.\* package organization to better reflect the
various API operations that can be performed on benchmarks.

This commit moves the 'start', 'status', and 'abort' actions into their
own sub-packages, renames a few methods accordingly, and updates some
variable names to reflect the change.

Commit also standardizes all package naming to 'benchmark' from 'bench'.
</description><key id="34394941">6319</key><summary>Benchmarks: Move classes into sub-packages to reflect namespace.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-27T17:36:31Z</created><updated>2014-09-08T19:55:37Z</updated><resolved>2014-07-15T20:16:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-06T09:27:00Z" id="45318605">I think the commits can go in - yet they still don't fix the problems we have with the benchmark API and we should address them very soon. IMO this is also not a feature though.
</comment><comment author="s1monw" created="2014-07-09T12:14:09Z" id="48461869">moving to 1.4 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search: Search template not replacing parameter after initial failure in parameter substitution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6318</link><project id="" key="" /><description>Steps to reproduce:

1) Restart ES

2) Run this (which returns an error as expected):

```
GET _search/template
{
  "template": {
    "query": { "match_all": {}},
    "size": "{{my_size}}"
  }
}
```

3) Then run this (which still returns an error - not expected):

```
GET _search/template
{
  "template": {
    "query": { "match_all": {}},
    "size": "{{my_size}}"
  },
  "params": {
    "my_size": 1
  }
}
```
</description><key id="34373442">6318</key><summary>Search: Search template not replacing parameter after initial failure in parameter substitution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>bug</label><label>v1.1.3</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-27T13:45:30Z</created><updated>2014-07-16T12:28:51Z</updated><resolved>2014-05-28T11:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-27T16:01:35Z" id="44296417">I can reproduce this... I will look 
</comment><comment author="GaelTadh" created="2014-05-27T16:46:22Z" id="44302348">If you disable the cache this behavior doesn't occur.
Brian

On Tue, May 27, 2014 at 5:02 PM, Simon Willnauer
notifications@github.comwrote:

&gt; I can reproduce this... I will look
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/6318#issuecomment-44296417
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot escape / character in regex queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6317</link><project id="" key="" /><description>In reading documentation, it seems that in regex queries i cannot escape / as it is not classified as a special character.

For example say I have a full_url of http://www.mycompany.com/pic2af45362bcd322cd/image1.jpg, where the 16 char hex can change and the image number can change. 

Logstash parses just the uri portion so i'm searching on the string of '/pic2af45362bcd322cd/image1.jpg'

If i was using pcre i would say something like... | "^\/pic[a-f0-9]{16}\/image[0-9].jpg$"

In reading the documentation on ES/lucene regex (queries are always anchored / not full pcre) i think i should be able to search like so: uri:/\/pic[a-f0-9]{16}\/image[0-9].jpg/

This does not seem to work. If I search for uri:/pic[a-f0-9]{16}/ it works, but i have a much less exact query. I also tried this query in sense and receive the same problem, it's almost as if it does not recognize the forward slashes in regex.
</description><key id="34368173">6317</key><summary>Cannot escape / character in regex queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hadojae</reporter><labels /><created>2014-05-27T12:45:32Z</created><updated>2014-05-28T20:19:13Z</updated><resolved>2014-05-28T20:19:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-28T20:19:13Z" id="44458958">Hi @hadojae 

Please ask questions like this in the forum.  The issues list is for bigs/feature requests.  Just a pointer: your field is probably `analyzed` and should be `not_analyzed`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multiple threads creating and closing connections results in random org.elasticsearch.client.transport.NoNodeAvailableException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6316</link><project id="" key="" /><description>I have code that has worked consistently well on 1.0.0.RC1 of ES but dies often on 1.1.1 ... it basically has a thread pool of workers doing long running scan/scroll queries (with "_shards:N;_local") and each creates its own TransportClient connection at the start of a run, and closes it at the end.  A finally block guarantees the close is run.  

First, after a bunch of connections are used, somewhere along the way a thread picks up new work item and creates a new transport client and uses the client the first time it causes a  org.elasticsearch.client.transport.NoNodeAvailableException when executing the query for the first time.  The cluster health is fine at the time.  No logs, or errors in any cluster node, cluster state shows healthy, no recovery, index is static at the time with no updates.

The same code worked flawlessly with 1.0.0.RC1 and now fails consistently and quickly, but not repeatable enough to see that it is some specific case in the code, more likely some race condition with the closing of a past connection, opening of new, and the timing of the first query.  Adding pauses seems silly, and 1 second after close and 1 second after open do not resolve it anyway.

Second issue that is new, is that I now also get Netty errors java.lang.OutOfMemoryError: Direct buffer memory with direct memory set to 1G and all connection open/close protected by try..finally blocks so nothing should be leaking.

It appears the creation and closing of TransportClients at higher volume does not bode well for ES at the moment.  I create the clients because I am doing large data extracts from different nodes and shards and am keeping nodes that do not need to be involved from participating for each shard, and previously had issues with clients timing out if they were in a pool and unused for a long time.  But regardless of pooling, there shouldn't be a leak or a break (exception) with create/close connections.

I have noticed this newer StackOverFlow issue as well which seems very similar:
http://stackoverflow.com/questions/23739137

And before people comment "use one client and share for all threads" ... I know that the clients can be used in that way, but I am still worried about this bug and resource leak and this issue is about those, not about designing how this code works.
</description><key id="34365445">6316</key><summary>Multiple threads creating and closing connections results in random org.elasticsearch.client.transport.NoNodeAvailableException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apatrida</reporter><labels><label>feedback_needed</label></labels><created>2014-05-27T12:04:49Z</created><updated>2015-01-26T20:49:47Z</updated><resolved>2015-01-26T20:49:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="apatrida" created="2014-05-27T12:17:23Z" id="44267623">Other note, for transport client SniffNodes is set to false
</comment><comment author="clintongormley" created="2014-12-30T19:09:05Z" id="68386537">Hi @jaysonminard 

Sorry for taking so long to get to this.  Is this issue still present on v1.4?  If so, if you could provide some code which reproduces the problem (albeit erratically) it'd be a good start for finding the problem.

thanks
</comment><comment author="opgalarza" created="2015-01-23T15:19:59Z" id="71208052">Hi @clintongormley,

We are currently facing this issue in our environment. We have a cluster of 4 machines. 1 of them functions as a master and the 3 others as data nodes.  In our code, we spawn 10 threads and each one gets a client instance as follows

run method in the thread

```
  @Override
public void run() {
    Client client = ESClient.getClient(clusterName,hostName,port);

    List&lt;String&gt; errors = new ArrayList&lt;String&gt;();
    String indexName = outputInfo.getElasticSearch().getIndexName();
    String typeName = outputInfo.getElasticSearch().getTypeName();

    for(Map&lt;String, Object&gt; recordToPersist: recordsToPersist){ 
        String line = Utils.getJsonFromObject(recordToPersist);
        try{
            ESService.putDocument(client, line, indexName, typeName);
        } catch(Exception e){
            errors.add(line);
        }
    }
    if(!errors.isEmpty()){
        logErrors(errors,outputInfo.getElasticSearch().getErrorLogFile());
    }
}
```

This is the putDocument method implementation :+1: 

```
public static IndexResponse putDocument(Client client, String jsonString, String indexName, String typeName) {  
    IndexResponse indexResponse = client.prepareIndex(indexName, typeName).setSource(jsonString).execute().actionGet();     
    return indexResponse;
}
```

and at some point we get the following exception :+1: 

org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: []
at org.elasticsearch.client.transport.TransportClientNodesService.ensureNodesAreAvailable(TransportClientNodesService.java:278)
at org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:197)
at org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:106)
at org.elasticsearch.client.support.AbstractClient.index(AbstractClient.java:98)
at org.elasticsearch.client.transport.TransportClient.index(TransportClient.java:334)
at org.elasticsearch.action.index.IndexRequestBuilder.doExecute(IndexRequestBuilder.java:313)
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91)
at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65)
at com.manwin.cloud.service.ESService.putDocument(ESService.java:39)
at com.manwin.cloud.elasticsearch.ElasticSearchPersistentWorker.run(ElasticSearchPersistentWorker.java:85)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)

We are using elasticsearch version 1.4.1. Hope this helps. In case you need more information please let me know.

Cheers
Omar
</comment><comment author="clintongormley" created="2015-01-26T18:38:55Z" id="71512711">@opgalarza presumably your nodes are being overwhelmed by the indexing requests - you want to look at the cluster logs, rather than the client logs.
</comment><comment author="opgalarza" created="2015-01-26T18:41:37Z" id="71513183">Ok @clintongormley , I will check that part in the cluster logs and keep you posted.

Cheers
Omar
</comment><comment author="apatrida" created="2015-01-26T19:02:08Z" id="71516744">My code had no indexing other way, was read-only on the index.
</comment><comment author="apatrida" created="2015-01-26T19:03:01Z" id="71516909">and I'm not on that client project so would have to start from scratch to re-create a scenario ... too much time has passed.  
</comment><comment author="clintongormley" created="2015-01-26T20:49:47Z" id="71535365">@jaysonminard yeah - i'm thinking that this issue has been resolved.  @opgalarza 's description sounds like a different problem.  thanks for getting back to us and sorry for the long delay :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Document that translog is striped file by file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6315</link><project id="" key="" /><description>Closes #6307
</description><key id="34332839">6315</key><summary>[docs] Document that translog is striped file by file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2014-05-26T22:59:38Z</created><updated>2014-07-11T14:54:18Z</updated><resolved>2014-07-11T14:54:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-26T23:03:00Z" id="44222769">I figured striping the translog would be hard enough that if Elasticsearch does grow that feature it'll take some time so it'd be better to have this documentation in the mean time.
</comment><comment author="kimchy" created="2014-05-27T09:22:21Z" id="44252771">I think the doc is confusing? on each ES flush we create a new translog, and that will be striped, but a single translog file is not striped. I will ping on #6307, I would find it surprising that this is the bottleneck.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BulkProcessor's close ignores in-flight bulkRequests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6314</link><project id="" key="" /><description>I have observed on a number of occasions that when I have large batch sizes / concurrent requests that I don't always get the same number of documents out of ES that I put in. Looking at:

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java#L204

``` java
    /**
     * Closes the processor. If flushing by time is enabled, then its shutdown. Any remaining bulk actions are flushed.
     */
    public synchronized void close() {
        if (closed) {
            return;
        }
        closed = true;
        if (this.scheduledFuture != null) {
            this.scheduledFuture.cancel(false);
            this.scheduler.shutdown();
        }
        if (bulkRequest.numberOfActions() &gt; 0) {
            execute();
        }
    }
```

What happens if there are concurrent requests in-flight before the `close` call is made? Shouldn't this method block on those finishing? Or is the requirement / expectation that the client call `close`
 on `TransportClient` which will block on the in-flight requests for up to 10 seconds? 

As far as I can tell, there is no reliable way to know it is safe to shutdown the JVM.
</description><key id="34331566">6314</key><summary>BulkProcessor's close ignores in-flight bulkRequests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">btiernay</reporter><labels><label>bug</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-26T22:09:56Z</created><updated>2014-07-17T14:33:09Z</updated><resolved>2014-07-17T14:33:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2014-05-27T10:50:14Z" id="44259999">This may be related but seems to concern error checking: https://github.com/elasticsearch/elasticsearch/issues/4301
</comment><comment author="btiernay" created="2014-05-27T12:55:25Z" id="44271356">This appears to be a symptom of the missing functionality described in #4158. Is there a client side workaround given #5038? 
</comment><comment author="javanna" created="2014-06-13T12:27:31Z" id="46005034">Hi @btiernay, #5038 is going to be fixed soon (see #6495 ), also I think it would be good to add a blocking variant of the `close` method, that tries and wait for all the in-flight bulk requests to be completed #4180 is just a merge away and achieves exactly this ;)

Does this address your concerns?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mac homebrew 1.2.0 "Unsupported major.minor version 51.0"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6313</link><project id="" key="" /><description>After updating elasticsearch with homebrew to 1.2.0 ES doesn't work. Doesn't look like a homebrew error. When switching back to 1.1.1 everything works fine.

```
elasticsearch --config=/usr/local/opt/elasticsearch/config/elasticsearch.yml
Exception in thread "main" java.lang.UnsupportedClassVersionError: org/elasticsearch/bootstrap/Elasticsearch : Unsupported major.minor version 51.0
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClassCond(ClassLoader.java:637)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:621)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
    at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
    at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
```
</description><key id="34330134">6313</key><summary>Mac homebrew 1.2.0 "Unsupported major.minor version 51.0"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AndreiRailean</reporter><labels /><created>2014-05-26T21:25:48Z</created><updated>2014-09-08T16:47:08Z</updated><resolved>2014-05-29T07:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-26T22:27:04Z" id="44221460">Hi @AndreiRailean,
from `1.2` on we are not supporting Java 1.6 anymore. Looks like you just need to upgrade Java on your system. Also I'm not familiar with homebrew, not sure whether the java version requirement can somehow be specified somewhere.
</comment><comment author="AndreiRailean" created="2014-05-29T07:09:23Z" id="44500990">At the time the error happened, I was running Version 7 Update 55.
</comment><comment author="dadoonet" created="2014-05-29T07:42:47Z" id="44502957">I might be wrong, but I think you were using JAVA 1.6 before even if 1.7 was installed on your mac.
I mean that your path or Java home was not set correctly.

Installing a new version of Java actually fixed that.

Closing as you solved it.
</comment><comment author="dadoonet" created="2014-05-29T07:44:43Z" id="44503081">Did you remove your comment where you wrote 

&gt; Upgrading to Version 7 Update 60 fixed the problem
</comment><comment author="AndreiRailean" created="2014-06-04T00:03:23Z" id="45037705">Upgrading to JRE 7.6 didn't do much. Had to get the JDK 7.6. Only with the new JDK could I start 1.2.0
</comment><comment author="AndreiRailean" created="2014-06-04T00:03:56Z" id="45037746">Get JDK 7 update 60 here http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html
</comment><comment author="uzbekjon" created="2014-07-02T13:53:58Z" id="47778412">Rather than installing JDK, it seem that you can simply define `$JAVA_HOME` variable and point it to your installation of Java 7. **How and why I explained in [the other issue comment](https://github.com/Homebrew/homebrew/issues/29610#issuecomment-47778002)**.
</comment><comment author="rianrainey" created="2014-07-17T21:15:14Z" id="49366715">@uzbekjon :+1: 
</comment><comment author="murnieza" created="2014-08-06T11:47:55Z" id="51324198">@uzbekjon  :+1: 
</comment><comment author="macool" created="2014-09-08T16:47:08Z" id="54849994">@uzbekjon :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added pagination support to `top_hits` aggregation by adding `from` option</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6312</link><project id="" key="" /><description>PR for #6299
</description><key id="34316624">6312</key><summary>Added pagination support to `top_hits` aggregation by adding `from` option</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-26T15:57:23Z</created><updated>2017-03-31T10:06:19Z</updated><resolved>2014-05-30T09:50:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-26T16:07:25Z" id="44201178">Looks good in general, but I think we could make the test better by making it a duel?
</comment><comment author="martijnvg" created="2014-05-27T15:07:20Z" id="44288844">@jpountz Updated the PR and made the pagination test a duel test
</comment><comment author="jpountz" created="2014-05-27T18:12:06Z" id="44313839">LGTM
</comment><comment author="martijnvg" created="2014-05-30T09:50:24Z" id="44634143">Closed via: https://github.com/elasticsearch/elasticsearch/commit/aab38fb2e6ae2b3d53d4e22dec5a9ea14eff367b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added syntax for single item specification.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6311</link><project id="" key="" /><description>Users are more likely to search for only one document than for multiple ones.
This commit extends the current syntax with 'id' and 'doc' for single item
search. Additionally, 'ids' and 'docs' now support a single value or a single
object respectively. For consitency with 'like_text', alternative syntax such
as 'like_doc', 'like_docs', 'like_id' and 'like_ids' are also possible.
</description><key id="34315154">6311</key><summary>Added syntax for single item specification.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-26T15:31:14Z</created><updated>2015-06-07T13:22:11Z</updated><resolved>2014-08-23T17:33:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-26T08:38:52Z" id="47200906">I don't think we should really go the `gazillion different ways to say X` way. I'd rather only have one single way to say it. I mean what is wrong with having a `docs` list with only one value?
</comment><comment author="clintongormley" created="2014-07-01T13:26:38Z" id="47655236">++ keep it simple
</comment><comment author="clintongormley" created="2014-08-22T16:58:23Z" id="53089247">Hi @alexksikes 

Where are you on this PR?  Are you redoing it? Have you abandoned it? If so please close
</comment><comment author="alexksikes" created="2014-08-23T17:33:21Z" id="53159704">This will be part of a more advanced item specification syntax. Closing for now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Values of a multi-value fields are compared at the same level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6310</link><project id="" key="" /><description>Previously, More Like This would create a new mlt query for each value of a
multi-value field. This could result in all the values of the field to be
selected, which defeats the purpose of More Like This. Instead, the correct
behavior is to generate only one mlt query for all the values of the field.
This commit provides the correct behavior for More Like This DSL. The fix for
More Like This API will be coming in another commit.
</description><key id="34305715">6310</key><summary>Values of a multi-value fields are compared at the same level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-26T12:57:55Z</created><updated>2015-06-07T13:22:24Z</updated><resolved>2014-06-03T11:47:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-03T09:43:33Z" id="44943890">LGTM except of the static assert that we should add
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No error shown on non-existing cluster setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6309</link><project id="" key="" /><description>For example setting the default number of shards. This throws no exception, even though it is not supported.

CURL Apply change:
curl -XPUT localhost:9200/_cluster/settings -d '{ "persistent" : { "index.number_of_shards" : 2 } }'

Change Response
{"acknowledged":true,"persistent":{},"transient":{}}

CURL List settings
curl -XGET localhost:9200/_cluster/settings

Settings Response
{"persistent":{},"transient":{}}

Mailing list topic: https://groups.google.com/forum/#!topic/elasticsearch/uOXvRid-VoA
</description><key id="34296305">6309</key><summary>No error shown on non-existing cluster setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">RobinUS2</reporter><labels /><created>2014-05-26T09:59:45Z</created><updated>2014-07-04T12:39:04Z</updated><resolved>2014-07-04T12:39:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-04T12:39:04Z" id="48039367">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolation on nested object with geobounding box filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6308</link><project id="" key="" /><description>I have got an issue for few days and I have no idea how can I resolve it. I got no matching while percolating a document when obviously it should match a created percolator. 

Here is how to reproduce this issue:

``` sh
#Create an index with a geo_point mapping as nested object
curl -XPUT 'http://localhost:9200/geonestedindex' -d '
{
   "mappings":{
      "test":{
         "properties":{
            "location":{
               "type":"nested",
               "properties":{
                  "point":{
                     "type":"geo_point"
                  }
               }
            }
         }
      }
   }
}'
```

``` sh
#Create a percolator

curl - XPOST 'http://localhost:9200/geonestedindex/.percolator/1' -d '
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "nested": {
          "filter": {
            "geo_bbox": {
              "point": {
                "top_left": [
                  4.559326171875,
                  45.08127861241874
                ],
                "bottom_right": [
                  5.2130126953125,
                  44.692088041727814
                ]
              }
            }
          },
          "path": "location"
        }
      }
    }
  }
}'
```

``` sh
#Trying to match a document
curl -XPOST 'http://localhost:9200/geonestedindex/test/_percolate' -d '
{
  "doc": {
    "location": {
      "point": "44.933, 4.9"
    }
  }
}'
```
</description><key id="34288861">6308</key><summary>Percolation on nested object with geobounding box filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">razafinr</reporter><labels /><created>2014-05-26T07:55:09Z</created><updated>2014-07-18T10:12:16Z</updated><resolved>2014-07-18T10:12:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-26T09:22:46Z" id="44172057">Hey, this works for me in 1.2.0 - can you test that version and report back? 

Thanks!
</comment><comment author="razafinr" created="2014-05-26T11:47:31Z" id="44181948">Thank you for your answer. 

In fact I tried with elasticsearch 1.2.0 and it worked. But when I added a template for my mapping in my config directory I got no matching again. But i may have found where the mistake comes from : I have several types with a geo_point field. Probably the error comes from it. 

Here is my template:

``` sh
{
    "template": "*",
    "mappings": {
        "test": {
            "properties": {
                "location": {
                    "type": "nested",
                    "properties":{
                        "point":{"type":"geo_point"}
                    }
                }
            }
        },

        "test2":{
            "_parent":{"type":"test"},
            "properties":{
                "point":{"type":"geo_point"}
            }
        }
    }
}
```

I tried to remove the "test2" type from my template and I got the matching again. When I was in elasticsearch 1.1.0 before, I got this template already so the error should be from there also. 
</comment><comment author="spinscale" created="2014-05-26T12:16:09Z" id="44183760">hey,

can you recreate a complete example with curl, that is not working on 1.2.0 or 1.1.2, so it can be fixed (I just want to make sure I am not missing anything, that's why I ask you to do it)? Thanks a lot!
</comment><comment author="razafinr" created="2014-05-26T12:44:02Z" id="44185725">Ok this is a complete curl example not working on elasticsearch 1.2.0:

``` sh
#Create an index with more than one geo_point field in different types

curl -XPUT 'http://localhost:9200/geonestedindex' -d '
{
"mappings": {
        "test": {
            "properties": {
                "location": {
                    "type": "nested",
                    "properties":{
                        "point":{"type":"geo_point"}
                    }
                }
            }
        },

        "test2":{
            "_parent":{"type":"test"},
            "properties":{
                "point":{"type":"geo_point"}
            }
        }
    }
}'
```

``` sh
#Create a percolator

curl - XPOST 'http://localhost:9200/geonestedindex/.percolator/1' -d '
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "nested": {
          "filter": {
            "geo_bbox": {
              "point": {
                "top_left": [
                  4.559326171875,
                  45.08127861241874
                ],
                "bottom_right": [
                  5.2130126953125,
                  44.692088041727814
                ]
              }
            }
          },
          "path": "location"
        }
      }
    }
  }
}'
```

``` sh
#Trying to match a document
curl -XPOST 'http://localhost:9200/geonestedindex/test/_percolate' -d '
{
  "doc": {
    "location": {
      "point": "44.933, 4.9"
    }
  }
}'
```

Without the second type "test2" it is working perfectly. 
</comment><comment author="razafinr" created="2014-05-27T07:10:07Z" id="44242020">I could find a solution if my problem with the help of Martjin van Groningen. The problem came from the fact that I got these two types in my mapping having the same:

``` sh
"properties":{
                "point":{"type":"geo_point"}
            }
```

So when I am creating my percolator, I have to specify the type in the source like this : 

``` sh
#Create a percolator

curl - XPOST 'http://localhost:9200/geonestedindex/.percolator/1' -d '
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "nested": {
          "filter": {
            "geo_bbox": {
              "point": {
                "top_left": [
                  4.559326171875,
                  45.08127861241874
                ],
                "bottom_right": [
                  5.2130126953125,
                  44.692088041727814
                ]
              }
            }
          },
          "path": "location"
        }
      }
    }
  },
  "type": "test" 
}'
```

With this I am succeeding to match the percolator. 

I had no idea where this error could come from because with any other kind of fields my percolator worked perfectly because I didn't have them duplicated in other types. 
</comment><comment author="spinscale" created="2014-07-18T10:12:16Z" id="49415708">Closing, as this is mentioned in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html#search-percolate
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Translog performance with multiple data paths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6307</link><project id="" key="" /><description>If you specify multiple locations as part of `data.path`, eg:
  `/mnt/data1,/mnt/data2,/mnt/data3`
Translogs get created in one of the directories.
Over time, path being used rotates between all of the locations provided.
But at any one time, the log is only stored in one place.

If each data path is a separate disk or array, this can lead to performance problems where you bottleneck on the read/write ability of one disk. With bulk indexing, this can easily grow and cause hot spots.

At a minimum, this behaviour should be documented, and even software RAID-0 can help here.
Ideally, the translog should be spread amongst all data paths at the same time, so as to avoid hammering one location.
</description><key id="34285809">6307</key><summary>Translog performance with multiple data paths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">avleen</reporter><labels><label>discuss</label></labels><created>2014-05-26T06:54:02Z</created><updated>2014-10-24T09:23:14Z</updated><resolved>2014-10-24T09:23:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-27T09:23:21Z" id="44252858">@avleen yea, striping the translog itself is more difficult in ES today. I do wonder if the translog is really the bottleneck in your case though, how did you arrive to this conclusion?
</comment><comment author="avleen" created="2014-05-27T16:12:25Z" id="44297863">Hi Shay,

Fairly easily: while bulk indexing, I ran `atop` on a server.
This showed the disk with the translog regularly 100% busy while other
disks were less busy.
It also showed the IO Service time on the disk increasing significantly.
This only happened while the translog was being written to (checked by
running `watch ls -l &lt;file&gt;` in another window).
On May 27, 2014 5:23 AM, "Shay Banon" notifications@github.com wrote:

&gt; @avleen https://github.com/avleen yea, striping the translog itself is
&gt; more difficult in ES today. I do wonder if the translog is really the
&gt; bottleneck in your case though, how did you arrive to this conclusion?
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/6307#issuecomment-44252858
&gt; .
</comment><comment author="nik9000" created="2014-05-27T16:36:34Z" id="44301154">I can imagine a situation where one disk is much less full then the other, say, after a big merge.  New translog entries would get put on that disk over and over again.  Though, I'm not sure if the translog would cause more trouble then the new segments being written there.
</comment><comment author="avleen" created="2014-05-29T19:35:36Z" id="44574723">The big spikes in i/o definitely correlated with the times the translog was
being written to.
I don't know how many segments were being written out at the same time, nor
their size.
On May 27, 2014 12:36 PM, "Nik Everett" notifications@github.com wrote:

&gt; I can imagine a situation where one disk is much less full then the other,
&gt; say, after a big merge. New translog entries would get put on that disk
&gt; over and over again. Though, I'm not sure if the translog would cause more
&gt; trouble then the new segments being written there.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/6307#issuecomment-44301154
&gt; .
</comment><comment author="JeffreyZZ" created="2014-07-01T04:57:15Z" id="47617029">We noticed a difference between the data nodes of a single path/drive and multiple paths/drives. The single-path data node has a better CPU usage ratio, where the user CPU usage / system CPU usage is high; while the multiple-path data nodes' system CPU usage is very high. 

Not sure if this is because single-path data node has a more optimized management of Lucene index files than multiple-paths ones.  Do we have any performance test results or recommendation around single-path vs. multiple-path?
</comment><comment author="clintongormley" created="2014-10-24T09:23:14Z" id="60363952">Striping the translog, while it might be possible with the multi data path setup, it would introduce the possibility of many new bugs and difficult to debug failures.

After discussion with other devs, we don't think we'll ever try to support this. I think the only way to do this reliably would be to use RAID-0 in this case.  At least worth benchmarking it.  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding Hebrew analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6306</link><project id="" key="" /><description /><key id="34274168">6306</key><summary>Adding Hebrew analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2014-05-25T22:21:09Z</created><updated>2014-06-15T06:43:32Z</updated><resolved>2014-05-27T12:22:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-27T12:19:46Z" id="44267840">Oops, @synhershko I thought you had already signed the CLA, but it appears not.  Please could you do so, otherwise I'll have to remove this commit: http://www.elasticsearch.org/contributor-agreement/

thanks
</comment><comment author="synhershko" created="2014-05-27T12:20:18Z" id="44267892">I actually have
</comment><comment author="clintongormley" created="2014-05-27T12:22:53Z" id="44268101">Sorry, I see you now. thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch does not respond with huge data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6305</link><project id="" key="" /><description>  Hi,

   I am working on centos5 and I run elasticsearch with version 1.0.0 with -Xms808m -Xmx808m -Xss256kparameters. There are 17 index and total 30200583 docs. Each index's docs count between 1000000 and 2000000. I create request query like ( each index have date field );

{
  "query": {
    "bool": {
      "must": [
        {
          "range": {
            "date": {
              "to": "2014-06-01 14:14:00",
              "from": "2014-04-01 00:00:00"
            }
          }
        }
      ],
      "should": [],
      "must_not": [],
      "minimum_number_should_match": 1
    }
  },
  "from": 0,
  "size": "50"
}

It give response;

{
   took: 5903
   timed_out: false
   _shards: {
      total: 17
      successful: 17
      failed: 0
   },
   hits: {
   total: 30200583
...
...
...}

However when I send query on elasticsearch-head tool for last 50 rows like;

{
  ...
  ...
  ...
  "from": 30200533,
  "size": "50"
}
It does not give a response and throw exception like;

ava.lang.OutOfMemoryError: Java heap space
        at org.apache.lucene.store.DataOutput.copyBytes(DataOutput.java:247)
        at org.apache.lucene.store.Directory.copy(Directory.java:186)
        at org.elasticsearch.index.store.Store$StoreDirectory.copy(Store.java:348)
        at org.apache.lucene.store.TrackingDirectoryWrapper.copy(TrackingDirectoryWrapper.java:50)
        at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4596)
        at org.apache.lucene.index.DocumentsWriterPerThread.sealFlushedSegment(DocumentsWriterPerThread.java:535)
        at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:502)
        at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:506)
        at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:616)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:370)
        at org.apache.lucene.index.StandardDirectoryReader.doOpenFromWriter(StandardDirectoryReader.java:285)
        at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:260)
        at org.apache.lucene.index.StandardDirectoryReader.doOpenIfChanged(StandardDirectoryReader.java:250)
        at org.apache.lucene.index.DirectoryReader.openIfChanged(DirectoryReader.java:170)
        at org.apache.lucene.search.XSearcherManager.refreshIfNeeded(XSearcherManager.java:123)
        at org.apache.lucene.search.XSearcherManager.refreshIfNeeded(XSearcherManager.java:59)
        at org.apache.lucene.search.XReferenceManager.doMaybeRefresh(XReferenceManager.java:180)
        at org.apache.lucene.search.XReferenceManager.maybeRefresh(XReferenceManager.java:229)
        at org.elasticsearch.index.engine.internal.InternalEngine.refresh(InternalEngine.java:730)
        at org.elasticsearch.index.shard.service.InternalIndexShard.refresh(InternalIndexShard.java:477)
        at org.elasticsearch.index.shard.service.InternalIndexShard$EngineRefresher$1.run(InternalIndexShard.java:924)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
What is the problem? Is it not enough java heap space or does my query cause this heap space error? 

I asked same question in stackoverflow. Soutions ,which recommended in stackoverflow, are not applicable for me. Anyone can give another solutions for this problem?
</description><key id="34255750">6305</key><summary>Elasticsearch does not respond with huge data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">umttt</reporter><labels /><created>2014-05-25T06:33:18Z</created><updated>2014-05-25T08:39:49Z</updated><resolved>2014-05-25T08:39:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-25T08:39:49Z" id="44125967">Hey! 

You should ask your question on the mailing list. You will get better support there.

Note that asking on a 800mb JVM for 50 hits by shard on 17 shards could lead to memory pressure.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: MapperParsingException when create default mapping with 'include_in_all' nested </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6304</link><project id="" key="" /><description>One noticeable issue found with ES 1.2.0 during our deployment is that it threw exception when created default mappings with &#8216;include_in_all&#8217; nested under it (doesn&#8217;t matter it&#8217;s set to true/false). For example, the following index creation command returns error when against ES 1.2.0 but it works well against ES 1.1.1 

PUT test 
{ 
   "mappings": { 
      "_default_": { 
        "include_in_all": true  
      } 
   } 
} 

Result 
{ 
   "error": "MapperParsingException[mapping [_default_]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [include_in_all : true]]; ", 
   "status": 400 
} 

Is this a regression with ES 1.2.0? 
</description><key id="34221475">6304</key><summary>Mapping: MapperParsingException when create default mapping with 'include_in_all' nested </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JeffreyZZ</reporter><labels><label>bug</label><label>regression</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-24T01:21:23Z</created><updated>2014-08-12T13:43:59Z</updated><resolved>2014-06-02T15:49:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-24T10:52:40Z" id="44083921">HI @JeffreyZZ 

That mapping is incorrect. `include_in_all` needs to be specified under a field, not at the top level.   In previous versions it was just ignored, but in 1.2 it now tells you that it is incorrect.

You're probably looking for:

```
PUT /test 
{
  "mappings": {
    "_default_": {
      "_all": {
        "enabled": false
      }
    }
  }
}
```
</comment><comment author="JeffreyZZ" created="2014-05-27T17:53:28Z" id="44310566">Make sense.  Thanks @clintongormley  !
</comment><comment author="skurfuerst" created="2014-05-28T11:58:57Z" id="44397065">Are you sure? According to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html, it says:

&gt; The root object mapping is an object type mapping that maps the root object (the type itself). On top 
&gt; of all the different mappings that can be set using the object type mapping, it allows for additional, 
&gt; type level mapping definitions.

and in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-object-type.html, it says:

&gt; include_in_all can be set on the object type level. When set, it propagates down to all the inner 
&gt; mappings defined within the object that do no explicitly set it.

Greets, Sebastian
</comment><comment author="clintongormley" created="2014-05-30T11:15:14Z" id="44639947">@skurfuerst 

You're absolutely right - this is a regression.  /cc @brwe 
</comment><comment author="brwe" created="2014-05-30T13:26:58Z" id="44650047">Yes, opened pull request #6353
</comment><comment author="JeffreyZZ" created="2014-05-31T03:19:24Z" id="44716591">@brwe @brwe  For ES 1.2.0, is there any workaround to this issue or any impact of this issue?
</comment><comment author="brwe" created="2014-06-01T17:28:26Z" id="44783323">You could create a dynamic template for the types (example below). "include_in_all" will then not be set in the root object but still be applied to all fields and objects. 

As for the impact, if "include_in_all" is already set in the root type and you upgrade to 1.2, a MapperParsingException will be thrown on startup. The next time that the type mapping is updated, the "include_at_all" setting will be removed. 

Unfortunately, I found that a side effect seems to be that it will not only be removed from the root object, but also from fields. I'll have to look into this further.

"include_in_all" via "dynamic_template":

```
PUT test
{
   "mappings": {
      "_default_": {
         "dynamic_templates": [
            {
               "include_all": {
                  "match": "*",
                  "mapping": {
                     "include_in_all": "true"
                  }
               }
            }
         ]
      }
   }
}
POST test/cat/1
{
    "text": "text"
}
GET test/_mapping

```

Result should be

```
{
   "test": {
      "mappings": {
         "_default_": {
            "dynamic_templates": [
               {
                  "include_all": {
                     "mapping": {
                        "include_in_all": "true"
                     },
                     "match": "*"
                  }
               }
            ],
            "properties": {}
         },
         "cat": {
            "dynamic_templates": [
               {
                  "include_all": {
                     "mapping": {
                        "include_in_all": "true"
                     },
                     "match": "*"
                  }
               }
            ],
            "properties": {
               "text": {
                  "type": "string",
                  "include_in_all": true
               }
            }
         }
      }
   }
}
```
</comment><comment author="JeffreyZZ" created="2014-06-02T07:00:48Z" id="44806986">@brwe @clintongormley 

Seems this regression also has impact on the index template if the template defines 'include_in_all' under the type. It allows you to create the template but you're NOT able to create the index of the template in ES 1.2.0.  There is the repro steps : 

[Step 1]  create the following index template against ES 1.2.0 cluster
PUT /_template/template_logs
{
   "order": 0,
   "template": "logs*",
   "settings": {
      "index.analysis.analyzer.keyword_analyzer.tokenizer": "keyword"
   },
   "mappings": {
      "LogMessage": {
         "include_in_all": false,
         "properties": {
            "activityId": {
               "analyzer": "keyword_analyzer",
               "type": "string"
            }
         }
      }
   }
}

[Step 2] Try to index this the following document 
PUT /logs-monday/LogMessage/1
{
   "activityId": "fcfdab46-730a-4918-b5bd-1da9448608b1"
}

[Result]
{
   "error": "RemoteTransportException[[ES-TEST-2-master][inet[/10.1.0.38:9300]][indices/create]]; nested: MapperParsingException[mapping [LogMessage]]; nested: MapperParsingException[Root type mapping not empty after parsing! Remaining fields: [include_in_all : false]]; ",
   "status": 400
}

[Expected] 
Index 'logs-monday' is created and 1 document is indexed. 

Try the above steps against ES 1.1.1, it works.  Any workaround?
</comment><comment author="brwe" created="2014-06-02T15:45:13Z" id="44854061">There is unfortunately no workaround. 
</comment><comment author="JeffreyZZ" created="2014-06-03T18:59:23Z" id="45006079">@brwe  Thanks for reply! BTW, any ETA for ES 1.2.1 release?
</comment><comment author="brwe" created="2014-06-04T06:07:17Z" id="45054542">We released yesterday: http://www.elasticsearch.org/blog/elasticsearch-1-2-1-released/
Sorry for the late reply.
</comment><comment author="JeffreyZZ" created="2014-06-04T16:25:05Z" id="45113345">@brwe  Excellent, thank you!
</comment><comment author="clintongormley" created="2014-06-06T10:12:56Z" id="45322027">Your mapping is invalid.  I'm assuming the intent of the above is to create the type `pseudo_doc` in any new index, which has the field `content`?

In this case, the `config/mappings/_default/pseudo_doc.json` file should look like this:

```
{
  "properties": {
    "content": {
      "dynamic": false,
      "properties": {
        "author_id": {
          "type": "string"
        },
        "author_name": {
          "type": "string"
        },
        "content": {
          "type": "string"
        },
        "title": {
          "type": "string"
        },
        "urls": {
          "type": "string"
        },
        "destination_url": {
          "type": "string"
        },
        "publisher": {
          "type": "string"
        }
      }
    }
  }
}
```

Also, I really recommend doing something like this with index templates, rather than with config files. It is much easier to manage such things via the API instead of via static config files.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add short_circuit option to limit filters, triggering a shard search failure on overly broad searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6303</link><project id="" key="" /><description>Proposing this to get a discussion started on how to identify when a limit filter is dropping potentially valid matching documents.

The case I'm attempting to tackle occurs when a user makes a broad search with high computational complexity, say a script filter.  What I need is a mechanism to be able to terminate searches on shards after exceeding a certain number of filtered documents.  A better timeout mechanism would also help to alleviate this problem.

Limit filters are currently the best means to restrict expensive searches, but unfortunately there is no context returned to identify when they are tripped.  This makes it impossible to know when to inform a user that their query was too broad vs. trust that there are in fact no matches.

I see throwing an unchecked exception as a massive kludge, but I don't see a much better option to handle this sort of situation.  Any feedback/recommendations welcome.
</description><key id="34204087">6303</key><summary>Add short_circuit option to limit filters, triggering a shard search failure on overly broad searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EricMCornelius</reporter><labels><label>feedback_needed</label></labels><created>2014-05-23T19:30:05Z</created><updated>2014-09-06T15:29:59Z</updated><resolved>2014-09-06T15:29:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jplock" created="2014-07-14T20:21:59Z" id="48954262">@kimchy is this something that you may consider adding?  It preserves backward compatibility and would be very useful.
</comment><comment author="clintongormley" created="2014-08-22T08:38:46Z" id="53036521">Hi @EricMCornelius 

I think this will be better handled with the new timeout mechanism that we're working on: https://github.com/elasticsearch/elasticsearch/pull/4586

would you agree?
</comment><comment author="EricMCornelius" created="2014-09-02T19:18:04Z" id="54203192">Seems like a reasonable approach.  Any idea when #4586 will land?
</comment><comment author="clintongormley" created="2014-09-06T15:29:59Z" id="54715437">@EricMCornelius It's still in the development phase - we need to be sure that it doesn't have a serious performance impact.

Closing in favour of #4586 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Keep track of when DNS name resolution fails.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6302</link><project id="" key="" /><description /><key id="34203888">6302</key><summary>Keep track of when DNS name resolution fails.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">justfalter</reporter><labels /><created>2014-05-23T19:26:55Z</created><updated>2014-05-23T19:27:30Z</updated><resolved>2014-05-23T19:27:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="justfalter" created="2014-05-23T19:27:30Z" id="44050949">Whoops! wrong place
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Routing + filter aggregation + extended_bounds = ReduceSearchPhaseException (503)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6301</link><project id="" key="" /><description>#### Description

I'm using a date histogram aggregation (with the extended bounds option) nested inside of a filter aggregation along with routing. Here are the combinations that work:
- `!routing, filter, extended_bounds`
- `routing, !filter, extended_bounds`
- `routing, filter, !extended_bounds`

Combining all three results in the following response from elasticsearch:

```
{"error":"ReduceSearchPhaseException[Failed to execute phase [merge], [reduce] ]; nested: UnsupportedOperationException; ","status":503}
```

and the following trace:

```
[2014-05-23 10:32:16,017][DEBUG][action.search.type       ] [Jumbo Carnation] failed to reduce search
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [merge], [reduce] 
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryAndFetchAction.java:79)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:404)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:198)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:174)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:171)
    at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:526)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.UnsupportedOperationException
    at java.util.Collections$EmptyListIterator.add(Collections.java:3059)
    at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.reduce(InternalHistogram.java:290)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:160)
    at org.elasticsearch.search.aggregations.bucket.InternalSingleBucketAggregation.reduce(InternalSingleBucketAggregation.java:69)
    at org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:146)
    at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:545)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryAndFetchAction.java:89)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryAndFetchAction.java:77)
    ... 8 more
```
#### Environment

Elasticsearch versions tested:
- `Version: 1.1.1, Build: f1585f0/2014-04-16T14:27:12Z, JVM: 1.7.0_45`
- `Version: 1.2.0, Build: c82387f/2014-05-22T12:49:13Z, JVM: 1.7.0_45`

Operating system:

```
ProductName:    Mac OS X
ProductVersion: 10.9.2
BuildVersion:   13C64
```
#### Steps to reproduce

Create index:

```
curl -XPUT localhost:9200/foo -d '
{
  "mappings": {
    "charges": {
      "_routing": {
        "require": true,
        "path": "account_id"
      },
      "properties": {
        "account_id": {
          "type": "integer"
        },
        "amount": {
          "type": "integer"
        },
        "created_at": {
          "type": "date"
        }
      }
    }
  }
}'
```

```
{"acknowledged":true}
```

Query:

```
curl -XGET localhost:9200/foo/charges/_search?routing=1 -d '
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "account_id": 1
              }
            }
          ]
        }
      }
    }
  },
  "aggs": {
    "charges": {
      "filter": {
        "bool": {
          "must": [
            {
              "term": {
                "_type": "charges"
              }
            }
          ]
        }
      },
      "aggs": {
        "monthly_stats": {
          "date_histogram": {
            "field": "created_at",
            "interval": "month",
            "min_doc_count": 0,
            "extended_bounds": {
              "min": "2014-01-01",
              "max": "2014-12-31"
            }
          },
          "aggs": {
            "revenue": {
              "sum": {
                "field": "amount"
              }
            }
          }
        }
      }
    }
  }
}'
```

```
{"error":"ReduceSearchPhaseException[Failed to execute phase [merge], [reduce] ]; nested: UnsupportedOperationException; ","status":503}
```
</description><key id="34196139">6301</key><summary>Routing + filter aggregation + extended_bounds = ReduceSearchPhaseException (503)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lafave</reporter><labels><label>feedback_needed</label></labels><created>2014-05-23T17:41:06Z</created><updated>2014-07-24T14:31:38Z</updated><resolved>2014-07-24T14:31:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="skratchdot" created="2014-05-28T21:25:13Z" id="44466588">I'd love to see this fixed as well (or at least understand the problem).  I'm running into a similar issue when using a nested aggregation.  For instance, here's a stripped down example query (and the resulting error):

``` json
{
    "query": {
        "term": {
            "foo": "bar"
        }
    },
    "aggs": {
        "types": {
            "terms": {
                "field": "_type"
            },
            "aggs": {
                "time_last_hour": {
                    "filter": {
                        "range": {
                            "date": {
                                "from": "now-1H/H"
                            }
                        }
                    },
                    "aggs": {
                        "graph": {
                            "date_histogram": {
                                "field": "date",
                                "interval": "1m",
                                "min_doc_count": 0,
                                "extended_bounds": {
                                    "min": "now-1H",
                                    "max": "now"
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
```

error:

``` json
{
    error: ReduceSearchPhaseException[Failed to execute phase[query], [reduce]];
    nested: UnsupportedOperationException;
    status: 503
}
```

If I remove the "min" or "max" from the "extended_bounds", I don't get an error.

Example Response:

``` json
{
    "took": 5,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 10582,
        "max_score": 0.0,
        "hits": []
    },
    "aggregations": {
        "types": {
            "buckets": [{
                "key": "foo",
                "doc_count": 5318,
                "time_last_hour": {
                    "doc_count": 0,
                    "graph": {
                        "buckets": []
                    }
                }
            }, {
                "key": "bar",
                "doc_count": 5263,
                "time_last_hour": {
                    "doc_count": 0,
                    "graph": {
                        "buckets": []
                    }
                }
            }, {
                "key": "baz",
                "doc_count": 1,
                "time_last_hour": {
                    "doc_count": 0,
                    "graph": {
                        "buckets": []
                    }
                }
            }]
        }
    }
}
```

Even if there are no results in my filter from the last hour, I would like the histogram to be populated with 60 minutes worth of "doc_count=0" data.

_Sorry if this is not the correct place for my comment.  Elasticsearch might be working as intended- and I just don't understand how date_histogram is supposed to work with filters and extended_bounds_
</comment><comment author="clintongormley" created="2014-07-03T19:52:07Z" id="47976642">@uboness this appears to have been fixed? 
</comment><comment author="jbrook" created="2014-07-07T17:47:26Z" id="48212236">We are running into this bug with 1.2.1. We are not using routing - just nested aggregations with extended_bounds. Is this fixed on master?
</comment><comment author="clintongormley" created="2014-07-07T17:49:01Z" id="48212447">@jbrook can you upload a simple recreation please?
</comment><comment author="clintongormley" created="2014-07-07T17:49:44Z" id="48212559">@jpountz do you know what the status of this ticket is?
</comment><comment author="jpountz" created="2014-07-24T14:30:53Z" id="50025525">@jbrook @clintongormley I expect this too have been fixed via https://github.com/elasticsearch/elasticsearch/pull/6484
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix MatchQueryParser not parsing fuzzy_transpositions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6300</link><project id="" key="" /><description /><key id="34185352">6300</key><summary>Fix MatchQueryParser not parsing fuzzy_transpositions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">alexbrasetvik</reporter><labels><label>:Query DSL</label><label>bug</label><label>v1.1.3</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T15:25:56Z</created><updated>2015-06-07T19:58:18Z</updated><resolved>2014-05-23T20:06:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-23T20:05:55Z" id="44054575">good catch thanks!
</comment><comment author="s1monw" created="2014-05-23T20:06:40Z" id="44054657">pushed
</comment><comment author="alexbrasetvik" created="2014-05-23T20:13:05Z" id="44055262">Thanks a lot! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `from` support to top_hits aggregator.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6299</link><project id="" key="" /><description /><key id="34184556">6299</key><summary>Add `from` support to top_hits aggregator.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Aggregations</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T15:17:27Z</created><updated>2017-03-31T10:06:19Z</updated><resolved>2014-05-30T09:46:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-23T15:22:02Z" id="44024931">+1
</comment><comment author="Kumen" created="2014-05-26T06:41:43Z" id="44161325">pagination for top_hits aggregator, definitely +1
</comment><comment author="artemredkin" created="2014-05-27T08:03:15Z" id="44245801">Judging by commit a989229bcd70888b3cb9e98ab58fb10ef47f62bc, it seem that you added pagination for hits, but what about groups (buckets) themselves? For example, if i group books by book author and search query returns 9 unique authors, can I show first page with 5 authors and next page with other 4 authors?
</comment><comment author="Kumen" created="2014-05-27T08:12:58Z" id="44246568">already tested, your assumption is correct. we need pagination support for buckets.
</comment><comment author="jpountz" created="2014-05-27T09:33:22Z" id="44253718">Paging is tricky. We might be able to expose it when sorting by term (would it work for you?), but if you are sorting by counts or by sub aggregation, then  https://github.com/elasticsearch/elasticsearch/issues/1305 would make counts wrong and ordering inconsistent across pages.
</comment><comment author="Kumen" created="2014-05-27T09:41:09Z" id="44254380">yes, that would work for me. please expose this feature.
</comment><comment author="artemredkin" created="2014-05-27T09:41:58Z" id="44254442">I need at least sort by term and sort by docs count. Is approximate count/paging possible?
</comment><comment author="jpountz" created="2014-05-27T09:54:02Z" id="44255515">I'm a bit reluctant to add paging support when sorting by counts given that it would give more accurate results on the 1st result of the 2nd page than on the last one of the 1st page. Please also note that the way it would work behind the scenes would not be different from what you could do on client side by first requesting 10 buckets for the 1st page and then 20 for the 2nd page and disregard the first 10 buckets, etc. for subsequent pages.
</comment><comment author="artemredkin" created="2014-05-27T10:00:34Z" id="44256104">Yes, it can be implemented on client (in my system it already is, but it's not very efficient). If there is only one way to sort/page buckets, then clients can either decide in runtime which implementation to use (which requires a lot of code and conditions) or implement grouping themselves altogether. Maybe I can help with some experiments to see if it can be done at all?
</comment><comment author="jpountz" created="2014-05-27T10:10:38Z" id="44257044">I think this feature would be easy to implement, what I'm more concerned about here is to expose a feature that would be error-prone. :(
</comment><comment author="Kumen" created="2014-05-27T10:14:36Z" id="44257369">In our system it is already implemented. But we have memory issues on requesting all possible buckets, therefore we need a way to effectively navigate through the buckets on server side. i thought it would help to get the buckets paged from the server.

edit: the memory issues are on the elasticsearch server not on the client side
</comment><comment author="artemredkin" created="2014-05-27T10:59:17Z" id="44260682">You already provide at least one feature, that can be inaccurate - cardinality aggregations. Also, from documentation about terms aggregation:
The higher the requested size is, the more accurate the results will be, but also, the more expensive it will be to compute the final results (both due to bigger priority queues that are managed on a shard level and due to bigger data transfers between the nodes and the client).
So you also have an error-prone feature as well (clients can set size to 0 on high cardinality field and shoot themselves in the foot).
Another consideration, even in case of simple solution you proposed (dropping n-1 pages on elasticsearch side) can be advantageous, since we can run elasticsearch on more powerful machines, then our backends.
</comment><comment author="jpountz" created="2014-05-27T18:27:42Z" id="44315847">This feature already has accuracy issues indeed, but in my opinion paging will make it even worse. For example, let's imagine that your top terms are term1, term2, ..., term10. If your page size is 5, it could happen that Elasticsearch returns term1, term2, term3, term4 and term6 on the first page (6 instead of 5 because of inaccuracy), and then term6, term7, term8, term9 and term10 (as expected). So you would have one term that would be completely invisible to your users (term5) and another one that would appear twice (term6). I think this is too confusing.
</comment><comment author="artemredkin" created="2014-05-27T18:36:17Z" id="44316971">Too bad, without counts sort this feature is only half useful. Can this issue be re-evaluated as a separate task, connected to the issue #256?
</comment><comment author="artemredkin" created="2014-05-28T05:59:50Z" id="44368600">And how about this: you provide java interface for sorter, and through plugin, we can add our own sorters for possibly-innacurate results?
</comment><comment author="Kumen" created="2014-05-28T06:23:50Z" id="44369832">in my concrete problem. i have million of documents that i want to aggregate by key. the elasticsearch fails at this point. in the worst possible case there are over 200.000 buckets and per bucket about 10 matched documents. i thought a effective way to minimize the memory consumption is it to page through the buckets on server side. is there any other solution except to enlarge the ram capacity ?
</comment><comment author="artemredkin" created="2014-05-28T07:48:26Z" id="44375232">Also, have you seen SolrCloud's implementation of facets? Something like: http://mail-archives.apache.org/mod_mbox/lucene-solr-user/201209.mbox/%3Calpine.DEB.2.02.1209261450570.2316@frisbee%3E
</comment><comment author="kimchy" created="2014-05-28T09:14:25Z" id="44382235">@artemredkin there is a difference between returning exact counts for specific terms, and guaranteeing total ordering. The first can be done with another round after picking the top N, the second can not.
</comment><comment author="jpountz" created="2014-05-28T09:18:57Z" id="44382617">@Kumen memory usage currently depends mostly on your number of buckets, not the size of the page that is requested. If you are not running Elasticsearch 1.2 yet, I would recommend on upgrading as memory usage of the terms aggregation improved significantly in this release.
</comment><comment author="Kumen" created="2014-05-28T09:29:16Z" id="44383517">I am currently using the version 1.3 (manual build from branch).
Thus i need more memory. 

thanks
</comment><comment author="artemredkin" created="2014-05-28T09:34:57Z" id="44383997">@kimchy I may be horribly wrong here (will do more digging today), but solr's field collapsing works in distributed environment and provides paging/ordering (at least i do not see in their documentation any indication, that it is not supported). Plus, @jpountz pointed to #1305 as a source for ordering problem.
In your opinion, can this problem (ordering of groups by count) be solved at all (leveraging cardinatlity agg, for example)? Maybe in later releases?
</comment><comment author="kimchy" created="2014-05-28T09:42:17Z" id="44384664">@artemredkin guaranteed total ordering can't be solved (with a 2 way execution) unless all the values are streamed, so by definition, pagination will not be "exact", that is the problem. You can say that for the top N (or paginated N), the count for each term will be exact by executing another round, but not the total order of them.

It is an interesting problem, specifically with the fact that we would love to solve it in a somewhat performant manner. We obviously would love to solve it, if we manage to come up with a way to do it that is :)
</comment><comment author="artemredkin" created="2014-05-28T10:32:42Z" id="44388782">@kimchy Aren't top N groups exactly what we need for pagination? I was going to implement in on client in 2 hops (it would be 3-way execution, yes?). On first  - get (1.5_size of group page) worth of terms, maybe with cardinality, sorting them, and on second hop - get those terms with top_hits. For second page - get (3_size of group page) worth of terms and so on. Is it wrong :) ? Or just slow for inclusion inside elasticsearch itself?
Anyway, thanks for explaining things, it would be awesome, it you solve this :).
</comment><comment author="martijnvg" created="2014-05-30T10:56:57Z" id="44638764">I didn't mean to stop this discussion by closing this issue... 

Adding pagination in the `terms` aggregation is tricky like @kimchy and @jpountz describe and the correctness would depend on the ordering. The correctness of the ordering depends to what `order` the `terms` aggregation is set to. If the `terms` aggregation's `order` is set to `_term` or to specific inner metric aggregations (`min` or `max` metric bucket, but not `avg` metric bucket), the ordering is correct.  

The 'result grouping' approach in ES relies on the terms aggregation to determine the correct groups and an inner `max` aggregation for ordering of the groups:
http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-top-hits-aggregation.html#_field_collapse_example

Pagination can be simulated by using terms aggregation's `exclude` option. On subsequent search requests the previous emitted term buckets should be added to the exclude option, this way previous seen groups don't end up in the next aggregation response.
</comment><comment author="artemredkin" created="2014-05-30T11:01:36Z" id="44639052">Hm,

&gt; or to specific inner metric aggregations
&gt; does this mean, that I can use 'cardinality' sub-agg to sort term groups?
</comment><comment author="martijnvg" created="2014-05-30T11:57:38Z" id="44643023">&gt; does this mean, that I can use 'cardinality' sub-agg to sort term groups?

Yes, you can sort by a `cardinality` inner metric aggregation, but the ordering of the buckets depend on the accuracy of the `cardinality` aggregation.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse Lucene's TermsEnum for faster _uid/version lookup during indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6298</link><project id="" key="" /><description>The TermsEnums used for lookup have highish cost to init, so if we
reuse them we may be able to stop using bloom filters.  I ran some bulk
update performance tests, showing that turning off blooms and reusing
the enums gets close to the same performance as master (using blooms
and not reusing the enums).

Closes #6212
</description><key id="34184215">6298</key><summary>Reuse Lucene's TermsEnum for faster _uid/version lookup during indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T15:13:43Z</created><updated>2015-06-07T13:24:57Z</updated><resolved>2014-05-31T21:44:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-05-28T15:30:35Z" id="44422255">![bulkindexing](https://cloud.githubusercontent.com/assets/796508/3106475/244f4716-e67c-11e3-938d-d7fc4c5c59a1.png)

I ran a bulk indexing perf test comparing master vs this patch, updating 10M small log-entry type docs ("index" command, passing _id), with random UUIDs (%10d, worst case for terms dict), using MMapDir, and the results look promising: reusing the TermsEnum gets back much of the performance that bloom filters buy us today.

However, this is a small test (10M docs), the index was fully hot...
</comment><comment author="mikemccand" created="2014-05-30T17:19:33Z" id="44676070">OK I went back to searching segments in reverse order, and downgraded the other nocommits to TODOs.  I think this is ready; I'll commit soon.
</comment><comment author="mikemccand" created="2014-05-31T21:44:09Z" id="44760516">Committed with #6212 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_cat/nodes causes NullPointerException in 1.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6297</link><project id="" key="" /><description>curl -s 'http://localhost:9200/_cat/nodes'
{"error":"NullPointerException[null]","status":500}

Nothing else is logged on the masters or data nodes.
We have Logstash 1.4.1 connected to the cluster, but this happens regardless of whether the Logstash nodes are connected or not.
</description><key id="34181481">6297</key><summary>_cat/nodes causes NullPointerException in 1.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">avleen</reporter><labels><label>bug</label></labels><created>2014-05-23T14:46:24Z</created><updated>2014-11-24T11:29:58Z</updated><resolved>2014-09-27T05:44:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-05-23T19:26:10Z" id="44050824">I can't repro locally. Can you post `/_cluster/state?filter_metadata` and `/_nodes?all` somewhere?
</comment><comment author="s1monw" created="2014-05-23T19:59:38Z" id="44053973">or do you see a stacktrace in the logs somewhere?
</comment><comment author="avleen" created="2014-05-23T22:08:04Z" id="44065702">Hi @drewr ew, I hope you don't mind, I emailed them to you as they contain data I'd prefer not to post publicly :)

@s1monw no stack traces from this in any logs on any ES node.
</comment><comment author="drewr" created="2014-05-23T22:32:14Z" id="44067235">@avleen I don't mind at all. However, I haven't seen anything yet. Where did you send it?  You can use first.last@elasticsearch.

Update: Nevermind, spam. :smiling_imp: 
</comment><comment author="avleen" created="2014-05-23T23:32:24Z" id="44070266">Hi Drew,

Just double checked.. Yup that's where I sent it, at 17:29 ET.
Has two attachments, a zip file and a text file.
On May 23, 2014 6:32 PM, "Drew Raines" notifications@github.com wrote:

&gt; @avleen https://github.com/avleen I don't mind at all. However, I
&gt; haven't seen anything yet. You can use first.last@elasticsearch.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/6297#issuecomment-44067235
&gt; .
</comment><comment author="mcleancraig" created="2014-06-04T10:51:22Z" id="45076545">FWIW I'm also seeing this, on a freshly-updated 1.2.1 cluster (from 1.1.1) with 64-bit Java 1.7.0_55, CentOS 5.8.
</comment><comment author="martijnvg" created="2014-06-20T20:26:23Z" id="46722824">This PR addresses should fix this NullPointerException:
https://github.com/elasticsearch/elasticsearch/pull/6190

The fix will be included in 1.2.2
</comment><comment author="mcnewton" created="2014-07-11T23:06:20Z" id="48792950">Hi,
For info - just installed ES for the first time a week ago (1.2.1 debian wheezy 64bit), and hit this issue. Upgraded to 1.2.2 yesterday, and it still reports the NullPointerException when requesting _cat/nodes. Installed using the debian packages. Also get on a newly installed 1.2.2 node (though joined to the same cluster).
Cheers,
Matthew
</comment><comment author="dadoonet" created="2014-07-12T09:05:02Z" id="48806366">Any stack trace in nodes logs?
</comment><comment author="mcnewton" created="2014-07-12T14:26:31Z" id="48813716">No, nothing in node logs. Is there any way to increase debugging, maybe? I wondered if the package hadn't upgraded correctly, but it's all reporting 1.2.2 (logs say version[1.2.2], pid[10140], build[9902f08/2014-07-09T12:02:32Z]).
</comment><comment author="faxm0dem" created="2014-07-25T06:10:41Z" id="50111428">I can confirm this bug is still present in 1.3.0, even after upgrading all nodes:

```
$ curl 0:9200
```

``` json
{
  "status" : 200,
  "name" : "mycluster",
  "version" : {
    "number" : "1.3.0",
    "build_hash" : "1265b1454eee7725a6918f57415c480028700fb4",
    "build_timestamp" : "2014-07-23T13:46:36Z",
    "build_snapshot" : false,
    "lucene_version" : "4.9"
  },
  "tagline" : "You Know, for Search"
}
```

```
$ curl 0:9200/_cat/nodes
```

``` json
{"error":"NullPointerException[null]","status":500}
```

Nothing in the logfile.
</comment><comment author="clintongormley" created="2014-07-25T06:19:34Z" id="50111894">@faxm0dem do you have multiple nodes running? if so did you look in the logs for all the nodes? 

could you send the output of:

```
curl 0:9200/_nodes
curl 0:9200/_nodes/stats
```

thanks
</comment><comment author="faxm0dem" created="2014-07-25T08:49:22Z" id="50123575">Yes, two nodes, nothing on either side.
Here's the output:

```
curl 0:9200/_nodes
```

``` json
{
  "cluster_name" : "telecom",
  "nodes" : {
    "JUOZAr-mT6e8e9nmwWH9ww" : {
      "name" : "node07-telecom",
      "transport_address" : "inet[/10.0.104.214:9300]",
      "host" : "node07",
      "ip" : "10.0.104.214",
      "version" : "1.3.0",
      "build" : "1265b14",
      "http_address" : "inet[/10.0.104.214:9200]",
      "settings" : {
        "node" : {
          "name" : "node07-telecom"
        },
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "8"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "name" : "node07-telecom",
        "pidfile" : "/var/run/elasticsearch/elasticsearch-telecom.pid",
        "path" : {
          "data" : "/var/lib/elasticsearch/telecom",
          "work" : "/tmp/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch/telecom",
          "logs" : "/var/log/elasticsearch/telecom"
        },
        "cluster" : {
          "name" : "telecom"
        },
        "indices" : {
          "memory" : {
            "index_buffer_size" : "30%"
          }
        },
        "discovery" : {
          "zen" : {
            "minimum_master_nodes" : "1",
            "ping" : {
              "unicast" : {
                "hosts" : [ "node07", "node38" ]
              },
              "multicast" : {
                "enabled" : "false"
              },
              "timeout" : "30s"
            }
          }
        }
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 16,
        "cpu" : {
          "vendor" : "Intel",
          "model" : "Xeon",
          "mhz" : 2527,
          "total_cores" : 16,
          "total_sockets" : 1,
          "cores_per_socket" : 16,
          "cache_size_in_bytes" : 8192
        },
        "mem" : {
          "total_in_bytes" : 25185079296
        },
        "swap" : {
          "total_in_bytes" : 279650304
        }
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 27129,
        "max_file_descriptors" : 65535,
        "mlockall" : false
      },
      "jvm" : {
        "pid" : 27129,
        "version" : "1.7.0_65",
        "vm_name" : "OpenJDK 64-Bit Server VM",
        "vm_version" : "24.65-b04",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1406268341800,
        "mem" : {
          "heap_init_in_bytes" : 17179869184,
          "heap_max_in_bytes" : 17066491904,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 224395264,
          "direct_max_in_bytes" : 17066491904
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space", "CMS Old Gen", "CMS Perm Gen" ]
      },
      "thread_pool" : {
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 16,
          "max" : 16,
          "queue_size" : "200"
        },
        "snapshot_data" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "bench" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "get" : {
          "type" : "fixed",
          "min" : 16,
          "max" : 16,
          "queue_size" : "1k"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 16,
          "max" : 16,
          "queue_size" : "1k"
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 16,
          "max" : 16,
          "queue_size" : "50"
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 48,
          "max" : 48,
          "queue_size" : "1k"
        },
        "percolate" : {
          "type" : "fixed",
          "min" : 16,
          "max" : 16,
          "queue_size" : "1k"
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 8,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "10.0.104.214",
          "name" : "eth0",
          "mac_address" : ""
        }
      },
      "transport" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0%0:9300]",
        "publish_address" : "inet[/10.0.104.214:9300]"
      },
      "http" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0%0:9200]",
        "publish_address" : "inet[/10.0.104.214:9200]",
        "max_content_length_in_bytes" : 104857600
      },
      "plugins" : [ ]
    },
    "JL38-jS9Sn67hQk2XxNZxw" : {
      "name" : "logstash-netflow",
      "transport_address" : "inet[/10.0.108.171:9303]",
      "host" : "node38",
      "ip" : "10.0.108.171",
      "version" : "1.1.1",
      "build" : "f1585f0",
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "settings" : {
        "path" : {
          "logs" : "/home/sysunix/logs"
        },
        "cluster" : {
          "name" : "telecom"
        },
        "node" : {
          "client" : "true",
          "name" : "logstash-netflow"
        },
        "discovery" : {
          "zen" : {
            "ping" : {
              "unicast" : {
                "hosts" : "node38:9300,node38:9301,node38:9302,node38:9303,node38:9304,node38:9305"
              },
              "multicast" : {
                "enabled" : "false"
              }
            }
          }
        },
        "http" : {
          "enabled" : "false"
        },
        "name" : "logstash-netflow"
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 32
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 8365,
        "max_file_descriptors" : 4096,
        "mlockall" : false
      },
      "jvm" : {
        "pid" : 8365,
        "version" : "1.7.0_55",
        "vm_name" : "OpenJDK 64-Bit Server VM",
        "vm_version" : "24.51-b03",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1402068863618,
        "mem" : {
          "heap_init_in_bytes" : 524288000,
          "heap_max_in_bytes" : 506855424,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 224395264,
          "direct_max_in_bytes" : 506855424
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space", "CMS Old Gen", "CMS Perm Gen" ]
      },
      "thread_pool" : {
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "200"
        },
        "get" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "50"
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 96,
          "max" : 96,
          "queue_size" : "1k"
        },
        "percolate" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 10,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "",
          "name" : "",
          "mac_address" : ""
        }
      },
      "transport" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0%0:9303]",
        "publish_address" : "inet[/10.0.108.171:9303]"
      },
      "plugins" : [ ]
    },
    "rPZ9EsahRl-9Cs_AOucOJQ" : {
      "name" : "logstash-netflow",
      "transport_address" : "inet[/10.0.108.171:9302]",
      "host" : "node38",
      "ip" : "10.0.108.171",
      "version" : "1.1.1",
      "build" : "f1585f0",
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "settings" : {
        "path" : {
          "logs" : "/home/sysunix/logs"
        },
        "cluster" : {
          "name" : "telecom"
        },
        "node" : {
          "client" : "true",
          "name" : "logstash-netflow"
        },
        "discovery" : {
          "zen" : {
            "ping" : {
              "unicast" : {
                "hosts" : "node38:9300,node38:9301,node38:9302,node38:9303,node38:9304,node38:9305"
              },
              "multicast" : {
                "enabled" : "false"
              }
            }
          }
        },
        "http" : {
          "enabled" : "false"
        },
        "name" : "logstash-netflow"
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 32
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 8365,
        "max_file_descriptors" : 4096,
        "mlockall" : false
      },
      "jvm" : {
        "pid" : 8365,
        "version" : "1.7.0_55",
        "vm_name" : "OpenJDK 64-Bit Server VM",
        "vm_version" : "24.51-b03",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1402068863618,
        "mem" : {
          "heap_init_in_bytes" : 524288000,
          "heap_max_in_bytes" : 506855424,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 224395264,
          "direct_max_in_bytes" : 506855424
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space", "CMS Old Gen", "CMS Perm Gen" ]
      },
      "thread_pool" : {
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "200"
        },
        "get" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "50"
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 96,
          "max" : 96,
          "queue_size" : "1k"
        },
        "percolate" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 10,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "",
          "name" : "",
          "mac_address" : ""
        }
      },
      "transport" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0%0:9302]",
        "publish_address" : "inet[/10.0.108.171:9302]"
      },
      "plugins" : [ ]
    },
    "myfhcxbYRCaqpJ_yAapmHw" : {
      "name" : "node38-telecom",
      "transport_address" : "inet[/10.0.108.171:9300]",
      "host" : "node38",
      "ip" : "10.0.108.171",
      "version" : "1.3.0",
      "build" : "1265b14",
      "http_address" : "inet[/10.0.108.171:9200]",
      "settings" : {
        "node" : {
          "name" : "node38-telecom"
        },
        "index" : {
          "number_of_replicas" : "1",
          "number_of_shards" : "8"
        },
        "bootstrap" : {
          "mlockall" : "true"
        },
        "name" : "node38-telecom",
        "pidfile" : "/var/run/elasticsearch/elasticsearch-telecom.pid",
        "path" : {
          "data" : "/var/lib/elasticsearch/telecom",
          "work" : "/tmp/elasticsearch",
          "home" : "/usr/share/elasticsearch",
          "conf" : "/etc/elasticsearch/telecom",
          "logs" : "/var/log/elasticsearch/telecom"
        },
        "cluster" : {
          "name" : "telecom"
        },
        "indices" : {
          "memory" : {
            "index_buffer_size" : "30%"
          }
        },
        "discovery" : {
          "zen" : {
            "minimum_master_nodes" : "1",
            "ping" : {
              "unicast" : {
                "hosts" : [ "node07", "node38" ]
              },
              "multicast" : {
                "enabled" : "false"
              },
              "timeout" : "30s"
            }
          }
        }
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 32,
        "cpu" : {
          "vendor" : "Intel",
          "model" : "Xeon",
          "mhz" : 2000,
          "total_cores" : 32,
          "total_sockets" : 1,
          "cores_per_socket" : 32,
          "cache_size_in_bytes" : 20480
        },
        "mem" : {
          "total_in_bytes" : 33617100800
        },
        "swap" : {
          "total_in_bytes" : 17459511296
        }
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 12731,
        "max_file_descriptors" : 65535,
        "mlockall" : false
      },
      "jvm" : {
        "pid" : 12731,
        "version" : "1.7.0_65",
        "vm_name" : "OpenJDK 64-Bit Server VM",
        "vm_version" : "24.65-b04",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1406268268972,
        "mem" : {
          "heap_init_in_bytes" : 17179869184,
          "heap_max_in_bytes" : 16979263488,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 224395264,
          "direct_max_in_bytes" : 16979263488
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space", "CMS Old Gen", "CMS Perm Gen" ]
      },
      "thread_pool" : {
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "200"
        },
        "snapshot_data" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "bench" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "get" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "50"
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 96,
          "max" : 96,
          "queue_size" : "1k"
        },
        "percolate" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 10,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "10.0.108.171",
          "name" : "eth0",
          "mac_address" : ""
        }
      },
      "transport" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0:9300]",
        "publish_address" : "inet[/10.0.108.171:9300]"
      },
      "http" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0:9200]",
        "publish_address" : "inet[/10.0.108.171:9200]",
        "max_content_length_in_bytes" : 104857600
      },
      "plugins" : [ ]
    },
    "9FwiLpriR12926UVB-YuVw" : {
      "name" : "logstash-netflow",
      "transport_address" : "inet[/10.0.108.171:9301]",
      "host" : "node38",
      "ip" : "10.0.108.171",
      "version" : "1.1.1",
      "build" : "f1585f0",
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "settings" : {
        "path" : {
          "logs" : "/home/sysunix/logs"
        },
        "cluster" : {
          "name" : "telecom"
        },
        "node" : {
          "client" : "true",
          "name" : "logstash-netflow"
        },
        "discovery" : {
          "zen" : {
            "ping" : {
              "unicast" : {
                "hosts" : "node38:9300,node38:9301,node38:9302,node38:9303,node38:9304,node38:9305"
              },
              "multicast" : {
                "enabled" : "false"
              }
            }
          }
        },
        "http" : {
          "enabled" : "false"
        },
        "name" : "logstash-netflow"
      },
      "os" : {
        "refresh_interval_in_millis" : 1000,
        "available_processors" : 32
      },
      "process" : {
        "refresh_interval_in_millis" : 1000,
        "id" : 8365,
        "max_file_descriptors" : 4096,
        "mlockall" : false
      },
      "jvm" : {
        "pid" : 8365,
        "version" : "1.7.0_55",
        "vm_name" : "OpenJDK 64-Bit Server VM",
        "vm_version" : "24.51-b03",
        "vm_vendor" : "Oracle Corporation",
        "start_time_in_millis" : 1402068863618,
        "mem" : {
          "heap_init_in_bytes" : 524288000,
          "heap_max_in_bytes" : 506855424,
          "non_heap_init_in_bytes" : 24313856,
          "non_heap_max_in_bytes" : 224395264,
          "direct_max_in_bytes" : 506855424
        },
        "gc_collectors" : [ "ParNew", "ConcurrentMarkSweep" ],
        "memory_pools" : [ "Code Cache", "Par Eden Space", "Par Survivor Space", "CMS Old Gen", "CMS Perm Gen" ]
      },
      "thread_pool" : {
        "generic" : {
          "type" : "cached",
          "keep_alive" : "30s",
          "queue_size" : -1
        },
        "index" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "200"
        },
        "get" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "snapshot" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "merge" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "suggest" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "bulk" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "50"
        },
        "optimize" : {
          "type" : "fixed",
          "min" : 1,
          "max" : 1,
          "queue_size" : -1
        },
        "warmer" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "flush" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "search" : {
          "type" : "fixed",
          "min" : 96,
          "max" : 96,
          "queue_size" : "1k"
        },
        "percolate" : {
          "type" : "fixed",
          "min" : 32,
          "max" : 32,
          "queue_size" : "1k"
        },
        "management" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 5,
          "keep_alive" : "5m",
          "queue_size" : -1
        },
        "refresh" : {
          "type" : "scaling",
          "min" : 1,
          "max" : 10,
          "keep_alive" : "5m",
          "queue_size" : -1
        }
      },
      "network" : {
        "refresh_interval_in_millis" : 5000,
        "primary_interface" : {
          "address" : "",
          "name" : "",
          "mac_address" : ""
        }
      },
      "transport" : {
        "bound_address" : "inet[/0:0:0:0:0:0:0:0%0:9301]",
        "publish_address" : "inet[/10.0.108.171:9301]"
      },
      "plugins" : [ ]
    }
  }
}
```

```
curl 0:9200/_nodes/stats
```

``` json
{
  "cluster_name" : "telecom",
  "nodes" : {
    "JUOZAr-mT6e8e9nmwWH9ww" : {
      "timestamp" : 1406277844538,
      "name" : "node07-telecom",
      "transport_address" : "inet[/10.0.104.214:9300]",
      "host" : "node07",
      "ip" : [ "inet[/10.0.104.214:9300]", "NONE" ],
      "indices" : {
        "docs" : {
          "count" : 0,
          "deleted" : 0
        },
        "store" : {
          "size_in_bytes" : 0,
          "throttle_time_in_millis" : 0
        },
        "indexing" : {
          "index_total" : 0,
          "index_time_in_millis" : 0,
          "index_current" : 0,
          "delete_total" : 0,
          "delete_time_in_millis" : 0,
          "delete_current" : 0
        },
        "get" : {
          "total" : 0,
          "time_in_millis" : 0,
          "exists_total" : 0,
          "exists_time_in_millis" : 0,
          "missing_total" : 0,
          "missing_time_in_millis" : 0,
          "current" : 0
        },
        "search" : {
          "open_contexts" : 0,
          "query_total" : 0,
          "query_time_in_millis" : 0,
          "query_current" : 0,
          "fetch_total" : 0,
          "fetch_time_in_millis" : 0,
          "fetch_current" : 0
        },
        "merges" : {
          "current" : 0,
          "current_docs" : 0,
          "current_size_in_bytes" : 0,
          "total" : 0,
          "total_time_in_millis" : 0,
          "total_docs" : 0,
          "total_size_in_bytes" : 0
        },
        "refresh" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "flush" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "warmer" : {
          "current" : 0,
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "filter_cache" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "id_cache" : {
          "memory_size_in_bytes" : 0
        },
        "fielddata" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "percolate" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0,
          "memory_size_in_bytes" : -1,
          "memory_size" : "-1b",
          "queries" : 0
        },
        "completion" : {
          "size_in_bytes" : 0
        },
        "segments" : {
          "count" : 0,
          "memory_in_bytes" : 0,
          "index_writer_memory_in_bytes" : 0,
          "version_map_memory_in_bytes" : 0
        },
        "translog" : {
          "operations" : 0,
          "size_in_bytes" : 0
        },
        "suggest" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0
        }
      },
      "os" : {
        "timestamp" : 1406277844539,
        "uptime_in_millis" : 2050688,
        "load_average" : [ 0.0, 0.0, 0.0 ],
        "cpu" : {
          "sys" : 0,
          "user" : 0,
          "idle" : 99,
          "usage" : 0,
          "stolen" : 0
        },
        "mem" : {
          "free_in_bytes" : 13301526528,
          "used_in_bytes" : 11883552768,
          "free_percent" : 94,
          "used_percent" : 5,
          "actual_free_in_bytes" : 23725084672,
          "actual_used_in_bytes" : 1459994624
        },
        "swap" : {
          "used_in_bytes" : 26796032,
          "free_in_bytes" : 252854272
        }
      },
      "process" : {
        "timestamp" : 1406277844540,
        "open_file_descriptors" : 464,
        "cpu" : {
          "percent" : 0,
          "sys_in_millis" : 17990,
          "user_in_millis" : 33500,
          "total_in_millis" : 51490
        },
        "mem" : {
          "resident_in_bytes" : 616067072,
          "share_in_bytes" : 13942784,
          "total_virtual_in_bytes" : 26445426688
        }
      },
      "jvm" : {
        "timestamp" : 1406277844540,
        "uptime_in_millis" : 9502740,
        "mem" : {
          "heap_used_in_bytes" : 575853064,
          "heap_used_percent" : 3,
          "heap_committed_in_bytes" : 17066491904,
          "heap_max_in_bytes" : 17066491904,
          "non_heap_used_in_bytes" : 31055048,
          "non_heap_committed_in_bytes" : 32374784,
          "pools" : {
            "young" : {
              "used_in_bytes" : 546352176,
              "max_in_bytes" : 907345920,
              "peak_used_in_bytes" : 907345920,
              "peak_max_in_bytes" : 907345920
            },
            "survivor" : {
              "used_in_bytes" : 29500888,
              "max_in_bytes" : 113377280,
              "peak_used_in_bytes" : 29500888,
              "peak_max_in_bytes" : 113377280
            },
            "old" : {
              "used_in_bytes" : 0,
              "max_in_bytes" : 16045768704,
              "peak_used_in_bytes" : 0,
              "peak_max_in_bytes" : 16045768704
            }
          }
        },
        "threads" : {
          "count" : 112,
          "peak_count" : 115
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 1,
              "collection_time_in_millis" : 58
            },
            "old" : {
              "collection_count" : 0,
              "collection_time_in_millis" : 0
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 179,
            "used_in_bytes" : 49807360,
            "total_capacity_in_bytes" : 49807360
          },
          "mapped" : {
            "count" : 0,
            "used_in_bytes" : 0,
            "total_capacity_in_bytes" : 0
          }
        }
      },
      "thread_pool" : {
        "generic" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 4,
          "completed" : 972
        },
        "index" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot_data" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bench" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "get" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "merge" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "suggest" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bulk" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "optimize" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "warmer" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "flush" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "search" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "percolate" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "management" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 1,
          "rejected" : 0,
          "largest" : 1,
          "completed" : 653
        },
        "refresh" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        }
      },
      "network" : {
        "tcp" : {
          "active_opens" : 421209,
          "passive_opens" : 138326,
          "curr_estab" : 132,
          "in_segs" : 110881241,
          "out_segs" : 114912697,
          "retrans_segs" : 105,
          "estab_resets" : 253951,
          "attempt_fails" : 4321,
          "in_errs" : 0,
          "out_rsts" : 68724493
        }
      },
      "fs" : {
        "timestamp" : 1406277844541,
        "total" : {
          "total_in_bytes" : 1648462135296,
          "free_in_bytes" : 1632006266880,
          "available_in_bytes" : 1632006266880,
          "disk_reads" : 11399,
          "disk_writes" : 3156978,
          "disk_io_op" : 3168377,
          "disk_read_size_in_bytes" : 515517952,
          "disk_write_size_in_bytes" : 302869040640,
          "disk_io_size_in_bytes" : 303384558592,
          "disk_queue" : "1.5E-4",
          "disk_service_time" : "0.1"
        },
        "data" : [ {
          "path" : "/var/lib/elasticsearch/telecom/telecom/nodes/0",
          "mount" : "/var/lib/elasticsearch",
          "dev" : "/dev/mapper/rootvg-elasticsearch",
          "total_in_bytes" : 1648462135296,
          "free_in_bytes" : 1632006266880,
          "available_in_bytes" : 1632006266880,
          "disk_reads" : 11399,
          "disk_writes" : 3156978,
          "disk_io_op" : 3168377,
          "disk_read_size_in_bytes" : 515517952,
          "disk_write_size_in_bytes" : 302869040640,
          "disk_io_size_in_bytes" : 303384558592,
          "disk_queue" : "1.5E-4",
          "disk_service_time" : "0.1"
        } ]
      },
      "transport" : {
        "server_open" : 65,
        "rx_count" : 19014,
        "rx_size_in_bytes" : 934110,
        "tx_count" : 19013,
        "tx_size_in_bytes" : 1151504
      },
      "http" : {
        "current_open" : 0,
        "total_opened" : 643
      },
      "fielddata_breaker" : {
        "maximum_size_in_bytes" : 10239895142,
        "maximum_size" : "9.5gb",
        "estimated_size_in_bytes" : 0,
        "estimated_size" : "0b",
        "overhead" : 1.03,
        "tripped" : 0
      }
    },
    "JL38-jS9Sn67hQk2XxNZxw" : {
      "timestamp" : 1406277844542,
      "name" : "logstash-netflow",
      "transport_address" : "inet[/10.0.108.171:9303]",
      "host" : "node38",
      "ip" : [ "inet[/10.0.108.171:9303]", "NONE" ],
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "indices" : {
        "docs" : {
          "count" : 0,
          "deleted" : 0
        },
        "store" : {
          "size_in_bytes" : 0,
          "throttle_time_in_millis" : 0
        },
        "indexing" : {
          "index_total" : 0,
          "index_time_in_millis" : 0,
          "index_current" : 0,
          "delete_total" : 0,
          "delete_time_in_millis" : 0,
          "delete_current" : 0
        },
        "get" : {
          "total" : 0,
          "time_in_millis" : 0,
          "exists_total" : 0,
          "exists_time_in_millis" : 0,
          "missing_total" : 0,
          "missing_time_in_millis" : 0,
          "current" : 0
        },
        "search" : {
          "open_contexts" : 0,
          "query_total" : 0,
          "query_time_in_millis" : 0,
          "query_current" : 0,
          "fetch_total" : 0,
          "fetch_time_in_millis" : 0,
          "fetch_current" : 0
        },
        "merges" : {
          "current" : 0,
          "current_docs" : 0,
          "current_size_in_bytes" : 0,
          "total" : 0,
          "total_time_in_millis" : 0,
          "total_docs" : 0,
          "total_size_in_bytes" : 0
        },
        "refresh" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "flush" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "warmer" : {
          "current" : 0,
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "filter_cache" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "id_cache" : {
          "memory_size_in_bytes" : 0
        },
        "fielddata" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "percolate" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0,
          "memory_size_in_bytes" : -1,
          "memory_size" : "-1b",
          "queries" : 0
        },
        "completion" : {
          "size_in_bytes" : 0
        },
        "segments" : {
          "count" : 0,
          "memory_in_bytes" : 0,
          "index_writer_memory_in_bytes" : 0,
          "version_map_memory_in_bytes" : 0
        },
        "translog" : {
          "operations" : 0,
          "size_in_bytes" : 0
        }
      },
      "os" : {
        "timestamp" : 1406277844542
      },
      "process" : {
        "timestamp" : 1406277844542,
        "open_file_descriptors" : 1377
      },
      "jvm" : {
        "timestamp" : 1406277844544,
        "uptime_in_millis" : 4208980926,
        "mem" : {
          "heap_used_in_bytes" : 226119440,
          "heap_used_percent" : 44,
          "heap_committed_in_bytes" : 506855424,
          "heap_max_in_bytes" : 506855424,
          "non_heap_used_in_bytes" : 68490160,
          "non_heap_committed_in_bytes" : 107167744,
          "pools" : {
            "young" : {
              "used_in_bytes" : 46953832,
              "max_in_bytes" : 139853824,
              "peak_used_in_bytes" : 139853824,
              "peak_max_in_bytes" : 139853824
            },
            "survivor" : {
              "used_in_bytes" : 110032,
              "max_in_bytes" : 17432576,
              "peak_used_in_bytes" : 17432576,
              "peak_max_in_bytes" : 17432576
            },
            "old" : {
              "used_in_bytes" : 179055576,
              "max_in_bytes" : 349569024,
              "peak_used_in_bytes" : 339622256,
              "peak_max_in_bytes" : 349569024
            }
          }
        },
        "threads" : {
          "count" : 428,
          "peak_count" : 442
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 11840,
              "collection_time_in_millis" : 67781
            },
            "old" : {
              "collection_count" : 14,
              "collection_time_in_millis" : 203
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 723,
            "used_in_bytes" : 199419308,
            "total_capacity_in_bytes" : 199419308
          },
          "mapped" : {
            "count" : 0,
            "used_in_bytes" : 0,
            "total_capacity_in_bytes" : 0
          }
        }
      },
      "thread_pool" : {
        "generic" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 4,
          "completed" : 420988
        },
        "index" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "get" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "merge" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "suggest" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bulk" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "optimize" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "warmer" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "flush" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "search" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "percolate" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "management" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 1,
          "rejected" : 0,
          "largest" : 2,
          "completed" : 766
        },
        "refresh" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        }
      },
      "network" : { },
      "fs" : {
        "timestamp" : 1406277844544,
        "total" : { },
        "data" : [ ]
      },
      "transport" : {
        "server_open" : 26,
        "rx_count" : 9160629,
        "rx_size_in_bytes" : 2570548643,
        "tx_count" : 9160591,
        "tx_size_in_bytes" : 1643358255
      },
      "fielddata_breaker" : {
        "maximum_size_in_bytes" : 405484339,
        "maximum_size" : "386.6mb",
        "estimated_size_in_bytes" : 0,
        "estimated_size" : "0b",
        "overhead" : 1.03,
        "tripped" : -1
      }
    },
    "rPZ9EsahRl-9Cs_AOucOJQ" : {
      "timestamp" : 1406277844542,
      "name" : "logstash-netflow",
      "transport_address" : "inet[/10.0.108.171:9302]",
      "host" : "node38",
      "ip" : [ "inet[/10.0.108.171:9302]", "NONE" ],
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "indices" : {
        "docs" : {
          "count" : 0,
          "deleted" : 0
        },
        "store" : {
          "size_in_bytes" : 0,
          "throttle_time_in_millis" : 0
        },
        "indexing" : {
          "index_total" : 0,
          "index_time_in_millis" : 0,
          "index_current" : 0,
          "delete_total" : 0,
          "delete_time_in_millis" : 0,
          "delete_current" : 0
        },
        "get" : {
          "total" : 0,
          "time_in_millis" : 0,
          "exists_total" : 0,
          "exists_time_in_millis" : 0,
          "missing_total" : 0,
          "missing_time_in_millis" : 0,
          "current" : 0
        },
        "search" : {
          "open_contexts" : 0,
          "query_total" : 0,
          "query_time_in_millis" : 0,
          "query_current" : 0,
          "fetch_total" : 0,
          "fetch_time_in_millis" : 0,
          "fetch_current" : 0
        },
        "merges" : {
          "current" : 0,
          "current_docs" : 0,
          "current_size_in_bytes" : 0,
          "total" : 0,
          "total_time_in_millis" : 0,
          "total_docs" : 0,
          "total_size_in_bytes" : 0
        },
        "refresh" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "flush" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "warmer" : {
          "current" : 0,
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "filter_cache" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "id_cache" : {
          "memory_size_in_bytes" : 0
        },
        "fielddata" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "percolate" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0,
          "memory_size_in_bytes" : -1,
          "memory_size" : "-1b",
          "queries" : 0
        },
        "completion" : {
          "size_in_bytes" : 0
        },
        "segments" : {
          "count" : 0,
          "memory_in_bytes" : 0,
          "index_writer_memory_in_bytes" : 0,
          "version_map_memory_in_bytes" : 0
        },
        "translog" : {
          "operations" : 0,
          "size_in_bytes" : 0
        }
      },
      "os" : {
        "timestamp" : 1406277844543
      },
      "process" : {
        "timestamp" : 1406277844543,
        "open_file_descriptors" : 1375
      },
      "jvm" : {
        "timestamp" : 1406277844544,
        "uptime_in_millis" : 4208980926,
        "mem" : {
          "heap_used_in_bytes" : 226123632,
          "heap_used_percent" : 44,
          "heap_committed_in_bytes" : 506855424,
          "heap_max_in_bytes" : 506855424,
          "non_heap_used_in_bytes" : 68490160,
          "non_heap_committed_in_bytes" : 107167744,
          "pools" : {
            "young" : {
              "used_in_bytes" : 46960112,
              "max_in_bytes" : 139853824,
              "peak_used_in_bytes" : 139853824,
              "peak_max_in_bytes" : 139853824
            },
            "survivor" : {
              "used_in_bytes" : 110032,
              "max_in_bytes" : 17432576,
              "peak_used_in_bytes" : 17432576,
              "peak_max_in_bytes" : 17432576
            },
            "old" : {
              "used_in_bytes" : 179055576,
              "max_in_bytes" : 349569024,
              "peak_used_in_bytes" : 339622256,
              "peak_max_in_bytes" : 349569024
            }
          }
        },
        "threads" : {
          "count" : 428,
          "peak_count" : 442
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 11840,
              "collection_time_in_millis" : 67781
            },
            "old" : {
              "collection_count" : 14,
              "collection_time_in_millis" : 203
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 723,
            "used_in_bytes" : 199419308,
            "total_capacity_in_bytes" : 199419308
          },
          "mapped" : {
            "count" : 0,
            "used_in_bytes" : 0,
            "total_capacity_in_bytes" : 0
          }
        }
      },
      "thread_pool" : {
        "generic" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 3,
          "completed" : 420986
        },
        "index" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "get" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "merge" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "suggest" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bulk" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "optimize" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "warmer" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "flush" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "search" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "percolate" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "management" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 1,
          "rejected" : 0,
          "largest" : 2,
          "completed" : 766
        },
        "refresh" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        }
      },
      "network" : { },
      "fs" : {
        "timestamp" : 1406277844544,
        "total" : { },
        "data" : [ ]
      },
      "transport" : {
        "server_open" : 26,
        "rx_count" : 9160652,
        "rx_size_in_bytes" : 2567605017,
        "tx_count" : 9160605,
        "tx_size_in_bytes" : 1635812229
      },
      "fielddata_breaker" : {
        "maximum_size_in_bytes" : 405484339,
        "maximum_size" : "386.6mb",
        "estimated_size_in_bytes" : 0,
        "estimated_size" : "0b",
        "overhead" : 1.03,
        "tripped" : -1
      }
    },
    "myfhcxbYRCaqpJ_yAapmHw" : {
      "timestamp" : 1406277844542,
      "name" : "node38-telecom",
      "transport_address" : "inet[/10.0.108.171:9300]",
      "host" : "node38",
      "ip" : [ "inet[/10.0.108.171:9300]", "NONE" ],
      "indices" : {
        "docs" : {
          "count" : 33434392,
          "deleted" : 0
        },
        "store" : {
          "size_in_bytes" : 16439595838,
          "throttle_time_in_millis" : 0
        },
        "indexing" : {
          "index_total" : 0,
          "index_time_in_millis" : 0,
          "index_current" : 0,
          "delete_total" : 0,
          "delete_time_in_millis" : 0,
          "delete_current" : 0
        },
        "get" : {
          "total" : 0,
          "time_in_millis" : 0,
          "exists_total" : 0,
          "exists_time_in_millis" : 0,
          "missing_total" : 0,
          "missing_time_in_millis" : 0,
          "current" : 0
        },
        "search" : {
          "open_contexts" : 0,
          "query_total" : 0,
          "query_time_in_millis" : 0,
          "query_current" : 0,
          "fetch_total" : 0,
          "fetch_time_in_millis" : 0,
          "fetch_current" : 0
        },
        "merges" : {
          "current" : 0,
          "current_docs" : 0,
          "current_size_in_bytes" : 0,
          "total" : 0,
          "total_time_in_millis" : 0,
          "total_docs" : 0,
          "total_size_in_bytes" : 0
        },
        "refresh" : {
          "total" : 26,
          "total_time_in_millis" : 0
        },
        "flush" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "warmer" : {
          "current" : 0,
          "total" : 52,
          "total_time_in_millis" : 2
        },
        "filter_cache" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "id_cache" : {
          "memory_size_in_bytes" : 0
        },
        "fielddata" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "percolate" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0,
          "memory_size_in_bytes" : -1,
          "memory_size" : "-1b",
          "queries" : 0
        },
        "completion" : {
          "size_in_bytes" : 0
        },
        "segments" : {
          "count" : 331,
          "memory_in_bytes" : 380822712,
          "index_writer_memory_in_bytes" : 0,
          "version_map_memory_in_bytes" : 0
        },
        "translog" : {
          "operations" : 0,
          "size_in_bytes" : 0
        },
        "suggest" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0
        }
      },
      "os" : {
        "timestamp" : 1406277844605,
        "uptime_in_millis" : 11125608,
        "load_average" : [ 0.02, 0.02, 0.0 ],
        "cpu" : {
          "sys" : 0,
          "user" : 0,
          "idle" : 99,
          "usage" : 0,
          "stolen" : 0
        },
        "mem" : {
          "free_in_bytes" : 9554894848,
          "used_in_bytes" : 24062205952,
          "free_percent" : 85,
          "used_percent" : 14,
          "actual_free_in_bytes" : 28848246784,
          "actual_used_in_bytes" : 4768854016
        },
        "swap" : {
          "used_in_bytes" : 306176000,
          "free_in_bytes" : 17153335296
        }
      },
      "process" : {
        "timestamp" : 1406277844605,
        "open_file_descriptors" : 1796,
        "cpu" : {
          "percent" : 1,
          "sys_in_millis" : 64390,
          "user_in_millis" : 113440,
          "total_in_millis" : 177830
        },
        "mem" : {
          "resident_in_bytes" : 2292084736,
          "share_in_bytes" : 16846848,
          "total_virtual_in_bytes" : 38485655552
        }
      },
      "jvm" : {
        "timestamp" : 1406277844607,
        "uptime_in_millis" : 9575635,
        "mem" : {
          "heap_used_in_bytes" : 1142720912,
          "heap_used_percent" : 6,
          "heap_committed_in_bytes" : 16979263488,
          "heap_max_in_bytes" : 16979263488,
          "non_heap_used_in_bytes" : 39112624,
          "non_heap_committed_in_bytes" : 39387136,
          "pools" : {
            "young" : {
              "used_in_bytes" : 978794664,
              "max_in_bytes" : 1605304320,
              "peak_used_in_bytes" : 1605304320,
              "peak_max_in_bytes" : 1605304320
            },
            "survivor" : {
              "used_in_bytes" : 3106256,
              "max_in_bytes" : 200605696,
              "peak_used_in_bytes" : 166923624,
              "peak_max_in_bytes" : 200605696
            },
            "old" : {
              "used_in_bytes" : 160819992,
              "max_in_bytes" : 15173353472,
              "peak_used_in_bytes" : 160819992,
              "peak_max_in_bytes" : 15173353472
            }
          }
        },
        "threads" : {
          "count" : 214,
          "peak_count" : 229
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 7,
              "collection_time_in_millis" : 403
            },
            "old" : {
              "collection_count" : 0,
              "collection_time_in_millis" : 0
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 336,
            "used_in_bytes" : 87049216,
            "total_capacity_in_bytes" : 87049216
          },
          "mapped" : {
            "count" : 440,
            "used_in_bytes" : 3388615670,
            "total_capacity_in_bytes" : 3388615670
          }
        }
      },
      "thread_pool" : {
        "generic" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 19,
          "completed" : 1054
        },
        "index" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot_data" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bench" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "get" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "merge" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "suggest" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bulk" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "optimize" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "warmer" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 1,
          "completed" : 26
        },
        "flush" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "search" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "percolate" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "management" : {
          "threads" : 5,
          "queue" : 0,
          "active" : 1,
          "rejected" : 0,
          "largest" : 5,
          "completed" : 1354
        },
        "refresh" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        }
      },
      "network" : {
        "tcp" : {
          "active_opens" : 140097716,
          "passive_opens" : 1880178,
          "curr_estab" : 290,
          "in_segs" : 5172144515,
          "out_segs" : 5814393262,
          "retrans_segs" : 212947,
          "estab_resets" : 1056639,
          "attempt_fails" : 27282,
          "in_errs" : 50905,
          "out_rsts" : 1112514
        }
      },
      "fs" : {
        "timestamp" : 1406277844608,
        "total" : {
          "total_in_bytes" : 2197949513728,
          "free_in_bytes" : 2181463736320,
          "available_in_bytes" : 2181463736320,
          "disk_reads" : 38753157,
          "disk_writes" : 243000636,
          "disk_io_op" : 281753793,
          "disk_read_size_in_bytes" : 4230850646528,
          "disk_write_size_in_bytes" : 13743848959488,
          "disk_io_size_in_bytes" : 17974699606016
        },
        "data" : [ {
          "path" : "/var/lib/elasticsearch/telecom/telecom/nodes/0",
          "mount" : "/var/lib/elasticsearch",
          "dev" : "/dev/mapper/raidvg-elasticsearch",
          "total_in_bytes" : 2197949513728,
          "free_in_bytes" : 2181463736320,
          "available_in_bytes" : 2181463736320,
          "disk_reads" : 38753157,
          "disk_writes" : 243000636,
          "disk_io_op" : 281753793,
          "disk_read_size_in_bytes" : 4230850646528,
          "disk_write_size_in_bytes" : 13743848959488,
          "disk_io_size_in_bytes" : 17974699606016
        } ]
      },
      "transport" : {
        "server_open" : 65,
        "rx_count" : 76220,
        "rx_size_in_bytes" : 4624394,
        "tx_count" : 76219,
        "tx_size_in_bytes" : 3602292
      },
      "http" : {
        "current_open" : 1,
        "total_opened" : 655
      },
      "fielddata_breaker" : {
        "maximum_size_in_bytes" : 10187558092,
        "maximum_size" : "9.4gb",
        "estimated_size_in_bytes" : 0,
        "estimated_size" : "0b",
        "overhead" : 1.03,
        "tripped" : 0
      }
    },
    "9FwiLpriR12926UVB-YuVw" : {
      "timestamp" : 1406277844542,
      "name" : "logstash-netflow",
      "transport_address" : "inet[/10.0.108.171:9301]",
      "host" : "node38",
      "ip" : [ "inet[/10.0.108.171:9301]", "NONE" ],
      "attributes" : {
        "client" : "true",
        "data" : "false"
      },
      "indices" : {
        "docs" : {
          "count" : 0,
          "deleted" : 0
        },
        "store" : {
          "size_in_bytes" : 0,
          "throttle_time_in_millis" : 0
        },
        "indexing" : {
          "index_total" : 0,
          "index_time_in_millis" : 0,
          "index_current" : 0,
          "delete_total" : 0,
          "delete_time_in_millis" : 0,
          "delete_current" : 0
        },
        "get" : {
          "total" : 0,
          "time_in_millis" : 0,
          "exists_total" : 0,
          "exists_time_in_millis" : 0,
          "missing_total" : 0,
          "missing_time_in_millis" : 0,
          "current" : 0
        },
        "search" : {
          "open_contexts" : 0,
          "query_total" : 0,
          "query_time_in_millis" : 0,
          "query_current" : 0,
          "fetch_total" : 0,
          "fetch_time_in_millis" : 0,
          "fetch_current" : 0
        },
        "merges" : {
          "current" : 0,
          "current_docs" : 0,
          "current_size_in_bytes" : 0,
          "total" : 0,
          "total_time_in_millis" : 0,
          "total_docs" : 0,
          "total_size_in_bytes" : 0
        },
        "refresh" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "flush" : {
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "warmer" : {
          "current" : 0,
          "total" : 0,
          "total_time_in_millis" : 0
        },
        "filter_cache" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "id_cache" : {
          "memory_size_in_bytes" : 0
        },
        "fielddata" : {
          "memory_size_in_bytes" : 0,
          "evictions" : 0
        },
        "percolate" : {
          "total" : 0,
          "time_in_millis" : 0,
          "current" : 0,
          "memory_size_in_bytes" : -1,
          "memory_size" : "-1b",
          "queries" : 0
        },
        "completion" : {
          "size_in_bytes" : 0
        },
        "segments" : {
          "count" : 0,
          "memory_in_bytes" : 0,
          "index_writer_memory_in_bytes" : 0,
          "version_map_memory_in_bytes" : 0
        },
        "translog" : {
          "operations" : 0,
          "size_in_bytes" : 0
        }
      },
      "os" : {
        "timestamp" : 1406277844543
      },
      "process" : {
        "timestamp" : 1406277844543,
        "open_file_descriptors" : 1376
      },
      "jvm" : {
        "timestamp" : 1406277844544,
        "uptime_in_millis" : 4208980926,
        "mem" : {
          "heap_used_in_bytes" : 226119440,
          "heap_used_percent" : 44,
          "heap_committed_in_bytes" : 506855424,
          "heap_max_in_bytes" : 506855424,
          "non_heap_used_in_bytes" : 68490160,
          "non_heap_committed_in_bytes" : 107167744,
          "pools" : {
            "young" : {
              "used_in_bytes" : 46955928,
              "max_in_bytes" : 139853824,
              "peak_used_in_bytes" : 139853824,
              "peak_max_in_bytes" : 139853824
            },
            "survivor" : {
              "used_in_bytes" : 110032,
              "max_in_bytes" : 17432576,
              "peak_used_in_bytes" : 17432576,
              "peak_max_in_bytes" : 17432576
            },
            "old" : {
              "used_in_bytes" : 179055576,
              "max_in_bytes" : 349569024,
              "peak_used_in_bytes" : 339622256,
              "peak_max_in_bytes" : 349569024
            }
          }
        },
        "threads" : {
          "count" : 428,
          "peak_count" : 442
        },
        "gc" : {
          "collectors" : {
            "young" : {
              "collection_count" : 11840,
              "collection_time_in_millis" : 67781
            },
            "old" : {
              "collection_count" : 14,
              "collection_time_in_millis" : 203
            }
          }
        },
        "buffer_pools" : {
          "direct" : {
            "count" : 723,
            "used_in_bytes" : 199419308,
            "total_capacity_in_bytes" : 199419308
          },
          "mapped" : {
            "count" : 0,
            "used_in_bytes" : 0,
            "total_capacity_in_bytes" : 0
          }
        }
      },
      "thread_pool" : {
        "generic" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 4,
          "completed" : 420979
        },
        "index" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "get" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "snapshot" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "merge" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "suggest" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "bulk" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "optimize" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "warmer" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "flush" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "search" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "percolate" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        },
        "management" : {
          "threads" : 1,
          "queue" : 0,
          "active" : 1,
          "rejected" : 0,
          "largest" : 2,
          "completed" : 766
        },
        "refresh" : {
          "threads" : 0,
          "queue" : 0,
          "active" : 0,
          "rejected" : 0,
          "largest" : 0,
          "completed" : 0
        }
      },
      "network" : { },
      "fs" : {
        "timestamp" : 1406277844544,
        "total" : { },
        "data" : [ ]
      },
      "transport" : {
        "server_open" : 26,
        "rx_count" : 8688419,
        "rx_size_in_bytes" : 2179836154,
        "tx_count" : 8688418,
        "tx_size_in_bytes" : 486036255
      },
      "fielddata_breaker" : {
        "maximum_size_in_bytes" : 405484339,
        "maximum_size" : "386.6mb",
        "estimated_size_in_bytes" : 0,
        "estimated_size" : "0b",
        "overhead" : 1.03,
        "tripped" : -1
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2014-07-25T10:21:57Z" id="50132150">thanks @faxm0dem 

@drewr could you take another look at this with the new info that has been provided please?
</comment><comment author="drewr" created="2014-08-19T16:22:30Z" id="52659604">Sorry, just now seeing this... @faxm0dem did anything else anomalous happen in the cluster before this?  Like, nodes not communicating properly, or running out of mem, anything like that?
</comment><comment author="faxm0dem" created="2014-08-20T06:29:11Z" id="52738659">Not really. I just upgraded my prod cluster to 1.3.2, and have the same issue.
</comment><comment author="chandanbansal" created="2014-09-11T13:20:41Z" id="55262212">Getting same error after connecting with logstash. Its working fine as single node and in cluster mode without logstash. Tried with ES 1.3.2 and 1.24 with logstash 1.4.2. is there any workaround for same? But data indexing and query is working fine.
</comment><comment author="faxm0dem" created="2014-09-12T07:17:25Z" id="55369304">I don't have logstash nodes in my devel cluster, and I still get the error
</comment><comment author="imotov" created="2014-09-27T05:44:40Z" id="57043344">Fixed by #7815.
</comment><comment author="mcnewton" created="2014-11-15T00:57:06Z" id="63152228">Just to confirm - was still seeing this in 1.3.4, but upgraded to 1.4.0 today and it is now working correctly.
Thanks!
Matthew
</comment><comment author="faxm0dem" created="2014-11-16T19:03:09Z" id="63233099">I confirm this is finally working!
</comment><comment author="Grauen" created="2014-11-19T14:18:48Z" id="63645811">I upgraded to 1.4.0 today and now I see this error. 

http://localhost:9200/_aliases?pretty=1
{
  "error" : "RemoteTransportException[[Raza][inet[/123.456.17.4:9300]][indices:admin/get]]; nested: ActionNotFoundTransportException[No handler for action [indices:admin/get]]; ",
  "status" : 500
}

http://0:9200/_cat/indices
{"error":"NullPointerException[null]","status":500}

However I am able to search indices directly 

http://localhost:9200/testindex/_search?
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
</comment><comment author="faxm0dem" created="2014-11-19T15:18:56Z" id="63655326">@Grauen are all your nodes 1.4.0?
</comment><comment author="clintongormley" created="2014-11-24T11:29:58Z" id="64180931">@Grauen This sounds like a bug that was fixed in https://github.com/elasticsearch/elasticsearch/pull/8387. if you are still seeing this problems when all of your nodes are running 1.4.0, then please open a new issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Guava doesn't explicitly remove entries when clearing the entire cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6296</link><project id="" key="" /><description>A call to `.cleanUp()` is needed to ensure the entries have been removed.
</description><key id="34180034">6296</key><summary>Guava doesn't explicitly remove entries when clearing the entire cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Internal</label><label>bug</label><label>v1.2.4</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T14:32:08Z</created><updated>2015-06-07T19:59:02Z</updated><resolved>2014-05-23T14:32:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-05-23T14:32:26Z" id="44016445">Fixed by https://github.com/elasticsearch/elasticsearch/commit/65ce5acfb41b4abd0c527aa0e870c2a1076d76cd
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only send diff of cluster state instead of full cluster state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6295</link><project id="" key="" /><description>If you have many nodes, and many index, even a small cluster state will trigger 500 MB of data to be sent to all nodes from the master. A few small rebalancing operations will kill the cluster.

This is a big issue.

Would be good if only the diff could be sent, then later merged. If node isn't at previous version, then fall back to current behaviour and send full state.
</description><key id="34174107">6295</key><summary>Only send diff of cluster state instead of full cluster state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">bluelu</reporter><labels><label>feature</label><label>high hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T13:23:06Z</created><updated>2016-03-15T00:08:51Z</updated><resolved>2015-04-29T16:58:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-07-04T14:59:49Z" id="48052702">@bluelu which version are you using? I haven't seen a compressed cluster state that is 500mb, we compress and all in our infra when we publish and so on.

Changing to send deltas will require quite a big change, its an option of course, but its a really big change since we rely on the full cluster state to be continuously published.

I have helped several users with large cluster state, and improved things internally. The size was not the issue, it was things like inefficient processing of large cluster state.
</comment><comment author="bluelu" created="2014-07-04T15:27:35Z" id="48056148">We are currently using elasticsearch-1.0.2.

{
  "cluster_name" : "abc",
  "status" : "green",
  "timed_out" : false,
  "number_of_nodes" : 505,
  "number_of_data_nodes" : 489,
  "active_primary_shards" : 8667,
  "active_shards" : 17334,
  "relocating_shards" : 2,
  "initializing_shards" : 0,
  "unassigned_shards" : 0
}
The uncompressed state is 2 MB (curl -XGET 'http://localhost:9200/_cluster/state') and compressed about 700 KB.

We have more than 500 nodes in the cluster, so each update (e.g. when a shard gets rebalanced) will cause the state to be send to all nodes, which in total is close to 500 MB in total to be sent. This takes a few seconds to complete and takes up all the bandwith.

I can also send you the complete cluster state in private if that helps.
</comment><comment author="bluelu" created="2014-09-22T10:23:50Z" id="56354751">@kimchy

Do you have any update on this if it will be integrated or not?

We also plan on using query warmers in the future, and then the updates also need to be dispatched to all the nodes. 

Creating a text diff of state (diff algorithm) and then applying that on the non master nodes, with fallback to the full cluster state if previous and current versions differ, might not be the cleanest solution,  but certainly shouldn't be all to difficult, as there is already code that the master waits for the nodes to confirm the updated cluster state. 
I agree that a cleaner solution (full updates) requires more work though.

This would dramatically reduce traffic and potentially as well the heap usage on the master node on larger clusters.
</comment><comment author="kimchy" created="2014-09-22T10:33:30Z" id="56355578">@bluelu no work has happened on this yet. Diff is one of the options, though generating the diff is one of the tricky parts here (we could do it more easily when we work on the object model, btw, of the cluster state). Indeed, we could send a diff and if the node receiving it can't apply due to changes, they can then request the full cluster state to be sent (with careful checks not to create a storm of full updates that are not needed).

It would be interesting how things would work in more recent versions. In your cluster state size, note that we do 2 things when publishing the cluster state, we serialize it using our internal serialization mechanism, so comparing it to json representation is not a real comparison, as its considerably smaller, and then we compress it. Also, on recent versions there is a better logic in applying cluster states on the receiving nodes.

Also, in upcoming 1.4, with the new zen discovery that is slated to it, there is much better support for multiple nodes. This will help as well.

The diff is something that would be interesting to explore in future versions, I am mainly trying to asses the urgency of it here, as in, is it really a problem in your use case?
</comment><comment author="bluelu" created="2014-09-23T15:00:26Z" id="56533455">@kimchy 

We throttled the bandwith on our master node so that it doesn't take all bandwith on the switch. It's not optimal but it seems to work for now. Before we had spikes of 1 gigabit over multiple seconds blocking all other connections on that switch (1 stream to each node). Now it just takes a little longer with less bandwith.

Apart from the slow restart (which we reduced by not starting all nodes at once, we will check behaviour on 1.3) it's the last one of the bigger issues which we saw so far on larger clusters.

For us, it's a problem, but it "works" with our workaround. But we would love to see this in &gt;1.5 if possible.

We will upgrade to the newest 1.3.\* in 2 weeks and then to 1.4.\* (when it's stable for sure) and report back then.
</comment><comment author="kimchy" created="2014-09-23T15:04:58Z" id="56534188">@bluelu thanks for the feedback!. I have another question, when you saw the 1gb saturation, was that when the cluster was forming (since there are a lot of cluster states updates happening then, which we reduced significantly in upcoming 1.4). If you issue a simple reroute after the cluster formed, what did you see then?
</comment><comment author="bluelu" created="2014-09-23T15:52:27Z" id="56542092">@kimchy 
I described this in another issue: #6372 

The cluster is forming without any issues and we didn't check the traffic then. This doesn't take much time at all until all nodes are added.

But it takes more minutes then until the cluster state jumps from 0 unassigned shards to let's say 5000 shards. After the first update, it get's faster. The more accurate the unassigned shards number is, the faster it seems to update. It's a little scary the first time, as one might expect that all data is lost ;-). 
If we add the nonssd servers, it even takes much longer (up to 10-15 minutes until the counter goes up). (might be related to slower storage, or more indexed shards, or even more nodes). We will try to test this with the new version to identify the exact cause.
During that phase we observed no traffic spikes (I guess because the state is just smaller, as no shards are allocated so far).

The traffic spikes occur if nodes are being rebalanced, we update the mapping, or do some other operation which requires the state to be sent.

We actually found out the bandwith issue as we had custom rebalancing code which shuffled indexes between 2 nodes back and forth and we wondered why everything felt so slow. This should be equal to a reroute command?

Here are more details about the slow start up (master and non master log):
https://gist.github.com/bluelu/3a9a9f7b629c3adb7e89
A colleague of mine added more information in the issue
</comment><comment author="bluelu" created="2014-12-06T14:39:44Z" id="65898988">Fyi, We upgraded to 1.4.1 today.

We are still seeing a lot of load on the network interface when the master node sends the cluster state on startup time. (About every minute one update)

I also observed that it takes some time for the cluster state to reach the nodes in that case (about 20 seconds...). I don't know if this will cause an issue in the future? hopefully not.

Node:
[2014-12-06 15:28:29,273][DEBUG][cluster.service          ] [node] set local cluster state to version 991 {elasticsearch[I80N5][clusterService#updateTask][T#1]}
[2014-12-06 15:29:11,667][DEBUG][cluster.service          ] [node] set local cluster state to version 992 {elasticsearch[I80N5][clusterService#updateTask][T#1]}

Master node:
[2014-12-06 15:28:12,758][DEBUG][cluster.service          ] [master] set local cluster state to version 991 {elasticsearch[I51N16][clusterService#updateTask][T#1]}
[2014-12-06 15:28:54,976][DEBUG][cluster.service          ] [master] set local cluster state to version 992 {elasticsearch[I51N16][clusterService#updateTask][T#1]}
</comment><comment author="ywelsch-t" created="2015-03-09T11:18:51Z" id="77836287">Hey @imotov, I'm working with @bluelu.

A more immediate patch to reduce the cluster state size which we plan to apply looks as follows: As we have many indices with the same mapping, deduplicating the index mappings reduces the cluster state to 1/3 of its size.
</comment><comment author="s1monw" created="2015-04-27T07:09:17Z" id="96526147">we had a boat load of failures related to this so I branched off `feature/cluster_state_diffs` and reverted the commit from master.
</comment><comment author="tmc24win" created="2016-03-14T20:34:05Z" id="196510471">I was wondering if there are plans to address the cluster state in an upcoming 2.x release.  We currently need to have thousands of small indexes to best support our data flow.  Right now we have almost 3000 indices and the GET request for cluster state takes around 40 seconds and effectively makes most admin tools (HQ, Head) non-functional.
</comment><comment author="imotov" created="2016-03-14T21:20:33Z" id="196525996">@DrGonzo424 this change (which was merged into 2.0.0) only improves node-to-node communication which works over transport protocol. HQ and Head are using REST API and will not be able to take advantage of cluster state diffs.
</comment><comment author="tmc24win" created="2016-03-14T23:04:55Z" id="196557413">Thanks for the quick reply, that is great to know that node-to-node communication will benefit from cluster state diffs as we are currently on 2.2.  Do you know if any of the admin tools do a better job of paging or caching the cluster state, to improve usability?  Right now it makes me a but uncomfortable that our administrative tools are not scaling with our cluster.  Perhaps there are some better admin tools out there.
</comment><comment author="imotov" created="2016-03-15T00:08:51Z" id="196577886">@DrGonzo424 I think https://discuss.elastic.co/ would be a much better place to ask this question. My favorite admin tool is [_cat API](https://www.elastic.co/guide/en/elasticsearch/reference/2.2/cat.html) it works fast even on large cluster states and does everything I need, but it might be too minimalistic for your purposes. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[JAVA-API] setSource has no effect on scroll search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6294</link><project id="" key="" /><description>For the following fluent buildup i get 0 result for query `Q` which is proper query like `{"query" :....}`. But on replacing `setSource` with `setQuery` and removing the outer "query" form in json i get correct results!

Although it is ok as a workaround but this means i can not use sorting.

``` java
client.prepareSearch("myindex").setSearchType(SearchType.SCAN)
  .setTypes("mytype").setScroll("1m").setSize(30).setSource(Q).execute().actionGet().getScrollId()
```

I am using scrollid from above to scroll further with

``` java
client.prepareSearchScroll(scrollid).setScrol("1m").execute().actionGet()
```

Thanks
</description><key id="34167616">6294</key><summary>[JAVA-API] setSource has no effect on scroll search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">kul</reporter><labels><label>:Java API</label></labels><created>2014-05-23T11:42:32Z</created><updated>2015-01-14T03:47:08Z</updated><resolved>2015-01-12T10:17:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:06:48Z" id="68386299">@dadoonet any ideas here?
</comment><comment author="dadoonet" created="2015-01-02T16:44:53Z" id="68540829">Hi @kul 

I tried to reproduce but it's working fine on my end. May be I did not get exactly what you are doing. If you still can reproduce your issue, could you paste a full Java example?

Here is what I did:

``` java
String source = "{\"query\":{\"match\":{\"gender\":\"male\"}}}";

// Testing without scroll
SearchResponse sr = client.prepareSearch(INDEX).setSource(source).get();
logger.info("{}", sr.toString());

// Testing with scroll
sr = client.prepareSearch(INDEX).setScroll("1m").setSource(source).get();
logger.info("{}", sr.toString());

// Both give expected results.
```
</comment><comment author="kul" created="2015-01-02T17:54:29Z" id="68546438">Hello @dadoonet ,

Thanks for testing this, what version are you using? I reported this for &lt;= 1.3.2
Also please note that i am doing a setSearchType(SearchType.SCAN) for possibly large results.

Thanks
</comment><comment author="dadoonet" created="2015-01-12T09:56:36Z" id="69547993">Indeed. Setting `setSearchType(SearchType.SCAN)` give no results.

The test case I used did not use `SearchType.SCAN` because you mentioned that you would like to sort results, right? But scan type does not allow sorting. Did I misunderstand what you were looking for?

That said, you are right. I can reproduce what you saw. 

``` java
SearchResponse sr = client().prepareSearch()
        .setScroll("1m")
        .setSource(source).get();
// Works fine
assertThat(sr.getHits().getHits().length, greaterThan(0));

sr = client().prepareSearch()
        .setSearchType(SearchType.SCAN)
        .setScroll("1m")
        .setSource(source).get();
// fails
assertThat(sr.getHits().getHits().length, greaterThan(0));
```

Working on it ATM.
</comment><comment author="kul" created="2015-01-12T10:08:10Z" id="69549227">@dadoonet ,
Do you think fixing this would then introduce ambiguity in API as you can pass a query which requests sorting but you do not get sorted results?

Any tips for scrolling large sorted results efficiently?
Thanks
</comment><comment author="dadoonet" created="2015-01-12T10:12:05Z" id="69549644">Actually, this what we should expect.

If you look at the documentation [here](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-scroll.html#scroll-scan), we wrote that:

&gt; A scanning scroll request differs from a standard scroll request in three ways:
&gt; - Sorting is disabled. Results are returned in the order they appear in the index.
&gt; - Aggregations are not supported.
&gt; - The response of the initial search request **will not contain any results in the hits array**. The first results will be returned by the first scroll request.
&gt; - The size parameter controls the number of results per shard, not per request, so a size of 10 which hits 5 shards will return a maximum of 50 results per scroll request.

So what you need to do in Java is exactly the same thing as you need to do in REST:

``` java
SearchResponse sr = client.prepareSearch(INDEX).setSearchType(SearchType.SCAN).setScroll("1m").setSource(source).get();

// Use sr.getScrollId() to iterate over results:
sr = client.prepareSearchScroll(sr.getScrollId()).setScroll(TimeValue.timeValueMinutes(2)).get();

// sr.getHits().getHits() will give you the first hits
```

Is it what you are looking for?
</comment><comment author="dadoonet" created="2015-01-12T10:17:36Z" id="69550182">&gt; Any tips for scrolling large sorted results efficiently?

No. Not at this moment. You can not really scroll over very large sorted result set because it would require to allocate a lot of memory to sort all results per shard before sending the results.

See http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/scan-scroll.html

I think we can close this issue but feel free to reopen if I misunderstood what you are looking for and or add more comments.
</comment><comment author="kul" created="2015-01-12T10:19:52Z" id="69550395">Yes exactly my point, enabling this would allow something like

``` java
String source = "{\"query\":{..}, \"sort\": [...]}"
SearchResponse sr = client.prepareSearch(INDEX).setSearchType(SearchType.SCAN).setScroll("1m").setSource(source).get();
```

So we can pass a sort query but still not get sorted results. I am OK with **the current** behavior too.

Thanks
</comment><comment author="clintongormley" created="2015-01-13T20:57:42Z" id="69818441">&gt; No. Not at this moment. You can not really scroll over very large sorted result set because it would require to allocate a lot of memory to sort all results per shard before sending the results.

Actually, since v1.2(?) scrolling sorted results is much more efficient. You can now do deep scrolling of sorted queries, just don't use the `SCAN` search type.
</comment><comment author="dadoonet" created="2015-01-13T21:06:01Z" id="69819816">Thanks @clintongormley for correcting my answer.
</comment><comment author="kul" created="2015-01-14T03:47:08Z" id="69865155">Thanks @clintongormley and @dadoonet !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix support for named filters/queries inside nested filters.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6293</link><project id="" key="" /><description>Named filters/queries inside nested queries/filters were being silently ignored.
</description><key id="34164550">6293</key><summary>Fix support for named filters/queries inside nested filters.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Nested Docs</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T10:57:17Z</created><updated>2015-06-07T13:25:13Z</updated><resolved>2014-07-16T21:44:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-02T08:30:02Z" id="44812890">I left a bunch of comments
</comment><comment author="martijnvg" created="2014-06-05T19:26:30Z" id="45263048">@s1monw I updated the PR
</comment><comment author="s1monw" created="2014-07-09T19:50:59Z" id="48525323">LGTM
</comment><comment author="clintongormley" created="2014-07-11T09:07:44Z" id="48709399">@martijnvg can we get this merged in?
</comment><comment author="martijnvg" created="2014-07-16T10:31:12Z" id="49147645">@clintongormley yes, we can get this merged in. I will do it today.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Function score without function throws NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6292</link><project id="" key="" /><description>```
PUT /t/test/1
{
  "text": "baseball bats"
}

GET /t/test/_search?explain
{
  "query": {
    "function_score": {
      "score_mode": "sum",
      "boost_mode": "replace",
      "functions": [
        {
          "filter": {
            "term": {
              "text": "baseball"
            }
          }
        }
      ]
    }
  }
}
```

Throws NPE.  With a function (eg boost_factor) it doesn't:

```
GET /t/test/_search?explain
{
  "query": {
    "function_score": {
      "score_mode": "sum",
      "boost_mode": "replace",
      "functions": [
        {
          "filter": {
            "term": {
              "text": "baseball"
            }
          }
        },
        "boost_factor": 1
      ]
    }
  }
}
```
</description><key id="34158272">6292</key><summary>Query DSL: Function score without function throws NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.2.3</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-23T09:21:46Z</created><updated>2014-07-11T11:56:02Z</updated><resolved>2014-07-11T11:56:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-23T12:06:36Z" id="44001353">I guess we should throw a parse exception here no?
</comment><comment author="s1monw" created="2014-05-23T12:07:03Z" id="44001386">ah @brwe  has is already assigned... I almost took it... :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[docs] Add imap river and security plugin and community supported MSI installer to docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6291</link><project id="" key="" /><description>Add imap river and security plugin to docs
Add community supported MSI installer to docs
</description><key id="34150674">6291</key><summary>[docs] Add imap river and security plugin and community supported MSI installer to docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">salyh</reporter><labels><label>docs</label></labels><created>2014-05-23T07:12:29Z</created><updated>2014-06-16T20:21:07Z</updated><resolved>2014-06-03T09:01:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T09:01:47Z" id="44940298">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding `Exceptions` utility to enable code contracts that fail fast</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6290</link><project id="" key="" /><description>Provides `Exceptions` utility to enable fail fast behavior with minimal boilerplate code.

Closes #6289
</description><key id="34144874">6290</key><summary>Adding `Exceptions` utility to enable code contracts that fail fast</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pickypg</reporter><labels><label>:Exceptions</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2014-05-23T04:41:19Z</created><updated>2015-10-13T18:33:10Z</updated><resolved>2015-10-13T18:33:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-10-15T09:15:12Z" id="59178343">It seems weird to me that the methods in Exceptions return the value. I would expect it to work like the Preconditions class in Guava [1] where the method has no return type and throws the exception if the condition is not met. So the example in your JavaDoc for the class would become:

```
// check required arguments
Exceptions.ifNull(value1, "value1 cannot be null");
Exceptions.ifEmpty(value2, "value2 cannot be empty");

this.value1 = value1;
this.value2 = value2;
this.value3 = value3;
```

This seems more intuitive to me since the only thing I would naturally expect a method named `Exceptions.ifNull()` to return, if anything at all, would be an exception which I can then throw. Also it nicely separates the checking of the arguments from any logic required since is could start to look messy if, for example, `value1` is a List and you create a new list object from the method argument.  In the current way you would get:

```
this.value1 = new ArrayList&lt;&gt;(Exceptions.ifNull(value1, "value1 cannot be null"));
```

Whereas I think the following would be easier to follow:

```
Exceptions.ifNull(value1, "value1 cannot be null");

this.value1 = new ArrayList&lt;&gt;(value1);
```

I would also recommend naming the class `Preconditions` like the Guava equivalent.

What do you think?

[1] https://code.google.com/p/guava-libraries/wiki/PreconditionsExplained
</comment><comment author="pickypg" created="2014-10-15T20:04:30Z" id="59266998">@colings86  Back when I wrote the first variant of this, I did not return anything, but I almost _always_ used the values directly in assignments. This led to the current form of the utility.

Having the return does not force you to use it, and I have personally always preferred doing your exact example even with the return available. It just reduces the amount of code necessary for the approach that I have found to be more common.

I did not want to name it `Preconditions` because I did not want to compete with it in any existing code (granted static imports would solve that), but I see no other reason to _not_ rename it.
</comment><comment author="clintongormley" created="2014-10-20T13:20:08Z" id="59752088">@javanna - please could you review this
</comment><comment author="javanna" created="2014-11-12T11:07:39Z" id="62702729">I like this! I tend to agree with @colings86 comments though. Although you save a line of code by returning a value, that doesn't mean that the code becomes more readable and intuitive. I'm leaning towards not returning any value as Colin suggested.
Also, I'd love to see some more commits added to this PR where these new tools get introduced to the codebase. Personally I think we have to see it applied to better judge the advantages and this should not be done afterwards.
</comment><comment author="clintongormley" created="2014-12-30T19:05:50Z" id="68386200">@pickypg any movement on this?
</comment><comment author="pickypg" created="2014-12-30T23:42:53Z" id="68411311">None, but I should have some free time soon (famously said many times).
</comment><comment author="javanna" created="2015-03-21T08:42:48Z" id="84282949">Hi @pickypg do you have time now? If not I am happy to take this over, you let me know ;)
</comment><comment author="pickypg" created="2015-04-06T15:40:02Z" id="90114660">@javanna I think I'm going to have to pass this to you. Thanks! Let me know if you have any questions (doubt it).
</comment><comment author="javanna" created="2015-05-28T09:09:48Z" id="106242758">I had another look at this to refresh my memory, I still think it is a good addition but it requires some more work, addressing comments review and adapting existing code to use the new api, to make sure it works out well. That is why I am marking it as adoptme and low-hanging-fruit. 
</comment><comment author="clintongormley" created="2015-10-13T18:33:10Z" id="147805604">Closing for lack of interest :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception management for internal code contracts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6289</link><project id="" key="" /><description>Exception management for some of the internal code is repetitive across the entire codebase, requiring a lot of boilerplate code that is sensitive to copy/paste issues as well as a lot of unintended `NullPointerException`s in places that mean to check for `null` while checking other conditions. I propose creating a utility to handle this boilerplate for the most common cases.

In the forthcoming pull request, I have created an `Exceptions` utility that works like the `Preconditions` utility in Guava (and others that I have used/written), which just gives a very simple and readable mechanism for failing fast with expected exceptions (in this case, `ElasticsearchIllegalArgumentException`s). I have been using things like this for years on projects and I have found that they make code shorter, more readable and more predictable.

It turns code like:

```
if (value1 == null) {
  throw new ElasticsearchIllegalArgumentException("value1 cannot be null");
}
if (Strings.isNullOrEmpty(value2)) {
  throw new ElasticsearchIllegalArgumentException("value2 cannot be empty");
}
if (value3 &lt; 0) {
  throw new ElasticsearchIllegalArgumentException("value3 cannot be negative");
}

// required
this.value1 = value1;
this.value2 = value2;
this.value3 = value3;
// optional
this.value4 = value4;
```

into:

```
// required
this.value1 = Exceptions.ifNull(value1, "value1 cannot be null");
this.value2 = Exceptions.ifEmpty(value2, "value2 cannot be empty");
this.value3 = Exceptions.ifNegative(value3, "value3 cannot be negative");
// optional
this.value4 = value4;
```

I also tend to use a few other functions not specified by `Preconditions`, such as inclusive and exclusive range checks and latitude/longitude checks, which I have implemented here. If they are actually embraced, it should make a simple way to provide predictable code contracts with minimal code.
</description><key id="34144480">6289</key><summary>Exception management for internal code contracts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/pickypg/following{/other_user}', u'events_url': u'https://api.github.com/users/pickypg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/pickypg/orgs', u'url': u'https://api.github.com/users/pickypg', u'gists_url': u'https://api.github.com/users/pickypg/gists{/gist_id}', u'html_url': u'https://github.com/pickypg', u'subscriptions_url': u'https://api.github.com/users/pickypg/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1501235?v=4', u'repos_url': u'https://api.github.com/users/pickypg/repos', u'received_events_url': u'https://api.github.com/users/pickypg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/pickypg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'pickypg', u'type': u'User', u'id': 1501235, u'followers_url': u'https://api.github.com/users/pickypg/followers'}</assignee><reporter username="">pickypg</reporter><labels /><created>2014-05-23T04:33:04Z</created><updated>2015-10-14T16:35:05Z</updated><resolved>2015-10-14T16:35:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-10-14T16:35:05Z" id="148108802">Closing as nobody has shown the interest to pick this up
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>improve error when mlockall fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6288</link><project id="" key="" /><description>The current text is not good here.
</description><key id="34125799">6288</key><summary>improve error when mlockall fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmuir</reporter><labels /><created>2014-05-22T21:55:58Z</created><updated>2014-07-11T15:17:50Z</updated><resolved>2014-05-23T14:16:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-22T21:56:25Z" id="43949172">LGTM
</comment><comment author="nik9000" created="2014-05-22T22:02:05Z" id="43949688">+1.  Run as root is bad.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update repositories.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6287</link><project id="" key="" /><description>upgrade to 1.2 to be consistent with the default version on the download page.
</description><key id="34109124">6287</key><summary>Update repositories.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ghost</reporter><labels><label>docs</label></labels><created>2014-05-22T18:34:44Z</created><updated>2014-06-25T22:20:24Z</updated><resolved>2014-06-03T10:31:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T10:31:27Z" id="44947820">Merged, thanks!

FYI I updated the version in a couple of more lines in the same page.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limit guava caches to 31.9GB</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6286</link><project id="" key="" /><description>Guava's caches have overflow issues around 32GB with our default segment
count of 16 and weight of 1 unit per byte.  We give them 100MB of headroom
so 31.9GB.

This limits the sizes of both the field data and filter caches, the two
large guava caches.

Closes #6268
</description><key id="34100217">6286</key><summary>Limit guava caches to 31.9GB</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2014-05-22T16:44:41Z</created><updated>2014-06-14T00:54:31Z</updated><resolved>2014-05-22T22:45:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-22T16:49:46Z" id="43914202">I put the limit directly into ByteSizeValue because I couldn't really think of a better place for it.  I also couldn't think of a good unit test for it.

I did review guava and concur with the overflow.  It looks to me like you need to give it enough room so that it doesn't overflow the int while it is adding things.  I figured 100MB would do.
</comment><comment author="jpountz" created="2014-05-22T16:58:17Z" id="43915258">Something that would be nice would be to have an assertion or a unit test somewhere that ensures that there is an overflow happening in the guava cache so that we don't forget to remove this work-around when the upstream bug is fixed.
</comment><comment author="nik9000" created="2014-05-22T16:58:45Z" id="43915312">I've just realized that in both cases there is a string called "size" that is a companion to the sizeInBytes that I modify.  In both cases I don't modify the string.  That feels funky.
</comment><comment author="nik9000" created="2014-05-22T16:58:52Z" id="43915330">&gt; Something that would be nice would be to have an assertion or a unit test somewhere that ensures that there is an overflow happening in the guava cache so that we don't forget to remove this work-around when the upstream bug is fixed.

Sure.  Let me do that.
</comment><comment author="nik9000" created="2014-05-22T21:57:46Z" id="43949289">Added test case and "fixed" the size string every time I changed the size in bytes.
</comment><comment author="jpountz" created="2014-05-22T22:07:09Z" id="43950140">Left one comment about the fact that we should probably not update the (String) size in `computeSizeInBytes` but other than that, it looks good to me. Thanks for the test!
</comment><comment author="nik9000" created="2014-05-22T22:10:10Z" id="43950379">Just updated.
</comment><comment author="jpountz" created="2014-05-22T22:45:40Z" id="43953173">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update doc to match changes in IndexResponse API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6285</link><project id="" key="" /><description>IndexResponse.id() -&gt;  IndexResponse.getId()
...
</description><key id="34098137">6285</key><summary>Update doc to match changes in IndexResponse API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">stephlag</reporter><labels><label>docs</label></labels><created>2014-05-22T16:20:42Z</created><updated>2014-06-26T09:41:29Z</updated><resolved>2014-06-03T11:49:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T10:33:09Z" id="44947974">Good point, could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so that we can merge this in?
</comment><comment author="stephlag" created="2014-06-03T11:40:08Z" id="44953564">Done
</comment><comment author="javanna" created="2014-06-03T11:49:19Z" id="44954604">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correcting typo in Javadoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6284</link><project id="" key="" /><description>Replacing
Note: the number of objects passed to this method must be and even number.
With
Note: the number of objects passed to this method must be an even number.
</description><key id="34097645">6284</key><summary>Correcting typo in Javadoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">stephlag</reporter><labels><label>docs</label></labels><created>2014-05-22T16:15:28Z</created><updated>2014-07-03T09:21:53Z</updated><resolved>2014-06-03T11:49:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pickypg" created="2014-05-25T04:22:52Z" id="44111147">LGTM
</comment><comment author="javanna" created="2014-06-03T11:17:41Z" id="44951465">Hi @stephlag could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this in?
</comment><comment author="stephlag" created="2014-06-03T11:41:33Z" id="44953722">Done
</comment><comment author="javanna" created="2014-06-03T11:49:32Z" id="44954621">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Registering a custom parser to be used an instance per thread</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6283</link><project id="" key="" /><description>Hi, as [requested on Twitter](https://twitter.com/kimchy/status/469261081136103424), I'm describing an issue I encountered yesterday.

I registered a custom query parser, relevant snippets:

```
// in RegisterJsonQueryParser.java: 
indicesQueriesRegistry.addQueryParser(parser); 

// in SirenJsonParserModule.java:
bind(RegisterJsonQueryParser.class); 

// in SirenPlugin.java:
module.addQueryParser(SirenJsonParser.NAME, SirenJsonParser.class); 
```

When issuing a query, I see a lot of different exceptions (ConcurrentModificationException, NullPointerException, ...). The problem is that `SirenJsonParser` is not thread-safe and ElasticSearch uses a single instance of it from different threads.

In comparison, this was never a problem in Solr.

Should I register my parser differently? I looked at the API but didn't notice anything relevant.

For now I'm creating a new parser in each SirenJsonParser.parse() call.

I'm using ElasticSearch `1.1.1`.
</description><key id="34084292">6283</key><summary>Registering a custom parser to be used an instance per thread</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jkot</reporter><labels /><created>2014-05-22T14:01:50Z</created><updated>2014-05-23T10:29:23Z</updated><resolved>2014-05-23T10:03:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-22T14:10:07Z" id="43892800">now that I understand the issue more, then this is by design. What state do you need to store on the parser level?
</comment><comment author="rendel" created="2014-05-22T17:52:10Z" id="43921510">Hi Shai,

In our scenario, a query is a json object with one or more lucene's boolean expression.
The problem is that our query parser includes an instance of the Lucene's flexible StandardQueryParser, which is used to parse the boolean expressions. Before delegating the parsing of the boolean expression to the instance of the StandardQueryParser, its configuration (e.g., StandardQueryConfigHandler) is modified according to some logic (i.e., each boolean expression in the same query can have different configuration). Therefore, the state we need to store is the configuration of the StandardQueryParser.

One quick solution on our side for the moment is just to create a new query parser for each query. We don't think this will have a huge impact on performance in most of the cases.
</comment><comment author="s1monw" created="2014-05-22T21:07:51Z" id="43944193">we use fixed thread pools so you can just use a ThreadLocal to reuse your parser instances.
</comment><comment author="kimchy" created="2014-05-23T09:51:29Z" id="43990419">right, as @s1monw suggested, use the relevant data structures you use as thread locals in the `SirenJsonParser`.
</comment><comment author="jkot" created="2014-05-23T10:03:42Z" id="43991326">that's what we will likely do, thanks for the info!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Issue with polygons near date line</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6282</link><project id="" key="" /><description>If a polygon is constructed which overlaps the date line but has a hole which lies entirely one to one side of the date line, JTS error saying that the hole is not within the bounds of the polygon because the code which splits the polygon either side of the date line does not add the hole to the correct component of the final set of polygons.  The fix ensures this selection happens correctly.

Closes #6179
</description><key id="34082657">6282</key><summary>Issue with polygons near date line</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Geo</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-22T13:43:57Z</created><updated>2015-06-07T20:06:21Z</updated><resolved>2014-06-02T14:09:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-06-02T13:45:32Z" id="44838293">LGTM
</comment><comment author="colings86" created="2014-06-02T14:09:23Z" id="44841070">Merged into master and 1.x but the PR did not automatically close so closing manually
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove the `indices_boost` URL param to search as it doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6281</link><project id="" key="" /><description>As reported in elasticsearch/elasticsearch-ruby#29, the `indices_boost` specified as the URL parameter seems to be ignored.

Relevant source code:

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/rest/action/search/RestSearchAction.java#L230

I've removed it from the JSON specs in 81cddacffa5e19c5de91ca991a6a9d23a1dec736.
</description><key id="34082461">6281</key><summary>Remove the `indices_boost` URL param to search as it doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">karmi</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-22T13:41:53Z</created><updated>2015-06-08T00:29:18Z</updated><resolved>2015-03-21T08:52:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Test: Randomly disable the filter cache.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6280</link><project id="" key="" /><description>"Randomize all the things"

Relates to #6278 and #6279
</description><key id="34081386">6280</key><summary>Test: Randomly disable the filter cache.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-22T13:29:37Z</created><updated>2014-09-03T22:53:03Z</updated><resolved>2014-05-22T21:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-22T21:07:19Z" id="43944138">nice randomization! LGTM
</comment><comment author="s1monw" created="2014-05-22T21:11:40Z" id="43944602">good stuff!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested: queries/filters/aggregations expect FixedBitSets, yet it isn't the case with NoneFilterCache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6279</link><project id="" key="" /><description>The nested queries, filters and aggregations expect that filters produce `FixedBitSet` instances. However, if the filter cache is disabled, you might get an index-based `DocsEnum` directly, so the set of documents needs to be loaded into a `FixedBitSet` before running the query.
</description><key id="34078787">6279</key><summary>Nested: queries/filters/aggregations expect FixedBitSets, yet it isn't the case with NoneFilterCache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.1.3</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-22T12:59:21Z</created><updated>2014-05-30T08:32:05Z</updated><resolved>2014-05-22T21:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggregations: `ReverseNestedAggregator` does not compute parent documents correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6278</link><project id="" key="" /><description>In order to compute parent documents based on a child document, `ReverseNestedAggregator` does the following:

``` java
int parentDoc = parentDocs.advance(childDoc);
```

But the behavior of `advance` is undefined when the target is less than or equal to the target document, which can happen if you have 2 matching child documents that have the same parent.

This works fine in most cases when the filter cache is enabled since `FixedBitSet` is permissive and would go the the next set bit after `childDoc`. But if the filter cache is disabled then you might run into trouble.
</description><key id="34076724">6278</key><summary>Aggregations: `ReverseNestedAggregator` does not compute parent documents correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-22T12:30:23Z</created><updated>2014-05-30T08:31:43Z</updated><resolved>2014-05-22T21:19:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Linux launch script : ES_HEAP_SIZE environment variable documented but not used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6277</link><project id="" key="" /><description>In the linux lanch script (bin/elasticsearch) the header documents the ES_HEAP_SIZE environment variable but there is no occurence of such a variable in the actual script.

The Windows launch script makes proper use of it though.

So either it should be removed from the header, and one should use JAVA_OPTS instead to specify memory allocation, or Windows and Linux scripts should be harmonized.
</description><key id="34073124">6277</key><summary>Linux launch script : ES_HEAP_SIZE environment variable documented but not used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicosensei</reporter><labels /><created>2014-05-22T11:33:54Z</created><updated>2014-05-22T12:29:57Z</updated><resolved>2014-05-22T12:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-22T12:29:57Z" id="43881390">`bin/elasticsearch` includes `bin/elasticsearch.in.sh` which does reference `ES_HEAP_SIZE`:

https://github.com/elasticsearch/elasticsearch/blob/master/bin/elasticsearch.in.sh#L11
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElastcSearch Help Request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6276</link><project id="" key="" /><description>I have set up a 3 VMs using Vagrant, and ansible (w/docker)

In my Hosts file:

I added these lines:
192.168.50.10 earth
192.168.50.11 hiveholm
192.168.50.12 moth

This just gives the private address of the VM a name

I use Vagrant to build and map 3 VM's on my Mac:

Vagrantfile:

```
# -*- mode: ruby -*-
# vi: set ft=ruby :
ROOT = File.dirname(File.absolute_path(__FILE__))

# Vagrantfile API/syntax version. Don't touch unless you know what you're doing!
VAGRANTFILE_API_VERSION = '2'

# Default env properties which can be overridden
# Example overrides:
#   echo "ENV['PASSENGER_DOCKER_PATH'] ||= '../../phusion/passenger-docker'   " &gt;&gt; ~/.vagrant.d/Vagrantfile
#   echo "ENV['BASE_BOX_URL']          ||= 'd\:/dev/vm/vagrant/boxes/phusion/'" &gt;&gt; ~/.vagrant.d/Vagrantfile
BASE_BOX_URL          = ENV['BASE_BOX_URL']    || 'https://oss-binaries.phusionpassenger.com/vagrant/boxes/'
VAGRANT_BOX_URL       = ENV['VAGRANT_BOX_URL'] || BASE_BOX_URL + 'ubuntu-12.04.3-amd64-vbox.box'
VMWARE_BOX_URL        = ENV['VMWARE_BOX_URL']  || BASE_BOX_URL + 'ubuntu-12.04.3-amd64-vmwarefusion.box'
BASEIMAGE_PATH        = ENV['BASEIMAGE_PATH' ] || '.'
PASSENGER1_DOCKER_PATH = ENV['PASSENGER1_PATH' ] || '../vm1'
PASSENGER2_DOCKER_PATH = ENV['PASSENGER2_PATH' ] || '../vm2'
PASSENGER3_DOCKER_PATH = ENV['PASSENGER3_PATH' ] || '../vm3'

DOCKERIZER_PATH       = ENV['DOCKERIZER_PATH'] || '../dockerizer'

$script = &lt;&lt;SCRIPT
wget -q -O - https://get.docker.io/gpg | apt-key add -
echo deb http://get.docker.io/ubuntu docker main &gt; /etc/apt/sources.list.d/docker.list
apt-get update -qq
apt-get install -q -y --force-yes lxc-docker
usermod -a -G docker vagrant
docker version
su - vagrant -c 'echo alias d=docker &gt;&gt; ~/.bash_aliases'
SCRIPT


class MyInstaller &lt; VagrantVbguest::Installers::Linux
  def install(opts=nil, &amp;block)
    super
    communicate.sudo('ln -s /opt/VBoxGuestAdditions-*/lib/VBoxGuestAdditions /usr/lib', opts, &amp;block)
  end
end

Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  config.vbguest.installer = MyInstaller

  config.vm.define :vm1 do |vm1_config|


    vm1_config.vm.box = 'phusion1-open-ubuntu-12.04-amd64'
    vm1_config.vm.box_url = VAGRANT_BOX_URL
    vm1_config.ssh.forward_agent = true

    passenger_docker_path = File.absolute_path(PASSENGER1_DOCKER_PATH, ROOT)
    if File.directory?(passenger_docker_path)
      vm1_config.vm.synced_folder passenger_docker_path, '/vagrant/passenger-docker'
    end

    baseimage_path = File.absolute_path(BASEIMAGE_PATH, ROOT)
    if File.directory?(baseimage_path)
      vm1_config.vm.synced_folder baseimage_path, "/vagrant/baseimage-docker"
    end

    dockerizer_path = File.absolute_path(DOCKERIZER_PATH, ROOT)
    if File.directory?(dockerizer_path)
      vm1_config.vm.synced_folder dockerizer_path, '/vagrant/dockerizer'
    end

    vm1_config.vm.provider :vmware_fusion do |f, override|
      override.vm.box_url = VMWARE_BOX_URL
      f.vmx['displayName'] = 'baseimage-docker'
    end

    #vm1_config.vm.network :forwarded_port, guest: 22, host: 22222
    #vm1_config.vm.network :forwarded_port, guest: 45678, host: 56789
    #vm1_config.vm.network :forwarded_port, guest: 54328, host: 55900

    if Dir.glob("#{File.dirname(__FILE__)}/.vagrant/machines/default/*/id").empty?
      vm1_config.vm.provision :shell, :inline =&gt; $script
    end

    #vm1_config.vm.network :hostonly, ip: "192.168.50.10"
    vm1_config.vm.network "private_network", ip: "192.168.50.10"

  end

  config.vm.define :vm2 do |vm2_config|

    vm2_config.vm.box = 'phusion1-open-ubuntu-12.04-amd64'
    vm2_config.vm.box_url = VAGRANT_BOX_URL
    vm2_config.ssh.forward_agent = true

    passenger_docker_path = File.absolute_path(PASSENGER2_DOCKER_PATH, ROOT)
    if File.directory?(passenger_docker_path)
      vm2_config.vm.synced_folder passenger_docker_path, '/vagrant/passenger-docker'
    end

    baseimage_path = File.absolute_path(BASEIMAGE_PATH, ROOT)
    if File.directory?(baseimage_path)
      vm2_config.vm.synced_folder baseimage_path, "/vagrant/baseimage-docker"
    end
    dockerizer_path = File.absolute_path(DOCKERIZER_PATH, ROOT)
    if File.directory?(dockerizer_path)
      vm2_config.vm.synced_folder dockerizer_path, '/vagrant/dockerizer'
    end

    vm2_config.vm.provider :vmware_fusion do |f, override|
      override.vm.box_url = VMWARE_BOX_URL
      f.vmx['displayName'] = 'baseimage-docker'
    end

    #vm2_config.vm.network :forwarded_port, guest: 22, host: 22223
    #vm2_config.vm.network :forwarded_port, guest: 45678, host: 56889
    #vm2_config.vm.network :forwarded_port, guest: 54328, host: 55901

    if Dir.glob("#{File.dirname(__FILE__)}/.vagrant/machines/default/*/id").empty?    
      vm2_config.vm.provision :shell, :inline =&gt; $script
    end

    #vm2_config.vm.network :hostonly, ip: "192.168.50.11"
    vm2_config.vm.network "private_network", ip: "192.168.50.11"

  end

  config.vm.define :vm3 do |vm3_config|


    vm3_config.vm.box = 'phusion1-open-ubuntu-12.04-amd64'
    vm3_config.vm.box_url = VAGRANT_BOX_URL
    vm3_config.ssh.forward_agent = true

    passenger_docker_path = File.absolute_path(PASSENGER3_DOCKER_PATH, ROOT)
    if File.directory?(passenger_docker_path)
      vm3_config.vm.synced_folder passenger_docker_path, '/vagrant/passenger-docker'
    end

    baseimage_path = File.absolute_path(BASEIMAGE_PATH, ROOT)
    if File.directory?(baseimage_path)
      vm3_config.vm.synced_folder baseimage_path, "/vagrant/baseimage-docker"
    end

    dockerizer_path = File.absolute_path(DOCKERIZER_PATH, ROOT)
    if File.directory?(dockerizer_path)
      vm3_config.vm.synced_folder dockerizer_path, '/vagrant/dockerizer'
    end

    vm3_config.vm.provider :vmware_fusion do |f, override|
      override.vm.box_url = VMWARE_BOX_URL
      f.vmx['displayName'] = 'baseimage-docker'
    end

    #vm3_config.vm.network :forwarded_port, guest: 22, host: 22224
    #vm3_config.vm.network :forwarded_port, guest: 45678, host: 56989
    #vm3_config.vm.network :forwarded_port, guest: 54328, host: 55902

    if Dir.glob("#{File.dirname(__FILE__)}/.vagrant/machines/default/*/id").empty?
      vm3_config.vm.provision :shell, :inline =&gt; $script
    end

    #vm3_config.vm.network :hostonly, ip: "192.168.50.12"
    vm3_config.vm.network "private_network", ip: "192.168.50.12"

  end


end
```

This builds and runs 3 VM's using the phusion baseimahe-docker base:
I then run anansible playbook to setup the VM's so they have their own sshd
ansible-playbook (terra1.yml)

```
- name: terra
  hosts: all
  # user: $user
  remote_user: root
  sudo: yes

  gather_facts: yes
  tasks:    
    - name: install pip
      shell: apt-get -y install python-pip

    - name: install docker-py
      pip: name=docker-py state=present

    - name: Do the docker build
      shell: cd /vagrant/image &amp;&amp; docker build -t traintracks_image_9 .

    - name: Run the container
      docker: ports=45678:22,54328:54328,9200:9200,9201:9201,9202:9202,9300:9300,9301:9301,9302:9302 name=test_traintracks_image_9g image=traintracks_image_9
```

 this installs a couple of things and builds the base image inside the container with the build step and then runs it.
 (Note this is applied to all 3 VMs but in this testing I am just building out the first vm (earth))

Last step install and setup and run ElastciSearch:

 I have reused the "the-ansibles" project playbook as a base for out playbook to setup and run elastic search
Project is called traintracks
ansible-playbook: terra_es1.yml:
# file: terra_es1.yml
- name: terra
    hosts: all
  
    remote_user: root
    sudo: yes
  
    gather_facts: yes
  
    vars_files:
      - host_vars/earth
  
    roles:
      - role: auth
        tags: step Begin
      - role: common
        tags: step Common
      - role: security
        tags: Step Security
      - role: ssl
        tags: step SSL
      - role: monit
        tags: step MONIT
      - role: python
        tags: step PYTHON
      - role: jdk
        tags: step JDK
      - role: elasticsearch
        tags: step ElasticSearch

installs and sets up all the required packages and finally installs elastic search

All this steps seem to SUCCEED or more correctly show no errors and we basically have not changed anything. 
But when you go into the docker container (ssh root@earth -p 45678)
and look at the supervisord log and the elasticsearch log it shows this problem it shows this error:

[2014-05-22 10:15:57,306][INFO ][node                     ] [Apryll] version[0.90.5], pid[2456], build[c8714e8/2013-09-17T12:50:20Z]
[2014-05-22 10:15:57,307][INFO ][node                     ] [Apryll] initializing ...
[2014-05-22 10:15:57,313][INFO ][plugins                  ] [Apryll] loaded [], sites []
[2014-05-22 10:15:59,390][INFO ][node                     ] [Apryll] initialized
[2014-05-22 10:15:59,390][INFO ][node                     ] [Apryll] starting ...
[2014-05-22 10:15:59,467][ERROR][bootstrap                ] [Apryll] {0.90.5}: Startup Failed ...
- BindTransportException[Failed to bind to [9300]]
  ChannelException[Failed to bind to: /192.168.50.10:9300]
      BindException[Cannot assign requested address]

I run netstat andit shows nothing is using the 9300 port but ES fails to bind it!

netstat -lc
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State  
tcp        0      0 82346d809c2f:domain     _:_                     LISTEN  
tcp        0      0 _:ssh                   *:_                     LISTEN  
tcp6       0      0 [::]:ssh                [::]:\*                  LISTEN  
udp        0      0 82346d809c2f:domain     _:_  
udp        0      0 82346d809c2f:ntp        _:_  
udp        0      0 localhost:ntp           _:_  
udp        0      0 _:ntp                   *:_  
udp6       0      0 fe80::34db:98ff:fe1:ntp [::]:\*  
udp6       0      0 localhost:ntp           [::]:\*  
udp6       0      0 [::]:ntp                [::]:\*  
Active UNIX domain sockets (only servers)
Proto RefCnt Flags       Type       State         I-Node   Path
unix  2      [ ACC ]     STREAM     LISTENING     55239    @/tmp/fam-root-
unix  2      [ ACC ]     STREAM     LISTENING     55224    /var/run/fail2ban/fail2ban.sock

This seems to indicate that nothing is using port 9300
but I run from the command link inside the docker Container:

So I killed supervisord and ran the elasticsearch command from rthe ssh shell

/usr/local/etc/elasticsearch/bin/elasticsearch -f

I GET THIS:
root@82346d809c2f:~# /usr/local/etc/elasticsearch/bin/elasticsearch -f
[2014-05-22 10:42:03,447][INFO ][node                     ] [Fan Boy] version[0.90.5], pid[11886], build[c8714e8/2013-09-17T12:50:20Z]
[2014-05-22 10:42:03,448][INFO ][node                     ] [Fan Boy] initializing ...
[2014-05-22 10:42:03,454][INFO ][plugins                  ] [Fan Boy] loaded [], sites []
[2014-05-22 10:42:05,564][INFO ][node                     ] [Fan Boy] initialized
[2014-05-22 10:42:05,564][INFO ][node                     ] [Fan Boy] starting ...
[2014-05-22 10:42:05,639][ERROR][bootstrap                ] [Fan Boy] {0.90.5}: Startup Failed ...
- BindTransportException[Failed to bind to [9300]]
  ChannelException[Failed to bind to: /192.168.50.10:9300]
      BindException[Cannot assign requested address]

But I dont get more information 

Any help would be appreciated!

Jeff Wilson
heisenberg@traintracks.io
</description><key id="34071275">6276</key><summary>ElastcSearch Help Request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">HeisenbergTT</reporter><labels /><created>2014-05-22T11:04:09Z</created><updated>2014-05-22T12:27:16Z</updated><resolved>2014-05-22T12:27:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-22T12:27:16Z" id="43881124">Hi @HeisenbergTT 

This issues list is for bugs and feature requests in Elasticsearch.  Please ask support questions in the forum instead.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replaced `exclude` with `include` to avoid double negation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6275</link><project id="" key="" /><description>...negation when set.

'exclude' is now marked as deprecated.

Closes #6248
</description><key id="34068983">6275</key><summary>Replaced `exclude` with `include` to avoid double negation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.3.0</label></labels><created>2014-05-22T10:28:32Z</created><updated>2015-06-07T13:26:42Z</updated><resolved>2014-05-27T08:05:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-22T21:07:54Z" id="43944206">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More Like This Query: replaced 'exclude' with 'include' to avoid double negation when set.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6274</link><project id="" key="" /><description>'exclude' is now marked as deprecated in 1.x.

Closes #6248
</description><key id="34068237">6274</key><summary>More Like This Query: replaced 'exclude' with 'include' to avoid double negation when set.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels /><created>2014-05-22T10:17:10Z</created><updated>2014-06-28T08:22:32Z</updated><resolved>2014-05-22T10:22:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>null buckets missing from terms aggregation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6273</link><project id="" key="" /><description>The terms aggregation creates a bucket for each value but doesn't include a `null` bucket. When there is a doc with a key set to `null` the cardinality is currently incorrect. 

Users may or may not want to include a `null` bucket so it should be configurable (default should be false in line with current behaviour), I propose

```
{
    "aggs" : {
        "genders" : {
            "terms" : { 
                "field" : "gender",
                "null": true
            }
        }
    }
}
```

And the response

```
{
    ...

    "aggregations" : {
        "genders" : {
            "buckets" : [
                {
                    "key" : "male",
                    "doc_count" : 10
                },
                {
                    "key" : "female",
                    "doc_count" : 10
                },
                {
                    "key" : null,
                    "doc_count" : 1  // indicates doc without gender field or field set to null
                },
            ]
        }
    }
}
```
</description><key id="34067633">6273</key><summary>null buckets missing from terms aggregation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">j0hnsmith</reporter><labels /><created>2014-05-22T10:08:24Z</created><updated>2015-05-15T14:41:21Z</updated><resolved>2015-05-15T14:41:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-22T12:26:03Z" id="43881007">You're looking for the [`missing` aggregation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-missing-aggregation.html#search-aggregations-bucket-missing-aggregation).

Note that there is an open issue (#5324) to add support for the `missing` bucket to all aggs 
</comment><comment author="j0hnsmith" created="2014-05-22T13:45:43Z" id="43889516">I'm _not_ looking for the `missing` aggregation (although that's what I'm using as a fallback), I want to be able to do a single aggregation that contains `'male'`, `'female'` and `null` keys. Perhaps there should also be an additional `_missing_key` term that includes docs that don't have a `gender` key. As @roytmana says in #5324, it shouldn't be difficult to get the sum of the doc counts for each term to add up to the total number of docs being considered.

What you're suggesting (I think) is to do two aggregations in the same search (my fallback solution), one `term` aggregation and one `missing` aggregation. This is sub optimal and nothing more than a workaround, `null` is a valid term if the key exists so should be included in the `terms` aggregation.

The reason why this is so important is when applying a filter to an aggregation, it has to be applied twice, then all the sub aggregations have to be be done twice etc etc which ends up in a confusing mess. Then the results have to be juggled to get the dataset that contains the complete picture.

ElasticSearch is an excellent tool and aggregations are very powerful however in this case jumping through hoops shouldn't be necessary.
</comment><comment author="roytmana" created="2014-05-22T13:53:11Z" id="43890516">and I would love to see not just missing bucket but also _other bucket if requested. It is very important for many use cases to retain entire data set as stats get rolled up. With facets I did it buy using stats facet and subtracting sum of all buckets including _missing. It was not bad since facets do not allow sub-aggs and I had a nice API for it which hid all the complexities. but with  sub-aggs it will be very ugly
</comment><comment author="clintongormley" created="2014-05-22T13:53:35Z" id="43890592">Actually, `null` is not a valid term. There is no way of storing the term `null` in Lucene.  Instead, that field has no value. Similarly, if you store: `["foo","bar",null]` then the field will have two values: `foo` and `bar`, not 3.

If you want to be able to distinguish a missing value from an explicit `null`, then you should use the `null_value` mapping on the field to substitute some real term when ES encounters a  `null`.
See [Dealing with null values](http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_dealing_with_null_values.html#_dealing_with_null_values) for more.

But yes, for convenience's sake and efficiency, the missing count should probably be included in the agg itself, as requested in #5324, so I'll close this issue in favour of that one.
</comment><comment author="roytmana" created="2014-05-22T14:02:32Z" id="43891846">null_value mapping won't help much as null can be introduced at object level (say you agg on person's country but his/her entire address is missing. I do not believe ES can handle that. I tried to use null_value but it only works when the actual scalar value is null not one of its owning objects

Please not to consider it to be a convenience. lack of _missing and  _other buckets is a major limitation when implementing dynamic system with user ad-hoc defined analytic, pivot tables etc
</comment><comment author="j0hnsmith" created="2014-05-22T14:34:14Z" id="43896137">@clintongormley "There is no way of storing the term null in Lucene."

Aha, now many things make sense to me, thanks for the explanation.

Given field values of `["foo","bar",null]`, even if Lucene only sees 2 values I'd like to see a configurable (but not default) way of ElasticSearch doing the work in the background to return the expected buckets then processing them accordingly. I understand that this may be far less performant that what currently exists however I believe it's a common enough case to be supported (and the performance difference is moot since if it's what a user wants they'll have to run the same queries anyway).
</comment><comment author="clintongormley" created="2014-05-22T14:37:40Z" id="43896650">@j0hnsmith The only way to make `["foo","bar",null]` store three values in the inverted index is to use `null_value` to map `null` to a concrete term, eg `_null_value_`.

Then it'll work just like all other terms and have its own bucket, and the `missing` agg will be just for those docs which have no value set for that field.
</comment><comment author="roytmana" created="2014-05-22T14:46:30Z" id="43897863">but as I said not if field is missing because its entire containing object is missing!
that solution will not work.
</comment><comment author="clintongormley" created="2014-05-22T14:47:59Z" id="43898092">@roytmana yes, but then you couldn't aggregate on the top level object anyway, as the "container" doesn't have an associated inverted index.  But it's a reasonably easy workaround to have a "null" object be represented by an object with a concrete field which contains a `null` value.
</comment><comment author="roytmana" created="2014-05-22T15:34:52Z" id="43904463">@clintongormley for a fairly deep graph (say three levels contract/party/address to compensate for party being null and still be able to aggregate on any party field including address fields (say party country) will require producing super ugly json for _source replacing blank party with party object with null substitution and every object type within party with its own null object with its own null values etc. And then people using _source will have to deal with it all in their application. I do it now in few cases but I would not want to do it all the time
</comment><comment author="jpountz" created="2015-05-15T14:41:19Z" id="102416550">Closing: with https://github.com/elastic/elasticsearch/pull/11042 you can now configure arbitrary keys for documents that are missing a value.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> [mlt] query does not support [ids]]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6272</link><project id="" key="" /><description>I recently update elasticsearch from 0.9x to 1.0.3, and i find more_like_this add a new parameters: ids from doc http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-mlt-query.html. However when i want to use it like:

```
{
  query: { 
    more_like_this: {         
      min_term_freq: 1,
      max_query_terms: 12,
      min_doc_freq: 1,
      ids: ["doc_id_xxxxx"]
    } 
  }
}
```

It returns: **nested: QueryParsingException[[index] [mlt] query does not support [ids]];**

but when changing ids to like_text, it works fine.

Thanks for any help.
</description><key id="34061384">6272</key><summary> [mlt] query does not support [ids]]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">crax</reporter><labels /><created>2014-05-22T08:35:16Z</created><updated>2014-05-23T08:02:55Z</updated><resolved>2014-05-23T08:02:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexksikes" created="2014-05-22T11:34:43Z" id="43876423">Make sure you update to 1.2.0 when it's out.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Won't re-allocate after update to 1.1.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6271</link><project id="" key="" /><description>We have a 2 nodes cluster which was running in 1.0.1

After updating to 1.1.1 on both machine. The cluster fails to re-allocate shards to one of the nodes.

If I create a new index, it gets allocated to both machines with proper sharding. If I then restart one of the node, no re-allocation happens.

I tried setting manually cluster.routing.allocation.enable to "all" on both node, to no avail.
</description><key id="34058916">6271</key><summary>Won't re-allocate after update to 1.1.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JimminiKin</reporter><labels /><created>2014-05-22T07:54:23Z</created><updated>2014-12-30T19:00:30Z</updated><resolved>2014-12-30T19:00:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T19:00:30Z" id="68385679">Hi @JimminiKin 

Sorry it has taken so long to get to this one.  I'm assuming the issue has resolved itself? We haven't seen other reports of this, so I'm guessing it was a configuration issue.

I'm going to close this ticket, but if you see something similar on a more recent version, please feel free to open another ticket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update network.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6270</link><project id="" key="" /><description>Fix typo
</description><key id="34056851">6270</key><summary>Update network.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">gauravarora</reporter><labels><label>docs</label></labels><created>2014-05-22T07:15:45Z</created><updated>2014-07-02T13:44:31Z</updated><resolved>2014-06-03T11:20:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T11:20:06Z" id="44951661">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index field names of documents.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6269</link><project id="" key="" /><description>The `exists` and `missing` filters need to merge postings lists of all existing
terms, which can be very costly, especially on high-cardinality fields. This
commit indexes the field names of a document under `_field_names` and uses it
to speed up the `exists` and `missing` filters.

This is only enabled for indices that are created on or after Elasticsearch
1.3.0.

Close #5659
</description><key id="34038695">6269</key><summary>Index field names of documents.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2014-05-21T23:47:40Z</created><updated>2014-06-19T10:25:29Z</updated><resolved>2014-06-19T10:25:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-06T08:26:37Z" id="45313906">I left some comments this will be a nice improvement!
</comment><comment author="jpountz" created="2014-06-13T09:36:00Z" id="45992726">@s1monw I just pushed a new commit and replied to your comments.
</comment><comment author="s1monw" created="2014-06-18T18:51:04Z" id="46477467">left one comment other than that LGTM
</comment><comment author="jpountz" created="2014-06-18T19:44:29Z" id="46484315">I'll fix the test. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Internal: Filter cache size limit not honored for 32GB or over</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6268</link><project id="" key="" /><description>Hi,

We are running an elasticsearch 1.1.1 6 node cluster with 256GB of ram, and using 96GB JVM heap sizes. I've noticed that when I set the filter cache size to 32GB or over with this command:

```
curl -XPUT "http://localhost:9200/_cluster/settings" -d'
{
    "transient" : {
       "indices.cache.filter.size" : "50%"
    }
}'
```

The field cache size keeps growing above and beyond the indicated limit. The relevant node stats show that the filter cache size is about 69GB in size, which is over the configured limit of 48GB

```
"filter_cache" : {
    "memory_size_in_bytes" : 74550217274,
    "evictions" : 8665179
},
```

I've enable debug logging on the node itself and it looks like the cache itself is getting created with the correct values:

```
[2014-05-21 00:31:57,215][DEBUG][indices.cache.filter     ] [ess02-006] using [node] weighted filter cache with size [50%], actual_size [47.9gb], expire [null], clean_interval [1m]
```

Whats strange is that when I set the limit to 31.9GB, the limit is enforced, which leads me to believe there is some sort of overflow going on.

Thanks,
Daniel
</description><key id="34007806">6268</key><summary>Internal: Filter cache size limit not honored for 32GB or over</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">danp60</reporter><labels><label>bug</label><label>v1.1.3</label><label>v1.2.1</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T17:56:07Z</created><updated>2014-10-03T22:42:04Z</updated><resolved>2014-05-22T22:44:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="danp60" created="2014-05-21T18:54:46Z" id="43799131">Hi,
I dug a little deeper into the caching logic, and I think I have found the root cause. The class `IndicesFilterCache` sets `concurrencyLevel` to a hardcoded 16:

``` java
private void buildCache() {
    CacheBuilder&lt;WeightedFilterCache.FilterCacheKey, DocIdSet&gt; cacheBuilder = CacheBuilder.newBuilder()
            .removalListener(this)
            .maximumWeight(sizeInBytes).weigher(new WeightedFilterCache.FilterCacheValueWeigher());

    // defaults to 4, but this is a busy map for all indices, increase it a bit
    cacheBuilder.concurrencyLevel(16);

    if (expire != null) {
        cacheBuilder.expireAfterAccess(expire.millis(), TimeUnit.MILLISECONDS);
    }

    cache = cacheBuilder.build();
}
```

https://github.com/elasticsearch/elasticsearch/blob/9ed34b5a9e9769b1264bf04d9b9a674794515bc6/src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java#L116

In the Guava libraries, the eviction code is as follows:

``` java
void evictEntries() {
    if (!map.evictsBySize()) {
        return;
    }

    drainRecencyQueue();
    while (totalWeight &gt; maxSegmentWeight) {
        ReferenceEntry&lt;K, V&gt; e = getNextEvictable();
        if (!removeEntry(e, e.getHash(), RemovalCause.SIZE)) {
            throw new AssertionError();
        }
    }
}
```

https://code.google.com/p/guava-libraries/source/browse/guava/src/com/google/common/cache/LocalCache.java#2659

Since `totalWeight` is an `int` and `maxSegmentWeight` is a `long` set to `maxWeight / concurrencyLevel`, when `maxWeight` is 32GB or above, then the value of `maxSegmentWeight` will be set to  above the maximum value of `int` and the check 

``` java
while (totalWeight &gt; maxSegmentWeight) {
```

will always fail.
</comment><comment author="jpountz" created="2014-05-22T00:22:51Z" id="43833893">Wow, good catch! I think it would make sense to file a bug to Guava?
</comment><comment author="nik9000" created="2014-05-22T12:21:02Z" id="43880555">&gt; Wow, good catch! I think it would make sense to file a bug to Guava?

Indeed!

I'd file that with Guava but also clamp the size of the cache in Elasticsearch to 32GB - 1 for the time being.

As an aside I imagine 96GB heaps cause super long pause times on hot spot.
</comment><comment author="jpountz" created="2014-05-22T12:21:57Z" id="43880632">+1
</comment><comment author="nik9000" created="2014-05-22T12:22:53Z" id="43880731">I've got the code open and have a few free moments so I can work on it if no one else wants it.
</comment><comment author="jpountz" created="2014-05-22T12:23:58Z" id="43880843">That works for me, feel free to ping me when it's ready and you want a review.
</comment><comment author="kimchy" created="2014-05-22T12:26:44Z" id="43881069">&gt; Wow, good catch! I think it would make sense to file a bug to Guava?

Huge ++!. @danp60 when you file the bug in guava, can you link back to it here?
</comment><comment author="nik9000" created="2014-05-22T12:30:33Z" id="43881438">I imagine you've already realized it but the work around is to force the cache size under 32GB.
</comment><comment author="jpountz" created="2014-05-22T12:32:29Z" id="43881609">Indeed. I think that's not too bad a workaround though since I would expect such a large filter cache to be quite wasteful compared to leaving the memory to the operating system so that it can do a better job with the filesystem cache.
</comment><comment author="danp60" created="2014-05-22T17:52:03Z" id="43921501">@kimchy I've filed the guava bug here: https://code.google.com/p/guava-libraries/issues/detail?id=1761&amp;colspec=ID%20Type%20Status%20Package%20Summary
</comment><comment author="jpountz" created="2014-05-22T18:30:13Z" id="43926026">@danp60 Thanks!
</comment><comment author="jpountz" created="2014-05-28T11:01:13Z" id="44391186">The bug has been [fixed upstream](https://code.google.com/p/guava-libraries/issues/detail?id=1761).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plain highlighter to use analyzer defined on a document level </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6267</link><project id="" key="" /><description>At the moment plain highligher only uses an analyzer defined for on the type level. However, during the indexing stage it is possible to define analyzer on per document level, for example mapping '_analyzer' to another field, containing required name. This commit attempts to make sure that highlighting works correctly in this scenario.

Important: the field containing the document analyzer is hardcoded to 'language_analyzer' at the moment. This should use document mappings instead. I just could not figure out a way to do it without introducing too many changes on DocumentMapper.

Closes #5497
</description><key id="34003925">6267</key><summary>Plain highlighter to use analyzer defined on a document level </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">mateuszkaczynski</reporter><labels><label>:Highlighting</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T17:09:24Z</created><updated>2015-06-07T15:00:04Z</updated><resolved>2014-05-28T07:25:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-22T09:38:07Z" id="43867010">@mateuszkaczynski Indeed it would be nice to have the analyzer that was used at indexing time to do the highlighting as well. I think the way to remove the hard-coded analyzer extraction would be to:
- add a method to `AnalysisMapper` to retrieve the analyzer to use given a `SearchContext` (similarly to what is done in `AnalyzerMapper.postParse`)
- fix `PlainHighlighter` to use this method. Note that you can get the `AnalyzerMapper` of a hit this way: 

``` java
AnalyzerMapper analyzerMapper = context.mapperService().documentMapper(hitContext.hit().type()).rootMapper(AnalyzerMapper.class);
```
</comment><comment author="clintongormley" created="2014-05-22T09:43:12Z" id="43867485">Given that the document has matched because of the analyzer applied at **query** time, not index time, perhaps we should use the query time analyzer instead?
</comment><comment author="mateuszkaczynski" created="2014-05-22T10:47:02Z" id="43872609">@clintongormley is there any scenario where they would actually differ (pardon my ignorance) ? Is it already reachable from within the highlighter context / would it be feasible to include it there? 
</comment><comment author="jpountz" created="2014-05-22T11:07:04Z" id="43874158">@clintongormley If there is a match, this means that the query matched the index-time tokens. So I think the plain highlighter needs to recompute those index-time tokens in order to try to match them with the tokens from the query? For example, if you compare it with the vector highighter, the latter would also use index-time tokens since it relies on term vectors that are computed at indexing time?

@mateuszkaczynski yes indeed: when you configure an analyzer per document, it is only going to be used at indexing time. At search time, Elasticsearch will use the analyzer that is defined for searching, it has no way to use a different per-document analyzer. `_analyzer` actually is quite an expert option, I don't think there are many use-cases for it.
</comment><comment author="nik9000" created="2014-05-22T11:14:30Z" id="43874773">&gt; @clintongormley is there any scenario where they would actually differ (pardon my ignorance) ? Is it already reachable from within the highlighter context / would it be feasible to include it there? 

There are several reasons you'd use a different one but they aren't super common.  You can perform a fast prefix search by adding an edge n-gram token filter at index time only.  You can make ascii folding optional by adding it with preserve_original=true at index time only.  The hebrew analyzer requires you to use different analyzers at query and index time as well or else it'll produce too many false positives.  Its useful, just not super common.

&gt; Given that the document has matched because of the analyzer applied at query time, not index time, perhaps we should use the query time analyzer instead?

Yeah, like @jpountz just said while I was writing this, you need to use the index one.  It'd break highlighting for the above use cases if you didn't. 
</comment><comment author="mateuszkaczynski" created="2014-05-22T11:58:03Z" id="43878342">@jengrant that does make sense. 
Regarding changes to `AnalyzerMapper`:
-    Is `SearchContext` sufficient? Would I not need to look at available fields from `HitContext` (both of which are on `HighlighterContext`?
-    If I get it right, it would be ideal to go with the same convention as in `postParse` method, i.e. set a variable on the context (which would then in turn require changing the interface) ?
</comment><comment author="jpountz" created="2014-05-22T12:04:18Z" id="43878899">Indeed you need a `HitContext`.

&gt;  If I get it right, it would be ideal to go with the same convention as in postParse method, i.e. set a variable on the context (which would then in turn require changing the interface) ?

I didn't think about it this way, but now that you mention it, it would indeed be nice to extract the default analyzer only once, even if a hit is highlighted several times, so I think it makes sense to cache the analyzer to use on the context for reuse.
</comment><comment author="mateuszkaczynski" created="2014-05-22T16:05:23Z" id="43908737">@jpountz I've just committed an update, does this seem more reasonable?  
</comment><comment author="jpountz" created="2014-05-22T21:07:00Z" id="43944111">Thanks @mateuszkaczynski , this looks good indeed. I left some minor comments mostly about naming and formatting but I think it's close!
</comment><comment author="mateuszkaczynski" created="2014-05-23T10:07:00Z" id="43991565">@jpountz Thanks for looking at it, fixed them.
</comment><comment author="jpountz" created="2014-05-23T11:24:05Z" id="43997006">Thanks, this looks good! Would you mind signing our [contributor license agreement](http://www.elasticsearch.org/contributor-agreement/) so that I can pull this change in?
</comment><comment author="mateuszkaczynski" created="2014-05-23T11:29:46Z" id="43997991">@jpountz great, already signed before submitting pull (unless I'm missing something?) 
</comment><comment author="jpountz" created="2014-05-23T11:37:00Z" id="43998996">Perfect, it must just be waiting for being processed, I'll check.
</comment><comment author="mateuszkaczynski" created="2014-05-27T09:35:38Z" id="44253912">@jpountz I was wondering if everything is fine with the license agreement, did I miss something / need to re-submit?
</comment><comment author="jpountz" created="2014-05-27T09:38:56Z" id="44254210">@mateuszkaczynski Pull request is good and your license agreement was processed but I'm traveling right now. I will merge the change when I come back, hopefully tomorrow. Sorry for the delay.
</comment><comment author="jpountz" created="2014-05-28T07:25:02Z" id="44373592">@mateuszkaczynski I just merged your pull request, thanks!
</comment><comment author="mateuszkaczynski" created="2014-05-28T09:41:23Z" id="44384576">@jpountz thank you so much for your help!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed conversion of date field values when using multiple date formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6266</link><project id="" key="" /><description>When multiple date formats are specified using the || syntax in the field mappings the date_histogram aggregation breaks.  This is because we are getting a parser rather than a printer from the date formatter for the object we use to convert the DateTime values back into Strings.  Simple fix to get the printer form the date format and test to back it up

Closes #6239
</description><key id="34002736">6266</key><summary>Fixed conversion of date field values when using multiple date formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">colings86</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T16:54:16Z</created><updated>2015-06-07T20:01:14Z</updated><resolved>2014-05-22T09:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-05-21T17:05:27Z" id="43785789">Closes #6239 
</comment><comment author="jpountz" created="2014-05-22T09:02:18Z" id="43864019">LGTM
</comment><comment author="s1monw" created="2014-05-22T09:03:04Z" id="43864095">+1 to push
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add configurable timeout for cluster state transmission to joining nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6265</link><project id="" key="" /><description>A customer encountered a problem where a very large mapping added to the indices (120 MB) caused new nodes joining the cluster to time out with errors similar to:

2014-05-20 18:57:54,428 INFO [hostname.domain][generic][T#1]] discovery.zen - [hostname.domain] failed to send join request to master [[master.domain][KvDPGxGJSOyuU-LgDBSYiw][master.domain][inet[/10.X.X.X:27892]]{data=false, max_local_storage_nodes=1, master=true}], reason [org.elasticsearch.ElasticsearchTimeoutException: Timeout waiting for task.]

A configurable timeout here would let the cluster start up and prevent it from dropping as cluster state was updated during normal operation.  This problem also led to direct buffer memory errors on the master as it attempted to write the full cluster state to the buffer multiple times to handle retries and other nodes joining the cluster.
</description><key id="34000422">6265</key><summary>Add configurable timeout for cluster state transmission to joining nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">seang-es</reporter><labels><label>discuss</label></labels><created>2014-05-21T16:36:08Z</created><updated>2015-10-14T16:34:28Z</updated><resolved>2015-10-14T16:34:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="seang-es" created="2014-05-21T16:36:38Z" id="43782185">See case #2877
</comment><comment author="clintongormley" created="2014-12-30T18:59:02Z" id="68385533">@s1monw what are your thoughts on this one?
</comment><comment author="kimchy" created="2014-12-30T19:08:47Z" id="68386505">at least the direct memory problem was fixed, since we better do slicing of large messages through Netty now so the low level NIO layer will not allocate such large direct buffers on thread locals
</comment><comment author="bleskes" created="2015-01-06T13:40:24Z" id="68865938">Maybe I'm missing something,  but I think `discovery.zen.join_timeout` is exactly what is asked for?
</comment><comment author="clintongormley" created="2015-10-14T16:34:28Z" id="148108476">thanks @bleskes 

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Creating an index that already exists should return 409</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6264</link><project id="" key="" /><description>- Create an index named "conflict"
- Create an index named "conflict" again
- Expected: 409 Conflict
- Actual: 400 Bad Request

```
C:\&gt;curl -v -XPUT "http://localhost:9200/conflict"
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1... connected
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; PUT /conflict HTTP/1.1
&gt; User-Agent: curl/7.21.7 (amd64-pc-win32) libcurl/7.21.7 OpenSSL/0.9.8r zlib/1.2.5
&gt; Host: localhost:9200
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 21
&lt;
{"acknowledged":true}* Connection #0 to host localhost left intact
* Closing connection #0

C:\&gt;curl -v -XPUT "http://localhost:9200/conflict"
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1... connected
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; PUT /conflict HTTP/1.1
&gt; User-Agent: curl/7.21.7 (amd64-pc-win32) libcurl/7.21.7 OpenSSL/0.9.8r zlib/1.2.5
&gt; Host: localhost:9200
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 400 Bad Request
&lt; Content-Type: application/json; charset=UTF-8
&lt; Content-Length: 79
&lt;
{"error":"IndexAlreadyExistsException[[conflict] already exists]","status":400}* Connection #0 to host localhost left intact
* Closing connection #0
```
</description><key id="33997869">6264</key><summary>Creating an index that already exists should return 409</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">hughbiquitous</reporter><labels /><created>2014-05-21T16:09:29Z</created><updated>2014-09-07T09:20:11Z</updated><resolved>2014-09-07T09:20:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-22T09:37:44Z" id="43866983">While semantically correct, I'm not sure that changing the status code to 409 really adds anything here.  We'd also have to change the codes in various other places, such as when trying to change the mapping on an existing field.

Currently, we use 409's exclusively for version conflicts or when trying to create a document which already exists, and I think I prefer reserving 409 for those situations.  (This is not quite true, we also use it in IndexPrimaryShardNotAllocatedException, but I plan on changing that).

How would this change help you? You can always check the message for IndexAlreadyExists.
</comment><comment author="hughbiquitous" created="2014-05-22T14:11:34Z" id="43892990">It just seems like the right thing to do in that situation. It feels hacky
to have to parse an error message when you could get the same information
from a status code.

Also, 400 Bad Request implies there was something wrong with the request I
issued. 409 Conflict implies that my request was fine, but couldn't be
fulfilled because of a conflict situation.

Does it help me a ton? No, not really. It's not like it's reporting success
when it's actually failing. Just seems like the right thing to do.

On Thu, May 22, 2014 at 5:38 AM, Clinton Gormley
notifications@github.comwrote:

&gt; While semantically correct, I'm not sure that changing the status code to
&gt; 409 really adds anything here. We'd also have to change the codes in
&gt; various other places, such as when trying to change the mapping on an
&gt; existing field.
&gt; 
&gt; Currently, we use 409's exclusively for version conflicts or when trying
&gt; to create a document which already exists, and I think I prefer reserving
&gt; 409 for those situations. (This is not quite true, we also use it in
&gt; IndexPrimaryShardNotAllocatedException, but I plan on changing that).
&gt; 
&gt; How would this change help you? You can always check the message for
&gt; IndexAlreadyExists.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/6264#issuecomment-43866983
&gt; .
</comment><comment author="clintongormley" created="2014-09-07T09:20:10Z" id="54741721">Opened #7632 to change IndexPrimaryShardNotAllocatedException response code.

Will leave the status code for creating an index that already exists as 400, so closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator: Fix assertion in percolation with nested docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6263</link><project id="" key="" /><description>Assertion was triggered for percolating documents with nested object
in mapping if the document did not actually contain a nested object.
Instead, in this case we can just use the SingleDocumentPercolatorIndex.
</description><key id="33994896">6263</key><summary>Percolator: Fix assertion in percolation with nested docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels><label>:Percolator</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T15:40:24Z</created><updated>2015-06-07T19:59:38Z</updated><resolved>2014-05-21T16:21:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-21T15:52:06Z" id="43774873">LGTM
</comment><comment author="brwe" created="2014-05-21T16:21:34Z" id="43779740">Pushed to 1.2, 1.x and master. Should I also backport to 1.1 branch?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix FieldDataWeighter generics to accept RamUsage instead of AtomicFieldData</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6262</link><project id="" key="" /><description>The `FieldDataWeighter` allowed to use a concrete subclass of the caches
generic type to be used that causes ClassCastException and also trips the
CirciutBreaker to not be decremented appropriately.

This was tripped by settings randomization also part of this commit.

Closes #6260
</description><key id="33994772">6262</key><summary>Fix FieldDataWeighter generics to accept RamUsage instead of AtomicFieldData</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Fielddata</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T15:39:10Z</created><updated>2015-06-07T20:00:08Z</updated><resolved>2014-05-21T15:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-21T15:41:49Z" id="43772939">LGTM
</comment><comment author="dakrone" created="2014-05-21T15:45:18Z" id="43773568">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indices Stats: Add FieldData and Filter eviction rates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6261</link><project id="" key="" /><description>This adds one-, five- and fifteen-minute eviction rates for FieldData and Filter stats.  Cumulative eviction counts don't give much insight into the nature and frequency of evictions.  Particularly as server uptime increases, cumulative counts become relatively useless.

By measuring and the rates over short time intervals, the user can more easily understand how their system is behaving.  Rates are backed by an exponentially weighted moving average, so more recent evictions contribute to the counter moreso than older evictions.

Current example output (updated 06/27/2014):

``` json
{
    "filter_cache": {
       "memory_size_in_bytes": 62312,
       "evictions": 135,
       "evictions_per_sec": {
         "1m": 58.4 , 
         "5m": 13.8, 
         "15m": 4.7
       }
    },
    "fielddata": {
        "memory_size_in_bytes": 2101455,
        "evictions": 42,
        "evictions_per_sec": {
          "1m": 21.4 , 
          "5m": 18.8, 
          "15m": 6.7
        }
    }
}
```
</description><key id="33994554">6261</key><summary>Indices Stats: Add FieldData and Filter eviction rates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/polyfractal/following{/other_user}', u'events_url': u'https://api.github.com/users/polyfractal/events{/privacy}', u'organizations_url': u'https://api.github.com/users/polyfractal/orgs', u'url': u'https://api.github.com/users/polyfractal', u'gists_url': u'https://api.github.com/users/polyfractal/gists{/gist_id}', u'html_url': u'https://github.com/polyfractal', u'subscriptions_url': u'https://api.github.com/users/polyfractal/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1224228?v=4', u'repos_url': u'https://api.github.com/users/polyfractal/repos', u'received_events_url': u'https://api.github.com/users/polyfractal/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/polyfractal/starred{/owner}{/repo}', u'site_admin': False, u'login': u'polyfractal', u'type': u'User', u'id': 1224228, u'followers_url': u'https://api.github.com/users/polyfractal/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>:Stats</label><label>enhancement</label></labels><created>2014-05-21T15:37:02Z</created><updated>2015-03-06T22:30:30Z</updated><resolved>2015-03-06T22:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2014-05-21T15:51:03Z" id="43774672">+1 Very nice!
</comment><comment author="karmi" created="2014-05-21T15:52:12Z" id="43774891">@polyfractal I'd probably vote for labels, better transparency. Although it's more verbose... (Are the number in the output actually flipped? Ie. `69.08162576026469` is 15mins?)
</comment><comment author="polyfractal" created="2014-05-21T15:59:57Z" id="43776247">@karmi Yeah, verbose is probably better than cryptic.  Probably round the values to two decimal points too?

The output looks that way because there was an immediate, brief spike.  The 1-min average is skewed heavily, while the 5- and 15-min intervals have a lot of `0`'s which keeps the average down.  Visually, the counters look like this:
- 1-min: `[69,0,0]`
- 5-min: `[69,0,0,0,0,0,0,0,0,0,0,0,0,0,0]`
- 15-min: `[69,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]`

If the spike was temporary, the 5- and 15-min would remain quite low and the 1-min would reset back to zero.  If the "spike" turns into a sustained event, you would see the 5- and 15-min start to climb until they "saturate" at the same rate as the 1-min.
</comment><comment author="karmi" created="2014-05-21T16:14:00Z" id="43778515">@polyfractal Ah, right, didn't figure out the skewing, which is natural. Unless we have this `load`-like format everywhere and people are used to it, I think labels like `1min`, `5min`, ... are better, nothing to explain in the docs etc. And dashboarding clients can always run something like `d3.values(myresponse.fielddata.rates)` to get the Array.
</comment><comment author="kimchy" created="2014-05-22T12:54:26Z" id="43883578">Nice!, quick comments:
- the serialization should be versioned, if we want to get this to 1.2
- The scheduled thread service should not be created for each MeterMetric, we should share the same scheduled thread service to use the scheduling support in ThreadPool shared node level instance. 
</comment><comment author="polyfractal" created="2014-05-22T18:13:13Z" id="43924032">- Array replaced with map
- Double values are rounded to three decimal points (should be more than enough for diagnostics)
- Uses the ThreadPool's scheduler now...think I did this correctly :)
- Serialization is now versioned, pegged at `&gt;= 1.3.0`.  Should this be versioned for `&gt;= 1.2.1` instead?

Output now looks like this:

``` json
"rates": {
    "1_min": 58.476,
    "5_min": 13.812,
    "15_min": 4.734
}
```
</comment><comment author="polyfractal" created="2014-05-23T12:15:33Z" id="44002036">Was chatting with @spinscale, who brought up the issue that timeunits are undefined in the above response.  It's unclear what the numbers actually mean.

We spent some time brainstorming, but didnt come up with a satisfying format.  Any opinions or new suggestions?  Some potential candidates:

``` json
"rates_per_sec": {
    "1_min_window": 58.476,
    "5_min_window": 13.812,
    "15_min_window": 4.734
}
```

``` json
"rates_per_sec": {
    "last_1min": 58.476,
    "last_5min": 13.812,
    "last_15min": 4.734
}
```

``` json
rates: {
    "time_unit" : "seconds", 
    "1m": 58.476 , 
    "5m": 13.812, 
    "15m": 4.734 
}
```

Or any combination of any of the above parts. I'm not in love with any of them...suggestions welcome!
</comment><comment author="nik9000" created="2014-05-23T12:36:51Z" id="44003559">Neat!

&gt; Was chatting with @spinscale, who brought up the issue that timeunits are undefined in the above response. It's unclear what the numbers actually mean.

``` js
"evictions_per_second": {
    "1m": 58.476 , 
    "5m": 13.812, 
    "15m": 4.734 
}
```

I guess the point is to make sure it makes you think of load average.
</comment><comment author="nik9000" created="2014-05-23T12:38:22Z" id="44003670">&gt; Cumulative eviction counts don't give much insight into the nature and frequency of evictions. Particularly as server uptime increases, cumulative counts become relatively useless.

They are super useful when you [feed them to rrdtool](http://ganglia.wikimedia.org/latest/?r=hour&amp;cs=&amp;ce=&amp;m=es_filter_cache_evictions&amp;s=by+name&amp;c=Elasticsearch+cluster+eqiad&amp;h=&amp;host_regex=&amp;max_graphs=0&amp;tab=m&amp;vn=&amp;hide-hf=false&amp;sh=1&amp;z=small&amp;hc=4)!
</comment><comment author="kimchy" created="2014-05-27T15:43:01Z" id="44293832">- I am +1 on `1m` compared to `1_min`.
- Maybe we can use something like `Strings#format1Decimals` instead of `DecimalFormat`, which is annoyingly expensive, we can maybe add 2 or 3 decimals aspects.
</comment><comment author="kimchy" created="2014-05-29T23:20:09Z" id="44597948">one more note, can we add those stats to the relevant cat APIs as well?
</comment><comment author="polyfractal" created="2014-06-09T18:25:48Z" id="45525546">Done.
- Formatting changed to "evictions_per_sec" and "1m", etc
- Now using format1Decimals.  One decimal point should be plenty for this metric imo
- Added to `_cat/nodes`
</comment><comment author="s1monw" created="2014-07-02T10:30:39Z" id="47760311">i left some comment, I am in general missing tests for this, I think we can test this pretty well though... if you need help lemme know.
</comment><comment author="clintongormley" created="2014-07-11T09:51:03Z" id="48712846">@polyfractal ping?
</comment><comment author="polyfractal" created="2014-07-11T13:42:21Z" id="48731315">Yep, this will get some attention this week.  Been on the road.
</comment><comment author="polyfractal" created="2014-07-14T19:35:08Z" id="48948738">- Eviction metrics broken out into a new class, which lives at `elasticsearch/common/metrics/EvictionStats`.  This doesn't feel like the correct location, but I couldn't find a better place that was common between Filter and FieldData stats.
- I opted to keep the version checks in FieldDataStats and FilterStats, in an effort to keep EvictionStats generic (e.g. Lee mentioned adding similar stats to Circuit Breaker in the future, which could use this same class).  If the version check went into EvictionStats, it wouldn't be reusable in elsewhere.
- Added integration tests. Needed to use a small sleep because of the update frequency of the Meter
- Added backwards compatibility tests.  Became rather hairy to make sure all contingencies are tested, but it works (so far).
</comment><comment author="s1monw" created="2014-07-15T13:11:51Z" id="49028888">I like this a lot - left some comments
</comment><comment author="polyfractal" created="2014-09-12T19:07:44Z" id="55447231">- Licenses added
- Updated min version to 1.5.0
- Updated tests to use awaitBusy, much cleaner :)

I'm running into a failure on the backwards compat tests.  Will followup with @dakrone about the CB failure to see if I'm doing something silly. 

```
org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.stats.NodesStatsResponse]
...
Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No CircuitBreaker with ordinal: 54
    at org.elasticsearch.common.breaker.CircuitBreaker$Name.readFrom(CircuitBreaker.java:62)
...
```
</comment><comment author="polyfractal" created="2014-10-01T21:26:41Z" id="57541430">Made some tweaks to stabilize the tests.  Among the various changes, I had to increase the size of docs indexed and searches executed to get the BackwardsCompat filter cache test to reliably pass.  

I think it's because there are extra nodes in the cluster (the external nodes) and Guava Cache doesn't always evict immediately or at the configured size due to the internal implementation.

Open to alternatives, I don't really like how unreliable these tests are when the search count is small...
</comment><comment author="clintongormley" created="2014-10-20T13:18:32Z" id="59751907">@dakrone could you review this please?
</comment><comment author="polyfractal" created="2014-10-28T17:37:01Z" id="60797772">@dakrone Requested changes made, thanks!  Anything else?
</comment><comment author="dakrone" created="2014-10-30T10:54:32Z" id="61074313">@polyfractal Took another look, this looks pretty good to me, but when I cherry-pick to to 1.x I get test failures that are reproducible with:

```
mvn clean test -Dtests.seed=493B76A9C3BDB91 -Dtests.class=org.elasticsearch.indices.cache.EvictionTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Amsterdam -Dtests.method="testFilterEvictions" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.processors=12
```
</comment><comment author="polyfractal" created="2014-10-30T11:30:02Z" id="61078037">Argh, will take a look, thanks.  These tests are being incredibly finicky to stabilize.  I think it's because Guava Cache doesn't guarantee evictions, and evictions only occur during writes, which makes it tricky to make the test deterministic.

Will investigate when I get a chance.
</comment><comment author="polyfractal" created="2014-11-07T15:24:57Z" id="62159541">@dakrone Haha, so apparently the [filter cache can be randomly disabled during tests](https://github.com/elasticsearch/elasticsearch/pull/6280), which I was not aware was possible.  Explicitly enabling the filter cache seems to stabilize everything.  Also did a few tweaks:
- Removed those unnecessary if's
- Replaced some hardcoded strings with statics
- Added a refresh after the indexing for good measure, in case the searches kicked off before the docs became live
</comment><comment author="dakrone" created="2014-11-11T13:00:56Z" id="62543392">@polyfractal I'm getting another reproducible test failure:

```
mvn clean test -Dtests.seed=7A4B986018B95F7A -Dtests.class=org.elasticsearch.indices.cache.EvictionTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Amsterdam -Dtests.method="testFilterEvictions" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.processors=12
```

It looks like waiting 10 seconds for an eviction is timing out:

```
  1&gt; [2014-11-11 13:59:30,299][ERROR][test                     ] FAILURE  : testFilterEvictions(org.elasticsearch.indices.cache.EvictionTests)
  1&gt; REPRODUCE WITH  : mvn clean test -Dtests.seed=7A4B986018B95F7A -Dtests.class=org.elasticsearch.indices.cache.EvictionTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Amsterdam -Dtests.method="testFilterEvictions" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.processors=12
  1&gt; Throwable:
  1&gt; java.lang.AssertionError
  1&gt;     __randomizedtesting.SeedInfo.seed([7A4B986018B95F7A:EB9956A7C320B860]:0)
  1&gt;     [...org.junit.*]
  1&gt;     org.elasticsearch.indices.cache.EvictionTests.waitForEvictions(EvictionTests.java:179)
  1&gt;     org.elasticsearch.indices.cache.EvictionTests.testFilterEvictions(EvictionTests.java:160)
  1&gt;     [...sun.*, java.lang.reflect.*, com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
  1&gt;     org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
  1&gt;     org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
  1&gt;     org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
  1&gt;     org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
  1&gt;     [...com.carrotsearch.randomizedtesting.*]
  1&gt;     java.lang.Thread.run(Thread.java:745)
  1&gt; 
  1&gt; [2014-11-11 13:59:30,310][INFO ][test                     ] Test testFilterEvictions(org.elasticsearch.indices.cache.EvictionTests) finished
FAILURE 12.7s | EvictionTests.testFilterEvictions &lt;&lt;&lt;
   &gt; Throwable #1: java.lang.AssertionError
   &gt;    at __randomizedtesting.SeedInfo.seed([7A4B986018B95F7A:EB9956A7C320B860]:0)
   &gt;    at org.junit.Assert.fail(Assert.java:92)
   &gt;    at org.junit.Assert.assertTrue(Assert.java:43)
   &gt;    at org.junit.Assert.assertTrue(Assert.java:54)
   &gt;    at org.elasticsearch.indices.cache.EvictionTests.waitForEvictions(EvictionTests.java:179)
   &gt;    at org.elasticsearch.indices.cache.EvictionTests.testFilterEvictions(EvictionTests.java:160)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
   &gt;    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
   &gt;    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   &gt;    at java.lang.reflect.Method.invoke(Method.java:483)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:827)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:863)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:877)
   &gt;    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
   &gt;    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
   &gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
   &gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
   &gt;    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
   &gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:798)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:458)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)
   &gt;    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)
   &gt;    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
   &gt;    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
   &gt;    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)
   &gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
   &gt;    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
   &gt;    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
   &gt;    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
   &gt;    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
   &gt;    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:365)
   &gt;    at java.lang.Thread.run(Thread.java:745)
```
</comment><comment author="polyfractal" created="2015-03-06T22:30:30Z" id="77648402">Discussed this PR with @dakrone ages ago, we decided it didn't make sense anymore due to the filter cache changes in Lucene.  Meant to close...and forgot to.  Closing now :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FieldData: Global ordinals cause ClassCastExceptions if used with a bounded fielddata cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6260</link><project id="" key="" /><description>I randomized the fielddata settings today and run into these exceptions:

```
InternalGlobalOrdinalsIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.AtomicFieldData
  1&gt;    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$FieldDataWeigher.weigh(IndicesFieldDataCache.java:102)
  1&gt;    at com.google.common.cache.LocalCache$Segment.setValue(LocalCache.java:2160)
  1&gt;    at com.google.common.cache.LocalCache$Segment.storeLoadedValue(LocalCache.java:3142)
  1&gt;    at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2351)
  1&gt;    at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2318)
  1&gt;    at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)
  1&gt;    at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2195)
  1&gt;    at com.google.common.cache.LocalCache.get(LocalCache.java:3934)
  1&gt;    at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)
  1&gt;    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:163)
  1&gt;    at org.elasticsearch.index.fielddata.plain.AbstractBytesIndexFieldData.loadGlobal(AbstractBytesIndexFieldData.java:75)
```

this causes the Circ. Breaker to not be reset to 0 etc. 
</description><key id="33993952">6260</key><summary>FieldData: Global ordinals cause ClassCastExceptions if used with a bounded fielddata cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Fielddata</label><label>blocker</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T15:31:38Z</created><updated>2015-06-07T19:59:43Z</updated><resolved>2014-05-21T15:52:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Drop unnecessary BW compat checks on master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6259</link><project id="" key="" /><description>We don't need to keep the BW compat code on master for conditional version reads / writes etc. Yet I kept the BW compat for indices and analysis though.
</description><key id="33986850">6259</key><summary>Drop unnecessary BW compat checks on master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels /><created>2014-05-21T14:22:31Z</created><updated>2014-12-10T14:04:49Z</updated><resolved>2014-12-10T14:04:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-22T09:42:15Z" id="43867398">+1 to this commit and to remove the slow scroll entirely
</comment><comment author="jpountz" created="2014-05-22T13:54:19Z" id="43890724">There are also some deprecated methods on master, I think we should remove them as well?
</comment><comment author="clintongormley" created="2014-11-11T19:00:29Z" id="62597926">@s1monw can this be merged?
</comment><comment author="s1monw" created="2014-12-10T14:04:39Z" id="66455304">closing this one is out of date and we cleaned this up somewhere else
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail queries that have two aggregations with the same name.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6258</link><project id="" key="" /><description>Close #6255
</description><key id="33980656">6258</key><summary>Fail queries that have two aggregations with the same name.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>:Aggregations</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T13:12:44Z</created><updated>2015-06-07T20:00:22Z</updated><resolved>2014-05-21T13:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-05-21T13:18:14Z" id="43752510">Looks good to me
</comment><comment author="s1monw" created="2014-05-21T13:19:26Z" id="43752655">LGTM - I think this can go into `1.2` and `1.1.2` if applicable
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Restored MetaData#concreteIndices(String[] indices) method</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6257</link><project id="" key="" /><description>Restored MetaData#concreteIndices(String[] indices) method, deprecated instead or removed for backwards compatibility in 1.x

Relates to #6059
</description><key id="33980569">6257</key><summary>Restored MetaData#concreteIndices(String[] indices) method</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>v1.2.0</label></labels><created>2014-05-21T13:11:41Z</created><updated>2014-07-16T21:44:47Z</updated><resolved>2014-05-21T13:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-21T13:12:24Z" id="43751880">LGTM
</comment><comment author="bleskes" created="2014-05-21T13:18:13Z" id="43752506">LGTM - note that the label says 1.2 but the PR is to 1.X . I presume it goes to master, 1.x &amp; 1.2
</comment><comment author="javanna" created="2014-05-21T13:28:39Z" id="43753755">Pushed to 1.x and 1.2, but not on master as deprecations don't make much sense there since no backwards compatibility is needed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search on _parent field with multi_match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6256</link><project id="" key="" /><description>In version 1.1.1, search on _parent field as part of multi_match query (when searching directly on a child document), results in a "nested: NullPointerException" error.
This used to word on a prior version.
Aa a note, it does work OK using a simple term query, and that's how it should be used anyway.
</description><key id="33976684">6256</key><summary>Search on _parent field with multi_match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">adamporat</reporter><labels /><created>2014-05-21T12:19:02Z</created><updated>2014-05-22T06:28:46Z</updated><resolved>2014-05-21T14:22:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-21T12:55:40Z" id="43750189">thanks for opening this, would it be possible to provide a gist that reproduces the issue? Can you tell if that works in `1.1.0` or didn't you try it there?
</comment><comment author="s1monw" created="2014-05-21T12:57:23Z" id="43750345">this might be related to  #6215 
</comment><comment author="martijnvg" created="2014-05-21T14:22:00Z" id="43760313">@adamporat This bug was also fixed via #6215 so when upgrading to the upcoming 1.1.2 or 1.2.0 should fix this issue.
</comment><comment author="adamporat" created="2014-05-22T06:28:46Z" id="43852808">Thank you
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: ClassCastException when sibling aggregations have the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6255</link><project id="" key="" /><description>If I run a search query of the following kind:

```
curl -XGET 'http://localhost:9200/index/_search?pretty' -d '{ "query" : { "match_all" : {} }, "aggs" : { "agg1" : { "terms" : { "field" : "stringField" } }, "agg1" : { "terms" : { "field" : "longField" } } } }'
```

It causes a ClassCastException in Elasticsearch:

```
{
  "error" : "ClassCastException[org.elasticsearch.search.aggregations.bucket.terms.LongTerms$Bucket cannot be cast to org.elasticsearch.search.aggregations.bucket.terms.StringTerms$Bucket]",
  "status" : 500
}
```

It looks like it is actually combining all aggregations with the same name into one combined aggregation.  The error occurs when the fields in the two aggregations are different types but will also give strange results when they are both the same type.  I have so far only tried this with the Terms aggregation but I suspect it will be an issue for other types too.

There should probably be some validation of sibling aggregations to ensure they have unique names and throw back a parse error to the client if there are multiple sibling aggregations with the same name.

More than this, since aggregations can be referenced from other aggregations by name, should aggregations names be unique across all aggregations and not just within their siblings?
</description><key id="33972484">6255</key><summary>Aggregations: ClassCastException when sibling aggregations have the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">colings86</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T11:06:49Z</created><updated>2014-07-16T12:34:49Z</updated><resolved>2014-05-21T13:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-21T11:09:44Z" id="43741547">&gt; There should probably be some validation of sibling aggregations to ensure they have unique names and throw back a parse error to the client if there are multiple sibling aggregations with the same name.

+1 I'll work on that
</comment><comment author="jpountz" created="2014-05-21T11:12:52Z" id="43741757">&gt; More than this, since aggregations can be referenced from other aggregations by name, should aggregations names be unique across all aggregations and not just within their siblings?

When referring to another aggregation (eg. for sorting the buckets of a terms aggregation), you need to specify the relative path from the current aggregation to the one that is used for sorting, so there should be no ambiguity. Or were you thinking about something else?
</comment><comment author="colings86" created="2014-05-21T11:23:06Z" id="43742487">No, that is what I was thinking of, so its just a problem for sibling aggregations then
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XFilteredQuery default strategy prefers query first in the deleted docs ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6254</link><project id="" key="" /><description>...case

Today we check if the DocIdSet we filter by is `fast` but the check fails
if the DocIdSet if wrapped in an `ApplyAcceptedDocsFilter` which is always
the case if the index has deleted documents. This commit unwraps
the original DocIdSet in the case of deleted documents.

Closes #6247
</description><key id="33968708">6254</key><summary>XFilteredQuery default strategy prefers query first in the deleted docs ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-21T10:09:13Z</created><updated>2015-06-08T14:35:21Z</updated><resolved>2014-05-21T11:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-21T10:17:13Z" id="43737452">pushed a cleanup commit
</comment><comment author="kimchy" created="2014-05-21T10:25:05Z" id="43738131">LGTM
</comment><comment author="jpountz" created="2014-05-21T10:26:26Z" id="43738246">+1
progress over perfection
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation Query string date range </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6253</link><project id="" key="" /><description>I modified the date encoding in the example about date range in query string documentation. 
Yesterday i tried to send a query like "date:[2014/05/01 TO 2014/05/20] and my es server answered me : 

Parse Failure [Failed to parse source [{\"query\":{\"query_string\":{\"query\":\"(evt.date:[2014/05/01 TO 2014/05/20])\"}},\"size\":10,\"from\":0}]]]; nested: ElasticSearchParseException[failed to parse date field [2014/05/01], tried both date format [dateOptionalTime], and timestamp number]; nested: IllegalArgumentException[Invalid format: \"2014/05/01\" is malformed at \"/05/01\"]; }]

Then i tried with "date:[2014-05-01 TO 2014-05-20]" and it worked. 
I don't know if it's a bug about date encoding in es query string, That's why through this pull request i submit you the problem that i met and maybe an issue.
Anyway you did a great job with elasticsearch.
Have a good day
Sorry about my poor English, i m not fluent. 
</description><key id="33965763">6253</key><summary>Documentation Query string date range </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">babeya</reporter><labels /><created>2014-05-21T09:25:11Z</created><updated>2014-07-23T14:33:20Z</updated><resolved>2014-07-23T14:33:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmark API might miss not yet starting benchmark in unabortable state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6252</link><project id="" key="" /><description>Today there is a race condition in how we handle the benchmark state that allows for benchmarks to be kicked off without the ability to abort them again. The main reason for this is that the state handling is flawed since applying the state is not serializable in a way that the executing nodes knows if a benchmark that is marked as `running` has been aborted. The abort call simply tires to stop the benchmark and if it's not in the executing nodes datastructure due to some sort of a delay or an exhausted threadpool it just ignores it. At that point the benchmark state is already updated in the cluster state and can not be aborted again.  

The solution to this is a bigger change though. We should move to cluster state listeners instead of per node RPC calls just like Snapshot &amp; Restore works
</description><key id="33960959">6252</key><summary>Benchmark API might miss not yet starting benchmark in unabortable state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>critical</label><label>stalled</label></labels><created>2014-05-21T08:15:35Z</created><updated>2015-08-26T14:59:48Z</updated><resolved>2015-08-26T14:59:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T14:59:48Z" id="135050461">The benchmark API efforts have been discontinued.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create *-latest.* s3 assets at release</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6251</link><project id="" key="" /><description>This is one way to add a `-latest` link for each ES asset.  It just uploads the key again instead of doing an s3 copy, which is cheap since we build on ec2 anyway.

This is preferable to doing a redirect in the downloads service so we can keep our release process confined in one place.
</description><key id="33919063">6251</key><summary>Create *-latest.* s3 assets at release</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2014-05-20T19:05:58Z</created><updated>2014-07-16T21:44:48Z</updated><resolved>2014-05-20T19:14:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-05-20T19:14:14Z" id="43670269">No good. This will skew our metrics.
</comment><comment author="s1monw" created="2014-05-22T07:57:12Z" id="43858582">@drewr I like the refactoring maybe we can get the refactoring in without the `latest` stuff?
</comment><comment author="drewr" created="2014-06-02T18:24:04Z" id="44873245">@s1monw ack, added in fc9c6e33
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Fielddata cat API added in 1.2.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6250</link><project id="" key="" /><description>I hope the formatting works! :)
</description><key id="33910006">6250</key><summary>[DOCS] Fielddata cat API added in 1.2.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">brusic</reporter><labels><label>docs</label></labels><created>2014-05-20T17:17:41Z</created><updated>2014-07-03T09:22:12Z</updated><resolved>2014-06-03T09:07:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-05-21T07:31:22Z" id="43721572">Hi @brusic, your change relates to `_cat/fielddata` but the commit and the PR mention `_cat/thread_pool`. Could you fix that please?
</comment><comment author="brusic" created="2014-05-22T14:38:53Z" id="43896809">How incredibly silly of me. Context switching is expensive!

Fixed.
</comment><comment author="javanna" created="2014-06-03T09:07:42Z" id="44940783">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update http.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6249</link><project id="" key="" /><description>Typo fix
</description><key id="33900015">6249</key><summary>Update http.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">volter</reporter><labels><label>docs</label></labels><created>2014-05-20T15:31:49Z</created><updated>2014-06-16T08:43:41Z</updated><resolved>2014-06-16T08:43:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T11:20:42Z" id="44951712">Hi @volter could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this in?
</comment><comment author="volter" created="2014-06-14T11:06:37Z" id="46084956">I signed on the 3rd of June.
</comment><comment author="javanna" created="2014-06-16T08:43:41Z" id="46153930">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Replaced `exclude` with `include` to avoid double negation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6248</link><project id="" key="" /><description /><key id="33893551">6248</key><summary>Replaced `exclude` with `include` to avoid double negation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-05-20T14:29:39Z</created><updated>2015-06-07T13:27:14Z</updated><resolved>2014-05-21T16:49:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-21T09:40:17Z" id="43734292">The change looks good but backward-incompatible. So I think this should only go to 2.0 and be added to the list of backward-compatibility breaks?
</comment><comment author="s1monw" created="2014-05-21T10:19:59Z" id="43737674">I think we can make this for 1.3 if we support both and drop the documentation for exclude! I like this muuch better! in master we can then drop the BW compat entirely
</comment><comment author="jpountz" created="2014-05-22T09:44:08Z" id="43867583">@alexksikes Do you plan to merge the change to the `1.x` branch?
</comment><comment author="alexksikes" created="2014-05-27T08:51:53Z" id="44250050">BW compat is covered by #6275 and merged with 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XFilteredQuery defaults to Query First strategy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6247</link><project id="" key="" /><description>In XFilteredQuery the strategy seems to almost always default to QueryFirst: https://github.com/elasticsearch/elasticsearch/blob/9ed34b5a9e9769b1264bf04d9b9a674794515bc6/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L226-L231

There the docIdSet is usually a NotDeletedDocIdSet or BitsFilteredDocIdSet built by ApplyAcceptedDocsFilter, and is not detected as a fast iterator.

On wide a query with a highly filtering filter, the performance impact is important, I have to manually force the "strategy" to something like "random_access_100".
</description><key id="33892326">6247</key><summary>XFilteredQuery defaults to Query First strategy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">hc</reporter><labels><label>:Search</label><label>blocker</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-20T14:17:48Z</created><updated>2015-06-08T14:34:55Z</updated><resolved>2014-05-21T11:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-20T15:21:32Z" id="43640463">The idea was that if its a "non fast bitset", which is something that don't implement `FixedBitSet`, then the computation is expensive (like the geo distance filter), thus it makes sense to have the query drive the filtered query execution.

I wonder though in which case it ends up being `ApplyAcceptedDocsFilter`, since this should be applies on the "top" level query. Can you share an example of when you run into it?
</comment><comment author="hc" created="2014-05-20T15:38:40Z" id="43642789">All FilteredQuery are wrapped into an ApplyAcceptedDocsFilter, no ?

An example of query where I encounter this problem:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "match": {
          "title": "trigger lot of documents the a with an some will i am there they some any"
        }
      },
      "filter": {
        "term": {
          "user_name": "joe"
        }
      }
    }
  }
}
```

There the query alone matches millions of documents, I have to force the strategy otherwise it does the query first.
</comment><comment author="kimchy" created="2014-05-20T19:55:03Z" id="43674778">@hc yea, it does, thanks for the example, let me think about how best to address it...
</comment><comment author="kimchy" created="2014-05-20T20:05:21Z" id="43675946">I chatted to @s1monw, we think we have a good solution for this, will update this ticket
</comment><comment author="s1monw" created="2014-05-21T10:09:45Z" id="43736884">I just opened a PR for this. I am not sure if it's a good solution at all but it should fix this immediate issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow for both `like_text` and `docs`/`ids` to be specified.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6246</link><project id="" key="" /><description /><key id="33890774">6246</key><summary>Allow for both `like_text` and `docs`/`ids` to be specified.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alexksikes</reporter><labels><label>:More Like This</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-20T14:01:17Z</created><updated>2015-06-07T13:27:49Z</updated><resolved>2014-05-22T11:53:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-22T09:53:42Z" id="43868422">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark abort accepts wildcard patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6245</link><project id="" key="" /><description>This adds support for sending a list of benchmark names and/or wildcard
patterns when submitting an abort request. Standard wildcards such as:
"_", "_xxx", and "xxx*" are supported. Multiple benchmark names and
wildcard patterns can be submitted together as a comma-separated list
from the REST API or as a string array from the Java API.

Closes #6185
</description><key id="33888454">6245</key><summary>Benchmark abort accepts wildcard patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-05-20T13:37:08Z</created><updated>2014-06-22T03:11:58Z</updated><resolved>2014-05-20T14:18:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-20T13:54:48Z" id="43628276">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve cluster update settings api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6244</link><project id="" key="" /><description>- If the minimum master nodes has been breached the reroute shouldn't be executed
- Cluster update settings call should also return in case of failures during the reroute.
</description><key id="33880789">6244</key><summary>Improve cluster update settings api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-20T11:51:34Z</created><updated>2015-06-07T13:28:05Z</updated><resolved>2014-05-20T16:30:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2014-05-20T11:59:56Z" id="43616340">LGTM. Left one comment. I also think we can improve a bit the description of the PR (cluster update settings may not return on reroute failure etc.)
</comment><comment author="javanna" created="2014-05-20T12:10:36Z" id="43617252">Left one comment as well, agreed on improving the description of the PR. BTW I think the call would  return in case of failure, but only after the ack timeout (with `acknowledged` flag set to `false`).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix grammar in dynamic mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6243</link><project id="" key="" /><description /><key id="33879733">6243</key><summary>Fix grammar in dynamic mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">robyoung</reporter><labels><label>docs</label></labels><created>2014-05-20T11:33:58Z</created><updated>2014-06-25T22:20:16Z</updated><resolved>2014-06-04T06:56:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T11:21:21Z" id="44951772">Hi @robyoung could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this in?
</comment><comment author="robyoung" created="2014-06-03T14:35:25Z" id="44972317">Done
</comment><comment author="javanna" created="2014-06-04T06:56:25Z" id="45057197">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Allow to disable plugin loading from classpath</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6242</link><project id="" key="" /><description>this PR allows us to disable plugin loading from the classpath during tests. 
</description><key id="33872172">6242</key><summary>Test: Allow to disable plugin loading from classpath</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-20T09:32:48Z</created><updated>2014-07-16T12:08:45Z</updated><resolved>2014-05-20T14:31:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-20T13:56:22Z" id="43628469">@jpountz @javanna pushed a new commit
</comment><comment author="jpountz" created="2014-05-20T13:57:32Z" id="43628612">LGTM
</comment><comment author="javanna" created="2014-05-20T14:00:02Z" id="43628941">LGTM ;)
</comment><comment author="s1monw" created="2014-05-20T14:31:56Z" id="43633451">pushed 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator should also take the filter of an alias into account.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6241</link><project id="" key="" /><description>At the moment if a document is being percolated via an alias the filter isn't taken into account. The percolator should take the alias filter into account.

The alias routing is taken into account.
</description><key id="33871061">6241</key><summary>Percolator should also take the filter of an alias into account.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-20T09:17:26Z</created><updated>2015-03-17T00:26:32Z</updated><resolved>2015-03-17T00:26:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jaddison" created="2015-02-04T17:25:48Z" id="72898066">Spent a day trying to figure out why this wasn't working - not sure why I couldn't find this issue before!

+1 to getting this done in 1.5.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Write to multiple indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6240</link><project id="" key="" /><description>It is possible to create an index as an alias to one or more indexes. When such an alias-index points to multiple real indexes, it is not possible to write to that alias. It would be nice to have write access to the alias.

The rest of this request contains some thoughts on this matter. I will use the following terms:
- "index-a": A normal index (not an alias)
- "index-b": Another index (not an alias)
- "alias-index": An alias index pointing to both "index-a" and "index-b"

When writing to the "alias-index", there are generally two options:
- Write to both of them simultaneously.
- Write to either "index-a" or "index-b".

We would need to declare which of the two options - let's call it the "target-definition" - we want. Again, there are two options:
- The "target-definition" is part of the alias definition.
- The "target-definition" is provided as an option to the index operation.

It is further possible to combine both approches, having a "target-definition" in the alias configuration and the option to override that definition when indexing a document.

In my opinion, making the "alias-index" behave like a real index will support the principle of least surprise and thus increase trust of users. So I think it would be best, if the "target-definition" was part of the "alias-index".

I also think that it is not really necessary to be able to change the "target-definition" per write operation: If the application already knows where it wants to write to, it can write to the specific index directly. The only advantage of such a feature would be that it could reduce network traffic if someone wanted to write to five indexes out of seven, for example. But then it would be more efficient to create another write index spanning the five indexes. Looks like I talked myself out of the second option for the "target-definition" :-)

Considering a writable alias, another question arises: what is the default behaviour? We have two options again:
- All index operations insert into both/all indexes.
- Writing is disabled by default.

The first option would further enforce the principle of least surprise, whereas the second option could prevent accidental inserts into the wrong index. I think this is just a matter of priorities (although I would prefer inserts to all indexes).

Thanks for reading all the way through the long text :-)
</description><key id="33870177">6240</key><summary>Write to multiple indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">soulmerge</reporter><labels><label>discuss</label></labels><created>2014-05-20T09:05:47Z</created><updated>2015-04-26T18:09:39Z</updated><resolved>2014-11-07T09:44:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="yeroc" created="2014-11-05T00:09:53Z" id="61738797">Has this request been considered at all?  It would be helpful for our use case where we need to be able to perform periodic re-indexing without going offline.  See http://www.mavengineering.com/blog/2014/02/12/seamless-elasticsearch-reindexing/ for a nice write-up but note how they have to query the alias and separately index into each of the underlying indices.  If this request were implemented this would be more efficient.
</comment><comment author="clintongormley" created="2014-11-07T09:44:08Z" id="62119722">Hi @soulmerge 

We've spent time today discussion this ticket.  In fact I opened one that was very similar several years ago (https://github.com/elasticsearch/elasticsearch/issues/2309).

I think that there are too many different data models and ways of indexing to be able to support them all with more complex APIs.  The append only use case is different from the update use case.  Multiple indices that exist because you're transitioning mapping is different from multiple indices reflecting time periods. 

Instead, we provide the primitives that allow you to do exactly what you need application side.  You can use the primitives to implement your exact model (eg using one alias for search across multiple indices, and one alias for the current write index).

Thanks for opening the issue, but we've decided against doing anything more here for now.
</comment><comment author="yeroc" created="2014-11-12T07:14:31Z" id="62679195">@clintongormley I agree that the proposal above was too complex and could be easily satisfied with multiple aliases (no need for the routing etc.) but that still doesn't address the situation where you need to index into multiple indexes while transitioning to a new index.  It sounds like you've only considered the case where users only need to index into a single index (eg. for time-based partitioning of indices).  

The other use case is where we actually need to write updates into two (or more) indices during index rebuilds.  Right now, we have to query a write-alias for its underlying indices and write to each one individually.  This leaves us open to race conditions (how often do we query the write-alias to get the current list of indices?  If we query too often, we hurt performance, on the flip side we end up writing to an alias we shouldn't be...)  so I respectfully disagree with the statement that the required primitives are in place today for this use case.  
</comment><comment author="clintongormley" created="2014-11-12T12:05:57Z" id="62708711">@yeroc no, I did consider that case as well, and I agree with you that the primitives are not yet in place to handle this easily.  However, the multi-index alias is not the right solution for this as it comes with its own finicky problems.  Instead, the changes API, which we are working on, should be a better solution.
</comment><comment author="soulmerge" created="2014-11-13T12:40:46Z" id="62884615">@clintongormley Is my understanding correct, that the underlying use-case of this bug-report (creating a new index for some database while still using the old index for reads and writes) will be solved with the upcoming changes API?

If this is the case and the changes API provides more or less the same features as the changes API in CouchDB (let's say "push notifications for changes in data"), I fail to understand how my use case can be implemented with the help of these new features?

Does this mean that writing to multiple indexes should be implemented using this API? This would just imply that the complexity of the multi-write implementation is moved from within the ES server (write to both indexes at once) to the clients (update the second index whenever the first index changes) and frankly does not add any value to the current scenario (write the same data to both indexes), as the race condition remains in both cases.

I would love to hear a clarification if I misunderstood your suggestion.

Cheers
</comment><comment author="clintongormley" created="2014-11-13T13:32:33Z" id="62890313">@soulmerge the changes API would indeed be like the couchDB change log.  The idea is that you would have an alias pointing to the old index.  Your application talks only to the alias.

You set up a new index and reindex your data from old to new using the changes APi.  At some point you switch the alias from old to new.  Your application continues talking to the same alias, but now it is using the new index instead.

The only missing part is this: at the moment we switch the alias, there may still be some changes in the old index which have not yet made it into the new index.  Somehow we need to pause writes to the old index until all transactions have been replicated. This could obviously be done in the application, but I'm still trying to figure out how we could do this natively so that it will be transparent to the user.
</comment><comment author="mfn" created="2015-04-26T10:25:04Z" id="96361041">Has there been any progress on this? I'm now in the midst of my third ES implementation and this is the third time this issue came up :)

Is https://github.com/elastic/elasticsearch/issues/1242 related? It bears the name "Changes API" but seems to resemble something else (notifications for outside ES?).

Currently I'm planning to have an alias for writing pointing to multiple indices but doing the writes manually by:
- read the aliases to figure out to which indices it points
- write to all these indices in a loop

The nasty things here are:
- every time I update a document I've to read the aliases
- handle logic if one indexing fails and the other doesn't (or, there could be even more indices, etc.)
</comment><comment author="clintongormley" created="2015-04-26T18:09:39Z" id="96416950">@mfn The Changes API is definitely related, but a few bits of functionality need to be built to support this:
- Sequence IDs (https://github.com/elastic/elasticsearch/issues/10708) so that we know which documents have been safely replicated to the replica shards
- Changes API which will allow us to follow all new documents in an index, sorted by sequence ID
- Reindex API which can follow the changes API and index the same (or an updated version of the) document to a new index

We are working on all of these parts, but they are big changes to get right.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aggregations: date_histogram aggregation breaks on date fields with multiple formats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6239</link><project id="" key="" /><description>The date format docs says you can separate different date formats using a double bar and that the first will be used to format any stored dates: [Date Format Docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html)

When using multiple date formats, however, the date_histogram aggregation throws "UnsupportedOperationException[Printing not supported]" which seems to be a Joda exception when a format doesn't have a printer. If I change the mapping to be a single format, the exception isn't thrown, so my guess is that the first format isn't parsed out to retrieve the printer.

I'm using ES 1.1.1, though I've also observed this on 1.1.0. 

Create the index:

```
&#10140;  ~  curl localhost:9200/my_index -XPUT -d'{"settings":{},"mappings":{"my_type":{"properties":{"my_date":{"type":"date","format":"dateOptionalTime||mm-DD-yyyy"}}}}}' 

{"acknowledged":true}%       
```

Add a document with a date:

```
&#10140;  ~  curl localhost:9200/my_index/my_type/1 -XPOST -d '{"my_date":"12-13-2014"}'

{"_index":"my_index","_type":"my_type","_id":"1","_version":1,"created":true}%  
```

Perform a date histogram facet (works):

```
&#10140;  ~  curl localhost:9200/my_index/my_type/_search -XPOST -d '{"facets":{"dates":{"date_histogram":{"field":"my_date","interval":"day"}}}}'

{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"my_index","_type":"my_type","_id":"1","_score":1.0, "_source" : {"my_date":"12-13-2014"}}]},"facets":{"dates":{"_type":"date_histogram","entries":[{"time":1389571200000,"count":1}]}}}%      
```

Perform an aggregation facet (fails with UnsupportedOperationException):

```
 &#10140;  ~  curl localhost:9200/my_index/my_type/_search -XPOST -d '{"aggs":{"dates":{"date_histogram":{"field":"my_date","interval":"day"}}}}'

{"error":"UnsupportedOperationException[Printing not supported]","status":500}% 
```

Change the mapping to contain only a single date format:

```
&#10140;  ~  curl localhost:9200/my_index/my_type/_mapping -XPUT -d '{"my_type":{"properties":{"my_date":{"type":"date","format":"dateOptionalTime"}}}}'

{"acknowledged":true}%                                                          
```

Check that the mapping changed:

```
&#10140;  ~  curl localhost:9200/my_index/my_type/_mapping      

{"my_index":{"mappings":{"my_type":{"properties":{"my_date":{"type":"date","format":"dateOptionalTime"}}}}}}%          
```

Run the original aggregation again, and see that it returns the same result as the facet:

```
&#10140;  ~  curl localhost:9200/my_index/my_type/_search -XPOST -d '{"aggs":{"dates":{"date_histogram":{"field":"my_date","interval":"day"}}}}'

{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"my_index","_type":"my_type","_id":"1","_score":1.0, "_source" : {"my_date":"12-13-2014"}}]},"aggregations":{"dates":{"buckets":[{"key_as_string":"2014-01-13T00:00:00.000Z","key":1389571200000,"doc_count":1}]}}}%                
```
</description><key id="33843098">6239</key><summary>Aggregations: date_histogram aggregation breaks on date fields with multiple formats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">maxlang</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T22:21:04Z</created><updated>2014-05-27T08:33:36Z</updated><resolved>2014-05-22T09:30:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="maxlang" created="2014-05-20T09:59:45Z" id="43607074">Here's the stacktrace FYI:

```
java.lang.UnsupportedOperationException: Printing not supported
at org.elasticsearch.common.joda.time.format.DateTimeFormatter.requirePrinter(DateTimeFormatter.java:660)
at org.elasticsearch.common.joda.time.format.DateTimeFormatter.print(DateTimeFormatter.java:598)
at org.elasticsearch.search.aggregations.support.numeric.ValueFormatter$DateTime.format(ValueFormatter.java:117)
at org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.toXContent(InternalHistogram.java:471)
at org.elasticsearch.search.aggregations.InternalAggregations.toXContentInternal(InternalAggregations.java:184)
at org.elasticsearch.search.aggregations.InternalAggregations.toXContent(InternalAggregations.java:175)
at org.elasticsearch.search.internal.InternalSearchResponse.toXContent(InternalSearchResponse.java:95)
at org.elasticsearch.action.search.SearchResponse.toXContent(SearchResponse.java:217)
at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:104)
at org.elasticsearch.rest.action.search.RestSearchAction$1.onResponse(RestSearchAction.java:98)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:198)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:180)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:156)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:150)
at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:407)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)
at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
```
</comment><comment author="colings86" created="2014-05-21T12:25:17Z" id="43747347">Documentation states that 

&gt; The first format will also act as the one that converts back from milliseconds to a string representation

Looks like this hasn't been set for the date_histogram aggregation
</comment><comment author="maxlang" created="2014-05-22T09:36:39Z" id="43866910">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>CharArraySet doesn't know how to lookup the original string in an ImmutableList.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6238</link><project id="" key="" /><description>This was causing the `stem_exclusion` list, when passed as an array, to be incorrect.

Closes #6237
</description><key id="33824593">6238</key><summary>CharArraySet doesn't know how to lookup the original string in an ImmutableList.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>:Analysis</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T18:29:54Z</created><updated>2015-06-07T20:02:11Z</updated><resolved>2014-05-19T19:30:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T19:23:05Z" id="43545032">oh I see - the problem is that ImmutableList.of() doesn't handle the array build a list of one object. Cool good catch
</comment><comment author="s1monw" created="2014-05-19T19:23:25Z" id="43545075">+1 to push to all our branches
</comment><comment author="clintongormley" created="2014-05-19T19:30:47Z" id="43545795">Merged

Closes #6237 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: `stem_exclusion` as array not working in language analyzers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6237</link><project id="" key="" /><description>```
DELETE /_all

PUT /t
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english": {
          "type": "english",
          "stem_exclusion": ["organization"]
        }
      }
    }
  }
}

GET /t/_analyze?analyzer=my_english&amp;text=organization
```

-&gt; returns term `organ`
</description><key id="33824442">6237</key><summary>Analysis: `stem_exclusion` as array not working in language analyzers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T18:27:59Z</created><updated>2014-07-16T12:09:29Z</updated><resolved>2014-05-19T19:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Register uppercase as an exposed ES token filter.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6236</link><project id="" key="" /><description>Just follow "lowercase" token filter example and register "uppercase" token filter as an exposed token filter. This will not, by itself, test whether ES is correctly handling "uppercase" TF; this is more of a "code as documentation" fix.

This PR is related to issue #6188.
</description><key id="33816977">6236</key><summary>Register uppercase as an exposed ES token filter.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tmacam</reporter><labels><label>test</label></labels><created>2014-05-19T16:50:04Z</created><updated>2014-07-11T00:06:24Z</updated><resolved>2014-05-19T18:20:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T17:12:25Z" id="43530790">this has already been added weeks ago here https://github.com/elasticsearch/elasticsearch/pull/5539

I think we can close this.
</comment><comment author="tmacam" created="2014-05-19T17:23:49Z" id="43532060">Well, this is just a minor update to a test that was not addressed in #5539.

In `AnalysisFactoryTests`,  `uppecase` is currently being associated with the Void.class and being labeled as a not exposed filter ( https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/index/analysis/AnalysisFactoryTests.java#L193). This PR just fixed that. I hope.
</comment><comment author="s1monw" created="2014-05-19T18:12:39Z" id="43537495">you are right - nevermind I will pull it in...
</comment><comment author="s1monw" created="2014-05-19T18:20:23Z" id="43538348">merged thx
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Test: Add dummy docs injection to indexRandom</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6235</link><project id="" key="" /><description>This adds `dummy docs` to `ElasticsearchIntegrationTest#indexRandom`.
It indexes document with an empty body into the indices specified by the docs
and deletes them after all docs have been indexed. This produces gaps in
the segments and enforces usage of accept docs on lower levels to ensure
the features work with delete documents as well.
</description><key id="33809787">6235</key><summary>Test: Add dummy docs injection to indexRandom</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T15:26:14Z</created><updated>2014-07-16T12:09:44Z</updated><resolved>2014-05-19T16:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-05-19T18:26:36Z" id="43539038">I really like that change!
</comment><comment author="s1monw" created="2014-05-19T18:28:51Z" id="43539268">@jpountz ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`limit` filter returns wrong results if deleted document are present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6234</link><project id="" key="" /><description>if delete documents are present and the `limit` filter is used the count might be wrong since `MatchDocIdSet#matchDoc(int)` is called before `acceptDocs` are checked in `MatchDocIdSet` 
</description><key id="33809324">6234</key><summary>`limit` filter returns wrong results if deleted document are present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T15:21:28Z</created><updated>2015-06-07T20:02:35Z</updated><resolved>2014-05-19T16:02:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add Groovy as a scripting language, add groovy sandboxing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6233</link><project id="" key="" /><description>Sandboxes the groovy scripting language with multiple configurable
whitelists:

`script.groovy.sandbox.receiver_whitelist`: comma-separated list of string
classes for objects that may have methods invoked.
`script.groovy.sandbox.package_whitelist`: comma-separated list of
packages under which new objects may be constructed.
`script.groovy.sandbox.class_whitelist` comma-separated list of classes
that are allowed to be constructed.

As well as a method blacklist:

`script.groovy.sandbox.method_blacklist`: comma-separated list of
methods that are never allowed to be invoked, regardless of target
object.

The sandbox can be entirely disabled by setting:

`script.groovy.sandbox.enabled: false`
</description><key id="33804618">6233</key><summary>Add Groovy as a scripting language, add groovy sandboxing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Scripting</label><label>feature</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T14:31:14Z</created><updated>2015-06-06T18:31:49Z</updated><resolved>2014-06-20T08:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T15:03:25Z" id="43515476">oh man this looks awesome! 
</comment><comment author="dakrone" created="2014-06-03T09:14:12Z" id="44941334">Added a number of commits to address the feedback, @s1monw can you take another look?
</comment><comment author="s1monw" created="2014-06-12T11:00:44Z" id="45878321">did another review.... looks pretty close... left some commetns
</comment><comment author="dakrone" created="2014-06-16T12:51:39Z" id="46173775">@s1monw updated this PR with all of the changes you recommended :)
</comment><comment author="s1monw" created="2014-06-18T11:17:32Z" id="46422845">this actually LGTM maybe @kimchy want's to take a look?
good job!
</comment><comment author="kimchy" created="2014-06-18T11:50:20Z" id="46425590">@dakrone this looks great!, a few comments:
- I would add a 3 value setting to dynamic script enable now, true, false, and sandbox.

For 2.0:
- I would remove mvel completely
- Default to sandbox in dynamic script setting, and groovy as the lang.

For 1.3:
- I would suggest also removing mvel completely, aside from security, its just buggy, if we decide to, then its the same changes as 2.0, including properly documenting how to install the mvel plugin and setting mvel as the lang by default, and properly mark it as a breaking change.
- If there is resistance from removing mvel, then keep mvel around and keep it as the default lang, next to groovy being an option. Default dynamic script should still be sandbox.
</comment><comment author="dakrone" created="2014-06-18T14:31:17Z" id="46442906">@kimchy Sounds good, I updated the documentation and completely removed mvel as well. I also added the 3-value setting for dynamic scripting.
</comment><comment author="s1monw" created="2014-06-18T18:59:17Z" id="46478535">so this means that upgrading to `1.3` from any prev version means you need to install the mvel plugin on the upgrade and pass `mvel` as a param in the request to opt in to this language? I think we really need to invest into an upgrade guide here!
</comment><comment author="dakrone" created="2014-06-19T09:04:21Z" id="46538260">@s1monw yes, the mvel plugin will have to be installed and the `script.default_lang: mvel` option would have to be added to continue using mvel as the default.

For an upgrade guide, do we want to add the two settings to the release notes? It will also be on the scripting blog post to come out after this is merged as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use Lucene's query rescorer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6232</link><project id="" key="" /><description>A query rescorer was added to lucene that basically does the same as our
rescorer. In-fact we designed it to work pretty similar. This commit
cuts over to the lucene version shipped with 4.8
</description><key id="33798455">6232</key><summary>Use Lucene's query rescorer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label></labels><created>2014-05-19T13:24:27Z</created><updated>2015-03-19T09:25:43Z</updated><resolved>2014-10-18T09:27:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T13:26:01Z" id="43502135">I just integrated this and ran the tests... yet I ran into [a lucene bug](https://issues.apache.org/jira/browse/LUCENE-5682) with the lucene rescorer... next time I will integrate earlier to make sure we catch those bugs earlier...
</comment><comment author="mikemccand" created="2014-05-19T13:50:06Z" id="43505874">Whoa, awesome!  I didn't realize this would be so simple.  Issue #5922 is open for this...
</comment><comment author="nik9000" created="2014-05-19T14:06:03Z" id="43508142">Nice!
</comment><comment author="s1monw" created="2014-07-09T10:15:27Z" id="48452596">I just pushed a new commit for this. The bug that I referenced is fixed but there are several problems with the lucene APIs that I personally think it's not worth it unless lucene fixes it's APIs I had to do a lot of things on top that are really confusing IMO
</comment><comment author="mikemccand" created="2014-07-09T13:58:34Z" id="48473562">We can fix the Lucene APIs: they are experimental.  E.g. we can change it to do the rescoring "in-place", but I find this sort of strange.  The full-resort after rescoring only a subset is odd because you're comparing apples to oranges at that point...

Alternatively, we could just add a utility API in Lucene on top of its existing APIs to do the in-place rescoring that ES wants, i.e. extract the topN from the first pass, rescore all of those, put back in and do a full resort.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing invalid jsons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6231</link><project id="" key="" /><description /><key id="33794411">6231</key><summary>Fixing invalid jsons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>docs</label></labels><created>2014-05-19T12:33:17Z</created><updated>2014-06-12T12:17:38Z</updated><resolved>2014-05-19T13:08:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T12:53:11Z" id="43497523">thanks for submitting - can you sign the CLA so I can pull it in?
</comment><comment author="synhershko" created="2014-05-19T12:57:23Z" id="43498102">Done, see you next week
</comment><comment author="s1monw" created="2014-05-19T13:08:44Z" id="43499678">pushed to 1.x and master - cu @ bbuzz!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Do not execute cluster state changes if current node is no longer master</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6230</link><project id="" key="" /><description>When a node steps down from being a master (because, for example, min_master_node is breached), it may still have cluster state update tasks queued up. Most (but not all) are tasks that should no longer be executed as the node no longer has authority to do so. Other cluster states updates, like electing the current node as master, should be executed even if the current node is no longer master.

This commit make sure that, by default, `ClusterStateUpdateTask` is not executed if the node is no longer master. Tasks that should run on non masters are changed to implement a new interface called `ClusterStateNonMasterUpdateTask`

Note - this PR is done against the `feature/improve_zen` branch, where we will give CI some time test it.
</description><key id="33788475">6230</key><summary>Do not execute cluster state changes if current node is no longer master</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>bug</label><label>resiliency</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T10:55:10Z</created><updated>2015-06-07T20:02:55Z</updated><resolved>2014-05-21T11:34:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-05-20T09:53:46Z" id="43606505">LGTM
</comment><comment author="martijnvg" created="2014-05-21T10:48:20Z" id="43739943">LGTM for the 2nd commit!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove backward compatibility layer introduced in #6149</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6229</link><project id="" key="" /><description>PR #6149 changed the `Versions.MATCH_ANY` value  from 0 to -3. This was done to allow 0 to be a valid external version. This commit removes the backward compatibility code from v2.0 and up.
</description><key id="33786676">6229</key><summary>Remove backward compatibility layer introduced in #6149</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:CRUD</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T10:26:05Z</created><updated>2015-06-08T08:45:39Z</updated><resolved>2015-03-24T12:16:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-19T12:34:34Z" id="43495852">do we still want to to use magic negative numbers for `MATCH_ANY`, `NOT_FOUND` and `NOT_SET` if we break bwc? having an enum represented by a byte and maybe a boolean indicating if a real version or the enum is written might work as well in the serialization?
</comment><comment author="bleskes" created="2014-05-19T14:34:12Z" id="43511569">@spinscale we actually only need to worry about `MATCH_ANY`. The rest isn't serializeed. So the stream level is easy (one added boolean). Not sure how the java api would like though .. now we have `.setVersion(Versions.MATCH_ANY)` ..
</comment><comment author="s1monw" created="2014-06-12T13:00:46Z" id="45887871">LGTM
</comment><comment author="clintongormley" created="2014-08-22T16:59:40Z" id="53089415">@bleskes needs merging, no?
</comment><comment author="clintongormley" created="2014-11-11T18:59:13Z" id="62597734">@bleskes still needs merging?
</comment><comment author="javanna" created="2015-03-21T08:44:50Z" id="84282995">hey @bleskes this can be closed as we already removed all of the bw comp code from master right?
</comment><comment author="bleskes" created="2015-03-23T20:45:05Z" id="85187502">@javanna not completely. Updated the PR to the latest master. Can you take a look? 
</comment><comment author="javanna" created="2015-03-24T09:13:52Z" id="85413382">LGTM
</comment><comment author="s1monw" created="2015-03-24T09:14:19Z" id="85413485">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The `clear_caches` setting for the Benchmark API doesn't accept `true`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6228</link><project id="" key="" /><description>The `clear_caches` setting for the [benchmark API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-benchmark.html#search-benchmark) does not accept a `true` value.

Since it can accept a `false` value, accepting `true` as well is expected -- more so, when the documentation uses the language usually used for boolean values, &#8220;Whether caches should be cleared on each iteration (...)&#8221;.

``` bash
curl -X PUT 'http://localhost:9200/_bench?pretty' -d '{
  "name":"my_benchmark",
  "clear_caches":true,
  "competitors":[
    {
      "name":"query_1",
      "requests":[
        {
          "query":{
            "match":{
              "_all":"a*"
            }
          }
        }
      ]
    }
  ]
}'

# [400] (0.004s)

{"error":"Failed parsing field [clear_caches] must specify which caches to clear"}
```
</description><key id="33784617">6228</key><summary>The `clear_caches` setting for the Benchmark API doesn't accept `true`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2014-05-19T09:51:46Z</created><updated>2015-08-26T14:59:38Z</updated><resolved>2015-08-26T14:59:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T11:30:56Z" id="43491195">I think this is a documenation issue - it can't accept `false` either
</comment><comment author="jpountz" created="2015-08-26T14:59:38Z" id="135050391">The benchmark API efforts have been discontinued.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>More strict parsing of ISO dates</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6227</link><project id="" key="" /><description>If you are using the default date or the named identifiers of dates,
the current implementation was allowed to read a year with only one
digit. In order to make this more strict, this fixes a year to be at
least 4 digits.

Closes #6158
</description><key id="33781826">6227</key><summary>More strict parsing of ISO dates</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>:Dates</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T09:10:02Z</created><updated>2015-07-07T07:38:26Z</updated><resolved>2015-07-07T07:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-20T09:05:12Z" id="43602202">talked with @clintongormley - makes sense to not replace those dates, but create new ones with a 'strict' prefix (like `strictDateOptionalTime`), which forces the year to have at least 4 digits, month 2 and day 2 digits as well. And this will be the default for named dates and automatic recognition
</comment><comment author="spinscale" created="2014-05-28T11:05:16Z" id="44391734">@clintongormley updated the PR, maybe you can take another look

Changes:
- Have a `strict` date format for each existing, where it made sense, which always requires you to have two digit months, days, weeks, hours, minutes and seconds and at least 4 digit years.
- Made the `strictOptionalDateTime` the default
- Added a couple of missing formats, which existed in the docs though (and vice versa)
- Added a lot of tests to make sure the parsing is actually strict

If this is ok, I will need to update documentation, I will add the missing formats and their format (added this info in the test so far), and mention the `strict` ones. And see, if we can reuse some of the code in order to not duplicate the DateFormat class.
</comment><comment author="clintongormley" created="2014-05-29T18:27:12Z" id="44566503">hi @spinscale 

Almost there, but there are two default formats used for detection.  one is now strict ,but the other is not:  https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/object/RootObjectMapper.java#L45

This means that this illegal date is still mapped:

```
PUT /t/t/1
{
  "date": "5/11/10"
}

GET /_mapping
```
</comment><comment author="spinscale" created="2014-05-30T15:32:00Z" id="44664312">Updated the rootobjectmapper to have a more strict date. Talked with @clintongormley to possible have strict mapping for self formatted dates as well, like `yyyy`, from what I read so far, this might be tricky, but I need to dig deeper into joda for this.
</comment><comment author="spinscale" created="2014-06-17T06:25:07Z" id="46271466">@clintongormley ok, so having a parsing format like `yyyy` does not seem possible at the moment... are still good to go with the rest? If so, I would add documentation and maybe try to cut out some code if possible
</comment><comment author="clintongormley" created="2014-06-17T17:41:36Z" id="46340454">LGTM
</comment><comment author="clintongormley" created="2014-07-11T09:42:54Z" id="48712195">@spinscale any news on getting this one merged?
</comment><comment author="clintongormley" created="2014-08-08T09:49:04Z" id="51582612">@brwe When this PR is in, then we can probably implement #1694 as well, as `1T` will no longer be accepted as a date.
</comment><comment author="clintongormley" created="2014-10-20T13:16:17Z" id="59751606">@spinscale giving this one back to you :)
</comment><comment author="clintongormley" created="2014-12-29T13:57:59Z" id="68259113">Also see #5328
</comment><comment author="spinscale" created="2015-05-20T11:49:29Z" id="103855634">I dug a bit around in joda time and still do not see any other pattern than to copy  the whole class and use the fixed date builders. Another approach would have been to take supplied date string and compare it with the date being created by the date formatter - if it doesnt match, then the formatting has been different. However this flaw has the problem, that we need to parse the date twice and it becomes really complex, when we use the `||` for allowing several dates in one field, as each format then needs to be tried out. This also has a performance penalty during index time.
</comment><comment author="spinscale" created="2015-05-22T11:56:54Z" id="104643035">Rebased this PR against master, fixed tests with loose dates, added documentation and added BWC check, so that indices created before 2.0.0 will fall back to the current defaults.
</comment><comment author="spinscale" created="2015-06-25T11:42:04Z" id="115217666">incoporporated your review comments @bleskes 
</comment><comment author="bleskes" created="2015-06-29T13:06:49Z" id="116652700">I went through and left some comments. Looks nice. I think we also need to document this on the 2.0 migration doc. Also would love someone who is more at home with the mapping code to have a look /cc @rjernst 
</comment><comment author="rjernst" created="2015-06-29T19:44:58Z" id="116814367">I left a couple comments on the mapping portion.
</comment><comment author="rjernst" created="2015-07-02T05:58:41Z" id="117919110">LGTM
</comment><comment author="spinscale" created="2015-07-02T06:29:09Z" id="117923462">@bleskes are you good with the documentation changes?
</comment><comment author="bleskes" created="2015-07-02T10:14:25Z" id="117990214">left one minor comment. I think we also need an entry in migrate_2_0.asciidoc ?
</comment><comment author="spinscale" created="2015-07-06T11:12:30Z" id="118820567">fixed the typo and added a small paragraph to the upgrade asciidoc
</comment><comment author="bleskes" created="2015-07-06T17:47:52Z" id="118937102">LGTM. Thx @spinscale 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Honor time delay when retrying recoveries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6226</link><project id="" key="" /><description>In some places we want to delay the start of a shard recovery because the source node is not ready to receive. At the moment the retry logic ignores the time delay parameter (`retryAfter`) causing a busy waiting like scenario. This is fixed in this commit.
</description><key id="33776281">6226</key><summary>Honor time delay when retrying recoveries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>bug</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-19T07:31:55Z</created><updated>2015-06-07T20:03:06Z</updated><resolved>2014-05-20T09:05:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T09:44:13Z" id="43483183">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve benchmark tests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6225</link><project id="" key="" /><description>This commit improves the ability to test the benchmark API by adding
several features.

Pause/resume: New API commands are added to pause running benchmarks and
to resume them. The commands are exposed in the REST API at
/_bench/(pause|resume).

Control commands: Adding the pause/resume resulted in a lot of duplicate
code. Consequently, this commit consolidates the code for 3 different
API commands into one. The result is a new control API that handles the
logic for pause, resume, and status requests. REST API has not changed
for status requests. The Java API has been slightly modified to accept a
control command parameter to specify pause/resume/status.

Mocking: A mock service is implemented which provides the ability to
pause benchmarks after they have been submitted, but before they
actually start executing. This allows easy testing of various benchmark
states w/o having to do tricks to coerce runtime state.

Closes #6224
</description><key id="33764359">6225</key><summary>Improve benchmark tests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels><label>stalled</label></labels><created>2014-05-19T00:06:39Z</created><updated>2015-06-08T14:35:47Z</updated><resolved>2014-10-21T23:29:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T07:59:08Z" id="43474939">I moved this to `1.3`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add mock benchmark service for testing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6224</link><project id="" key="" /><description>The benchmark API needs better mock classes for testing.
</description><key id="33762588">6224</key><summary>Add mock benchmark service for testing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-18T22:19:54Z</created><updated>2015-10-14T16:33:48Z</updated><resolved>2015-10-14T16:33:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Do not throw an exception on unavailable nodes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6223</link><project id="" key="" /><description>Changed behavior to not throw an exception on a status or abort request
when there are no available benchmark nodes.

Closes #6146
</description><key id="33762547">6223</key><summary>Benchmark: Do not throw an exception on unavailable nodes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>enhancement</label></labels><created>2014-05-18T22:17:40Z</created><updated>2015-03-19T20:05:17Z</updated><resolved>2014-05-19T18:43:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T08:02:46Z" id="43475203">IMO we should only do this for list and not for abort. That means this is effectively a one line change can you please revert the abort related changes?
</comment><comment author="s1monw" created="2014-05-19T18:38:54Z" id="43540386">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark abort accepts wildcard patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6222</link><project id="" key="" /><description>This adds support for sending a list of benchmark names and/or wildcard
patterns when submitting an abort request. Standard wildcards such as:
"_", "_xxx", and "xxx*" are supported. Multiple benchmark names and
wildcard patterns can be submitted together as a comma-separated list
from the REST API or as a string array from the Java API.

Closes #6185
</description><key id="33762146">6222</key><summary>Benchmark abort accepts wildcard patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-18T21:55:39Z</created><updated>2014-10-21T23:34:46Z</updated><resolved>2014-05-20T14:34:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-19T07:56:24Z" id="43474750">I left a bunch of comments - I also don't see a single test that checks if the patterns work, can we have a test here as well?
</comment><comment author="s1monw" created="2014-05-19T07:58:21Z" id="43474887">FYI I removed the tags on this PR in favor of the issues tags!
</comment><comment author="s1monw" created="2014-05-20T14:34:53Z" id="43633882">superseded by #6245 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't report terms as live if all it's docs are filtered out</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6221</link><project id="" key="" /><description>FilterableTermsEnum allows to filter stats by supplying per segment
bits. Today if all docs are filtered out the term is still reported as
live but shouldn't.

Relates to #6211
</description><key id="33759446">6221</key><summary>Don't report terms as live if all it's docs are filtered out</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Core</label><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-18T20:24:27Z</created><updated>2015-06-07T20:03:26Z</updated><resolved>2014-05-19T11:54:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-05-19T10:37:59Z" id="43487344">Looks good - the only possible side effect is in FilterableTermsEnum.seekExact() any IOException will now leave currentDocFreq as prior value whereas before it would set NOT_FOUND. I don't think we define expected behaviour or rely on it in the event of IOException so should be fine.
</comment><comment author="s1monw" created="2014-05-19T10:56:39Z" id="43488682">&gt; Looks good - the only possible side effect is in FilterableTermsEnum.seekExact() any IOException will now leave currentDocFreq as prior value whereas before it would set NOT_FOUND. I don't think we define expected behaviour or rely on it in the event of IOException so should be fine.

IMO what we see now is the right behavior. if we see and exception we don't change the internal state.
</comment><comment author="markharwood" created="2014-05-19T11:02:24Z" id="43489067">Makes sense
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Need to add /usr/share/elasticsearch/bin when seraching for elasticsearch.bin.sh to  elaslticsearch script </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6220</link><project id="" key="" /><description>After installation in unbuntu desktop 13.x, without ES_HOME set.
A bit of digging shows that elasticsearch shell script does not include bin.
Making a symbolic link with bin, solves the issue.

starting elasticserach complains about missing ES_CLASSPATH being absent.
Error message:
/usr/share/elasticsearch/bin/elasticsearch: 1: /usr/share/elasticsearch/bin/elasticsearch: dirname: not found
/usr/share/elasticsearch/bin/elasticsearch: 1: /usr/share/elasticsearch/bin/elasticsearch: dirname: not found
You must set the ES_CLASSPATH va

Looks like you need to include "bin" in some system:
Details:
## In ElasticSearch.sh in /usr/share/elasticsearch/bin/elasticsearch, does not include "bin":
# If an include wasn't specified in the environment, then search for one...

if [ "x$ES_INCLUDE" = "x" ]; then
    # Locations (in order) to use when searching for an include file.
    for include in /usr/share/elasticsearch/elasticsearch.in.sh \
                   /usr/local/share/elasticsearch/elasticsearch.in.sh \
                   /opt/elasticsearch/elasticsearch.in.sh \
                   ~/.elasticsearch.in.sh \
                   "`dirname "$0"`"/elasticsearch.in.sh; do
        if [ -r "$include" ]; then
            . "$include"
            break
        fi
    done
</description><key id="33758380">6220</key><summary>Need to add /usr/share/elasticsearch/bin when seraching for elasticsearch.bin.sh to  elaslticsearch script </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">linearregression</reporter><labels><label>:Packaging</label><label>discuss</label></labels><created>2014-05-18T19:33:14Z</created><updated>2015-10-14T16:33:29Z</updated><resolved>2015-10-14T16:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-19T06:37:48Z" id="43470054">hey,

the idea with the packages is, that you use the init scripts to easily be able to start/stop elasticsearch. Was there any reason, why you cannot use those? Still it might make sense to add some extra feature in the scripts here, so one can start elasticsearch manually as well.
</comment><comment author="linearregression" created="2014-05-21T00:25:39Z" id="43699762">I built from source to try things out and kinda notice that. This is more like a "nice to get that add in", as people can always set the ES_HOME also.
</comment><comment author="clintongormley" created="2015-10-14T16:33:29Z" id="148108183">Closing as won't fix
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#6187 Don't create AllTokenStream when all fields have default boost</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6219</link><project id="" key="" /><description /><key id="33748341">6219</key><summary>#6187 Don't create AllTokenStream when all fields have default boost</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels /><created>2014-05-18T09:23:12Z</created><updated>2014-06-15T18:20:48Z</updated><resolved>2014-05-20T14:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2014-05-18T18:39:38Z" id="43447589">wondering, if the reason for this change is to optimize the overhead of creating the token stream + analysis attributes, maybe we should have a thread local AllField so we won't recreate it every time even when there is custom boost on one of the fields?
</comment><comment author="mikemccand" created="2014-05-18T19:00:42Z" id="43448193">In 4.9 with the new Field TokenStream reuse API (IW passes previous tokenStream to Field.tokenStream), we can reuse the AllTokenStream instance from before.

But I'm not sure how much of hit that is here ... the binary search per incrementToken is likely costly?

I think we should just remove AllField/AllTokenStream and instead append multiple _all Field instances directly to the Lucene Document?  Lucene concatenates the tokens itself ... so then we don't need the binary search.

Or Simon suggested maybe somehow using copy field, except it doesn't carry over boosts.
</comment><comment author="kimchy" created="2014-05-18T19:05:32Z" id="43448354">I see, in any case, this optimization makes a lot of sense when all boosts are default. Wondered if this came out of perf numbers that resulted form the analysis attributes, or the boost value lookup.
</comment><comment author="mikemccand" created="2014-05-18T19:13:11Z" id="43448543">I only noticed this because I was running ES under a profiler during bulk indexing, and this popped up in one of the stack traces; I never narrowed it down to specifically where the slowness was.

But I did do a quick before/after test with append-only log file docs (1 shard, 0 replicas, no index throttling, 1 indexing thread) and it looks like indexing was a few percent faster with this optimization, though the runs are noisy.

I'll add a static assert to remind us to cutover to reusing the AllTokenStream instance when we upgrade to Lucene 4.9.
</comment><comment author="mikemccand" created="2014-05-20T08:46:14Z" id="43600499">I added a few tests to make sure we get AllTokenStream or don't get it in the various cases, and also for the AllTermQuery when offsets were indexed; I think this is ready.  I'll commit soon to master &amp; 1.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update nodes-stats.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6218</link><project id="" key="" /><description>simple changes in stats example i have correct the ? with &amp; for pretty option not so much :)
</description><key id="33748124">6218</key><summary>Update nodes-stats.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">darkwarriors</reporter><labels><label>docs</label></labels><created>2014-05-18T09:04:43Z</created><updated>2014-06-15T15:01:25Z</updated><resolved>2014-06-03T11:48:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-03T11:21:57Z" id="44951820">Hi @darkwarriors could you please sign our [CLA](http://www.elasticsearch.org/contributor-agreement/) so we can merge this in?
</comment><comment author="darkwarriors" created="2014-06-03T11:44:52Z" id="44954114">hi @javanna i have done it just now
</comment><comment author="javanna" created="2014-06-03T11:48:59Z" id="44954575">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use default forceAnalyzeQueryString if no query builder is present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6217</link><project id="" key="" /><description>In the single field case no query builder is selected which causes NPE
when the query has only a numeric field.

Closes #6215
</description><key id="33746478">6217</key><summary>Use default forceAnalyzeQueryString if no query builder is present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Search</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-18T06:35:22Z</created><updated>2015-06-07T20:04:07Z</updated><resolved>2014-05-18T08:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix offsets in DateHistogramBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6216</link><project id="" key="" /><description>This PR fixes #5586 by correcting the behavior of the current `preOffset` and `postOffset` methods in DateHistogramBuilder and adds new versions of these methods that take `DateHistogram.Interval`.  It is patterned after the interval property that already exists in this class.

This PR is an alternative to #5587.  That PR removes the current methods and replaces them with `String` based fields.  This seems inconsistent with the other methods in the class and causes a change to the contract currently established by DateHistogramBuilder.
</description><key id="33742553">6216</key><summary>Fix offsets in DateHistogramBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ctrimble</reporter><labels /><created>2014-05-18T00:26:30Z</created><updated>2014-08-08T09:09:07Z</updated><resolved>2014-08-08T09:08:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-08-08T09:08:59Z" id="51579281">Closing as this has already been fixed by #6814
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search by integer NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6215</link><project id="" key="" /><description>I have field on index is Long type, when I try to search, I'm getting NullPointerException. 

Elasticsearch: Version 1.1.1
Here's my traceback:

[2014-05-17 21:09:18,922][DEBUG][action.search.type       ] [Orbit] [skoob][0], node[LlsPNTswRgGWnzxnGrgBMQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2f53db06] lastShard [true]
org.elasticsearch.search.SearchParseException: [index3][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
   "query": {
      "multi_match": {
         "query": 2014,
         "fields": [
            "year"
         ]
      }
   },
   "size": 20
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:507)
    at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.search.MultiMatchQuery.forceAnalyzeQueryString(MultiMatchQuery.java:266)
    at org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:178)
    at org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:57)
    at org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:71)
    at org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:164)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:223)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:330)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)
    ... 11 more
</description><key id="33742439">6215</key><summary>Search by integer NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">farchanjo</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-18T00:15:03Z</created><updated>2014-05-21T12:57:23Z</updated><resolved>2014-05-18T08:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-18T06:35:13Z" id="43432693">thanks for reporting this - I already referenced a commit and will fix that very soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MultiPercolateRequest breaks when setting Routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6214</link><project id="" key="" /><description>Edit test:
https://github.com/elasticsearch/elasticsearch/blob/master/src/test/java/org/elasticsearch/percolator/MultiPercolatorTests.java

``` java
        MultiPercolateRequestBuilder builder = client().prepareMultiPercolate();
        int numPercolateRequest = randomIntBetween(50, 100);
        for (int i = 0; i &lt; numPercolateRequest; i++) {
            builder.add(
                    client().preparePercolate()
                            .setGetRequest(Requests.getRequest("test").type("type").id("1"))
                            .setRouting("test")
                            .setIndices("test").setDocumentType("type"));
        }

        MultiPercolateResponse response = builder.execute().actionGet();
```
</description><key id="33742334">6214</key><summary>MultiPercolateRequest breaks when setting Routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">justjico</reporter><labels /><created>2014-05-18T00:06:07Z</created><updated>2015-01-07T07:56:14Z</updated><resolved>2015-01-07T07:56:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T18:55:47Z" id="68385221">@martijnvg could you look at this one please?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"porter2" option in stemmer token filter return porter v 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6213</link><project id="" key="" /><description>There are two implementations of the original Porter stemmer in Lucene:
- PorterStemFilter
- PorterStemmer (based on Snowball)

The Porter2 algorithm is actually implemented as the EnglishStemmer (based on Snowball).

However, the StemmerTokenFilter uses this logic:

```
    } else if ("english".equalsIgnoreCase(language)) {
        return new SnowballFilter(tokenStream, new EnglishStemmer());
    ...
    } else if ("porter".equalsIgnoreCase(language)) {
        return new PorterStemFilter(tokenStream);
    } else if ("porter2".equalsIgnoreCase(language)) {
        return new SnowballFilter(tokenStream, new PorterStemmer());
```

The `porter2` language actually returns the original Porter algorithm, when it should return the EnglishStemmer.

I don't know if there are any differences in performance or output between the PorterStemmer and the PorterStemFilter, but possibly we should just expose one of those classes via the `porter` option.  

Not sure what to do with `porter2`.  To be correct, it should be changed to point to the EnglishStemmer, but that's not backwards compatible.  Possibly we should just deprecate it (and warn when it is used) then later remove it. 
</description><key id="33729515">6213</key><summary>"porter2" option in stemmer token filter return porter v 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>bug</label></labels><created>2014-05-17T12:17:10Z</created><updated>2014-06-11T10:31:18Z</updated><resolved>2014-06-11T10:31:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rmuir" created="2014-05-17T12:25:30Z" id="43405490">There are actually 3 algorithms:

PorterStemFilter: this is the fast one. its like twice as fast indexing as using Snowball.
Snowball(Porter): this is _almost_ exactly identical to PorterStemFilter, but much slower and the "original" IIRC. Whereas PorterStemFilter is a little refined (see "Points of difference from the published algorithm" at http://tartarus.org/martin/PorterStemmer/)
Snowball(English): Also slow, but implements the Porter2 algorithm (differences described here: http://snowball.tartarus.org/algorithms/english/stemmer.html)

So whatever we do, we should encourage PorterStemFilter in my opinion because the differences here are all very minor, but this one is much faster.
</comment><comment author="clintongormley" created="2014-05-17T12:32:05Z" id="43405639">OK, then I suggest that all we do here is to deprecate the `porter2` option and warn on use (at least in the 1.x branch) and remove it in master.
</comment><comment author="clintongormley" created="2014-05-17T12:34:33Z" id="43405687">Note: the porter-in-snowball version can still be accessed via the `snowball` token filter. I'm just talking about deprecating accessing that class via the `stemmer` token filter.
</comment><comment author="rmuir" created="2014-05-17T12:39:40Z" id="43405807">@clintongormley sounds like the right solution to me.

Related, I like how this factory is otherwise named by language. I really wish "english" was just an alias to "porter" (so you get the fast one if you do that), and "kstem" was instead "light_english" (for consistency). But I suppose thats more complicated...
</comment><comment author="clintongormley" created="2014-05-17T12:48:04Z" id="43405980">Hmm, so you'd actually rather repoint `english` to `porter`,  and repoint `porter2` to EnglishStemmer?  Yeah, there's no easy migration path that I can see there.

Adding `light_english` as an alias for `kstem` should be easy though.
</comment><comment author="rmuir" created="2014-05-17T12:50:27Z" id="43406043">well personally i'd prefer porter and porter2 didnt exist here at all and it was just the language names (with minimal and light options). So I am +1 for deprecating porter2 option!

I am just unhappy that if you supply "english", you get a slow stemmer.
</comment><comment author="clintongormley" created="2014-06-10T10:21:40Z" id="45596797">Currently the `stemmer` token filter returns these stemmers:

```
english                 Snowball English (Porter2)
porter                  Porter Stemmer (fast)
porter2                 Snowball Porter
kstem                   kstem
```

Note: `porter2` has been undocumented since Nov 2012.  See #2451

Instead we should use return these:

```
english                 Porter Stemmer (fast)
porter                  Porter Stemmer (fast)
porter2                 Snowball English (Porter2)
light_english           kstem
```

Can we use the index version to return the old stemmers on an existing index and the new stemmers for a new index?  Of course, if somebody is using these algorithms outside Elasticsearch, then this could be problematic, as it is a breaking change.  Alternatively, we could do:

```
fast_english            Porter Stemmer (fast)
english                 Snowball English (Porter2)
porter                  Porter Stemmer (fast) (remove docs)
porter2                 (remove code)
light_english           kstem
kstem                   (remove docs)
```
</comment><comment author="rmuir" created="2014-06-10T10:27:14Z" id="45597246">&gt; Of course, if somebody is using these algorithms outside Elasticsearch, then this could be problematic, as it is a breaking change.

What do you mean? Via the analyze API or whatever? we just document the change for such people, they cant hold us back and they have no index to reindex.
</comment><comment author="clintongormley" created="2014-06-10T10:29:53Z" id="45597439">I'm referring to the comment https://github.com/elasticsearch/elasticsearch/issues/2451#issuecomment-10964769 (not necessarily using the analyze api)
</comment><comment author="rmuir" created="2014-06-10T11:07:07Z" id="45600211">I see, i dont think there is much we can do there except document what we did... 

+1 for improving the English situation here! AFAIK NgramTokenizerFactory is an example of one that currently peeks at the index version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexing: Versions.loadDocIdAndVersion should reuse TermsEnums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6212</link><project id="" key="" /><description>Today, every version lookup creates a new TermsEnum for each segment in the index, but this is quite costly, e.g. on NIOFSDir it must clone the IO buffer, and because BlockTreeTermsReader has a lot of internal state.

We'd need a ThreadLocal somewhere/somehow... I have a start at a utility class here: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5675/lucene/test-framework/src/java/org/apache/lucene/index/PerThreadPKLookup.java maybe we can adapt/use this.
</description><key id="33690225">6212</key><summary>Indexing: Versions.loadDocIdAndVersion should reuse TermsEnums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/mikemccand/following{/other_user}', u'events_url': u'https://api.github.com/users/mikemccand/events{/privacy}', u'organizations_url': u'https://api.github.com/users/mikemccand/orgs', u'url': u'https://api.github.com/users/mikemccand', u'gists_url': u'https://api.github.com/users/mikemccand/gists{/gist_id}', u'html_url': u'https://github.com/mikemccand', u'subscriptions_url': u'https://api.github.com/users/mikemccand/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/796508?v=4', u'repos_url': u'https://api.github.com/users/mikemccand/repos', u'received_events_url': u'https://api.github.com/users/mikemccand/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/mikemccand/starred{/owner}{/repo}', u'site_admin': False, u'login': u'mikemccand', u'type': u'User', u'id': 796508, u'followers_url': u'https://api.github.com/users/mikemccand/followers'}</assignee><reporter username="">mikemccand</reporter><labels><label>enhancement</label><label>release highlight</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T16:36:36Z</created><updated>2014-07-25T15:00:29Z</updated><resolved>2014-05-31T21:37:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>[TEST] NPE with seed ED695B6A3A249CA6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6211</link><project id="" key="" /><description>With -Dtests.seed=ED695B6A3A249CA6, in `FreqTermsEnumTests` term has been skipped so not available in reference array.
</description><key id="33684587">6211</key><summary>[TEST] NPE with seed ED695B6A3A249CA6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2014-05-16T15:28:19Z</created><updated>2015-06-10T09:36:12Z</updated><resolved>2014-05-19T01:05:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="markharwood" created="2014-05-16T17:21:04Z" id="43356948">I don't think this should go in as I think it is masking something fishy.

If I run the unmodified master test and remove the "if(rarely()" around the writer commit calls the test passes with this seed. If I leave the "rarely" bit in we have issues with deleted items and this seed.
Needs some further investigation into commits and visibility of deleted items
</comment><comment author="s1monw" created="2014-05-16T20:02:49Z" id="43373496">I think this is actually a bug in the test code `FilterableTermsEnum` claims it has found the term which is true but all docs that contain it are filtered out so we essentially return a docFreq = 0 now the question is should we return the term as `found` if no document is `live`? ie use the patch below:

```
diff --git a/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java b/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java
index 2591dc0..74834f7 100644
--- a/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java
+++ b/src/main/java/org/elasticsearch/common/lucene/index/FilterableTermsEnum.java
@@ -165,6 +165,12 @@ public class FilterableTermsEnum extends TermsEnum {
             currentTotalTermFreq = totalTermFreq;
             current = text;
         }
+        if (found &amp;&amp; currentDocFreq == 0) {
+            current = null;
+            currentDocFreq = -1;
+            currentTotalTermFreq = -1;
+            return false;
+        }
         return found;
     }
```

or fix the test to just ignore that in the filtered case:

```
diff --git a/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java b/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
index e05ef1d..4c30e73 100644
--- a/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
+++ b/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
@@ -200,10 +200,10 @@ public class FreqTermsEnumTests extends ElasticsearchLuceneTestCase {
                 if (!termsEnum.seekExact(new BytesRef(term))) {
                     continue;
                 }
-                if (docFreq) {
+                if (docFreq &amp;&amp; termsEnum.docFreq() &gt; 0) {
                     assertThat("cycle " + i + ", term " + term + ", docFreq", termsEnum.docFreq(), equalTo(reference.get(term).docFreq));
                 }
-                if (totalTermFreq) {
+                if (totalTermFreq &amp;&amp; termsEnum.docFreq() &gt; 0) {
                     assertThat("cycle " + i + ", term " + term + ", totalTermFreq", termsEnum.totalTermFreq(), equalTo(reference.get(term).totalTermFreq));
                 }
             }
```

or alternatively 

```
diff --git a/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java b/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
index e05ef1d..aac36ba 100644
--- a/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
+++ b/src/test/java/org/elasticsearch/common/lucene/index/FreqTermsEnumTests.java
@@ -138,6 +138,11 @@ public class FreqTermsEnumTests extends ElasticsearchLuceneTestCase {
                 }
             }
         }
+        for (String term : terms) {
+            if(!referenceFilter.containsKey(term)) {
+               referenceFilter.put(term, new FreqHolder());
+            }
+        }
         filter = new TermsFilter(filterTerms);
     }
```
</comment><comment author="jpountz" created="2014-05-17T09:00:10Z" id="43401792">+1 to not reporting a term as live if it has a freq of 0.
</comment><comment author="s1monw" created="2014-05-18T19:59:31Z" id="43449830">I agree with @jpountz we should not report the term as live. If we do this we should also fix `FreqTermsEnum` to only cache the `currentDocFreq` and `currentTotalTermFreq` if we actually found the term. if it's not found it should manually put `-1` in for those values as `seekExact` etc. doesn't guarantee to reset those values to `-1` 

given that this is new I guess we should fix it for `1.2`
</comment><comment author="s1monw" created="2014-05-18T20:25:07Z" id="43450517">I opened #6221 to fix this...
</comment><comment author="dadoonet" created="2014-05-19T01:05:04Z" id="43459245">Cool. Closing this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Some minor cleanups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6210</link><project id="" key="" /><description>I have some commits that makes findbugs little happier...
</description><key id="33677186">6210</key><summary>Some minor cleanups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T14:03:45Z</created><updated>2015-06-07T13:28:12Z</updated><resolved>2014-05-16T18:56:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow primaries that have never been allocated to be allocated if under the low watermark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6209</link><project id="" key="" /><description>(In the DiskThresholdDecider)

Fixes #6196
</description><key id="33676560">6209</key><summary>Allow primaries that have never been allocated to be allocated if under the low watermark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T13:56:18Z</created><updated>2015-06-07T13:28:22Z</updated><resolved>2014-07-07T10:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T10:07:37Z" id="45850917">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove SoftReferences from StreamInput/StreamOutput</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6208</link><project id="" key="" /><description>We try to reuse character arrays and UTF8 writers with softreferences.
SoftReferences have negative impact on GC and should be avoided in
general. Yet in this case it can simply replaced with a per-stream
Bytes/CharsRef that is thread local and has the same lifetime as the
stream.
</description><key id="33671356">6208</key><summary>Remove SoftReferences from StreamInput/StreamOutput</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Internal</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T12:42:19Z</created><updated>2015-06-07T13:28:28Z</updated><resolved>2014-05-16T19:08:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-16T13:26:13Z" id="43330416">I've actually been looking into GC issues lately on my cluser.  Soft references account for a pretty small amount of the young gen pause time.  Weak references account for about 1/8th of though.  Still, no reason not to do this.
</comment><comment author="rmuir" created="2014-05-16T13:45:21Z" id="43332486">Why does StreamOutput write with standard UTF-8 and StreamInput read with modified UTF-8? These are incompatible for some unicode characters...
</comment><comment author="s1monw" created="2014-05-16T13:47:10Z" id="43332684">@rmuir wait that is completely unrelated to this issue no? Can you open a followup?
</comment><comment author="s1monw" created="2014-05-16T14:02:55Z" id="43334449">&gt; Why does StreamOutput write with standard UTF-8 and StreamInput read with modified UTF-8? These are incompatible for some unicode characters...

@rmuir that is not entirely true the write part that I fixed is `writeText` and read part is `readString` the corresponding `writeString` also writes `modified UTF-8`. Not saying we should move to standard unicode
</comment><comment author="rmuir" created="2014-05-16T14:06:35Z" id="43334868">your right, as long as writeText is always paired with readText and vice versa, it will be ok. but this is dangerous, because bugs could sit latent (and simple tests would not find them). so some care should be taken to reduce this risk...
</comment><comment author="kimchy" created="2014-05-16T18:59:28Z" id="43367401">+1, it cleans the code. I am less concerned about the SoftReference aspect, since its a static thread local.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryID should not be a per JVM but per Node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6207</link><project id="" key="" /><description>Today the RecovyerID is taken from a static atomic long which
is essentially a per JVM ID. We run the tests within the same
JVM and that means we don't really simulate what happens in
production environments. Instead we should use a per node generated
ID.
</description><key id="33669319">6207</key><summary>RecoveryID should not be a per JVM but per Node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Recovery</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T12:07:51Z</created><updated>2015-06-07T13:28:33Z</updated><resolved>2014-05-16T13:00:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-16T12:29:17Z" id="43325587">github is flaky today it seems....
</comment><comment author="javanna" created="2014-05-16T12:42:04Z" id="43326540">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added install instruction with apt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6206</link><project id="" key="" /><description /><key id="33661104">6206</key><summary>added install instruction with apt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">mef</reporter><labels /><created>2014-05-16T09:48:58Z</created><updated>2014-06-23T06:30:57Z</updated><resolved>2014-05-16T17:07:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-16T12:25:23Z" id="43325276">Hi @mef 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="mef" created="2014-05-16T15:28:21Z" id="43344763">Hi @clintongormley,

I have some issues with your CLA. Why was it created on the first place, and has there ever been a need for such process? Or was it created just by mimetism of _some_ other open source projects without an actual reflexion?

&gt; CLAs are generally common and accepted in most Open Source software projects

No, they are not. There is an ongoing controversy about it, cf. [Contributor Agreements Considered Harmful](http://www.oscon.com/oscon2011/public/schedule/detail/19242)

&gt; to ensure we have the agreement of our contributors to use their work

You have the agreement, it's called a Github pull request between your open source project and one of its github forks. You should probably talk to legal people specialized in the topic.

&gt; The CLA help us achieve that goal

Doesn't the CLA reduce the number of contributions, by adding un-necessary administrative tasks?
Please take into account that in addition of signing the CLA, you're asking us to:
1. Provide our personal information to Adobe Echosign for the electronic signature. Adobe, personal data, leaks...
2. Agree to a customer disclosure agreement
3. Agree to Adove Echosign's terms and services.

&gt; The next step is for you to select the appropriate agreement below, (...)

How about we do the following instead:
- option 1: "The next step is for you (Elasticsearch) to get informed and debate internally about the necessity of having CLAs. You realize it's not necessary and get more lean, great !"'. I'm sure you can push people and have this done in 2 weeks.
- option 2: "The next step is for you (Elasticsearch) to accept and merge my pull request, by considering that this message is an explicit approval of your CLAs terms (cf. above: "You have the agreement").
- option 3: "The next step is for you (Elasticsearch) to discard my pull request, re-do the changes I have made on your own repository, commit, and end of story"

That's about it. I hope one of the solutions offered above will let us move on. I hope option 1 will be chosen. Sorry for the rant. I don't mean to troll, but to be constructive. 
</comment><comment author="clintongormley" created="2014-05-16T17:07:38Z" id="43355627">Sorry that you feel that way. We as  a company have debated this before and as an employee I need to stick to our established policy. I'll be treating this as a bug report rather than as a PR.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixed typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6205</link><project id="" key="" /><description /><key id="33660323">6205</key><summary>fixed typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">MungoH</reporter><labels><label>docs</label></labels><created>2014-05-16T09:36:32Z</created><updated>2014-06-12T08:11:51Z</updated><resolved>2014-06-05T17:50:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-16T12:26:20Z" id="43325350">Hi @MungoH 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="javanna" created="2014-06-05T17:50:26Z" id="45251774">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Enable DiskThresholdDecider by default, change default limits to 85/90%</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6204</link><project id="" key="" /><description>Fixes #6200
Fixes #6201
</description><key id="33654533">6204</key><summary>Enable DiskThresholdDecider by default, change default limits to 85/90%</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels><label>:Allocation</label><label>breaking</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T07:58:47Z</created><updated>2015-06-06T16:57:12Z</updated><resolved>2014-06-12T14:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-06-12T10:03:59Z" id="45850628">left one comment other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>put-mapping typo fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6203</link><project id="" key="" /><description>split backwardscompatibility to backwards compatibility
</description><key id="33651159">6203</key><summary>put-mapping typo fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">skv-headless</reporter><labels><label>docs</label></labels><created>2014-05-16T06:41:23Z</created><updated>2014-06-15T16:27:59Z</updated><resolved>2014-06-05T17:55:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-16T12:26:48Z" id="43325391">Hi @skv-headless 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="javanna" created="2014-06-05T17:55:21Z" id="45252379">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>closed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6202</link><project id="" key="" /><description /><key id="33648737">6202</key><summary>closed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fizx</reporter><labels /><created>2014-05-16T05:28:48Z</created><updated>2014-07-16T21:45:05Z</updated><resolved>2014-05-16T05:40:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allocation: Change high/low watermark defaults for disk based allocation decider</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6201</link><project id="" key="" /><description>The current values for

`cluster.routing.allocation.disk.watermark.low
cluster.routing.allocation.disk.watermark.high`

are too low -- 70% and 85%.  On a 4 TB disk this means we can leave 1.2 TB unallocated.  

We should decide on and implement new, higher defaults.   I'll propose low=85% and high=90%.

We should also update the docs as part of this work as they reference the defaults.
</description><key id="33637987">6201</key><summary>Allocation: Change high/low watermark defaults for disk based allocation decider</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevinkluge</reporter><labels><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T00:33:23Z</created><updated>2014-07-16T12:14:07Z</updated><resolved>2014-06-12T14:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allocation: Enable disk-space allocation decider by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6200</link><project id="" key="" /><description>We should make the allocation decider that considers disk space enabled by default.  This will help protect against filling up a server's disk.
</description><key id="33637756">6200</key><summary>Allocation: Enable disk-space allocation decider by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kevinkluge</reporter><labels><label>breaking</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-16T00:28:19Z</created><updated>2014-07-16T12:13:53Z</updated><resolved>2014-06-12T14:58:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Prevent the init script from starting several instances of elastic searc...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6199</link><project id="" key="" /><description>...h.
</description><key id="33636690">6199</key><summary>Prevent the init script from starting several instances of elastic searc...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">haloflightleader</reporter><labels><label>:Packaging</label><label>feedback_needed</label></labels><created>2014-05-16T00:07:44Z</created><updated>2014-11-11T18:58:20Z</updated><resolved>2014-11-11T18:58:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-19T06:30:48Z" id="43469694">can you explain the reasoning behind this PR? Under what circumstances didnt elasticsearch shutdown and why would your PR solve this by sending the same signal again for three times?

Thanks!
</comment><comment author="haloflightleader" created="2014-06-06T17:35:29Z" id="45363554">We can divide the startup of elastic search into two categories:
1. Where we can communicate with it through system calls.
2. Where we can't.

If you restart elastic search more than once fast enough, you start sending the killproc to an elastic search that's incapable of reacting to it. The inadvertent result is there becomes more than one running instance of elastic search.

This change prevents that. It insists that the shutdown takes place gracefully. It asks three times, and at the fourth time it kills it hard.

I hope that makes sense.
</comment><comment author="spinscale" created="2014-06-13T12:35:18Z" id="46005787">Hey,

doesnt the built-in killproc function do exactly this already? At least on centos, maybe it is different on other linux distros. Which one are you using?
</comment><comment author="electrical" created="2014-06-13T12:43:51Z" id="46006568">This will also cause issues when one is actually wanting to run multiple instances of elasticsearch on a single node.
</comment><comment author="kantselovich" created="2014-06-13T22:38:42Z" id="46067491">(I'm on the same team as @haloflightleader  )

@spinscale  Centos 6.x  
@electrical   True.  There should be a better way to do this. We will see if we can change the pull req. 

With the current init script, we reliably observe 2 processes of elastic search on restart attempt.  
</comment><comment author="clintongormley" created="2014-10-20T13:14:28Z" id="59751359">Hi @haloflightleader and @kantselovich 

Any news on this PR? Or should I close?
</comment><comment author="clintongormley" created="2014-11-11T18:58:20Z" id="62597582">No more feedback. Closing this for now, feel free to reopen with a new PR.

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adds toString() functionality to SuggestRequestBuilder + SuggestBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6198</link><project id="" key="" /><description>Just like SearchRequestBuilder, this adds toString() functionality to SuggestRequestBuilder + SuggestBuilder
</description><key id="33632261">6198</key><summary>Adds toString() functionality to SuggestRequestBuilder + SuggestBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/areek/following{/other_user}', u'events_url': u'https://api.github.com/users/areek/events{/privacy}', u'organizations_url': u'https://api.github.com/users/areek/orgs', u'url': u'https://api.github.com/users/areek', u'gists_url': u'https://api.github.com/users/areek/gists{/gist_id}', u'html_url': u'https://github.com/areek', u'subscriptions_url': u'https://api.github.com/users/areek/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/753679?v=4', u'repos_url': u'https://api.github.com/users/areek/repos', u'received_events_url': u'https://api.github.com/users/areek/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/areek/starred{/owner}{/repo}', u'site_admin': False, u'login': u'areek', u'type': u'User', u'id': 753679, u'followers_url': u'https://api.github.com/users/areek/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>:Suggesters</label><label>enhancement</label><label>feedback_needed</label><label>review</label></labels><created>2014-05-15T22:48:38Z</created><updated>2016-03-08T14:31:00Z</updated><resolved>2016-03-08T14:31:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2014-08-08T09:36:46Z" id="51581599">LGTM, @bly2k can you rebase and get this in?
</comment><comment author="EdgeCaseBerg" created="2016-02-12T15:47:46Z" id="183381374">Would love to see this merged in. (And if possible back-ported to older versions of ES as well!)
</comment><comment author="nik9000" created="2016-02-12T15:55:25Z" id="183384451">I think the whole "helper method for an xcontent toString" is a good idea. I see us repeat it quite a bit. Anyone involved in the query refactoring like the idea? @abeyad or @colings86?

Personally I'd just do the wrapInObject=true case.

Is it too crazy to implement it with a default method on an interface that extends ToXContent? Heck, it could even be **_on**_ ToXContent if we limit the length of the result.
</comment><comment author="colings86" created="2016-02-12T16:00:31Z" id="183385964">As part of the refactoring of SuggestBuilder we should be making SuggestBuilder extend ToXContentToBytes which will solve half of this and I don't see why we can't add toString() to SuggestRequestBuilder too. I think it will end up being done as part of the suggester refactoring PRs rather than merging this PR though. @cbuescher @areek what do you think?
</comment><comment author="abeyad" created="2016-02-12T16:05:10Z" id="183387363">We already have `SuggestBuilder` extending `ToXContentToBytes` as part of the suggest refactoring PRs.  This will end up in master when the feature branch is more complete.  Implementing a `toString()` method on these will be very easy afterward (just a matter of calling `toXContent` on the `SuggestBuilder`).  If this PR can wait, then the feature/suggest-refactoring branch will have these features "built-in".
</comment><comment author="nik9000" created="2016-02-12T16:06:44Z" id="183388202">I believe its safe to have it wait for the suggest-refactoring branch. Its waited this long....
</comment><comment author="colings86" created="2016-02-12T16:18:31Z" id="183393222">++

Just a note that ToXContentToBytes implements toString() for you and calls the abstract ToXContent method so there will be no need to add implementing toString() :)
</comment><comment author="clintongormley" created="2016-03-08T14:31:00Z" id="193804381">Closed in favour of https://github.com/elastic/elasticsearch/tree/feature-suggest-refactoring
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Entirely cut over to TopDocs#merge for merging shard docs in the reduce phase </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6197</link><project id="" key="" /><description>From Lucene 4.8 the TopDocs#merge supports slicing of the shard level top docs, by taking the offset into account (for pagination). Because of this the custom shard level topdocs merging can be removed in favour for TopDocs#merge.
</description><key id="33627410">6197</key><summary>Entirely cut over to TopDocs#merge for merging shard docs in the reduce phase </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T21:33:41Z</created><updated>2015-06-07T13:28:45Z</updated><resolved>2014-05-22T08:42:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T21:34:58Z" id="43268141">I love the stats :)
</comment><comment author="jpountz" created="2014-05-15T21:36:08Z" id="43268251">LGTM
</comment><comment author="s1monw" created="2014-05-15T21:40:26Z" id="43268675">left one comment! The code removal is awesome!
</comment><comment author="martijnvg" created="2014-05-15T22:17:03Z" id="43271911">@s1monw Updated PR, I made use of a static TopDocs instance.
</comment><comment author="martijnvg" created="2014-05-21T10:32:20Z" id="43738698">@s1monw Updated the PR, with a change to the QueryRescorerTests#testEquivalence test, to not rely on ordering and just verify if scores are equal, which gets around inconsistencies if regular hits and rescored hits have multiple documents with the same score.
</comment><comment author="s1monw" created="2014-05-21T12:04:43Z" id="43745584">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allocation: Disk-aware allocation decider should allow initial primary allocation if under the high watermark</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6196</link><project id="" key="" /><description>For _initial_ primary allocation, it should still be allowed if above the low watermark, and denied if above the high watermark.
</description><key id="33625533">6196</key><summary>Allocation: Disk-aware allocation decider should allow initial primary allocation if under the high watermark</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T21:10:11Z</created><updated>2014-07-16T12:15:11Z</updated><resolved>2014-07-07T10:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Randomize CMS settings in index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6195</link><project id="" key="" /><description>This commit adds randomization for:
- `index.merge.scheduler.max_thread_count`
- `index.merge.scheduler.max_merge_count`

This commit also moves to use
EsExecutors#boundedNumberOfProcessors(Settings) to default
configure the default `max_thread_count` for better reproducibility

Closes #6194
</description><key id="33623571">6195</key><summary>Randomize CMS settings in index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-05-15T20:50:25Z</created><updated>2014-06-28T03:34:32Z</updated><resolved>2014-05-15T21:18:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T20:57:33Z" id="43263792">@jpountz pushed a new commit - good catch
</comment><comment author="jpountz" created="2014-05-15T21:04:14Z" id="43264599">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize MergeScheduler Settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6194</link><project id="" key="" /><description>ConcurrentMS has settings we should randomize in the index template just like LuceneTestCase does.
</description><key id="33622011">6194</key><summary>Randomize MergeScheduler Settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T20:31:36Z</created><updated>2014-05-15T21:18:15Z</updated><resolved>2014-05-15T21:18:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Shade mustache into org.elasticsearch.common package</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6193</link><project id="" key="" /><description>Previously we shared the jar but never rewrote the packages such
that the shading had no effect.

Closes #6192
</description><key id="33613413">6193</key><summary>Shade mustache into org.elasticsearch.common package</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>:Packaging</label><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T18:46:12Z</created><updated>2015-06-07T20:05:35Z</updated><resolved>2014-05-15T19:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-05-15T18:54:19Z" id="43250352">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mustache dependency not shaded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6192</link><project id="" key="" /><description>The `com.github.spullara.mustache.java:compiler` dependency is not being shaded in the Elasticsearch fat jar:

```
$ curl -s http://repo1.maven.org/maven2/org/elasticsearch/elasticsearch/1.1.1/elasticsearch-1.1.1.jar | jar tv | grep -i mustache
     0 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/
  5705 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/MustacheScriptEngineService.class
  1598 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.class
  3246 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/MustacheScriptEngineService$MustacheExecutableScript.class
     0 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/
   175 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/Binding.class
   639 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/Code.class
     0 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/
  6514 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/DefaultCode.class
  1951 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/DefaultMustache.class
   758 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/DepthLimitedWriter.class
  4121 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ExtendCode.class
  1025 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ExtendNameCode.class
  1737 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/IterableCode$1.class
  5438 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/IterableCode.class
  2183 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/NotIterableCode.class
  3939 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/PartialCode.class
  1772 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ValueCode$1.class
  5351 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ValueCode.class
[...]
```

Strange since it is being included in the list of shaded dependencies: https://github.com/elasticsearch/elasticsearch/blob/master/pom.xml#L585

Maybe it's because it is declared as an optional dependency? https://github.com/elasticsearch/elasticsearch/blob/master/pom.xml#L176
</description><key id="33611396">6192</key><summary>Mustache dependency not shaded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">plaflamme</reporter><labels><label>bug</label><label>v1.1.2</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T18:23:17Z</created><updated>2014-05-15T19:25:45Z</updated><resolved>2014-05-15T19:25:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T18:45:59Z" id="43249342">good catch I will provide a PR in a bit...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Before deleting a local unused shard copy, verify we're connected to the node it's supposed to be on</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6191</link><project id="" key="" /><description>This is yet another safety guard to make sure we don't delete data if the local copy is the only one (even if it's not part of the cluster state any more)
</description><key id="33604123">6191</key><summary>Before deleting a local unused shard copy, verify we're connected to the node it's supposed to be on</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>:Recovery</label><label>enhancement</label><label>resiliency</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T16:53:05Z</created><updated>2015-06-07T13:29:02Z</updated><resolved>2014-05-20T09:17:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-18T06:58:58Z" id="43433012">LGTM though - do we have the chance to get a test that reflects the problem or the reason we add this?
</comment><comment author="bleskes" created="2014-05-18T19:58:58Z" id="43449806">@s1monw this is caused by a rare race condition in cluster state processing which the reason for infrequent failures on our `feature/improve_zen` branch. Writing a test that recreates it is failure hard. We try to keep that branch not to far from master which why I made the PR. 

PS. I bumped the labels from 1.2.0 to 1.3.0 so there is no rush.
</comment><comment author="s1monw" created="2014-05-18T20:27:34Z" id="43450575">cool lets get this into `1.3` I agree a test is hard here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NullPointerException in cat-recovery API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6190</link><project id="" key="" /><description>The recovery type isn't serialized. This can cause a NPE like this:

```
Caused by: java.lang.NullPointerException
        at org.elasticsearch.indices.recovery.RecoveryState.writeTo(RecoveryState.java:244)
        at org.elasticsearch.action.admin.indices.recovery.ShardRecoveryResponse.writeTo(ShardRecoveryResponse.java:87)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)
        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:62)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:413)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:399)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
```
</description><key id="33601094">6190</key><summary>Fix NullPointerException in cat-recovery API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>:CAT API</label><label>bug</label><label>v1.2.2</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T16:17:23Z</created><updated>2015-06-07T20:05:54Z</updated><resolved>2014-06-20T20:21:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T19:02:58Z" id="43251366">the change looks good but how come this never triggered a failure? can you add a test maybe?
</comment><comment author="s1monw" created="2014-06-12T09:15:45Z" id="45846577">@martijnvg can we make this fail somehow with a simple test?
</comment><comment author="martijnvg" created="2014-06-12T11:43:50Z" id="45881493">@s1monw Updated the PR, when backporting I will add the 1.2.2 version to the 1.2 branch.
</comment><comment author="s1monw" created="2014-06-18T18:53:50Z" id="46477827">left a comment regarding the version check, other than that LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When sending shard start/failed message due to a cluster state change, use the master indicated in the new state rather than current</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6189</link><project id="" key="" /><description>This commit also adds extra protection in other cases against a master node being de-elected and thus being null.
</description><key id="33592761">6189</key><summary>When sending shard start/failed message due to a cluster state change, use the master indicated in the new state rather than current</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>:Cluster</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T14:50:28Z</created><updated>2015-06-08T14:36:09Z</updated><resolved>2014-05-15T16:46:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T15:21:14Z" id="43223435">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Uppercase token filter missing and causing mapping failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6188</link><project id="" key="" /><description>I am getting consistent IndexCreationExceptions while using `uppercase` token filter in a custom analyzer, both in 1.0 and 1.1.1:

```
{"error":"IndexCreationException[[play] failed to create index]; nested: IllegalArgumentException[Custom Analyzer [my_analyzer] failed to find filter under name [uppercase]]; ","status":400}
```

curl session:
https://gist.github.com/tmacam/08467d4649fab00d239d

original play session (just replace `lowercase` by `uppercase` in mapping settings and replace the query by `VALID`):
https://found.no/play/gist/da9f64d23ef54a9f5466
</description><key id="33592298">6188</key><summary>Uppercase token filter missing and causing mapping failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tmacam</reporter><labels /><created>2014-05-15T14:46:27Z</created><updated>2014-05-19T16:50:04Z</updated><resolved>2014-05-16T11:40:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-16T11:40:19Z" id="43322219">Hi @tmacam 

The uppercase token filter was only added 7 weeks ago in 49c19005055314d1488a11f1d7a3a97ed19cb5e8 so I'm guessing that Play is using an older version of Elasticsearch
</comment><comment author="mattmo" created="2014-05-18T00:35:03Z" id="43428098">@clintongormley 

We just hit this as well and we're running elasticsearch 1.1.1. It looks like that change didn't make it into the 1.1.1 branch. Are the branch names/tag names in github accurate for releases?
</comment><comment author="mattmo" created="2014-05-18T01:16:21Z" id="43428693">Just verified that the script provided by @tmacam (https://gist.github.com/tmacam/08467d4649fab00d239d) does work on ES build from master. Would be nice if the docs made this clear. 
</comment><comment author="s1monw" created="2014-05-18T10:01:43Z" id="43435885">@mattmo I fixed the [docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-uppercase-tokenfilter.html) - thanks for pinging again! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't use AllTokenStream if no fields were boosted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6187</link><project id="" key="" /><description>I noticed we create a new AllTokenStream per indexed doc to index the _all field, and remap any per-field boosts to payloads, but if the AllEntries saw no boosts (it already has a boolean customBoost() method to check this) then I think we can skip wrapping with AllTokenStream?

The cost of AllTokenStream.incrementToken is non-trivial because on each token it does a binary search to look up the boost for that entry.  Separately, I think this binary search may not be necessary (can't it just use the "current" entry's boost?).

But stepping back, can't ES just add multiple instances of the _all field, rather than making a custom Reader impl (AllEntries) and TokenFilter (AllTokenStream) that does the concatenating on the fly?  When Lucene inverts the multi-valued field it logically appends them together.
</description><key id="33589362">6187</key><summary>Don't use AllTokenStream if no fields were boosted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikemccand</reporter><labels><label>:Core</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T14:18:21Z</created><updated>2015-06-07T13:29:16Z</updated><resolved>2014-05-20T14:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mikemccand" created="2014-05-18T09:26:11Z" id="43435237">Pull request here: https://github.com/elasticsearch/elasticsearch/pull/6219

I also noticed &amp; fixed a possible bug in AllFieldMapper.queryTermToString that would fail to return AllTermQuery if the field was index with offsets (for postings highlighter)...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scalable cluster state propagation (zen discovery)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6186</link><project id="" key="" /><description>In the current implementation of zen discovery the cluster state broadcasting by the master does not scale too well as every change in the cluster state results in the cluster state being sent to every single node by the master.

This can lead to quite some traffic, in an cluster with 500 nodes and 15000 shards, even when the cluster state by itself has only 1MB, the master node will sent a total of 500MB per cluster state update.

So it should be possible to only send a diff of the changes to the nodes instead of the full cluster state. In case the nodes recognize that they missed an update, they could query the master node to resend the full cluster state to them.

Another possibility would be a tiered distribution of the state, where the master node does not directly send the updates to every single node, but through other nodes which forward the state to the other nodes in the cluster. A gossipping protocol could here be used.

If the clusterstate does not need to be distributed in realtime, one could also throttle the propagation of the state, so that when changes come in rapid succession, the changes can be merged before the state has been sent out to every node.

Any ideas?
</description><key id="33583415">6186</key><summary>Scalable cluster state propagation (zen discovery)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">miccon</reporter><labels /><created>2014-05-15T13:10:05Z</created><updated>2014-12-30T18:54:52Z</updated><resolved>2014-12-30T18:54:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T18:54:52Z" id="68385116">Duplicate of #6295 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Benchmark API should accept match patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6185</link><project id="" key="" /><description>If I run abort with `*` as the index it barfs with `"Benchmark with name [*] not found"` we should support the wildcards to abort all benchmarks and simple patterns.
</description><key id="33568447">6185</key><summary>Benchmark: Benchmark API should accept match patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>:Benchmark</label><label>enhancement</label></labels><created>2014-05-15T09:25:22Z</created><updated>2015-03-19T15:24:40Z</updated><resolved>2014-05-20T14:18:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Stats combination</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6184</link><project id="" key="" /><description>I'd like to calculate the relation between the stats of some fields. For example, the average revenue per rental day would be the total of NET divided by the total of DURATION. Is this an answer ES can give me directly, or must send two queries and do my own manipulation of the responses ?
</description><key id="33565230">6184</key><summary>Stats combination</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">utilum</reporter><labels /><created>2014-05-15T08:42:55Z</created><updated>2014-05-15T08:53:46Z</updated><resolved>2014-05-15T08:53:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-15T08:53:46Z" id="43183923">@utilum This issues list is for bugs and feature requests. Please ask questions about how to use Elasticsearch in the forum instead.

https://groups.google.com/forum/?fromgroups#!forum/elasticsearch

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add execution control for active benchmarks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6183</link><project id="" key="" /><description>This commit adds the ability to pause and resume active benchmarks. It
adds new API calls for sending control requests.

TODO:
1. Pausing internally is implemented using a busy wait. Should this be
changed to use proper blocking concurrency primitives?
2. The returned status code can be incorrectly reported in some cases.
This is due to BenchmarkResponse.mergeState() assuming a natural
ordering of enum values.
3. Needs refactoring of test cases to use pause/resume feature.

Closes #6173
</description><key id="33555956">6183</key><summary>Add execution control for active benchmarks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-15T05:20:41Z</created><updated>2014-10-21T23:38:29Z</updated><resolved>2014-07-10T17:05:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-16T09:52:20Z" id="43315070">it's hard to review at this point since the PR seems to be messed up did you merge it up with master without rebasing?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Switch to shared thread pool for all snapshot repositories</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6182</link><project id="" key="" /><description> Closes #6181
</description><key id="33545134">6182</key><summary>Switch to shared thread pool for all snapshot repositories</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>:Snapshot/Restore</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T00:08:02Z</created><updated>2015-06-08T14:36:47Z</updated><resolved>2014-05-16T23:43:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-15T09:44:09Z" id="43189281">LGTM
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unregistering snapshot repositories causes thread leaks</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6181</link><project id="" key="" /><description /><key id="33545017">6181</key><summary>Unregistering snapshot repositories causes thread leaks</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-15T00:05:40Z</created><updated>2014-07-03T18:59:46Z</updated><resolved>2014-05-16T23:43:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-18T10:04:57Z" id="43435947">should we get this into `1.1.2` as well?
</comment><comment author="imotov" created="2014-05-19T14:22:16Z" id="43510073">It would be difficult because of all deprecated gateways that are still there.
</comment><comment author="s1monw" created="2014-05-19T14:22:49Z" id="43510152">oh yeah I see makes sense - in that case no `1.1.2`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect RegExp flags handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6180</link><project id="" key="" /><description>We noticed in our deployment that complement and interval regexp operations are working despite flags not being explicitly specified.

Despite the documentation here: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-regexp-query.html#_optional_operators

It appears that ES is actually defaulting to "ALL" in the event that no flags are explicitly passed:
https://github.com/elasticsearch/elasticsearch/blob/9ed34b5a9e9769b1264bf04d9b9a674794515bc6/src/main/java/org/elasticsearch/index/query/RegexpFilterBuilder.java#L65

Not clear whether the documentation or the codebase is incorrect.  I also don't know enough about lucene internals to know whether it's safe to disable the AUTOMATA flag - though I imagine the rest should be fine.
</description><key id="33533060">6180</key><summary>Incorrect RegExp flags handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">EricMCornelius</reporter><labels /><created>2014-05-14T21:05:57Z</created><updated>2014-12-30T18:53:58Z</updated><resolved>2014-12-30T18:53:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geo:Valid Polygon crossing dateline fails to parse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6179</link><project id="" key="" /><description>Attempt to upload polygon at https://gist.github.com/anonymous/7f1bb6d7e9cd72f5977c
fails with 
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [geometry]
...
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1

Repro

curl -XDELETE 'http://localhost:9200/test'

curl -XPOST 'http://localhost:9200/test' -d '{  
  "mappings":{  
    "test":{  
      "properties":{  
        "geometry":{  
          "type":"geo_shape",  
          "tree":"quadtree",  
          "tree_levels":14,  
          "distance_error_pct":0.0  
        }  
      }  
    }  
  }  
}'

curl -XPOST 'http://localhost:9200/test/test/1' -d '{  
  "geometry": {  
    "type": "Polygon",  
    "coordinates": [[[-186,0],[-176,0],[-176,3],[-183,3],[-183,5],[-176,5],[-176,8],[-186,8],[-186,0]],[[-185,1],[-181,1],[-181,2],[-184,2],[-184,6],[-178,6],[-178,7],[-185,7],[-185,1]],[[-179,1],[-177,1],[-177,2],[-179,2],[-179,1]],[[-180,0],[-180,-90],[-180,90],[-180,0]]]  
  }  
}'

Additionally, there is a unit test at:
https://github.com/marcuswr/elasticsearch-dateline/commit/cbf9db12615c55ba8a8801aa3eaa3704ac2943c6
</description><key id="33529509">6179</key><summary>Geo:Valid Polygon crossing dateline fails to parse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">marcuswr</reporter><labels><label>bug</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-14T20:24:43Z</created><updated>2014-06-02T14:05:04Z</updated><resolved>2014-06-02T14:05:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Highlight fields in request order</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6178</link><project id="" key="" /><description>Because json objects are unordered this also adds an explicit order syntax
that looks like

``` js
    "highlight": {
        "fields": [
            {"title":{ /*params*/ }},
            {"text":{ /*params*/ }}
        ]
    }
```

This is not useful for any of the builtin highlighters but will be useful
in plugins.

Closes #4649
</description><key id="33524375">6178</key><summary>Highlight fields in request order</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Highlighting</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-14T19:28:20Z</created><updated>2015-06-07T13:29:29Z</updated><resolved>2014-05-22T14:53:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2014-05-14T19:32:32Z" id="43126574">This doesn't actually implement conditional highlighting but it gives plugins the one tool that they need to implement it themselves: guaranteed execution order.  Without it the fields are highlighted in hashmap iteration order which means the plugin can't push data from one field to the next reliably. 
</comment><comment author="jpountz" created="2014-05-22T09:51:22Z" id="43868214">@nik9000 I left a minor comment about the name of the parameter that allows to make the field order explicit but other than that this looks good to me!
</comment><comment author="nik9000" created="2014-05-22T12:03:20Z" id="43878815">Done!
</comment><comment author="nik9000" created="2014-05-22T12:12:04Z" id="43879667">Sorry, not quite done.  On more moment.
</comment><comment author="nik9000" created="2014-05-22T12:23:31Z" id="43880784">Done.
</comment><comment author="jpountz" created="2014-05-22T14:53:36Z" id="43898887">Merged. Thanks, Nik!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[TEST] Randomize number of available processors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6177</link><project id="" key="" /><description>We configure the threadpools according to the number of processors which is
different on every machine. Yet, we had some test failures related to this
and #6174 that only happened reproducibly on a node with 1 available processor.
This commit does:
- sometimes randomize the number of available processors
- if we don't randomize we should set the actual number of available processors
  in the settings on the test node
- always print out the num of processors when a test fails to make sure we can
  reproduce the thread pool settings with the reproduce info line

Closes #6176
</description><key id="33521963">6177</key><summary>[TEST] Randomize number of available processors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2014-05-14T18:59:21Z</created><updated>2014-06-12T21:10:33Z</updated><resolved>2014-05-15T10:26:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-14T19:00:45Z" id="43122855">without the fix we that closed  #6174 this commandline produces a failure consistently:

```
mvn test -Dtests.class=*.BenchmarkIntegrationTest -Dtests.processors=1
```

and now we also get the number of processors in the output

```
mvn test -Dtests.seed=930CF86FB03004CC -Dtests.class=org.elasticsearch.action.bench.BenchmarkIntegrationTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="testListBenchmarks" -Des.logger.level=INFO -Dtests.heap.size=512m -Dtests.processors=1
```
</comment><comment author="javanna" created="2014-05-15T10:03:44Z" id="43191344">looks good
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Randomize the nodes number of processors </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6176</link><project id="" key="" /><description>We configure the threadpools according to the number of processors which is different on every machine. Yet, we just had some test failures related to #6174 that only happened reproducibly on a node with 1 CPU (available processors returned 1). We should:
- sometimes randomize the number of available processors
- if we don't randomize we should set the actual number of available processors in the settings on the test node
- always print out the num of processors when a test fails to make sure we can reproduce the thread pool settings with the commandline
</description><key id="33520426">6176</key><summary>Randomize the nodes number of processors </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>test</label></labels><created>2014-05-14T18:42:11Z</created><updated>2014-05-15T10:26:41Z</updated><resolved>2014-05-15T10:26:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix bug for BENCH thread pool size == 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6175</link><project id="" key="" /><description>On small hardware, the BENCH thread pool can be set to size 1. This is
problematic as it means that while a benchmark is active, there are no
threads available to service administrative tasks such as listing and
aborting. This change fixes that by executing list and abort operations
on the GENERIC thread pool.

Closes #6174
</description><key id="33515589">6175</key><summary>Fix bug for BENCH thread pool size == 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-14T17:44:25Z</created><updated>2014-10-21T23:38:44Z</updated><resolved>2014-05-14T18:05:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-14T17:48:54Z" id="43113874">LGTM all tests pass for me with this patch and 1 CPU :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Benchmark: Use GENERIC thread pool for status/abort benchmark calls</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6174</link><project id="" key="" /><description>On a small machine, the BENCH thread pool can get sized to 1, leaving no threads available to perform other tasks such as listing and aborting.
</description><key id="33514349">6174</key><summary>Benchmark: Use GENERIC thread pool for status/abort benchmark calls</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>:Benchmark</label><label>bug</label></labels><created>2014-05-14T17:29:42Z</created><updated>2015-03-19T15:43:07Z</updated><resolved>2014-05-14T18:05:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-14T17:41:52Z" id="43112973">+1 to use `GENERIC` good that we found this before this gets released
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add pause feature to benchmark API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6173</link><project id="" key="" /><description>During testing it came up that having the ability to pause an active benchmark would be very useful.
</description><key id="33511890">6173</key><summary>Add pause feature to benchmark API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-14T17:01:16Z</created><updated>2015-08-26T14:58:49Z</updated><resolved>2015-08-26T14:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T14:58:49Z" id="135050168">The benchmark API efforts have been discontinued.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>nullpointerexception at https://github.com/elasticsearch/elasticsearch/blob/v1.1.1/src/main/java/org/elasticsearch/percolator/PercolatorService.java#L301</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6172</link><project id="" key="" /><description>for this kind of query

```
GET /idx/type/5359b8d10cf2f2ff4ec71ed7/_percolate
{
  "filter": {
    "or": {
      "filters": [
        {}
      ]
    }
  }
}
```
</description><key id="33510874">6172</key><summary>nullpointerexception at https://github.com/elasticsearch/elasticsearch/blob/v1.1.1/src/main/java/org/elasticsearch/percolator/PercolatorService.java#L301</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">OlegYch</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2014-05-14T16:49:15Z</created><updated>2016-05-25T09:57:47Z</updated><resolved>2016-05-25T09:57:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="vaibhavkulkar" created="2014-12-09T07:11:53Z" id="66243297">This issue is still reproducible ?
</comment><comment author="clintongormley" created="2015-10-14T16:32:27Z" id="148107935">This is still broken in 2.0
</comment><comment author="qwerty4030" created="2016-05-21T19:44:14Z" id="220796709">@clintongormley I took a look and seems like this is a fairly easy fix. I'm assuming the expected behavior is to ignore the `filter` in this case (all percolators executed). I wanted to add an integration test and noticed the [PercolateRequestBuilder](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/action/percolate/PercolateRequestBuilder.java) and [PercolateSourceBuilder](https://github.com/elastic/elasticsearch/blob/2.x/core/src/main/java/org/elasticsearch/action/percolate/PercolateSourceBuilder.java) had the `filter` option removed in this commit: [Query DSL: Remove filter parsers.](https://github.com/elastic/elasticsearch/commit/a0af88e99630b9d3f0c2cf4997f2e82f1f834d41) However the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/2.3/search-percolate.html#_percolate_api) indicates both the `query` and `filter` options are still supported. My plan was to add the `filter` option back to those java client classes as a `QueryBuilder`. Let me know if I'm not on the right track here. Thanks.
</comment><comment author="martijnvg" created="2016-05-23T08:38:40Z" id="220919872">Hey @qwerty4030 the best way to fix this NPE is to replace line 321 in `PercolatorService.java` (2.x branch) with this:

``` java
ParsedQuery parsedQuery = documentIndexService.queryParserService().parseInnerFilter(parser);
if (parsedQuery != null) {
   context.percolateQuery(new ConstantScoreQuery(parsedQuery.query()));
}
```

Filters are deprecated and therefor adding filter methods to the Java api isn't a good idea.
This issue doesn't occur in the master branch (percolator has been rewritten), but it makes sense to fix it in 2.x and 2.3 branches. If you open a PR for this then I'm happy to merge it.
</comment><comment author="martijnvg" created="2016-05-25T09:57:47Z" id="221527696">Fixed via #18563
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create a new discovery plugin for DNS SRV records</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6171</link><project id="" key="" /><description>It looks like it'd be fairly easy to use DNS SRV records to find the list of hosts to use in unicast mode for discovery.   It'd be very easy to use this w/ a local d-dns server, or Route53, etc, and not have to worry about any credentials for a cloud provider or specialty plugins.
</description><key id="33509112">6171</key><summary>Create a new discovery plugin for DNS SRV records</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">matthewbarr</reporter><labels><label>:Discovery</label></labels><created>2014-05-14T16:31:31Z</created><updated>2015-05-14T22:02:35Z</updated><resolved>2015-05-12T22:23:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nilsga" created="2015-01-19T06:08:44Z" id="70450089">This would certainly be a very useful feature. Is this being considered?
</comment><comment author="dakrone" created="2015-04-10T17:40:14Z" id="91632625">@matthewbarr this sounds like a great idea, would you be willing to take a stab at a plugin?
</comment><comment author="grantr" created="2015-04-30T04:38:22Z" id="97660377">I've got a working prototype of this at https://github.com/grantr/elasticsearch-srv-discovery.
</comment><comment author="dakrone" created="2015-05-12T22:23:01Z" id="101440505">Okay, closing this since @grantr has a plugin for it -

@grantr would you like me to add this to the list of community plugins?
</comment><comment author="grantr" created="2015-05-13T00:06:38Z" id="101464498">@dakrone sure. It's just a prototype now but I hope to make it a real project in the medium term.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide a java client api (maven) package without external dependencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6170</link><project id="" key="" /><description>The standard elasticsearch package on maven central has a lot of lucene dependencies which are not generally needed for using the java client api (TransportClient/Client-Node).
Excluding all lucene dependencies is also not possible because TransportClient needs lucene-core for reading the Lucene version.
</description><key id="33501979">6170</key><summary>Provide a java client api (maven) package without external dependencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">benjamin84</reporter><labels><label>:Java API</label><label>adoptme</label><label>high hanging fruit</label></labels><created>2014-05-14T15:16:37Z</created><updated>2016-09-27T11:04:12Z</updated><resolved>2016-09-27T11:04:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="YuriKap" created="2015-01-09T19:29:46Z" id="69385551">We are trying to move from an older version of Lucene to ElasticSearch. We would like for our app to communicate with ElasticSearch via TransportClient but can't (without shading or doing classloader magic) since we currently have dependencies on an older version of the Lucene library. It would be really nice if we could have a TransportClient without the lucene-core dependency. 

Is there any chance that the dependency will be removed at some point?
</comment><comment author="dakrone" created="2015-04-10T17:38:02Z" id="91631886">We discussed this and think this is a good idea, maybe we can perform this using some kind of Maven shading to make sure we have a clean cut between the internal and client APIs.
</comment><comment author="YuriKap" created="2015-04-10T17:42:12Z" id="91632962">:+1: 
</comment><comment author="sureshg" created="2015-11-29T11:36:50Z" id="160405892">:+1: 
</comment><comment author="dadoonet" created="2015-11-29T12:23:46Z" id="160413274">Note that anyone can build his own shaded version. Read this: https://www.elastic.co/blog/to-shade-or-not-to-shade

Not sure if we still need to keep this issue opened.
</comment><comment author="sureshg" created="2015-12-01T00:01:35Z" id="160803442">@dadoonet IMHO this issue is for creating a thin Java client for Elasticsearch with minimal dependencies. Shaded version still has all the same dependencies. 
</comment><comment author="massfords" created="2015-12-01T00:23:15Z" id="160807010">I think the issue description sums it up nicely. The maven shade plugin is a viable solution but it's hardly elegant. At a minimum, I would hope ES would provide a client only dependency that allowed users to interact with the Client interface (Transport or otherwise) without bringing in all of the Lucene dependencies. The big win here is that ES does the packaging and it's one less thing we need to maintain locally in our own source repo. 

Whether this new module uses the Shade plugin to hide Lucene or perhaps removes Lucene is largely an implementation detail. We could debate the merits of a smaller dependency tree as a separate issue. As it stands now, if you have a component like Apache Jackrabbit then you're unable to use ES in your application in any capacity without the shade plugin. Again, I can roll my own (which I have) but I don't want to have to continue to do this.

I think this issue is worth addressing.
</comment><comment author="dadoonet" created="2015-12-01T02:55:07Z" id="160832543">May be it's the same issue as https://github.com/elastic/elasticsearch/issues/7743?
</comment><comment author="matt-blanchette" created="2015-12-19T06:24:15Z" id="165953212">+1 for Java client with few dependencies
</comment><comment author="dakrone" created="2016-09-27T11:04:12Z" id="249834070">I don't think we are planning on doing this with a transport client. Instead, we are working on an HTTP client for Java that will be part it this, see: https://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/index.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unified MetaData#concreteIndices methods into a single method that accepts indices (or aliases) and indices options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6169</link><project id="" key="" /><description>The goal of this PR is to have a single method in `MetaData` that allows to resolve aliases or indices (eventually containing wildcard expressions) to concrete indices. That way all the apis will just refer to this same new method passing the proper indices options as argument.

In order to achieve that a new internal flag has been added to IndicesOptions, which tells whether aliases can be resolved to multiple indices or not.

Also it's now possible to decide not to expand wildcards at all via indices options, while previously it was possible to decide whether to expand them to open indices, closed indices, or both.

Note that the PR is against 1.x to handle backwards compatibility properly when needed. Bw comp checks will be removed when porting to master.
</description><key id="33490246">6169</key><summary>Unified MetaData#concreteIndices methods into a single method that accepts indices (or aliases) and indices options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>:Index APIs</label><label>enhancement</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-14T13:11:00Z</created><updated>2015-06-07T13:29:36Z</updated><resolved>2014-05-15T19:07:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2014-05-14T13:26:22Z" id="43079425">Left a few comments, overall great direction. While you're at it, would be nice to have a builder for the `IndexOptions` (relieve all parsers from keeping all these bool flags around)
</comment><comment author="javanna" created="2014-05-14T16:07:16Z" id="43100926">I just pushed a few more commits that address the comments and finalize the refactoring.
</comment><comment author="s1monw" created="2014-05-15T11:56:31Z" id="43200538">LGTM
</comment><comment author="javanna" created="2014-05-15T19:07:55Z" id="43251986">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>disk.threshold_enabled allocation creates dynamics that can cause runaway disk space usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6168</link><project id="" key="" /><description>Enabling disk.threshold_enabled for shard allocation can cause shards to continuously get relocated around the cluster and potentially lead to completely running out of disk space if the shards (including replicas) is at least half of total disk space. 

The mechanism is as follows:
1. Run a cluster restart or have a substantial percentage of nodes restart, triggering many shards to reinitialize
2. Shards will get allocated onto other nodes and begin initializing. Some of those nodes may trigger the high disk threshold level for the disk allocation.
3. When the high disk threshold is triggered shards will start relocating. 
4. Relocation effectively creates a duplicate shard, increasing disk space across the cluster.
5. Because other nodes are also initializing/relocating nodes, new shard relocations may again trigger the high threshold
6. Repeat to step 3. (Slowly increasing the number of simultaneously relocating shards in the cluster.)

Here's a graph showing relocating shards continuously increasing:
![image](https://cloud.githubusercontent.com/assets/820871/2971078/dd98e172-db65-11e3-9ae5-29ac509d10f1.png)

cluster.routing.allocation.cluster_concurrent_rebalance was set to 6. This was observed in ES 1.1.1

A number of other config options affect the dynamics:
- Shard size and number of shards (more likely to happen with big shards that take longer to move)
- Throttling of recovery/relocation
- Shard Allocation Awareness limiting where nodes can be moved

Some possible solutions:
- Treat cluster_concurrent_rebalance as a hard maximum (see also #6141)
- Have the relocation/initialization algorithm calculate what disk space will be once all shards are fully moved rather than what disk space currently is.

Maybe related to #4790 
</description><key id="33489219">6168</key><summary>disk.threshold_enabled allocation creates dynamics that can cause runaway disk space usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dakrone/following{/other_user}', u'events_url': u'https://api.github.com/users/dakrone/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dakrone/orgs', u'url': u'https://api.github.com/users/dakrone', u'gists_url': u'https://api.github.com/users/dakrone/gists{/gist_id}', u'html_url': u'https://github.com/dakrone', u'subscriptions_url': u'https://api.github.com/users/dakrone/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/19060?v=4', u'repos_url': u'https://api.github.com/users/dakrone/repos', u'received_events_url': u'https://api.github.com/users/dakrone/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dakrone/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dakrone', u'type': u'User', u'id': 19060, u'followers_url': u'https://api.github.com/users/dakrone/followers'}</assignee><reporter username="">gibrown</reporter><labels><label>:Allocation</label></labels><created>2014-05-14T12:58:35Z</created><updated>2014-12-24T19:13:24Z</updated><resolved>2014-12-24T18:55:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="demon" created="2014-07-29T02:39:40Z" id="50428230">I spent a good part of my day today dealing with this issue. I believe it's exactly what you describe (and I'm pretty sure it's actually the same bug as #4790).

"Have the relocation/initialization algorithm calculate what disk space will be once all shards are fully moved rather than what disk space currently is."

This is the key, I think, to finding a solution to this bug. The comment from @xstevens on the other issue shows that it can happen even without disk_threshold enabled--just by adding nodes and the algorithm shuffling things around because it can't get comfy.
</comment><comment author="nik9000" created="2014-07-29T14:41:24Z" id="50485040">Another idea is to add another shard [balance weigher](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cluster-update-settings.html#_balanced_shards) that tries to make sure that shards of similar sizes are equally distributed across the cluster.  It'd be useful for more then just solving this problem and, I think, this problem is exacerbated by bad scattering on large shards.
</comment><comment author="dakrone" created="2014-07-29T14:45:26Z" id="50485728">@nik9000 yes, I agree, I have a branch for that somewhere that I have been meaning to get back to...
</comment><comment author="nik9000" created="2014-07-29T14:50:34Z" id="50486509">@demon, @gibrown can you think of a reason why pushing for even shard distribution by size wouldn't help this?  I'm sure it isn't enough to prevent the problem 100% but it should help.
</comment><comment author="demon" created="2014-07-29T15:00:19Z" id="50488025">I can't see why it wouldn't help.
</comment><comment author="nik9000" created="2014-07-29T17:24:47Z" id="50508872">I've been thinking that making sure that shards a more smoothly allocated by size can help us with our uneven load problems we've been having.  Size is decent stand in for the actual load caused by the shard.  It isn't perfect because some see much more use then others but it is something.
</comment><comment author="nik9000" created="2014-07-30T17:59:51Z" id="50654723">@dakrone let me know if this is something that you'll get to quickly - its kind of annoying to us at this point so I'm happy to work on it.  I just don't want to duplicate effort.
</comment><comment author="dakrone" created="2014-07-30T18:06:12Z" id="50655649">@nik9000 I don't think I'll be able to get to this too quickly, so if you'd like to work on it that would be awesome!
</comment><comment author="nik9000" created="2014-07-30T18:19:23Z" id="50657572">If you have a branch started for this is it worth sending it to me as a
starting point?

On Wed, Jul 30, 2014 at 2:06 PM, Lee Hinman notifications@github.com
wrote:

&gt; @nik9000 https://github.com/nik9000 I don't think I'll be able to get
&gt; to this too quickly, so if you'd like to work on it that would be awesome!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6168#issuecomment-50655649
&gt; .
</comment><comment author="gibrown" created="2014-07-31T18:06:21Z" id="50796147">Its not clear to me whether pushing for even shard distribution pays attention to the expected disk space used after a relocation has been finished. For instance, if a shard is in the process of getting moved to another node does this alg calculate the expected disk space after that relocation completes, and not just the current disk space used?
</comment><comment author="dakrone" created="2014-07-31T18:22:45Z" id="50798282">@nik9000 it's only a single commit, I didn't get as far as I had hoped with it: https://github.com/dakrone/elasticsearch/commit/493e1fd4857f8a02fde6aa22061ff3e854b24798, so not sure if it's even useful

@gibrown so the way the allocator works is by "simulating" adding all the shards different combinations of the nodes, so it could take the "after" disk usage into account by using the `InternalClusterInfoService`, which keeps track of the space on each node and size of each shard.
</comment><comment author="gibrown" created="2014-07-31T18:49:49Z" id="50802063">@dakrone this is the proposed change to the allocator  ("pushing for even shard distribution by size") or what it normally does? 

If the latter then I don't understand why we were seeing multiple shards getting relocated to a node that did not have enough disk space to accommodate those shards.
</comment><comment author="nik9000" created="2014-08-06T09:27:31Z" id="51312163">It looks like this _should_ already be handled:

``` java
        // Secondly, check that allocating the shard to this node doesn't put it above the high watermark
```

The only way we'd allocate if there wasn't enough space is:
1.  There is only on node.
2.  There node hasn't yet received the disk information.
3.  If the shard is a primary and isn't yet assigned.

Beyond that I _think_ we shouldn't allocate onto a node without enough space.  It looks like that gets stopped.
</comment><comment author="dakrone" created="2014-08-11T12:41:50Z" id="51774526">&gt; There node hasn't yet received the disk information.

Only if it doesn't have a shard size, if it doesn't have the host's disk usage it averages the usage across all hosts in the cluster.
</comment><comment author="clintongormley" created="2014-12-24T18:35:46Z" id="68068655">@dakrone With #7785 now merged in, what else needs to happen for us to be able to close this ticket?
</comment><comment author="dakrone" created="2014-12-24T18:55:39Z" id="68069474">I believe with #7785 and #8659 merged this is resolved, closing this. Feel free to reopen if needed!
</comment><comment author="gibrown" created="2014-12-24T19:13:24Z" id="68070268">Looks good to me. Thanks @dakrone et al.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Weird behaviour with highlighting when using highlight_query. Some field names work while others don't.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6167</link><project id="" key="" /><description>I noticed a weird highlighting issue when using highlight_query where some fields were not highlighted but others were. For example result highlighting doesn't work with field names like 'firstName', 'fa', 'f0' and 'g0' but works with field names 'f' and 'g' when using also '*' to highlight all fields. See the script below to get clearer image of the issue.

This occurred at least with elasticsearch-1.1.1.

Script:

```
#!/bin/bash
for fieldName in g g0 f f0; do
  curl -XDELETE localhost:9200/test
  curl -XPUT localhost:9200/test/test/1?pretty -d '{"'"$fieldName"'":"Content","subdoc":[{"title":"test document"}]}'
  sleep 1
  curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "query": {
        "query_string" : {
            "query" : "document AND '"$fieldName"':Content"
        }
    },
    "highlight": {
        "number_of_fragments" : 4,
        "pre_tags":["&lt;b&gt;"],
        "post_tags":["&lt;\u002fb&gt;"],
        "fields":{
            "*":{
                "highlight_query":{
                    "match":{
                        "*":{
                            "query":"document"
                        }
                    }
                }
            },
            "'"$fieldName"'":{
                "highlight_query":{
                    "match":{
                        "'"$fieldName"'":{
                            "query":"Content"
                        }
                    }
                }
            }
        }
    }
  }'
done
```

Output:

```
{"acknowledged":true}{
  "_index" : "test",
  "_type" : "test",
  "_id" : "1",
  "_version" : 1,
  "created" : true
}
{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.32546654,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.32546654, "_source" : {"g":"Content","subdoc":[{"title":"test document"}]},
      "highlight" : {
        "g" : [ "&lt;b&gt;Content&lt;/b&gt;" ],
        "subdoc.title" : [ "test &lt;b&gt;document&lt;/b&gt;" ]
      }
    } ]
  }
}
{"acknowledged":true}{
  "_index" : "test",
  "_type" : "test",
  "_id" : "1",
  "_version" : 1,
  "created" : true
}
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.32546654,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.32546654, "_source" : {"g0":"Content","subdoc":[{"title":"test document"}]},
      "highlight" : {
        "subdoc.title" : [ "test &lt;b&gt;document&lt;/b&gt;" ]
      }
    } ]
  }
}
{"acknowledged":true}{
  "_index" : "test",
  "_type" : "test",
  "_id" : "1",
  "_version" : 1,
  "created" : true
}
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.32546654,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.32546654, "_source" : {"f":"Content","subdoc":[{"title":"test document"}]},
      "highlight" : {
        "f" : [ "&lt;b&gt;Content&lt;/b&gt;" ],
        "subdoc.title" : [ "test &lt;b&gt;document&lt;/b&gt;" ]
      }
    } ]
  }
}
{"acknowledged":true}{
  "_index" : "test",
  "_type" : "test",
  "_id" : "1",
  "_version" : 1,
  "created" : true
}
{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 0.32546654,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "1",
      "_score" : 0.32546654, "_source" : {"f0":"Content","subdoc":[{"title":"test document"}]},
      "highlight" : {
        "subdoc.title" : [ "test &lt;b&gt;document&lt;/b&gt;" ]
      }
    } ]
  }
}
```

Markus
</description><key id="33486705">6167</key><summary>Weird behaviour with highlighting when using highlight_query. Some field names work while others don't.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">make</reporter><labels><label>:Highlighting</label><label>adoptme</label><label>bug</label></labels><created>2014-05-14T12:21:29Z</created><updated>2016-11-24T18:45:54Z</updated><resolved>2016-11-24T18:45:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="make" created="2014-10-03T06:37:29Z" id="57761165">Got this to work in elasticsearch 1.3.4 by changing the order of elements in highlight.fields.
Why does the order of elements have an effect on results?
So this works in 1.3.4:

``` bash
#!/bin/bash
for fieldName in g g0 f f0; do
  curl -XDELETE localhost:9200/test
  curl -XPUT localhost:9200/test/test/1?pretty -d '{"'"$fieldName"'":"Content","subdoc":[{"title":"test document"}]}'
  sleep 1
  curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "query": {
        "query_string" : {
            "query" : "document AND '"$fieldName"':Content"
        }
    },
    "highlight": {
        "number_of_fragments" : 4,
        "pre_tags":["&lt;b&gt;"],
        "post_tags":["&lt;\u002fb&gt;"],
        "fields":{
            "'"$fieldName"'":{
                "highlight_query":{
                    "match":{
                        "'"$fieldName"'":{
                            "query":"Content"
                        }
                    }
                }
            },
            "*":{
                "highlight_query":{
                    "match":{
                        "*":{
                            "query":"document"
                        }
                    }
                }
            }
        }
    }
  }'
done
```
</comment><comment author="nik9000" created="2014-10-03T11:39:53Z" id="57783442">Two questions:

Can you reproduce it without the highlight_query?  Can you reproduce it
without the \* field?

I imagine it has more to do with the \* field then the highlight_query. As
to why order matters:  elasticsearch 1.3 executes highlighting in the order
it is received. That may be relevant here.
On Oct 3, 2014 2:37 AM, "make" notifications@github.com wrote:

&gt; Got this to work in elasticsearch 1.3.4 by changing the order of elements
&gt; in highlight.fields.
&gt; Why does the order of elements have an effect on results?
&gt; So this works in 1.3.4:
&gt; 
&gt; #!/bin/bashfor fieldName in g g0 f f0; do
&gt;   curl -XDELETE localhost:9200/test
&gt;   curl -XPUT localhost:9200/test/test/1?pretty -d '{"'"$fieldName"'":"Content","subdoc":[{"title":"test document"}]}'
&gt;   sleep 1
&gt;   curl -XGET localhost:9200/test/test/_search?pretty -d '{    "query": {        "query_string" : {            "query" : "document AND '"$fieldName"':Content"        }    },    "highlight": {        "number_of_fragments" : 4,        "pre_tags":["&lt;b&gt;"],        "post_tags":["&lt;\u002fb&gt;"],        "fields":{            "'"$fieldName"'":{                "highlight_query":{                    "match":{                        "'"$fieldName"'":{                            "query":"Content"                        }                    }                }            },            "_":{                "highlight_query":{                    "match":{                        "_":{                            "query":"document"                        }                    }                }            }        }    }  }'done
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/6167#issuecomment-57761165
&gt; .
</comment><comment author="make" created="2014-10-06T05:53:32Z" id="57977028">The case is that I want to search documents that match some query string in any field AND the other query string only in one field. And I want highlighting to show only real matches.
So the script:

```
#!/bin/bash
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test/test/1?pretty -d '{
    "content":"some content",
    "subdoc":[{"title":"some test document"}]
}'
sleep 1
curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "query": {
        "query_string" : {
            "query" : "document AND content:some"
        }
    },
    "highlight": {
        "number_of_fragments" : 4,
        "pre_tags":["&lt;b&gt;"],
        "post_tags":["&lt;\u002fb&gt;"],
        "fields":{
            "*":{}
        }
    }
}'
```

has output:

```
...
      "highlight" : {
        "content" : [ "&lt;b&gt;some&lt;/b&gt; content" ],
        "subdoc.title" : [ "&lt;b&gt;some&lt;/b&gt; test &lt;b&gt;document&lt;/b&gt;" ]
      }
...
```

while I except the following output (the query_string has 'content:some') :

```
...
      "highlight" : {
        "content" : [ "&lt;b&gt;some&lt;/b&gt; content" ],
        "subdoc.title" : [ "some test &lt;b&gt;document&lt;/b&gt;" ]
      }
...
```
</comment><comment author="make" created="2014-10-06T06:13:30Z" id="57977888">Same occurs with bool query:

``` bash
#!/bin/bash
curl -XDELETE localhost:9200/test
curl -XPUT localhost:9200/test/test/1?pretty -d '{
    "content":"some content",
    "subdoc":[{"title":"some test document"}]
}'
sleep 1
curl -XGET localhost:9200/test/test/_search?pretty -d '{
    "query": {
        "bool": {
            "should": [
                { "match": { "_all":    "document" }},
                { "match": { "content": "some" }}
            ]
        }
    },
    "highlight": {
        "number_of_fragments" : 4,
        "pre_tags":["&lt;b&gt;"],
        "post_tags":["&lt;\u002fb&gt;"],
        "fields":{
            "*":{}
        }
    }
}'

```
</comment><comment author="clintongormley" created="2016-11-24T18:45:54Z" id="262831181">Just realised the problem.  The `match` query is for a single field, so it was just matching whichever field came first.  This will no longer match any fields in 5.0</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update guide on vm.swappiness values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6166</link><project id="" key="" /><description>Recent versions of the Linux kernel (3.5-rc1, RHEL/CentOS &gt; 2.6.32-303) have [changed the behaviour vm.swappiness](http://www.mysqlperformanceblog.com/2014/04/28/oom-relation-vm-swappiness0-new-kernel/). A swappiness value of `0` in these kernel's will prevent swapping out 
anonymous memory, other than unpinned file backed pages, making it much more likely the the OOM killer will be activated rather than the desired behaviour of the kernel using swap space. A value of `1` will still allow the use of swap space in emergencies.
</description><key id="33486281">6166</key><summary>Update guide on vm.swappiness values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aliles</reporter><labels /><created>2014-05-14T12:15:00Z</created><updated>2014-08-07T18:37:11Z</updated><resolved>2014-08-07T18:37:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T12:39:43Z" id="43074734">Hmm, I'm wondering if this advice (the current advice plus the changes in the PR) is really what we're aiming for.

I'm of the opinion that you never want ES to swap.  As soon as it starts using swap, you've lost.  Assuming you're only running Elasticsearch on a node by itself (the usual scenario) and you've set `ES_HEAP_SIZE` to an appropriate value, you should never need to use swap.

If some weird circumstances lead ES to require more memory than is configured and available, I think I'd prefer the OOM killer to do its job rather than have a degraded node impacting the rest of my cluster.  
</comment><comment author="aliles" created="2014-05-14T13:10:16Z" id="43077667">Very true, and a good point. the OOM killer does have a reputation for being somewhat counter-intuitive in what gets killed. Postgres used to joke that it would search for server instances first. I've seen what we believe to be sshd get killed rather than other out of control processes. For our deployment I prefer to allow swapping as a last resort and trust system monitoring and automated health tools to remove bad nodes.

But that's probably overly complex for the reference docs and the wording around '0' should altered and simplified. Thoughts?

On Wed, May 14, 2014 at 10:40 PM, Clinton Gormley
notifications@github.com wrote:

&gt; Hmm, I'm wondering if this advice (the current advice plus the changes in the PR) is really what we're aiming for.
&gt; I'm of the opinion that you never want ES to swap.  As soon as it starts using swap, you've lost.  Assuming you're only running Elasticsearch on a node by itself (the usual scenario) and you've set `ES_HEAP_SIZE` to an appropriate value, you should never need to use swap.
&gt; 
&gt; ## If some weird circumstances lead ES to require more memory than is configured and available, I think I'd prefer the OOM killer to do its job rather than have a degraded node impacting the rest of my cluster.  
&gt; 
&gt; Reply to this email directly or view it on GitHub:
&gt; https://github.com/elasticsearch/elasticsearch/pull/6166#issuecomment-43074734
</comment><comment author="clintongormley" created="2014-05-14T14:12:39Z" id="43085158">OK, I've had a go at rewriting that whole section to be more useful.  What do you think?

Diff is here: https://github.com/elasticsearch/elasticsearch/commit/8f0991c14f2c4815491db37bc89f3c1030d950ff
but it'd be easier to read the new version here:  http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html?refresh#setup-configuration-memory
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6165</link><project id="" key="" /><description>Minor typo in docs.
</description><key id="33479040">6165</key><summary>Fix typo in docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gauravarora</reporter><labels /><created>2014-05-14T10:15:47Z</created><updated>2014-07-16T21:45:10Z</updated><resolved>2014-05-14T10:37:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T10:37:12Z" id="43065470">Merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Security: Make JSONP responses optional.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6164</link><project id="" key="" /><description>This poses a significant security threat by being on by default. If an attacker can entice a user to load a legitimate ElasticSearch query as a script tag, they can effectively bypass SOP and exfiltrate the contents of a local ElasticSearch instance (or bypass firewall rules by using a victim's browser to talk to a remote instance). 

This would be trivial to implement from an attackers perspective, and I'll be submitting a pull request to the BeEF project (https://github.com/beefproject/beef) to automate this sort of attack, as it'll be useful during a pentesting engagement. 

If you have any questions or change requests please let me know, and I'll be more than happy to accommodate. 

Thanks!
Fitblip
</description><key id="33469273">6164</key><summary>Security: Make JSONP responses optional.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Fitblip</reporter><labels><label>:Settings</label><label>enhancement</label><label>v1.3.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-14T07:51:15Z</created><updated>2015-06-07T13:29:44Z</updated><resolved>2014-06-19T06:37:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Fitblip" created="2014-05-14T07:52:04Z" id="43051927">Crap, didn't mean to include the first commit. If you want me to remove it and re-submit my pull request, please let me know. 
</comment><comment author="kimchy" created="2014-05-28T13:15:48Z" id="44404498">I like this, and we were discussing internally to potentially disable this by default for 1.3, but at the very least people should be able to disable it. Few comments:
- Can you remove the CORS comment? We have an idea on how to potentially solve this cleanly while still retaining out of the box experience, we will open an issue soon about it, it will be addressed in 1.3
- For now, can you default it to true, and remove setting it in the settings, and lets open a different issue to discuss its default value?
</comment><comment author="Fitblip" created="2014-05-29T08:00:09Z" id="44505318">Agreed, thanks for the comments and insight, I'll work on that getting
implemented (hopefully) tomorrow and send an updated pull request.  :)

As far as keeping it on by default, that's totally fair, I didn't think
about breaking backwards compatibility when I submitted this. We just
pushed to remediate this issue within our environment and noticed there
wasn't even a config directive for it!

What is JSONP usually used for? Kibana?

Fitblip

On Wed, May 28, 2014 at 6:16 AM, Shay Banon notifications@github.comwrote:

&gt; I like this, and we were discussing internally to potentially disable this
&gt; by default for 1.3, but at the very least people should be able to disable
&gt; it. Few comments:
&gt; - Can you remove the CORS comment? We have an idea on how to
&gt;   potentially solve this cleanly while still retaining out of the box
&gt;   experience, we will open an issue soon about it, it will be addressed in 1.3
&gt; - For now, can you default it to true, and remove setting it in the
&gt;   settings, and lets open a different issue to discuss its default value?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/6164#issuecomment-44404498
&gt; .
</comment><comment author="Fitblip" created="2014-05-29T21:45:24Z" id="44589520">Hey @kimchy,

I've implemented your requested changes. Does everything look good?

Also if you guys would like to discuss JSONP, CORS, or really anything security related, please feel free to reach out to me. I use elasticsearch enough that I'm happy to give back in whatever form I can. 

Fitblip
</comment><comment author="kimchy" created="2014-05-29T22:36:11Z" id="44594511">@Fitblip thanks!, will definitely keep you in the loop and @ you on the issue we open. @spinscale looks good to me, what do you think?
</comment><comment author="Fitblip" created="2014-05-30T06:14:26Z" id="44618391">Awesome! Glad to help in any way I can :+1: 
</comment><comment author="spinscale" created="2014-05-30T06:28:38Z" id="44619050">This looks great. left very minor comments. I'd like to include it, and just saw that you haven't signed the CLA. As soon as you do that (under http://www.elasticsearch.org/contributor-agreement/), I am happy to get this PR in!
</comment><comment author="Fitblip" created="2014-05-30T19:23:04Z" id="44689640">Hey @spinscale,

I think I've implemented the changes you requested (I had some confusion on the first one), and signed the CLA.

Let me know if you need anything else from me :smiley: 
</comment><comment author="spinscale" created="2014-06-02T15:03:11Z" id="44848165">Hey there,

I have been thinking about this a bit, and have another question about this impl. If you disable sending of JSONP, do you really want to execute the search request and return the results without a callback or do you want to return an error message telling the user that JSONP is disabled (and also not wasting precious process time executing a query).

What do you think makes more sense? I am leaning towards the option of returning an error, but maybe this is not considered best practice?
</comment><comment author="Fitblip" created="2014-06-02T21:50:05Z" id="44896117">Hey @spinscale,

That's a good point actually. Returning a JSONP-formatted error response probably makes the most sense. 

I can re-factor this to make that happen, but it may be a couple days. 

Thanks!
</comment><comment author="Fitblip" created="2014-06-03T07:31:53Z" id="44928274">@spinscale 

This should throw an error message when JSONP is disabled, but I'm not sure where the actually query is getting executed, so I 't know if by not actually passing back the buffer it's never read from/executed or what. By the time NettyHttpChannel.sendResponse() is invoked is the query already run?

Or is this where it's executed?
https://github.com/Fitblip/elasticsearch/blob/master/src/main/java/org/elasticsearch/http/netty/NettyHttpChannel.java#L128
</comment><comment author="spinscale" created="2014-06-03T08:42:24Z" id="44938677">@Fitblip yeah the request has already been executed by then, if you have a RestResponse object

I guess we need to take a look at `HttpServer.dispatchRequest()` or `RestController.dispatchRequest()` and reject the request, before its being parsed.

I realize that you already put a considerable amount of time into this one, so I can just check by myself if you want, but I am as happy to help and assist, if you want to move forward (thanks a lot for all your work so far!)

Also, we need a test here, when we are clear about the actual impl, to make sure, the setting works as expected :-)
</comment><comment author="Fitblip" created="2014-06-03T08:57:46Z" id="44939934">Awesome! Thanks for the info. For the most recent push do you have any preferences as to the parameters passed back through JSONP? Right now it's just `error` and `status` of 501 (HTTP not implemented), though we can either change that or remove it entirely. Up to you guys. 

I can certainly take a stab at checking the parameter and bailing out. We can hammer out impl details once I have something to propose :).

I'll also be happy to write the test/s.
</comment><comment author="Fitblip" created="2014-06-04T10:11:30Z" id="45073437">Hey @spinscale,

It looks like RestController.dispatchRequest is the place to do it (to me). I've essentially reverted all my changes to `NettyHttpChannel.java`, and implemented something that should work in `RestController.java`. I don't have time to test it tonight, but does my logic look sound to you? Anything else I should be concerned about?

I'll make sure things are working as expected tomorrow (not sure if I'm calling settings the right way), and fix any issues that crop up. 

Thanks!
</comment><comment author="spinscale" created="2014-06-04T11:47:24Z" id="45080823">looks good (sorry, travelling the rest of the week, so I dont have too much time to comment)
</comment><comment author="Fitblip" created="2014-06-04T18:51:56Z" id="45134769">No worries @spinscale. This should be good to go (minus the test). I've tested it on my local machine and it appears to work as expected. 

I'll work on getting the test done, but I'm not so familiar with java tests (I mostly come from a python background), but I'm sure I'll figure something out. 
</comment><comment author="spinscale" created="2014-06-10T08:52:50Z" id="45589053">hey,

I took a look at this and like it so far, I also have a test ready, so if you are ok with this, I take your work, add a test and push it.
</comment><comment author="Fitblip" created="2014-06-10T10:05:35Z" id="45595487">Awesome! I guess we were both working on tests at the same time :).

If you don't mind, I'd like to get some help figuring out where my tests could be improved, and push this whole thing in together. That way I can actually write quality tests moving forward! If you prefer email communication you can reach me at my github username @gmail.com
</comment><comment author="spinscale" created="2014-06-10T10:58:07Z" id="45599548">actually my test looks nearly the same, nice work! Just had some minor nitpicks. Apart from that we are good and almost ready to go! Maybe you can squash the commits and force push so we only have one history commit, when getting it into ES.
</comment><comment author="Fitblip" created="2014-06-11T05:55:17Z" id="45704639">Cool - thanks a ton for your help in all this @spinscale. One last thing, I notice that my disabled test seems to fail. Any idea why? It could very well be my eclipse env, but it seems like I should release something or call some sort of finished routine. For my JsonpOptionEnabledTest things are all green, but not for the disabled test. Traceback inline. 

```
[2014-06-10 22:52:32,915][ERROR][org.elasticsearch.test   ] FAILURE  : testThatJSONPisDisabled(org.elasticsearch.options.jsonp.JsonpOptionDisabledTest)
REPRODUCE WITH  : mvn test -Dtests.seed=95F2428CAE02E7BE -Dtests.class=org.elasticsearch.options.jsonp.JsonpOptionDisabledTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Dtests.method="testThatJSONPisDisabled"
Throwable:
java.lang.RuntimeException: 1 arrays have not been released
    org.elasticsearch.test.cache.recycler.MockBigArrays.ensureAllArraysAreReleased(MockBigArrays.java:68)
    org.elasticsearch.test.ElasticsearchTestCase.ensureAllArraysReleased(ElasticsearchTestCase.java:135)
    [...sun.*, com.carrotsearch.randomizedtesting.*, java.lang.reflect.*]
    org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)
    org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)
    org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    [...com.carrotsearch.randomizedtesting.*]
    org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)
    org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    [...com.carrotsearch.randomizedtesting.*]
    org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)
    org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)
    [...com.carrotsearch.randomizedtesting.*]
    org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)
    org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)
    org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)
    org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)
    [...com.carrotsearch.randomizedtesting.*]
    java.lang.Thread.run(Unknown Source)
```
</comment><comment author="Fitblip" created="2014-06-11T05:55:32Z" id="45704654">JsonpOptionEnabledTest test output:

```
[2014-06-10 22:54:25,483][INFO ][org.elasticsearch.test   ] Setup TestCluster [shared-doctor-PC-CHILD_VM=[0]-CLUSTER_SEED=[-5018658234085937559]-HASH=[2724E9BADD16B]] with seed [BA5A24ED1437EE69] using [3] data nodes and [0] client nodes
[2014-06-10 22:54:25,492][INFO ][org.elasticsearch.test   ] Test testThatJSONPisEnabled(org.elasticsearch.options.jsonp.JsonpOptionEnabledTest) started
[2014-06-10 22:54:25,500][INFO ][org.elasticsearch.test   ] Setup TestCluster [TEST-doctor-PC-CHILD_VM=[0]-CLUSTER_SEED=[-5583103424550574148]-HASH=[2724E9CC19E5F]] with seed [B284D504136567BC] using [1] data nodes and [0] client nodes
[2014-06-10 22:54:25,618][INFO ][org.elasticsearch.node   ] [node_0] version[2.0.0-SNAPSHOT], pid[11164], build[${build/NA]
[2014-06-10 22:54:25,618][INFO ][org.elasticsearch.node   ] [node_0] initializing ...
[2014-06-10 22:54:25,620][INFO ][org.elasticsearch.plugins] [node_0] loaded [], sites []
[2014-06-10 22:54:26,790][INFO ][org.elasticsearch.node   ] [node_0] initialized
[2014-06-10 22:54:26,790][INFO ][org.elasticsearch.node   ] [node_0] starting ...
[2014-06-10 22:54:26,896][INFO ][org.elasticsearch.test.transport] [node_0] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.10.120:9300]}
[2014-06-10 22:54:29,988][INFO ][org.elasticsearch.cluster.service] [node_0] new_master [node_0][4fiTwUX6SA6IvLptNgNMRw][doctor-PC][inet[/192.168.10.120:9300]], reason: zen-disco-join (elected_as_master)
[2014-06-10 22:54:30,005][INFO ][org.elasticsearch.discovery] [node_0] TEST-doctor-PC-CHILD_VM=[0]-CLUSTER_SEED=[-5583103424550574148]-HASH=[2724E9CC19E5F]/4fiTwUX6SA6IvLptNgNMRw
[2014-06-10 22:54:30,014][INFO ][org.elasticsearch.gateway] [node_0] recovered [0] indices into cluster_state
[2014-06-10 22:54:30,041][INFO ][org.elasticsearch.http   ] [node_0] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.10.120:9200]}
[2014-06-10 22:54:30,042][INFO ][org.elasticsearch.node   ] [node_0] started
[2014-06-10 22:54:30,042][INFO ][org.elasticsearch.test   ] Start Shared Node [node_0] not shared
[2014-06-10 22:54:30,048][INFO ][org.elasticsearch.plugins] [transport_client_node_0] loaded [], sites []
[2014-06-10 22:54:30,170][INFO ][org.elasticsearch.options.jsonp] [JsonpOptionEnabledTest#testThatJSONPisEnabled]: before test
[2014-06-10 22:54:30,207][INFO ][org.elasticsearch.options.jsonp] [JsonpOptionEnabledTest#testThatJSONPisEnabled]: cleaning up after test
[2014-06-10 22:54:30,238][INFO ][org.elasticsearch.node   ] [node_0] stopping ...
[2014-06-10 22:54:30,244][INFO ][org.elasticsearch.node   ] [node_0] stopped
[2014-06-10 22:54:30,244][INFO ][org.elasticsearch.node   ] [node_0] closing ...
[2014-06-10 22:54:30,246][INFO ][org.elasticsearch.node   ] [node_0] closed
[2014-06-10 22:54:30,246][INFO ][org.elasticsearch.options.jsonp] [JsonpOptionEnabledTest#testThatJSONPisEnabled]: cleaned up after test
[2014-06-10 22:54:30,247][INFO ][org.elasticsearch.test   ] Wipe data directory for all nodes locations: [C:\Users\doctor\Documents\GitHub\elasticsearch\data\TEST-doctor-PC-CHILD_VM=[0]-CLUSTER_SEED=[-5583103424550574148]-HASH=[2724E9CC19E5F]\nodes\0] success: true
[2014-06-10 22:54:30,264][INFO ][org.elasticsearch.test   ] Test testThatJSONPisEnabled(org.elasticsearch.options.jsonp.JsonpOptionEnabledTest) finished
```
</comment><comment author="spinscale" created="2014-06-13T14:36:02Z" id="46018533">couple of things here:

The test fails, because you do not free resources.. the builder actually has a `close()` method, that would be called, if you supplied it directly to the BytesRestResponse - luckily we catch those in the tests :-)

I suggest the following code to fix it and to have the right content type

```
                XContentBuilder builder = channel.newBuilder();
                builder.startObject().field("error","JSONP is disabled.").endObject();
                RestResponse response = new BytesRestResponse(FORBIDDEN, builder);
                response.addHeader("Content-Type", "application/javascript");
                channel.sendResponse(response);
```

If you do this, you can change the test to check for this as well

```
        assertThat(response.response(), containsString("JSONP is disabled"));
        assertThat(response.getHeader("Content-Type"), is("application/javascript"));
```

However, the test will fail, as there is a tiny bug in `NettyHttpChannel.sendResponse()`, that overrides the callback response. The method should check if the header is set already and only act if it isnt

```
            if (!resp.headers().contains(HttpHeaders.Names.CONTENT_TYPE)) {
                resp.headers().add(HttpHeaders.Names.CONTENT_TYPE, response.contentType());
            }
            if (!resp.headers().contains(HttpHeaders.Names.CONTENT_LENGTH)) {
                resp.headers().add(HttpHeaders.Names.CONTENT_LENGTH, String.valueOf(buffer.readableBytes()));
            }
```

I think thats it (with the exception of squashing) :-)
</comment><comment author="Fitblip" created="2014-06-13T22:51:46Z" id="46068409">Wonderful! This is exactly what I was looking for. I'll get those commits
in tonight, and assuming I get the all-green from you squash everything
together, and we can get this closed!

Thanks for all your help in all this. If you ever find yourself on the west
coast, I definitely owe you some beers ;)

Ryan

On Fri, Jun 13, 2014 at 7:36 AM, Alexander Reelsen &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; couple of things here:
&gt; 
&gt; The test fails, because you do not free resources.. the builder actually
&gt; has a close() method, that would be called, if you supplied it directly
&gt; to the BytesRestResponse - luckily we catch those in the tests :-)
&gt; 
&gt; I suggest the following code to fix it and to have the right content type
&gt; 
&gt; ```
&gt;             XContentBuilder builder = channel.newBuilder();
&gt;             builder.startObject().field("error","JSONP is disabled.").endObject();
&gt;             RestResponse response = new BytesRestResponse(FORBIDDEN, builder);
&gt;             response.addHeader("Content-Type", "application/javascript");
&gt;             channel.sendResponse(response);
&gt; ```
&gt; 
&gt; If you do this, you can change the test to check for this as well
&gt; 
&gt; ```
&gt;     assertThat(response.response(), containsString("JSONP is disabled"));
&gt;     assertThat(response.getHeader("Content-Type"), is("application/javascript"));
&gt; ```
&gt; 
&gt; However, the test will fail, as there is a tiny bug in
&gt; NettyHttpChannel.sendResponse(), that overrides the callback response.
&gt; The method should check if the header is set already and only act if it isnt
&gt; 
&gt; ```
&gt;         if (!resp.headers().contains(HttpHeaders.Names.CONTENT_TYPE)) {
&gt;             resp.headers().add(HttpHeaders.Names.CONTENT_TYPE, response.contentType());
&gt;         }
&gt;         if (!resp.headers().contains(HttpHeaders.Names.CONTENT_LENGTH)) {
&gt;             resp.headers().add(HttpHeaders.Names.CONTENT_LENGTH, String.valueOf(buffer.readableBytes()));
&gt;         }
&gt; ```
&gt; 
&gt; I think thats it (with the exception of squashing) :-)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/6164#issuecomment-46018533
&gt; .
</comment><comment author="Fitblip" created="2014-06-14T22:33:14Z" id="46101446">@spinscale this _should_ be good to go. All tests are green and everything works as expected. Squashing. 
</comment><comment author="spinscale" created="2014-06-19T06:40:59Z" id="46528289">Finally. Thanks a lot for all the time you invested here! Highly appreciated.

Will come back to your west coast offer on the long run :-)
</comment><comment author="Fitblip" created="2014-06-19T06:41:41Z" id="46528333">Woo hoo! Thanks a lot for your help as well :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[debian package] java dependency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6163</link><project id="" key="" /><description>The debian package should have an explicit dependency on java
</description><key id="33464543">6163</key><summary>[debian package] java dependency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sanga</reporter><labels><label>:Packaging</label></labels><created>2014-05-14T06:27:03Z</created><updated>2016-10-31T04:09:55Z</updated><resolved>2014-10-24T09:05:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="robob4him" created="2014-05-16T18:13:52Z" id="43362368">+1
</comment><comment author="uschindler" created="2014-09-29T18:42:48Z" id="57208158">-1

I want to configure it with my own Java version, so I dont want any Java installed automatically from any Debian/Ubuntu repository. The workaround would only be to install a fake java package, but that makes it harder to install.
</comment><comment author="nik9000" created="2014-09-29T18:58:42Z" id="57210550">Isn't there a virtual package or something that just say "I need some version of Java, you figure it out"?  I remember something like that in Debian but not the specifics.  It'd be annoying if you aren't using a Debian package for Java though.  As much as that's not recommended it'd be nice to support it somehow.
</comment><comment author="robob4him" created="2014-09-29T19:05:15Z" id="57211546">@uschindler Why don't you install your desired java version prior to elasticsearch? If the package is built properly it should "provide" the dependency elasticsearch has, yes? eg: openjdk-6-jre-headless "provides" java6-runtime-headless. Otherwise, why use a package manager if you don't tell the package manager what all your dependencies are?
</comment><comment author="uschindler" created="2014-09-29T20:06:37Z" id="57220700">I want to prevent (and that is the intention of the ES developers) to "automatically" install any crazy java version. Lucene is very sensitive to Java bugs, so unless there is some conflicts in the package on several Java versions like 7u40, 7u45, 7u51 or 7GA (build 147), its very risky to add a dependency on "any" java. Also IBM J9 is a no-go for Lucene.

If you get a good error message on startup that a Java is missing, one can istall one and its his own risk to choose the right one. The package maintainer of a separate repository like ES's cannot do this.
</comment><comment author="clintongormley" created="2014-10-24T09:05:36Z" id="60362273">Got to agree with @uschindler here - we recommend the latest JVM and prefer Oracle.  Requiring a debian package of Java will just make things more complex.

Closing
</comment><comment author="timwsuqld" created="2016-10-31T04:09:55Z" id="257210363">This is just doing packaging the wrong way. If you are going to install a java version manually, you can install Elastic Search manually too, because you've already decided that you don't want your package manager to handle dependencies for you.
If you are going to package something, you need to declare dependencies, otherwise the package isn't actually useful.
Anyone who wants to manually install java, and use a package for Elastic Search can use a dummy package that satisfies the JRE requirement.

Please reconsider this decision so your apt repo and deb packages are actually useful
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Only use dateline fix when polygon crosses dateline.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6162</link><project id="" key="" /><description>- es1.x attempts to split polygons on the dateline along
  with holes, and then re-assign the holes to the new
  polygons.  In certain circumstances this assignment
  fails (https://github.com/elasticsearch/elasticsearch/issues/5773)
  The geojson format makes it clear which hole belongs to which
  polygon, and these issues can be avoided for polygons which
  do not span the dateline.
- there are additional issues with splitting polygons on the
  dateline.  Some shapes are lost in certain circumstances (bug
  yet to be filed).
  Sample shape: https://gist.github.com/anonymous/7f1bb6d7e9cd72f5977c
</description><key id="33463099">6162</key><summary>Only use dateline fix when polygon crosses dateline.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">marcuswr</reporter><labels /><created>2014-05-14T05:51:02Z</created><updated>2014-07-28T09:13:05Z</updated><resolved>2014-07-28T09:13:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="marcuswr" created="2014-05-16T18:01:17Z" id="43361127">@spinscale could you please take a look at this patch and provide feedback?
</comment><comment author="clintongormley" created="2014-07-28T09:13:05Z" id="50315398">Hi @marcuswr 

Thanks for the PR, sorry it took a while to get to it.  It looks like this issue has already been fixed by https://github.com/elasticsearch/elasticsearch/pull/6976
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix typo in network docs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6161</link><project id="" key="" /><description /><key id="33441981">6161</key><summary>Fix typo in network docs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">rdeaton</reporter><labels><label>docs</label></labels><created>2014-05-13T21:42:08Z</created><updated>2014-06-26T16:25:00Z</updated><resolved>2014-06-03T11:25:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:46:06Z" id="43061459">HI @rdeaton 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="rdeaton" created="2014-05-14T16:39:51Z" id="43105221">Done
</comment><comment author="javanna" created="2014-06-03T11:25:54Z" id="44952119">Ops I just realized I merged a slighlty more recent PR with exactly the same change (#6270). Sorry about that! Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[DOCS] Cat recovery API update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6160</link><project id="" key="" /><description>This is an update for the _cat/recovery API documentation. The examples
have been updated. Removed the bottom paragraph explaining why there
could be values &gt; 100%. This can no longer happen so that had to be
removed.

Closes #6159
</description><key id="33433890">6160</key><summary>[DOCS] Cat recovery API update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">aleph-zero</reporter><labels /><created>2014-05-13T20:08:34Z</created><updated>2014-10-21T23:35:53Z</updated><resolved>2014-05-19T00:45:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-05-14T16:14:45Z" id="43101956">:+1: 
</comment><comment author="aleph-zero" created="2014-05-14T16:16:42Z" id="43102218">You rock @drewr. Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cat recovery API documentation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6159</link><project id="" key="" /><description>The API documentation for _cat/recovery is out of date.
</description><key id="33433804">6159</key><summary>Cat recovery API documentation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/aleph-zero/following{/other_user}', u'events_url': u'https://api.github.com/users/aleph-zero/events{/privacy}', u'organizations_url': u'https://api.github.com/users/aleph-zero/orgs', u'url': u'https://api.github.com/users/aleph-zero', u'gists_url': u'https://api.github.com/users/aleph-zero/gists{/gist_id}', u'html_url': u'https://github.com/aleph-zero', u'subscriptions_url': u'https://api.github.com/users/aleph-zero/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/452273?v=4', u'repos_url': u'https://api.github.com/users/aleph-zero/repos', u'received_events_url': u'https://api.github.com/users/aleph-zero/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/aleph-zero/starred{/owner}{/repo}', u'site_admin': False, u'login': u'aleph-zero', u'type': u'User', u'id': 452273, u'followers_url': u'https://api.github.com/users/aleph-zero/followers'}</assignee><reporter username="">aleph-zero</reporter><labels><label>docs</label><label>v1.2.0</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T20:07:28Z</created><updated>2014-05-19T09:47:45Z</updated><resolved>2014-05-19T00:45:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2014-05-16T11:06:26Z" id="43320125">are we getting this in? seems like it needs a target version too?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dynamic date detection accepting years &lt; 4 digits</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6158</link><project id="" key="" /><description>Dynamic date detection should only detect 4 digit years, but it is misinterpreting the `5` in `5/12/14` as the year `0005`:

```
DELETE /t

PUT /t/t/1
{
  "d": "5/12/14"
}

GET /t/_search
{
  "aggs": {
    "foo": {
      "date_histogram": {
        "field": "d",
        "interval": "day"
      }
    }
  }
}
```

Returns: 

```
"aggregations": {
  "foo": {
     "buckets": [
        {
           "key_as_string": "0005/12/14 00:00:00",
           "key": -61979385600000,
           "doc_count": 1
        }
     ]
  }
}
```
</description><key id="33421400">6158</key><summary>Dynamic date detection accepting years &lt; 4 digits</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>bug</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T17:41:31Z</created><updated>2015-07-07T07:38:25Z</updated><resolved>2015-07-07T07:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-05-15T15:57:39Z" id="43228492">looked at the iso 8601 spec, which seems to define a year has to have four digits... however the predefined joda time formatters do not seem to be strict here
</comment><comment author="s1monw" created="2014-05-15T15:58:30Z" id="43228633">maybe we should file a bug in JODA or maybe it's already fixed and we should upgrade?
</comment><comment author="spinscale" created="2014-05-15T15:59:51Z" id="43228815">yeah, there is a `strict` option in joda, not sure if we can make use of that, investigating next
</comment><comment author="spinscale" created="2014-05-16T08:53:22Z" id="43310398">By copy pasting a fair share of code around (from the `ISODateTimeFormat` class), we can definately make this more strict. Asked on the joda ML if this the intended behaviour and if there are smarter ways to make parsing strict and will then decide on the implementation
</comment><comment author="kevinkluge" created="2014-05-16T14:28:00Z" id="43337410">Given that this is somewhat of a breaking change (it's a bug fix that will likely break some ingestion) I'm going to push to 2.0.  

For now people that encounter the issue have to define a mapping for the field
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update nested-query.asciidoc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6157</link><project id="" key="" /><description>Added note that fields inside a nested query must be full qualified.
</description><key id="33418526">6157</key><summary>Update nested-query.asciidoc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">shadow000fire</reporter><labels /><created>2014-05-13T17:06:07Z</created><updated>2014-06-12T10:49:18Z</updated><resolved>2014-06-12T10:49:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:46:51Z" id="43061537">Hi @shadow000fire 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="clintongormley" created="2014-06-12T10:46:19Z" id="45877291">CLA not signed. Treating as a bug report
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to parse [date]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6156</link><project id="" key="" /><description>Every month I encounter the strange problem of elasticsearch refusing to index data from logstash on the 13th complaining that the Month field must be between 1-12. But on the 14th it seems to have figured out that the month field is not the Day field and everything starts indexing again. I know this sounds ridiculous but it has happened for 6 months without fail in 3 different versions of elastic search. .90, 1.0, 1.1. Can someone please tell me I'm not just losing my mind? Maybe tell me where the settings for date parsing are located so I can update it to the format my logs use? Thank you for any information you can provide... 

[2014-05-13 07:32:17,679][DEBUG][action.bulk              ] [Leonus] [logstash-2014.05.13][3] failed to execute bulk item (index) index {[logstash-2014.05.13][websphere][3lgoq87JQLe3fbk_gm80Lw], source[{"message":"[5/13/14 6:46:42:536 EDT] 00011380 SibMessage    W   [:] CWSJY0003W: MQJCA4013: A connection to a queue manager failed for activationSpec 'javax.jms.Queue:jms/WP.EVENTS.Q@NYPROD.QM1 &lt;-862592831&gt;'. Check the queue manager error logs for details.","@version":"1","@timestamp":"2014-05-13T10:46:42.536Z","type":"websphere","host":"mcsps02.militarycars.com","path":"/opt/IBM/WebSphere/AppServer/profiles/Mcsbpmps01/logs/McsBpm85De01.AppCluster.mcsps02Node01/SystemOut.log","TimeDate":"5/13/14 6:46:42:536 EDT","DATE":"5/13/14","TIME":"6:46:42:536","TIMEZONE":"EDT","EventCode":"00011380","EventType":"SibMessage","ReadWriteOperation":"W","Description":"  [:] CWSJY0003W: MQJCA4013: A connection to a queue manager failed for activationSpec 'javax.jms.Queue:jms/WP.EVENTS.Q@NYPROD.QM1 &lt;-862592831&gt;'. Check the queue manager error logs for details."}]}
org.elasticsearch.index.mapper.MapperParsingException: failed to parse [DATE]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:418)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:616)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:469)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:363)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardIndexOperation(TransportShardBulkAction.java:406)
    at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:157)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [5/13/14], tried both date format [yyyy/MM/dd HH:mm:ss||yyyy/MM/dd], and timestamp number with locale []
    at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:562)
    at org.elasticsearch.index.mapper.core.DateFieldMapper.innerParseCreateField(DateFieldMapper.java:490)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:215)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:408)
    ... 12 more
Caused by: org.elasticsearch.common.joda.time.IllegalFieldValueException: Cannot parse "5/13/14": Value 13 for monthOfYear must be in the range [1,12]
    at org.elasticsearch.common.joda.time.field.FieldUtils.verifyValueBounds(FieldUtils.java:218)
    at org.elasticsearch.common.joda.time.chrono.BasicMonthOfYearDateTimeField.set(BasicMonthOfYearDateTimeField.java:299)
    at org.elasticsearch.common.joda.time.format.DateTimeParserBucket$SavedField.set(DateTimeParserBucket.java:483)
    at org.elasticsearch.common.joda.time.format.DateTimeParserBucket.computeMillis(DateTimeParserBucket.java:366)
    at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:749)
    at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:556)
    ... 15 more
</description><key id="33417257">6156</key><summary>Failed to parse [date]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blackfirekitsune</reporter><labels /><created>2014-05-13T16:51:21Z</created><updated>2014-05-13T17:43:47Z</updated><resolved>2014-05-13T17:43:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-13T17:43:47Z" id="42987755">Hi @blackfirekitsune 

Elasticsearch is misinterpreting your dates, the first time that it sees them, because:

1) they don't have 4 digit years, and
2) they are in US format

See https://github.com/elasticsearch/elasticsearch/issues/6158

I suggest that you either delete that field in logstash (you already have the `@timestamp`)  or you specify the expected date `format` for that field in the mapping.

Closing this in favour of #6158
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Function Score: Add missing whitespace in error message when throwing exception</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6155</link><project id="" key="" /><description>DecayFunctionParser throws a parse exception with a string containing "scaleand origin", this fixes the spacing issue.
</description><key id="33417171">6155</key><summary>Function Score: Add missing whitespace in error message when throwing exception</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">skyebook</reporter><labels><label>:Exceptions</label><label>enhancement</label><label>v1.4.0.Beta1</label><label>v2.0.0-beta1</label></labels><created>2014-05-13T16:50:11Z</created><updated>2015-06-07T13:30:00Z</updated><resolved>2014-08-01T10:44:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-06-05T08:19:17Z" id="45192866">Hi @skyebook thanks for the fix, could you please sign our CLA so we can merge it in? http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="skyebook" created="2014-07-28T20:07:59Z" id="50391767">Hi @javanna was going through old mail and saw that I never did this, sorry about that.  Just signed the CLA, should be good to go now.
</comment><comment author="javanna" created="2014-08-01T10:44:50Z" id="50871486">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GET document is ignoring the TTL value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6154</link><project id="" key="" /><description>When enabling TTL using a mapping and publishing docs to the index,  TTL is "ignored" when using GET/id to retrieve it. The TTL value is "only" used by the purger thread, thus setting a value for TTL lower than the configured "indices.ttl.interval" doesn't make sense. "indices.ttl.interval" controls the interval the purge thread is ran. 

Documentation states that TTL can be configured in milliseconds (its the default actually). I believe it doesn't make sense to configure the purge interval to run every x milliseconds.

My 2 cents:
- 1) Keep running the purge thread interval like it is. But honour the TTL value when a doc is retrieved (i.e. NOT return it). 
- 2) If you set a TTL, it SHOULD be honoured at all times, irregardless of purge threads running. You will get unexpected results otherwise. 

Sander
</description><key id="33406915">6154</key><summary>GET document is ignoring the TTL value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sspilleman</reporter><labels /><created>2014-05-13T15:04:58Z</created><updated>2014-12-30T18:35:51Z</updated><resolved>2014-12-30T18:35:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-30T18:35:51Z" id="68383147">Hi @sspilleman 

Sorry it has taken a while to get to this.  The documentation states that expired documents are purged every 60 seconds by default (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-ttl-field.html#_note_on_documents_expiration).  Nowhere do we promise instant purging, and to do so would add expense to every CRUD and search request.

We're not going to support that as it would impact performance across the board.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation: fuzzy query's max_expansion fix</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6153</link><project id="" key="" /><description>See line org.elasticsearch.index.query.FuzzyQueryParser:70

``` java
  int maxExpansions = FuzzyQuery.defaultMaxExpansions;
```

The contents of org.apache.lucene.search.FuzzyQuery

``` java
  public final static int defaultPrefixLength = 0;
  public final static int defaultMaxExpansions = 50;
```
</description><key id="33402786">6153</key><summary>Documentation: fuzzy query's max_expansion fix</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">dnavre</reporter><labels><label>docs</label></labels><created>2014-05-13T14:24:06Z</created><updated>2014-07-16T21:45:12Z</updated><resolved>2014-06-05T17:52:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:48:52Z" id="43061713">Hi @dnavre 

Thanks for the fix. Please could I ask you to sign the CLA so that we can get your commit merged in? http://www.elasticsearch.org/contributor-agreement

thanks
</comment><comment author="dnavre" created="2014-05-14T16:34:52Z" id="43104558">I've done that already :) You should have received my signature yesterday.
</comment><comment author="clintongormley" created="2014-05-14T18:54:25Z" id="43122085">Ah thanks - it takes some time to come through.  I'll check again soon
</comment><comment author="javanna" created="2014-06-05T17:52:40Z" id="45252074">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Type is the 'doc-type', not the word 'type'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/6152</link><project id="" key="" /><description /><key id="33400406">6152</key><summary>Type is the 'doc-type', not the word 'type'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dieswaytoofast</reporter><labels /><created>2014-05-13T13:57:37Z</created><updated>2014-06-24T15:51:07Z</updated><resolved>2014-05-14T09:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-05-14T09:50:43Z" id="43061870">Merged, many thanks!
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>