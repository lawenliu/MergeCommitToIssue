<rss><channel><title /><link /><description /><language /><issue end="0" start="0" total="0" /><build-info><version /><build-number /><build-date /></build-info><item><title>Retry on conflict execution of updates *within* bulk request was off by one</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3447</link><project id="" key="" /><description>if the user set requestOnConflict to 3, we would only try again twice.

Note: this does _not_ hold for normal updates.
</description><key id="17682258">3447</key><summary>Retry on conflict execution of updates *within* bulk request was off by one</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-06T10:11:24Z</created><updated>2013-08-06T17:04:36Z</updated><resolved>2013-08-06T11:06:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added IndicesAdminClient.getTemplates()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3446</link><project id="" key="" /><description>In addition to creating and removing a template, one can now receive the template as well, even receiving multiple templates using simple regexes.

Closes #3439
</description><key id="17676346">3446</key><summary>Added IndicesAdminClient.getTemplates()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-06T07:14:40Z</created><updated>2014-07-16T21:52:42Z</updated><resolved>2013-08-12T13:03:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-12T13:02:59Z" id="22491336">Closed by https://github.com/elasticsearch/elasticsearch/commit/721e031cb6ec7aaadaaec1a524bc556b79352f86 in 0.90 and https://github.com/elasticsearch/elasticsearch/commit/45c0d1de0446dd348bc6501c63dc34652a5c9401 and https://github.com/elasticsearch/elasticsearch/commit/5c853fb22d61c90b82a387765f2165e4e6a70d0f in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not able to use Java API in JBoss EAP6 module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3445</link><project id="" key="" /><description>If I use api in simple servlet attaching elasticsearch libs in WEB-INF/lib everything is ok.

But if I create a module in JBoss with elasticsearch and give my web-app a dependency on it (via jboss-deployment-structure.xml), it cannot load neither TransportClient, nor Node-client throwing inject errors like these:

```
org.elasticsearch.common.inject.CreationException: Guice creation errors:

1) Error injecting constructor, java.lang.NoClassDefFoundError: sun/misc/Unsafe
  at org.elasticsearch.threadpool.ThreadPool.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.threadpool.ThreadPool
Caused by: java.lang.NoClassDefFoundError: sun/misc/Unsafe
    at org.elasticsearch.common.util.concurrent.jsr166e.Striped64.getUnsafe(Striped64.java:321)
    at org.elasticsearch.common.util.concurrent.jsr166e.Striped64.&lt;clinit&gt;(Striped64.java:301)
    at org.elasticsearch.common.metrics.CounterMetric.&lt;init&gt;(CounterMetric.java:28)
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.&lt;init&gt;(EsAbortPolicy.java:30)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.&lt;init&gt;(EsThreadPoolExecutor.java:36)
    at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:292)
    at org.elasticsearch.threadpool.ThreadPool.build(ThreadPool.java:249)
    at org.elasticsearch.threadpool.ThreadPool.&lt;init&gt;(ThreadPool.java:119)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:177)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:127)
    at ru.deltasolutions.switchyard.component.es.ESHandler.doStart(ESHandler.java:117)
    at org.switchyard.deploy.BaseServiceHandler.start(BaseServiceHandler.java:60)
    at org.switchyard.deploy.internal.Deployment.deployReferenceBindings(Deployment.java:309)
    at org.switchyard.deploy.internal.Deployment.start(Deployment.java:141)
    at org.switchyard.as7.extension.deployment.SwitchYardDeployment.start(SwitchYardDeployment.java:101)
    at org.switchyard.as7.extension.services.SwitchYardService.start(SwitchYardService.java:73)
    at org.jboss.msc.service.ServiceControllerImpl$StartTask.startService(ServiceControllerImpl.java:1811)
    at org.jboss.msc.service.ServiceControllerImpl$StartTask.run(ServiceControllerImpl.java:1746)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ClassNotFoundException: sun.misc.Unsafe from [Module "org.elasticsearch:main" from local module loader @49404e39 (finder: local module finder @1ccfa5c1 (roots: C:\Java\jboss-eap-6.1\modules,C:\Java\jboss-eap-6.1\modules\system\layers\soa,C:\Java\jboss-eap-6.1\modules\system\layers\ds,C:\Java\jboss-eap-6.1\modules\system\layers\base))]
    at org.jboss.modules.ModuleClassLoader.findClass(ModuleClassLoader.java:196)
    at org.jboss.modules.ConcurrentClassLoader.performLoadClassUnchecked(ConcurrentClassLoader.java:444)
    at org.jboss.modules.ConcurrentClassLoader.performLoadClassChecked(ConcurrentClassLoader.java:432)
    at org.jboss.modules.ConcurrentClassLoader.performLoadClass(ConcurrentClassLoader.java:374)
    at org.jboss.modules.ConcurrentClassLoader.loadClass(ConcurrentClassLoader.java:119)
    ... 42 more

2) Error injecting constructor, java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.util.concurrent.jsr166e.LongAdder
  at org.elasticsearch.threadpool.ThreadPool.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.threadpool.ThreadPool
    for parameter 1 at org.elasticsearch.transport.netty.NettyTransport.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.transport.netty.NettyTransport
  while locating org.elasticsearch.transport.Transport
    for parameter 1 at org.elasticsearch.transport.TransportService.&lt;init&gt;(Unknown Source)
  while locating org.elasticsearch.transport.TransportService
Caused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.util.concurrent.jsr166e.LongAdder
    at org.elasticsearch.common.metrics.CounterMetric.&lt;init&gt;(CounterMetric.java:28)
    at org.elasticsearch.common.util.concurrent.EsAbortPolicy.&lt;init&gt;(EsAbortPolicy.java:30)
    at org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.&lt;init&gt;(EsThreadPoolExecutor.java:36)
    at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:292)
    at org.elasticsearch.threadpool.ThreadPool.build(ThreadPool.java:249)
    at org.elasticsearch.threadpool.ThreadPool.&lt;init&gt;(ThreadPool.java:119)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
    at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)
    at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
    at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:177)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:127)
    at ru.deltasolutions.switchyard.component.es.ESHandler.doStart(ESHandler.java:117)
    at org.switchyard.deploy.BaseServiceHandler.start(BaseServiceHandler.java:60)
    at org.switchyard.deploy.internal.Deployment.deployReferenceBindings(Deployment.java:309)
    at org.switchyard.deploy.internal.Deployment.start(Deployment.java:141)
    at org.switchyard.as7.extension.deployment.SwitchYardDeployment.start(SwitchYardDeployment.java:101)
    at org.switchyard.as7.extension.services.SwitchYardService.start(SwitchYardService.java:73)
    at org.jboss.msc.service.ServiceControllerImpl$StartTask.startService(ServiceControllerImpl.java:1811)
    at org.jboss.msc.service.ServiceControllerImpl$StartTask.run(ServiceControllerImpl.java:1746)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)

```

and so on up to 56 errors in TransportClient and up to over 2000 errors in Node client.

As I see, that's because of elasticsearch depends on special (or simple?) classloading mechanism. JBoss AS uses module classloaders and deployment classloader is different from ES module classloader.

It's a pity, but my deployment format denies me from including ES jars into the deployment, they only could be a module.

The last resort is to use Jest to communicate to ES cluster, but it seems to be not fast enough to perform at needed transaction rate...
</description><key id="17648671">3445</key><summary>Not able to use Java API in JBoss EAP6 module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">synclpz</reporter><labels><label>docs</label></labels><created>2013-08-05T18:02:45Z</created><updated>2014-12-03T16:03:44Z</updated><resolved>2014-12-03T16:03:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-09T12:39:10Z" id="22391470">I am not a jboss user, but judging from the exception, somehow your jboss application is not able to load sun.misc.Unsafe, which is located in the `rt.jar` from your JDK. Maybe you need to include that in your jboss application as well?
</comment><comment author="synclpz" created="2013-08-11T12:16:53Z" id="22456823">Sorry, you are wrong. The module which contains elasticsearch jars cannot find many classes, not just sun.misc.Unsafe. Seems like it is general problem in managed classloading environments.

Also, I need to state that "user-space" java applications (I thought ES is such one) should not ever depend on sun.misc.Unsafe, cause it is JVM internal wrapper for native system functions such is pointers, memory allocation, CAS and so on.

Concerning your statement about rt.jar not included in jboss classpath - it also can't be true, cause JBoss cannot start without jvm root classes.

I think the problem is that elasticsearch jar is full of built-in "dependencies" like built-in Guice, Inject, JSRs and so on. May be placing such dependencies in custom packages and packing them into single jar of elasticsearch is the wrong way for managed environments such as JBoss modules or OSGi.

If you google "org.elasticsearch.common.inject.CreationException: Guice creation errors" you will see some posts concerning managed class loading...

If you need help, I could provide you enviroment and instructions with examples.

At the moment we switched to Jest client instead of joining the cluster. Performance is good enough, but I think we can gain more with native interface,
</comment><comment author="spinscale" created="2013-08-12T08:49:09Z" id="22480110">Is there any kind of 'starting point' what kind of classes cannot be found? Can you make a list of these classes, so we can try to understand more?

Regarding the usage of `sun.misc.Unsafe`: Sometimes you need to use those classes, if you need more performance. And in this case, we do.

Not sure about how JBoss handles this nowadays, but isnt JBoss creating a new class loader for every eap - where it is free to exclude the `rt.jar` dependency?

All the built-in dependencies (like the @Inject annocation you are referring to) have been moved to a different location into their own package (like `org.elasticsearch.common.inject`) to make sure they do not collide with other jars which might be in your class path.

I am not sure how to proceed here. Do you see this as an elasticsearch problem, or is it rather a Guice problem inside of a JBoss container? As I do not have any JBoss setup lying around, having a shell script to reproduce this might be a great idea - I really dont want to take half a day to get JBoss up and running (sorry, past experiences with oooold versions).
</comment><comment author="ehclark" created="2013-09-02T19:48:13Z" id="23676089">This is a JBoss modules classloading problem.  Loading sun.misc.Unsafe is one problem.  A second is that Elastic Search extends Lucene classes and accesses protected methods.  If you put the Lucene and Elastic Search jars in different JBoss modules it won't work.  Here is my module.xml for elastic search.  I have it working using the Java client API in Jboss 7.1.1

```
&lt;?xml version="1.0" encoding="UTF-8"?&gt;

&lt;!--
  ~ JBoss, Home of Professional Open Source.
  ~ Copyright 2010, Red Hat, Inc., and individual contributors
  ~ as indicated by the @author tags. See the copyright.txt file in the
  ~ distribution for a full listing of individual contributors.
  ~
  ~ This is free software; you can redistribute it and/or modify it
  ~ under the terms of the GNU Lesser General Public License as
  ~ published by the Free Software Foundation; either version 2.1 of
  ~ the License, or (at your option) any later version.
  ~
  ~ This software is distributed in the hope that it will be useful,
  ~ but WITHOUT ANY WARRANTY; without even the implied warranty of
  ~ MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
  ~ Lesser General Public License for more details.
  ~
  ~ You should have received a copy of the GNU Lesser General Public
  ~ License along with this software; if not, write to the Free
  ~ Software Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
  ~ 02110-1301 USA, or see the FSF site: http://www.fsf.org.
  --&gt;

&lt;module xmlns="urn:jboss:module:1.1" name="org.elasticsearch"&gt;
  &lt;resources&gt;
    &lt;resource-root path="elasticsearch-0.90.3.jar"/&gt;
    &lt;resource-root path="lucene-analyzers-common-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-codecs-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-core-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-grouping-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-highlighter-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-join-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-memory-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-misc-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-queries-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-queryparser-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-sandbox-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-spatial-4.4.0.jar"/&gt;
    &lt;resource-root path="lucene-suggest-4.4.0.jar"/&gt;
    &lt;!-- Insert resources here --&gt;
  &lt;/resources&gt;

  &lt;dependencies&gt;
    &lt;module name="sun.jdk" export="true" &gt;
        &lt;imports&gt;
            &lt;include path="sun/misc/Unsafe" /&gt;
        &lt;/imports&gt;
    &lt;/module&gt;
    &lt;module name="org.apache.log4j"/&gt;
    &lt;module name="org.apache.commons.logging"/&gt;
    &lt;module name="javax.api"/&gt;
  &lt;/dependencies&gt;
&lt;/module&gt;
```
</comment><comment author="synclpz" created="2013-09-03T08:21:52Z" id="23696580">Thanks much!

I did not try with lucene + ES together in one module.
So, the bug should be closed and your module.xml plus comments should be added to ES docs!
</comment><comment author="spinscale" created="2013-09-13T07:39:46Z" id="24378187">@ehclark thanks a lot for the help!

I will include this in our documentation to help others. Anything else you might want to add, so it is easier for other users to understand this?
</comment><comment author="clintongormley" created="2014-08-08T13:51:26Z" id="51603487">@dadoonet please could you take a look at adding this (or an explanation) to the java docs somewhere.
</comment><comment author="dadoonet" created="2014-12-03T16:03:44Z" id="65434263">Closing in favor of PR #8766
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using parent property in update API with doc_as_upsert=true does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3444</link><project id="" key="" /><description>If I try to add a document using the bulk update API (doc_as_upsert=true) and set its parent property, I do not see the document as the child of the specified parent when I make a has_parent query. Can you please help see what could be wrong with the below. Thanks in advance.

Following is the mapping:

mappings: {
    store: {
        properties: {
            name: {
                type: string
            }
            owner: {
                type: string
            }
        }
    }
    department: {
        _routing: {
            required: true
        }
        properties: {
            name: {
                type: string
            }
            numberOfProducts: {
                type: long
            }
        }
       _parent: {
            type: store
        }
    }
}

Following is the bulk input file:

{ "index" : { "_index" : "parent_child", "_type" : "store", "_id" : "store1" } }
{ "name" : "auchan", "owner" : "chris" }
{ "index" : { "_index" : "parent_child", "_type" : "department", "_id" : "department1", "parent" : "store1" } }
{ "name" : "toys", "numberOfProducts" : 150  }
{ "update" : { "_index" : "parent_child", "_type" : "department", "_id" : "department2", "parent" : "store1" } }
{"doc" : { "name" : "dolls", "numberOfProducts" : 300  }, "doc_as_upsert" : "true"}

Query - 

{
  "query": {
    "has_parent": {
      "type": "store",
      "query": {
        "match_all": {}
      }
    }
  }
}
</description><key id="17647328">3444</key><summary>Using parent property in update API with doc_as_upsert=true does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">poorvamandar</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-05T17:36:06Z</created><updated>2013-12-23T21:24:23Z</updated><resolved>2013-08-06T10:25:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-06T10:29:25Z" id="22170266">A small note: found and fixed the same issue on upserts (slightly different from doc_as_upsert case)
</comment><comment author="poorvamandar" created="2013-08-08T17:46:38Z" id="22341225">Thanks much for the quick fix!
</comment><comment author="poorvamandar" created="2013-08-08T17:53:01Z" id="22341666">Hi Luca - is there a nightly build that I can use to get this fix? Or will I have to do my own build?
</comment><comment author="javanna" created="2013-08-09T00:34:14Z" id="22368523">@poorvamandar the fix was released with 0.90.3, no need for any nightly build, just download the latest version of elasticsearch ;)
</comment><comment author="poorvamandar" created="2013-08-09T18:03:12Z" id="22412647">Ah, thanks Luca!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3443</link><project id="" key="" /><description>This is a somewhat gnarly implementation of what I describe in #3442.  It really isn't ready but I'd love some feedback on the approach before I spend any more time making it really right.

At this point I know I need to
- Write tests for it and do more research into using the spare/bytesSpare correctly.
- Move the actual analysis out of the map phase and into the reduce phase.
- Maybe let the user specify an analyzer to use or turn this off entirely.  OTOH I think using the normal analyzer sans shingle filter is more than likely what a user wants out of suggestion analysis.  It is certainly what I want.
- Undo the unnecessary import change Eclipse has made.

You can test this using the instructions in #3442 until I make real tests.

Closes #3442
</description><key id="17638999">3443</key><summary>Analyze suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels /><created>2013-08-05T14:58:47Z</created><updated>2014-07-11T19:07:25Z</updated><resolved>2013-08-06T15:03:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-08-06T15:03:29Z" id="22185100">Replacing this with something better.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight suggestions</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3442</link><project id="" key="" /><description>I think it'd be cool if the suggestion API returned the suggestions in a tokenized form so consumers could easily highlight the changed tokens.  This particular tokenization would have to ignore any shingle filters to in the analyzer because we'd end up highlighting whole changed phrases and that really isn't how users think.

I'm thinking it should work like this:

``` bash
curl -XDELETE "http://localhost:9200/test?pretty"
curl -XPOST "http://localhost:9200/test?pretty" -d '{
  "settings": {
    "index": {
      "number_of_replicas": 0
      "analysis":{
        "analyzer":{
          "suggest":{
            "type": "custom",
            "tokenizer": "standard",
            "filter": [ "standard", "lowercase", "suggest_shingle" ]
          }
        },
        "filter":{
          "suggest_shingle":{
            "type": "shingle",
            "min_shingle_size": 2,
            "max_shingle_size": 5,
            "output_unigrams": true
          }
        }
      }
    }
  }
}'
curl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d '{
  "test":{
    "foo":{
      "analyzer":"suggest"
    }
  }
}'

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true&amp;wait_for_status=yellow'

curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "foo": "I love shingles for suggestions"
}'

curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "foo": "but not for finding differences"
}'

sleep 1

curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "query": {
    "match_all": {}
  },
  "suggest": {
    "text": "findning differrences",
    "phrase":{
      "phrase":{
        "field": "foo",
        "max_errors": 5
      }
    }
  }
}'
```

The final request would return something like this:

``` json
{
  "took" : 39,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 2,
    "failed" : 3,
    "failures" : [ {
      "status" : 400,
      "reason" : "ElasticSearchIllegalArgumentException[generator field [foo] doesn't exist]"
    }, {
      "status" : 400,
      "reason" : "ElasticSearchIllegalArgumentException[generator field [foo] doesn't exist]"
    }, {
      "status" : 400,
      "reason" : "ElasticSearchIllegalArgumentException[generator field [foo] doesn't exist]"
    } ]
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test",
      "_id" : "JBXVvQv6QUCHsUAc6XGvRg",
      "_score" : 1.0, "_source" : {
  "foo": "I love shingles for suggestions"
}
    }, {
      "_index" : "test",
      "_type" : "test",
      "_id" : "VnE5ZKNNRp2V_mRKqASnYQ",
      "_score" : 1.0, "_source" : {
  "foo": "but not for finding differences"
}
    } ]
  },
  "suggest" : {
    "phrase" : [ {
      "text" : "differrences",
      "offset" : 0,
      "length" : 12,
      "options" : [ {
        "text" : "finding differences",
        "score" : 0.49144784,
        "changes" : [ {
          "text" : "findning",
          "is" : "finding",
          "from" : 0,
          "to" : 8
        }, {
          "text" : "differrences",
          "is" : "differences",
          "from" : 9,
          "to" : 21
        } ]
      }, {
        "text" : "findning differences",
        "score" : 0.3803136,
        "changes" : [ {
          "text" : "differrences",
          "is" : "differences",
          "from" : 9,
          "to" : 21
        } ]
      }, {
        "text" : "finding differrences",
        "score" : 0.37071815,
        "changes" : [ {
          "text" : "findning",
          "is" : "finding",
          "from" : 0,
          "to" : 8
        } ]
      } ]
    } ]
  }
}
```
</description><key id="17638207">3442</key><summary>Highlight suggestions</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nik9000</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-05T14:43:06Z</created><updated>2013-08-07T08:29:53Z</updated><resolved>2013-08-06T19:11:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-05T16:22:19Z" id="22117622">Hey @nik9000 thanks for opening this issue. I'm not sure if this is the right approach to go here since pure tokenization not always what you want. I think we should rather provide an API that provides a highlighted version of the corrected string or it should return plain offsets. Yet, why I think we should not plain tokenize is that if you have synonyms etc. in pre or post filters tokens will not correspond to actual corrected substrings.

The way I think we should do that is to provide pre and post tokens similar to the highlight API and return a "highlighted" version of the string. I think the big advantage of this is that you can display it directly to the user and I don't wanna overcomplicate the response format. We can also return actual offsets which will require some processing in the client which I would want to prevent. Yet, if you wanna take a look at implementing this, we are maintaining the splitted tokens until we are done and join them together. we can add a flag to `Correction` and highlight if it's a replacement?
</comment><comment author="nik9000" created="2013-08-05T17:50:50Z" id="22123637">I agree with your point about the API.  Elasticsearch should do the highlighting.
I agree with your point about synonyms but won't using the Corrections cause trouble with shingles?  If the whole search is corrected using a single shingle (which I think is likely for me) then we'd mark the whole thing as changed which isn't really useful.
</comment><comment author="s1monw" created="2013-08-05T18:09:31Z" id="22124924">&gt; I agree with your point about synonyms but won't using the Corrections cause trouble with shingles? If the whole search is corrected using a single shingle (which I think is likely for me) then we'd mark the whole thing as changed which isn't really useful.

internally we use shingles or ngrams to lookup the frequencies but those are only in the index and the bigrams / trigrams that are build from the query are build at runtime and not from the shingle filter so that each `Candidate` corresponds to a unigram. I will look closer into this tomorrow and provide some examples
</comment><comment author="s1monw" created="2013-08-06T12:08:35Z" id="22174255">Hey @nik9000 I think we can add this in a backwards compatible way by simply using a custom join function on `Correction` what we need is support for `pre_tag` &amp; `post_tag` on the request side something like 

```
"suggest": {
    "text": "findning differrences",
    "phrase":{
      "phrase":{
        "field": "foo",
        "max_errors": 5
        "highlight" : {
          "pre_tag" : "&lt;b&gt;",
          "post_tag" : "&lt;/b&gt;"
        }
      }
    }
  }
```

so if `highlight` is set we will return the result highlighted like this:

```
"options" : [ {
        "text" : "&lt;b&gt;finding differences&lt;/b&gt;",
        "score" : 0.49144784,

```

does this make sense? I think what we need here is a notion of a `boolean userInput;` in the `Candidate` class such that we can differentiate if this is actually a term that got suggested or not. We can easily make this distinction in NoisyChannelSpellChecker line 96. In Line 82 we actually check if it is a shingle or not and if so we ignore it.
</comment><comment author="nik9000" created="2013-08-06T15:03:07Z" id="22185070">@s1monw I think I'd prefer to return the unlighted version next to the highlighted version.  That way I don't have to re-strip the highlight components when building the suggestion query.

Here is an updated curl testing script:

``` bash
curl -XDELETE "http://localhost:9200/test?pretty"
curl -XPOST "http://localhost:9200/test?pretty" -d '{
  "settings": {
    "index": {
      "number_of_replicas": 0
      "analysis":{
        "analyzer":{
          "suggest":{
            "type": "custom",
            "tokenizer": "standard",
            "filter": [ "standard", "lowercase", "suggest_shingle" ]
          }
        },
        "filter":{
          "suggest_shingle":{
            "type": "shingle",
            "min_shingle_size": 2,
            "max_shingle_size": 5,
            "output_unigrams": true
          }
        }
      }
    }
  }
}'
curl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d '{
  "test":{
    "foo":{
      "analyzer":"suggest"
    }
  }
}'

curl -XGET 'http://localhost:9200/_cluster/health?pretty=true&amp;wait_for_status=yellow'

curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "foo": "I love shingles for suggestions"
}'

curl -XPOST "http://localhost:9200/test/test?pretty" -d '{
  "foo": "but not for finding differences"
}'

sleep 1

curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "query": {
    "match_all": {}
  },
  "suggest": {
    "text": "findning differrences",
    "phrase":{
      "phrase":{
        "field": "foo",
        "max_errors": 5
      }
    }
  }
}'



curl -XGET "http://localhost:9200/test/test/_search?pretty" -d '{
  "query": {
    "match_all": {}
  },
  "suggest": {
    "text": "findning differrences",
    "phrase":{
      "phrase":{
        "field": "foo",
        "max_errors": 5,
        "highlight": {
          "pre_tag": "&lt;em&gt;",
          "post_tag": "&lt;/em&gt;"
        }
      }
    }
  }
}'
```

The first doesn't have highlighting enabled so the suggest section should look exactly like a search without this feature:

``` json
  "suggest" : {
    "phrase" : [ {
      "text" : "findning differrences",
      "offset" : 0,
      "length" : 21,
      "options" : [ {
        "text" : "finding differences",
        "score" : 0.49144784
      }, {
        "text" : "findning differences",
        "score" : 0.3803136
      }, {
        "text" : "finding differrences",
        "score" : 0.37071815
      } ]
    } ]
```

and the second search should have highlighting like this:

``` json
  "suggest" : {
    "phrase" : [ {
      "text" : "findning differrences",
      "offset" : 0,
      "length" : 21,
      "options" : [ {
        "text" : "finding differences",
        "highlighted" : "&lt;em&gt;finding&lt;/em&gt; &lt;em&gt;differences&lt;/em&gt;",
        "score" : 0.49144784
      }, {
        "text" : "findning differences",
        "highlighted" : "findning &lt;em&gt;differences&lt;/em&gt;",
        "score" : 0.3803136
      }, {
        "text" : "finding differrences",
        "highlighted" : "&lt;em&gt;finding&lt;/em&gt; differrences",
        "score" : 0.37071815
      } ]
    } ]
  }
```
</comment><comment author="s1monw" created="2013-08-06T15:08:41Z" id="22185553">oh that is an very good reason for having it side by side to the original suggestion....! agreed!
</comment><comment author="s1monw" created="2013-08-06T19:03:57Z" id="22202752">@nik9000 I changed your commit message a bit and running tests now. I will push this very shortly! Unfortunately we can't add this to 0.90 at this point since it breaks the wire format. It's a pity though. But anyway thanks so much for adding this!
</comment><comment author="s1monw" created="2013-08-07T08:29:53Z" id="22236852">Backported to 0.90 with version compatibility on the transport protocol. I kept the version checks for master (1.0) for now to have the same code in master and 0.90 but that doesn't mean a major version upgrade from 0.90 to 1.0 doesn't require a full cluster restart
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Streamline Search / Broadcast (count, suggest, refresh, ...) APIs header</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3441</link><project id="" key="" /><description>Streamline the search and broadcast related APIs shards header response to have the same logic into building the header.

Partial execution is always indicated by `total != successful`. Actual failures are counted in the failed counter with corresponding failure messages in the header.
</description><key id="17629114">3441</key><summary>Streamline Search / Broadcast (count, suggest, refresh, ...) APIs header</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-05T10:52:56Z</created><updated>2013-08-05T10:55:56Z</updated><resolved>2013-08-05T10:55:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add size option to percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3440</link><project id="" key="" /><description>Add a `size` option to the percolate api in order to limit the number of matches being returned:

``` bash
curl -XGET 'localhost:9200/my-index/my-type/_percolate' -d '{
   "size" : 10,
   "doc" : {...}
}'
```

In the above request no more than 10 matches will be returned. The `count` field will still return the total number of matches the document matched with. 

The `size` option is not applicable for the count percolate api. 
</description><key id="17628082">3440</key><summary>Add size option to percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-08-05T10:23:34Z</created><updated>2013-08-07T08:27:49Z</updated><resolved>2013-08-07T08:27:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add client method to get a specific index template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3439</link><project id="" key="" /><description>As there is right now only `IndicesAdminClient.deleteTemplate()` and `IndicesAdminClient.createTemplate()`, it makes sense to add `IndicesAdminClient.getTemplates()`.

This also implies to create an API which does not need to use the cluster state.
</description><key id="17626808">3439</key><summary>Add client method to get a specific index template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-05T09:45:53Z</created><updated>2013-08-12T13:00:10Z</updated><resolved>2013-08-12T12:07:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Strange percolation behaviour</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3438</link><project id="" key="" /><description>Hi,

I am facing a weird percolation behaviour on 0.90.2 (master has a different percolation now, so I didnt try yet), which can be reproduced with this little script

```
curl -X DELETE http://localhost:9200/test
curl -X PUT http://localhost:9200/test

curl -X PUT 'http://localhost:9200/_percolator/test/myName' -d '{ "query" : {"filtered":{"query":{"match_all":{}},"filter":{"range":{"count":{"from":20}}}}}}'
curl -X POST 'http://localhost:9200/test/counter/_percolate' -d '{"doc":{ "count":19 } }'
curl -X POST 'http://localhost:9200/test/counter/_percolate' -d '{"doc":{ "count":19 } }'

curl -X PUT 'http://localhost:9200/_percolator/test/myName' -d '{ "query" : {"filtered":{"query":{"match_all":{}},"filter":{"range":{"count":{"from":20}}}}}}'
curl -X POST 'http://localhost:9200/test/counter/_percolate' -d '{"doc":{ "count":19 } }'
curl -X POST 'http://localhost:9200/test/counter/_percolate' -d '{"doc":{ "count":19 } }'

curl -X POST 'http://localhost:9200/test/counter/_percolate' -d '{"doc":{ "count":21 } }'
```

As you can see, the results for the first percolation operation are plain wrong. However when registering the same percolator a second time, everything works.

I still hope I did something very wrong, but cannot find it immediately at the moment :-)
</description><key id="17607226">3438</key><summary>Strange percolation behaviour</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-04T12:04:57Z</created><updated>2013-08-05T08:21:52Z</updated><resolved>2013-08-05T08:21:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-04T20:55:31Z" id="22078786">When the first percolate query is indexed the field `count` doesn't exists yet in any type in the test index and it is treated as a string field, which is default behaviour. When the first document with a `count` field is indexed, the mapping knows it is a number based field (long). When the second percolate query is indexed the range filter is really used as a numeric range filter, because the count field in the mapping is of a number based type, therefore the percolate queries after the second index request work as expected.

If you change the create index request into the following also the first two percolate requests should work as expected:

``` bash
curl -XPUT 'localhost:9200/test' -d '{
    "mappings" : {
        "counter" : {
            "properties" : {
                "count" : {
                    "type" : "long"
                }
            }
        }
    }
}'
```

A range filter can get parsed into two different Lucene queries:
- `TermRangeFilter` - Meant for alphabetic ranges and is used when the field in the mapping isn't a number based field.
- `NumericRangeFilter` - Does numeric ranges and is used when the field in mapping is a number based field.

When indexing documents the type can be automatically guessed based on the json type, this isn't the case for fields mentioned in the the percolate queries (would be nice though), this is a bit confusing.
</comment><comment author="spinscale" created="2013-08-05T08:21:52Z" id="22092674">This all makes sense. Something the user definately has to be aware of. Most simple workaround:

a) Create the mapping manually beforehand for all fields to percolate on
b) Index data first before creating a percolator so the mapping is already in place
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Overwriting pidfile on startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3437</link><project id="" key="" /><description>The current implementation does not overwrite, but only prepend the new PID into the pidfile.
So if the process is 4 digits long, but the file is already there with a 5 digit number, the file will contain 5 digits after the write.

Note: If the pidfile still exists this usually means, there either is already an instance running using this pidfile or the process has not finished correctly.

Closes #3425
</description><key id="17607114">3437</key><summary>Overwriting pidfile on startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-04T11:54:32Z</created><updated>2014-06-26T20:07:04Z</updated><resolved>2013-08-05T09:40:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-04T11:55:19Z" id="22070559">Minor comment to this PR - I am still thinking of the pidfile creation should be moved out of `Bootstrap` and into the node creation... needs to be discussed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added index templates REST support for HEAD and proper 404</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3436</link><project id="" key="" /><description>- Added HEAD support for index templates to find out of they exist
- Returning a 404 instead of a 200 if a GET hits on a non-existing index template

Closes #3434
</description><key id="17607109">3436</key><summary>Added index templates REST support for HEAD and proper 404</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-08-04T11:53:41Z</created><updated>2014-07-04T09:35:50Z</updated><resolved>2013-08-05T08:48:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-08-04T11:57:07Z" id="22070580">Looks good. I think the next step would be to add specific internal API to get index template (similar to what we did with mappings) without using the cluster state API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HighlightBuilder should be consistent with SearchContextHighlight.Field options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3435</link><project id="" key="" /><description>Today the HighlighBuilder doesn't expose all the options that are possible via the REST  API. We need to expose the missing options via the Java API for consistency as well.
</description><key id="17578440">3435</key><summary>HighlightBuilder should be consistent with SearchContextHighlight.Field options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-02T19:56:37Z</created><updated>2013-08-02T20:21:34Z</updated><resolved>2013-08-02T20:21:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-02T19:57:10Z" id="22031189">This PR fixes this issue https://github.com/elasticsearch/elasticsearch/pull/3275
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Current Template API is not RESTful</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3434</link><project id="" key="" /><description>The current template API does not behave RESTful
- HEAD on a template does not work
- GET on a non-existing template returns a 200 status message
</description><key id="17563973">3434</key><summary>Current Template API is not RESTful</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-02T14:49:58Z</created><updated>2013-08-05T08:48:35Z</updated><resolved>2013-08-05T08:48:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added support for readable_format parameter when printing out time and size values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3433</link><project id="" key="" /><description>The following are the API affected by this change and support now the readable_format flag (default false when not specified):
- indices segments
- indices stats
- indices status
- cluster nodes stats
- cluster nodes info

Closes #3432
</description><key id="17563711">3433</key><summary>Added support for readable_format parameter when printing out time and size values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-08-02T14:45:11Z</created><updated>2014-07-01T17:04:39Z</updated><resolved>2013-08-06T14:16:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add option to disable printing out readable size and time values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3432</link><project id="" key="" /><description>When we generate responses, mainly in cluster monitoring apis, we currently print time and size values twice, in two formats: the raw number and the readable format. Although the readable version is easy to read for human beings, it generates a lot of garbage in memory, which is something that we want to disable if not needed.

Let's then add support for a new "readable_format" parameter that allows to control whether we want to print out the readable version or not.

The plan is to keep it backward compatible on 0.90 (the default value will be true , meaning that readable values will be printed out by default). As of 1.0 we'll change the default to false, therefore in order to have readable values printed out you'll need to explicitly ask for them.
</description><key id="17563119">3432</key><summary>Add option to disable printing out readable size and time values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-02T14:32:08Z</created><updated>2013-09-05T08:55:50Z</updated><resolved>2013-08-06T14:15:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-02T19:39:42Z" id="22030283">@javanna did you close this by accident?
</comment><comment author="javanna" created="2013-08-03T07:46:59Z" id="22051389">@s1monw weird, not sure what happened, but yeah it shouldn't be closed ;)
</comment><comment author="mattweber" created="2013-08-26T21:47:31Z" id="23297791">Hey @javanna I think some of the stats might mixed around.  For example the jvm heap stats, the "heap_used" is listed in bytes and "heap_used_in_bytes" is human readable. 
</comment><comment author="rdeaton" created="2013-08-26T21:49:24Z" id="23297919">To follow up the above, here's some samples

0.90.3

```
     "mem" : {
      "heap_used" : "944.3mb",
      "heap_used_in_bytes" : 990229488,
      "heap_committed" : "3.8gb",
      "heap_committed_in_bytes" : 4151836672,
```

master with human=true

```
    "mem" : {
      "heap_used_in_bytes" : "442.1mb",
      "heap_used" : 463651520,
      "heap_committed_in_bytes" : "7.8gb",
      "heap_committed" : 8432910336,
```

master with human=false

```
    "mem" : {
      "heap_used" : 1313525704,
      "heap_committed" : 8432910336,
      "non_heap_used" : 43150512,
      "non_heap_committed" : 66523136,
```
</comment><comment author="javanna" created="2013-08-27T07:56:06Z" id="23318764">Thanks guys for pointing that out, looking into it ;)
</comment><comment author="javanna" created="2013-08-27T09:57:40Z" id="23324765">@mattweber @rdeaton thanks a lot!
It was a horrible mistake, but luckily it's fixed now :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>distributed aggregation consistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3431</link><project id="" key="" /><description>Very exciting - just what we need. One question I have is about consistency of the results given the distributed nature of the calculations. Now that ES is moving into the territory of BI it is even more important. Current facet implementation does not guarantee correct counts when ordered by count because distributed calculation and subsequent collation of the results. Will aggregation framework make any such guarantees?
Without them BI applications will suffer greatly as businessbusers must have exact and not approximate results. Total count or sum must stay the same matter how we aggregate inside a giben bucket
Thanks
Alex
</description><key id="17561558">3431</key><summary>distributed aggregation consistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-08-02T14:01:54Z</created><updated>2013-08-02T14:11:39Z</updated><resolved>2013-08-02T14:11:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-08-02T14:11:39Z" id="22008005">sorry meant to be a comment but mobile version of github got me confused
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add count percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3430</link><project id="" key="" /><description>Add a new percolate api that only returns the number of percolate queries that have matched with the document being percolated. The actual query ids are not included.

The percolate total count will be put in the `total` field and is the only result that will be returned from the dedicated count apis.

The `total` field will also be included in the already existing percolate and percolating existing document apis and are equal to the number of matches.
## Count percolate api

Request:

``` bash
curl -XGET 'localhost:9200/my-index/my-type/_percolate/count' -d '{
   "doc" : {
       "message" : "some message"
   }
}'
```

Response:

``` json
{
   ... // header
   "total" : 3
}
```
## Percolate existing document count api

Request:

``` bash
curl -XGET 'localhost:9200/my-index/my-type/1/_percolate/count'
```

Response:

``` json
{
    ... // header
   "total" : 5
}
```
</description><key id="17552210">3430</key><summary>Add count percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-08-02T09:42:07Z</created><updated>2013-08-02T10:44:00Z</updated><resolved>2013-08-02T10:30:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added support for named filters in top-level filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3429</link><project id="" key="" /><description>Closes #3097
</description><key id="17549873">3429</key><summary>Added support for named filters in top-level filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-08-02T08:36:55Z</created><updated>2014-06-18T10:42:10Z</updated><resolved>2013-08-02T15:26:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-02T15:26:13Z" id="22013118">Merged into 85b7efa08bd0c1429799641898647dd89d155102 and backported to 0.90 (83028b44fb652052c66aabbeaffce3c390ad8efa)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The ignore_indices=missing option should also work for indices queries and filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3428</link><project id="" key="" /><description>At the moment, when you add the ignore_indices=missing option, it has no effect in indices that are missing within a indices query or filter.

The same ignore settings should be used there.
</description><key id="17528250">3428</key><summary>The ignore_indices=missing option should also work for indices queries and filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">folke</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-08-01T20:07:38Z</created><updated>2013-12-11T21:30:05Z</updated><resolved>2013-12-11T21:30:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-01T22:10:52Z" id="21973670">@folke are you referring to the search api? I just checked and ignore_indices is supported there, so maybe I misunderstood. Could you please elaborate a little more on what behaviour you currently see and what you would like to see instead?
</comment><comment author="kimchy" created="2013-08-01T22:12:27Z" id="21973759">I think it refersn to the `indices` query/ filter we have in our query DSL. I prefer to have that option settable in the actual indices query/filter, and one can pick and choose what to do there.
</comment><comment author="folke" created="2013-08-01T22:45:16Z" id="21975447">@kimchy that's indeed what I was refering to and would be great if that would be settable there.
</comment><comment author="javanna" created="2013-09-05T08:01:15Z" id="23849721">It looks like we use the ignore missing option by default in the indices query/filter, thus I'm not sure if we should add it as an option. The problem is that you get IndexMissingException anyway when all specified indices (within the query) are missing, according to our usual ignore_indices=missing contract.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search on a shard group while relocation final flip happens might fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3427</link><project id="" key="" /><description>At the final stage of a relocation, during the final flip of the states, a search request might hit a node that would then execute it on a shard that has already relocated. 

For this, we need to execute broadcast and search operations against initializing shards as well, but only as a last resort. The operation will be rejected if not applicable (i.e. `IndexShard#searcher()` checked for read allowed).

Note, this requires careful though about which failures we send back. If we try and initializing shard and it fails, its failure should not override an actual failure of an active shard.

Also, removed an atomic integer used in broadcast request and use a similar shard index trick we now have in our search execution.
</description><key id="17517069">3427</key><summary>Search on a shard group while relocation final flip happens might fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-01T16:31:08Z</created><updated>2013-08-01T16:37:18Z</updated><resolved>2013-08-01T16:37:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Native custom score script: performance bottleneck in getting array fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3426</link><project id="" key="" /><description>For my searches, I use a native script custom score query which processes array (multivalued) fields from the indexed documents.

To obtain them, it gets the
- org.elasticsearch.index.fielddata.ScriptDocValues.Doubles
- org.elasticsearch.index.fielddata.ScriptDocValues.Longs
  objects from doc() --- which I understand to be the most efficient way of obtaining them (as they are cached in memory).

When profiling, however, it appears that the script spends a very large amount of time in the Doubles.getValues() and Longs.getValues() methods. While the arrays themselves are not enormous (should only rarely exceed 200 entries), repeating the operation for all documents in the index proves to be the bottleneck in custom scoring.

Is there a way of getting around this bottleneck? Does it matter that the getValues methods construct the output list by extending the underlying array (and copying the memory?) every time a new value is added?

(Using 0.90.2)
</description><key id="17513439">3426</key><summary>Native custom score script: performance bottleneck in getting array fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">maciejkula</reporter><labels /><created>2013-08-01T15:27:17Z</created><updated>2014-11-14T16:49:11Z</updated><resolved>2013-09-14T06:55:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="wkasai" created="2013-09-11T09:45:03Z" id="24224828">I met the same problem ( in 0.90.3 ), I tried another way, source().extractRawValues(...), which is able to get the array data

In this case, org.apache.lucene.codecs.compressing.LZ4.decompress() takes much time in spite of getValueByOrd(). 
</comment><comment author="jpountz" created="2013-09-11T09:55:27Z" id="24225371">@wkasai _source is optimized for getting all values of a few documents (to display summaries in responses for example) which explains why it can be slow for scoring, since you need to get only a few field values for many documents. This is why doc() is the recommended way of accessing field values in search scripts.
</comment><comment author="wkasai" created="2013-09-11T10:05:20Z" id="24226048">@jpountz Thanks you for your reply. I will use doc().get for my purpose. So my concern is now the same with maciejkula's. 
</comment><comment author="dadoonet" created="2013-09-14T06:55:15Z" id="24438247">I ran a performance test on 600k docs (means **16.185.000 Lucene docs** when you consider nested docs).
I ran about **4.945.000** calls to `org.elasticsearch.index.fielddata.ScriptDocValues$Doubles.getValues()` with a total time of **138.169ms**. It means to me somehow that one call is about **0.027 ms**.

Looking at the source code we really load that in cache.

We could potentially expose more methods to optimize not using a List&lt;Double&gt; but use our optimized iterator, but we still unsure by how much it will really help.

Closing for now but feel free to reopen or add a new one.
</comment><comment author="wkasai" created="2013-09-15T11:21:55Z" id="24469710">I reviewed inner code, finally agreed reluctantly to using iterator in this situation, Lucene seems to provide block data access step by step.

http://svn.apache.org/repos/asf/lucene/dev/tags/lucene_solr_4_4_0/lucene/core/src/java/org/apache/lucene/util/packed/AppendingLongBuffer.java

I attemped to provide arbitrary period (including past) "trend sort" by counting up the ordered long arrays (time of some event occurrence for a document) with binary search. This approach requires direct index-base access to data in list. 

I hope elasticsearch will supply such a sort option on the future, because it seems to be useful in many real services. 
</comment><comment author="ghiron" created="2014-11-14T10:47:30Z" id="63041627">Hi guys ,
I have the same problem here (ES 1.3.3)

I need to get some "large" Long arrays (btw 300 and 1000 items).

My benchmark on my own data set (~18K documents)
First run (so, items are not in cache) : 
- ScriptDocValues.Longs longs = docLookup.get("path") : 22 510 ns / item
- longs.getValues() : 3 576 ns / item

Next runs (items are in cache) : 
- ScriptDocValues.Longs longs = docLookup.get("path") : 52 ns
- longs.getValues() : 2 401 ns

So cache system works fine but iteration to return list is slow :-(

My full scoring script spend half of execution time on this specific piece of code. It's even slower than distance calculation.

Any idea / work in progress to fix this issue ?

Thanks
</comment><comment author="maciejkula" created="2014-11-14T11:00:22Z" id="63043241">I know that in pre 1.0 versions reading list fields reallocated memory for the array with every element (so reading n elements required n reallocations), but I think that has been fixed since.

One thing I've found helpful is accessing the underlying array storage of the `SlicedLongList` object you get from `getValues`. This helps if you iterate over your long array: https://github.com/elasticsearch/elasticsearch/blob/1816951b6b0320e7a011436c7c7519ec2bfabc6e/src/main/java/org/elasticsearch/common/util/SlicedLongList.java

One further possibility is to access the `internalValues` of `ScriptDocValues.Longs` directly, without building an intermediate `List` on `getValues`, but I haven't had much success with that.
</comment><comment author="jpountz" created="2014-11-14T14:49:13Z" id="63074078">@maciejkula The trade-off is that script doc values are a bit slow but are easy to interface with scripts and have a more stable API. If you have needs for better performance, you could try to talk directly to fielddata (which is where script doc values get their data), but I would like to warn you that the API is harder to use and that we reserve ourselves the right to change it anytime (eg. it happened in 1.4 in order to improve doc values support).
</comment><comment author="ghiron" created="2014-11-14T15:25:02Z" id="63079309">@maciejkula
I guess I missed something in your explanation because getValues method is slow so even if I use SlicedLongList object it's still slow ?

@jpountz 
How can I use fielddata ? Do you have example ?

thanks
</comment><comment author="maciejkula" created="2014-11-14T15:43:30Z" id="63082332">Yes, getting the data is still slow. But if (like me) you do a lot of computation on the data afterwards there is some benefit from accessing the underlying array directly.

Sorry, I don't have an example, I don't think I ever managed to use that successfully.
</comment><comment author="ghiron" created="2014-11-14T15:50:27Z" id="63083540">@maciejkula : sorry bad alias on my last comment about example
I need an equivalent to [CollectionUtils.containsAny](https://commons.apache.org/proper/commons-collections/javadocs/api-3.2.1/org/apache/commons/collections/CollectionUtils.html#containsAny%28java.util.Collection, java.util.Collection%29) function : that means one single access to all this field data and on all documents.
</comment><comment author="jpountz" created="2014-11-14T16:36:52Z" id="63091350">@ghiron You should be able to have access to it through DocLookup. Then you can look at eg. `NumericRangeFieldDataFilter` to see how it is used.

However, I'm wondering if scripts and fielddata are really what you need for your use-case. `CollectionUtils.containsAny` is typically what a filter intersection would do (but in a much more efficient way than fielddata).
</comment><comment author="ghiron" created="2014-11-14T16:48:38Z" id="63093278">@jpountz maybe I will implement this as a filter but right now it's just one of all score criteria. Use filter instead will probably drop a lot of "matching" (high score) documents.
Anyway I'll look NumericRangeFieldDataFilter! Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>pid file not properly overwritten</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3425</link><project id="" key="" /><description>I had an instance running on a server, and the PID file had a 5-digit process ID in it. I did a restart, and the new process ID was 4-digits, but the file sitll had the 5th digit from the prior process.

When writing the pid file, it should truncate it first if it already exists.

Running elasticsearch 0.90.0 on FreeBSD.

Thanks!
</description><key id="17511656">3425</key><summary>pid file not properly overwritten</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">khera</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-08-01T14:56:37Z</created><updated>2013-08-05T09:39:58Z</updated><resolved>2013-08-05T09:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-08-02T07:05:02Z" id="21989725">Hey,

can you please provide more information. how you did this? I just tried the following (under Mac OS):

```
bin/elasticsearch -p /tmp/pid
sleep 10 &amp;&amp; cat /tmp/pid
36756
bin/elasticsearch -p /tmp/pid
sleep 10 &amp;&amp; cat /tmp/pid
36792
```

In my case the truncation works flawlessly (there is no check in elasticsearch if the pid file already exists). Anything you did different with the exception of your operating system? I am wondering if the sigar library is completely supported under freebsd, but judging from the official documentation it looks like it.
</comment><comment author="khera" created="2013-08-02T16:29:27Z" id="22017252">Your example shows the PID both of length 5.

Try this:

echo aaaaaaaaaa &gt; /tmp/pid

then run your test.

Here is what it looks like to me:

[root@logger]# /usr/local/etc/rc.d/elasticsearch start
Starting elasticsearch.
[root@logger]# cat /var/run/elasticsearch.pid
40757[root@logger]#
[root@logger]# /usr/local/etc/rc.d/elasticsearch stop
Stopping elasticsearch.
[root@logger]# echo aaaaaaaaaa &gt; /var/run/elasticsearch.pid
[root@logger]# cat /var/run/elasticsearch.pid
aaaaaaaaaa
[root@logger]# /usr/local/etc/rc.d/elasticsearch start
Starting elasticsearch.
[root@logger]# cat /var/run/elasticsearch.pid
75686aaaaa

Basically, if the pid assigned to elastic search is 5 digits long, and then
after a restart, the PID becomes 3 or 4 digits long, whatever was leftover
will still be there.

So if the first PID is 12345 and the second pid is 4321, what will be in
the file is 43215.

On FreeBSD, the PIDs are not necessarily monotonically increasing, so this
has actually happened to me, and is not just a theoretical problem. I'm not
so familiar with Java, but in C or Perl, I'd add the O_TRUNC flag to the
file open.

Thanks!

On Fri, Aug 2, 2013 at 3:05 AM, Alexander Reelsen
notifications@github.comwrote:

&gt; Hey,
&gt; 
&gt; can you please provide more information. how you did this? I just tried
&gt; the following (under Mac OS):
&gt; 
&gt; bin/elasticsearch -p /tmp/pid
&gt; sleep 10 &amp;&amp; cat /tmp/pid
&gt; 36756
&gt; bin/elasticsearch -p /tmp/pid
&gt; sleep 10 &amp;&amp; cat /tmp/pid
&gt; 36792
&gt; 
&gt; In my case the truncation works flawlessly (there is no check in
&gt; elasticsearch if the pid file already exists). Anything you did different
&gt; with the exception of your operating system? I am wondering if the sigar
&gt; library is completely supported under freebsd, but judging from the
&gt; official documentation it looks like it.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3425#issuecomment-21989725
&gt; .
</comment><comment author="spinscale" created="2013-08-03T14:09:58Z" id="22055419">sorry, I got you wrong. Will fix it.

Thanks for your patience to report!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Does Aggregation solve my use case?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3424</link><project id="" key="" /><description>Could you please comment if aggregation will solve my use case?

If I have documents with these values.

_source: {date:    01.01.2013    desc:    XXX     value:    100}
_source: {date:    02.01.2012    desc:    XXX     value:    200}
_source: {date:    03.01.2011    desc:    XXX     value:    300}
_source: {date:    04.01.2011    desc:    YYY     value:    400}
_source: {date:    05.01.2011    desc:    YYY     value:    500}

I need to produce:

Desc       Last Date        Last Value
XXX        01.01.2013        100
YYY        05.01.2011        500

I need to get the documents with the most recent date for that desc so I can pull the value off of it.
If aggregation can not give me the document would it be possible to give the document id with the most recent date for that desc?  And I could use an id filter to get the documents. 

Thanks for your time.
</description><key id="17506183">3424</key><summary>Does Aggregation solve my use case?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rmattler</reporter><labels /><created>2013-08-01T13:10:39Z</created><updated>2013-08-01T22:39:11Z</updated><resolved>2013-08-01T22:39:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-01T22:39:11Z" id="21975165">Would you mind sending your question to the [elasticsearch users group](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch)? Github issues are usually bugs, improvements or feature requests.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>function score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3423</link><project id="" key="" /><description># Function score

&lt;code&gt;function_score&lt;/code&gt; allows to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents.

&lt;code&gt;function_score&lt;/code&gt; provides the same functionality that &lt;code&gt;custom_boost_factor&lt;/code&gt;, &lt;code&gt;custom_score&lt;/code&gt; and &lt;code&gt;custom_filters_score&lt;/code&gt; provided but furthermore adds the option to score a document depending on the distance of a numeric field value from a user given reference (see description below).
## Using function score

&lt;code&gt;function_score&lt;/code&gt; can be used with only one function like this:

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "FUNCTION": {}
} 
```

Furthermore, several functions can be combined. In this case one can optionally choose to apply the function only if a document matches a given filter:

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "functions": [
        {
            "filter": {},
            "FUNCTION": {}
        },
        {
            "FUNCTION": {}
        }
    ],
    "score_mode": "(mult|max|...)"
}
```

If no filter is given with a function this is equivalent to specifying &lt;code&gt;"match_all": {}&lt;/code&gt;

&lt;code&gt;score_mode&lt;/code&gt; defines how functions are combined before multiplying to the score of the query:
- "multiply":  all functions are multiplied 
-  "total":  functions are summed 
-  "avg":  average of functions is computed
-  "first": the first function that has a matching filter with it is applied
-  "max": the function yielding the maximum score is applied 
-   "min": the function yielding the minimum score is applied 

The default is &lt;code&gt;"multiply"&lt;/code&gt;.
# Score functions

&lt;code&gt;function_score&lt;/code&gt; provides three types of score functions.
## Script score

The &lt;code&gt;script_score&lt;/code&gt; function allows to wrap another query and customize the scoring of it optionally with a computation derived from other field values in the doc (numeric ones) using script expression. Here is a simple sample:

```
"script_score" : {
    "script" : "_score * doc['my_numeric_field'].value"
}
```

On top of the different scripting field values and expression, the &lt;code&gt;_score&lt;/code&gt; script parameter can be used to retrieve the score based on the wrapped query.

Scripts are cached for faster execution. If the script has parameters that it needs to take into account, it is preferable to use the same script, and provide parameters to it:

```
"script_score": {
    "lang": "lang",
    "params": {
        "param1": value1,
        "param2": value2
     },
    "script": "_score * doc['my_numeric_field'].value / pow(param1, param2)"
}
```
## Boost factor

The &lt;code&gt;boost_factor&lt;/code&gt; score allows to multiply the score by the provided &lt;code&gt;boost_factor&lt;/code&gt;. This can sometimes be desired since boost value set on specific queries gets normalized, while for this score function it does not.

```
"boost_factor" : number
```
## Decay functions

Decay functions score a document with a function that decays depending on the distance of a numeric field value of the document from a user given reference. This is similar to a range query, but with smooth edges instead of boxes.

To use distance scoring on a query that has numerical fields, the user has to define 
1. a reference and
2. a scale

for each field. A reference is needed to define a distance for the document and a scale to define the rate of decay. The decay function is specified by

```
"DECAY_FUNCTION": {
    "FIELD_NAME": {
          "reference": "11, 12",
          "scale": "2km"
    }
}
```

where &lt;code&gt;DECAY_FUNCTION&lt;/code&gt; can be "linear", "exp" and "gauss".
### Normal decay, keyword "gauss"

The score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\exp(-\frac{(fieldvalue_{doc}-reference)^2}{2scale^2})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\exp(-\frac{(fieldvalue_{doc}-reference)^2}{2scale^2})" title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;
### Exponential decay, keyword "exp"

The score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\exp(-\frac{abs(x-\mu)^2}{\sigma})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\exp(-\frac{|fieldvalue_{doc}-reference|}{scale})" title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;
### 'Linear' decay, keyword "linear"

The score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\frac{abs(x-\mu)^2}{\sigma})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\max(\frac{scale-|fieldvalue_{doc}-reference|}{scale},0)" title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

In contrast to the normal and exponential decay, this function actually sets the score to 0 if the field value exceeds the user given scale value.
### Choosing an appropriate scale

For all three functions, it might not always be easy to define an appropriate scale. Rather than defining the scale parameter directly, one can optionally define a distance at which the function should compute a particular factor.

For example, your documents might represents hotels and contain a geo location field. You want to compute a decay function depending on how far the hotel is from a given location. You might not immediately see what scale to choose for the gauss function, but you can say something like: "At a distance of 2km from the desired location, the score should be reduced by half."
You can provide this parameter like this:

```
  "DECAY_FUNCTION": {
    "location": {
          "reference": "11, 12",
          "scale": "2km",
          "scale_weight" : 0.5
    }
}
```

The parameter "scale" will then be adjusted automatically to assure that the score function computes a score of 0.5 for hotels that are 2km away from the desired location.
### Detailed example

Suppose you are searching for a hotel in a certain town. Your budget is limited. Also, you would like the hotel to be close to the town center, so the farther the hotel is from the desired location the less likely you are to check in.
You would like the query results that match your criterion (for example, "hotel, Nancy, non-smoker") to be scored with respect to distance to the town center and also the price. 

Intuitively, you would like to define the town center as the origin and maybe you are willing to walk 2km to the town center from the hotel.
In this case your _reference_ for the location field is the town center and the _scale_ is ~2km.

If your budget is low, you would probably prefer something cheap above something expensive. 
For the price field, the _reference_ would be 0 Euros and the _scale_ depends on how much you are willing to pay, for example 20 Euros. 

In this example, the fields might be called "price" for the price of the hotel and "location" for the coordinates of this hotel. 

The function for "price" in this case would be 

```
"DECAY_FUNCTION": {
    "price": {
          "reference": "0",
          "scale": "20"
    }
}
```

and for "location"

```
"DECAY_FUNCTION": {
    "location": {
          "reference": "11, 12",
          "scale": "2km"
    }
}
```

where &lt;code&gt;DECAY_FUNCTION&lt;/code&gt; can be "linear", "exp" and "gauss".

Suppose you want to multiply these two functions on the original score, the request would look like this:

```
curl 'localhost:9200/hotels/_search/' -d '{
"query": {
    "function_score": {
        "functions": [
            {
                "DECAY_FUNCTION": {
                    "price": {
                        "reference": "0",
                        "scale": "20"
                    }
                }
            },
            {
                "DECAY_FUNCTION": {
                    "location": {
                        "reference": "11, 12",
                        "scale": "2km"
                    }
                }
            }
        ],
        "query": {
            "match": {
                "properties": "balcony"
            }
        },
        "score_mode": "multiply"
    }
}
}'
```

Next, we show how the computed score looks like for each of the three possible decay functions.
#### Normal decay, keyword "gauss"

When choosing "gauss" as decay function in the above example, the multiplier to the original score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\mathcal{S}(doc)=\exp(-\frac{(location_{doc}-reference_{loc})^2}{2scale_{loc}})\exp(-\frac{(price_{doc}-reference_{price})^2}{2scale_{price}})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\exp(-\frac{(location_{doc}-reference_{loc})^2}{2scale_{loc}})\exp(-\frac{(price_{doc}-reference_{price})^2}{2scale_{price}})," title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

A contour and surface plot of the multiplier looks like this:

![gausscontour](https://f.cloud.github.com/assets/4320215/768157/cd0e18a6-e898-11e2-9b3c-f0145078bd6f.png)
![gausssurf](https://f.cloud.github.com/assets/4320215/768160/ec43c928-e898-11e2-8e0d-f3c4519dbd89.png)

Suppose your original search results matches three hotels : "Backback Nap", "Drink n Drive" and "BnB Bellevue". 
"Drink n Drive" is pretty far from your defined location (nearly 2 km) and is not too cheap (about 13 Euros) so it gets a low factor a factor of 0.56. "BnB Bellevue" and "Backback Nap" are both pretty close to the defined location but "BnB Bellevue" is cheaper, so it gets a multiplier of 0.86 whereas "Backpack Nap" gets a value of 0.66."
### Exponential decay, keyword "exp"

When choosing "exp" as decay function in the above example, the multiplier to the original score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\exp(-\frac{abs(x-\mu)^2}{\sigma})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\exp(-\frac{|location_{doc}-reference_{loc}|}{scale_{loc}})\exp(-\frac{|price_{doc}-reference_{price}|}{scale_{price}})," title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

A contour and surface plot of the multiplier looks like this:

![expcontour](https://f.cloud.github.com/assets/4320215/768161/082975c0-e899-11e2-86f7-174c3a729d64.png)
![expsurf](https://f.cloud.github.com/assets/4320215/768162/0b606884-e899-11e2-907b-aefc77eefef6.png)
### 'Linear' decay, keyword "linear"

When choosing "exp" as decay function in the above example, the multiplier to the original score is computed as
&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\frac{abs(x-\mu)^2}{\sigma})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\max(\frac{scale_{loc}-|location_{doc}-reference_{loc}|}{scale_{loc}},0)\max(\frac{scale_{price}-|price_{doc}-reference_{price}|}{scale_{price}},0)," title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

A contour and surface plot of the multiplier looks like this:

![lincontour](https://f.cloud.github.com/assets/4320215/768164/1775b0ca-e899-11e2-9f4a-776b406305c6.png)
![linsurf](https://f.cloud.github.com/assets/4320215/768165/19d8b1aa-e899-11e2-91bc-6b0553e8d722.png)
#### Supported fields for decay functions

Only single valued numeric fields, including time and geo locations, should be supported. 
#### What is a field is missing?

Is the numeric field is missing in the document, the function will return 1.
# Relation to custom_boost_factor, custom_score and custom_filters_score

The custom boost factor query 

```
"custom_boost_factor" : {
    "query" : {
        ....
    },
    "boost_factor" : 5.2
}
```

becomes

```
"function_score" : {
    "query" : {
        ....
    },
    "boost_factor" : 5.2
}
```

The custom script score

```
"custom_score" : {
    "query" : {
        ....
    },
    "params" : {
        "param1" : 2,
        "param2" : 3.1
    },
    "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)"
}
```

 becomes

```
"function_score" : {
    "query" : {
        ....
    },
    "script_score" : {

        "params" : {
            "param1" : 2,
            "param2" : 3.1
        },
        "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)"
    }
}
```

and the custom filters score query 

```
"custom_filters_score" : {
    "query" : {
        &#8230;
     },
    "filters" : [
        {
            "filter" : { &#8230;},
            "boost" : "3"
        },
        {
            "filter" : {&#8230;},
            "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)"
        }
    ],
    "params" : {
        "param1" : 2,
        "param2" : 3.1
    }
    "score_mode" : "first"
}       
```

becomes:

```
"function_score" : {
    "query" : {
        &#8230;
    },
    "functions" : [
        {
            "filter" : {&#8230;},
            "boost" : "3"
        },
        {
            "filter" : { &#8230; },
            "script_score" : { 
                "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)",
                "params" : {
                    "param1" : 2,
                    "param2" : 3.1
                }

            }
        }
    ],
    "score_mode" : "first",     
}       
```

This issue replaces Issues #3307 and #3407 
</description><key id="17505472">3423</key><summary>function score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>feature</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-01T12:54:17Z</created><updated>2013-09-16T08:19:33Z</updated><resolved>2013-08-06T16:54:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-06T21:03:51Z" id="22211167">NICE! :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Newline in http response body</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3422</link><project id="" key="" /><description>It would be nice to have a new line in all http responses. Many examples include curl usage, but lack of newline in responses makes that not so convenient.

For example (`export PS1="$ &gt; "`):

```
$ &gt; curl -s http://web521.local:9200/blah
No handler found for uri [/blah] and method [GET]$ &gt;
```

It would be nice to see:

```
$ &gt; curl -s http://web521.local:9200/blah
No handler found for uri [/blah] and method [GET]
$ &gt;
```

Same applies to json responses, `}` at the beginning of the line looks annoying :)
</description><key id="17498188">3422</key><summary>Newline in http response body</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bobrik</reporter><labels /><created>2013-08-01T09:31:09Z</created><updated>2013-10-14T09:26:27Z</updated><resolved>2013-10-14T09:26:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-08-01T10:03:46Z" id="21926520">I often use the following notation in my own scripts:

```
$ &gt; curl -s http://web521.local:9200/blah; echo
```
</comment><comment author="spinscale" created="2013-08-05T12:25:22Z" id="22102717">Alternatively you can configure your shell to add that newline automatically. zsh supports this for example ([prezto](https://github.com/sorin-ionescu/prezto) is a nice set of tools/themes for zsh setting this for you).
</comment><comment author="nik9000" created="2013-08-07T00:47:58Z" id="22222712">It'd be useful to me if ?pretty=true http calls always ended in a newline.
</comment><comment author="javanna" created="2013-10-05T16:52:11Z" id="25752131">@bobrik from 0.90.6 you'll get a line feed when using `pretty=true`. Is that satisfying for you?
</comment><comment author="bobrik" created="2013-10-12T16:27:01Z" id="26200628">@javanna yep, sounds good :)
</comment><comment author="javanna" created="2013-10-14T09:26:27Z" id="26244626">Cool thanks for the feedback @bobrik , I'm closing this one then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin Manager should support -remove group/artifact/version naming</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3421</link><project id="" key="" /><description>When installing a plugin, we use:

``` sh
bin/plugin --install groupid/artifactid/version
```

But when removing the plugin, we only support:

``` sh
bin/plugin --remove dirname
```

where `dirname` is the directory name of the plugin under `/plugins` dir.
</description><key id="17497097">3421</key><summary>Plugin Manager should support -remove group/artifact/version naming</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-08-01T09:03:56Z</created><updated>2013-09-09T19:18:40Z</updated><resolved>2013-09-09T19:17:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve alias support in the percolate api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3420</link><project id="" key="" /><description>Support for percolating via an alias is already implemented via #3173, if an alias is resolved to more than one index, then from the response it is impossible to know what id belongs to what concrete index. This can become even a bigger issue if percolate queries with the same id exist in different indices.

For this reason the response needs to be enhanced to include the concrete index per match:

``` json
{
  "matches" : [
      {
         "id" : "1",
         "_index" : "my-index1"
      },
      ...
  ]
}
```

The current format will still be supported by setting `percolate_format=ids` parameter in the query string. Useful in the case percolation is only executed on just one index.

This issue will also add support for specifying multiple indices in the percolate request:

``` bash
curl -XGET 'localhost:9200/my-index1,my-index2/my-type/_percolate' -d '...'
```
</description><key id="17494471">3420</key><summary>Improve alias support in the percolate api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-08-01T07:47:15Z</created><updated>2013-08-01T08:50:47Z</updated><resolved>2013-08-01T08:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query/Filter Facet should support 64bit counter, not 32</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3419</link><project id="" key="" /><description>The counters for query and filter facets now use 32bit integers for counting the matches, they should be 64bit ones.
</description><key id="17473433">3419</key><summary>Query/Filter Facet should support 64bit counter, not 32</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-31T20:01:26Z</created><updated>2013-07-31T20:11:04Z</updated><resolved>2013-07-31T20:11:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>"Get" and "Multi Get" operations return incorrect `_source` fields for single-valued arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3418</link><project id="" key="" /><description>When an indexed documents contains a property indexed as an array of objects, incorrect JSON is returned when this document is fetched while using the  `fields` parameter:

``` bash
curl localhost:9250 | grep number; echo

curl -X DELETE localhost:9250/arrays

curl -X POST localhost:9250/arrays -d '{
  "mappings" : {
    "book" : {
      "properties": {
        "title" : {
          "type" : "string"
        },
        "authors" : {
          "properties" : {
            "nick": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}'

curl -X PUT localhost:9250/arrays/chat/1 -d '{
  "title" : "Test 1",
  "authors" : [ { "nick" : "john" } ]
}'

curl -X PUT localhost:9250/arrays/chat/2 -d '{
  "title" : "Test 1",
  "authors" : [ { "nick" : "john" }, { "nick" : "mary" } ]
}'

echo; echo 'GET 1'; echo

curl -XGET 'localhost:9250/arrays/chat/1?pretty&amp;fields=authors'

#  "authors" : {
#    "nick" : "john"
#  }

echo; echo 'GET 2'; echo;

curl -XGET 'localhost:9250/arrays/chat/2?pretty&amp;fields=authors'

# "authors" : [ {
#   "nick" : "john"
# }, {
#   "nick" : "mary"
# } ]

echo; echo 'MGET 1'; echo

curl -XPOST 'localhost:9250/_mget?pretty&amp;fields=authors' -d '{
  "docs" : [
    { "_index" : "arrays", "_type" : "chat", "_id" : 1 }
  ]
}'

# "authors" : {
#   "nick" : "john"
# }

echo; echo 'MGET 2'; echo;

curl -XPOST 'localhost:9250/_mget?pretty&amp;fields=authors' -d '{
  "docs" : [
    { "_index" : "arrays", "_type" : "chat", "_id" : 2 }
  ]
}'

# "authors" : [ {
#   "nick" : "john"
# }, {
#   "nick" : "mary"
# } ]

echo; echo '_source'; echo;

curl -XGET 'localhost:9250/arrays/chat/1/_source?pretty'

# {
#   "title" : "Test 1",
#   "authors" : [ { "nick" : "john" } ]
# }
```
</description><key id="17466324">3418</key><summary>"Get" and "Multi Get" operations return incorrect `_source` fields for single-valued arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">karmi</reporter><labels /><created>2013-07-31T17:54:59Z</created><updated>2013-08-01T06:18:07Z</updated><resolved>2013-08-01T06:17:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2013-08-01T06:17:40Z" id="21916930">In current master, this inconsistency is fixed by #3301. The solution is to use `source_include` instead of `fields:

``` bash
curl -X DELETE localhost:9250/arrays

curl -X POST localhost:9250/arrays -d '{
  "mappings" : {
    "book" : {
      "properties": {
        "title" : {
          "type" : "string"
        },
        "authors" : {
          "properties" : {
            "nick": {
              "type": "string"
            }
          }
        }
      }
    }
  }
}'

curl -X PUT localhost:9250/arrays/chat/1 -d '{
  "title" : "Test 1",
  "authors" : [ { "nick" : "john" } ]
}'

curl -X PUT localhost:9250/arrays/chat/2 -d '{
  "title" : "Test 1",
  "authors" : [ { "nick" : "john" }, { "nick" : "mary" } ]
}'

echo; echo 'GET 1'; echo

curl -XGET 'localhost:9250/arrays/chat/1?pretty&amp;source_include=authors'

echo; echo 'GET 2'; echo;

curl -XGET 'localhost:9250/arrays/chat/2?pretty&amp;source_include=authors'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reroute eagerly on shard started events</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3417</link><project id="" key="" /><description>We have an optimization where we try to delay reroute after we processed the shard started events to try and combine a few into the same event. With teh queueing of shard started events in places, we don't need to do it, and we can reroute right away, which will actually reduce the amount of cluster state events we send.

This will also have a nice side effect of not missing on "waitForRelocatingShards(0)" on cluster health checks since relocations will happen right away.
</description><key id="17456113">3417</key><summary>Reroute eagerly on shard started events</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-31T14:58:07Z</created><updated>2013-07-31T14:58:43Z</updated><resolved>2013-07-31T14:58:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added prefix suggestions based on AnalyzingSuggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3416</link><project id="" key="" /><description>This commit introduces near realtime suggestions. For more information about
its usage refer to github issue #3376

From the implementation point of view, a custom AnalyzingSuggester is used
in combination with a custom postingsformat (which is not exposed to the user
anywhere for him to use).

Closes #3376
</description><key id="17453839">3416</key><summary>Added prefix suggestions based on AnalyzingSuggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-07-31T14:17:58Z</created><updated>2014-06-13T15:05:05Z</updated><resolved>2013-08-01T06:56:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-07-31T14:29:02Z" id="21866628">LGTM - man I am excited! push it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Include size values for cluster state </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3415</link><project id="" key="" /><description>When getting the status using `_status`, the size of each index is given. However, the translog and cluster state's sizes aren't given. This means that we cannot really tell how much space is being occupied by the index on disk.

It would be nice if these 2 pieces of information can be included in the response for each index in `_status`, so that we can calculate the exact disk space the index occupies.
</description><key id="17446593">3415</key><summary>Include size values for cluster state </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/abeyad/following{/other_user}', u'events_url': u'https://api.github.com/users/abeyad/events{/privacy}', u'organizations_url': u'https://api.github.com/users/abeyad/orgs', u'url': u'https://api.github.com/users/abeyad', u'gists_url': u'https://api.github.com/users/abeyad/gists{/gist_id}', u'html_url': u'https://github.com/abeyad', u'subscriptions_url': u'https://api.github.com/users/abeyad/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/1631297?v=4', u'repos_url': u'https://api.github.com/users/abeyad/repos', u'received_events_url': u'https://api.github.com/users/abeyad/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/abeyad/starred{/owner}{/repo}', u'site_admin': False, u'login': u'abeyad', u'type': u'User', u'id': 1631297, u'followers_url': u'https://api.github.com/users/abeyad/followers'}</assignee><reporter username="">F21</reporter><labels><label>:Stats</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-07-31T11:29:57Z</created><updated>2017-03-02T19:20:30Z</updated><resolved>2017-03-02T19:20:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rpedela" created="2013-12-20T01:25:55Z" id="30982612">+1

I would actually like to know how much disk space is used in total including the stored data.
</comment><comment author="clintongormley" created="2014-08-08T13:46:19Z" id="51602255">The `translog` size is now available in the nodes-stats API.  
</comment><comment author="jpountz" created="2014-09-05T09:02:49Z" id="54601119">Upgrading to `adoptme`: let's provide the size of the compressed cluster state in our monitoring APIs. NOTE: this would be the size of the compressed serialized representation of the cluster state that is typically used for inter-node communication, not the amount of memory that the cluster state occupies on each node.
</comment><comment author="SahilC" created="2016-01-18T12:31:06Z" id="172516466">Hi, I'm new to ES, and I want to contribute. Can I take this up? 
</comment><comment author="clintongormley" created="2016-01-18T13:47:52Z" id="172530853">Hi @SahilC 

Yes, anything labelled adoptme is up for grabs :)
</comment><comment author="SahilC" created="2016-01-19T13:50:08Z" id="172859453">Hey Clint, please assign this to me. Additionally, I may need a little bit of help getting on my feet with this, could you tell me where to get started? 
</comment><comment author="clintongormley" created="2016-01-20T13:48:35Z" id="173209561">@SahilC no need to assign - just send a PR when you're ready.  Sorry I can't provide advice about where to start, that's part of the learning process :)  The forums may helpful to you: https://discuss.elastic.co/
</comment><comment author="mi-hol" created="2016-12-16T18:52:28Z" id="267668116">@clintongormley  with v2.4.2 released on Nov 16 I was wondering if this issue should be closed
</comment><comment author="clintongormley" created="2016-12-19T16:11:17Z" id="268004431">@mi-hol no, this hasn't been done yet</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String values are stored in float field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3414</link><project id="" key="" /><description>(Version 0.90.0) I have a float field defined in a mapping and I was able to put a string values into the float field as long as the string value can pass java.lang.Float.parserFloat(). Later sorting on this field will result in exception like this:

```
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:573)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:484)
        at org.elasticsearch.search.SearchService.createContext(SearchService.java:469)
        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:462)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:234)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAc
tion.java:141)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstP
hase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(Trans
portSearchTypeAction.java:205)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(Trans
portSearchTypeAction.java:192)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTy
peAction.java:178)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
Caused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [latitude]: value_field [loc_lat] isn't a number field, but a string
        at org.elasticsearch.search.facet.termsstats.TermsStatsFacetParser.parse(TermsStatsFacetParser.java:127)
        at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:92)
        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:561)
        ... 12 more
```
</description><key id="17430131">3414</key><summary>String values are stored in float field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">shaoweite</reporter><labels /><created>2013-07-31T01:22:45Z</created><updated>2013-10-15T16:10:57Z</updated><resolved>2013-10-15T16:10:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-31T06:49:01Z" id="21843450">Hey,

can you also show us the mapping of your index and a sample document you indexed to trigger this, so we get a complete picture of this problem? Does this also happen with 0.90.2?

Thanks!
</comment><comment author="spinscale" created="2013-07-31T07:57:48Z" id="21846035">I took a closer look, this was my test and it worked

```
curl -X DELETE localhost:9200/foo
curl -X PUT localhost:9200/foo

curl -X PUT localhost:9200/foo/bar/_mapping -d '{
    "bar" : { "properties" : { "value" : { "type":"float" } } }
}'

curl -X PUT localhost:9200/foo/bar/1 -d '{ "value" : "2e04" }'
curl -X PUT localhost:9200/foo/bar/2 -d '{ "value" : "2e05" }'
curl -X PUT localhost:9200/foo/bar/3 -d '{ "value" : "2e06" }'
curl -X PUT localhost:9200/foo/bar/4 -d '{ "value" : "2e07" }'

curl -X GET localhost:9200/foo/_refresh
curl -X GET localhost:9200/foo/bar/_mapping


curl -X POST localhost:9200/foo/bar/_search -d '{ "query" : { "match_all" : {} }, "sort" : [  { "value" : { "order":"desc"} } ]}'

curl -X POST localhost:9200/foo/bar/_search -d '{ "query" : { "match_all" : {} }, "sort" : [  { "value" : { "order":"asc"} } ]}'

curl -X POST localhost:9200/foo/bar/_search -d '{ "query" : { "match_all" : {} }, "sort" : [  "value" ]}'

```

However checking your exception above, it actually tells, that you are trying to some faceting stuff. Can you provide the facet query as well in order to reproduce your issue? The mapping for the `value_field` seems to describe it as a string, but I need more information in order to be sure.
</comment><comment author="shaoweite" created="2013-07-31T17:06:39Z" id="21878619">Thanks for the quick response. Here is the mapping:

{
  "dev_bg_20130729232025_a" : {
    "combinedlog" : {
      "properties" : {
        "app_id" : {
          "type" : "string"
        },
        "app_name" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "company_id" : {
          "type" : "long"
        },
        "company_name" : {
          "type" : "string"
        },
        "device" : {
          "type" : "string"
        },
        "doctype" : {
          "type" : "string"
        },
        "email_cc" : {
          "type" : "string"
        },
        "email_date" : {
          "type" : "string"
        },
        "email_from" : {
          "type" : "string"
        },
        "email_subject" : {
          "type" : "string"
        },
        "email_to" : {
          "type" : "string"
        },
        "file_name" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "ip_src" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "keyphrases" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "loc_acc" : {
          "type" : "string"
        },
        "loc_city" : {
          "type" : "string"
        },
        "loc_country" : {
          "type" : "string"
        },
        "loc_lat" : {
          "type" : "float"
        },
        "loc_lon" : {
          "type" : "float"
        },
        "loc_region" : {
          "type" : "string"
        },
        "log_type" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "mime" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "page_title" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "referer" : {
          "type" : "string"
        },
        "req_channel" : {
          "type" : "string"
        },
        "req_id" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "req_id2" : {
          "type" : "string"
        },
        "req_ts" : {
          "type" : "string"
        },
        "request" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "request_ts" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "response_bytes" : {
          "type" : "string"
        },
        "response_time" : {
          "type" : "string"
        },
        "ts" : {
          "type" : "long"
        },
        "url" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "user_agent" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "user_email" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "user_id" : {
          "type" : "string"
        },
        "user_name" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "xact" : {
          "type" : "string"
        }
      }
    }
  }
}

The field that I had such issues is 'loc_lon'. Here is two sampe docs with one having float value of 'loc_lon' but the other string:

{
  "took" : 4,
  "timed_out" : false,
  "_shards" : {
    "total" : 10,
    "successful" : 10,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : null,
    "hits" : [ {
      "_index" : "dev_bg_20130730173313_a",
      "_type" : "combinedlog",
      "_id" : "UfGYbH8AAQEAAEhHB7IAAADE",
      "_score" : null, "_source" : {"loc_city": "beijing", "app_name": "Google Apps", "referer": "", "file_name": "", "loc_region": "beijing", "app_id": "1", "company_id": "", "xact": "UfGYbH8AAQEAAEhHB7IAAADE", "page_title": "", "keywords": "", "email_date": "2013-07-25T21%3a28%3a10.093Z", "loc_country": "cn", "file_ext": "", "ip_src": "10.1.0.157", "user_id": "", "response_bytes": "3151", "ts": "20130725212812", "company_name": "", "email_cc": "qa@acme.com qa@acme.com", "user_name": "", "loc_lon": 116.39631301781112, "action": "", "loc_acc": "", "email_to": "John Willson jwilson@acme.com", "device": "", "log_type": "access", "response_time": "284057", "url": "https%3a//m.google.com/Microsoft-Server-ActiveSync?User=qa@acme.com&amp;DeviceId=ApplC8QF8GP4DDP9&amp;DeviceType=iPhone&amp;Cmd=Sync", "request": "POST /Microsoft-Server-ActiveSync/?User=qa@acme.com&amp;DeviceId=ApplC8QF8GP4DDP9&amp;DeviceType=iPhone&amp;Cmd=Sync HTTP/1.1", "loc_lat": 39.90631301781111, "user_agent": "Apple-iPhone3C3/1002.329", "email_subject": "Test email \u6e2c\u8a66\u90f5\u4ef6", "mime": "", "email_from": "QAqa@acme.com", "user_email": ""},
      "sort" : [ 20130725212812 ]
    }, {
      "_index" : "dev_bg_20130730173313_a",
      "_type" : "combinedlog",
      "_id" : "UdMTYH8AAAEAAFeKGfoAAACG00",
      "_score" : null, "_source" : {"loc_city":"new_york","req_id2":"0","req_channel":"0","file_name":"Hello World.docx","loc_region":"new_york","ts":"20130702175232","loc_lon":"1235","company_id":1,"loc_lat":"1235","company_name":"acme.com","req_id":"UdMTYH8AAAEAAFeKGfoAAACG","doctype":"docx","user_email":"qa@acme.com","user_id":"166","loc_country":"us","keyphrases":"","prev_tags":"","log_type":"beacon","ip_src":"10.1.0.132","user_name":"QA Test"},
      "sort" : [ 20130702175232 ]
    } ]

The facet query is:

[2013-07-29 23:19:22,390][DEBUG][action.search.type       ] [Eshu] [dev][4], node[kt-02Pi7Q7ifvVj_ynxdXQ],
 [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6bc71ff5]
org.elasticsearch.search.SearchParseException: [dev_bg_a][4]: from[-1],size[-1],sort[&lt;custom:"ts": org.elastics
earch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@4fd2667e&gt;!]: Parse Failure [Failed to parse
 source [{"sort": [{"ts": {"order": "desc"}}], "filter": {"and": [{"range": {"ts": [{"to": "20130729231921", "f
rom": "20130722231921"}]}}, {"term": {"company_id": 1}}]}, "facets": {"aggregate": {"facet_filter": {"and": [{"
range": {"ts": [{"to": "20130729231921", "from": "20130722231921"}]}}, {"term": {"company_id": 1}}]}, "terms": 
{"field": "loc_country", "size": 30}}, "latitude": {"facet_filter": {"and": [{"range": {"ts": [{"to": "20130729
231921", "from": "20130722231921"}]}}, {"term": {"company_id": 1}}]}, "terms_stats": {"value_field": "loc_lat",
 "key_field": "loc_country", "size": 30}}, "longitude": {"facet_filter": {"and": [{"range": {"ts": [{"to": "201
30729231921", "from": "20130722231921"}]}}, {"term": {"company_id": 1}}]}, "terms_stats": {"value_field": "loc_
lon", "key_field": "loc_country", "size": 30}}}, "size": 100}]]

Thanks for helping.

Regards,
Wei
</comment><comment author="shaoweite" created="2013-07-31T17:47:02Z" id="21881433">Forgot to reply wrt 0.90.2. Actually even on 0.90.0 I could no longer reproduce the exception from the facet query. The query now works without such exception even though data are still mixed with string and float values. I will keep investigating and provide more feedbacks if possible.

Thanks,
Wei
</comment><comment author="spinscale" created="2013-08-02T06:40:44Z" id="21989035">I just saw this commit https://github.com/elasticsearch/elasticsearch/commit/31fd7764e782384e3a278815dbd2a7c3cf065ed5

Judging from that I suppose that your terms stats query might have gone over several indices, and in one of the indices the field for stats was not not yet mapped.

Maybe this helps that you are not able to reproduce, as all your indices now contain data.
</comment><comment author="spinscale" created="2013-10-15T16:10:57Z" id="26348474">closing for now due to lack of new infos. feel free to reopen, if you stumble over this again.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Delete API ack to wait also for actual deletion of shards from disk</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3413</link><project id="" key="" /><description>Currently, we callback for the ack on delete when the index gets removed from the relevant metadata of the nodes involved. We should also wait for it to be deleted from the data nodes involved.
</description><key id="17407224">3413</key><summary>Delete API ack to wait also for actual deletion of shards from disk</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-30T16:57:38Z</created><updated>2013-07-30T16:58:36Z</updated><resolved>2013-07-30T16:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Forbid usage of StringReader in favor of FastStringReader.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3412</link><project id="" key="" /><description>StringReader is synchronized although input streams should always be consumed
by a single thread at a time. FastStringReader on the other hand is completely
thread unsafe.
</description><key id="17407148">3412</key><summary>Forbid usage of StringReader in favor of FastStringReader.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-07-30T16:56:16Z</created><updated>2014-07-16T21:52:45Z</updated><resolved>2013-07-31T07:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Forbid usage of new StringReader(String)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3411</link><project id="" key="" /><description>We have FastStringReader and should ensure it is always used instead of StringReader.
</description><key id="17400047">3411</key><summary>Forbid usage of new StringReader(String)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-30T14:51:04Z</created><updated>2013-07-31T07:45:51Z</updated><resolved>2013-07-31T07:41:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-07-30T17:47:03Z" id="21808982">pull request looks good to me! go push it
</comment><comment author="jpountz" created="2013-07-31T07:45:51Z" id="21845555">Pushed. Thanks for the review, Simon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve how aliases are handled in the cluster state.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3410</link><project id="" key="" /><description /><key id="17393781">3410</key><summary>Improve how aliases are handled in the cluster state.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-30T12:46:43Z</created><updated>2013-07-30T15:08:21Z</updated><resolved>2013-07-30T15:08:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use Analyzer.tokenStream(fieldName, text) instead of Analyzer.tokenStream(fieldName, new FastStringReader(text))</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3409</link><project id="" key="" /><description>Lucene 4.4 introduced the `tokenStream(String fieldName, String text)` helper method  which nicely reuses the string reader. Whenever analyzing Strings, we should use this method instead of using the Reader method with a FastStringReader.
</description><key id="17392650">3409</key><summary>Use Analyzer.tokenStream(fieldName, text) instead of Analyzer.tokenStream(fieldName, new FastStringReader(text))</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-30T12:17:18Z</created><updated>2013-07-30T15:19:13Z</updated><resolved>2013-07-30T15:19:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-07-30T12:40:39Z" id="21787810">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify custom_boost_factor, custom_score and custom_filters_score and add distance scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3408</link><project id="" key="" /><description>These commits close issue #3423
</description><key id="17386067">3408</key><summary>Unify custom_boost_factor, custom_score and custom_filters_score and add distance scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-07-30T09:33:48Z</created><updated>2014-06-14T17:32:08Z</updated><resolved>2013-10-18T15:19:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-08-05T11:01:28Z" id="22099254">this looks really good. Lets get the changes in including moving ScoreFunction to doubles and then push this!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify custom_boost_factor, custom_score and custom_filters_score</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3407</link><project id="" key="" /><description># Unify custom scores

The custom boost factor, custom script boost and the filters function query all do the same thing: They take a query and for each found document compute a new score based on the query score and some script, come custom boost factor or a combination of these two. However, the json format for these three functionalities is very different. This makes it hard to add new functions.

It would be nice to consolidate the three custom scores under the keyword &lt;code&gt;function_score&lt;/code&gt;. 

The new format should provide the same functionality as before that is: compute a new score with one function:

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "function": {}
} 
```

or allow to combine the newly computed scores 

```
"function_score": {
    "(query|filter)": {},
    "boost": "boost for the whole query",
    "functions": [
        {
            "filter": {},
            "function": {}
        },
        {
            "function": {}
        }
    ],
    "score_mode": "(mult|max|...)"
}
```

&lt;code&gt;function&lt;/code&gt; here can be either

```
"script_score": {
    "lang": "lang",
    "params": {
        "param1": "value1",
        "param2": "value2"
     },
    "script": "some script"
}
```

or

```
"boost_factor" : number
```
## Changes

The custom boost factor query 

```
"custom_boost_factor" : {
    "query" : {
        ....
    },
    "boost_factor" : 5.2
}
```

would become

```
"function_score" : {
    "query" : {
        ....
    },
    "boost_factor" : 5.2
}
```

The custom script score

```
"custom_score" : {
    "query" : {
        ....
    },
    "params" : {
        "param1" : 2,
        "param2" : 3.1
    },
    "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)"
}
```

would become

```
"function_score" : {
    "query" : {
        ....
    },
    "script_score" : {

        "params" : {
            "param1" : 2,
            "param2" : 3.1
        },
        "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)"
    }
}
```

and the custom filters score query 

```
"custom_filters_score" : {
    "query" : {
        "match_all" : {}
     },
    "filters" : [
        {
            "filter" : { "range" : { "age" : {"from" : 0, "to" : 10} } },
            "boost" : "3"
        },
        {
            "filter" : { "range" : { "age" : {"from" : 10, "to" : 20} } },
            "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)"
        }
    ],
    "score_mode" : "first",
    "params" : {
        "param1" : 2,
        "param2" : 3.1
    }
    "score_mode" : "first"
}       
```

becomes:

```
"function_score" : {
    "query" : {
        "match_all" : {}
    },
    "functions" : [
        {
            "filter" : { "range" : { "age" : {"from" : 0, "to" : 10} } },
            "boost" : "3"
        },
        {
            "filter" : { "range" : { "age" : {"from" : 10, "to" : 20} } },
            "script_score" : { 
                "script" : "_score * doc['my_numeric_field'].value / pow(param1, param2)",
                "params" : {
                    "param1" : 2,
                    "param2" : 3.1
                }

            }
        }
    ],
    "score_mode" : "first",     
}       
```
</description><key id="17384168">3407</key><summary>Unify custom_boost_factor, custom_score and custom_filters_score</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-07-30T09:00:27Z</created><updated>2013-08-01T12:55:29Z</updated><resolved>2013-08-01T12:55:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-08-01T12:55:29Z" id="21934185">replaced by issue #3423
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for #3399 - auto_expand_replicas causing very large amount of cluste...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3406</link><project id="" key="" /><description>...r state changes when a node joins or leaves the cluster
</description><key id="17365837">3406</key><summary>Fix for #3399 - auto_expand_replicas causing very large amount of cluste...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">moberg</reporter><labels /><created>2013-07-29T21:54:32Z</created><updated>2014-06-20T16:28:39Z</updated><resolved>2013-07-30T20:38:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-30T20:38:39Z" id="21820922">Pushed to master and 0.90. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature to support passing in the parent value via a field path during indexing, same as _id &amp; _routing. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3405</link><project id="" key="" /><description>Currently I see that the _id &amp; _routing can be extracted from the source/document by creating the mapping as below.  

```
{
    "tweet" : {
       "_id" : {
            "path" : "post_id"
        }
    }
}
```

But _parent value can't be set this way.
Could you please add feature to support passing in the parent value via a field path during indexing, same as _id &amp; _routing?
</description><key id="17363020">3405</key><summary>Feature to support passing in the parent value via a field path during indexing, same as _id &amp; _routing. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">govindm</reporter><labels /><created>2013-07-29T20:59:30Z</created><updated>2014-07-09T15:23:49Z</updated><resolved>2014-07-04T11:08:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="govindm" created="2013-07-29T21:00:35Z" id="21751167">in case more info required please refrer to this thread.
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/KSEf3FZPh64
</comment><comment author="SimonSimCity" created="2014-01-14T10:35:31Z" id="32254020">:+1: I needed that for one of my projects, where I use Couchbase and can just use the data I have inside the document ...
</comment><comment author="clintongormley" created="2014-07-04T11:08:10Z" id="48031790">Closed in favour of #6730
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add version support to get and mget apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3404</link><project id="" key="" /><description>Add version support to mget and get apis, that only will perform the get operation if the version of the document to be fetched matches with the provided version.

Both get and mget apis will support the following parameters: `version` and `version_type`.
</description><key id="17339556">3404</key><summary>Add version support to get and mget apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-07-29T13:35:45Z</created><updated>2013-07-29T13:53:39Z</updated><resolved>2013-07-29T13:53:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Save percolation result to document itself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3403</link><project id="" key="" /><description>Referencing:

https://github.com/elasticsearch/elasticsearch/issues/3173#issuecomment-21196963
https://twitter.com/Argorak/statuses/357893281193017344
https://twitter.com/jeroenrosenberg/status/361807942443479040

I'd like to propose an additional feature for the percolator: save the result in the document itself (or a child document, avoiding updating the document). This could be used e.g. for tagging of input data for later search by certain rules.

I often use the percolator to categorize incoming data. This data is given by external services and is messy, though fixable by simple "search and clean". We use the percolator to register (sometimes user-created) queries that map those entries to our internal values.

Currently, this works like this:
- Percolate the document
- Write the result to the target field
- Index the document

Allowing this within the percolation step itself would vastly reduce our network overhead in this case and (if bulk percolation happens) also allow us to do bulk actions in one step.

The interface could be similar to certain queries (like the `terms` query):

`{
   ... percolation parameters
   "save_to": { "path": "data.categories" }
}`
</description><key id="17337948">3403</key><summary>Save percolation result to document itself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">skade</reporter><labels /><created>2013-07-29T12:58:39Z</created><updated>2014-08-08T13:44:45Z</updated><resolved>2014-08-08T13:44:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jeroenr" created="2013-07-29T13:12:29Z" id="21718628">This sounds really useful for my use case. I'm writing a chat application with an elasticsearch backend for geo spatial search. Users can enable a service to get notified as soon as a chat candidate comes available within their range.

Currently, it works like this:
- User A gets online and finds no chat candidates
- User A enables the notification service and goes offline awaiting the notification. This is stored using partial doc update.
- User B gets online. Besides the search for online matching candidates, I do a second (asynchronous) search for matching offline users with the notification service enabled.
- The second query finds user A and pushes user A to user B

I was hoping to implement this more gracefully using the percolator. When a user enables the notification service I'd like to register a percolate query that's executed when the index changes. If matches are found for the percolate query I would like to push the resulting documents to the user that registered the percolate query.
</comment><comment author="martijnvg" created="2013-08-27T20:14:29Z" id="23366927">I'm unsure about this feature, it can save traffic from / to the cluster, but it can result in a lot of write operations in the cluster without any way to limit it. A document being percolated can match with many percolator queries, I think it is better to just both use the multi percolate api and bulk api from the client side.
</comment><comment author="skade" created="2013-08-30T08:00:11Z" id="23546097">@martijnvg The way I am proposing it (save the result of the percolation to a field of the doc or a child doc), the number of write operations would be bounded: at most one per doc, depending on the implementation.
</comment><comment author="martijnvg" created="2013-08-31T13:02:10Z" id="23605886">@skade A percolate request can match with millions of queries (the requests scatters to all targeted shards) how would you save that kind of results? Save all matches to one document or save it as separate documents? In the first case this one document would be massive and in the second case the number of write operations is unbounded.
</comment><comment author="skade" created="2013-09-02T20:55:54Z" id="23678074">@martijnvg No, I never talked about a document per hit.

The other argument a bit arguing the edge-case. If I save all to one document - which I would probably do externally anyways - it is in my hands to keep the data managable. The hypothetical case of a vey big document would strike in any case.
</comment><comment author="martijnvg" created="2013-09-03T17:19:20Z" id="23730280">&gt; No, I never talked about a document per hit.

Ok, then I misunderstood you, so just one child doc for all percolate matches. 

But I still don't understand how you would manage what matches go into the document and what matches don't get in. Obviously one can select a few percolate queries that need to be executed (limiting by query or filter), but I'm thinking about the case where more than just a few queries match with a document being percolated.

The other doubt I have is that having this feature in the percolate api, makes this api also effectively a write and read api, which doesn't seem too nice to me.
</comment><comment author="kimchy" created="2013-09-03T17:33:34Z" id="23731477">I also feel that the amount of configuration options that will go with this feature will make it quite complex (how do they percolation matches end up actually being indexed - child doc, additional elements, merging logic, ...). I also don't think people will actually pay that much compared to just doing it on the client side, and retaining the flexibility involved there.
</comment><comment author="clintongormley" created="2014-08-08T13:44:45Z" id="51601893">Agreed on the complexity issue here, and everybody will have a slightly different use case which needs extra options to support it.  Much simpler to do exactly what you need client side.  I'm going to close this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MoreLikeThisFieldQueryBuilder defaulted failOnUnsupportedField inconsistently to the REST api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3402</link><project id="" key="" /><description>builder defaulted to false while REST api defaults to true
</description><key id="17337886">3402</key><summary>MoreLikeThisFieldQueryBuilder defaulted failOnUnsupportedField inconsistently to the REST api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-29T12:57:15Z</created><updated>2013-07-29T20:17:27Z</updated><resolved>2013-07-29T13:05:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-07-29T20:17:27Z" id="21748055">Thanks @bleskes !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added support for acknowledgement from other nodes in open/close index api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3401</link><project id="" key="" /><description /><key id="17297468">3401</key><summary>Added support for acknowledgement from other nodes in open/close index api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels /><created>2013-07-27T12:22:34Z</created><updated>2014-07-16T21:52:46Z</updated><resolved>2013-07-29T13:34:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-29T13:34:38Z" id="21719874">Merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Open/Close indices to support acknowledgement from other nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3400</link><project id="" key="" /><description>The open/close index api currently returns back its response after the cluster state change has been digested by the master node, but the updated cluster state hasn't necessarily been published to all the nodes yet.

Add support for acknowledgement like we do with aliases. We wait for an ack from other nodes till the (configurable) timeout expires (default 10 seconds). The acknowledged flag in the response will reflect whether the cluster state change has been acknowledged by all the other nodes or not.
</description><key id="17297133">3400</key><summary>Open/Close indices to support acknowledgement from other nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-27T11:40:49Z</created><updated>2013-07-29T13:13:13Z</updated><resolved>2013-07-29T13:13:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>auto_expand_replicas causing very large amount of cluster state changes when a node joins or leaves the cluster - causing the master to become unresponsive</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3399</link><project id="" key="" /><description>In our cluster we have 8 nodes and about 100 indices. Each index have one shard and is replicated to every node using the setting &#8220;auto_expand_replicas=&#8217;0-all&#8217;&#8221;. 

We observed that when a node leaves the cluster, the master node becomes unresponsive for some time. The more indices we added the longer time it got unresponsive. During this time restarted nodes were sometimes not able to join back into the cluster, causing a split brain scenario, or were just hung at startup.

Looking at the source code for how the node leave and join events is handled I think I have identified the bug. The ClusterChangedEvent is propagated to MetaDataUpdateSettingsService#clusterChanged which will loop through every index and if the number of nodes has changed, fire updateSettings for that index. When a node joins or leaves and using the auto_expand_replicas setting, every index will be affected. So for 100 indices it will fire off 100 updateSettings. 

The problem is that each call updateSettings results in a new cluster state, which again will propagate back to the MetaDataUpdateSettingsService#clusterChanged, resulting in an exponential number of cluster state changes. This fills the log with messages like this:

[2013-07-26 20:55:45,726][INFO ][cluster.metadata         ] [master] [index1] auto expanded replicas to [5]
[2013-07-26 20:55:45,726][INFO ][cluster.metadata         ] [master] [index2] auto expanded replicas to [5]
[2013-07-26 20:55:45,726][INFO ][cluster.metadata         ] [master] [index3] auto expanded replicas to [5]

My proposed fix is to group the updates together by fNumberOfReplicas and only trigger one update for each fNumberOfReplicas. In our case, when &#8220;auto_expand_replicas&#8221; is set to &#8220;0-all&#8221; this will result in one cluster state change instead of a flood of changes. 

The fix passes all the tests and solves the problem we have been observing in production and been able to reproduce in our development environment. 

Will update the ticket asap with a link to the commit for fix.
</description><key id="17285653">3399</key><summary>auto_expand_replicas causing very large amount of cluster state changes when a node joins or leaves the cluster - causing the master to become unresponsive</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">moberg</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-26T21:30:36Z</created><updated>2013-07-30T20:35:45Z</updated><resolved>2013-07-30T20:35:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-26T21:40:26Z" id="21649856">Batching the cluster events will definitely help. Btw, in 0.90 branch (upcoming 0.90.3) we fix the part about cluster not being responsive due to large amount of cluster change events. The cluster state publishing and the ping requests were using the same `HIGH` transport channel, now we have a dedicated transport channel for pings, means they will not "get behind" potentially a large amount of cluster state events.
</comment><comment author="moberg" created="2013-07-26T21:41:32Z" id="21649902">Link to fix: 
https://github.com/moberg/elasticsearch/commit/2327c5604243360e776e1a75f59f7219e2b038ee
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added precision parameter to histogram facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3398</link><project id="" key="" /><description>This change adds a `precision` parameter to the histogram facet that produces variable width buckets with a fixed number of significant figures. For example, a `precision` of 1 creates the following buckets:
- 1 [0.95 &lt;= x &lt; 1.5)
- 2 [1.5 &lt;= x &lt; 2.5)
- 3 [2.5 &lt;= x &lt; 3.5)
- ...
- 7 [6.5 &lt;= x &lt; 7.5)
- 8 [7.5-8.5)
- 9 [8.5-9.5)
- 10 [9.5-15)
- 20 [15-25)
- 30 [25-35)
- ...
- 70 [65-75)
- 80 [75-85)
- 90 [85-95)
- 100 [95-150)
- 200 [150-250)
- 300 [250-350)
- etc.

This is particularly useful for graphing long tailed distributions. We use it to produce CDF plots, like this...

![screen shot 2013-07-26 at 12 28 28](https://f.cloud.github.com/assets/151069/862133/8a933abe-f5e6-11e2-834b-c9645623fb4c.png)
</description><key id="17260680">3398</key><summary>Added precision parameter to histogram facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">richardpoole</reporter><labels><label>discuss</label></labels><created>2013-07-26T12:17:35Z</created><updated>2014-08-08T08:28:46Z</updated><resolved>2014-08-08T08:28:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-08T08:28:46Z" id="51575846">Hi,

For this kind of use-case, I think it would be more flexible to either define the ranges on client-side and use a range aggregation, or apply the histogram to a script that could be eg. a logarithm of the field values.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regression ? #678 : terms facet on an IP field returns terms as numbers, not IPs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3397</link><project id="" key="" /><description>I'm trying to make a terms facet of an IP address field (with the ip type). It returns numbers, not IP addresses. It works if the type is string. I thought it was fixed in #678. I'm using 0.90.3.
The idea is to use Kibana to draw a pie of the top IP addresses in my logs.
</description><key id="17259566">3397</key><summary>Regression ? #678 : terms facet on an IP field returns terms as numbers, not IPs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dav3860</reporter><labels /><created>2013-07-26T11:43:04Z</created><updated>2013-10-05T15:25:51Z</updated><resolved>2013-10-05T15:25:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Lanzaa" created="2013-08-02T21:00:08Z" id="22035523">This is a duplicate of Issue #3321 
</comment><comment author="javanna" created="2013-10-05T15:25:51Z" id="25750530">Thanks @Lanzaa for pointing that out, closing as duplicate of #3321 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamically updating index.gc_deletes not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3396</link><project id="" key="" /><description>It seems that dynamically changing `index.gc_deletes` is broken. The corresponding field in code is `RobinEngine.gcDeletesInMillis` 

```
this.gcDeletesInMillis = indexSettings.getAsTime(INDEX_GC_DELETES, TimeValue.timeValueSeconds(60)).millis();
```

It is only ever modified after a previous info log:

```
if (gcDeletesInMillis != RobinEngine.this.gcDeletesInMillis) {
     logger.info("updating index.gc_deletes from [{}] to [{}]", TimeValue.timeValueMillis(RobinEngine.this.gcDeletesInMillis), TimeValue.timeValueMillis(gcDeletesInMillis));
     RobinEngine.this.gcDeletesInMillis = gcDeletesInMillis;
}
```

This line never gets logged though. 

Reopening the index works as expected.
</description><key id="17257905">3396</key><summary>dynamically updating index.gc_deletes not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">sfussenegger</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-26T10:46:04Z</created><updated>2013-07-30T11:05:53Z</updated><resolved>2013-07-30T11:05:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-26T11:18:17Z" id="21614885">@sfussenegger I fixed this recently in master as part of a bigger change. I will port that little fix to the 0.90 branch and it will be available in the next release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Hebrew Support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3395</link><project id="" key="" /><description>Hi,

I'm using XenForo forum software that uses ElasticSearch. I'm mainly interested in  "Stemming analyzer with language" feature, but ElasticSearch doesn't support Hebrew, so this needs to be done first in order for the feature above to work.

If you need anything in terms of Hebrew language (although I see by the first/last names that some of the developers here are Israeli), I would be happy to help.

Thanks in advance.

-Mike.
</description><key id="17237018">3395</key><summary>Add Hebrew Support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Moshe1010</reporter><labels /><created>2013-07-25T21:38:51Z</created><updated>2013-08-01T06:44:19Z</updated><resolved>2013-07-30T12:17:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-07-25T22:14:46Z" id="21588778">Hi, I do not know Hebrew myself but after some googling I found [HebMorph](https://github.com/synhershko/HebMorph) (also it seems it contains integration code for Lucene). It is based on Hspell.

The second option could be Hunspell dictionary, there are available Hebrew .dic and .aff files, but the _"only problem"_ is that although Hunspell can support language like Hebrew, I am afraid the Lucene implementation is not ready for right-to-left languages yet. But I am sure this would be cool enhancement! :cool: 

My 2 cents,
Lukas
</comment><comment author="Moshe1010" created="2013-07-25T22:30:19Z" id="21589502">Hi,

Thank you for the quick reply. It seems like ElasticSearch does support RTL since it supports arabic languages, which are RTL exactly as Hebrew. Is there any difference between Languages in RTL in terms of the Lucene? 

Thanks.
</comment><comment author="lukas-vlcek" created="2013-07-25T23:23:39Z" id="21591854">If you mean the arabic analyzer (listed here http://www.elasticsearch.org/guide/reference/index-modules/analysis/lang-analyzer/) I read in Lucene API that [it does stemming](http://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/ar/ArabicAnalyzer.html) (so no lemmatization).

[Hunspell token filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/hunspell-tokenfilter/) is a general token filter that can use any Hunspell compatible dictionaries (they are used mainly for spellchecking, for example in OpenOffice, Mozzilla, ... etc). So if you have a dictionary that Hunspell can understand you can use it with Lucene (and Elasticsearch). But as I said I am not sure if Lucene implementation of Hunspell supports RTL languages now.
</comment><comment author="spinscale" created="2013-07-30T12:17:25Z" id="21786673">Hey,

that [stackoverflow post](http://stackoverflow.com/questions/1063856/lucene-hebrew-analyzer) might also shed some light. Seems that hebrew with lucene is quite hard. This [dzone post](http://architects.dzone.com/articles/hebrew-search-elasticsearch) talks about using elasticsearch with hebrew language (using hebmorph), maybe that can help you as well 

I am going to close the issue here, as elasticsearch is not the right place to tackle this problem, but rather Lucene. Anyway you might want to ask on the mailinglist as, as there are many more eyes looking at your problem, maybe someone already tried to solve it.

Maybe @synhershko can help with his expertise as well!
</comment><comment author="synhershko" created="2013-07-30T12:44:22Z" id="21787996">@lukas-vlcek RTL has nothing to do with IR capabilities. For Arabic there is aramorph and light10, the former is a lemmatizer the latter stemmer. Hebrew is a bigger challenge for many reasons, none of which have to do with RTL...

@Moshe1010 HebMorph (see my github) is a project made just for that, and released under AGPL3.

Unfortunately my blog which contains a lot of info on the subject is down at the moment and will be that way for another week or so, but the link to dzone @spinscale posted was taken from my blog and basically will give you all that you need.
</comment><comment author="lukas-vlcek" created="2013-07-30T15:21:35Z" id="21798537">@synhershko sure, I did not say it had, I just pointed out that Lucene Hunspell implementation does not (in my understanding) support RTL languages now. That's a different topic.
</comment><comment author="Moshe1010" created="2013-08-01T01:40:02Z" id="21908818">So, is it possible to integrate HebMorph in Elastic Search? (Officially)
</comment><comment author="synhershko" created="2013-08-01T06:44:19Z" id="21917842">http://architects.dzone.com/articles/hebrew-search-elasticsearch
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Show file-based template data in /_cluster/state API call</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3394</link><project id="" key="" /><description>In 0.90.x, the /_cluster/state handler shows metadata from API-injected templates, but not those read from file-based templates.

It's difficult to see if file-based templates have been recognised without creating an index - having both included here would be helpful (unless it's already shown somewhere else?)
</description><key id="17217746">3394</key><summary>Show file-based template data in /_cluster/state API call</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hynd</reporter><labels /><created>2013-07-25T15:27:24Z</created><updated>2014-08-08T13:42:52Z</updated><resolved>2014-08-08T13:42:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T13:42:52Z" id="51601639">Hi @hynd 

File based templates could differ between nodes.  Really, it's better to manage templates (and other settings) via the API anyway, in which case they'll be available in the cluster state.  I'm going to close this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ability to read/re-read file-based templates on demand</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3393</link><project id="" key="" /><description>In 0.90.x, as far as i can tell, file-based templates are only read when a cluster starts up. 
It would be great to be able to have templates to be read from disk on index creation (particularly when using a CMS to curate them).
</description><key id="17217344">3393</key><summary>Ability to read/re-read file-based templates on demand</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hynd</reporter><labels /><created>2013-07-25T15:20:45Z</created><updated>2013-08-07T08:32:07Z</updated><resolved>2013-08-07T08:32:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-08-07T00:32:02Z" id="22222160">Index templates are read upon index creation. The elasticsearch.yml file is not.
</comment><comment author="hynd" created="2013-08-07T08:32:07Z" id="22236948">Sorry Ivan, i must have dreamt it not working - i tried again and it is picking them up dynamically.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RegexpQueryBuilder should implement MultiTermQueryBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3392</link><project id="" key="" /><description>Lucene RegexpQuery is now a MultiTermQuery. The builder should reflect this property.
</description><key id="17213341">3392</key><summary>RegexpQueryBuilder should implement MultiTermQueryBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-07-25T14:13:14Z</created><updated>2013-11-04T11:33:51Z</updated><resolved>2013-10-31T08:25:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Api for getting all current settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3391</link><project id="" key="" /><description>Currently there is no way to pull out the default settings unless you set them to something or unless you look at the source code. It would be nice to have an api endpoint for seeing all the settings as they are configured (default or not)
</description><key id="17201779">3391</key><summary>Api for getting all current settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">quizlet-checkout-temp</reporter><labels /><created>2013-07-25T09:33:47Z</created><updated>2014-02-14T18:09:48Z</updated><resolved>2014-02-14T18:09:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="RyanGordon" created="2013-07-25T09:34:46Z" id="21542876">Derp, I posted this under the wrong account, but anyway just so you know
</comment><comment author="javanna" created="2013-11-04T15:48:02Z" id="27695173">Makes sense! Looks like a duplicate of #2738 though. What do you think @RyanGordon ?
</comment><comment author="javanna" created="2014-02-14T18:09:48Z" id="35109032">Closing as duplicate of #2738 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation Inconsistencies</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3390</link><project id="" key="" /><description>It seems like a lot of recent settings/features are undocumented or not updated to reflect the new values in the elasticsearch.org documentation. It would be really great to see that become more consistent in general.

Specifically I noticed that on http://www.elasticsearch.org/guide/reference/modules/threadpool/ it says the default is unlimited however in 90.1 that changed to be 1000 by default.
</description><key id="17201753">3390</key><summary>Documentation Inconsistencies</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">quizlet-checkout-temp</reporter><labels /><created>2013-07-25T09:32:53Z</created><updated>2013-10-30T11:30:05Z</updated><resolved>2013-10-30T11:30:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="RyanGordon" created="2013-07-25T09:37:21Z" id="21543005">Derp, I posted this under the wrong account, but anyway just so you know
</comment><comment author="s1monw" created="2013-07-25T17:34:19Z" id="21570767">thanks for opening this I will look into it soon
</comment><comment author="javanna" created="2013-07-26T07:51:24Z" id="21606373">@s1monw #3387 seems related, could you have a look at it too?
</comment><comment author="spinscale" created="2013-10-30T11:30:05Z" id="27381621">Hey, we try to keep as up-to-date as possible with documentation (you may have seen, we have documentation per version now, which should make it easier to track).

This specific one is fixed, but whenever you encounter something, just hit us with another github issue. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FilterBuilder and QueryBuilder equals</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3389</link><project id="" key="" /><description>Hey, I'm writing some middleware that parses some amount of input into query/filters using the builders in org.elasticsearch.index.query, and it seems that it uses default equals. This makes writing unit tests cumbersome afaik, is there any nice workaround for this?

If not, I don't mind submitting a pull request for this...
</description><key id="17196364">3389</key><summary>FilterBuilder and QueryBuilder equals</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mhickman</reporter><labels /><created>2013-07-25T06:47:31Z</created><updated>2013-10-09T15:03:36Z</updated><resolved>2013-10-09T15:03:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-09T09:57:16Z" id="25959237">Right, none of the filter/query builders implement the `equals` method. Not sure if they should though. But they all implement the `ToXContent` interface, and their `toString` method returns their json representation. The workaround would be to compare them as strings in your tests. How does that sound @blueplanet200 ?
</comment><comment author="mhickman" created="2013-10-09T14:51:56Z" id="25977403">Yeah that's fine, it's what I went with sorry for not updating the issue.

On Wed, Oct 9, 2013 at 2:57 AM, Luca Cavanna &lt;notifications@github.com="mailto:notifications@github.com"&gt;&gt; wrote:

Right, none of the filter/query builders implement the equals method. Not sure if they should though. But they all implement the ToXContent interface, and their toString method returns their json representation. The workaround would be to compare them as strings in your tests. How does that sound @blueplanet200 ?

&#8212;
Reply to this email directly or view it on GitHub.
</comment><comment author="javanna" created="2013-10-09T15:03:36Z" id="25978450">Cool, thanks for your feedback, closing this one then.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Routing with range values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3388</link><project id="" key="" /><description>Hi

I want to index 30 days of data, in each day i will get GB's of data
I Have 30 days of data, i have 10 shards, i want to route in such away , that first 3 days of data should go to first shard, next 3 days of data should go to next shard ..etc

I am routing with date field, but i found data belongs to 1, 10,17 is going to 1 st 2, 11, .. going to next shard...etc . 

Looking for u r response
</description><key id="17195108">3388</key><summary>Routing with range values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">MaheshParimi</reporter><labels /><created>2013-07-25T05:53:21Z</created><updated>2013-07-25T06:13:20Z</updated><resolved>2013-07-25T06:13:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-07-25T06:13:20Z" id="21535097">Please use mailing list for questions. See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation for Cluster Update settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3387</link><project id="" key="" /><description>The documentation has not yet reflected [this](#2509) change. http://www.elasticsearch.org/guide/reference/api/admin-cluster-update-settings/ has no mention of the fact that some threadpool settings can be changed on the fly.
</description><key id="17188167">3387</key><summary>Documentation for Cluster Update settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rdeaton</reporter><labels /><created>2013-07-25T00:18:00Z</created><updated>2013-07-31T14:20:50Z</updated><resolved>2013-07-25T07:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-25T07:42:49Z" id="21538092">Thanks for pointing that out. We have a [separate repository](https://github.com/elasticsearch/elasticsearch.github.com) for the documentation, which everyone can contribute to. Would you like to send a pull request there with your changes? Have a look at http://www.elasticsearch.org/contributing-to-elasticsearch/.

Closing this issue since not related to the elasticsearch core.
</comment><comment author="dadoonet" created="2013-07-31T14:20:50Z" id="21866006">Heya,

I submitted a PR here about that: https://github.com/elasticsearch/elasticsearch.github.com/pull/484
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Possible Threadlocal leak in Tomcat</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3386</link><project id="" key="" /><description>Here is my Java initialization code for use with Spring:

```
public class IndexerClientFactory {

private Client client;
private Node node;

@Autowired
String elasticsearchHostname;

@Autowired
String elasticsearchNode1Name;

public Client searchClient() {

    Settings settings = ImmutableSettings.settingsBuilder()
            .put("node.name", elasticsearchNode1Name)
            .put("network.host", elasticsearchHostname)
            .build();

    node = NodeBuilder.nodeBuilder()
            .settings(settings)
            .data(false)
            .client(true)
            .node();

    client = node.client();

    return client;
}

public void cleanShutdown() {

    if (client != null) {
        client.close();
    }

    if (node != null) {
        node.close();
    }

}
}
```

And the associated Spring xml:

```
&lt;bean destroy-method="cleanShutdown" name="searchClientFactoryBean"
      class="info.afilias.intdir.indexer.context.IndexerClientFactory"/&gt;

&lt;bean name="searchClient" factory-bean="searchClientFactoryBean" factory-method="searchClient"/&gt;
```

I can see cleanShutdown() being called in the logs.

Yet I still see the following also in the logs:

SEVERE: The web application [/indexer] created a ThreadLocal with key of type [org.elasticsearch.common.inject.InjectorImpl$1](value [org.elasticsearch.common.inject.InjectorImpl$1@556f7ce3]) and a value of type [java.lang.Object[]](value [[Ljava.lang.Object;@a7046e7]) but failed to remove it when the web application was stopped. Threads are going to be renewed over time to try and avoid a probable memory leak.
</description><key id="17185001">3386</key><summary>Possible Threadlocal leak in Tomcat</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MattFriedman</reporter><labels /><created>2013-07-24T22:38:38Z</created><updated>2014-08-08T13:40:26Z</updated><resolved>2014-08-08T13:40:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-07-25T02:53:43Z" id="21530426">As far as I remember, it should be relative to:
- https://code.google.com/p/guava-libraries/issues/detail?id=92
- https://code.google.com/p/google-guice/issues/detail?id=288

Sadly it's not fixed yet although a patch seems to exist: https://code.google.com/p/google-guice/issues/detail?id=288#c88

Seems to be the same issue as #282.
</comment><comment author="colinmorelli" created="2014-01-11T01:50:34Z" id="32083334">Is there any chance of the patch being applied to ElasticSearch? My understanding is that the bundled version of Guice in ES is already slightly customized - why not add this fix?
</comment><comment author="clintongormley" created="2014-08-08T13:40:26Z" id="51601368">Closed in favour of #283 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XML format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3385</link><project id="" key="" /><description>To support Elasticsearch in legacy environments, it would be nice to have XML as an output option in the core. For example, applying XSL stylesheets to Elasticsearch output for XML/XHTML formatting is more convenient with XML present, it avoids transforming JSON to XML.

The XML format is realized by adding another jackson dependency, jackson-dataformat-xml.

Extra XML namespaces can be added in a properties file named `xml-namespaces.properties` in the class path, e.g. `dc = http://purl.org/dc/elements/1.1/`. Field names could then start with `dc:` to get resolved.

The JSON output is wrapped into a `root` element with default namespace `http://elasticsearch.org/ns/1.0/` and preferred prefix `es`/
</description><key id="17182170">3385</key><summary>XML format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-07-24T21:31:12Z</created><updated>2014-07-28T09:36:48Z</updated><resolved>2014-07-28T09:36:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-24T21:45:38Z" id="21518659">Few quick notes:
- Why not use common base classes, like `JsonXContentGenerator`?
- We don't use author tags in the javadoc of a class
- can you squash your pull request?

And now for the more complex aspect :). I thought about it a bit, and I am concerned regarding adding XML support. I am concerned that it will lead to questions like, why is it an attribute and not an element, and all the complexity that comes with XML compared to JSON...
</comment><comment author="jprante" created="2013-07-25T07:46:05Z" id="21538200">Yes, there is a burden in XML and we all dislike it. Unfortunately I am surrounded by XML workflows :( 

I can add documentation about the pitfalls and limitations about array/object model, also mentioned in http://wiki.fasterxml.com/JacksonExtensionXmlDataBinding

Another option could be an XML plugin. I can volunteer in supporting a XML plugin so you don't need to worry about customers bothering with XML stuff. Maybe there is a chance to extend the XContents API so that a service factory could instantiate additional formats by using the META-INF/services mechanism?
</comment><comment author="tlrx" created="2013-07-25T16:05:38Z" id="21564869">+1  for external plugin 

It reminds me the view plugin I made few months ago http://tlrx.github.io/elasticsearch-view-plugin/. Such a plugin could support XSLT for specific use cases.
</comment><comment author="kimchy" created="2013-07-25T19:45:00Z" id="21579518">Lets try adn get the pull request sorted, and discuss here if to support it or not. I am on the fence, not saying no :)
</comment><comment author="jprante" created="2013-07-25T19:51:31Z" id="21579911">Sorry for the noise. I'm wrestling with git squashing and finally did remove the branch :(

A cleaned version is here: https://github.com/jprante/elasticsearch/commit/ed3d1b6c986df7da8b9d5942091718bf6556e567 
</comment><comment author="kimchy" created="2013-07-28T23:58:38Z" id="21694006">Lets keep it open for now and see how people react, I am sill digesting if it should be in or not. If not, no problem with allowing to plug other content impls.
</comment><comment author="clintongormley" created="2014-07-28T09:36:48Z" id="50317442">This has been implemented as a plugin https://github.com/jprante/elasticsearch-xml

closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Allow to update/merge the dynamic flag</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3384</link><project id="" key="" /><description>Allow to update / merge in the dynamic flag on object or the root object mapper, meaning that it can be changed dynamically using the update mapping API.
</description><key id="17180607">3384</key><summary>Mapping: Allow to update/merge the dynamic flag</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-24T20:56:55Z</created><updated>2013-07-24T20:57:29Z</updated><resolved>2013-07-24T20:57:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Facet between two non numeric fiels</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3383</link><project id="" key="" /><description>Hi great piece of software

I have been playing around witht the facets and I can't seem to find a way to do the following action:
I would like to count distinct values of field bucketed on another field.
Say I want to count the number of different ip addresses for a given user. 

I have looked at histogram where my key would be user and value would be IP but IP is not numeric.

Is there a way to do this and  have I missed the obvious should I look into making a custom facet is this to memory consuming.
</description><key id="17168972">3383</key><summary>Facet between two non numeric fiels</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nmaillard</reporter><labels /><created>2013-07-24T17:11:18Z</created><updated>2014-08-08T13:37:30Z</updated><resolved>2014-08-08T13:37:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-07-24T17:25:04Z" id="21501182">I think you are looking for aggregations:

https://github.com/elasticsearch/elasticsearch/issues/3300

This functionality is targeted for ES 1.0.
</comment><comment author="uboness" created="2013-07-24T17:34:04Z" id="21501886">@nmaillard, as @mattweber mentioned the future aggregations module will enable you to build your custom "views" and build different types of aggregations. That said, we currently don't have an aggregation dedicated for returning unique value counts per bucket (we do plan to add it, though for high cardinality fields it will be close estimation rather than an exact count)
</comment><comment author="nmaillard" created="2013-07-24T18:25:10Z" id="21505430">thanks for all the replies
good news i'll eagerly be waiting for es 1.0 then
Any idea at all on the roadmap
</comment><comment author="clintongormley" created="2014-08-08T13:37:30Z" id="51601084">The cardinality agg is now available. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose recursion level for Hunspell token filter. Closes #3369</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3382</link><project id="" key="" /><description>Allows to override recursion level for the Hunspell token filter. By default the recursion level is set to 2 which is not suitable for certain languages/dictionaries. For example for `cs_CZ` (czech) language based on OpenOffice dictionaries it gives better results with recursion level 1 or even better 0.

One can override the `recursion_level` using the following configuration:

```
{
    "filter" : {
        "type" : "hunspell",
        "locale" : "cs_CZ",
        "recursion_level" : 0  // optional, defaults to 2
    }
}
```

Note: requires Lucene 4.4
</description><key id="17167511">3382</key><summary>Expose recursion level for Hunspell token filter. Closes #3369</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-07-24T16:41:23Z</created><updated>2014-07-16T21:52:47Z</updated><resolved>2013-07-25T07:53:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-25T07:53:15Z" id="21538470">Thx for your contribution! Included in 0.90 and master branches.. documentation waiting in a PR as well. See https://github.com/elasticsearch/elasticsearch.github.com/pull/482
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating mapping with `ignore_conflicts` parameter timeouts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3381</link><project id="" key="" /><description>When running the following code against Elasticsearch built from 0.90 branch at b2d0802, the response never finishes:

``` bash
curl -X DELETE http://localhost:9200/mapped-index

curl -X POST http://localhost:9200/mapped-index -d '{}'

curl -X PUT "http://localhost:9200/mapped-index/article/_mapping" -d '{"article":{"properties":{"body":{"type":"string"}}}}'

curl -X PUT "http://localhost:9200/mapped-index/article/_mapping" -d '{"article":{"properties":{"body":{"type":"integer"}}}}'

# =&gt; Conflict (OK)

curl -X PUT "http://localhost:9200/mapped-index/article/_mapping?ignore_conflicts=true" -d '{"article":{"properties":{"body":{"type":"integer"}}}}'

# =&gt; Timeout (FAIL)
```
</description><key id="17161413">3381</key><summary>Updating mapping with `ignore_conflicts` parameter timeouts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">karmi</reporter><labels><label>regression</label><label>v0.90.0.Beta1</label><label>v0.90.3</label></labels><created>2013-07-24T14:53:42Z</created><updated>2013-07-25T15:02:46Z</updated><resolved>2013-07-25T14:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-25T15:02:46Z" id="21560170">Side note: the bug was introduced with [this recent commit](https://github.com/elasticsearch/elasticsearch/commit/4930b93c26506e8f063a3b817f0434bd17fca14c), thus it hadn't been released yet. Good catch @karmi !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolating existing document api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3380</link><project id="" key="" /><description>A common use case is to percolate a document that has just been indexed. A big advantage would be to just specify the index, type and id of a document (which is returned with each index response) that has just been indexed, internally a get request can then fetch the content of the recently indexed document and then the percolate api will execute on that. 

Percolate an existing document with id 1 and type tweet from index twitter:

``` bash
curl -XGET 'localhost:9200/twitter/tweet/1/_percolate'
```

Percolate an existing document with id 2 and type tweet from index twitter-2013: (percolating 2013 tweet against 2012 queries)

``` bash
curl -XGET 'localhost:9200/twitter-2013/tweet/2/_percolate?percolate_index=twitter-2012'
```

Additional options for percolating an existing document on top of existing percolator options:
- `id` - The id of the document to retrieve the source for.
- `percolate_index` - The index containing the percolate queries. Defaults to the `index` defined in the url.
- `percolate_type` - The percolate type (used for parsing the document). Default to `type` defined in the url.
- `routing` - The routing value to use when retrieving the document to percolate.
- `preference` - Which shard to prefer when retrieving the existing document. 
- `percolate_routing` - The routing value to use when percolating the existing document.
- `percolate_preference` - Which shard to prefer when executing the percolate request.
- `version` - Enables a version check. If the fetched document's version isn't equal to the specified version then the request fails with a version conflict and the percolation request is aborted.
- `version_type` - Whether internal or external versioning is used. Defaults to internal versioning.

Internally the percolate api will issue a get request for fetching the`_source` of the document to percolate.
For this feature to work the `_source` for documents to be percolated need to be stored.
</description><key id="17160993">3380</key><summary>Percolating existing document api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-07-24T14:46:15Z</created><updated>2014-08-06T09:32:30Z</updated><resolved>2013-07-26T14:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="phrone" created="2013-12-30T14:32:34Z" id="31347947">Hi! Great work, but would it be possible to use a specified document field instead of the _source?
</comment><comment author="martijnvg" created="2013-12-30T22:29:26Z" id="31373050">@phrone This is actually possible via get source filtering [1], but it just needs to be exposed to the percolate rest api.

1: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/docs-get.html#get-source-filtering
</comment><comment author="fransflippo" created="2014-06-26T15:22:10Z" id="47239716">Just what I need! Is this included in the Java API yet?
</comment><comment author="martijnvg" created="2014-06-27T09:19:13Z" id="47323749">@fransflippo Yes, it is included in the Java api. There is a PercolateRequestBuilder#setGetRequest(GetRequest) method. The passed get request is internally used to fetch the document needed for percolating.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cluster malfunctioning </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3379</link><project id="" key="" /><description>I have 4 nodes each running on the separate ip. All are confugured for the same unicast.hosts in elasticsearch.yml : 
discovery.zen.ping.unicast.hosts: ["172.20.37.247", "172.20.32.141"]

If I do restart to ES on 172.20.32.141, the cluster nodes repair:
[root@yaniv-db ~]# curl -XGET 'http://localhost:9200/_nodes?pretty'
{
  "ok" : true,
  "cluster_name" : "attlogs",
  "nodes" : {
    "6yH7OSdwQ2eFPVyJV94qbw" : {
      "name" : "Terrax the Tamer",
      "transport_address" : "inet[/172.16.252.30:9300]",
      "hostname" : "yaniv-db.infra.attucdev.com",
      "version" : "0.90.0",
      "http_address" : "inet[/172.16.252.30:9200]"
    },
    "_kz9M5u4TmeTBdQPt2BAOw" : {
      "name" : "Illusion",
      "transport_address" : "inet[/172.20.32.141:9300]",
      "hostname" : "wxs-141",
      "version" : "0.90.0",
      "http_address" : "inet[/172.20.32.141:9200]"
    },
    "n6FQ0tEIR9S_sYIvJKUVZA" : {
      "name" : "Citizen V",
      "transport_address" : "inet[/172.20.37.247:9300]",
      "hostname" : "infra247.tasdev.com",
      "version" : "0.90.0",
      "http_address" : "inet[/172.20.37.247:9200]"
    },
    "x6Nl9thJRier2UcPnSHT7g" : {
      "name" : "Powerpax",
      "transport_address" : "inet[/172.20.20.130:9300]",
      "hostname" : "oracle",
      "version" : "0.90.0",
      "http_address" : "inet[/172.20.20.130:9200]"
    }
  }
But if I do restart of ES on 172.20.37.247,  I have to detached clusters:
}[root@wxs-141 ~]# curl -XGET 'http://172.20.37.247:9200/_nodes?pretty'
{
  "ok" : true,
  "cluster_name" : "attlogs",
  "nodes" : {
    "QX_Nz_PARIaRBqFmvdo_tg" : {
      "name" : "Digitek",
      "transport_address" : "inet[/172.20.37.247:9300]",
      "hostname" : "infra247.tasdev.com",
      "version" : "0.90.0",
      "http_address" : "inet[/172.20.37.247:9200]"
    }
  }
}[root@wxs-141 ~]# 
[root@wxs-141 ~]# 
[root@wxs-141 ~]# 
[root@wxs-141 ~]# 
[root@wxs-141 ~]# curl -XGET 'http://172.20.32.141:9200/_nodes?pretty'
{
  "ok" : true,
  "cluster_name" : "attlogs",
  "nodes" : {
    "6yH7OSdwQ2eFPVyJV94qbw" : {
      "name" : "Terrax the Tamer",
      "transport_address" : "inet[/172.16.252.30:9300]",
      "hostname" : "yaniv-db.infra.attucdev.com",
      "version" : "0.90.0",
      "http_address" : "inet[/172.16.252.30:9200]"
    },
    "x6Nl9thJRier2UcPnSHT7g" : {
      "name" : "Powerpax",
      "transport_address" : "inet[/172.20.20.130:9300]",
      "hostname" : "oracle",
      "version" : "0.90.0",
      "http_address" : "inet[/172.20.20.130:9200]"
    },
    "1KddTsgKSVeNxTG7v9kQ4w" : {
      "name" : "Silver Fox",
      "transport_address" : "inet[/172.20.32.141:9300]",
      "hostname" : "wxs-141",
      "version" : "0.90.0",
      "http_address" : "inet[/172.20.32.141:9200]"
    }
  }
[root@wxs-141 ~]# 

I wonder what is wrong in my configuration and what is the difference between 2 unicast hosts.
</description><key id="17155461">3379</key><summary>cluster malfunctioning </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ala123</reporter><labels /><created>2013-07-24T13:12:11Z</created><updated>2013-07-24T14:08:19Z</updated><resolved>2013-07-24T14:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-24T14:08:19Z" id="21486995">Could you please send this kind of question to the elasticsearch google group (https://groups.google.com/forum/#!forum/elasticsearch)?

Github issues are more for bugs that you find or enhancements that you would like to be implemented.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose recursion level for Hunspell token filter. Closes #3369</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3378</link><project id="" key="" /><description>Allows to override recursion level for the Hunspell token filter. By default the recursion level is set to 2 which is not suitable for certain languages/dictionaries. For example for `cs_CZ` (czech) language based on OpenOffice dictionaries it gives better results with recursion level 1 or even better 0.

One can override the `recursion_level` using the following configuration:

```
{
    "filter" : {
        "type" : "hunspell",
        "locale" : "cs_CZ",
        "recursion_level" : 0  // optional, defaults to 2
    }
}
```

Note: requires Lucene 4.4
</description><key id="17152919">3378</key><summary>Expose recursion level for Hunspell token filter. Closes #3369</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-07-24T12:09:30Z</created><updated>2014-07-16T21:52:47Z</updated><resolved>2013-07-24T16:38:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-07-24T16:38:24Z" id="21498039">Closing, I am going to prepare a new PR adapted to https://github.com/elasticsearch/elasticsearch/commit/ed473e272dfa369de771358a3c6a9b1075dd3d43
</comment><comment author="lukas-vlcek" created="2013-07-24T16:42:20Z" id="21498286">Updated PR: https://github.com/elasticsearch/elasticsearch/pull/3382
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API ignores mapping when multi-field fully qualified path name is identical</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3377</link><project id="" key="" /><description>Here is a mapping setting up a multi-field.  We are going to try using the Analyze API on both "brand" and "brand.brand", which you would expect to give the same results.  Instead, it appears that the mapping is being ignored in the "brand.brand" case.

``` bash
curl -XPOST localhost:9200/test_multifield_same_name -d '{
    "mappings" : {
        "type" : {
            "properties" : {
                "brand" : {
                    "type" : "multi_field",
                    "fields" : {
                        "brand" : { "type" : "string", "analyzer" : "whitespace" },
                        "untouched" : { "type" : "string", "index" : "not_analyzed" }
                    }
                }
            }
        }
    }
}'
```

The field using just the simple path name, works fine (applies whitespace):

``` bash
curl -XPOST "localhost:9200/test_multifield_same_name/_analyze?pretty=true&amp;field=brand" -d 'This Is Just A Test With Capitalized Words'

{
  "tokens" : [ {
    "token" : "This",
    "start_offset" : 0,
    "end_offset" : 4,
    "type" : "word",
    "position" : 1
  }, {
    "token" : "Is",
    "start_offset" : 5,
    "end_offset" : 7,
    "type" : "word",
    "position" : 2
  }, {
    "token" : "Just",
    "start_offset" : 8,
    "end_offset" : 12,
    "type" : "word",
    "position" : 3
  }, {
    "token" : "A",
    "start_offset" : 13,
    "end_offset" : 14,
    "type" : "word",
    "position" : 4
  }, {
    "token" : "Test",
    "start_offset" : 15,
    "end_offset" : 19,
    "type" : "word",
    "position" : 5
  }, {
    "token" : "With",
    "start_offset" : 20,
    "end_offset" : 24,
    "type" : "word",
    "position" : 6
  }, {
    "token" : "Capitalized",
    "start_offset" : 25,
    "end_offset" : 36,
    "type" : "word",
    "position" : 7
  }, {
    "token" : "Words",
    "start_offset" : 37,
    "end_offset" : 42,
    "type" : "word",
    "position" : 8
  } ]
}
```

But if we refer to the field using the fully qualified path name, it looks like Elasticsearch applies the default (Standard) analyzer instead of whitespace:

``` bash
curl -XPOST "localhost:9200/test_multifield_same_name/_analyze?pretty=true&amp;field=brand.brand" -d 'This Is Just A Test With Capitalized Words'

{
  "tokens" : [ {
    "token" : "just",
    "start_offset" : 8,
    "end_offset" : 12,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 3
  }, {
    "token" : "test",
    "start_offset" : 15,
    "end_offset" : 19,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 5
  }, {
    "token" : "capitalized",
    "start_offset" : 25,
    "end_offset" : 36,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 7
  }, {
    "token" : "words",
    "start_offset" : 37,
    "end_offset" : 42,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 8
  } ]
}
```

(Related to issue from https://github.com/polyfractal/elasticsearch-inquisitor/issues/21)
</description><key id="17151523">3377</key><summary>Analyze API ignores mapping when multi-field fully qualified path name is identical</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">polyfractal</reporter><labels /><created>2013-07-24T11:28:46Z</created><updated>2014-07-08T19:29:27Z</updated><resolved>2014-07-08T19:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-25T14:54:51Z" id="21559532">I think this is expected behavior. According to [documentation](http://www.elasticsearch.org/guide/reference/mapping/multi-field-type/): "With multi_field mapping, the field that has the same name as the property is treated as if it was mapped without a multi field. That&#8217;s the &#8220;default&#8221; field. It can be accessed regularly, for example using name or using typed navigation tweet.name."
</comment><comment author="polyfractal" created="2013-08-15T23:00:03Z" id="22737066">Hmm, that seems counter-intuitive to me.  `type.brand.brand` and `type.brand.untouched` are the fully qualified paths, yet `type.brand.brand` acts as if it is undefined and uses the Standard analyzer.

I feel that `brand`, `type.brand` and `type.brand.brand` should be equivalent.  The first two maintain backwards compatibility from before multi-fields were introduced, the third is the true fully qualified path.

It's mostly a cosmetic issue, I agree, but it is not what one would expect when working with full paths.
</comment><comment author="clintongormley" created="2014-07-08T19:29:27Z" id="48388472">No longer relevant because of the change in multi-fields. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Completion prefix suggestion </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3376</link><project id="" key="" /><description>**Note**: This is an experimental feature!

Traditionally FST suggesters needed to create an in-memory structure upfront, which needed to be in sync with the data inserted/deleted. This step to create a FST can be really expensive and long lasting on production systems.

So, why not trying to create an efficient FST alike structure on index time, load that quickly into memory and use this for suggestions?

Before deep diving into implementation details, let's start with a small sample
# Sample

Create a simple mapping

```
curl -X DELETE localhost:9200/music
curl -X PUT localhost:9200/music

curl -X PUT localhost:9200/music/song/_mapping -d '{
  "song" : {
        "properties" : {
            "name" : { "type" : "string" },
            "suggest" : { "type" : "completion",
                          "index_analyzer" : "stopword",
                          "search_analyzer" : "simple",
                          "payloads" : true
            }
        }
    }
}'

curl -X PUT 'localhost:9200/music/song/1?refresh=true' -d '{
    "name" : "Nevermind",
    "suggest" : { 
        "input": [ "Nevermind", "Nirvana" ],
        "output": "Nirvana - Nevermind",
        "payload" : { "artistId" : 2321 }
    }
}'
```

A request looks like this

```
curl -X POST 'localhost:9200/music/_suggest' -d '{
    "song-suggest" : {
        "text" : "nev",
        "completion" : {
            "field" : "suggest"
        }
    }
}'
```

This is the response

```
{
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "song-suggest" : [ {
    "text" : "nev",
    "offset" : 0,
    "length" : 10,
    "options" : [ {
      "text" : "Nirvana - Nevermind",
      "score" : 1.0, "payload" : {"artistId":2321}
    } ]
  } ]
}
```

As you can see, the text returned is the provided output during indexing. Also the payload is included, which might carry a reference ID to the artist and thus makes it easy to retrieve further information.
# Mapping options

In order to support prefix suggestion the field has to be marked as type `completion`.

```
{
    ...
     "properties" : {
        "suggestField" {
            "type" : "completion"
            "index_analyzer" : "stopword",
            "search_analyzer" : "simple",
        }
    }
}
```

While the `type` field is mandatory, the `index_analyzer` and `search_analyzer` fields can be omitted. The `simple` analyzer is used by default.
## Payloads

If you want to return payloads, you have to explicitely enable them by using `payloads: true` - payloads can contain arbitrary JSON, but must be a JSON object, with opening `{` and closing `}` - no pure strings or arrays allowed.
## Preserve separators

In addition, you can set `preserve_separators: false` in case you in case you want to return "Foo Fighters" when searching for "foof" (using the correct analyzer of course).
## Preserve position increments

You can set `preserve_position_increments: false` in order to not count increase position increments, which is needed if the first word is a stopword and you are using an analyzer to filter out stopwords. This would allow you to suggest for `b` and get back `The Beatles`
# Indexing
## Simple case

The most simple case to index is like this

```
"suggestField" : [ "The Prodigy Firestarter", "Firestarter"]
```

Depending on the analyzer used
## Outputs

Defining an output will always return the output for a found suggestion.

```
"suggestField" : { 
  "input" : [ "The Prodigy Firestarter", "Firestarter"],
  "output" : "The Prodigy, Firestarter",
}
```
## Weights

You should define custom weights instead of relying on the default one (see the drawbacks section). The weight must be an positive integer (**no float**) and defines the order of your suggestions.

```
"suggestField" : { 
  "input" : [ "The Prodigy Firestarter", "Firestarter"],
  "output" : "The Prodigy, Firestarter",
  "weight" : 42
}
```

Also custom weights can make your suggestions valuable. Using weights you could boost the most played song or the best rated hotel first in your suggestions.
# Search

Searches are working exactly like the phrase and term suggesters

```
curl -X POST 'localhost:9200/music/_suggest' -d '{
    "song-suggest" : {
        "text" : "nev",
        "completion" : {
            "field" : "suggest"
        }
    }
}'
```
# Drawbacks
## Using term frequency as default weight

If you do not specify a weight, the term frequency is used. This only makes sense if you optimize to a single segment or have large segments. If you do not, having custom weights might yield the results you are awaiting. So using term frequences as a weight indicator is not the best solution and you should set weight yourself.
</description><key id="17147252">3376</key><summary>Completion prefix suggestion </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>feature</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-24T09:29:30Z</created><updated>2013-08-06T14:06:17Z</updated><resolved>2013-08-01T06:56:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added a depth count to XContentParser and uses it to verify query/filter/search elements parsers adhere to the contract of consuming all elements in their scope.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3375</link><project id="" key="" /><description>Fixed an issue with FuzzLikeThisFieldQueryParser discovered by the above check.
Match all filter parser now correctly skips all the children of its scope.
Match all query parser correctly deals with wrong internal structures and reports the deprecation of norms_field (which was silently ignored).

Closes #3292
</description><key id="17144900">3375</key><summary>Added a depth count to XContentParser and uses it to verify query/filter/search elements parsers adhere to the contract of consuming all elements in their scope.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-07-24T08:29:39Z</created><updated>2014-07-07T07:27:26Z</updated><resolved>2013-09-30T07:35:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-25T12:53:15Z" id="21551449">+1 - huge usability boost in terms of people understand wrong JSON
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FuzzyLikeThisFieldQueryBuilder defaulted failOnUnsupportedField inconsistently to the REST api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3374</link><project id="" key="" /><description>The builder starts with it failOnUnsupportedField set to false, while the REST api defaults to true.
</description><key id="17143736">3374</key><summary>FuzzyLikeThisFieldQueryBuilder defaulted failOnUnsupportedField inconsistently to the REST api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-24T07:51:51Z</created><updated>2013-07-24T07:58:49Z</updated><resolved>2013-07-24T07:58:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Validate query validates anything as true on empty cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3373</link><project id="" key="" /><description>```
# curl -X POST localhost:9200/_validate/query -d 'not a query' 
{"valid":true,"_shards":{"total":0,"successful":0,"failed":0}} 
```
</description><key id="17136227">3373</key><summary>Validate query validates anything as true on empty cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">HonzaKral</reporter><labels /><created>2013-07-24T02:09:58Z</created><updated>2014-05-12T10:11:55Z</updated><resolved>2014-05-10T20:14:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-27T13:04:59Z" id="21665134">I wonder what we should do here, probably return

```
SearchPhaseExecutionException[Failed to execute phase [initial], No indices / shards to search on, requested indices are []]
```

like we do when searching on an empty cluster?
</comment><comment author="javanna" created="2014-05-09T22:57:54Z" id="42722187">This is not true anymore, we currently return:

```
{"error":"IndexMissingException[[_all] missing]","status":404}
```

when validating on empty cluster. On the other hand the search api doesn't return error anymore by default.

Are we good with this behaviour? Thoughts @clintongormley ?
</comment><comment author="clintongormley" created="2014-05-10T12:16:29Z" id="42740197">@javanna I'm good with the current behaviour.
</comment><comment author="javanna" created="2014-05-10T20:14:45Z" id="42753040">Cool closing then ;)
</comment><comment author="javanna" created="2014-05-12T10:11:55Z" id="42815713">Added a test for this as well in #6114.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ridiculously long Scroll_id</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3372</link><project id="" key="" /><description>Does Scroll_id get longer with the number of shards? I have 500 shards (for using custom routing and speeding up search queries), and I when I tried to do a scan search, I got back a scroll_id of length 20708. 

My guess is that this is because the scroll_id is some id for each shard concatenated together. (Is my guess completely off?) Is there a way to make each shard use the same id, or have a table that translates a public scroll_id that's returned to one that's used internally?
</description><key id="17134005">3372</key><summary>Ridiculously long Scroll_id</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottc52</reporter><labels /><created>2013-07-24T00:43:54Z</created><updated>2013-07-24T00:48:15Z</updated><resolved>2013-07-24T00:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-24T00:48:14Z" id="21456904">@scottc52, could you repost your question on the mailing list? See http://www.elasticsearch.org/help/ for more information. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[re-open 3180] Update ttl after update document </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3371</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/issues/3180

ok, can you add parameter "autoupdate" : [true|false] for _ttl?

``` json
{
  "product": {
    "dynamic": true,
    "_ttl": {
      "autoupdate": true,
      "enabled": true,
      "default": 864000000
    },
    "properties": {
      "title": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}
```
</description><key id="17124311">3371</key><summary>[re-open 3180] Update ttl after update document </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">max-inetgiant</reporter><labels /><created>2013-07-23T20:44:03Z</created><updated>2015-08-26T14:15:28Z</updated><resolved>2015-08-26T14:15:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2015-08-26T14:15:28Z" id="135035979">I don't think we should add more parameters to the update API for something that can be easily addressed from client side. Additionally, `_ttl` already has traps, I'm on the fence about adding even more traps.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Git build info when we build a distribution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3370</link><project id="" key="" /><description>Would be nice to have git data (like commit hash, UTC time, ...) of a build of ES.

Initial thoughts are we should have a `Build` class similar to `Version` class (we don't want to have this info as part of Version, since we serialize it on each request), that would read its info from a properties file that gets included in our jar file. We can use this plugin: http://mojo.codehaus.org/buildnumber-maven-plugin/ to do it potentially.

We should return the Build info on top of the Version info in the following cases:
- when doing `GET /`
- `bin/elasticsearch -v`
- when the server starts up
</description><key id="17109253">3370</key><summary>Add Git build info when we build a distribution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-23T15:58:35Z</created><updated>2013-07-24T18:25:14Z</updated><resolved>2013-07-24T18:25:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose recursion level for Hunspell token filter (post Lucene 4.4 upgrade)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3369</link><project id="" key="" /><description>After upgrade to Lucene 4.4 it would be very useful to expose recursion level in Hunspell as a configuration parameter in ES too. Here is related Lucene ticket:
https://issues.apache.org/jira/browse/LUCENE-4542

Let me know if you want patch for this, should be simple.
</description><key id="17107182">3369</key><summary>Expose recursion level for Hunspell token filter (post Lucene 4.4 upgrade)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-23T15:23:26Z</created><updated>2013-07-25T07:17:53Z</updated><resolved>2013-07-25T07:17:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add pending cluster tasks api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3368</link><project id="" key="" /><description>Add an api that shows how many cluster state related tasks (like create index, fail shard etc) are pending. 

Usually this will return an empty lists of tasks, because cluster state related tasks execute fast. This api is useful for diagnostic purposes. Rest api:

``` bash
curl -XGET 'localhost:9200/_cluster/pending_tasks'
```
</description><key id="17099961">3368</key><summary>Add pending cluster tasks api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-23T13:14:42Z</created><updated>2013-07-23T13:37:38Z</updated><resolved>2013-07-23T13:34:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Different numHits 0.90.1 vs 0.90.2 filter with lookup terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3367</link><project id="" key="" /><description>Hi,

When I run an identical query on 0.90.1 and 0.90.2 I get different results:

``` json
{
  "filter": {
    "terms": {
      "field": {
        "id": "bar",
        "type": "doctype",
        "path": "lookupField"
      }
    }
  },
  "query": {
    "match_all": {}
  }
}
```

In 0.90.1 when filter returned no results the result of the query was also empty, i.e. 0 hits. 
In 0.90.2 it returns everything. ~~The latter behavior is probably better and more expected~~, but I haven't seen it documented anywhere. Did I miss something?
</description><key id="17098523">3367</key><summary>Different numHits 0.90.1 vs 0.90.2 filter with lookup terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">scoolen</reporter><labels /><created>2013-07-23T12:42:51Z</created><updated>2013-07-24T12:33:56Z</updated><resolved>2013-07-24T12:33:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="scoolen" created="2013-07-23T13:22:39Z" id="21413096">There is a little bit more going on...

0.90.2 returns a `MatchNoDocsQuery` when it is the only filter applied, but returns **all** results when combined with a filter that returns everything (e.g. via an _and_ filter).
</comment><comment author="martijnvg" created="2013-07-23T14:01:30Z" id="21415798">@scoolen Do you have a particular test case for this change in behaviour? I'm curious if the document specified in the terms filter exists or if the path field exists. 

I think the 0.90.1 behaviour is expected behaviour in most cases.
</comment><comment author="scoolen" created="2013-07-23T14:12:30Z" id="21416596">Yes, I tested a couple of scenarios while switching back and forth between version 0.90.1 and 0.90.2:
1. Test with single lookup terms filter
2. Test with compound filter (type filter AND lookup)
3. Test with compound filter with just a single sub filter (lookup terms)

I tested these three scenarios with known values:
1. Empty lookup (no results)
2. No matching document lookup
3. Lookup with expected number of results
</comment><comment author="martijnvg" created="2013-07-23T21:44:53Z" id="21448426">@scoolen I can confirm the difference in behaviour in both versions. It isn't reproducible in the current 0.90 and master branch. The following unreleased commit fixes this issue: https://github.com/elasticsearch/elasticsearch/commit/74a7c46b0e0e4c921e30574214da6b77ef354b18

In case a filter is resolved to `null` which for example happens when a terms filter lookup can't find a document (but potentially also other filters return `null`) and in the case when there is no query, this just returns all docs. This issue has been has been fixed with the above commit.

In 0.90.1 the terms filter didn't return `null` and that is why this behaviour didn't occur in that version.
</comment><comment author="Dynom" created="2013-07-24T07:07:38Z" id="21467392">Excellent news!
</comment><comment author="Dynom" created="2013-07-24T07:09:09Z" id="21467457">For reference: this seems like a duplicate of https://github.com/elasticsearch/elasticsearch/issues/3356
</comment><comment author="scoolen" created="2013-07-24T07:16:34Z" id="21467690">@martijnvg Ok cool. For now we will stick to 0.90.1 then. Thanks for your help.
</comment><comment author="martijnvg" created="2013-07-24T12:33:56Z" id="21481324">@Dynom This is a duplicate of #3356, it isn't specific to just `terms` filter. I will close this issue now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add optimize thread pool (size 1) dedicated to perform explicit optimize API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3366</link><project id="" key="" /><description>Have a dedicated thread pool for explicit `optimize` calls (shard level optimize operations). By default, the size should be 1 to work the same with how things work currently allowing for only 1 shard level optimize on a node.

The change allows to see the optimize thread pool stats now, and potentially increase the thread pool size for beefy machines.
</description><key id="17090681">3366</key><summary>Add optimize thread pool (size 1) dedicated to perform explicit optimize API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-23T09:04:49Z</created><updated>2013-07-23T09:05:25Z</updated><resolved>2013-07-23T09:05:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cluster State Update APIs (master node) to respect master_timeout better</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3365</link><project id="" key="" /><description>Currently, the master node might be processing too many cluster state events, and then be blocked on waiting for its respective even to be processed. We can use the new cluster state update timeout support to use the master_timeout value and respect it.
</description><key id="17050912">3365</key><summary>Cluster State Update APIs (master node) to respect master_timeout better</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-22T14:57:29Z</created><updated>2013-07-22T14:58:40Z</updated><resolved>2013-07-22T14:58:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Hang when modifying the cluster state and an uncaught exception is thrown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3364</link><project id="" key="" /><description>As mentioned in #3363 we don't decrement the CountDownLatch and never return a response if there's an uncaught exception in some of the TransportMasterNodeOperationActions that modify the cluster state. Some actions do catch Throwable and don't suffer from this issue, while some others don't.

We need to go over those and fix this issue so that we always decrement the CountDownLatch and give back the error that was thrown instead of hanging waiting for the thread to finish its execution.
</description><key id="17046738">3364</key><summary>Hang when modifying the cluster state and an uncaught exception is thrown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-22T13:40:20Z</created><updated>2013-07-23T14:23:16Z</updated><resolved>2013-07-23T14:23:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-23T14:23:16Z" id="21417341">Solved with 4930b93c26506e8f063a3b817f0434bd17fca14c .

Removed CountDownLatch in all the master operations and introduced a global Listener that is always notified when something goes wrong.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when trying to add single alias without index or alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3363</link><project id="" key="" /><description>Missing validation when trying to add a single alias and index or alias (or both) are missing. The following requests lead to a NullPointerException:

curl -XPUT localhost:9200/_alias
curl -XPUT localhost:9200/index1/_alias
curl -XPUT localhost:9200/_alias/alias1

We need to add proper validation for such cases. Also, in those cases the request never returns due to a missing CountDownLatch#countDown when there's an uncaught exception, which we are going to address on a separate issue.
</description><key id="17041149">3363</key><summary>NullPointerException when trying to add single alias without index or alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-22T11:07:30Z</created><updated>2013-07-22T13:40:20Z</updated><resolved>2013-07-22T13:30:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Transport: Add a dedicated ping channel</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3362</link><project id="" key="" /><description>Today, we have a low/med/high channel groups in our transport layer. High is used to publish cluster state and send ping requests. Sometimes, the overhead of publishing large cluster states can interfere with ping requests.

Introduce a new, dedicated ping channel (with size 1) to have a channel that only handles ping requests.
</description><key id="17035094">3362</key><summary>Transport: Add a dedicated ping channel</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-22T08:14:00Z</created><updated>2013-07-22T08:30:08Z</updated><resolved>2013-07-22T08:30:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Zen Discovery Cluster Events to have Priority.URGENT</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3361</link><project id="" key="" /><description>Master node cluster state events resulting in zen discovery (node gets added, removed, for example) should be processed with priority URGENT as its always better to process them as fast as possible, and not let other events get in the way.
</description><key id="17018636">3361</key><summary>Zen Discovery Cluster Events to have Priority.URGENT</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-21T12:57:08Z</created><updated>2013-11-01T11:42:15Z</updated><resolved>2013-07-21T12:57:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a "score_mode" parameter to rescoring query.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3360</link><project id="" key="" /><description>Default value is "total", possible values are: "max", "min", "avg",
"multiply" and "total".
- "total": the final score of a document is the sum of the original
  query score with the rescore query score.
- "max": only the highest score count.
- "min": only the lowest score is kept (if the document doesn't match
  the rescore query, the original query score is used).
- "avg": average of both scores
- "multiply": product of both scores

Closes #3258
</description><key id="17007368">3360</key><summary>Add a "score_mode" parameter to rescoring query.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hc</reporter><labels /><created>2013-07-20T15:05:10Z</created><updated>2014-06-25T21:21:23Z</updated><resolved>2013-07-26T10:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-07-26T10:32:41Z" id="21613110">pushed - no idea why github doesn't get it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>[0.90.0, 0.90.2] Can't use empty replacement string in pattern_replace filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3359</link><project id="" key="" /><description>After upgrading from 0.20.0.rc1 to 0.90.2, this happens when we use blank replacement string in pattern_replace filter:

```
...
org.elasticsearch.indices.IndexCreationException: [xxx] failed to create index
    at org.elasticsearch.indices.InternalIndicesService.createIndex(InternalIndicesService.java:382)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:296)
    at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:162)
    at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:321)
    at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:95)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:636)
Caused by: org.elasticsearch.ElasticSearchIllegalArgumentException: replacement is missing for [whitespace_remove] token filter of type 'pattern_replace'
    at org.elasticsearch.index.analysis.PatternReplaceTokenFilterFactory.&lt;init&gt;(PatternReplaceTokenFilterFactory.java:54)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
    at org.elasticsearch.common.inject.InjectorImpl$4$1.call(InjectorImpl.java:763)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.InjectorImpl$4.get(InjectorImpl.java:759)
    at org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:221)
    at $Proxy19.create(Unknown Source)
    at org.elasticsearch.index.analysis.AnalysisService.&lt;init&gt;(AnalysisService.java:152)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
    at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
    at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
    at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
    at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
    at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)
    at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
    at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
    at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
    at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
    at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
    at
...
```

Our elasticsearch.json configuration looks like this (note `whitespace_remove` at the bottom, we need that to strip any whitespace in between characters):

``` json
{
  "cluster": {
    "name": "1188_production_19_07"
  },
  "action": {
    "auto_create_index": false
  },
  "indices": {
    "memory": {
      "index_buffer_size": "1024m"
    }
  },
  "index": {
    "number_of_replicas": 2,
    "number_of_shards": 3,
    "analysis": {
      "analyzer": {
        "autocomplete_exact_index_analyzer": {
          "type": "custom",
          "tokenizer": "keyword",
          "filter": [
            "standard",
            "lowercase",
            "edge_ngram"
          ]
        },
        "autocomplete_exact_search_analyzer": {
          "type": "custom",
          "tokenizer": "keyword",
          "filter": [
            "standard",
            "lowercase"
          ]
        },
        "autocomplete_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "standard",
            "lowercase",
            "edge_ngram"
          ]
        },
        "autocomplete_search_analyzer": {
          "type": "custom",
          "tokenizer": "keyword",
          "filter": [
            "whitespace_remove",
            "lowercase"
          ]
        },
        "ascii_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase",
            "czech_stem",
            "asciifolding"
          ]
        },
        "ascii_search_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase",
            "czech_stem",
            "asciifolding"
          ]
        },
        "untouched_ascii_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase",
            "asciifolding"
          ]
        },
        "untouched_ascii_search_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase",
            "asciifolding"
          ]
        },
        "czech_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase",
            "czech_stem"
          ]
        },
        "czech_search_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase",
            "czech_stem"
          ]
        },
        "untouched_czech_search_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase"
          ]
        },
        "untouched_czech_index_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "language": "Czech",
          "filter": [
            "standard",
            "lowercase"
          ]
        }
      },
      "filter": {
        "full_ngram": {
          "type": "nGram",
          "min_gram": "1",
          "max_gram": "20"
        },
        "edge_ngram": {
          "type": "edgeNGram",
          "min_gram": 1,
          "max_gram": 20,
          "side": "front"
        },
        "whitespace_remove": {
          "type": "pattern_replace",
          "pattern": " ",
          "replacement": ""
        }
      }
    }
  }
}
```

I tried to search when this change occurred, so I tried it on 0.90.0, 0.90.1 and 0.90.2. They all produce the same result.

I've tried to look around and thought the JSON parser was the culprit, so I went digging and discovered, that JSON Loader returns null value for empty strings in JSONs, but YML loader does return empty strings for YMLs (which is by itself strange and should not happen imo :)) ) by adding relevant values to test-settings.json and test-settings.yml + their tests:

https://gist.github.com/NoICE/6039088
(note these added lines: https://gist.github.com/NoICE/6039088#file-jsonsettingsloadertests-java-L51-L53
https://gist.github.com/NoICE/6039088#file-yamlsettingsloadertests-java-L51-L53)

So I rewrote our elasticsearch.json to elasticsearch.yml, but the error still remains.

So:
- JSON parser returns null for "" values
- YML parser does return "" for "" values
- pattern replace does not allow empty string in either JSON or YML format (so we can rule out JSON parser is the culprit, maybe...)

Let me know if I can provide some more tests or something...
</description><key id="16975760">3359</key><summary>[0.90.0, 0.90.2] Can't use empty replacement string in pattern_replace filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">NoICE</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-19T15:07:35Z</created><updated>2013-07-19T17:14:36Z</updated><resolved>2013-07-19T17:14:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-19T15:09:23Z" id="21255398">It happens because we change a bit the logic in our settings behavior to have empty strings represent no value set (for other reasons). We can fix it in the pattern replace one easily...
</comment><comment author="NoICE" created="2013-07-19T15:12:51Z" id="21255648">Cool :) So if everything goes well, the fix will be available in 0.90.3? Can I help with something?
</comment><comment author="kimchy" created="2013-07-19T17:14:36Z" id="21263397">Simple fix, pushed it: eb75a815dbb4bd6c2035e4147e472d5d7ec667a4.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch.log integrate to Kibana</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3358</link><project id="" key="" /><description>Hi to All,

I am working on elastic search (search engine) i need to do 2 things,
1.Elastic Search.log(consists of resources information &amp; database data) into kibana.(Directly without logstach).
2.Elastic Search.log into kibana.(using logstach).
</description><key id="16974609">3358</key><summary>ElasticSearch.log integrate to Kibana</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vasanthakumarr</reporter><labels /><created>2013-07-19T14:46:29Z</created><updated>2013-07-22T07:43:23Z</updated><resolved>2013-07-22T07:43:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-22T07:43:23Z" id="21328997">please stop using github issues as a support channel, this is the reason for the google group at https://groups.google.com/forum/#!forum/elasticsearch

Also, if you want to get help, you should read http://www.elasticsearch.org/help in order to raise the chances of a good solution.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting doesn't work with term vectors enabled and some complex queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3357</link><project id="" key="" /><description>This is related to https://issues.apache.org/jira/browse/LUCENE-4734. If you have term vectors enabled and try to highlight a proximity query or a phrase which has eg. 2 terms at the same position, no snippet will be returned.
</description><key id="16969287">3357</key><summary>Highlighting doesn't work with term vectors enabled and some complex queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-19T12:51:28Z</created><updated>2013-09-23T17:21:23Z</updated><resolved>2013-07-22T17:44:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Missing filter works differently in top-level versus filtered query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3356</link><project id="" key="" /><description>Given the following script: https://gist.github.com/dakrone/6034875

I was expecting the same output for both queries (no documents matched), but instead I get:

```
&#8756; ./missing-filter-bug.zsh
Regular filter for null_value=true, existence=false
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "missing-test",
      "_type" : "doc",
      "_id" : "1",
      "_score" : 1.0, "_source" : { "id": "1" }
    }, {
      "_index" : "missing-test",
      "_type" : "doc",
      "_id" : "2",
      "_score" : 1.0, "_source" : { "id": "2", "myfield": "foo" }
    } ]
  }
}
Filtered query with filter for null_value=true, existence=false
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 0,
    "max_score" : null,
    "hits" : [ ]
  }
}
```
</description><key id="16954853">3356</key><summary>Missing filter works differently in top-level versus filtered query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-19T03:25:41Z</created><updated>2013-11-06T15:32:35Z</updated><resolved>2013-07-19T11:12:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-19T11:10:17Z" id="21244093">I hope its intentional the usage of null value set to true on hte missing filter, and not really have a null value on the mapping. It is a bug, which happens because the missing filter returns a null filter which ends up not matching anything with a filtered query, compared to top level filter.
</comment><comment author="dakrone" created="2013-07-19T14:01:56Z" id="21251067">Yea, it was intentional, just trying to make the reproduce script as small as possible. Thanks for the fix!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limits are not consumed using Systemd (ulimit -n / ulimit -l)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3355</link><project id="" key="" /><description>See http://www.freedesktop.org/software/systemd/man/systemd.exec.html
- LimitNOFILE
- LimitMEMLOCK
</description><key id="16946541">3355</key><summary>Limits are not consumed using Systemd (ulimit -n / ulimit -l)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">skymeyer</reporter><labels /><created>2013-07-18T22:05:49Z</created><updated>2014-09-23T16:33:07Z</updated><resolved>2013-07-19T16:29:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="skymeyer" created="2013-07-18T22:36:29Z" id="21220195">Tested on openSUSE 12.3 x86_64 using rpm package for 0.90.2:
https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.2.noarch.rpm

Elastic Search is (re)started using the included script "/etc/rc.d/init.d/elasticsearch"

## Reproduction

### Limit config

/etc/security/limits.conf

```
elasticsearch          soft    nofile          65535
elasticsearch          hard    nofile          65535
elasticsearch          -       memlock         unlimited
```

### Verify limits

```
linux-drwq:~ # su -s /bin/bash -c "ulimit -n" elasticsearch
65535
linux-drwq:~ # su -s /bin/bash -c "ulimit -l" elasticsearch
unlimited
```

### Max open files test case

/etc/sysconfig/elasticsearch:

```
# Maximum number of open files
MAX_OPEN_FILES=65535
```

Result max_open_files:

```
/etc/rc.d/init.d/elasticsearch restart
[2013-07-18 11:12:11,006][INFO ][bootstrap                ] max_open_files [4072]
[2013-07-18 11:12:11,014][INFO ][node                     ] [node1] {0.90.2}[28406]: initializing ...
```

### Using bootstrap.mlockall: true

/etc/sysconfig/elasticsearch:

```
# Maximum number of open files
MAX_OPEN_FILES=65535

# Maximum amount of locked memory
MAX_LOCKED_MEMORY=unlimited
```

Result max_open_files and mlockall error:

```
/etc/rc.d/init.d/elasticsearch restart
[2013-07-18 11:16:24,949][INFO ][bootstrap                ] max_open_files [4072]
[2013-07-18 11:16:25,000][WARN ][common.jna               ] Unknown mlockall error 0
```

## After applying fix

Removing limits from /etc/security/limits.conf

```
#elasticsearch          soft    nofile          65535
#elasticsearch          hard    nofile          65535
#elasticsearch          -       memlock         unlimited
```

/etc/sysconfig/elasticsearch:

```
# Maximum number of open files
MAX_OPEN_FILES=65535

# Maximum amount of locked memory
MAX_LOCKED_MEMORY=unlimited
```

/etc/elasticsearch/elasticsearch.yml:

```
bootstrap.mlockall: true
```

Actual fix in /etc/systemd/system/elasticsearch.service 

```
[Service]
...
LimitMEMLOCK=infinity
LimitNOFILE=65535
```

Note: run the following command after altering the above file:

```
systemctl --system daemon-reload
```

Result max_open_files and no mlockall error:

```
/etc/rc.d/init.d/elasticsearch restart
[2013-07-18 11:20:21,476][INFO ][bootstrap                ] max_open_files [65511]
[2013-07-18 11:20:21,616][INFO ][node                     ] [node1] {0.90.2}[28558]: initializing ...
```
</comment><comment author="spinscale" created="2013-07-19T16:29:21Z" id="21260733">Closed by https://github.com/elasticsearch/elasticsearch/commit/300175714482cf54325e6c46ace2abbf3b07f0c8 in 0.90 and https://github.com/elasticsearch/elasticsearch/commit/08e35e4dbea1da5080bf5d03267b24186d5c2e28 in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index can get stuck closed if any of its shards has half or less of its lucene indexes running</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3354</link><project id="" key="" /><description>You can wedge an index in the closed state by closing it when any one of its shards has half or less of its lucene indexes running.  I'm not sure if they just have to be open or if they have to be up to date.  Here is the gist:  https://gist.github.com/nik9000/6030428
</description><key id="16927530">3354</key><summary>Index can get stuck closed if any of its shards has half or less of its lucene indexes running</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>:Allocation</label><label>adoptme</label><label>bug</label></labels><created>2013-07-18T15:50:59Z</created><updated>2016-01-20T13:55:49Z</updated><resolved>2016-01-19T14:50:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-07-18T16:03:55Z" id="21194395">Sorry for the long gist.
</comment><comment author="clintongormley" created="2014-11-29T16:08:20Z" id="64956340">This is still a problem.  Updated the script to work with current master:

```
echo "This works as expected:"
echo "Cleanly launch three nodes"
rm -rf data/*
./bin/elasticsearch -p es1.pid -d
./bin/elasticsearch -p es2.pid -d
./bin/elasticsearch -p es3.pid -d
sleep 10
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=3"
echo "Create more replicas then we have nodes"
curl -XPOST "http://localhost:9200/test_close?pretty=true" -d '{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 3
  }
}'
echo "Wait until all the shards that are going to allocate have allocated"
sleep 5
curl -XGET "http://localhost:9200/_cluster/health?pretty=true"
echo "Close the index"
curl -XPOST "http://localhost:9200/test_close/_close?pretty=true"
echo "Wait a bit and open it"
sleep 2
curl -XPOST "http://localhost:9200/test_close/_open?pretty=true"
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow"
for pidfile in *.pid; do kill $(cat $pidfile); rm $pidfile ; done

echo "PHASE 1 ******************"

echo "Currently leaves the index stuck closed"
echo "Cleanly launch two nodes"
rm -rf data/*
./bin/elasticsearch -p es1.pid -d
./bin/elasticsearch -p es2.pid -d
sleep 10
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=2"
echo "Create more replicas then we have nodes"
curl -XPOST "http://localhost:9200/test_close?pretty=true" -d '{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 3
  }
}'
echo "Wait until all the shards that are going to allocate have allocated"
sleep 5
curl -XGET "http://localhost:9200/_cluster/health?pretty=true"
echo "Close the index"
curl -XPOST "http://localhost:9200/test_close/_close?pretty=true"
echo "Wait a bit and open it"
sleep 2
curl -XPOST "http://localhost:9200/test_close/_open?pretty=true"
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow"

exit

for pidfile in *.pid; do kill $(cat $pidfile); rm $pidfile ; done


echo "PHASE 2 ******************"


echo "Launching a new node during recovery also doesn't fix it"
echo "Cleanly launch two nodes"
rm -rf data/*
./bin/elasticsearch -p es1.pid -d
./bin/elasticsearch -p es2.pid -d
sleep 10
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=2"
echo "Create more replicas then we have nodes"
curl -XPOST "http://localhost:9200/test_close?pretty=true" -d '{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 3
  }
}'
echo "Wait until all the shards that are going to allocate have allocated"
sleep 5
curl -XGET "http://localhost:9200/_cluster/health?pretty=true"
echo "Close the index"
curl -XPOST "http://localhost:9200/test_close/_close?pretty=true"
echo "Wait a bit and open it"
sleep 2
curl -XPOST "http://localhost:9200/test_close/_open?pretty=true"
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow&amp;timeout=10s"
./bin/elasticsearch -p es3.pid -d
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow&amp;timeout=20s"
for pidfile in *.pid; do kill $(cat $pidfile); rm $pidfile ; done


echo "PHASE 3 ******************"



echo "Also currently leaves the index stuck closed"
echo "Cleanly launch three nodes"
rm -rf data/*
./bin/elasticsearch -p es1.pid -d
./bin/elasticsearch -p es2.pid -d
./bin/elasticsearch -p es3.pid -d
sleep 10
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=3"
echo "Create more replicas then we have nodes"
curl -XPOST "http://localhost:9200/test_close?pretty=true" -d '{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 3
  }
}'
echo "Wait until all the shards that are going to allocate have allocated"
sleep 5
curl -XGET "http://localhost:9200/_cluster/health?pretty=true"
echo "Kill a server"
kill $(cat es3.pid)
rm es3.pid
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=2"
echo "Close the index"
curl -XPOST "http://localhost:9200/test_close/_close?pretty=true"
echo "Wait a bit and open it"
sleep 2
curl -XPOST "http://localhost:9200/test_close/_open?pretty=true"
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow"
for pidfile in *.pid; do kill $(cat $pidfile); rm $pidfile ; done

echo "PHASE 4 ******************"



echo "But if you bring the third node back online then the index does recover"
echo "Cleanly launch three nodes"
rm -rf data/*
./bin/elasticsearch -p es1.pid -d
./bin/elasticsearch -p es2.pid -d
./bin/elasticsearch -p es3.pid -d
sleep 10
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=3"
echo "Create more replicas then we have nodes"
curl -XPOST "http://localhost:9200/test_close?pretty=true" -d '{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 3
  }
}'
echo "Wait until all the shards that are going to allocate have allocated"
sleep 5
curl -XGET "http://localhost:9200/_cluster/health?pretty=true"
echo "Kill a server"
kill $(cat es3.pid)
rm es3.pid
curl -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_nodes=2"
echo "Close the index"
curl -XPOST "http://localhost:9200/test_close/_close?pretty=true"
echo "Wait a bit and open it"
sleep 2
curl -XPOST "http://localhost:9200/test_close/_open?pretty=true"
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow&amp;timeout=10s"
./bin/elasticsearch -p es3.pid -d
curl -s -XGET "http://localhost:9200/_cluster/health?pretty=true&amp;timeout=5s&amp;wait_for_status=yellow&amp;timeout=20s"
for pidfile in *.pid; do kill $(cat $pidfile); rm $pidfile ; done
```
</comment><comment author="bleskes" created="2014-11-30T22:34:05Z" id="65003578">The source of the matter is the fact that we don't recover a primary unless there are a quorum of shard copies available on disk somewhere to choose a primary from (index.recovery.initial_shards controls this). When opening the index, the high replica count suggests some copies are lost and not available and thus we block the selection of a primary and the recovery. Setting the replica counts to something lower (3 in the examples above, making 2 a quorum) will cause the primary to be assigned. 

I do think we need better transparency as what's going on. Perhaps the open API should return some information about primary not being allocated - it will be hard to get that information though.  Perhaps we should inline an allocation status report that is suggested on https://github.com/elasticsearch/elasticsearch/issues/8686#issuecomment-64808853
</comment><comment author="clintongormley" created="2014-12-01T10:04:03Z" id="65042845">@bleskes nice idea.  Currently the `open` request just returns `acknowledged: true`, so some insight into the problem would definitely help.
</comment><comment author="ywelsch" created="2016-01-19T11:23:09Z" id="172822006">#15708 makes `index.recovery.initial_shards` obsolete. This means that a single shard is only needed to successfully reopen index. #15708 is for v3.0.0 though. Can we close this @clintongormley @bleskes?
</comment><comment author="bleskes" created="2016-01-19T11:35:31Z" id="172824609">+1 to closing this.
</comment><comment author="nik9000" created="2016-01-19T14:50:58Z" id="172876003">This was either my first or second interaction with the ES team on github, btw.
</comment><comment author="clintongormley" created="2016-01-20T13:55:49Z" id="173211159">w00t
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't allow unallocated indexes to be closed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3353</link><project id="" key="" /><description>Refuse to close indexes that have not had their primary shard shard allocated post api action because this leaves the index in an un-openable state.

Closes #3313
</description><key id="16915140">3353</key><summary>Don't allow unallocated indexes to be closed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nik9000</reporter><labels /><created>2013-07-18T11:26:26Z</created><updated>2014-06-26T13:54:46Z</updated><resolved>2013-07-26T20:39:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-07-26T18:15:22Z" id="21638439">Did you guys have a chance to get a look at this?  We had some email conversations where you asked for some changes and I've made them.  Please let me know if there is anything else I should do.
</comment><comment author="imotov" created="2013-07-26T18:33:06Z" id="21639521">@nik9000, Sorry about that. I am not sure what happened. I posted a comment here a couple of days ago asking you to take a look at the modifications that I made to the test in your PR, but somehow this comment didn't make it through.  Could you take a look at https://github.com/imotov/elasticsearch/commit/e279df207cf383344227f596cb1da18d3f6cdfb8 and let me know if you are ok with these changes?
</comment><comment author="nik9000" created="2013-07-26T18:55:25Z" id="21640922">Your change makes sense to me but why revert the changes that I made pull the now duplicate test code into methods?
</comment><comment author="imotov" created="2013-07-26T19:04:08Z" id="21641525">We are in the process of pulling some of this duplicate code to the upper layers of hierarchy (see AbstractSharedClusterTest, for example). So, I didn't want to interfere with the process by creating non-compatible abstraction layer on the test level.
</comment><comment author="nik9000" created="2013-07-26T19:06:49Z" id="21641686">Makes sense to me.  Is there something I need to do to switch this pull request with yours?
</comment><comment author="imotov" created="2013-07-26T19:12:49Z" id="21642024">No, you are all set. I am just going to pull the modified version into 0.90 and master.
</comment><comment author="imotov" created="2013-07-26T20:39:31Z" id="21646737">Pulled to 0.90 and master. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geopoint parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3352</link><project id="" key="" /><description>Changed `GeoPoint` parsing in serveral parsers using `GeoPoint.parse()`

Closes #3351
</description><key id="16914050">3352</key><summary>Geopoint parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-07-18T10:53:13Z</created><updated>2014-07-16T21:52:50Z</updated><resolved>2013-07-18T12:15:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-07-18T12:17:42Z" id="21179251">@s1monw I'll update the documentation
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoPoint parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3351</link><project id="" key="" /><description>The `GeoPoint` class is used in several parsers. Each of these classes implements geopoint parsing on its own. To simplify the code and guarantee consistency these parsing methods must be refactored.
</description><key id="16913682">3351</key><summary>GeoPoint parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>non-issue</label><label>v1.0.0.Beta1</label></labels><created>2013-07-18T10:41:16Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-07-18T12:15:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Conflicting META-INF between elasticsearch &amp; lucene-core/lucene-codecs for packaging into standalone binary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3350</link><project id="" key="" /><description>When bundling lucene-core, lucene-codec, &amp; elasticsearch into a single executable JAR, there are conflicting files in META-INF:

elasticsearch:
        0  06-26-13 08:56   META-INF/services/org.apache.lucene.codecs.Codec
        0  06-26-13 08:56   META-INF/services/org.apache.lucene.codecs.DocValuesFormat
      147  06-26-13 08:56   META-INF/services/org.apache.lucene.codecs.PostingsFormat

lucene-core:
      987  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.Codec
      853  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.DocValuesFormat
      909  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.PostingsFormat

lucene-codecs:
      897  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.Codec
      909  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.DocValuesFormat
     1079  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.PostingsFormat

When packaging everything into a standalone binary, it becomes unclear which ones I need.  Using the ones from elasticsearch, I get:
Caused by: java.lang.ExceptionInInitializerError
    at org.elasticsearch.index.codec.CodecModule.configure(CodecModule.java:118)
Caused by: java.lang.IllegalArgumentException: A SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Direct' does not exist. You need to add the corresponding JAR file supporting this SPI to your classpath.The current classpath supports the following names: [XBloomFilter, es090]
</description><key id="16894226">3350</key><summary>Conflicting META-INF between elasticsearch &amp; lucene-core/lucene-codecs for packaging into standalone binary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">vlovich</reporter><labels><label>non-issue</label></labels><created>2013-07-17T22:34:18Z</created><updated>2016-08-18T17:13:29Z</updated><resolved>2013-07-20T06:15:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-07-18T08:05:08Z" id="21168465">you need all of them. I don't know how you bundle that but can you merge them somehow? I don't think this is an issue since this is just how the SPI loader stuff works that lucene uses?
</comment><comment author="rmuir" created="2013-07-19T23:30:22Z" id="21283417">UweSays: You have 2 possibilities, choose one:
- Don't merge the JAR files to one big one!
- Don't use a simple copy ANT task to merge, because the META-INF metadata must be merged (means the META-INF/services files with same name must be appended in the final JAR). The Maven-Shade plugin for creating such "Uber JARs" can do this for you, but there are other possibilities (see http://maven.apache.org/plugins/maven-shade-plugin/examples/resource-transformers.html).
</comment><comment author="s1monw" created="2013-07-20T06:15:20Z" id="21288957">thanks robert ;) I don't think it's an issue so I am closing....(this could have been said by uwe as well :)
</comment><comment author="Vadi" created="2013-10-17T16:24:23Z" id="26523641">I don't understand why some one would not want to bundle single jar. We always use single jar approach for quick testing. Not sure how to solve this issue. I am having the same issue with 0.90.5

```
[INFO] +- org.elasticsearch:elasticsearch:jar:0.90.5:compile
[INFO] |  +- org.apache.lucene:lucene-core:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-codecs:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-queries:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-memory:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-highlighter:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-queryparser:jar:4.4.0:compile
[INFO] |  |  \- org.apache.lucene:lucene-sandbox:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-suggest:jar:4.4.0:compile
[INFO] |  |  \- org.apache.lucene:lucene-misc:jar:4.4.0:compile
[INFO] |  +- org.apache.lucene:lucene-join:jar:4.4.0:compile
[INFO] |  |  \- org.apache.lucene:lucene-grouping:jar:4.4.0:compile
[INFO] |  \- org.apache.lucene:lucene-spatial:jar:4.4.0:compile
[INFO] |     \- com.spatial4j:spatial4j:jar:0.3:compile
```
</comment><comment author="ahmedahmedov" created="2016-07-12T15:18:49Z" id="232081371">I am having the same issue here. I am already fed up with having to deal with lucene related version incompatibility issues. It almost never works out of the box. Although I have spent my whole day trying to get neo4j helloworld java api to work, I still haven't found a solution. Previous to this problem, I had to debug so many other lucene related issues. 
</comment><comment author="rjernst" created="2016-07-12T17:57:09Z" id="232127020">@ahmedahmedov This issue is long closed, and the options for how to solve were clearly spelled out in the comment https://github.com/elastic/elasticsearch/issues/3350#issuecomment-21283417.

If you are still having issues, I suggest asking on https://discuss.elastic.co.
</comment><comment author="mdelano" created="2016-08-18T17:13:29Z" id="240791805">So anyone building a single jar is going to encounter this issue. It would save some grief if this worked out of the box for projects planning to roll a fat jar. 

http://stackoverflow.com/questions/39023903/elasticsearch-transportclient-fails-with-could-not-initialize-class-org-elastics/39023904#39023904
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Groovy API documentation wrong</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3349</link><project id="" key="" /><description>1. missing import

import org.elasticsearch.groovy.client.*
1. different api

GClient client = node.getClient();

http://www.elasticsearch.org/guide/clients/groovy-api/client/
</description><key id="16875783">3349</key><summary>Groovy API documentation wrong</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hsn10</reporter><labels /><created>2013-07-17T16:52:15Z</created><updated>2013-07-18T11:41:53Z</updated><resolved>2013-07-18T11:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-18T11:37:59Z" id="21177647">Thanks for pointing that out. 
The reference itself is a separate [github project](https://github.com/elasticsearch/elasticsearch.github.com) and anybody can contribute to it. Have a look [here](http://www.elasticsearch.org/contributing-to-elasticsearch/) to know more (contributing to the documentation section in your case).

Do you feel like sending a pull request there with your changes?

Closing the issue here since it refers to the documentation.
</comment><comment author="hsn10" created="2013-07-18T11:41:53Z" id="21177807">no
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms lookup doesn't support all "execution" modes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3348</link><project id="" key="" /><description>The terms lookup functionality only supports the "plain" execution mode.  It would be beneficial to support each mode available to a normal terms filter (and, or, bool, etc).
</description><key id="16873489">3348</key><summary>terms lookup doesn't support all "execution" modes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-07-17T16:09:32Z</created><updated>2013-07-19T15:04:55Z</updated><resolved>2013-07-19T15:04:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-17T16:10:49Z" id="21124259">Thats actually intentional, since it makes a lot of sense with the caching of the lookup filter itself (as a single Lucene TermsFilter). Did you see a case where you needed a different execution mode?
</comment><comment author="mattweber" created="2013-07-17T16:26:18Z" id="21125323">I figured it was due to caching.  I don't have an immediate need for different modes but I was thinking of it more for an "and" mode.
</comment><comment author="kimchy" created="2013-07-19T12:48:20Z" id="21247002">I think that if we end up with an actual case for it we can think of adding it, I can't think of one where it would end up being better in the context of terms lookup.
</comment><comment author="mattweber" created="2013-07-19T15:04:55Z" id="21255073">Sounds good, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child query appears to not respect 'type' argument when run against an alias? (0.90.2)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3347</link><project id="" key="" /><description>Curl recreation here: https://gist.github.com/erikcameron/6021828

When operations are done through an alias, it looks like has_child will return a hit if any child documents match the query, not just those of the specified type. In the example above, I index a parent type, "folks", and child types "nice_kids" and "mean_kids." I put one document in each, a parent record ("mom") a nice_kids record ("nice") and a mean_kids record ("mean").

Searching for "mean" in the nice_kids type (or vice versa) appropriately yields nothing. But a has_child query searching for query_string "mean" in the nice_kids type (or vice versa) returns the parent, if an alias is used.

The curl recreation should have the relevant details, but obviously please let me know if there's anything I left out.

Thanks for all the hard work,
-Erik
</description><key id="16872672">3347</key><summary>has_child query appears to not respect 'type' argument when run against an alias? (0.90.2)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikcameron</reporter><labels /><created>2013-07-17T15:56:23Z</created><updated>2014-07-03T10:15:04Z</updated><resolved>2014-07-03T10:15:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T10:15:04Z" id="47889288">HI @erikcameron 

Not sure which issue fixed this, but it is fixed in v0.90.6

thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_id could become non-unique within a index when using _routing fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3346</link><project id="" key="" /><description># Bug Description

Usually, the _id field is known as global unique in a index, right?  
But I found it become non-unique when a doc's routing field is modified to another value and reindex to ES. Then, there will be two doc alive in diffrent shard but the same index.

It seems the delete operation is broadcasted to all shards, but the index operation not. 

Since it's hard to monitor if the routing filed is modified, the only thing I can do is do an delete operation before each index operation, I really don't like it &gt;_&lt;
# how to reproduce the bug

`Tested under both v0.90.0`
## [1] Create A Index

``` shell
curl -XPUT 'http://localhost:9200/user' -d '
{
    "mappings": {
        "User": {
            "store": "no",
            "_id": {
                    "type": "string",
                    "index": "not_analyzed",
                    "store": "yes"
            },
            "_type": {
                "enabled": true
            },
            "_routing": {
                "path": "tag",
                "required": true
            },
            "properties": {
                "tag": {
                    "type": "string",
                    "index": "not_analyzed"
                }
            }
        }
    }
}
'
```
## [2] Input Data

``` shell
curl -XPOST 'http://localhost:9200/user/User/123' -d '{"tag" : "good"}'
```

{"ok":true,"_index":"user","_type":"User","_id":"123","_version":1}

``` shell
curl -XPOST 'http://localhost:9200/user/User/123' -d '{"tag" : "bad"}'
```

{"ok":true,"_index":"user","_type":"User","_id":"123","_version":1}
## [3] Search

``` shell
curl -XPOST 'http://localhost:9200/user/User/_search' -d '{
  "query": {
    "term": {
      "_id": "123"
    }
  },
  "facets": {
    "tag": {
      "terms": {
        "field": "tag"
      }
    }
  }
}'
```

Result:

```
{
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 10,
        "successful": 10,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 0.30685282,
        "hits": [
            {
                "_index": "user",
                "_type": "User",
                "_id": "123",
                "_score": 0.30685282,
                "_source": {
                    "tag": "bad"
                }
            },
            {
                "_index": "user",
                "_type": "User",
                "_id": "123",
                "_score": 0.30685282,
                "_source": {
                    "tag": "good"
                }
            }
        ]
    },
    "facets": {
        "tag": {
            "_type": "terms",
            "missing": 0,
            "total": 2,
            "other": 0,
            "terms": [
                {
                    "term": "good",
                    "count": 1
                },
                {
                    "term": "bad",
                    "count": 1
                }
            ]
        }
    }
}
```
</description><key id="16868037">3346</key><summary>_id could become non-unique within a index when using _routing fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">icedfish</reporter><labels /><created>2013-07-17T14:42:03Z</created><updated>2013-07-25T02:41:20Z</updated><resolved>2013-07-22T12:18:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-18T16:55:09Z" id="21197876">Elasticsearch will not guarantee the unique aspect of an ID when using custom routing. The unique aspect is maintained on the shard level. The cost of trying to make sure the ID is globally unique will be to expensive across the cluster.
</comment><comment author="fcrosfly" created="2013-07-19T01:49:06Z" id="21227003">&gt; _&lt; support
</comment><comment author="icedfish" created="2013-07-19T02:29:47Z" id="21228065">@kimchy I known it's hard to maintan the unique globally,  but could you add the issue to the doc page of routing field?  It's totallly not expected to have this problem when I choose to using _routing several months ago &gt;&gt;_&lt;&lt;

I'm trying  to find a way to detect the ids that already be non-unique, and fix the non-unique issue by a batch.
But I find it hard to find them...

I try to find the duplicate by term facet count like below:

```
{
  "query": {
    "match_all": {}
  },
  "facets": {
    "count": {
      "terms": {
        "field": "_id",
        "size": 10,
        "order": "count"
      }
    }
  }
}
```

But I found it only works when I test the query on a single node cluster,  but don't work on my production cluster with more nodes, all the count return 1.
Is it because the index contains too many ids?  (about 10M)

And when I sepecific a duplicate id in filter query, it can give the right count nunber :

```
{
  "query": {
    "term": {
      "_id": "276052286"
    }
  },
.....
}
```

Could you give me some suggest? thanks.
</comment><comment author="javanna" created="2013-07-22T12:18:26Z" id="21340076">@icedfish I added a "id uniqueness" section to the doc page for the [_routing field](http://www.elasticsearch.org/guide/reference/mapping/routing-field/).
</comment><comment author="icedfish" created="2013-07-22T14:29:54Z" id="21347676">Hi @javanna could you told me a possible way to find the non-unique ids?  Or there's no way to find them ?
</comment><comment author="spinscale" created="2013-07-23T06:58:29Z" id="21396482">Untested, but if you have indexed the id, you could run a terms facet on it (or maybe if you have not indexed it, you can use a `script_field` inside your terms facet - not a hundred percent sure, but maybe worth a try).
</comment><comment author="imotov" created="2013-07-23T13:47:11Z" id="21414768">terms facet is the simplest way to go, but in order for it to work you will have to fit all ids into memory. The reason for this is that terms facets are calculated on each shard first and then reduced on a single node. But the nature of the problem is that within a shard all ids are unique. So, each shard will have to send a complete set of ids to the reducer where they will have to be de-duplicated. To make long story short, for terms facet to work the `size` parameter on the facet will have to be set to the number of ids and such facet might be too big for your nodes. 

A more light-weight solution would be to extract all ids, sort them and then find repeated ids. You can write a script that would use [scan/scroll](http://www.elasticsearch.org/guide/reference/api/search/scroll/) to retrieve all ids in your index or you can use handy [es2unix](https://github.com/elasticsearch/es2unix) script that already has this functionality. After es2unix is installed, you can find duplicate ids by simply running the following script:

```
es ids user User | sort | uniq -d 
```
</comment><comment author="icedfish" created="2013-07-24T08:48:14Z" id="21471438">Hi @imotov, thanks for your  help!
I tried es2unix, but I think there may be something wrong with the scroll function ( I checked the source code es2unix ids function is using the scroll function ) .
I got much much more ids than the total ids the index should really have , I even find an id appers for 79519 times...

I'll do some more research to locate the issue. I think it's could be my second issue for project ES : ) 
</comment><comment author="imotov" created="2013-07-25T02:41:20Z" id="21530110">@icedfish if you have ruby and tire installed, you can try running the following script instead of es2unix.

```
require 'rubygems'
require 'tire'

s = Tire.scan('user', )
s.each_document do |document|
  print document._type, " ", document.id, "\n"
end
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add local flag support to all the read operations that get executed on the master node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3345</link><project id="" key="" /><description>All the operations that involve the cluster state get executed on the master node by default, but it's usually possible to explicitly force their local execution through a local flag when it comes to reading from the cluster state. Some of those operations don't support the local flag (yet) though. We should add the local flag in the base class and support it by default so that all the classes that extend TransportMasterNodeOperationAction get it for free.
</description><key id="16859170">3345</key><summary>Add local flag support to all the read operations that get executed on the master node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">javanna</reporter><labels><label>enhancement</label><label>v1.0.0</label></labels><created>2013-07-17T11:14:41Z</created><updated>2014-02-09T11:17:30Z</updated><resolved>2014-01-20T11:35:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-18T17:29:28Z" id="21200172">@javanna, you are right, if an operation just reads the cluster state (like /_cluster/health or /_cluster/state) it can be executed locally. But if an operation modifies the cluster state it should be executed on the master for consistency  reasons. I think we could identify all "read only" operations and change them to inherit from TransportClusterInfoAction instead of TransportMasterNodeOperationAction. What do you think?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexMissingException not thrown when doing a query on a non-existent index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3344</link><project id="" key="" /><description>I'm having a weird issue where the Java interface is not throwing an IndexMissingException in 0.90.2 like it used to in 0.20.4.

When I use the REST interface, everything appears to be fine:
curl -XGET localhost:9200/twitter/_status
{"error":"IndexMissingException[[twitter] missing]","status":404}

curl -XGET 'http://localhost:9200/twitter/tweet/_count?q=user:kimchy'
{"error":"IndexMissingException[[twitter] missing]","status":404}

However, when I use the java interface, executing a CountRequestBuilder, I get the following exception:

org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:168)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:122)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
Caused by: java.io.StreamCorruptedException: unexpected end of block data
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1370)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)
    at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:499)
</description><key id="16843098">3344</key><summary>IndexMissingException not thrown when doing a query on a non-existent index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">vlovich</reporter><labels /><created>2013-07-17T01:10:43Z</created><updated>2013-10-22T01:25:39Z</updated><resolved>2013-07-30T14:09:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-17T06:43:37Z" id="21094917">Hey,

I just did a test, and everything worked as expected

```
new CountRequestBuilder(client()).setIndices("nonExisting").setQuery(matchAllQuery()).get();
// throws an IndexMissingException
```

Your problem looks slightly different. Are you running the query from a remote system to an elasticsearch node/cluster? Can you make sure that both java applications are using the same version of the JVM (even minor version) and retry?
</comment><comment author="vlovich" created="2013-07-17T06:46:38Z" id="21095004">Running this on OSX.  I believe I only have 1 version of java installed.  it is 1.7.0_17

Client is embedded ES 0.90.2 configured to run as client only with no local data.
Server is standalone cluster with roughly default configuration (aside from cluster name).
</comment><comment author="vlovich" created="2013-07-17T07:03:41Z" id="21095514">Code that when run on my ES instance, generates this problem:

https://gist.github.com/vlovich/d996d0c25aacc9228c2e
</comment><comment author="spinscale" created="2013-07-17T07:14:33Z" id="21095879">Can you provide the full test case, including how you start elasticsearch inside your java class (or dont you and I misunderstood you?)? You do not start a node in there and you do not specify the cluster name for the transport client.

Having the same JVM and operating system I should then be able to recreate your problem.
</comment><comment author="vlovich" created="2013-07-17T07:19:39Z" id="21096069">Sorry if my description of the test case is confusing.  It's a bit late here :)

The code that I provided above creates a standalone client using the Java API to the cluster.
The cluster was a basic install from the tarball installed as a service using git://github.com/elasticsearch/elasticsearch-servicewrapper.git

What's interesting is that if I just start elasticsearch normally, there is no issue.  When I launch it via the service, then the exception gets thrown.  Maybe it's some weird permissions issue? 
</comment><comment author="vlovich" created="2013-07-17T07:37:02Z" id="21096707">Not a permissions issue.  Looks like a bug in servicewrapper.git?  Not sure where the source code for those binaries is.

Changing the launchctl script to launch elasticsearch directly in foreground mode solves the issue.
</comment><comment author="spinscale" created="2013-07-17T07:42:50Z" id="21096936">Hey,

to be honest, I still suspect a difference in your java versions (one being java 1.6 which is used when the service wrapper is started and one being javv 1.7 from your IDE tests). Looking at https://github.com/elasticsearch/elasticsearch-servicewrapper/blob/master/service/elasticsearch#L361 and following lines the path used as default java home is an old java 1.6 location.

Can you make sure by checking processes that this is not the case (as this would perfectly explain the different behaviour when using the servicewrapper). Using `lsof` might help as well.
</comment><comment author="vlovich" created="2013-07-17T07:52:31Z" id="21097304">/usr/bin/java -version
java version "1.7.0_17"
Java(TM) SE Runtime Environment (build 1.7.0_17-b02)
Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)

ps aux | grep -i [e]last
root           37612   0.1  0.0  2433436   1208   ??  Ss    7:51AM   0:00.09 /bin/sh /vol/data/data/v3/elasticsearch/0.90.2/bin/service/elasticsearch launchdinternal
root           37677   0.0  2.1  3887368 178032   ??  S     7:51AM   0:03.53 /usr/bin/java -d64 -Delasticsearch-service -Des.path.home=/vol/data/data/v3/elasticsearch/0.90.2 -Xss256k -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Djava.awt.headless=true -Xms1024m -Xmx1024m -Djava.library.path=/vol/data/data/v3/elasticsearch/0.90.2/bin/service/lib -classpath /vol/data/data/v3/elasticsearch/0.90.2/bin/service/lib/wrapper.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/elasticsearch-0.90.2.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/elasticsearch-0.90.2.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/jna-3.3.0.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/jts-1.12.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/log4j-1.2.17.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-analyzers-common-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-codecs-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-core-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-grouping-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-highlighter-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-join-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-memory-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-queries-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-queryparser-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-sandbox-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-spatial-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/lucene-suggest-4.3.1.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/spatial4j-0.3.jar:/vol/data/data/v3/elasticsearch/0.90.2/lib/sigar/sigar-1.6.4.jar -Dwrapper.key=P3k3XMg7gno48hPL -Dwrapper.port=32000 -Dwrapper.jvm.port.min=31000 -Dwrapper.jvm.port.max=31999 -Dwrapper.disable_console_input=TRUE -Dwrapper.pid=37676 -Dwrapper.version=3.5.14 -Dwrapper.native_library=wrapper -Dwrapper.service=TRUE -Dwrapper.cpu.timeout=10 -Dwrapper.jvmid=1 org.tanukisoftware.wrapper.WrapperSimpleApp org.elasticsearch.bootstrap.ElasticSearchF
root           37676   0.0  0.0  2471728    848   ??  S     7:51AM   0:00.05 /vol/data/data/v3/elasticsearch/0.90.2/bin/service/exec/elasticsearch-macosx-universal-64 /vol/data/data/v3/elasticsearch/0.90.2/bin/service/elasticsearch.conf wrapper.syslog.ident=elasticsearch wrapper.pidfile=/vol/data/data/v3/elasticsearch/0.90.2/bin/service/./elasticsearch.pid wrapper.name=elasticsearch wrapper.displayname=ElasticSearch wrapper.daemonize=TRUE wrapper.statusfile=/vol/data/data/v3/elasticsearch/0.90.2/bin/service/./elasticsearch.status wrapper.java.statusfile=/vol/data/data/v3/elasticsearch/0.90.2/bin/service/./elasticsearch.java.status wrapper.script.version=3.5.14
</comment><comment author="vlovich" created="2013-07-17T07:52:46Z" id="21097317">Doesn't look like there's any java 1.6 involved
</comment><comment author="vlovich" created="2013-07-17T07:56:32Z" id="21097485">Oh.  lsof shows 1.6 is involved some how.  How do I solve this?

 /System/Library/Java/JavaVirtualMachines/

java    37677 root  txt      REG                1,2    100768 7963697 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/bin/java
java    37677 root  txt      REG                1,2     89664 7964357 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libjli.jnilib
java    37677 root  txt      REG                1,2    103472 7963543 /System/Library/Frameworks/JavaVM.framework/Versions/A/JavaVM
java    37677 root  txt      REG                1,2     98400 7963509 /System/Library/PrivateFrameworks/JavaLaunching.framework/Versions/A/JavaLaunching
java    37677 root  txt      REG                1,2  13070064 7964369 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libserver.dylib
java    37677 root  txt      REG                1,2    116544 7964361 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libjvmlinkage.dylib
java    37677 root  txt      REG                1,2    103520 7964373 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libverify.dylib
java    37677 root  txt      REG                1,2    159216 7963519 /System/Library/Frameworks/JavaVM.framework/Versions/A/Frameworks/JavaNativeFoundation.framework/Versions/A/JavaNativeFoundation
java    37677 root  txt      REG                1,2    402352 7964353 /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Libraries/libjava.jnilib
java    37677 root  txt      REG                1,2    187056 7963528 /System/Library/Frameworks/JavaVM.framework/Versions/A/Frameworks/JavaRuntimeSupport.framework/Versions/A/JavaRuntimeSupport
</comment><comment author="spinscale" created="2013-07-17T07:57:52Z" id="21097542">you can see the JVM elasticsearch is running with at `http://localhost:9200/_cluster/nodes?jvm`
</comment><comment author="vlovich" created="2013-07-17T07:59:18Z" id="21097606">You are correct.  It's using 1.6.  How do I fix this issue?
</comment><comment author="spinscale" created="2013-07-17T08:01:41Z" id="21097697">try setting `JAVA_HOME` environment variable to the correct java version before starting the service wrapper (mine is at `/Library/Java/JavaVirtualMachines/jdk1.7.0_17.jdk`)
</comment><comment author="vlovich" created="2013-07-17T08:04:27Z" id="21097795">No.  That doesn't work.  Wouldn't be expected to either since service/bin/elasticsearch start will invoke launchctl.  The environment variable I guess needs to be in launchctl?
</comment><comment author="vlovich" created="2013-07-17T08:06:57Z" id="21097905">Hmm... even specifying it in the plist doesn't seem to work.
</comment><comment author="spinscale" created="2013-07-17T08:14:29Z" id="21098189">this works like a charm when checking via `http://localhost:9200/_cluster/nodes?jvm`

```
# uses java 6
unset JAVA_HOME
bin/service/elasticsearch start
# uses java7
export JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.7.0_17.jdk/Contents/Home"
bin/service/elasticsearch start
```

so please check your environment settings in your shell...
</comment><comment author="vlovich" created="2013-07-17T08:19:32Z" id="21098407"> # echo $JAVA_HOME

 # JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.7.0_17.jdk/Contents/Home" bin/service/elasticsearch start
Starting ElasticSearch. Detected Mac OSX and installed launchd daemon.
Must be root to perform this action.

 # sudo bash
 # JAVA_HOME="/Library/Java/JavaVirtualMachines/jdk1.7.0_17.jdk/Contents/Home" bin/service/elasticsearch start

Still reports 1.6

This works if I never install the launchctl script
</comment><comment author="vlovich" created="2013-07-17T08:26:55Z" id="21098745">Hmm.. changing bin/service/elasticsearch to use /usr/libexec/java_home as the default value fixed things.  Don't know why it wouldn't pick up my environment variable.  /usr/libexec/java_home seems like the more correct default value anyways.
</comment><comment author="spinscale" created="2013-07-30T14:09:08Z" id="21793172">Closing. No elasticsearch issue. More setup/environment related IMO. If you extract anything out of your issue, which is worth documenting, please tell us.
</comment><comment author="vlovich" created="2013-07-30T21:20:10Z" id="21823752">It would be useful for the service wrapper script to pick up the correct version of java installed.

The correct way on OSX I believe is to query /usr/libexec/java_home if it is available.

Overall would be useful to document that all clients and servers are running the same jvm and how to check the jvm version using curl.
</comment><comment author="kwince" created="2013-10-22T01:25:39Z" id="26770964">Hmmm, I'm having the same problem on Ubuntu 12.04. I have the default java, and then 3 others. The $JAVA_HOME dir is correct, but ElasticSearch is not picking it up, at least, the 0.95.0 version I have now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: update ThaiAnalyzerProvider to use custom stopwords setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3343</link><project id="" key="" /><description>Modified ThaiAnalyzerProvider so it is now possible to set stopwords for the thai analyzer in index settings.

Issue: #3342
</description><key id="16819113">3343</key><summary>Analysis: update ThaiAnalyzerProvider to use custom stopwords setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">robinhughes</reporter><labels /><created>2013-07-16T16:22:20Z</created><updated>2014-06-17T07:07:09Z</updated><resolved>2013-07-18T10:05:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-18T10:05:04Z" id="21173850">This is merged in. Thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thai language analyzer ignores stopwords configuration setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3342</link><project id="" key="" /><description /><key id="16818709">3342</key><summary>Thai language analyzer ignores stopwords configuration setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">robinhughes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-16T16:15:21Z</created><updated>2013-07-18T10:06:12Z</updated><resolved>2013-07-18T10:05:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-18T10:06:12Z" id="21173893">Merged request was pulled in. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added the "pattern_capture" token filter from Lucene 4.4</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3341</link><project id="" key="" /><description>The XPatternCaptureGroupTokenFilter.java file can be removed once we
upgrade to Lucene 4.4.

This change required the addition of getArray() to Settings.java, as
getAsArray() could have broken patterns by trying to treat a single
string as a comma-delimited value.

Closes #3340
</description><key id="16809800">3341</key><summary>Added the "pattern_capture" token filter from Lucene 4.4</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-07-16T13:40:15Z</created><updated>2014-07-03T21:55:54Z</updated><resolved>2013-07-16T16:20:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-07-16T16:14:32Z" id="21053362">@kimchy I've removed getArray and implemented it as a flag to getAsArray instead.  Also added tests for
`pattern_capture`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Add the pattern_capture token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3340</link><project id="" key="" /><description>The `pattern_capture` token filter, unlike the `pattern` tokenizer, emits a token for every capture group in the regular expression.  Patterns are not anchored to the beginning and end of the string, so each pattern can match multiple times, and matches are allowed to overlap.

For instance a pattern like :

```
"(([a-z]+)(\d*))"
```

when matched against: 

```
"abc123def456"
```

would produce the tokens: [ `abc123`, `abc`, `123`, `def456`, `def`, `456` ]

If `preserve_original` is set to `true` then it would also emit the original token: `abc123def456`.

This is particularly useful for indexing text like camel-case code, eg `stripHTML` where a user may search for `"strip html"` or `"striphtml"`:

```
curl -XPUT localhost:9200/test/  -d '
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "code" : {
               "type" : "pattern_capture",
               "preserve_original" : 1,
               "patterns" : [
                  "(\\p{Ll}+|\\p{Lu}\\p{Ll}+|\\p{Lu}+)",
                  "(\\d+)"
               ]
            }
         },
         "analyzer" : {
            "code" : {
               "tokenizer" : "pattern",
               "filter" : [ "code", "lowercase" ]
            }
         }
      }
   }
}
'
```

When used to analyze the text `"import static org.apache.commons.lang.StringEscapeUtils.escapeHtml"`, this emits the tokens: [  `import`, `static`, `org`, `apache`, `commons`, `lang`, `stringescapeutils`, `string`, `escape`, `utils`, `escapehtml`, `escape`, `html` ]

Another example is analyzing email addresses:

```
curl -XPUT localhost:9200/test/  -d '
{
   "settings" : {
      "analysis" : {
         "filter" : {
            "email" : {
               "type" : "pattern_capture",
               "preserve_original" : 1,
               "patterns" : [
                  "(\\w+)",
                  "(\\p{L}+)",
                  "(\\d+)",
                  "@(.+)"
               ]
            }
         },
         "analyzer" : {
            "email" : {
               "tokenizer" : "uax_url_email",
               "filter" : [ "email", "lowercase",  "unique" ]
            }
         }
      }
   }
}
'
```

When the above analyzer is used on an email address like `john-smith_123@foo-bar.com` it would produce the following tokens: [`john-smith_123@foo-bar.com`, `john`, `smith_123`, `smith`, `123`, `foo`, `foo-bar.com`, `bar`, `com`]

Multiple patterns are required to allow overlapping captures, but also means that patterns are less dense and easier to understand.

Note: All tokens are emitted in the same position, and with the same character offsets, so when combined with highlighting, the whole original token will be highlighted, not just the matching subset.  For instance, querying the above email address for `"smith"` would highlight `"&lt;em&gt;john-smith_123@foo-bar.com&lt;/em&gt;"`, not `"john-&lt;em&gt;smith&lt;/em&gt;_123@foo-bar.com"`
</description><key id="16809333">3340</key><summary>Analysis: Add the pattern_capture token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-16T13:30:42Z</created><updated>2013-08-11T07:04:10Z</updated><resolved>2013-07-16T16:20:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow bin/plugin to set -D JVM parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3339</link><project id="" key="" /><description>Currently the bin/plugin command did not allow one to set jvm parameters
for startup. Usually this parameters are not needed (no need to configure
heap sizes for such a short running process), but one could not set the
configuration path. And that one is important for plugins in order find
out, where the plugin directory is.

This is especially problematic when elasticsearch is installed as
debian/rpm package, because the configuration file is not placed in the
same directory structure the plugin shell script is put.

This pull request allows to call bin/plugin like this

bin/plugin -Des.default.config=/etc/elasticsearch/elasticsearch.yml -install mobz/elasticsearch-head

As a last small improvement, the PluginManager now outputs the directort
the plugin was installed to in order to avoid confusion.

Closes #3304
</description><key id="16808453">3339</key><summary>Allow bin/plugin to set -D JVM parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-07-16T13:09:50Z</created><updated>2014-06-14T12:57:19Z</updated><resolved>2013-08-02T07:27:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-08-01T15:38:46Z" id="21945272">:+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ELASTIC_SEARCH + ELASTIC_SEARCH_HEAD+RIVER_JDBC+MYSQL</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3338</link><project id="" key="" /><description>Hi, 

I am using this combination ELASTIC_SEARCH + ELASTIC_SEARCH_HEAD+RIVER_JDBC+MYSQL to achieve the search options in json documents...When i put my request and i tried to get i am unable to get JSON objects as my output (Ex: like mysql result set) .Help me sorted from this issue..
</description><key id="16807430">3338</key><summary>ELASTIC_SEARCH + ELASTIC_SEARCH_HEAD+RIVER_JDBC+MYSQL</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vasanthakumarr</reporter><labels /><created>2013-07-16T12:44:01Z</created><updated>2013-07-17T06:03:25Z</updated><resolved>2013-07-17T06:03:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-17T06:03:25Z" id="21093824">Please use the google group for questions like this, as there are far more people watching. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing doc link in the README</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3337</link><project id="" key="" /><description>Here is the [line 195 of the README](https://github.com/elasticsearch/elasticsearch/blame/master/README.textile#L195):

&gt; We have just covered a very small portion of what ElasticSearch is all about. For more information, please refer to: .

I guess there should be a link after &#8220;to:&#8221;.
</description><key id="16799837">3337</key><summary>Missing doc link in the README</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bfontaine</reporter><labels /><created>2013-07-16T09:09:29Z</created><updated>2013-07-17T17:09:44Z</updated><resolved>2013-07-17T17:09:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-17T17:09:44Z" id="21128284">Fixed, thanks for pointing that out!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>memory leak while scrolling over index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3336</link><project id="" key="" /><description>(I've tried to report this via the google group. Either I'm being moderated or google doesn't like me. sorry for posting it twice)

I'm trying to scroll over all documents of an ElasticSearch Index using
a match_all query. I've set ES_HEAP_SIZE to 8G but I'm not able to
complete the operation, because elasticsearch runs out of memory (see
log at the end of this mail).

The head plugin tells me the index size is around 400GB with around 9.5M
documents. I'm using a single document type with the following mapping:

&lt;pre&gt;
{
    "src_doc": {
        "_all": {
            "enabled": false
        }, 
        "_source": {
            "enabled": false
        }, 
        "properties": {
            "content": {
                "type": "binary"
            }, 
            "exception": {
                "index": "no", 
                "store": true, 
                "type": "string"
            }, 
            "last_update": {
                "format": "YYYY-MM-dd", 
                "store": true, 
                "type": "date"
            }, 
            "title": {
                "index": "no", 
                "store": true, 
                "type": "string"
            }, 
            "uid": {
                "index": "no", 
                "type": "string"
            }, 
            "url": {
                "index": "no", 
                "store": true, 
                "type": "string"
            }
        }
    }
}
&lt;/pre&gt;


I'm using ElasticSearch 0.90.2, but the issue has already been in
0.90.0. The Cluster is a single node, which is not being used otherwise.

Here's the log:

&lt;pre&gt;
2013-07-12T13:59:39.40468 java.lang.OutOfMemoryError: Java heap space
2013-07-12T13:59:39.43523 Dumping heap to java_pid8908.hprof ...
2013-07-12T14:00:14.69555 Heap dump file created [8513498041 bytes in 35.278 secs]
2013-07-12T14:00:14.73700 [2013-07-12 16:00:14,702][WARN ][http.netty               ] [graph.8908] Caught exception while handling client http traffic, closing connection [id: 0x8d823a3d, /127.0.0.1:52874 =&gt; /127.0.0.1:9250]
2013-07-12T14:00:14.73702 java.lang.OutOfMemoryError: Java heap space
2013-07-12T14:00:14.73702   at java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:57)
2013-07-12T14:00:14.73702   at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
2013-07-12T14:00:14.73702   at org.elasticsearch.common.netty.buffer.CompositeChannelBuffer.toByteBuffer(CompositeChannelBuffer.java:649)
2013-07-12T14:00:14.73703   at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.toByteBuffer(AbstractChannelBuffer.java:530)
2013-07-12T14:00:14.73703   at org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:77)
2013-07-12T14:00:14.73703   at org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:46)
2013-07-12T14:00:14.73703   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:194)
2013-07-12T14:00:14.73703   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:152)
2013-07-12T14:00:14.73704   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:335)
2013-07-12T14:00:14.73704   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
2013-07-12T14:00:14.73705   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
2013-07-12T14:00:14.73706   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
2013-07-12T14:00:14.73706   at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2013-07-12T14:00:14.73706   at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2013-07-12T14:00:14.73706   at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2013-07-12T14:00:14.73706   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2013-07-12T14:00:14.73707   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2013-07-12T14:00:14.73707   at java.lang.Thread.run(Thread.java:722)
2013-07-12T14:00:18.29386 [2013-07-12 16:00:18,293][WARN ][http.netty               ] [graph.8908] Caught exception while handling client http traffic, closing connection [id: 0xd2b513bc, /127.0.0.1:52875 =&gt; /127.0.0.1:9250]
2013-07-12T14:00:18.29388 java.lang.OutOfMemoryError: Java heap space
2013-07-12T14:00:18.29388   at java.nio.HeapByteBuffer.&lt;init&gt;(HeapByteBuffer.java:57)
2013-07-12T14:00:18.29388   at java.nio.ByteBuffer.allocate(ByteBuffer.java:331)
2013-07-12T14:00:18.29388   at org.elasticsearch.common.netty.buffer.CompositeChannelBuffer.toByteBuffer(CompositeChannelBuffer.java:649)
2013-07-12T14:00:18.29389   at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.toByteBuffer(AbstractChannelBuffer.java:530)
2013-07-12T14:00:18.29389   at org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:77)
2013-07-12T14:00:18.29389   at org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:46)
2013-07-12T14:00:18.29389   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:194)
2013-07-12T14:00:18.29389   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:152)
2013-07-12T14:00:18.29390   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:335)
2013-07-12T14:00:18.29390   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
2013-07-12T14:00:18.29391   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
2013-07-12T14:00:18.29391   at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
2013-07-12T14:00:18.29391   at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
2013-07-12T14:00:18.29391   at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
2013-07-12T14:00:18.29391   at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
2013-07-12T14:00:18.29391   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
2013-07-12T14:00:18.29392   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
2013-07-12T14:00:18.29392   at java.lang.Thread.run(Thread.java:722)
&lt;/pre&gt;

</description><key id="16793856">3336</key><summary>memory leak while scrolling over index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">schmir</reporter><labels /><created>2013-07-16T05:57:08Z</created><updated>2013-07-17T14:05:37Z</updated><resolved>2013-07-17T13:43:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T06:38:13Z" id="21023978">Hi,

Can you please also post the requests you are issuing as cURL commands? 

Cheers,
Boaz
</comment><comment author="schmir" created="2013-07-16T10:20:48Z" id="21033133">sure. the requests look like

&lt;pre&gt;
curl -XGET 'http://localhost:9250/src_idx/_search?search_type=scan&amp;scroll=30s&amp;size=10' -d '{"fields": "*", "query": {"match_all": {}}}'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
curl -XGET 'http://localhost:9250/_search/scroll?scroll_id=c2NhbjsyOzMyOjBCVmFVNDVaUjg2YlBRc19DM29FeXc7MzE6MEJWYVU0NVpSODZiUFFzX0Mzb0V5dzsxO3RvdGFsX2hpdHM6OTQyNzIzMDs%3D&amp;scroll=10m' -d '""'
...
&lt;/pre&gt;

which just made me double check my code, because the scroll_id doesn't
change contrary to what I had assumed.
</comment><comment author="schmir" created="2013-07-16T10:21:48Z" id="21033174">I can also put the heap dump file online if you like to look at it.
</comment><comment author="bleskes" created="2013-07-16T11:02:31Z" id="21034959">The heap dump would be great. Thx.
</comment><comment author="schmir" created="2013-07-16T11:17:08Z" id="21035566">here's the heap dump: http://graph.brainbot.com/java_pid8908.hprof.gz
</comment><comment author="bleskes" created="2013-07-16T13:23:12Z" id="21041281">Thx. A short update - I wrote a little to reproduce locally but couldn't. I'm downloading the heap dump and will have a go at it later..
</comment><comment author="bleskes" created="2013-07-17T13:43:17Z" id="21113384">Hi,

Thanks for the memory dump, it really helped.

Here is what's going on - stored fields in Lucene are persisted to disk, in a compressed form, per Lucene segment (those are the building block of a lucene index, which in ES terminology maps to a single shard). When you retrieve a doc, lucene goes to disk and decompresses the data you need and sends it back. When it does so, it re-uses memory buffers to temporarily store the decompressed data so ES can send them back to the user. Those memory buffers are cached per segment per thread.

In your case, each document has around 9MB of stored content (does this make sense to you?), I guess most of it is the binary content. At the moment of the dump, your index contained 264 segments (total of the two shards the node has). Those cached memory buffers add up to 8GB (remember, they are per segment per thread).

The good news are that in the next version of Lucene (4.4) does not cache the memory buffers if they too big, which will solve the problem at the expense of some performance degradation. It will probably ship with the next minor version of ES  (0.90.3).

Until that time, you can try and reduce the memory signature by
- only allocating one shard to a node (will roughly half the number of segments)
- frequently run optimize (which will reduce the number of segments). How often depends on how frequently you index new document

However, if you continuously index/delete data, it will be very hard to guaranty the number of segments do not go beyond a certain level.

I'm closing this issue now because there is no real action for me to take now. Feel free to comment on it if you have any questions or if something is not clear.

Cheers,
Boaz
</comment><comment author="schmir" created="2013-07-17T14:05:37Z" id="21114962">Many thanks for looking into this and the detailed explanation!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>searching for +P +P301 finding: P-P341</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3335</link><project id="" key="" /><description>Hello, I don't know is this good place for this kind of issues, I'm using simple search method (match) in text, and for phrase p-p301 ES showing me product with keywords: p-p341 how can I improve search to show exact product with keyword: p-p301? In product p-p341 there is no any p-p301 keyword.
</description><key id="16791678">3335</key><summary>searching for +P +P301 finding: P-P341</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">panshop</reporter><labels /><created>2013-07-16T03:54:14Z</created><updated>2013-07-16T06:57:28Z</updated><resolved>2013-07-16T06:57:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-16T06:57:28Z" id="21024614">Please use the google group for issues like this, as there are many more people looking at your problems. Thanks!

Also see http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update without routing value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3334</link><project id="" key="" /><description>Hi,
I am trying to use the update api with a type that has a parent. I want to be able to send an update request without any routing, so that the request gets sent to all shards. Currently, if I don't specify a routing, it seems to default to something (perhaps based on the id?), so the update is only successful if I happen to hit the correct shard by luck. Has anybody else had to deal with this problem before?
</description><key id="16780307">3334</key><summary>Update without routing value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottc52</reporter><labels /><created>2013-07-15T21:33:35Z</created><updated>2013-07-17T17:32:18Z</updated><resolved>2013-07-16T06:55:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-16T06:55:48Z" id="21024561">The ID of a document is used to decide to which shard the request is sent. You may configure your mapping, to extract the routing out of a field from your document. Alternatively you can use the alias API and configure an automatic routing.

See http://www.elasticsearch.org/guide/reference/mapping/routing-field/ and http://www.elasticsearch.org/guide/reference/api/admin-indices-aliases/

Also, please do not use github issues for this, but rather the google groups (as many more people are looking at your problem and will have different ideas to help you). If you still think you found a bug, please read http://www.elasticsearch.org/help/ first.

Thanks!
</comment><comment author="scottc52" created="2013-07-16T19:37:35Z" id="21067625">Yes, but I want to be able to update an existing entry without knowing it's routing value. Since, I don't know the routing value, I can't send it in my request. I know that you can't index a new entry without a routing value, but it seems that I should be able to update one that already exists (since you can both find and delete entries just from their id even if they have custom routing). 

I understand that internally elasticsearch is finding my document, doing some computations on it, and reindexing it. It  shouldn't be difficult to search for the old document using the id, and reindex the updated document using the same routing value as the old document.

Sorry if this isn't the right place for this. I guess this isn't really a bug so much as a feature request.
</comment><comment author="javanna" created="2013-07-17T14:45:55Z" id="21117798">Hi, 
since you indexed the document in the first place providing the routing, it should be easy to provide the same value when trying to update it. You could for instance adding it to your documents and configure a path for it in your mapping like @spinscale mentioned.

Here is the reasoning behind it: as you mentioned we need to know which shard to hit when we index a document. An update is pretty much the same, except for the get that we need in order to retrieve the doc, modify it and reindex it. Given that the doc is retrieved by id, using the get api internally, we need to know the routing value (id by default) since the get is always executed on a single shard. It doesn't make much sense to switch to a search request here, because that would require to query all the shards that the index is composed of. Furthermore, that way it would be possible to retrieve only committed documents in lucene, since the get api is real time while the search api is (near) real time. There might be documents that you can retrieve using get (from the transaction log) but not (yet) searching for them. Hope this makes things clearer.
</comment><comment author="scottc52" created="2013-07-17T17:32:18Z" id="21129744">Ah, okay. Thank you. That helps me understand what's happening internally a little better.

In my situation, I guess I'll just have to either query ES for the routing or treat the id and the routing value as a tuple throughout my application, so I'll always have access to the routing information.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not Getting installed </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3333</link><project id="" key="" /><description>Hi All,

This is the error i am getting while installing the plugin in for river-jdbc in windows,
Plugin installation assumed to be site plugin, but contains source code, abortin
g installation...
Kindly give the solutions  asap
</description><key id="16770707">3333</key><summary>Not Getting installed </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vasanthakumarr</reporter><labels /><created>2013-07-15T18:35:59Z</created><updated>2013-07-16T06:51:47Z</updated><resolved>2013-07-16T06:51:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-07-15T18:49:23Z" id="20993917">Please consider closing this ticket and asking this kind of questions in the ES mailing list http://www.elasticsearch.org/community/ there will be a wider audience there to answer and won't pollute issues.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed nullshape indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3332</link><project id="" key="" /><description>The current shape builders allow parsing `null` shapes but if these values get indexed a parsing exception (nullpointer) is thrown. This commit catches the actual `null` shape and ignored any field creation.

Closes #3310
</description><key id="16761203">3332</key><summary>Fixed nullshape indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-07-15T15:45:15Z</created><updated>2014-07-01T17:37:27Z</updated><resolved>2013-07-16T10:53:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-16T06:49:37Z" id="21024332">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set spare becore comparing comparator bottom value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3331</link><project id="" key="" /><description>The actual documents value was never calculated if setSpare wasn't called
before compareBottom was called on a certain document.

Closes #3309
</description><key id="16753298">3331</key><summary>Set spare becore comparing comparator bottom value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-07-15T13:17:07Z</created><updated>2014-07-16T21:52:52Z</updated><resolved>2013-07-17T06:41:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rename existsAliases to aliasesExist</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3330</link><project id="" key="" /><description>Rename IndicesAdminClient#existsAliases to IndicesAdminClient#aliasesExist. Also `IndicesExistsAliasesResponse` will be renamed to `AliasesExistResponse`.

This is a breaking change only for the Java api, and _not_ for the rest api. 
</description><key id="16751324">3330</key><summary>Rename existsAliases to aliasesExist</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-15T12:25:27Z</created><updated>2013-07-15T12:39:24Z</updated><resolved>2013-07-15T12:32:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>about bulk_size_bytes in kafka river </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3329</link><project id="" key="" /><description>bulkSize = XContentMapValues.nodeIntegerValue(indexSettings.get("bulk_size_bytes"), 10_1024_1024);

So if I input 10485760 as bulk size bytes ,the final bulk size would be 1?
</description><key id="16737246">3329</key><summary>about bulk_size_bytes in kafka river </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xiaoliang-samsung</reporter><labels /><created>2013-07-15T02:21:09Z</created><updated>2013-07-15T07:47:03Z</updated><resolved>2013-07-15T07:47:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-15T07:47:02Z" id="20955640">please use the google group for questions like this (where you already got an answer). It is better to post there as more people will take a look. github issues should only be used for bugs.

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unexpected results with shingle tokenfilter and match with "and" operator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3328</link><project id="" key="" /><description>``` sh
#2013-07-14 15:19:38:885 [CREATE] ("products")
#
curl -X POST http://localhost:9200/products -d '{"settings":{"number_of_shards":1,"analysis":{"analyzer":{"searchkick":{"type":"custom","tokenizer":"standard","filter":["standard","lowercase","despacer"]}},"filter":{"despacer":{"type":"shingle","token_separator":""}}}},"mappings":{"document":{"properties":{"name":{"type":"string","analyzer":"searchkick"}}}}}'

#2013-07-14 15:19:38:885 [200]
#
# {"ok":true,"acknowledged":true}

#2013-07-14 15:19:38:890 [document/] ("products")
#
curl -X POST "http://localhost:9200/products/document/" -d '{"name":"dish washer soap"}'

#2013-07-14 15:19:38:891 [201]
#
# {"ok":true,"_index":"products","_type":"document","_id":"WHiXL30XSe2FrHAASzIfZw","_version":1}

#2013-07-14 15:19:38:898 [_refresh] ("products")
#
curl -X POST "http://localhost:9200/products/_refresh"

#2013-07-14 15:19:38:898 [200]
#
# {"ok":true,"_shards":{"total":2,"successful":1,"failed":0}}

#2013-07-14 15:19:38:902 [_search] (["products"])
#
curl -X GET 'http://localhost:9200/products/document/_search?pretty' -d '{"query":{"match":{"name":{"query":"dish soap","operator":"and"}}}}'

#2013-07-14 15:19:38:902 [200] (1 msec)
#
# {"took":1,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
```

I expect the query to match "dish washer soap", but it returns no results.  It works with the "or" operator.
</description><key id="16734372">3328</key><summary>Unexpected results with shingle tokenfilter and match with "and" operator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ankane</reporter><labels /><created>2013-07-14T22:26:38Z</created><updated>2013-07-17T12:14:07Z</updated><resolved>2013-07-17T12:14:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-17T12:14:07Z" id="21108635">When the test document is indexed the `searchkick` analyzer generates the following tokens:

```
curl -X POST "http://localhost:9200/products/_analyze?pretty&amp;field=name" -d 'dish washer soap'   
{
  "tokens" : [ {
    "token" : "dish",
    "start_offset" : 0,
    "end_offset" : 4,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  }, {
    "token" : "dishwasher",
    "start_offset" : 0,
    "end_offset" : 11,
    "type" : "shingle",
    "position" : 1
  }, {
    "token" : "washer",
    "start_offset" : 5,
    "end_offset" : 11,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 2
  }, {
    "token" : "washersoap",
    "start_offset" : 5,
    "end_offset" : 16,
    "type" : "shingle",
    "position" : 2
  }, {
    "token" : "soap",
    "start_offset" : 12,
    "end_offset" : 16,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 3
  } ]
}
```

and this is what it produces for the query:

```
$curl -X GET 'http://localhost:9200/products/document/_validate/query?pretty&amp;explain=true' -d '{"match":{"name":{"query":"dish soap","operator":"and"}}}'
{
  "valid" : true,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "explanations" : [ {
    "index" : "products",
    "valid" : true,
    "explanation" : "+name:dish +name:dishsoap +name:soap"
  } ]
}
```

As you can see, the query is searching for 3 tokens `dish`, `dishsoap` and `soap`, but the token `dishsoap` is not present in the test document, that's why this document is not returned.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add found field for bulk deletes. Closes #3320</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3327</link><project id="" key="" /><description /><key id="16727638">3327</key><summary>Add found field for bulk deletes. Closes #3320</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-07-14T13:02:25Z</created><updated>2014-07-16T21:52:52Z</updated><resolved>2013-07-15T13:13:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-07-15T08:27:37Z" id="20956968">Change looks good, I will pull it in.
</comment><comment author="martijnvg" created="2013-07-15T13:13:27Z" id="20968384">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.90.2 plugin manager - Command [--url] unknown</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3326</link><project id="" key="" /><description>elasticsearch - 0.90.2

/opt/elasticsearch-0.90.2 # bin/plugin --url /opt/mobz-elasticsearch-head-0c2ac0b.zip --install head
Usage:
    -u, --url     [plugin location]   : Set exact URL to download the plugin from
    -i, --install [plugin name]       : Downloads and installs listed plugins [*]
    -r, --remove  [plugin name]       : Removes listed plugins
    -l, --list                        : List installed plugins
    -v, --verbose                     : Prints verbose messages
    -h, --help                        : Prints this help message

 [*] Plugin name could be:
     elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)
     groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
     username/repository          for site plugins (download from github master)

Message:
   Command [--url] unknown.
</description><key id="16723439">3326</key><summary>0.90.2 plugin manager - Command [--url] unknown</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">heinemannj</reporter><labels /><created>2013-07-14T03:58:53Z</created><updated>2014-09-23T16:00:43Z</updated><resolved>2013-07-14T04:09:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-07-14T04:09:26Z" id="20931351">Duplicate of #3245
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mget: the fields parameter should accept "*" to retrieve all "store"d fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3325</link><project id="" key="" /><description>Very much like documented on http://www.elasticsearch.org/guide/reference/api/search/fields/ , it would be helpful if `_mget` would also support `"field": "*"` to retrieve all fields which have been marked as stored.
</description><key id="16718068">3325</key><summary>Mget: the fields parameter should accept "*" to retrieve all "store"d fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">mfn</reporter><labels /><created>2013-07-13T18:20:08Z</created><updated>2013-11-04T10:13:10Z</updated><resolved>2013-11-04T10:13:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-16T08:24:44Z" id="21027838">Hi @mfn,

Having things consistent between the endpoints is of course a good thing. With (m)get we have the slight complication of realtime, where the fields are not actually stored _yet_ in lucene. When you specify the fields you want to have in a realtime get, we actually go to the source and extract them for you. For `*` we'd have to go and first figure out which fields are to be retrieved. This doesn't mean we can't do it but just to say it is a bit tricky. 

Is there any reason why you are particularly interested in stored fields (as opposed to all document fields)? We are also working on a better way to control how the `_source` field is returned allowing you to only select parts of it as well ( see issue #3301 ). Will this also help your case?
</comment><comment author="bleskes" created="2013-11-04T10:13:10Z" id="27674942">Closing this for lack of activity. Please feel free to reopen.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No logs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3324</link><project id="" key="" /><description>I have 0.90.1 installed from the .deb, currently running with these options (ps output):

108      19063  1.5  1.1 1630052 282388 ?      Sl   15:34   0:39 /usr/lib/jvm/java-6-openjdk/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.pidfile=/var/run/elasticsearch.pid -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-0.90.1.jar:/usr/share/elasticsearch/lib/_:/usr/share/elasticsearch/lib/sigar/_ -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.ElasticSearch

However, in /var/log/elasticsearch there is only elasticsearch.log, which is empty.

When I su to the elasticsearch user, I can append to elasticsearch.log and create a sibling file. I did lsof on 19063 and found no log file to be open. I was unable to lsof any child process (exit code 1).
</description><key id="16715508">3324</key><summary>No logs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">schnittchen</reporter><labels /><created>2013-07-13T14:19:22Z</created><updated>2013-07-15T17:35:26Z</updated><resolved>2013-07-15T17:35:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-14T17:26:17Z" id="20939951">Hey,

can you remove the elasticsearch.log file and see if it gets recreated on startup?
The directory permissions are correct as well?
Did you upgrade from an older elasticsearch package or install this one without having any elasticsearch package installed before?
And what distribution are you using this on?
Is this reproducible if you uninstall the elasticsearch package (and purge it in order to remove all configuration files, make sure you have a copy available) and install it again?

Thanks a lot for your help (sorry for all the questions, but I'd like to track this one down)!
</comment><comment author="schnittchen" created="2013-07-14T17:36:03Z" id="20940110">Removed the empty log and restarted, no log present.
Directory is 755 elasticsearch:elasticsearch.

I upgraded from 0.19 or 0.20 to 0.90.0rc2 IIRC, then to 0.90.1. It's a debian system using the official .deb package for installing ES.

Will try purging and reinstalling tomorrow.
</comment><comment author="spinscale" created="2013-07-15T07:43:09Z" id="20955512">Hey,

wondering why there is no elasticsearch user shown in your UID column of the ps output.. can you check if the user ids match?
</comment><comment author="schnittchen" created="2013-07-15T08:24:27Z" id="20956865">Interesting, some processes show spelled-out user, some the UID. UID 108 is elasticsearch according to passwd. However, BINGO, I couldn't su to 108: "Unknown id: 108"
</comment><comment author="schnittchen" created="2013-07-15T08:27:00Z" id="20956946">We're doing a promo thing today, so I don't want to rip ES out and reinstall. Maybe I will get to that tonight.
</comment><comment author="spinscale" created="2013-07-15T09:37:33Z" id="20959711">I suspect a small bug in our upgrade process (when replacing a .deb with a new one), but need to investigate first...
</comment><comment author="spinscale" created="2013-07-15T10:37:49Z" id="20962091">The upgrade scripts look ok. How did you upgrade? Usually the scripts try to fix permissions after upgrade, Wondering what went wrong on your side. Any chance to recreate this in your setup would be great!

No hurries though. Good luck with your promo!
</comment><comment author="schnittchen" created="2013-07-15T17:35:26Z" id="20987361">Purging (both user and group were gone afterwards) and reinstalling helped.I now have a /etc/elasticsearch/logging.yml and things are logged into several filed in /var/log/elasticsearch/. Had to comment out MAX_OPEN_FILES in the init.d script though because ulimiting does not work in my vserver setup.

Thanks for your efforts!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"_score" stuff is not working with CustomFiltersScoreQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3323</link><project id="" key="" /><description>"_score" stuff is not working with CustomFiltersScoreQuery. it seems failed parsing of attribute "_score". Please find error details below.

Query Failed [Failed to execute main query]]; nested: PropertyAccessException[[Error: unresolvable property or identifier: _score] [Near : {... _score=_score+50 ....}] ^ [Line: 1, Column: 8]]; 

Could you please provide input to resolve this issue.
</description><key id="16712209">3323</key><summary>"_score" stuff is not working with CustomFiltersScoreQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kuttiKumarv</reporter><labels /><created>2013-07-13T07:41:59Z</created><updated>2013-07-17T06:37:36Z</updated><resolved>2013-07-17T06:37:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-13T11:56:35Z" id="20918929">Its intentional, each filter matching is simply going to add to the boost which the internal query score will be multiplied by. If you want full control over how score is calculated, using `custom_score`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search performance degrades after processing a large response</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3322</link><project id="" key="" /><description>In our testing we've found that search performance can degrade quite a bit after processing a large search response.  We've tracked this down to the usage of TIntObjectHashMap in HandlesStreamInput.  We found that after processing a large search response, the capacity of these maps increased dramatically.  Even though clear() is called on the maps in the reset() methods, the capacities of the maps stay high and adversely impact the performance of subsequent clear() calls (since clear() must iterate over the entire capacity of each map).

In local testing we've found that this performance issue can be fixed by re-creating the hash maps in the reset() methods if the capacity exceeds a certain limit (say 10k).  We also believe that this could be fixed by calling compact() on the maps after clear()  (since that will also reduce their capacity and thus make operations fast again).

Please let me know if you'd like any additional information about this issue or the proposed fix.
</description><key id="16693085">3322</key><summary>Search performance degrades after processing a large response</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">derekcicerone</reporter><labels /><created>2013-07-12T17:35:33Z</created><updated>2013-09-14T22:43:55Z</updated><resolved>2013-09-14T21:58:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-17T07:18:35Z" id="21096020">Interesting, what is the search request that you execute? Just wondering what can cause this list to grow that much?
</comment><comment author="derekcicerone" created="2013-07-17T09:09:01Z" id="21100672">That's a good question. We encountered this issue on one of our deployments when users noticed poor performance (searches taking upwards of 5-10 seconds versus the normal times of well under a second). Our initial investigation has focused on determining the symptoms of the slow performance and creating a workaround. We believe the issue begins when a single query results in a large number of search hits (possibly 100k - 1 million results of relatively small size: a dozen or so fields each containing a few words of stored text each). After our server processes a few queries like this and all elasticsearch client searcher threads enter the bad state, overall search performance degrades significantly. 

We worked around the issue by creating a patched jar which modifies the code I mentioned earlier. We have been testing with that jar in the same environment and under various stress scenarios for the past few days and so far it seems to resolve the issue. Longer term we are also going to be refactoring our queries to fix ones that can potentially return so many results (we are migrating from a legacy search architecture that didn't restrict the number of results provided for any given query so this will be somewhat tricky). We will also be looking into whether our schema needs to mark almost all fields as stored (also a carryover from the legacy system).

We are still actively testing the workaround to hopefully nail down a specific query and data set which reproduces the issue so we can conclusively verify the fix. Our platform performs searches of varying complexity in several places so we've had some trouble nailing down a specific problematic query.

I hope this information is helpful. Please let me know if I can provide any more context to aid your investigation. 

Derek

&gt; On Jul 17, 2013, at 3:19 AM, Shay Banon notifications@github.com wrote:
&gt; 
&gt; Interesting, what is the search request that you execute? Just wondering what can cause this list to grow that much?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="kimchy" created="2013-07-17T13:55:53Z" id="21114272">do the queries vary that much? a sample one would be a great start. I wonder specifically if you list fields to be returned as part of the search request?
</comment><comment author="derekcicerone" created="2013-07-17T17:06:12Z" id="21128073">Yeah, the queries vary quite a bit (there are likely over 100 unique queries across the platform).  We use them both interactively based on user-initiated searches and also programatically to find various objects.

Here is an example of a query that I recently rewrote to use the elasticsearch java APIs:

``` java
        SearchRequestBuilder request = this.searchEngine.getClient().prepareSearch(ServerSearchEngineService.INDEX_NAME)
            .setNoFields()
            .setQuery(QueryBuilders.matchAllQuery())
            .setScroll("1m")
            .setSearchType(SearchType.SCAN)
            .setSize(searchQueryPageSize)
            .setTypes(ItemType.MODEL.getType());
```

This query is used to scan all the objects of type "model" in the search index (there can be milllions of models).  The "searchQueryPageSize" constant is set to 100k.

Then there are other queries which we translate from our legacy API calls into elasticsearch calls.  They often execute with the size set to 100k and they may or may not specify the desired fields.  Here is an example of what one of those looks like in its JSON form:

``` json
{
  "size" : 21,
  "query" : {
    "bool" : {
      "must" : [ {
        "bool" : {
          "should" : {
            "match" : {
              "exactName" : {
                "query" : "g",
                "type" : "boolean",
                "boost" : 1.0
              }
            }
          }
        }
      }, {
        "bool" : {
          "should" : {
            "bool" : {
              "should" : [ {
                "match" : {
                  "assignableTypes" : {
                    "query" : "com.palantir.finance.commons.service.ontology.model.Model",
                    "type" : "boolean",
                    "boost" : 1.0
                  }
                }
              }, {
                "match" : {
                  "assignableTypes" : {
                    "query" : "com.palantir.finance.commons.service.document.Document",
                    "type" : "boolean",
                    "boost" : 1.0
                  }
                }
              }, {
                "match" : {
                  "assignableTypes" : {
                    "query" : "java.lang.Enum",
                    "type" : "boolean",
                    "boost" : 1.0
                  }
                }
              }, {
                "match" : {
                  "assignableTypes" : {
                    "query" : "com.palantir.finance.commons.service.authentication.User",
                    "type" : "boolean",
                    "boost" : 1.0
                  }
                }
              } ]
            }
          }
        }
      } ]
    }
  },
  "filter" : {
    "terms" : {
      "permissions" : [ 1000043, 2, 1003047 ]
    }
  },
  "fields" : "*"
}
```

When this particular query is run, we also run anywhere from 5-50 other queries in parallel at the same time.  The reason we run so many queries in parallel is that our legacy search system was not very expressive, so it was necessary to use a lot of queries to express some of the more complicated queries.  We'll also be looking into rewriting these queries to use the elasticsearch APIs in the near future.

I can also provide our mappings if you like as well (they are specified in a single JSON file).
</comment><comment author="kimchy" created="2013-09-14T21:58:55Z" id="24460061">closing as its fixed in the above commits.
</comment><comment author="derekcicerone" created="2013-09-14T22:43:55Z" id="24460682">Awesome, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms facet on ip field returns raw integers instead of ip addresses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3321</link><project id="" key="" /><description>Terms facet on ip field returns raw integers instead of ip addresses.

When asked:

```
curl -XGET http://172.16.0.134:9200/nginx-2013.07.12/_search?pretty -d'
{
  "facets": {
    "pie": {
      "terms": {
        "field": "clientip",
        "size": 10,
        "exclude": []
      },
      "facet_filter": {
        "fquery": {
          "query": {
            "filtered": {
              "query": {
                "query_string": {
                  "query": "*"
                }
              },
              "filter": {
                "range": {
                  "@timestamp": {
                    "from": "2013-07-12T12:09:30.122Z",
                    "to": "2013-07-12T12:14:30.122Z"
                  }
                }
              }
            }
          }
        }
      }
    }
  },
  "size": 0
}'
```

it sais:

```
{
  "took" : 43,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0
  },
  "hits" : {
    "total" : 1308357,
    "max_score" : 1.0,
    "hits" : [ ]
  },
  "facets" : {
    "pie" : {
      "_type" : "terms",
      "missing" : 0,
      "total" : 14220,
      "other" : 7100,
      "terms" : [ {
        "term" : 1472557106,
        "count" : 1727
      }, {
        "term" : 1501212747,
        "count" : 1621
      }, {
        "term" : 1556945832,
        "count" : 1616
      }, {
        "term" : 1498126311,
        "count" : 566
      }, {
        "term" : 1541613928,
        "count" : 438
      }, {
        "term" : 1541613412,
        "count" : 346
      }, {
        "term" : 2488386185,
        "count" : 227
      }, {
        "term" : 3245280414,
        "count" : 208
      }, {
        "term" : 2999036429,
        "count" : 198
      }, {
        "term" : 1299254797,
        "count" : 173
      } ]
    }
  }
}
```

Mappings:

```
curl -XGET http://172.16.0.134:9200/nginx-2013.07.12/_mapping?pretty
{
  "nginx-2013.07.12" : {
    "nginx" : {
      "_all" : {
        "enabled" : false
      },
      "_source" : {
        "compress" : true
      },
      "properties" : {
        "@timestamp" : {
          "type" : "date",
          "format" : "dateOptionalTime"
        },
        "agent" : {
          "type" : "string"
        },
        "app_id" : {
          "type" : "string"
        },
        "auth" : {
          "type" : "string"
        },
        "bytes" : {
          "type" : "integer"
        },
        "clientip" : {
          "type" : "ip"
        },
        "duration" : {
          "type" : "float"
        },
        "hosting" : {
          "type" : "string"
        },
        "httpversion" : {
          "type" : "string"
        },
        "ident" : {
          "type" : "string"
        },
        "message" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "physical" : {
          "type" : "string"
        },
        "referrer" : {
          "type" : "string"
        },
        "request" : {
          "type" : "string"
        },
        "request_size" : {
          "type" : "integer"
        },
        "response" : {
          "type" : "integer"
        },
        "source" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "source_host" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "source_path" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "tags" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "timestamp" : {
          "type" : "date",
          "format" : "dateOptionalTime"
        },
        "type" : {
          "type" : "string",
          "index" : "not_analyzed",
          "omit_norms" : true,
          "index_options" : "docs"
        },
        "verb" : {
          "type" : "string"
        },
        "vhost" : {
          "type" : "string"
        }
      }
    }
  }
}
```
</description><key id="16680841">3321</key><summary>Terms facet on ip field returns raw integers instead of ip addresses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mkaluza</reporter><labels /><created>2013-07-12T13:16:03Z</created><updated>2014-06-16T19:04:48Z</updated><resolved>2013-12-05T07:51:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-07-20T10:24:16Z" id="21291376">this is the same as: https://github.com/elasticsearch/elasticsearch/issues/2462
I don't really see it being fixed anytime soon(maybe on 1.0 this will be different?). 
Probably it makes sense for you to just have a multi field for this, having a value as an ip and another as string. Then you could use the first for whatever you are currently using, and the second to facet.
</comment><comment author="ralphm" created="2013-07-23T18:39:52Z" id="21436282">@lmenezes how does that help exactly? For facet queries like the above you'd still get facets that have to be processed before presenting them in a UI.

That said, I'm getting back IP addresses as strings when faceting with a field of type `ip` in a clean test.

However, I found that if there are other mappings, where a field by the same name has type `string`, I am getting responses like the following, which don't even look like IP addresses to me:

```
    "facets" : {
        "pie" : {
            "_type" : "terms",
            "missing" : 133599,
            "total" : 1683808,
            "other" : 701976,
            "terms" : [ {
                "term" : "\\\b",
                "count" : 105238
            }, {
                "term" : "X\u0001\u0000",
                "count" : 105238
            }, {
                "term" : "T\u0010\u0000",
                "count" : 105238
            }, {
                "term" : "P\u0002\u0000\u0000",
                "count" : 105238
            }, {
                "term" : "L \u0000\u0000",
                "count" : 105238
            }, {
                "term" : "H\u0004\u0000\u0000\u0000",
                "count" : 105238
            }, {
                "term" : "D@\u0000\u0000\u0000",
                "count" : 105238
            }, {
                "term" : "@\b\u0000\u0000\u0000\u0000",
                "count" : 105238
            }, {
                "term" : "&lt;\u0001\u0000\u0000\u0000\u0000\n",
                "count" : 70859
            }, {
                "term" : "8\u0010\u0000\u0000\u0000\u0001&amp;",
                "count" : 69069
            } ]
        }
    }
```
</comment><comment author="lmenezes" created="2013-07-23T19:00:47Z" id="21437716">@ralphm not really sure what you meant there... but running that:

```
curl -XPOST http://localhost:9200/foo

curl -XPUT http://localhost:9200/foo/bar/_mapping -d '{ "bar": { "properties": { "clientip": { "type": "multi_field", "fields": { "clientip": { "type": "ip" }, "clientip_facet": { "type": "string", "index": "not_analyzed" } } } } } }'

curl -XPUT http://localhost:9200/foo/bar/1 -d '{"clientip":"192.168.0.1"}'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{"clientip":"192.168.0.2"}'
curl -XPUT http://localhost:9200/foo/bar/3 -d '{"clientip":"192.168.0.3"}'
curl -XPUT http://localhost:9200/foo/bar/4 -d '{"clientip":"192.168.0.4"}'

curl -XGET http://localhost:9200/foo/bar/_search -d '{ "facets": { "pie": { "terms": { "field": "clientip", "size": 10 } } } }'
curl -XGET http://localhost:9200/foo/bar/_search -d '{ "facets": { "pie": { "terms": { "field": "clientip_facet", "size": 10 } } } }'
```

might give you an idea of what I meant. 
Of course you have to replicate some information here, but I see no better way currently for achieving the same.
</comment><comment author="ralphm" created="2013-07-23T19:53:37Z" id="21441072">@lmenezes Ah, yes. I just did the same thing on my own.

Interestingly, I now also see those IPs as integers on my other installation. I'm not entirely sure what the difference is between these two installs. They should be identical.
</comment><comment author="avleen" created="2013-10-31T06:55:29Z" id="27465636">Just for reference, I found that if you set  `"index" : "not_analyzed"` on the `ip` field, it doesn't break.
</comment><comment author="mkaluza" created="2013-10-31T16:08:04Z" id="27499361">Thanks for the tip. Unfortunately I need an index on this field :/

2013/10/31 avleen notifications@github.com

&gt; Just for reference, I found that if you set "index" : "not_analyzed" on
&gt; the ip field, it doesn't break.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3321#issuecomment-27465636
&gt; .
</comment><comment author="avleen" created="2013-10-31T16:16:00Z" id="27500193">That's fine :-)

Not analysing only means that analyser isn't run against the content of the
field. The analyser tokenizes the contacts for doing things like frequency
searches of characters in words the field will still be indexed, and you
will still be able to do searches against it, including wild card and range
searches.

For a while I was also under the impression that you had to analyse in
order to index this is simply not the case. In fact, with my input from log
stash, I make every field not analysed, except for the source field. It
speeds up indexing and reduces index size quite noticeably .
On 31 Oct 2013 11:09, "mkaluza" notifications@github.com wrote:

&gt; Thanks for the tip. Unfortunately I need an index on this field :/
&gt; 
&gt; 2013/10/31 avleen notifications@github.com
&gt; 
&gt; &gt; Just for reference, I found that if you set "index" : "not_analyzed" on
&gt; &gt; the ip field, it doesn't break.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub&lt;
&gt; &gt; https://github.com/elasticsearch/elasticsearch/issues/3321#issuecomment-27465636&gt;
&gt; &gt; 
&gt; &gt; .
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3321#issuecomment-27499361
&gt; .
</comment><comment author="stonith" created="2013-11-16T09:42:13Z" id="28623230">@avleen Term facets return ip's for you when your mapping is like: "clientip": { "type": "ip", "index": "not_analyzed" } ? I'm getting integers.

UPDATE: I just realized that "index": "not_analyzed" isn't a valid option for type ip: 

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-ip-type.html
</comment><comment author="rottenbytes" created="2013-12-05T07:36:54Z" id="29877116">:+1: on this one, it's pretty boring
</comment><comment author="dadoonet" created="2013-12-05T07:51:58Z" id="29877733">Heya,

The advice provided by @lmenezes could really help you guys to deal with this issue: https://github.com/elasticsearch/elasticsearch/issues/3321#issuecomment-21437716

I think we can close this one as #3300 (see IP Range) will fix it.

Feel free to reopen if you don't think so.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_bulk response for delete operations lack the `found` field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3320</link><project id="" key="" /><description>When deleting a document the classical way, we get the `found` field, telling us whether the document was already deleted or not:

```
$ curl -XDELETE 'localhost:9200/index/type/1'
{"ok":true,"found":true,"_index":"index","_type":"type","_id":"1","_version":2}
# `found` is true

$ curl -XDELETE 'localhost:9200/index/type/1'
{"ok":true,"found":false,"_index":"index","_type":"type","_id":"1","_version":3}
# `found` is now false
```

When using the `_bulk` api, we get the same answer, except for the missing `found` field:

```
$ curl -XDELETE 'localhost:9200/index/type/1' -d "$(echo -e '{"delete":{"_id":"1"}}\n ')"
{"took":1,"items":[{"delete":{"_index":"index","_type":"type","_id":"1","_version":1,"ok":true}}]}
# `found` is missing

$ curl -XDELETE 'localhost:9200/index/type/1' -d "$(echo -e '{"delete":{"_id":"1"}}\n ')"
{"took":1,"items":[{"delete":{"_index":"index","_type":"type","_id":"1","_version":2,"ok":true}}]}
# `found` is missing
```
</description><key id="16679009">3320</key><summary>_bulk response for delete operations lack the `found` field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-12T12:23:58Z</created><updated>2014-01-21T19:09:13Z</updated><resolved>2013-07-15T13:11:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added support for multiple indices in open/close index apis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3319</link><project id="" key="" /><description>Open/Close index api supports now multiple indices the same way as the delete index api works. The only exception is when dealing with all indices: it's required to explicitly use _all or a pattern that identifies all the indices, not just an empty array of indices.

Closes #3217
</description><key id="16676477">3319</key><summary>Added support for multiple indices in open/close index apis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-07-12T11:02:18Z</created><updated>2014-06-26T22:45:37Z</updated><resolved>2013-07-16T13:23:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Highlight array element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3318</link><project id="" key="" /><description>Here is mapping/settings url --&gt; https://gist.github.com/kuttiKumarv/5983294

document 1001 result:

```
highlight: {
    product.value: [
        &lt;em&gt;Scissors&lt;/em&gt; AMBAN101 operation kit &lt;em&gt;scissors&lt;/em&gt; release version 1.0
        &lt;em&gt;Scissors&lt;/em&gt; AMBAN104 operation kit &lt;em&gt;Scissors&lt;/em&gt; release version 1.0
    ]
}
```

my query here is "Is it possible to get product id along with highlight product value"

product.id=101  ===&gt;  &lt;em&gt;Scissors&lt;/em&gt; AMBAN101 operation kit &lt;em&gt;scissors&lt;/em&gt; release version 1.0
product.id=104  ===&gt;  &lt;em&gt;Scissors&lt;/em&gt; AMBAN104 operation kit &lt;em&gt;Scissors&lt;/em&gt; release version 1.0

thanks
Kumar.V
</description><key id="16675184">3318</key><summary>Elasticsearch Highlight array element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kuttiKumarv</reporter><labels><label>:Highlighting</label></labels><created>2013-07-12T10:20:14Z</created><updated>2014-08-08T12:48:33Z</updated><resolved>2014-08-08T12:48:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-16T09:48:08Z" id="21031609">currently, you will need to index each product as its own document, then you can get the product id back with the highlighting of it as part of a single hit.
</comment><comment author="spancer" created="2013-11-08T02:58:08Z" id="28029797">@kimchy Hi shay, does that mean for an array filed?

For example, I have an array filed "names":["spancer ray", "james bond", "james ray"],  while search across filed "names" with text "ray", can the highlight returns something like this below?

&lt;pre&gt;
&lt;code&gt;
highlight: {
    names: [
        spancer &amp;lt;em&amp;gt;ray&amp;lt;/em&amp;gt; , james bond, james  &amp;lt;em&amp;gt;ray&amp;lt;/em&amp;gt;
    ]
}
&lt;/code&gt;
&lt;/pre&gt;

I'd more like to have the above result returned instead of getting result like below

&lt;pre&gt;
&lt;code&gt;
highlight: {
names: [
spancer  &amp;lt;em&amp;gt;ray&amp;lt;/em&amp;gt;,
james  &amp;lt;em&amp;gt;ray&amp;lt;/em&amp;gt;
]
}
&lt;/code&gt;
&lt;/pre&gt;
</comment><comment author="clintongormley" created="2014-08-08T12:48:33Z" id="51595908">You need to use nested docs for this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix offsets handling of the n-gram and edge n-gram tokenizers and token filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3317</link><project id="" key="" /><description>Lucene 4.4 will fix these tokenizers and token filters so that they don't break highlighting anymore. So it would be nice to import them into Elasticsearch. See https://issues.apache.org/jira/browse/LUCENE-3907
</description><key id="16674384">3317</key><summary>Fix offsets handling of the n-gram and edge n-gram tokenizers and token filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-07-12T09:57:55Z</created><updated>2013-07-12T10:01:16Z</updated><resolved>2013-07-12T10:01:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-07-12T10:01:16Z" id="20868229">Already fixed in fccbe9c185889a981c9172d848e373b40826d10b and 03887d72a49cae01ac946cca27b2125fb7bddbab
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Special case the _index field in queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3316</link><project id="" key="" /><description>You can already use the `_id` field in a `term`/`terms` query and it does the right thing, in spite of the fact that the `_id` field doesn't really exist.  It would be nice to do the same thing with `_index`, instead of having to resort to the `indices` query.

The `_type` field does actually exist and is indexed, so adding `_index` would complete the trinity.
</description><key id="16646931">3316</key><summary>Special case the _index field in queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2013-07-11T18:46:47Z</created><updated>2015-07-06T10:34:41Z</updated><resolved>2015-07-06T10:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rename _boost "name" to "path"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3315</link><project id="" key="" /><description>All of the metadata fields (eg `_id`, `_routing` etc) allow you to configure the `path` to a document field, except the `_boost` field, which expects a `name` instead.

Please can we deprecate `name` and add support for `path` instead, to make all the settings consistent.
</description><key id="16645944">3315</key><summary>Rename _boost "name" to "path"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-07-11T18:28:33Z</created><updated>2014-01-09T15:05:57Z</updated><resolved>2014-01-09T10:10:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-01-09T10:10:41Z" id="31917314">Closing this issue as it turns out this is not only a rename, but also making the document boost work with a path instead of a simple name (that has max depth 1). That said, we are going to deprecate document boost in #4664.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>class org.apache.lucene.morphology.analyzer.MorphologyAnalyzer overrides final method tokenStream</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3314</link><project id="" key="" /><description>version 0.9.20 create index error 
IndexCreationException[[tovar] failed to create index]; nested: VerifyError[class org.apache.lucene.morphology.analyzer.MorphologyAnalyzer overrides final method tokenStream.(Ljava/lang/String;Ljava/io/Reader;)Lorg/apache/lucene/analysis/TokenStream;];
</description><key id="16616634">3314</key><summary>class org.apache.lucene.morphology.analyzer.MorphologyAnalyzer overrides final method tokenStream</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nekulin</reporter><labels /><created>2013-07-11T06:51:37Z</created><updated>2013-07-14T13:26:47Z</updated><resolved>2013-07-14T13:26:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-11T07:21:52Z" id="20794451">Looks like you are using the morphological analysis plugin (which is not part of the core elasticsearch distribution), which might not yet have been updated to the latest lucene version.

cc @imotov - IIRC you wrote the plugin, maybe you can take a look? :-)
</comment><comment author="nekulin" created="2013-07-11T08:00:01Z" id="20796174">Yes, I also thought about that, how to uninstall the plugin, do not tell me. I just blew elasticsearch and set again and the plugin campaign remained. Remove it from the folder analysis-morphology?
</comment><comment author="imotov" created="2013-07-14T13:26:47Z" id="20936396">@nekulin, this error means that you have created an index (called tovar) that is using MorphologyAnalyzer from the analysis-morphology plugin and the version of your analysis-morphology plugin is not compatible with your version of elasticsearch. You need to remove old version of the plugin and install appropriate version of the plugin. If you are using elasticsearch 0.90.2, you can do it by running the following commands from elasticsearch home directory:

```
$ bin/plugin -remove analysis-morphology
$ bin/plugin -install analysis-morphology -url http://dl.bintray.com/content/imotov/elasticsearch-plugins/elasticsearch-analysis-morphology-1.1.0.zip
```

If you still have any problems with it, please open an issue in the [elasticsearch-analysis-morphology](https://github.com/imotov/elasticsearch-analysis-morphology) project.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Closing an index right after it has been creating leaves it in an unopenable state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3313</link><project id="" key="" /><description>Closing an index right after it has been creating leaves it in an unopenable state.  Specifically, opening the index results in a bunch of shards being unassigned and not getting assigned automatically.  Reproduction curl commands ready to pase into a shell:  https://gist.github.com/nik9000/5970277
</description><key id="16600687">3313</key><summary>Closing an index right after it has been creating leaves it in an unopenable state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nik9000</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-10T21:00:55Z</created><updated>2013-07-26T20:35:58Z</updated><resolved>2013-07-26T20:35:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-14T17:18:26Z" id="20939811">Confirmed, this is indeed the case. There is simple workaround for this issue though - just make sure that the index gets to at least to yellow state before trying to close it. I am curious is there a particular use case that led to this being a problem?
</comment><comment author="nik9000" created="2013-07-14T19:48:42Z" id="20942464">On Sun, Jul 14, 2013 at 1:18 PM, Igor Motov notifications@github.comwrote:

&gt; Confirmed, this is indeed the case. There is simple workaround for this
&gt; issue though - just make sure that the index gets to at least to yellow
&gt; state before trying to close it. I am curious is there a particular use
&gt; case that led to this being a problem?

I was building a script that can build an index from scratch as well as
update it portions of its configuration are out of date and being lazy
about how I implemented it figuring that an empty index would be cheap to
open and close.  After I stopped being lazy and specifying the analysers
during index creation my problem went away.  It'd probably have been good
enough to add a note to
http://www.elasticsearch.org/guide/reference/api/admin-indices-open-close/and
I wouldn't have tried it.

Are there other non-index-creation cases that put the index in this state?
If so it might be worth implementing something stops the close action.

Nik
</comment><comment author="kimchy" created="2013-07-14T20:04:47Z" id="20942731">@imotov I believe as you mentioned that we can really open an index only after all the primary shards have been allocation at least once. This is because we can't recreate the `primaryAllocatedPostApi` flag (we could potentially, but its not supported now).

I suggest that a simple fix for now is to reject a close index request if one of its index shard routing info has the primaryAllocatedPostApi set to false.
</comment><comment author="nik9000" created="2013-07-18T11:07:11Z" id="21176419">While working on a patch for the issue kimchy mentioned I noticed a look alike issue:  https://gist.github.com/nik9000/6028478

I'll post another github issue after some more investigation.
</comment><comment author="imotov" created="2013-07-20T00:13:52Z" id="21284580">Hi @nik9000. Thanks for the PR. I am just thinking maybe we can remove `assertRed();` and `assert false;` from the test. This way we are still testing fast closing, but even if closing is not fast enough test wouldn't fail. I also feel that 128 shards might be a bit excessive. Maybe reduce it to 50 or even 20?

I looked at the related problem with quorum as well. Not really sure what we should do about it. Should we even allow creation of an index with 3 replicas and `index.recovery.initial_shards=quorum` on a single node? On the other side, even if we have 4 nodes, it's not always obvious if we can fulfill `index.recovery.initial_shards` requirements or not. So, we could store some flag in index metadata that would indicate that this index wasn't fully allocated at least once yet. And if this flag is set, LocalGatewayAllocator would ignore `requiredAllocation` or as in case of prematurely closed index create missing shards as needed. We could even use this flag to block any operations on such index, so it would be really obvious that this index is not in a proper state. @kimchy what do you think?
</comment><comment author="nik9000" created="2013-07-20T00:51:43Z" id="21285340">On Fri, Jul 19, 2013 at 8:14 PM, Igor Motov notifications@github.comwrote:

&gt; Hi @nik9000 https://github.com/nik9000. Thanks for the PR.

Thanks for taking the time to read it!

&gt; I am just thinking maybe we can remove assertRed(); and assert false;from the test. This way we are still testing fast closing, but even if
&gt; closing is not fast enough test wouldn't fail. I also feel that 128 shards
&gt; might be a bit excessive. Maybe reduce it to 50 or even 20?
&gt; 
&gt; I'm not sure it'd be a good test if sometimes it didn't verify anything.
&gt; If we were in JUnit I'd say we could use the Assume api but I'm not really
&gt; sure what the right thing is in TestNG.

As to the 128 shards it was just a number that seemed to trigger the
behavior.  IIRC 20 wouldn't have consistently triggered the problem on my
laptop.  50 probably would but I didn't want to risk someone having a
faster machine than mine and getting an unexpectedly useless/failing test.

&gt; I looked at the related problem with quorum as well. Not really sure what
&gt; we should do about it. Should we even allow creation of an index with 3
&gt; replicas and index.recovery.initial_shards=quorum on a single node? On
&gt; the other side, even if we have 4 nodes, it's not always obvious if we can
&gt; fulfill index.recovery.initial_shards requirements or not. So, we could
&gt; store some flag in index metadata that would indicate that this index
&gt; wasn't fully allocated at least once yet. And if this flag is set,
&gt; LocalGatewayAllocator would ignore requiredAllocation or as in case of
&gt; prematurely closed index create missing shards as needed. We could even use
&gt; this flag to block any operations on such index, so it would be really
&gt; obvious that this index is not in a proper state. @kimchyhttps://github.com/kimchywhat do you think?
&gt; 
&gt; For my book stopping people when they ask for a configuration that just
&gt; isn't going to fully allocate sounds like the right thing to do.  It'd
&gt; probably make sense to have a force flag that gets the unchecked behavior
&gt; but with a warning that things might not work properly if you don't bring
&gt; those nodes online.

What about the case where when you create the index everything makes sense
and allocates properly but then you lose a node?  Without that node you can
close the index but it won't open again until you bring that node back
online.  At least, that is what I saw when I was playing with
https://github.com/elasticsearch/elasticsearch/issues/3354.

&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3313#issuecomment-21284580
&gt; .
</comment><comment author="nik9000" created="2013-07-20T00:52:59Z" id="21285363">I could work the test so it used a small number of shards and just tried
again if it didn't hit the problem.  That wouldn't be too tough.  I'll have
a look at that sometime in the next few days.

On Fri, Jul 19, 2013 at 8:51 PM, Nikolas Everett nik9000@gmail.com wrote:

&gt; On Fri, Jul 19, 2013 at 8:14 PM, Igor Motov notifications@github.comwrote:
&gt; 
&gt; &gt; Hi @nik9000 https://github.com/nik9000. Thanks for the PR.
&gt; 
&gt; Thanks for taking the time to read it!
&gt; 
&gt; &gt; I am just thinking maybe we can remove assertRed(); and assert false;from the test. This way we are still testing fast closing, but even if
&gt; &gt; closing is not fast enough test wouldn't fail. I also feel that 128 shards
&gt; &gt; might be a bit excessive. Maybe reduce it to 50 or even 20?
&gt; &gt; 
&gt; &gt; I'm not sure it'd be a good test if sometimes it didn't verify anything.
&gt; &gt; If we were in JUnit I'd say we could use the Assume api but I'm not really
&gt; &gt; sure what the right thing is in TestNG.
&gt; 
&gt; As to the 128 shards it was just a number that seemed to trigger the
&gt; behavior.  IIRC 20 wouldn't have consistently triggered the problem on my
&gt; laptop.  50 probably would but I didn't want to risk someone having a
&gt; faster machine than mine and getting an unexpectedly useless/failing test.
&gt; 
&gt; &gt; I looked at the related problem with quorum as well. Not really sure what
&gt; &gt; we should do about it. Should we even allow creation of an index with 3
&gt; &gt; replicas and index.recovery.initial_shards=quorum on a single node? On
&gt; &gt; the other side, even if we have 4 nodes, it's not always obvious if we can
&gt; &gt; fulfill index.recovery.initial_shards requirements or not. So, we could
&gt; &gt; store some flag in index metadata that would indicate that this index
&gt; &gt; wasn't fully allocated at least once yet. And if this flag is set,
&gt; &gt; LocalGatewayAllocator would ignore requiredAllocation or as in case of
&gt; &gt; prematurely closed index create missing shards as needed. We could even use
&gt; &gt; this flag to block any operations on such index, so it would be really
&gt; &gt; obvious that this index is not in a proper state. @kimchyhttps://github.com/kimchywhat do you think?
&gt; &gt; 
&gt; &gt; For my book stopping people when they ask for a configuration that just
&gt; &gt; isn't going to fully allocate sounds like the right thing to do.  It'd
&gt; &gt; probably make sense to have a force flag that gets the unchecked behavior
&gt; &gt; but with a warning that things might not work properly if you don't bring
&gt; &gt; those nodes online.
&gt; 
&gt; What about the case where when you create the index everything makes sense
&gt; and allocates properly but then you lose a node?  Without that node you can
&gt; close the index but it won't open again until you bring that node back
&gt; online.  At least, that is what I saw when I was playing with
&gt; https://github.com/elasticsearch/elasticsearch/issues/3354.
&gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3313#issuecomment-21284580
&gt; &gt; .
</comment><comment author="imotov" created="2013-07-20T01:04:01Z" id="21285609">Interesting, I was able to consistently reproduce it with 10 shards (and even 5 in most cases), hence the suggested number. I was thinking about retrying logic as well, but you would still need to remove assertRed() to remove race condition between checking index health and closing the index and then do clean up of index that failed to create the issue. 
</comment><comment author="nik9000" created="2013-07-20T18:36:48Z" id="21298231">I've updated the pull request with a retry logic and it looks like I can
reproduce it with two shards!  I suppose I should have tried ratcheting
down the number rather than watching my logs.  Anyway I feel better with
the retry logic making sure the test actually does something but runs more
quickly if it can get away with it.

Nik

On Fri, Jul 19, 2013 at 9:04 PM, Igor Motov notifications@github.comwrote:

&gt; Interesting, I was able to consistently reproduce it with 10 shards (and
&gt; even 5 in most cases), hence the suggested number. I was thinking about
&gt; retrying logic as well, but you would still need to remove assertRed() to
&gt; remove race condition between checking index health and closing the index
&gt; and then do clean up of index that failed to create the issue.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3313#issuecomment-21285609
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature suggestion - dynamic template allowing _parent definition</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3312</link><project id="" key="" /><description>The use case is this - 

I have (rather big) documents that can have many sub-attributes which are updated frequently. These attributes are kept as child documents, with the big document as the parent.
The parent document can have arbitrary type, and those types are dynamic and can be created on the fly.

The child type has a common structure - &lt;parent-type&gt;_attr (so for example for parent "book" the child type will be "book_attr").

I want to be able to have a template that automatically defines the right parent mapping on the child type. 
So I need to be able to set a template that uses regex and not wildcard on the name and allows using captures from it in the mapping definition. so it will be something like - 

{
    "template" : "(.*)_attr",
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "attributes" : {
           "_parent" : {
            "type" : "{0}"
            }
        }
    }
}
</description><key id="16592173">3312</key><summary>Feature suggestion - dynamic template allowing _parent definition</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels /><created>2013-07-10T18:17:09Z</created><updated>2014-07-01T13:44:22Z</updated><resolved>2014-07-01T13:44:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Asimov4" created="2014-06-26T21:13:06Z" id="47280878">Do you imply:
1. that dynamic mappings for the _parent field don't work
or
2. that you would like a new way to set dynamic mappings with regexps
?
</comment><comment author="rore" created="2014-06-27T09:12:10Z" id="47323167">#2. I"d like to be able to define the index names the template works for as a regex and not a wildcard, and to be able to use capture groups from the regex inside the template definition. So, in my example, you can assign a template to catch specific format of index names, and automatically create a child type where the parent is derived from the captured index name.
</comment><comment author="clintongormley" created="2014-07-01T13:44:22Z" id="47657301">i'm -1 on this.  It just introduces a level of complexity that i'm uncomfortable with, plus it is not bwc.  Currently we allow simple patterns (ie shell globbing). We can't support those and regexes in the same place without having some flag... ie more complexity.

Elasticsearch should support common use cases simply, and allow more complex stuff to be done via the app.  I don't think the API should be bent to support corner cases, as it makes it more complex (and likely buggier) for everybody.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check for packaged config file in plugin install script</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3311</link><project id="" key="" /><description>In case you are running the plugin command from a packaged installation
like the debian or RPM package, a change of the path.plugins directory
in the elasticsearch.yml directory was ignored, because it was not loaded.

This change checks for the existence of /etc/elasticsearch/elasticsearch.yml
and the absence of $ES_HOME/config/elasticsearch.yml - in that case a package
installation is assumed and the -Des.default.config parameter is appended to
the JAVA_OPTS if it does not exist yet.

Closes #3304
</description><key id="16577601">3311</key><summary>Check for packaged config file in plugin install script</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-07-10T13:40:14Z</created><updated>2014-07-16T21:52:53Z</updated><resolved>2013-07-15T14:03:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-07-11T21:41:15Z" id="20844553">+1
</comment><comment author="kimchy" created="2013-07-11T21:43:03Z" id="20844660">why not use the same logic we pass the config file location in the packaged installation to bin/elasticsearch in bin/plugin? 
</comment><comment author="spinscale" created="2013-07-14T17:30:32Z" id="20940017">I am confused (hot weekend), what do you mean exactly? I dont see anything in the shell script, which could help us there
</comment><comment author="kimchy" created="2013-07-14T17:31:42Z" id="20940036">I mean the same we allow users to control the location of config when using bin/elasticsearch and its being used in teh deb pacakge, why not just do the same with the plugin file?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geo_shape null geometry</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3310</link><project id="" key="" /><description>I have a geojson file generated using ogr2ogr from a shape file.Unfortunately some of the geometry objects are null but I still want to index the other meta data. Currently I get a parse error on these documents because the geo_shape parser is not handling json nulls. I think it would be nicer to not fail and simply not index the geo_shape field for those fields. Would it be possible to fix this?

Here's a sample document. It fails on the geometry field (which is mapped to geo_shape). The error I get is: MapperParsingException[failed to parse [geometry]]; nested: ElasticSearchParseException[Shape must be an object consisting of type and coordinates];

{ "type": "Feature", "properties": { "name": "&#21513;&#20117;&#30010;&#23470;&#30000;", "qs_id": 856730, "gn_id": null, "woe_id": 28484701, "gn_id_eh": null, "woe_id_eh": null, "gn_name": null, "gn_ascii": null, "gn_country": null, "gn_admin1": null, "gn_admin2": null, "gn_pop": null, "gn_fclass": null, "gn_fcode": null, "woe_name": "&#21513;&#20117;&#30010;&#23470;&#30000;", "woe_nameen": null, "placetype": "LocalAdmin", "iso": "JP", "language": "JPN", "parent_id": 28379393, "woe_local": 28379393, "woe_lau": 28484701, "woe_adm2": 0, "woe_adm1": 58646425, "woe_adm0": 23424856, "name_local": "&#12358;&#12365;&#12399;&#24066;", "name_lau": "&#21513;&#20117;&#30010;&#23470;&#30000;", "name_adm2": null, "name_adm1": "&#31119;&#23713;&#30476;", "name_adm0": "&#26085;&#26412;", "gns_id": null, "accuracy": null, "matchtype": null, "geom_qual": null, "woe_funk": null, "photos": null, "photos_all": null, "woemembers": null, "photos_1k": null, "photos_9k": null, "photos_sr": 0, "photos_9r": 0, "pop_sr": 0 }, "geometry": null }
</description><key id="16575219">3310</key><summary>geo_shape null geometry</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels><label>bug</label><label>v1.0.0.Beta1</label></labels><created>2013-07-10T12:45:31Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-07-16T10:52:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-07-15T14:50:58Z" id="20974597">Hi @jillesvangurp, setting shapes to `null` is part of the geo-refactoring in 1.0 inspired by #2708. Nevertheless the current version also throws a parsing exception. I'll fix this for 1.0 as soon as possible.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Script based sorting is applied only after pagination</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3309</link><project id="" key="" /><description>When a query requests both sorting and pagination, the expected behavior is that sorting is executed _before_ pagination. And indeed this is the actual behavior when the query requests sorting on a field level. However, when doing script based sorting, it seems that sorting is only executed _after_ pagination. This is behavior is unexpected and especially surprising because the same sort may yield different results depending on whether it is specified on a field level or via a script.

Example

Suppose we have 100 files indexed, named "file001", "file002", ..., "file100", with the file name being stored in a "filename" field. Then the following query

```
{
  "from" : 0,
  "size" : 10,
  "query" : {
    "match_all" : { }
  },
  "sort" : [ {
    "filename" : {
      "order" : "asc"
    }
  } ]
}
```

yields the expected ordering "file001", "file002", ..., "file010". The seemingly equivalent query

```
{
  "from" : 0,
  "size" : 10,
  "query" : {
    "match_all" : { }
  },
  "sort" : [ {
    "_script" : {
      "script" : "doc['filename'].value",
      "type" : "string"
    }
  } ]
}
```

may return, for example, "file005", "file007", "file020", "file027", "file035", "file050", "file067", "file080", "file092", "file097". Which is pretty useless and certainly not what the client would expect.

This behavior was observed in ElasticSearch 0.90.2, but I didn't test previous versions.
</description><key id="16572372">3309</key><summary>Script based sorting is applied only after pagination</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">peschlowp</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-10T11:29:19Z</created><updated>2013-07-15T13:44:58Z</updated><resolved>2013-07-15T13:44:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-11T21:51:05Z" id="20845199">script sorting doesn't sort after pagination, can you provide a full curl recreation so we can check it out?
</comment><comment author="imotov" created="2013-07-12T02:41:24Z" id="20855388">It looks like a bug that was introduced between 0.90.0RC2 and 0.90.0. Here is a [repro](https://gist.github.com/imotov/5980913). I didn't have a chance to dig into it yet but it looks like it got broken after this [commit](https://github.com/elasticsearch/elasticsearch/commit/f372f7c109b550c6b20b8196713aa313ad6c249f).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distance scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3308</link><project id="" key="" /><description># 

It might sometimes be desirable to have a tool available that allows to multiply the original score for a document with a function that decays depending on the distance of a numeric field value of the document from a user given reference.

To use distance scoring on a query that has numerical fields, the user has to define
1. a reference and
2. a scale

for each field you want the function applied on. A reference is needed to define a distance for the document and a scale to define the rate of decay.

For each field in each found document, a decay function is be computed. The original score of the query is then multiplied with the individual function values computed for the fields. Distance scoring in this case behaves like a range query with smoothed box edges.

Distance scoring can be applied for an arbitrary number of numeric fields.
## Example

Suppose you are searching for a hotel in a certain town. Your budget is limited. Also, you would like the hotel to be close to the town center, so the farther the hotel is from the desired location the less likely you are to check in.
You would like the query results that match your criterion (for example, "hotel, Nancy, non-smoker") to be scored with respect to distance to the town center and also the price.

Intuitively, you would like to define the town center as the origin and maybe you are willing to walk 2km to the town center from the hotel.
In this case your _reference_ for the location field is the town center and the _scale_ is ~2km.

If your budget is low, you would probably prefer something cheap above something expensive.
For the price field, the _reference_ would be 0 Euros and the _scale_ depends on how much you are willing to pay, for example 20 Euros.
## Usage

In the above example, the fields might be called "price" for the price of the hotel and "location" for the coordinates of this hotel.

To apply distance scoring, embed any query in a distance_score query like this:

```
curl 'localhost:9200/hotels/_search/' -d '{
    "query" : {
        "distance_score" : {
            "query" : {
                 "match": { "properties": "balcony" }
            },
            "gauss" : {
                "location" : {
                    "reference": "11, 12",
                    "scale" : "2km"
                },
                "price" : {
                    "reference": "0",
                    "scale" : "20"
                }
            }
        }
    }
}'
```

The parameters (reference and scale) for the function (here is is "gauss") are given after the query for each field. Three different functions can be applied: Gauss ("gauss"), linear decay ("lin") and exponential decay ("exp").
## Supported fields

Only single valued numeric fields, including time and geo locations, are be supported.
## What is a field is missing?

Is the numeric field is missing in the document, that field will not be taken into account at all for this document. The function value for this field is set to 1 for this document.

Closes #3307
</description><key id="16535022">3308</key><summary>Distance scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-07-09T16:45:46Z</created><updated>2014-07-16T21:52:54Z</updated><resolved>2013-07-30T09:35:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-07-30T09:35:38Z" id="21779697">I opened a new pull request #3408 that replaces this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distance Scoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3307</link><project id="" key="" /><description># Distance scoring

It might sometimes be desirable to have a tool available that allows to multiply the original score for a document with a function that decays depending on the distance of a numeric field value of the document from a user given reference.

In the most simple case, for each field in each found document a decay function could be computed. The original score of the query could then be multiplied with the individual function values computed for the fields. Distance scoring in this case behaves like a range query with smoothed box edges.

Distance scoring could be applied for an arbitrary number of numeric fields.

To use distance scoring on a query that has numerical fields, the user would have to define at least
1. a reference and
2. a scale

for each field. A reference is needed to define a distance for the document and a scale to define the rate of decay.
## Example

Suppose you are searching for a hotel in a certain town. Your budget is limited. Also, you would like the hotel to be close to the town center, so the farther the hotel is from the desired location the less likely you are to check in.
You would like the query results that match your criterion (for example, "hotel, Nancy, non-smoker") to be scored with respect to distance to the town center and also the price. 

Intuitively, you would like to define the town center as the origin and maybe you are willing to walk 2km to the town center from the hotel.
In this case your _reference_ for the location field is the town center and the _scale_ is ~2km.

If your budget is low, you would probably prefer something cheap above something expensive. 
For the price field, the _reference_ would be 0 Euros and the _scale_ depends on how much you are willing to pay, for example 20 Euros. 
## Usage

In the above example, the fields might be called "price" for the price of the hotel and "location" for the coordinates of this hotel. 
For both fields, the user should be able to define a decay function and also how to combine the decay functions for different fields before the decay factor is multiplied to the score of the original query
The json request could look like this:

```
curl 'localhost:9200/hotels/_search/' -d '{
"query": {
    "function_score": {
        "functions": [
            {
                "DECAY_FUNCTION": {
                    "price": {
                        "reference": "0",
                        "scale": "20"
                    }
                }
            },
            {
                "DECAY_FUNCTION": {
                    "location": {
                        "reference": "11, 12",
                        "scale": "2km"
                    }
                }
            }
        ],
        "query": {
            "match": {
                "properties": "balcony"
            }
        },
        "score_mode": "multiply"
    }
}
}'
```
## Decay Functions

A huge variety of decay functions could be implemented. Here are three examples:
### Normal decay

A contour plot for the normal decay for two fields looks like this (if the decay functions are multiplied):

![gausscontour](https://f.cloud.github.com/assets/4320215/768157/cd0e18a6-e898-11e2-9b3c-f0145078bd6f.png)
![gausssurf](https://f.cloud.github.com/assets/4320215/768160/ec43c928-e898-11e2-8e0d-f3c4519dbd89.png)

Suppose your original search results matches three hotels : "Backback Nap", "Drink n Drive" and "BnB Bellevue". 
"Drink n Drive" is pretty far from your defined location (nealy 2 km) and is not too cheap (about 13 Euros) so it gets a low factor a factor of 0.56. "BnB Bellevue" and "Backback Nap" are both pretty close to the defined location but "BnB Bellevue" is cheaper, so it gets a multiplier of 0.86 whereas "Backpack Nap" gets a value of 0.66."

The multiplier to the original score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\exp(-\frac{(x-\mu)^2}{\sigma^2})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\exp(-\frac{(location_{doc}-\mu_l)^2}{2\sigma^2_l})\exp(-\frac{(price_{doc}-\mu_p)^2}{2\sigma^2_p})," title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

where &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mu_l"  /&gt;&lt;/a&gt; is the town center, &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\sigma_l"  /&gt;&lt;/a&gt; is the scale of the location (2km in this case), &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mu_p"  /&gt;&lt;/a&gt; is the price reference (0 Euros since your budget is low) and &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\sigma_p"  /&gt;&lt;/a&gt; is the scale of the price (20 Euros in this case).
### Exponential decay

A contour plot for the exponential decay for two fields looks like this:

![expcontour](https://f.cloud.github.com/assets/4320215/768161/082975c0-e899-11e2-86f7-174c3a729d64.png)
![expsurf](https://f.cloud.github.com/assets/4320215/768162/0b606884-e899-11e2-907b-aefc77eefef6.png)

The multiplier to the original score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\exp(-\frac{abs(x-\mu)^2}{\sigma})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\exp(-\frac{|location_{doc}-\mu_l|}{\sigma_l})\exp(-\frac{|price_{doc}-\mu_p|}{\sigma_p})," title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

where again &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mu_l"  /&gt;&lt;/a&gt; is the town center, &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\sigma_l"  /&gt;&lt;/a&gt; is the scale of the location (2km in this case), &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mu_p"  /&gt;&lt;/a&gt; is the price reference (0 Euros since your budget is low) and &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\sigma_p"  /&gt;&lt;/a&gt; is the scale of the price (20 Euros in this case).
### 'Linear' decay

A contour plot for the 'linear' decay for two fields looks like this:

![lincontour](https://f.cloud.github.com/assets/4320215/768164/1775b0ca-e899-11e2-9f4a-776b406305c6.png)
![linsurf](https://f.cloud.github.com/assets/4320215/768165/19d8b1aa-e899-11e2-91bc-6b0553e8d722.png)

The multiplier to the original score is computed as

&lt;a href="http://www.codecogs.com/eqnedit.php?latex=\frac{abs(x-\mu)^2}{\sigma})" target="_blank"&gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mathcal{S}(doc)=\max(\frac{\sigma_l-|location_{doc}-\mu_l|}{\sigma_l},0)\max(\frac{\sigma_p-|price_{doc}-\mu_p|}{\sigma_p},0)," title="\exp(-\frac{x-\mu}{2\sigma^2})," /&gt;&lt;/a&gt;

where again &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mu_l"  /&gt;&lt;/a&gt; is the town center, &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\sigma_l"  /&gt;&lt;/a&gt; is the scale of the location (2km in this case), &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\mu_p"  /&gt;&lt;/a&gt; is the price reference (0 Euros since your budget is low) and &lt;a href="http://www.codecogs.com/eqnedit.php" &gt;&lt;img src="http://latex.codecogs.com/gif.latex?\sigma_p"  /&gt;&lt;/a&gt; is the scale of the price (20 Euros in this case).

In contrast to the normal and exponential decay, this function actually sets the score to 0 if the field value exceeds the user gived scale value.
## Supported fields

Only single valued numeric fields, including time and geo locations, should be supported. 
## What is a field is missing?

Is the numeric field is missing in the document, that field should not be taken into account at all for this document.
## Consolidate with custom_boost_factor, custom_score and custom_filters_score

It might make sense to consolidate this functionality with the custom_boost_factor, custom_score and custom_filters_score. 
See https://github.com/elasticsearch/elasticsearch/issues/3407
</description><key id="16534404">3307</key><summary>Distance Scoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels /><created>2013-07-09T16:32:23Z</created><updated>2013-08-01T12:55:03Z</updated><resolved>2013-08-01T12:55:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brwe" created="2013-08-01T12:55:03Z" id="21934163">Replaced by issue #3423
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make field data able to support more than 2B ordinals per segment.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3306</link><project id="" key="" /><description>Although segments are limited to 2B documents, there is not limit on the number
of unique values that a segment may store. This commit replaces 'int' with
'long' every time a number is used to represent an ordinal and modifies the
data-structures used to store ordinals so that they can actually support more
than 2B ordinals per segment.

This commit also improves memory usage of the multi-ordinals data-structures
and the transient memory usage which is required to build them (OrdinalsBuilder)
by using Lucene's PackedInts data-structures. In the end, loading the ordinals
mapping from disk may be a little slower, field-data-based features such as
faceting may be slightly slower or faster depending on whether being nicer to
the CPU caches balances the overhead of the additional abstraction or not, and
memory usage should be better in all cases, especially when the size of the
ordinals mapping is not negligible compared to the size of the values (numeric
data for example).
</description><key id="16534051">3306</key><summary>Make field data able to support more than 2B ordinals per segment.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-07-09T16:24:46Z</created><updated>2014-06-14T08:48:56Z</updated><resolved>2013-07-19T07:29:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-07-11T12:13:08Z" id="20806753">Hi,

I like it - reusing existing lucene infrastructure makes the code much cleaner and removes quite a bit.

I do wonder whether the performance penalty (almost double for multi valued string fields) is worth the memory reduction. I understand the bottleneck is in the compression logic, if so I have two ideas that may help:
1) The previous ordinals implementation didn't use packedints (they were used in the builder, but `SparseMultiArrayOrdinals` uses `PositiveIntPool` which isn't packed). Maybe we try only using packedints rather then compressed packed ints? I wonder how much compression help anyway because ordinal distributions are, I think, fairly random accessing both small and large numbers for many docs. That means that the deltas will be big.  Perhaps it is worth while to check whether non compressed packedints will give similar memory benefits with less cpu overhead.
2) The facets use an iterator to access the ordinals for every doc ( see `MultiOrdinals.MultiIter`). Currently the iterator just asks the `AppendingLongBuffer` for integers on by one. Perhaps we can push iteration down by adding an iterator to `AppendingLongBuffer`? This means we don't have to figure out which block &amp; min value to use with every access.

Does this make sense?

I noticed you removed the caching of the count arrays used in ordinal based caching I'm not the one to voice an opinion about whether it's good or not but I know this was under discussion previously. I want to make sure it gets some attention.

Last I noticed in your test timing that mult-valued long facets takes ~2.5 times those of multi value strings. We discussed it and it has to do with the fact that numeric values always use maps. Given this performance difference, I wonder whether we should invest in an ordinal based implementation for numeric data as well - especially if there are not so many values. This _is_ another issue, though, which can do/discuss later.

Cheers,
Boaz
</comment><comment author="jpountz" created="2013-07-11T13:17:59Z" id="20810025">&gt; I do wonder whether the performance penalty (almost double for multi valued string fields) is worth the memory reduction.

I wanted to keep things simple for the first iteration of this pull request, but if we think performance is worth a little more complexity, I think we could make things faster by eg. not using paged implementations when the total number of ordinals remains small.

But if we start optimizing this stuff, I think we should make sure the benchmark makes sense. Eg. right now it runs an unfiltered `match_all` query which is very nice to uncompressed data since memory accesses are completely sequential and all values are visited.

&gt; SparseMultiArrayOrdinals uses PositiveIntPool

At first, I only wanted to fix the API in the pull request but PositiveIntPool decided me to change OrdinalsBuilder as well: if we had only replaced ints with longs, PositiveIntPool would have become a PositiveLongPool and memory usage would have increased by 2x.

&gt; adding an iterator to AppendingLongBuffer

I thought about it too. AppendingLongBuffer today has an iterator but the way it works is that even if you want to read a single value, it will decompress a block of 1024 values. If you are visiting every document (`match_all` query) this is OK because you will visit every value anyway but if your documents have only a few values and if the query is selective (eg. if it visits 1 every 1000 documents) then block decompression might be slower.

&gt; I noticed you removed the caching of the count arrays used in ordinal based caching I'm not the one to voice an opinion about whether it's good or not but I know this was under discussion previously. I want to make sure it gets some attention.

The reasoning I had is that the cached int[] was replaced with a paged array which has fixed-size pages (16K) and should thus be GC-friendly. I'm not enough familiar with GC to know whether it is enough to skip recycling so I'm open to suggestions

&gt;  Given this performance difference, I wonder whether we should invest in an ordinal based implementation for numeric data as well - especially if there are not so many values. 

Actually our multi-valued field data impl for longs have ordinals, they are just not used for faceting, see the end of TermsFacetParser:

``` java
        if (indexFieldData instanceof IndexNumericFieldData) {
            IndexNumericFieldData indexNumericFieldData = (IndexNumericFieldData) indexFieldData;
            if (indexNumericFieldData.getNumericType().isFloatingPoint()) {
                return new TermsDoubleFacetExecutor(indexNumericFieldData, size, comparatorType, allTerms, context, excluded, searchScript);
            } else {
                return new TermsLongFacetExecutor(indexNumericFieldData, size, comparatorType, allTerms, context, excluded, searchScript);
            }
        } else {
            if (script != null || "map".equals(executionHint)) {
                return new TermsStringFacetExecutor(indexFieldData, size, comparatorType, allTerms, context, excluded, pattern, searchScript);
            } else if (indexFieldData instanceof IndexFieldData.WithOrdinals) {
                return new TermsStringOrdinalsFacetExecutor((IndexFieldData.WithOrdinals) indexFieldData, size, comparatorType, allTerms, context, excluded, pattern, ordinalsCacheAbove);
            } else {
                return new TermsStringFacetExecutor(indexFieldData, size, comparatorType, allTerms, context, excluded, pattern, searchScript);
            }
        }
```

For strings data we return a specific executor when the field data impl has ordinals but we don't for numeric field data.
</comment><comment author="s1monw" created="2013-07-19T07:44:42Z" id="21236059">nice - thanks @jpountz 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_explain endpoint does not work with has_child query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3305</link><project id="" key="" /><description>Steps to reproduce:

Create a child type: 

PUT http://es:9200/hrtest/child/_mapping
{
    "child" : {
        "_parent" : {
            "type" : "parent"
        }
    }
}

Index a parent:
POST http://es:9200/hrtest/parent/p1
{
    "name" : "my parent" 
}

Index a child: 
POST http://es:9200/hrtest/child/c1?parent=p1
{
    "path" : "/top" 
}

Try searching:
POST http://es:9200/hrtest/parent/_search
{
  "query": {
    "has_child": {
      "type": "child",
      "query": {
        "bool": {
          "must": [
            {
              "term": {
                "path": "/top"
              }
            }
          ]
        }
      }
    }
  }
}

Now try _explain:
POST http://es:9200/hrtest/parent/p1/_explain
{
  "query": {
    "has_child": {
      "type": "child",
      "query": {
        "bool": {
          "must": [
            {
              "term": {
                "path": "/top"
              }
            }
          ]
        }
      }
    }
  }
}

You get an error:
{
    "error": "ElasticSearchIllegalStateException[has_child filter hasn't executed properly]",
    "status": 500
}
</description><key id="16525500">3305</key><summary>_explain endpoint does not work with has_child query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">rore</reporter><labels /><created>2013-07-09T13:50:46Z</created><updated>2014-03-05T20:19:23Z</updated><resolved>2014-03-05T20:19:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-07-16T07:43:28Z" id="21026158">The explain function for parent child queries was never implemented. This error doesn't relate to this, but to the fact the explain api doesn't invoke these queries properly. I think this is easily fixable.

If this is fixed then the explain api will not explain the specific parent child queries, but is able to explain the other queries that have been specified.
</comment><comment author="kimchy" created="2013-09-19T20:37:55Z" id="24771353">@martijnvg I suggest at least we will try and have a good failure for this somehow? it can be very confusing to figure out specifically when running search with explain flag (where it happens as well)
</comment><comment author="martijnvg" created="2013-09-19T21:03:47Z" id="24773164">@kimchy Make sense, I'll go and check it out.
</comment><comment author="lebowitz" created="2014-03-05T00:03:01Z" id="36695154">Hit this today.  It is a bit disconcerting when a query result doesn't have an explain plan.  I thought I had done something wrong.  @martijnvg good to see you again, we met at Basis Tech.
</comment><comment author="martijnvg" created="2014-03-05T09:03:34Z" id="36722101">Hi @lebowitz What ES version are you using? In older 0.90.x versions this used to generate an error like is mentioned above. 

If you use ask for a explain in the latest 0.90.x versions and 1.0.x versions the explain api should tell you inside the explain response that the parent child queries don't implement the explain function.
</comment><comment author="lebowitz" created="2014-03-05T12:27:07Z" id="36737214">I'm on 90.9.  Thanks for the explain
</comment><comment author="martijnvg" created="2014-03-05T20:19:23Z" id="36788470">The actual error has been fixed and I opened #5351 for adding the actual explain support.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>bin/plugin in Debian package ignores path.plugins from /etc/elasticsearch/elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3304</link><project id="" key="" /><description>bin/plugin script in Debian package does not pass -Des.default.config to ElasticSearch.

As a result, if path.plugins is changed in /etc/elasticsearch/elasticsearch.yml, bin/plugin will install plugins to the wrong path.
</description><key id="16522452">3304</key><summary>bin/plugin in Debian package ignores path.plugins from /etc/elasticsearch/elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dottedmag</reporter><labels /><created>2013-07-09T12:42:19Z</created><updated>2013-08-02T07:27:52Z</updated><resolved>2013-08-02T07:27:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-10T10:02:46Z" id="20732503">I think it is possible to peek in the configuration directories of the the rpm/deb first and then execute the rest, I will try that soon...
</comment><comment author="spinscale" created="2013-07-15T14:08:28Z" id="20971632">referenced the wrong issue, reopening, sorry
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST error readability</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3303</link><project id="" key="" /><description>Errors returned by the REST API are not easily human readable due to the way the data is stored in the `error` property.
Eg. https://gist.github.com/missinglink/e3cb9b127ae00e8c561c

You can 'prettify' the results, but it is still un-readable.
Eg. https://gist.github.com/pecke01/5956684

Note: (this specific error was caused by invalid syntax, the outer curly brackets were missing)

This issue makes it difficult for beginners to understand syntax errors and for advanced users to debug quickly when they make silly errors.

Ideally usage of `?pretty=1` would return "developer friendly" messages.

Any thoughts / suggestions? Maybe there is a tool which may help?
</description><key id="16520767">3303</key><summary>REST error readability</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">missinglink</reporter><labels /><created>2013-07-09T11:57:03Z</created><updated>2015-04-27T17:44:57Z</updated><resolved>2015-04-24T07:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="damienalexandre" created="2013-07-10T13:58:23Z" id="20743679">When I have this kind of error, I'm always opening my ElasticSearch logs. Errors are way more complete and usefull in there! Look at what I get for an undesirable `,` at the end of my query: https://gist.github.com/damienalexandre/5966477#file-gistfile1-txt-L38

I have the full stack trace, and the exception line 38:

&gt; org.elasticsearch.common.jackson.core.JsonParseException: Unexpected character ('}' (code 125)): was expecting either valid name character (for unquoted name) or double-quote (for quoted) to start field name

And my query is readable, as I sent it.

I agree that errors returned by the API could be improved :+1: 
</comment><comment author="markharwood" created="2014-09-05T09:01:56Z" id="54601040">I think the right long-term answer here (for query parse failures at least) is to change our query parsing logic so that when it encounters an error it can somehow annotate the point in the original JSON with the exact problem in the same way search highlighting shows where search terms are found in text or an IDE might highlight a compilation error.

This is clearly an ambitious goal so marking this issue as "high hanging fruit" in order to explore what can be done there. 
</comment><comment author="missinglink" created="2014-09-05T13:42:25Z" id="54626027">In the case of `json` parser errors, I agree with @markharwood  that ideally it would behave somewhat like the `js` parser in nodejs; which highlights the position the error occurred and facilitates rapid debugging.

``` bash
/tmp/broken.js:5
var broken code here;
           ^^^^
SyntaxError: Unexpected identifier
```

This however is presentation logic which should not be the role of the API layer; however if it provided such granular information that allowed me to produce a view like the above then I'd be pretty happy.

The general issue is that the error messages don't appear to be machine readable nor human readable.

In the gist example above, the message I am scanning for is:
`JsonParseException[Unexpected character (':' (code 58)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')\n at [Source: [B@16953ad9; line: 2, column: 13]]`

The rest of the message is pretty much just noise and contains a bunch of characters which make machine parsing this section very difficult. Also any attempt to parse error messages in such a way would be very bad practise as they are bound to change over time.

Since we're talking `json` I think a familiar model for the errors could be one similar to the [native javascript Error object](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Error) and would look roughly something like this:

``` javascript
{
  "error": "I exist for backward compatibility",
  "status": 500,
  "message": "I am a human readable short description of the error",
  "class": "I am the Class of the error",
  "lineNumber": "I am very useful",
  "columnNumber": "as am I",
  "input": "I am the original input as related to the line and column numbers"
}
```

I guess the discussion should really be about how external errors are modelled, as they are currently not structured at all?
</comment><comment author="missinglink" created="2014-10-07T12:28:53Z" id="58176276">@polyfractal I saw you wrote a 'pretty error' parser for inquisitor, do you have any suggestions? 

ref: https://github.com/polyfractal/elasticsearch-inquisitor#pretty-errors
</comment><comment author="polyfractal" created="2014-10-07T21:40:15Z" id="58267573">@missinglink Not much useful help, unfortunately.  The parser in Inquisitor is _very_ naive.  It recognizes that ES exceptions tend to be large blocks of exceptions surrounded by brackets (`[...]`), with interior exceptions starting with `nested:`. It [uses those two rules to split](https://github.com/polyfractal/elasticsearch-inquisitor/blob/master/_site/js/controllers/QueryInput.js#L94-L114).

It's a good "80%" solution, but fails for a variety of exceptions that don't follow the same pattern.  And it definitely doesn't produce machine-readable output, just something that humans can digest (since the most "interior" exception is usually the one you want, that's what Inquisitor shows as the main error)
</comment><comment author="rashidkpc" created="2014-10-10T17:06:30Z" id="58685021">This is really important for Kibana as well, right now we have a rather difficult time informing the user what went wrong. +1 to the modeling it on the javascript error object. Error types/codes might be useful as well.
</comment><comment author="zakmagnus" created="2014-11-07T23:32:14Z" id="62231129">What if the "parse error" is just that the query is too long? It might not be a good idea to parrot the entire thing back out to the caller. Also, note how silly it is that each of your shards gives you the same exact error. Constructing that string in memory (again, consider the case of an overlong query) might kill your node. If you OOM trying to build an error message, then you're going to receive a very unhelpful response.

Here is a thought: why is it necessary for the response to contain the request? Shouldn't the requester already know what its own request looks like? It seems reasonable to me that if the response is just "parse error starting at character ###, unexpected XYZ", then the requester can reasonably be expected to piece that together into a pretty display if it cares to.
</comment><comment author="polyfractal" created="2014-11-08T14:15:17Z" id="62259031">&gt; What if the "parse error" is just that the query is too long? It might not be a good idea to parrot the entire thing back out to the caller. Constructing that string in memory (again, consider the case of an overlong query) might kill your node. If you OOM trying to build an error message, then you're going to receive a very unhelpful response.

This generally isn't a problem, since ES places a limit on the size of the request body via the `http.max_content_length` setting (which defaults to 100mb).  

So unless the limit has been set unreasonably high, or the search threadpool has been increased to allow an unreasonable number of queries to execute simultaneously, I think it is unlikely you would get an OOM in this particular part of the code.

Note: if the query would have caused an OOM when writing the parse error exception, it probably would have triggered an exception upstream in the networking stack first :)

&gt; Also, note how silly it is that each of your shards gives you the same exact error. 

It's possible, although not common, for shards to return different errors (e.g. forgot to install a plugin on one node, different mappings if searching cross-index, corruption, etc).  

&gt; Here is a thought: why is it necessary for the response to contain the request? Shouldn't the requester already know what its own request looks like? It seems reasonable to me that if the response is just "parse error starting at character ###, unexpected XYZ", then the requester can reasonably be expected to piece that together into a pretty display if it cares to.

I agree with your sentiment, but you'd be surprised (or disappointed) at how many production environments don't log their own queries.  Many systems have no idea what's going through them and rely on the ES logs for retrospective analysis.

I think removing the queries from the exceptions would make them even more irritating, since many people don't log their own queries client-side.
</comment><comment author="zakmagnus" created="2014-11-09T21:01:49Z" id="62319462">I have actually seen OOM's like this. The problem is not the raw amount of data, but rather the fact that it all gets concatenated together into one contiguous string. Also, again, each shard reports the same thing, with the original query embedded in it. So, it tries to build up a string of length roughly (number of shards) x (length of query). That can be much longer than any individual request is allowed to be.

I am indeed surprised to learn that many clients apparently don't know what query they're getting a response for. I didn't think about anything like that when I made my suggestion. Still, how about making it somehow configurable, whether the original request has to be part of an error message? The realistic set of clients should be supported, but I also think that reasonable clients should have a way to operate reasonably. If it's configurable, then everyone can have what they need.
</comment><comment author="rashidkpc" created="2015-03-03T15:41:35Z" id="76969450">Any additional thoughts on the `?pretty` method of returning developer/machine friendly errors? I'd love to at least get the name of the exceptions involved in an array. Ideally with key information points about each. But at least the names of the exceptions.
</comment><comment author="rashidkpc" created="2015-04-10T21:23:43Z" id="91694988">**poke**
</comment><comment author="szeitlin" created="2015-04-13T21:24:45Z" id="92504776">yeah, I second this request. I just got one of these errors. 
</comment><comment author="markharwood" created="2015-04-14T09:43:19Z" id="92722917">@s1monw  is keen to overhaul the parser framework and so this issue is likely to be addressed as part of that activity. Currently all data nodes parse the same JSON query and generally report the same parse issues. In Simon's proposal only the receiving node performs parsing/validation and for valid queries sends out non-JSON representations of the query to data nodes for execution. This is a big change and so improved error management will be addressed as part of that refactoring.
</comment><comment author="s1monw" created="2015-04-14T10:43:39Z" id="92764101">+1 to what @markharwood said. It's not helpful to add half baked solutions IMO we should get these exceptions when we parse the query and then we can render all the required things. Today we do that too late (on the executing node) and we are moving query parsing to the coordinating node (REST node) so we can throw the right exception and render more infos. I like to do these things right even if it takes much longer sorry for the delay.
</comment><comment author="rashidkpc" created="2015-04-14T23:52:53Z" id="93117523">Important to consider this isn't limited to query parsing. This goes much deeper than the node that generates the error.

Simply moving validation to the coordinating node won't fix the premise of this ticket, which is that that REST errors are neither human nor machine readable. For example, invalid scripts will return a concatenated string leaving us with no way to tell the user what went wrong. 

```
{
    "responses": [
        {
            "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.09][0]: SearchParseException[[logstash-2015.04.09][0]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@364f85f&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.10][0]: SearchParseException[[logstash-2015.04.10][0]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@222fa4da&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.11][0]: SearchParseException[[logstash-2015.04.11][0]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@6c4d674c&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.12][0]: SearchParseException[[logstash-2015.04.12][0]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@4db8e620&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.13][0]: SearchParseException[[logstash-2015.04.13][0]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@2ecffeb5&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.13][1]: SearchParseException[[logstash-2015.04.13][1]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@4db8e620&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.14][1]: SearchParseException[[logstash-2015.04.14][1]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@595b3515&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.13][2]: SearchParseException[[logstash-2015.04.13][2]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@6c4d674c&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.14][2]: SearchParseException[[logstash-2015.04.14][2]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@57f2c505&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.13][3]: SearchParseException[[logstash-2015.04.13][3]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@57f2c505&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.14][3]: SearchParseException[[logstash-2015.04.14][3]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@6ceec13a&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.13][4]: SearchParseException[[logstash-2015.04.13][4]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@65eff5be&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }{[Y0G3ct1dSgir04D--V8XEA][logstash-2015.04.14][4]: SearchParseException[[logstash-2015.04.14][4]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[-1],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@18841706&gt;!]: Parse Failure [Failed to parse source [{\"size\":500,\"sort\":{\"@timestamp\":\"desc\"},\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"analyze_wildcard\":true,\"query\":\"Squirrels*\"}},\"filter\":{\"bool\":{\"must\":[{\"range\":{\"@timestamp\":{\"gte\":1428649200000,\"lte\":1429426799999}}}],\"must_not\":[]}}}},\"highlight\":{\"pre_tags\":[\"@kibana-highlighted-field@\"],\"post_tags\":[\"@/kibana-highlighted-field@\"],\"fields\":{\"*\":{}}},\"aggs\":{\"2\":{\"date_histogram\":{\"field\":\"@timestamp\",\"interval\":\"3h\",\"pre_zone\":\"-07:00\",\"pre_zone_adjust_large_interval\":true,\"min_doc_count\":0,\"extended_bounds\":{\"min\":1428649200000,\"max\":1429426799999}}}},\"fields\":[\"*\",\"_source\"],\"script_fields\":{\"chunk\":{\"script\":\"deadbeef\",\"lang\":\"expression\"}},\"fielddata_fields\":[\"@timestamp\",\"utc_time\",\"relatedContent.article:modified_time\",\"relatedContent.article:published_time\"]}]]]; nested: ExpressionScriptCompilationException[Unknown variable [deadbeef] in expression]; }]"
        }
    ]
}
```

In this it would be great to get something like (could certainly be improved upon):

```
[{
 exception: "ExpressionScriptCompilationException",
 details: {
  type: "UnknownVar",
  variable: "deadbeef"
 }
}]
```

Or if a shard fails due to corruption, all we can tell the user is that the shard failed, we have no idea why, and neither do they. 

```
"failures": [
                    {
                        "index": "logstash-2015.04.14",
                        "shard": 1,
                        "status": 500,
                        "reason": "QueryPhaseExecutionException[[logstash-2015.04.14][1]: query[filtered(_all:squirrels*)-&gt;BooleanFilter(+cache(@timestamp:[1428649200000 TO 1429426799999]))],from[0],size[500],sort[&lt;custom:\"@timestamp\": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@2b92372&gt;!]: Query Failed [Failed to execute main query]]; nested: RuntimeException[java.io.IOException: read past EOF: NIOFSIndexInput(path=\"/Users/khanr/Projects/elasticsearch/data/beer/nodes/0/indices/logstash-2015.04.14/1/index/_a_Lucene41_0.tim\") off: 0 len: 1024 pos: 9943 chunkLen: 1024 end: 25880: NIOFSIndexInput(path=\"/Users/khanr/Projects/elasticsearch/data/beer/nodes/0/indices/logstash-2015.04.14/1/index/_a_Lucene41_0.tim\")]; nested: IOException[read past EOF: NIOFSIndexInput(path=\"/Users/khanr/Projects/elasticsearch/data/beer/nodes/0/indices/logstash-2015.04.14/1/index/_a_Lucene41_0.tim\") off: 0 len: 1024 pos: 9943 chunkLen: 1024 end: 25880: NIOFSIndexInput(path=\"/Users/khanr/Projects/elasticsearch/data/beer/nodes/0/indices/logstash-2015.04.14/1/index/_a_Lucene41_0.tim\")]; nested: EOFException[read past EOF: NIOFSIndexInput(path=\"/Users/khanr/Projects/elasticsearch/data/beer/nodes/0/indices/logstash-2015.04.14/1/index/_a_Lucene41_0.tim\") off: 0 len: 1024 pos: 9943 chunkLen: 1024 end: 25880]; "
                    }
                ]
```

Here I'd love to have:

```
[{
 exception: "EOFException",
 details: {
  type: "ReadPastEOF",
  path: "/Users/khanr/Projects/elasticsearch/data/beer/nodes/0/indices/logstash-2015.04.14/1/index/_a_Lucene41_0.tim",
  off: 0, 
  len: 1024, 
  pos: 9943, 
  chunkLen: 1024, 
  end: 25880
 }
}]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added source fetching and filtering parameters to search, get, multi-get, get-source and explain requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3302</link><project id="" key="" /><description>Closes #3301
</description><key id="16515886">3302</key><summary>Added source fetching and filtering parameters to search, get, multi-get, get-source and explain requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-07-09T09:39:51Z</created><updated>2014-07-16T21:52:55Z</updated><resolved>2013-07-30T11:23:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add finer control over `_source` retrieval, in `get`, `mget`, `get_source`, `explain` &amp; `search` API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3301</link><project id="" key="" /><description>At the moment all of the above API offer the `field` parameter to retrieve part of the stored documents. However, the `fields` option was built to expose Lucene's stored fields and thus has some limitations when use to extract data from `_source`. The most important one is potentially flatting the document structure. 

This feature adds a new parameter that allows directly retrieving parts of the `_source`, without conforming to the store fields structure.

To maintain backward compatibility, you can still retrieve the `_source` by specifying `fields=["_source"]` but this special treatment will be removed in the future. 
## Get API

The Get api parameters are supplied via the query string. New `_source`,`_source_include` &amp; `_source_exclude` parameters are added, according to the following:

**A flag to control _source retrieval**

```
curl -XGET 'http://localhost:9200/index/type/1?_source=false'
```

or (default)

```
curl -XGET 'http://localhost:9200/index/type/1?_source=true'
```

**Only retrieve part of the source**

```
curl -XGET 'http://localhost:9200/index/type/1?_source=title,author'
```

or 

```
curl -XGET 'http://localhost:9200/index/type/1?_source_include=title,content&amp;_source_exclude=content.full_text'
```
## Multi Get API

The Multi Get API allows you to control `_source` both on the query string (same syntax as the `get` API) or on a per document basis.

**Query String defaults**

```
curl -XGET 'http://localhost:9200/index/type/_mget?_source=false' -d'{
  ids: [1, 2, 3]
}'
```

or

```
curl -XGET 'http://localhost:9200/index/type/_mget?_source_include=title,content&amp;_source_exclude=content.full_text' -d'{
  ids: [1, 2, 3]
}'
```

etc.

**Per document settings**

```
curl -XGET 'http://localhost:9200/_mget' -d '{
    docs: [
        { "_index": "test" , _type: "type1", "_id": "1", "_source": false },
        { "_index": "test" , _type: "type1", "_id": "2", "_source": "title" },
        { "_index": "test" , _type: "type1", "_id": "3", "_source": [ "title", "author" ] },
        { "_index": "test" , _type: "type1", "_id": "4", 
          "_source": { "include": "content" , "exclude" : "content.full_text" }  
        },
        { "_index": "test" , _type: "type1", "_id": "5", 
          "_source": { "include": [ "title", "content" ] , "exclude" : [ "content.full_text" ]}  
        }
    ]
}'
```
## Get_source API

The `get/_source` API is an API that is already dedicated for `_source` retrieval. As such, it has a slightly different parameter naming:

```
curl -XGET 'http://localhost:9200/index/type/1/_source?include=title,content&amp;exclude=content.full_text'
```
## Explain API

The `explain` API also offers the `fields` parameter. It is now extend with query string parameters, just like the `get` API:

```
curl -XPOST 'http://localhost:9200/index/type/1/_explain?_source=false' -d'{
    "query" : { "term" : { "message" : "search" } }
}'
```

or

```
curl -XPOST 'http://localhost:9200/index/type/1/_explain?_source=title,author' -d'{
    "query" : { "term" : { "message" : "search" } }
}'
```

and

```
curl -XPOST 'http://localhost:9200/index/type/1/_explain?_source_include=title,content&amp;_source_exclude=content.full_text' -d'{
    "query" : { "term" : { "message" : "search" } }
}'
```
# Search API

The `search` API was added an extra `_source` key in the body, with the same options as all the above:

```
curl -XPOST 'http://localhost:9200/_search' -d'{
    "query" : { "term" : { "message" : "search" } },
    "_source" : false
}'
```

and

```
curl -XPOST 'http://localhost:9200/_search' -d'{
    "query" : { "term" : { "message" : "search" } },
    "_source" : "title"
}'
```

```
curl -XPOST 'http://localhost:9200/_search' -d'{
    "query" : { "term" : { "message" : "search" } },
    "_source" : [ "title" , "author" ]
}'
```

```
curl -XPOST 'http://localhost:9200/_search' -d'{
    "query" : { "term" : { "message" : "search" } },
    "_source": { "include": [ "title", "content" ] , "exclude" : [ "content.full_text" ]} 
}'
```

Also the `search` API supports accepting `_source` retrieval settings as query string parameters. The format is identical to the `get` API: `_source`, `_source_include` &amp; `_source_exclude`. In the case where the parameters are supplied both in the request body and the query string, the query string parameter override the body.
</description><key id="16513458">3301</key><summary>Add finer control over `_source` retrieval, in `get`, `mget`, `get_source`, `explain` &amp; `search` API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-07-09T08:35:18Z</created><updated>2014-07-08T12:30:56Z</updated><resolved>2013-07-30T11:22:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aggregation Module - Phase 1 - Functional Design</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3300</link><project id="" key="" /><description>**_NOTE: at this point we're focusing more on the functional design aspect rather than performance. Once we get this nailed down, we'll see how far we can push and optimize.**_
### Background

The new aggregations module is due to elasticsearch 1.0 release, and aims to serve as the next generation replacement for the functionality we currently refer to as "faceting". Facets, currently provide a great way to aggregate data within a document set context. This context is defined by the executed query in combination with the different levels of filters that are defined (filtered queries, top level filters, and facet level filters). Although powerful as is, the current facets implementation was not designed from ground up to support complex aggregations and thus limited. The main problem with the current implementation stem in the fact that they are hard coded to work on one level and that the different types of facets (which account for the different types of aggregations we support) cannot be mixed and matched dynamically at query time. It is not possible to compose facets out of other facet and the user is effectively bound to the top level aggregations that we defined and nothing more than that.

The goal with the new aggregations module is to break the barriers the current facet implementation put in place. The new name ("Aggregations") also indicate the intention here - a generic yet extremely powerful framework for defining aggregations - any type of aggregation. The idea here is to have each aggregation defined as a "standalone" aggregation that can perform its task within any context (as a top level aggregation or embedded within other aggregations that can potentially narrow its computation scope). We would like to take all the knowledge and experience we've gained over the years working with facets and apply it when building the new framework.

Before we dive into the meaty part, it's important to set some key concepts and terminology first.
### Key Concepts &amp; Terminology
- **Aggregation** - An aggregation is the result of an aggregation :). There are many types of aggregations, some look similar , others have their own unique structure (all depending on the nature of the aggregation). For example, a `terms` aggregation holds a list of objects (buckets), each holding information about a unique term. While an `avg` aggregation, just holds the avg number aggregated over all values of a specific field/s within a well defined set of documents.
- **Aggregator** - An aggregator is the computation unit in elasticsearch which generates aggregations. It is effectively responsible for aggregating the data during query phase, and at the end of this phase, create the output aggregation. Each aggregation type has a dedicated aggregator which knows how to compute and generate it.

There are two types of aggregators/aggregations:
- **Bucket** - A family of aggregators whos main responsibility is to define the current document set context and split it into buckets, where each bucket defines a well defined document set context. Typically, all aggregators of this type will also return the document count in each bucket. This aggregator is composable, meaning, one can define other aggregations under it. It will then perform these defined aggregations for each of the buckets it builds. It is therefore possible to create buckets within buckets within buckets... up to any level of hierarchy one desires. For example, one can define a filter bucket that holds all the "active" users (for example, if the documents represent website users/visitors), under which she'll define a range bucket that build 3 buckets to represent different user age groups, under each age group she'll define a terms bucket to narrow down the most common tags each age group is using on the website. As you can see, creating hierarchies of buckets can be extremely powerful can immensely help when sliding &amp; dicing your data.
- **Calc** - A family of aggregators whos sole responsibility is to perform computation and calculate numbers. It always operates in a well defined scope of a document set. This document set scope is either the top most level one - the scope defined by the search query, or otherwise defined by a higher level bucket aggregator (as discussed above). The Calc Aggregators typically work on field values, therefore utilizing the field data from which they extract these values. But one can utilise scripts to compute custom values which will be aggregated in different ways (depending on the specific calc aggregator that is used). If combining (mixing &amp; matching) all different types of aggregators, while bucket aggregators can be placed anywhere in the aggregation definition "tree", calc aggregators are always "leaves" on the tree as (unlike bucket aggregators) they cannot contain other aggregators.
#### Structuring Aggregations

The following snippet captures the basic structure of aggregations:

``` json
"aggregations" : {
    "&lt;aggregation_name&gt;" : {
        "&lt;aggregation_type&gt;" : { 
            &lt;aggregation_body&gt;
        },
        ["aggregations" : { [&lt;sub_aggregation&gt;]* } ]
    }
    [,"&lt;aggregation_name_2&gt;" : { ... } ]*

}
```

The `aggregations` object (can also be `aggs` for short) in the json holds the aggregations you'd like to be computed. Each aggregation is associated with a logical name that the user defines (e.g. if the aggregation computes the average price, then it'll make sense to call it `avg_price`). These logical names, also uniquely identify the aggregations you define (you'll use the same names/keys to identify the aggregations in the response). Each aggregation has a specific type (`&lt;aggregation_type&gt;` in the above snippet) and is typically the first key within the named aggregation body. Each type of aggregation define its own body, depending on the nature of the aggregation (eg. the `avg` aggregation will define the field on which the avg will be calculated). At the same level of the aggregation type definition, one can optionally define a set of additional aggregations, but this only makes sense if the aggregation you defined is a bucketing aggregation. In this scenario, the aggregation you define on the bucketing aggregation level will be computed for all the buckets built by the bucketing aggregation. For example, if the you define a set of aggregations under the `range` aggregation, these aggregations will be computed for each of the range buckets that are defined.

In this manner, you can mix &amp; match bucketing and calculating aggregations any way you'd like, create any set of complex hierarchies by embedding aggregations (of type bucket or calc) within other bucket aggregations. To better grasp how they can all work together, please refer to the examples section below.
### Calc Aggregators

In this section will provide an overview of all calc aggregations available to date. 

All the calc aggregators we have today belong to the same family which we like to call `stats`. All the aggregator in this family are based on values that can either come from the field data or from a script that the user defines. 

These aggregators operate on the following context: { _D_, _FV_ } where _D_ is the the set of documents from which the field values are extracted, and _FV_ is the set of values that should be aggregated. The aggregations take all those field values and calculates statistical values (some only calculate on value - they're called `single value stats aggregators`, while others generate a set of values - these are called `multi-value stats aggregators`).

Here are all currently available stats aggregators
#### Avg

Single Value Aggregator - Will return the average over all field values in the aggregation context, or what ever values the script generates

``` json
"aggs" : {
    "avg_price" : { "avg" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "avg_price" : { "avg" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "avg_price" : { "avg" : { "field" : "price", "script" : "_value" } }
}
```

_NOTE: when `field` and `script` are both specified, the script will be called for every value of the field in the context, and within the script you can access this value using the reserved variable `_value`.

Output:

``` json
"avg_price" : {
    "value" : 10
}
```
#### Min

Single Value Aggregator - Will return the minimum value among all field values in the aggregation context, or what ever values the script generates

``` json
"aggs" : {
    "min_price" : { "min" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "min_price" : { "min" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "min_price" : { "min" : { "field" : "price", "script" : "_value" } }
}
```

Output:

``` json
"min_price" : {
    "value" : 1
}
```
#### Max

Single Value Aggregator - Will return the maximum value among all field values in the aggregation context, or what ever values the script generates

``` json
"aggs" : {
    "max_price" : { "max" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "max_price" : { "max" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "max_price" : { "max" : { "field" : "price", "script" : "_value" } }
}
```

Output:

``` json
"max_price" : {
    "value" : 100
}
```
#### Sum

Single Value Aggregator - Will return the sum of all field values in the aggregation context, or what ever values the script generates

``` json
"aggs" : {
    "sum_price" : { "sum" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "sum_price" : { "sum" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "sum_price" : { "sum" : { "field" : "price", "script" : "_value" } }
}
```

Output:

``` json
"sum_price" : {
    "value" : 350
}
```
#### Count

Single Value Aggregator - Will return the number of field values in the aggregation context, or what ever values the script generates

``` json
"aggs" : {
    "prices_count" : { "count" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "prices_count" : { "count" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "prices_count" : { "count" : { "field" : "price", "script" : "_value" } }
}
```

Output:

``` json
"prices_count" : {
    "value" : 400
}
```
#### Stats

Multi Value Aggregator - Will return the following stats aggregated over the field values in the aggregation context, or what ever values the script generates:
- avg
- min
- max
- count
- sum

``` json
"aggs" : {
    "price_stats" : { "stats" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "prices_stats" : { "stats" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "prices_stats" : { "stats" : { "field" : "price", "script" : "_value" } }
}
```

Output:

``` json
"prices_stats" : {
    "min" : 1,
    "max" : 10,
    "avg" : 5.5,
    "sum" : 55,
    "count" : 10,
}
```
#### Extended Stats

Multi Value Aggregator - an extended version of the Stats aggregation above, where in addition to its aggregated statistics the following will also be aggregated:
- sum_of_squares
- variance
- std_deviation

``` json
"aggs" : {
    "price_stats" : { "extended_stats" : { "field" : "price" } }
}
```

``` json
"aggs" : {
    "prices_stats" : { "extended_stats" : { "script" : "doc['price']" } }
}
```

``` json
"aggs" : {
    "prices_stats" : { "extended_stats" : { "field" : "price", "script" : "_value" } }
}
```

Output:

``` json
"value_stats": {
    "count": 10,
    "min": 1.0,
    "max": 10.0,
    "avg": 5.5,
    "sum": 55.0,
    "sum_of_squares": 385.0,
    "variance": 8.25,
    "std_deviation": 2.8722813232690143
}
```
### Bucket Aggregators

Bucket aggregators don't calculate values over fields like the `calc` aggregators do, but instead, they create buckets of documents. Each bucket defines a criteria (depends on the aggregation type) that determines whether or not a document in the current context "falls" in it. In other words, the buckets effectively define document sets (a.k.a docsets) on which the sub-aggregations are running on.

There a different bucket aggregators, each with a different "bucketing" strategy. Some define a single bucket, some define fixed number of multiple bucket, and others dynamically create the buckets while evaluating the docs.

The following describe the currently supported bucket aggregators.
#### Global

Defines a single bucket of all the documents within the search execution context. This context is defined by the indices and the document types you're searching on, but is **not** influenced by the search query itself.

_Note, global aggregators can only be placed as top level aggregators (it makes no sense to embed a global aggregator within another bucket aggregator)_

``` json
"aggs" : {
    "global_stats" : {
        "global" : {}, // global has an empty body
        "aggs" : {
            "avg_price" : { "avg" : { "field" : "price" } }
        }
    }
}
```

Output

``` json
"aggs" : {
    "global_stats" : {
        "doc_count" : 100,
        "avg_price" : { "value" : 56.3 }
    }
}
```
#### Filter

Defines a single bucket of all the documents in the current docset context which match a specified filter. Often this will be used to narrow down the current aggregation context to a specific set of documents.

``` json
"aggs" : {
    "active_items" : {
        "filter" : { "term" : { "active" : true } },
        "aggs" : {
            "avg_price" : { "avg" : { "field" : "price" } }
        }
    }
}
```

Output

``` json
"aggs" : {
    "active_items" : {
        "doc_count" : 100,
        "avg_price" : { "value" : 56.3 }
    }
}
```
#### Missing

A field data based single bucket aggregator, that creates a bucket of all documents in the current docset context that are missing a field value. This aggregator will often be used in conjunction with other field data bucket aggregators (such as ranges) to return information for all the documents that could not be placed in any of the other buckets due to missing field data values. (The examples bellow show how well the range and the missing aggregators play together).

``` json
"aggs" : {
    "missing_price" : {
        "missing" : { "field" : "price" }
    }
}
```

Output

``` json
"aggs" : {
    "missing_price" : {
        "doc_count" : 10
    }
}
```
#### Terms

A field data based multi-bucket aggregator where buckets are dynamically built - one per unique value (term) of a specific field. For each such bucket the document count will be aggregated (accounting for all the documents in the current docset context that have that term for the specified field). This aggregator is very similar to how the terms facet works except that it is an aggregator just like any other aggregator, meaning it can be embedded in other bucket aggregators and it can also hold any types of sub-aggregators itself.

``` json
"aggs" : {
    "genders" : {
        "terms" : { "field" : "gender" },
        "aggs" : {
            "avg_height" : { "avg" : { "field" : "height" } }
        }
    }
}
```

Output

``` json
"aggs" : {
    "genders" : {
        "terms" : [
            {
                "term" : "male",
                "doc_count" : 10,
                "avg_height" : 178.5
            },
            {
                "term" : "female",
                "doc_count" : 10,
                "avg_height" : 165
            },
        ]
    }
}
```

**TODO: do we want to get rid of the "terms" level in the response and directly put the terms array under the aggregation name? (we do that in range aggregation)**
##### Options

| Name | Default | Required | Description |
| :-- | :-: | :-: | :-- |
| field | - | yes/no | the name of the field from which the terms will be taken. It is required if there is no other field data based aggregator in the current aggregation context and the **script** option is also not set |
| size | 10 | no | Only the top _n_ terms will be returned, the size determines what this _n_ is |
| order | count desc | no | the order in which the term bucket will be sorted, see bellow for possible values |
| script | - | no | one can choose to let a script generate the terms instead of extracting them verbatim from the field data. If the script is define along with the field, then this script will be executed for every term/value of the field data with a special variable **_value** which will provide access to that value from within the script (this is as opposed to specifying only the script, without the field, in which case the script will execute once per document in the aggregation context) |
##### About order

One can define the order in which the term buckets will be sorted and therefore return in the response. There are 4 fixed/pre-defined order types and one more dynamic:

Order by term (alphabetically) ascending/descending:

``` json
"aggs" : {
    "genders" : {
        "terms" : { "field" : "gender", "order": { "_term" : "desc" } }
    }
}
```

Order by count (alphabetically) ascending/descending:

``` json
"aggs" : {
    "genders" : {
        "terms" : { "field" : "gender", "order": { "_count" : "asc" } }
    }
}
```

Order by direct embedded calc aggregation, ascending/descending. For single value calc aggregation:

``` json
"aggs" : {
    "genders" : {
        "terms" : { "field" : "gender", "order": { "avg_price" : "asc" } },
        "aggs" : {
            "avg_price" : { "avg" : { "field" : "price" } }
        }
    }
}
```

Or, for multi-value calc aggregation:

``` json
"aggs" : {
    "genders" : {
        "terms" : { "field" : "gender", "order": { "price_stats.avg" : "desc" } },
        "aggs" : {
            "price_stats" : { "stats" : { "field" : "price" } }
        }
    }
}
```
#### Range

A field data bucket aggregation that enables the user to define a field on which the bucketing will work and a set of ranges. The aggregator will check each field data value in the current docset context against each bucket range and "bucket" the relevant document &amp; values if they match. Note, that here, not only we're bucketing by document, we're also bucketing by value. For example, let's say we're bucketing on multi-value field, and document _D_ has values [1, 2, 3, 4, 5] for the field. In addition, there is a range bucket [ x &lt; 4 ]. When evaluating document _D_, it seems to fall right in this range bucket, but it does so due to field values [1, 2, 3], not because values [4, 5]. Now&#8230; if this bucket will also have a sub-aggregators associated with it (say, sum aggregator), the system will make sure to only aggregate values [1, 2, 3] excluding [4, 5](as 4 and 5 as values, don't really belong to this bucket). This is quite different than the other bucket aggregators we've seen until now which mainly focused on whether the document falls in the bucket or not. Here we also keep track of the values belonging to each bucket.

``` json
"aggs" : {
    "age_groups" : {
        "range" : { 
            "field" : "age",
            "ranges" : [
                { "to" : 5 },
                { "from" : 5, "to" : 10 },
                { "from" : 10, "to" : 15 },
                { "from" : 15}
            ]
        },
        "aggs" : {
            "avg_height" : { "avg" : { "field" : "height" } }
        }
    }
}

```

Output

``` json
"aggregations" : {
    "age_groups" : [
        {
            "to" : 5.0,
            "doc_count" : 10,
            "avg_height" : 95
        },
        {
            "from" : 5.0,
            "to" : 10.0,
            "doc_count" : 5,
            "avg_height" : 130
        },
        {
            "from" : 10.0
            "to" : 15.0,
            "doc_count" : 4,
            "avg_height" : 160
        },
        {
            "from" : 15.0,
            "doc_count" : 10,
            "avg_height" : 175.5
        }
    ]
}
```

Of course, you normally don't want to store the **age** as a field, but store the birthdate instead. We can use scripts to generate the age:

``` json
"aggs" : {
    "age_groups" : {
        "range" : { 
            "script" : "DateTime.now().year - doc['birthdate'].date.year",
            "ranges" : [
                { "to" : 5 },
                { "from" : 5, "to" : 10 },
                { "from" : 10, "to" : 15 },
                { "from" : 15}
            ]
        },
        "aggs" : {
            "avg_height" : { "avg" : { "field" : "height" } }
        }
    }
}

```

As with all other aggregations, leaving out the **field** from calc aggregator, will fall back on the field by which the range bucketing is done.

``` json
"aggs" : {
    "age_groups" : {
        "range" : { 
            "field" : "age",
            "ranges" : [
                { "to" : 5 },
                { "from" : 5, "to" : 10 },
                { "from" : 10, "to" : 15 },
                { "from" : 15}
            ]
        },
        "aggs" : {
            "min" : { "min" : { } },
            "max" : { "max" : { } }
        }
    }
}
```

Output

``` json
"aggregations" : {
    "age_groups" : [
        {
            "to" : 5.0,
            "doc_count" : 10,
            "min" : 4.0,
            "max" : 5.0
        },
        {
            "from" : 5.0,
            "to" : 10.0,
            "doc_count" : 5,
            "min" : 5.0,
            "max" : 8.0
        },
        {
            "from" : 10.0
            "to" : 15.0,
            "doc_count" : 4,
            "min" : 11.0,
            "max" : 13.0
        },
        {
            "from" : 15.0,
            "doc_count" : 10,
            "min" : 15.0,
            "max" : 22.0
        }
    ]
}
```

Furthermore, you can also define a value script which will serve as a transformation to the field data value:

``` json
"aggs" : {
    "age_groups" : {
        "range" : { 
            "field" : "count",
            "script" : "_value - 3"
            "ranges" : [
                { "to" : 6 },
                { "from" : 6 }
            ]
        },
        "aggs" : {
            "min" : { "min" : {} },
            "min_count" : { "min" : { "field" : "count" } }
        }
    }
}
```

Output

``` json
"aggregations": {
    "count_ranges": [
      {
        "to": 6.0,
        "doc_count": 8,
        "min": {
          "value": -2.0
        },
        "min_count": {
          "value": 1.0
        }
      },
      {
        "from": 6.0,
        "doc_count": 2,
        "min": {
          "value": 6.0
        },
        "min_count": {
          "value": 9.0
        }
      }
    ]
  }
```

Notice, the **min** aggregation above acts on the actual values that were used for the bucketing (after the transformation by the script), while the **min_count** aggregation act on the values of the count field that fall within their bucket.
#### Date Range

A range aggregation that is dedicated for date values. The main difference between this date range agg. to the normal range agg. is that the `from` and `to` values can be expressed in _Date Math_ expressions, and it is also possible to specify a date format by which the `from` and `to` json fields will be returned in the response:

``` json
"aggs": {
    "range": {
        "date_range": {
            "field": "date",
            "format": "MM-yyy",
            "ranges": [
                {
                    "to": "now-10M/M"
                },
                {
                    "from": "now-10M/M"
                }
            ]
        }
    }
}
```

In the example above, we created two range buckets:
- the first will bucket all documents dated prior to 10 months ago
- the second will bucket all document dated since 10 months ago

``` json
"aggregations": {
    "range": [
        {
            "to": 1.3437792E+12,
            "to_as_string": "08-2012",
            "doc_count": 7
        },
        {
            "from": 1.3437792E+12,
            "from_as_string": "08-2012",
            "doc_count": 2
        }
    ]
}
```
#### IP Range

Just like the dedicated date range aggregation, there is also a dedicated range aggregation for IPv4 typed fields:

``` json
"aggs" : {
    "ip_ranges" : {
        "ip_range" : {
            "field" : "ip",
            "ranges" : [
                { "to" : "10.0.0.5" },
                { "from" : "10.0.0.5" }
            ]
        }
    }
}
```

Output:

``` json
"aggregations": {
    "ip_ranges": [
        {
            "to": 167772165,
            "to_as_string": "10.0.0.5",
            "doc_count": 4
        },
        {
            "from": 167772165,
            "from_as_string": "10.0.0.5",
            "doc_count": 6
        }
    ]
}
```

IP ranges can also be defined as CIDR masks:

``` json
"aggs" : {
    "ip_ranges" : {
        "ip_range" : {
            "field" : "ip",
            "ranges" : [
                { "mask" : "10.0.0.0/25" },
                { "mask" : "10.0.0.127/25" }
            ]
        }
    }
}
```

Output:

``` json
"aggregations": {
    "ip_ranges": [
      {
        "key": "10.0.0.0/25",
        "from": 1.6777216E+8,
        "from_as_string": "10.0.0.0",
        "to": 167772287,
        "to_as_string": "10.0.0.127",
        "doc_count": 127
      },
      {
        "key": "10.0.0.127/25",
        "from": 1.6777216E+8,
        "from_as_string": "10.0.0.0",
        "to": 167772287,
        "to_as_string": "10.0.0.127",
        "doc_count": 127
      }
    ]
}
```
#### Histogram

An aggregation that can be applied to numeric fields, and dynamically builds fixed size (a.k.a. interval) buckets over all the values of the document fields in the docset context. For example, if the documents have a field that holds a price (numeric), we can ask this aggregator to dynamically build buckets with interval 5 (in case of `price` it may represent $5). When the aggregation executes, the price field of every document within the aggregation context will be evaluated and will be **rounded** down to its closes bucket - for example, if the price is `32` and the bucket size is `5` then the rounding will yield `30` and thus the document will "fall" into the bucket the bucket that is associated withe the key `30`. To make this more formal, here is the rounding function that is used:

`bucket_key = value - value % interval`

A basic histogram aggergation on a single numeric field `value` (maybe be single or multi valued field)

``` json
"aggs" : {
    "value_histo" : {
            "histogram" : {
                    "field" : "value",
                    "interval" : 3
            }
    }
}
```

An histogram aggregation on multiple fields

``` json
"aggs" : {
    "value_histo" : {
            "histogram" : {
                    "field" : [ "value", "values" ],
                    "interval" : 3
            }
    }
}
```

The output of the histogram is an array of the buckets, where each bucket holds its key and the number of documents that fall in it. This array can be sorted based on different attributes in an ascending or descending order:
- `_key` - The buckets will be sorted by their key
- `_count` - The buckets will be sorted by the number of documents that fall in them
- `aggName` - Bucket may hold other aggegations that will be applied to those documents that fall in them. It is possible to sort the buckets based on direct single-valued **calc** aggregations that they hold
- `aggName` &amp; `valueName` - It is also possible to sort buckets based on direct multi-valued **calc** aggregations that they hold

Sorting by bucket `key` descending

``` json
"aggs" : {
    "histo" : {
        "histogram" : {
            "field" : "value",
            "interval" : 3,
            "order" : { "_key" : "desc" }
        }
    }
}
```

Sorting by document count ascending

``` json
"aggs" : {
    "histo" : {
        "histogram" : {
            "field" : "value",
            "interval" : 3,
            "order" : { "_count" : "asc" }
        }
    }
}
```

Adding a sum aggregation (which is a single valued calc aggregation) to the buckets and sorting by it

``` json
"aggs" : {
    "histo" : {
        "histogram" : {
            "field" : "value",
            "interval" : 3,
            "order" : { "value_sum" : "asc" }
        },
        "aggs" : {
            "value_sum" : { "sum" : {} }
        }
    }
}
```

Adding a stats aggregation (which is a multi-valued calc aggregation) to the buckets and sorting by the avg

``` json
"aggs" : {
    "histo" : {
        "histogram" : {
            "field" : "value",
            "interval" : 3,
            "order" : { "value_stats.avg" : "desc" }
        },
        "aggs" : {
            "value_stats" : { "stats" : {} }
        }
    }
}
```

Using value scripts to "preprocess" the values before the bucketing

``` json
"aggs" : {
    "histo" : {
        "histogram" : {
            "field" : "value",
            "script" : "_value * 4",
            "interval" : 3,
            "order" : { "sum" : "desc"}
        },
        "aggregations" : {
            "sum" : { "sum" : {} }
        }
    }
}
```

It's also possible to use document level scripts to compute the value by which the documents will be "bucketted"

``` json
"aggs" : {
    "histo" : {
        "histogram" : {
            "script" : "doc['value'].value + doc['value2'].value",
            "interval" : 3,
            "order" : { "stats.sum" : "desc" }
        },
        "aggregations" : {
            "stats" : { "stats" : {} }
        }
    }
}
```

Output:

``` json
"aggregations": {
  "histo": [
    {
      "key": 21,
      "doc_count": 2,
      "stats": {
        "count": 2,
        "min": 8.0,
        "max": 9.0,
        "avg": 8.5,
        "sum": 17.0
      }
    },
    {
      "key": 15,
      "doc_count": 2,
      "stats": {
        "count": 2,
        "min": 5.0,
        "max": 6.0,
        "avg": 5.5,
        "sum": 11.0
      }
    },
    {
      "key": 24,
      "doc_count": 1,
      "stats": {
        "count": 1,
        "min": 10.0,
        "max": 10.0,
        "avg": 10.0,
        "sum": 10.0
      }
    },
    {
      "key": 18,
      "doc_count": 1,
      "stats": {
        "count": 1,
        "min": 7.0,
        "max": 7.0,
        "avg": 7.0,
        "sum": 7.0
      }
    },
    {
      "key": 9,
      "doc_count": 2,
      "stats": {
        "count": 2,
        "min": 2.0,
        "max": 3.0,
        "avg": 2.5,
        "sum": 5.0
      }
    },
    {
      "key": 12,
      "doc_count": 1,
      "stats": {
        "count": 1,
        "min": 4.0,
        "max": 4.0,
        "avg": 4.0,
        "sum": 4.0
      }
    },
    {
      "key": 6,
      "doc_count": 1,
      "stats": {
        "count": 1,
        "min": 1.0,
        "max": 1.0,
        "avg": 1.0,
        "sum": 1.0
      }
    }
  ]
}
```
#### Date Histogram

Date histogram is a similar aggregation to the normal histogram (as described above) except that it can only work on date fields. Since dates are indexed internally as long values, it's possible to use the normal histogram on dates as well, but problem though stems in the fact that time based intervals are not fixed (think of leap years and on the number of days in a month). For this reason, we need a spcial support for time based data. From functionality perspective, this historam supports the same features as the normal histogram. The main difference though is that the interval can be specified by time expressions.

Building a month length bucket intervals

``` json
"aggs" : {
    "histo" : {
        "date_histogram" : {
            "field" : "date",
            "interval" : "month"
        }
    }
}
```

or based on 1.5 months

``` json
"aggs" : {
    "histo" : {
        "date_histogram" : {
            "field" : "date",
            "interval" : "1.5M"
        }
    }
}
```

Other available expressions for interval: `year`, `quarter`, `week`, `day`, `hour`, `minute`, `second`

Since internally, dates are represented as 64bit numbers, these numbers are returned as the bucket keys (each key representing a date). For this reason, it is also possible to define a date format, which will result in returning the dates as formatted strings next to the numeric key values:

``` json
"aggs" : {
    "histo" : {
        "date_histogram" : {
            "field" : "date",
            "interval" : "1M",
            "format" : "yyyy-MM-dd"
        }
    }
}
```

Output:

``` json
"aggregations": {
    "histo": [
        {
          "key_as_string": "2012-02-02",
          "key": 1328140800000,
          "doc_count": 1
        },
        {
          "key_as_string": "2012-03-02",
          "key": 1330646400000,
          "doc_count": 2
        },
        ...
    ]
}
```

Timezones are also supported, enabling the user to define by which timezone they'd like to bucket the documents (this support is very similar to the TZ support in the DateHistogram facet).

Similar to the current date histogram facet, pref_offset &amp; post_offset will are also supported, for offsets applying pre rounding and post rounding. The values are time values with a possible `-` sign. For example, to offset a week rounding to start on Sunday instead of Monday, one can pass pre_offset of -1d to decrease a day before doing the week (monday based) rounding, and then have post_offset set to -1d to actually set the return value to be Sunday, and not Monday.

Like with the normal histogram, both document level scripts and value scripts are supported. It is possilbe to control the order of the buckets that are returned. And of course, nest other aggregations within the buckets.

Both the normal `histogram` and the `date_histogram` now support computing/returning empty buckets. This can be controlled by setting the `compute_empty_buckets` parameter to `true` (defaults to `false`). 
#### Geo Distance

An aggregation that works on `geo_point` fields. Conceptually, it works very similar to range aggregation. The user can define a point of `origin` and a set of distance range buckets. The aggregation evaluate the distance of each document from the `origin` point and determine the bucket it belongs to based on the ranges (a document belongs to a bucket if the distance between the document and the `origin` falls within the distance range of the bucket).

``` json
"aggs" : {
    "rings" : {
        "geo_distance" : {
            "field" : "location",
            "origin" : "52.3760, 4.894",
            "ranges" : [
                { "to" : 100 },
                { "from" : 100, "to" : 300 },
                { "from" : 300 }
            ]
        }
    }
}
```

Output

``` json
"aggregations": {
  "rings": [
    {
      "unit": "km",
      "to": 100.0,
      "doc_count": 3
    },
    {
      "unit": "km",
      "from": 100.0,
      "to": 300.0,
      "doc_count": 1
    },
    {
      "unit": "km",
      "from": 300.0,
      "doc_count": 7
    }
  ]
}
```

The specified `field` must be of type `geo_point` (which can only be set explicitly in the mappings). And it can also hold an array of `geo_point` fields, in which case all will be taken into account during aggregation. The `origin` point can accept all format `geo_point` supports:
- Object format: `{ "lat" : 52.3760, "lon" : 4.894 }` - this is the safest format as it's the most explicit about the `lat` &amp; `lon` values
- String format: `"52.3760, 4.894"` - where the first number is the `lat` and the second is the `lon`
- Array format: `[4.894, 52.3760]` - which is based on the GeoJson standard and where the first number is the `lon` and the second one is the `lat`

By default, the distance unit is `km` but it can also accept: `mi` (miles), `in` (inch), `yd` (yards), `m` (meters), `cm` (centimeters), `mm` (millimeters).

``` json
"aggs" : {
    "rings" : {
        "geo_distance" : {
            "field" : "location",
            "origin" : "52.3760, 4.894",
            "unit" : "mi",
            "ranges" : [
                { "to" : 100 },
                { "from" : 100, "to" : 300 },
                { "from" : 300 }
            ]
        }
    }
}
```

There are two distance calculation modes: `arc` (the default) and `plane`. The `arc` calculation is the most accurate one but also the more expensive one in terms of performance. The `plane` is faster but less accurate. Consider using `plane` when your search context is narrow smaller areas (like cities or even countries). `plane` may return higher error mergins for searches across very large areans (e.g. cross atlantic search).

``` json
"aggs" : {
    "rings" : {
        "geo_distance" : {
            "field" : "location",
            "origin" : "52.3760, 4.894",
            "distance_type" : "plane",
            "ranges" : [
                { "to" : 100 },
                { "from" : 100, "to" : 300 },
                { "from" : 300 }
            ]
        }
    }
}
```
#### Nested

A special single bucket aggregation which enables aggregating nested documents:

assuming the following mapping:

``` json
"type" : {
        "properties" : {
            "nested" : { "type" : "nested" }
        }
    }
}
```

Here's how a nested aggregation can be defined:

``` json
"aggs" : {
    "nested_value_stats" : {
        "nested" : {
            "path" : "nested"
        },
        "aggs" : {
            "stats" : {
                "stats" : { "field" : "nested.value" }
            }
        }
    }
}
```

As you can see above, the nested aggregation requires the path of the nested documents within the top level documents. Then one can define any type of aggregation over these nested documents.

Output:

``` json
"aggregations": {
    "employees_salaries": {
        "doc_count": 25,
        "stats": {
            "count": 25,
            "min": 1.0,
            "max": 9.0,
            "avg": 5.0,
            "sum": 125.0
        }
    }
}
```
### Examples
#### Filter + Range + Missing + Stats

Analyse the online product catalog web access logs. The following aggregation will only aggregate those logs from yesterday (the **filter** aggregation), providing information for different price ranges (the **range** aggregation), where per price range we'll return the price stats on that range and the total page views for those documents in the each range. We're also interested in finding all the bloopers - all those products that for some reason don't have prices associated with them and still they are exposed to the user and being accessed and viewed.

``` json
"aggs" : {
    "yesterday" : {
        "filter" : { "range" : { "date" { "gt" : "now-1d/d", "lt" : "now/d" } } },
        "aggs" : {
            "missing_price" : {
                "missing" : { "field" : "price" },
                "aggs" : {
                    "total_page_views" : { "sum" : { "field" : "page_views" } }
                }
            },
            "prices" : {
                "range" : {
                    "field" : "price",
                    "ranges" : [
                        { "to" : 100 },
                        { "from" : 100, "to" : 200 },
                        { "from" : 200, "to" 300 },
                        { "from" : 300 }
                    ]
                },
                "aggs" : {
                    "price_stats" : { "stats" : {} },
                    "total_page_views" : { "sum" : { "field" : "page_views" } }
                }
            }
        }
    }
}
```
#### Aggregating Hierarchical Data

Quite often you'd like to get aggregations on location in an hierarchical manner. For example, show all countries and how many documents fall within each country, and for each country show a breakdown by city. Here's a simple way to do it using hierarchical terms aggregations:

``` json
"aggs" : {
    "country" : {
        "terms" : { "field" : "country" },
        "aggs" : {
            "city" : {
                "terms" : { "field" : "city" }
            }
        }
    }
}
```
</description><key id="16500044">3300</key><summary>Aggregation Module - Phase 1 - Functional Design</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">uboness</reporter><labels><label>feature</label><label>v1.0.0.Beta2</label></labels><created>2013-07-08T23:27:14Z</created><updated>2016-04-20T01:29:33Z</updated><resolved>2013-11-24T11:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-07-09T00:26:01Z" id="20645972">Definitely watching this thread. Too much to digest right now, but good work. I assumed that the long rumored facet refactoring would be at the implementation level and not at this higher level.

First question that pops into mind is what is the target release for this issue? The issue is tagged v1.0.0.Beta1, so will 1.0 be held off until this issue is release? So far, there are no new major issues tagged 1.0 with the exception of this one. Once again, very exciting work.
</comment><comment author="mattweber" created="2013-07-09T00:35:30Z" id="20646283">Great work, these are so powerful!
</comment><comment author="uboness" created="2013-07-09T01:27:29Z" id="20648022">@brusic 

&gt; I assumed that the long rumored facet refactoring would be at the implementation level and not at this higher level.

So there are two reasons why we took this path: 
1. we don't want to break/modify/change anything in the current facets while we're working on the aggregations 
2. It is a big change, not only from implementation perspective, but also from a functional perspective - the way one approaches data aggregations in elasticsearch changes quite a bit, to the extent we believe it deserves its own module and set of apis.

&gt; First question that pops into mind is what is the target release for this issue? The issue is tagged v1.0.0.Beta1, so will 1.0 be held off until this issue is release? So far, there are no new major issues tagged 1.0 with the exception of this one. Once again, very exciting work.

the target release is indeed 1.0, and rest assured that more work will join this one :). As for 1.0 release timelines... we always planned to have this functionality in 1.0 and we always took it into account, so it doesn't really put a delay on things (just part of the work that needs to be done)
</comment><comment author="itsadok" created="2013-07-09T05:07:23Z" id="20653717">You didn't mention it in Date Histogram, but are you going to add pre/post
offset, like in #1599? Or should something like this be done manually, with
a script?
</comment><comment author="jprante" created="2013-07-09T07:45:00Z" id="20658325">Many thanks for the summary! I assume this is the facet refactoring we all wait for. 

I would love to see term labels, collations for sorting, and pattern based formatting for the values in the terms aggregation. ("asc" and "desc" for an "order" is definitely not sufficient)
</comment><comment author="uboness" created="2013-07-09T08:03:29Z" id="20659045">@itsadok definitely! added to the above description
</comment><comment author="pecke01" created="2013-07-09T08:25:19Z" id="20659905">@uboness Great write up. Thanks for sharing this with the community.

One thing that I have found useful in other aggregation engines I used is what they have called `buckets`. Instead of defining ranges like, 1 to 5, 6 to 10.. etc. It is possible to get an even spread by defining the number of buckets. 
Example requesting 3 buckets for price I would be able to get something like this:
 1 - 12$ (10)
13 - 51$ (10)
51 - 120 (10)

Filtering will then adjust the buckets but will still be an even spread in 3 of them. This is ofc possible by doing several queries today but if it would be possible to get in there `auto-magically` I would like it. This might be possible from what you described above but I might have just not understood it.
</comment><comment author="uboness" created="2013-07-09T08:32:14Z" id="20660208">@jprante this is just the initial phase, where we take all existing functionality of the facets and supporting it. In a later stage we definitely plan to enhance it further

&gt; I would love to see term labels

Can you elaborate on that?

&gt; pattern based formatting for the values

We have this currently supported for date values only... won't be hard to add it to numeric as well.

&gt; "asc" and "desc" for an "order" is definitely not sufficient

Why?
</comment><comment author="markharwood" created="2013-07-09T10:28:56Z" id="20665468">I think I may have another category of faceting/summarisation to consider. 

I'm interested in representing the strength of relationships between different buckets - i.e. the results are represented as a weighted graph not neatly contained hierarchies. Nodes are categories, edges represent varying degrees of association between categories.

One example might be showing how various IT skills in job ads are related (web vs database vs unix vs java vs search etc).

In my example implementation I instrumented various branches of my query with a number of "tagging wrapper" queries whose only purpose is to mark a point in the query tree that produces results of a certain type e.g.
- tag=web
  - query=css OR html OR jquery ..
- tag=database
  - query=oracle OR mysql...
- tag=search
  - query=lucene OR solr or "elastic search" or elasticsearch...

A special facetter can then find these TaggingScorers in the executing query tree and observe the doc ids firing from each of the Scorer streams and record strengths of association between pairs of tags as they are seen to match the same doc. Of course the categories (web vs java) etc could be defined as part of the facet info, acting only as a perspective onto a query's results, but I can imagine there are scenarios where, as in my example, they are defined as an integral part of the query selection criteria and faceting can usefully "listen in" on the various parts of the query to extract these categories.

There are a couple of concepts in here that have me thinking:
1. Is there generally a useful role for adding metadata to query clauses (e.g. for faceting or highlighting/explain introspection)
2. Weighted concept graphs are an interesting form of summary that can be derived quickly from large amounts of data as a by-product of Lucene scoring

There's a lot happening with facets design right now so I'm keen to throw these ideas into the mix.
</comment><comment author="aparo" created="2013-07-14T07:29:36Z" id="20932892">This is a good way to improve facet/aggregation.
Just an hint: in the output the _type of aggregation is missing. I often used in postprocessing of facet results and to check errors.
Probably it's missing because it's a second level detail.
</comment><comment author="julianhille" created="2013-07-15T08:18:00Z" id="20956639">Hi,

my 2 cents:
I like the idea of the whole new way of aggregation, it will be, if speed is good, way more flexible.

My ideas what id like to see:
- i strongly support the automagically "range sizes" like pecke01 said. We're asked about it a lot.
- An "all facets" would be nice. On terms i often have to hard code a limit like 1000 to get all of them. But i dont know when the length of the terms reach this limit. Thus a "give me all facets" would be nice.
- we use a lot of filters and also a lot of facets, but nearly every facet if have to put in more than twice.
  As an example if we have two books with different authors and the same different prices. I could filter for the price and get only one bock (as expected).
  But the facet for author needs every facet filter besides its own.

``` javascript
{
  query: {match_all:{}},
  filter: {
    and: [
      term: {
       price: 10
      },
      term: {
        author: someauthor
      }
    ]
  },
  facets: {
    author: {
      facet_filter: {
        term: {
          price: 10
        }
      },
      terms: {
        field: author,
        size: 10
      }
    }
  }
}
```

this should be solved different. Like a exclude filter for field option or something like that. Sorry at this point i dont come up with an idea to solve it.
</comment><comment author="fredbenenson" created="2013-07-16T14:46:16Z" id="21046783">Very excited about this new direction for facets &amp; aggregations.

I wanted to confirm one thing, and had a question about another. 

First: a specific example (related to my reference to the elasticfacets plugin), where we're aggregating by a date histogram but also doing an aggregation of terms within each bucket.

Everything I've read in this issue indicates this is possible, so I'm just asking for confirmation. Here's how I think it'd look:

``` json
"aggs" : {
    "histo" : {
        "date_histogram" : {
            "field" : "date",
            "interval" : "month"
        }
    },
    "aggregations" : {
        "genders" : {
            "terms" : { "field" : "gender", "order": { "_term" : "desc" } }
        }
    }
}
```

The output would then show the count of documents per-gender per-month based on their `date` field.

Second: will 1.0 be backwards compatible with old-school (e.g. 0.20.1) faceting? 

Thanks!
</comment><comment author="mattweber" created="2013-07-16T14:58:14Z" id="21047705">Hey Fred, 

I can confirm this will work.  You need to move your genders aggregation up into the histo object like this though:

```
"aggs" : {
    "histo" : {
        "date_histogram" : {
            "field" : "date",
            "interval" : "month"
        },
        "aggregations" : {
            "genders" : {
                "terms" : { "field" : "gender", "order": { "_term" : "desc" } }
            }
        }
    }
}
```
</comment><comment author="jrick1977" created="2013-07-16T18:13:14Z" id="21061797">This is looking really good.  One thing I cannot seem to find is an example of a nested aggregation, there is an example of a query but no example of the results.  The type of query I would be interested in seeing would look like this:

```
    "aggs": {
        "genders": {
            "terms": {
                "field": "gender"
            },
            "aggs": {
                "age_groups" : {
                    "range" : {
                        "field" : "age",
                        "ranges" : [
                            { "to" : 5 },
                            { "from" : 5, "to" : 10 },
                            { "from" : 10, "to" : 15 },
                            { "from" : 15}
                        ]
                    },
                    "aggs" : {
                        "avg_height" : { "avg" : { "field" : "height" } }
                    }
                }
            }
        }
    }
```

I believe this should return an aggregation by gender and age.
</comment><comment author="uboness" created="2013-07-18T15:48:57Z" id="21193152">@fredbenenson 

&gt; will 1.0 be backwards compatible with old-school (e.g. 0.20.1) faceting?

We're definitely keeping the facet module for time being... the aggregations is just an additional separated api
</comment><comment author="uboness" created="2013-07-18T15:58:09Z" id="21193833">@jrick1977 

you'd get back something like this:

``` json
"aggregations": {
  "genders": {
    "terms": [
      {
        "term": "female",
        "doc_count": 4,
        "age_groups": [
          {
            "to": 20.0,
            "doc_count": 1,
            "avg_height": {
              "value": 160.0
            }
          },
          {
            "from": 20.0,
            "to": 25.0,
            "doc_count": 0,
            "avg_height": {
              "value": null
            }
          },
          {
            "from": 25.0,
            "to": 30.0,
            "doc_count": 2,
            "avg_height": {
              "value": 160.0
            }
          },
          {
            "from": 30.0,
            "doc_count": 1,
            "avg_height": {
              "value": 173.0
            }
          }
        ]
      },
      {
        "term": "male",
        "doc_count": 3,
        "age_groups": [
          {
            "to": 20.0,
            "doc_count": 0,
            "avg_height": {
              "value": null
            }
          },
          {
            "from": 20.0,
            "to": 25.0,
            "doc_count": 1,
            "avg_height": {
              "value": 175.0
            }
          },
          {
            "from": 25.0,
            "to": 30.0,
            "doc_count": 0,
            "avg_height": {
              "value": null
            }
          },
          {
            "from": 30.0,
            "doc_count": 2,
            "avg_height": {
              "value": 178.5
            }
          }
        ]
      }
    ]
  }
}
```
</comment><comment author="jprante" created="2013-07-18T20:19:05Z" id="21211672">@uboness 

With facet labels, a caller might be able to pass a map of codes and string values (the labels). If the aggregation completes and should list the values in the entries, they are matched against the codes to obtain a label. So, codes as field values could drive e.g. language dependent visualization, without extra lookup loop by the caller.

The reason why order asc/desc is not enough is because it always assume Unicode canonical sorting order. For multilingual texts localized to an environment, this does not suffice. For example, I need german phone book sorting order not only in sorting fields but also in facet entries. It would be nice to have Unicode locale- and collation-aware sorting of entries. Explained here for Java http://docs.oracle.com/javase/tutorial/i18n/text/collationintro.html ICU has much more sophisticated collations http://userguide.icu-project.org/collation/concepts

Another example to have custom sorting is natural sort order.

See also my pull requests, for ICU facets

https://github.com/elasticsearch/elasticsearch-analysis-icu/pull/7

and for collation-based sort keys

https://github.com/elasticsearch/elasticsearch/pull/2338

Thanks, and keep up the good work!
</comment><comment author="jrick1977" created="2013-07-18T20:30:17Z" id="21212467">@uboness Perfect!
</comment><comment author="uboness" created="2013-07-18T20:42:02Z" id="21213255">@jprante 

&gt; With facet labels, a caller might be able to pass a map of codes and string values (the labels). If the aggregation completes and should list the values in the entries, they are matched against the codes to obtain a label. So, codes as field values could drive e.g. language dependent visualization, without extra lookup loop by the caller.

gotcha

&gt; The reason why order asc/desc is not enough...

sure... I guess I misunderstood you, from order direction point of view asc/desc is enough... it's just that we need to be able make the order object extensible to support things like collation... ie:

``` json
"order" : {
   "by" : "name",
   "direction" : "asc",
   ...
}
```

I think it makes sense to support the above as well (and the form we have today for simplicity).
</comment><comment author="lukas-vlcek" created="2013-08-01T16:55:13Z" id="21951994">Looking pretty nice!

If I may one thing: At the beginning I was confused by the terminology a bit. After some time I realized that one can think of this in terms of relational-algebra operations used in traditional SQL.
- Bucket Aggregator -&gt; Grouping
- Calc Aggregator -&gt; [vanilla] Aggregator

Nice definition of these operations in context of Map-Reduce can be found in [Chapter 2.](http://i.stanford.edu/~ullman/mmds/ch2.pdf) (p.32.), [Mining of Massive Datasets](http://i.stanford.edu/~ullman/mmds.html). Maybe using terms Grouping and Aggregation would make it sound more familiar to people with "traditional / old-fashioned" background?

Also it seems to me that the terms Aggregation and Aggregator are somehow interchangeable? At least from the end user perspective it would not hurt to get rid of one of them? At least I do not see what role the Aggregator (as an computational unit) plays in this for now. May be it will make more sense from the Java API perspective later?

In the end some questions:
- Do the buckets have to be distinct or can they overlap?
- Would it make sense for a single bucket to have more then a one calc leave associated with it?
</comment><comment author="jprante" created="2013-08-01T18:38:19Z" id="21959690">+1 for "Grouping" and "Aggregation" terms
</comment><comment author="uboness" created="2013-08-01T18:57:42Z" id="21961244">@lukas-vlcek @jrick1977 

thanks for this feedback!

Reg. changing terminology... one thing to note here is that both from impl. &amp; user perspectives, both the grouping action and the aggregating actions (as you refer to them) are a type of an aggregation. Where ever you can define an aggregation, you can either put a clac or a bucket aggregation there. For this reason we need to have one name to refer to both, and we believed `aggregations` or `aggs` fit best. It's a framework that supports different types of aggregations. Changing the name "bucket" to "group" is fine... if the feedback we get is that it's a more fitting name, we'll do that, but we still need to refer to them as aggregations (just of different kind). making this distinction in the terminology will require changing the API to reflect that, and by that, most chances, will make the API more verbose/complex.

btw, if you have a better name of `calc aggregations` feel free to suggest that (we're kinda on the fence reg. this name).

&gt; Also it seems to me that the terms Aggregation and Aggregator are somehow interchangeable? At least from the end user perspective it would not hurt to get rid of one of them? At least I do not see what role the Aggregator (as an computational unit) plays in this for now. May be it will make more sense from the Java API perspective later?

Aggregation &amp; Aggregator are two different things. From the user perspective, you don't need to know `aggregator`, just `aggregation`. The way you can look at it - an `aggregator` is the dynamic runtime representation of the `aggregation`. It does the actual aggregation **job**, and its output is the corresponding `aggregation` - which can be seen as a basic "static" data structure that holds the result of the aggregation. (if you compare it to facetings, `aggregator` is like `facet executor` and `aggregation` is like `facet`).

I agree that the user documentation should probably not mention aggregators at all... we'll fix that once we have formal docs for it.

&gt; Do the buckets have to be distinct or can they overlap?

Yes... for example you can have multiple ranges that overlap each other... no restriction from impl perspective.

&gt; Would it make sense for a single bucket to have more then a one calc leave associated with it?

It could, yeah... a simple example would be to get 2 different stats aggregation on 2 different fields
</comment><comment author="rmattler" created="2013-08-01T23:44:40Z" id="21977894">Could you please comment if aggregation will solve my use case?

If I have documents with these values.

_source: {date: 01.01.2013 desc: XXX value: 100}
_source: {date: 02.01.2012 desc: XXX value: 200}
_source: {date: 03.01.2011 desc: XXX value: 300}
_source: {date: 04.01.2011 desc: YYY value: 400}
_source: {date: 05.01.2011 desc: YYY value: 500}

I need to produce:

Desc Last Date Last Value
XXX 01.01.2013 100
YYY 05.01.2011 500

I need to get the documents with the most recent date for that desc so I can pull the value off of it.
If aggregation can not give me the document would it be possible to give the document id with the most recent date for that desc? And I could use an id filter to get the documents.

Thanks for your time.
</comment><comment author="mattweber" created="2013-08-02T01:42:20Z" id="21981813">@rmattler This should be asked on the mailing list.  Ask it there and myself and others will actually respond.
</comment><comment author="roytmana" created="2013-08-02T14:13:34Z" id="22008126">Very exciting - just what we need. One question I have is about consistency of the results given the distributed nature of the calculations. Now that ES is moving into the territory of BI it is even more important. Current facet implementation does not guarantee correct counts when ordered by count because distributed calculation and subsequent collation of the results. Will aggregation framework make any such guarantees?
Without them BI applications will suffer greatly as businessbusers must have exact and not approximate results. Total count or sum must stay the same no matter how we aggregate inside a given bucket
Thanks
Alex
</comment><comment author="markharwood" created="2013-08-08T16:25:02Z" id="22335801">&gt; BI applications will suffer greatly as businessbusers must have exact and not approximate results

Some additional thoughts about approaches for building user faith in numbers (dealing with fuzziness and the need for context):

As we move into BI a worry of mine is that search engines are not databases and they are designed to produce fuzzy sets (elements belong to the result set to varying degrees). In our search apps we frequently fail to explain to end users that the results can vary massively in match quality so they should not always put too much stock in any exact numbers we show.
I think the first ES facet I wrote was the one that "buckets" the match scores of docs so you can draw a quality distribution for all of the search results. It was largely for my own benefit to view the "long tail" of low-quality matches for various query types (more like this, fuzzy etc). 
I can think of a number of approaches to coping-with-fuzzy-sets, none of which are ideal :
a) Offer clients tools to first "trim" the long tail of crap e.g. only facet on the top N matches or 
b) Facet summaries can have the option to aggregate quality scores rather than absolute doc counts or
c) "Fuzzy" criteria/result sets are automatically spotted and facets not offered or suitable accuracy warnings displayed alongside any aggregations
d) Do nothing - educate users about interpreting results

Users also have to consider any natural skews in the data and it is often useful to put the numbers (inaccurately matched as they may be) into some sort of context:
- The ability to express geo coverage as % of a background distribution: ( for why, see http://xkcd.com/1138/ )
- "Top" term selection criteria for term-based facets need not always be "most popular" - Mike McCandless and I got into that on his Jira search project: http://goo.gl/vU73gc
- Time-based buckets should be able to diff against corpus stats due to possible fluctuations in corpus size e.g. indeed.com plot skills as a % of all job ads: http://www.indeed.com/jobtrends?q=elasticsearch%2C+solr&amp;l=

In all these scenarios, some background source of stats is required alongside the query result counts as context. The source of this data is typically found looking into the whole corpus.

For performance or API complexity reasons we may choose not to factor any support for these concerns into the core ES design, but it is at least worth acknowledging these issues exist while we are in the design phase.

Cheers
Mark
</comment><comment author="roytmana" created="2013-08-08T22:21:20Z" id="22362651">Mark, 

One of the ways I deal with it now is by calculating two extra  totals per say terms stats facet using stat facets - one total  for the query and the other totals for "missing" for example if facet field is blank. With this two totals I can produce at leas consistent grand total, missing and other counts/sums even if returned top facets are no 100% correct other bucket captures the diff from corect grand total. Then user can request more facet values to be fetched and grand total will stay correct

I wish terms stats facet got missing and other totals consistent with terms facet to make it easier to deal with it but that of course does not solve consistency issue it just makes it less noticeable.

I understand performance concerns of the consistency but ideally it would be left to the implementor to choose slower over  fuzzy if only you supported consistency guarantees
</comment><comment author="fterrier" created="2013-08-13T09:43:33Z" id="22553659">+1 for the automagically "range sizes" like @pecke01 said
</comment><comment author="btiernay" created="2013-09-05T01:36:31Z" id="23838613">I'm curious if this feature will support my use case which can be described as "nested query aggregation on a per hit basis". The difference here is that the aggregation context would be a hit and its direct and indirect sub-documents and not a top level doc set. A key difference is that the aggregations could be used to compute a score which can in turn be globally sorted upon. This is a departure from the Aggregations Module which appears to not have the concept of a hit. 

In SQL terms, this would be the equivalent of a subquery in both a select list projection and order by clause. Although this type of aggregation is currently supported in a limited fashion using scripting and custom scoring, some types of aggregation are simply not possible in an efficient manner (e.g. number of grandchildren with a unique field value).

@martijnvg I'd be curious to get your insight here since you seem to be the "nested guy" :)
</comment><comment author="netconstructor" created="2013-11-08T16:29:41Z" id="28076101">A+ guys... can't wait for these enhancements to get finalized!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't reset TokenStreams twice when highlighting.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3299</link><project id="" key="" /><description>When using PlainHighlighter, TokenStreams are resetted both before highlighting
and at the beginning of highlighting, causing issues with analyzers that read
in reset() such as PatternAnalyzer. This commit removes the call to reset which
was performed before passing the TokenStream to the highlighter.
</description><key id="16468737">3299</key><summary>Don't reset TokenStreams twice when highlighting.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-07-08T12:35:06Z</created><updated>2014-06-24T22:44:50Z</updated><resolved>2013-07-08T12:49:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Cannot create templates for _percolator indexes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3298</link><project id="" key="" /><description>I posted an issue to the Google Group a few months ago:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/ZMG6wrN8ViM

Basically, setting `index.mapper.dynamic` to false breaks the Percolators, because the types do not created automatically.

@imotov suggested that a fix would be to create a template to override the value of `index.mapper.dynamic` back to true:

&gt; You can add a template for the _percolator index that would override the setting. All you need to do is to create file config/templates/percolator_template.json with the following content:

```
{
    "percolator_template" : {
        "template" : "_percolator",
        "settings" : {
            "index.mapper.dynamic": true
        }
}
```

However, it turns out that this file is not being read by Elastic Search. Trying to add the template manually results in the error "InvalidIndexTemplateException[index_template [percolator_test] invalid, cause [template must not start with '_']]".

It looks like this is [returned by the MetaDataCreateIndexService](https://github.com/elasticsearch/elasticsearch/blob/0.90/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java#L160). Is this correct? Is there a valid reason why templates cannot be created that refer to the percolator index? We've tried to 'hack around' the validation by using a regular expression but that didn't seem to work.

It's worth noting that https://github.com/elasticsearch/elasticsearch/issues/788 would also fix this problem for us, and in a nicer way.

It would be good to get some feedback on this as it means we have to manually intervene every time a new index is created that needs percolators.

(Additionally, it's not clear from the documentation that the templates are only read when the index is originally created. If I get a chance I will put in a pull request against the docs.)
</description><key id="16468578">3298</key><summary>Cannot create templates for _percolator indexes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">36degrees</reporter><labels /><created>2013-07-08T12:30:57Z</created><updated>2014-08-08T12:41:02Z</updated><resolved>2014-08-08T12:41:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-27T20:02:11Z" id="23366056">The redesigned percolator in master requires you to store the queries in the `_percolator` type, so if dynamic mapping is disabled you just need to manually add the _percolator type yourself. Also any index can contain the `_percolator` type and because of that you can define index templates for your percolate indices.
</comment><comment author="36degrees" created="2013-09-12T12:59:34Z" id="24316368">Are there any solutions for this for the current version (0.90.*)? 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Weekly DateHistogramFacet producing wrong results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3297</link><project id="" key="" /><description>This failing test shows how the weekly date histogram facet gets confused once timezones and offets are applied to it. Are they applied in the correct order?
</description><key id="16442473">3297</key><summary>Weekly DateHistogramFacet producing wrong results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">synhershko</reporter><labels><label>:Aggregations</label><label>test</label></labels><created>2013-07-07T12:39:43Z</created><updated>2015-04-26T19:01:15Z</updated><resolved>2015-04-26T19:01:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-04-26T19:01:14Z" id="96420936">Sorry it has taken so long to get to this. I've just tested it out on 1.5.1 with aggs, and it works correctly now:

```
DELETE t

PUT t
{
  "mappings": {
    "t": {
      "properties": {
        "num": {
          "type": "integer"
        },
        "date": {
          "type": "date"
        }
      }
    }
  }
}

POST t/t/_bulk
{ "index": {}}
{ "num": 0, "date": "2013-01-28T23:01:01+02:00"}
{ "index": {}}
{ "num": 1, "date": "2013-03-30T23:01:01+02:00"}
{ "index": {}}
{ "num": 2, "date": "2013-03-31T01:01:01+02:00"}
{ "index": {}}
{ "num": 3, "date": "2013-03-31T16:01:01+02:00"}
{ "index": {}}
{ "num": 4, "date": "2013-03-27T04:01:01+02:00"}
{ "index": {}}
{ "num": 5, "date": "2013-04-03T03:01:01+02:00"}

GET t/_search?size=0
{
  "aggs": {
    "stats1": {
      "date_histogram": {
        "field": "date",
        "interval": "day",
        "time_zone": "00:00"
      }
    },
    "stats2": {
      "date_histogram": {
        "field": "date",
        "interval": "day",
        "time_zone": "+02:00"
      }
    },
    "stats_weekly": {
      "date_histogram": {
        "field": "date",
        "interval": "week",
        "time_zone": "+2:00",
        "offset": "-1d"
      }
    }
  }
}
```

Returns:

```
    {
       "took": 2,
       "timed_out": false,
       "_shards": {
          "total": 5,
          "successful": 5,
          "failed": 0
       },
       "hits": {
          "total": 6,
          "max_score": 0,
          "hits": []
       },
       "aggregations": {
          "stats_weekly": {
             "buckets": [
                {
                   "key_as_string": "2013-01-27T00:00:00.000Z",
                   "key": 1359244800000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-03-24T00:00:00.000Z",
                   "key": 1364083200000,
                   "doc_count": 2
                },
                {
                   "key_as_string": "2013-03-31T00:00:00.000Z",
                   "key": 1364688000000,
                   "doc_count": 3
                }
             ]
          },
          "stats1": {
             "buckets": [
                {
                   "key_as_string": "2013-01-28T00:00:00.000Z",
                   "key": 1359331200000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-03-27T00:00:00.000Z",
                   "key": 1364342400000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-03-30T00:00:00.000Z",
                   "key": 1364601600000,
                   "doc_count": 2
                },
                {
                   "key_as_string": "2013-03-31T00:00:00.000Z",
                   "key": 1364688000000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-04-03T00:00:00.000Z",
                   "key": 1364947200000,
                   "doc_count": 1
                }
             ]
          },
          "stats2": {
             "buckets": [
                {
                   "key_as_string": "2013-01-28T00:00:00.000Z",
                   "key": 1359331200000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-03-27T00:00:00.000Z",
                   "key": 1364342400000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-03-30T00:00:00.000Z",
                   "key": 1364601600000,
                   "doc_count": 1
                },
                {
                   "key_as_string": "2013-03-31T00:00:00.000Z",
                   "key": 1364688000000,
                   "doc_count": 2
                },
                {
                   "key_as_string": "2013-04-03T00:00:00.000Z",
                   "key": 1364947200000,
                   "doc_count": 1
                }
             ]
          }
       }
    }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set max open files for systemd startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3296</link><project id="" key="" /><description>...elasticsearch

Note that EnfironmentFile isn't loaded in time for:

LimitNOFILE=$MAX_OPEN_FILES

to work. Without this elasticsearch has the system default limit (likely
1-4K).
</description><key id="16418352">3296</key><summary>Set max open files for systemd startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">awithersdd</reporter><labels /><created>2013-07-05T20:54:53Z</created><updated>2014-07-16T21:52:55Z</updated><resolved>2013-07-19T16:29:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-19T16:29:25Z" id="21260737">Closed by https://github.com/elasticsearch/elasticsearch/commit/300175714482cf54325e6c46ace2abbf3b07f0c8 in 0.90 and https://github.com/elasticsearch/elasticsearch/commit/08e35e4dbea1da5080bf5d03267b24186d5c2e28 in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Could not create TransportClient object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3295</link><project id="" key="" /><description>Hi Guys

Here's java code:

```
    final Settings settings = ImmutableSettings.settingsBuilder()
            .put("cluster.name", "elasticsearch")
            .put("client.transport.sniff", false)
            .put("client", true)
            .put("data", true)
            .build();

    final TransportClient client = new TransportClient(settings);
```

above line throws following exception:

Exception in thread "main" java.lang.NoSuchMethodError: org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(Ljava/lang/CharSequence;IILorg/apache/lucene/util/BytesRef;)V
    at org.elasticsearch.common.Strings.toUTF8Bytes(Strings.java:1502)
    at org.elasticsearch.common.Strings.toUTF8Bytes(Strings.java:1498)
    at org.elasticsearch.search.facet.filter.InternalFilterFacet.&lt;clinit&gt;(InternalFilterFacet.java:40)
    at org.elasticsearch.search.facet.TransportFacetModule.configure(TransportFacetModule.java:40)
    at org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)
    at org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:201)
    at org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:82)
    at org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:130)
    at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)
    at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)
    at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:177)
    at org.elasticsearch.client.transport.TransportClient.&lt;init&gt;(TransportClient.java:119)
    at com.intentline.elasticsearch.ElasticSearchHelper.initDashboardIndexClient(ElasticSearchHelper.java:37)

FYI, I'm using lucene-core-4.3.1.jar and elasticsearch-0.90.2.jar
</description><key id="16403318">3295</key><summary>Could not create TransportClient object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">songday</reporter><labels /><created>2013-07-05T13:33:06Z</created><updated>2017-04-11T13:45:37Z</updated><resolved>2013-08-09T11:23:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-05T14:05:56Z" id="20520505">hey,

this looks like you are having another lucene jar (an older version maybe, maybe from another project) in your classpath? If you are using maven, using `mvn dependency:tree` might show your problem. 
</comment><comment author="songday" created="2013-07-10T02:40:22Z" id="20718542">Thank you for your reply, but I'm sure I use right jar files
</comment><comment author="spinscale" created="2013-07-10T09:47:05Z" id="20731836">I dont have any doubt that you are using the right files, but maybe your dependency tooling is not. Without further information it is going to be pretty tough to provide any help, so can you maybe talk a little bit about your setup, so we can try to figure out what is broken?
</comment><comment author="spinscale" created="2013-08-09T11:23:54Z" id="22388599">Closing for now. Happy to reopen if more information is provided in order to to debug this issue.
</comment><comment author="rMethre" created="2017-04-11T13:45:37Z" id="293267962">@songday Did u find any solution for the above issue??</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix xcontent serialization of timestamp index field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3294</link><project id="" key="" /><description>The index field was serialized as a boolean instead of showing the
'analyed', 'not_analzyed', 'no' options. Fixed by calling
indexTokenizeOptionToString() in the builder.

Closes #3174
</description><key id="16403260">3294</key><summary>Fix xcontent serialization of timestamp index field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-07-05T13:31:20Z</created><updated>2014-06-29T14:22:25Z</updated><resolved>2013-07-15T16:16:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-15T16:16:15Z" id="20980867">Closed in
- master https://github.com/elasticsearch/elasticsearch/commit/28b9e250536f8a554abb26b49d6a80a0d4fb4f03
- 0.90 https://github.com/elasticsearch/elasticsearch/commit/4510b6aa830b11ee5c18ae88aedc76e9af7bd313
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>primary shards not evenly distributed in case of reallocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3293</link><project id="" key="" /><description>It seems that the primary shards (with replication) are not evenly distributed over all nodes in case of a reallocation. An even distribution of primary shards would be a great enhancement for memory management, especially regarding the field cache when _primary_first is used for an operation.
</description><key id="16370115">3293</key><summary>primary shards not evenly distributed in case of reallocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">christianbader</reporter><labels><label>non-issue</label></labels><created>2013-07-04T14:24:10Z</created><updated>2014-05-19T07:08:37Z</updated><resolved>2013-07-16T19:44:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-07-04T15:20:58Z" id="20482438">hi @christianbader, by default the distribution of primary shards is not an important factor of allocation balance. Have you tried to change the  `cluster.routing.allocation.balance.primary` setting?
</comment><comment author="christianbader" created="2013-07-08T14:42:22Z" id="20610015">It seems that the &lt;code&gt;cluster.routing.allocation.balance.primary&lt;/code&gt; setting is not what i'm looking for because it just affects the distribution of primary shards between indexes per node.
</comment><comment author="s1monw" created="2013-07-16T19:44:20Z" id="21068088">hey @christianbader  we introduced `cluster.routing.allocation.balance.primary` mainly for tie-breaking reasons and by default it only has a `minor-vote` in the balance function (low weight 0.05 by default) We did this since for most users primary distribution is not a big deal since operations are executed on all replicas anyway and the primary is just the `leader` on indexing etc. Yet you can certainly give the primary balance a higher weight but this might trigger unnecessary rebalance operations. if you can deal with this then it might be ok.

i'd still be interested what you are trying to do ie. can you give us more info about your usecase?

I am closing this since it's not really an issue. feel free further comment here.
</comment><comment author="iksnalybok" created="2013-12-12T15:46:58Z" id="30433545">Hi,

We are also facing the need to have balanced primary shards.

Our use case is the following:
- one index, with index.number_of_shards=10 and index.number_of_replicas=1
- many incoming documents to index (or update) AND **percolate**
- not that many searches

We don't have (yet) many percolators, but some of them are huge queries involving fuzziness and consume a lot of cpu. In our two nodes cluster, all primary shards finally end up to be on one node. In this case, we observe a very high load (cpu) on the node owning the primary shards, while the second node is really quiet.

The reason is that indexing + percolation happen on the node owning primary shard. Reading around about this issue, I've seen that indexing and synchronization of replicas cost more or less the same; in our case percolation makes the difference. Being able to balance primary shards would allow to balance the cpu load.

NB: with two nodes, playing with cluster.routing.allocation.balance.primary has no effect.

Thanks.
</comment><comment author="SoAG" created="2014-03-14T15:13:25Z" id="37657779">Hi,

@iksnalybok we are facing a similar problem. Just wondering if you have found a solution yet?

Thanks
</comment><comment author="iksnalybok" created="2014-05-08T19:37:42Z" id="42595985">Hi,

Sorry, I don't monitor this mailbox very often!

The short answer is not really.

Our real issue is about percolation: in pre 1.x releases, it occurs on the
primary shards, which, when gathered on the same machine, put all the
percolator-related load on this machine.

It seems that the new percolator design introduced in 1.0 solves this issue.

However, we have not migrated yet. Our quick win is to add a third machine.
When we have such a case, switching the guilty machine off (well, kindly
removing it temporarily from the cluster) rebalances the primaries on the
two other nodes. It pretty ugly and not optimal, but it helps.

Rgds
ik

On Fri, Mar 14, 2014 at 4:13 PM, Roland Jungnickel &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; Hi,
&gt; 
&gt; @iksnalybok https://github.com/iksnalybok we are facing a similar
&gt; problem. Just wondering if you have found a solution yet?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3293#issuecomment-37657779
&gt; .
</comment><comment author="kimchy" created="2014-05-08T22:37:47Z" id="42615026">I am confused by the statement that indexing and percolation happen on the primary shard? Indexing happens on both the primary and the replica, and percolation happens on both as well, so there really isn't a difference between the 2 in that regard...
</comment><comment author="iksnalybok" created="2014-05-16T21:17:05Z" id="43380537">Hi Shay,

I'll try to clarify.

In our case, we are using the index-and-percolate-at-the-same-time api
present in 0.90.

So
http://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/docs-index_.html applies:

&gt; The index operation is directed to the primary shard based on its route (see the Routing section above) and performed on the actual node containing this shard. After the primary shard completes the operation, if needed, the update is distributed to applicable replicas.

Rephrasing it, the index operation (in our case, the index+percolation operation) is performed on the actual node containing the primary shard. So when all primary shards are on the same node, all index+percolation operations happen on this single node.

Note: percolation is not distributed in 0.90.x.
Note: while indexing will also happen on the replica, percolation will not.

In our case, percolation is a costly operation (much heavier than indexing), and is the cause of the high cpu usage.

Regards,
Ik

On Fri, May 9, 2014 at 12:38 AM, Shay Banon notifications@github.comwrote:

&gt; I am confused by the statement that indexing and percolation happen on the
&gt; primary shard? Indexing happens on both the primary and the replica, and
&gt; percolation happens on both as well, so there really isn't a difference
&gt; between the 2 in that regard...
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3293#issuecomment-42615026
&gt; .
</comment><comment author="kimchy" created="2014-05-18T18:42:43Z" id="43447679">@iksnalybok I see. This is no longer relevant in 1.x version, since percolation works completely differently, I don't think there is anything to do here?
</comment><comment author="iksnalybok" created="2014-05-19T07:08:37Z" id="43471687">Indeed, it's a 0.90.x issue. We don't expect a fix. We'd rather migrate to
1.x.

On Sun, May 18, 2014 at 8:43 PM, Shay Banon notifications@github.comwrote:

&gt; @iksnalybok https://github.com/iksnalybok I see. This is no longer
&gt; relevant in 1.x version, since percolation works completely differently, I
&gt; don't think there is anything to do here?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3293#issuecomment-43447679
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Malformed query syntax can cause the parsers to break out of scope</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3292</link><project id="" key="" /><description>elasticsearch does not check that the parser implementations finish on the correct token. It is possible for the parsers to either underrun or overrun the token stream. This can change the structure of the remainder of the query.

Many of the elasticsearch parsers implementations will underrun in the event of malformed content.

For example, match_all:

``` JavaScript
"match_all":{
  "test":{}
}
```

The logic within match_all increments tokens until a CLOSE_OBJECT token is reached. It will terminate on the closing bracket of the test object, not the closing bracket of the match_all object. This causes an erroneous additional bracket to be processed by the surrounding query.

In the example below, I have labelled the bracket pairs as they are interpreted by the elasticsearch parsers.

``` JavaScript
{[1]
    "bool":{[2]
        "must":{[3]
            "match_all":{[4]
                "match_all":{[5]
                }[4]
            }[3]
        }[2],
        "must":{[6]
            "term":{[7]
                "field":"term"
            }[7]
        }[6]
    }[1]
}
```

The second must clause is considered to be external to the bool object, due to the erroneous processing of the match_all above. The parsed query is equivalent to a match_all query.

From reading the code, I conclude that similar issues exist in 24 of the elasticsearch parsers. I believe that this should be fixed by the framework enforcing a scope on the parsers, rather than (or as well as) fixing the individual parsers.

This issue has the potential to confuse users in the event of typos. For example, a match_all that was accidentally nested within a match_all would cause this issue.
</description><key id="16369932">3292</key><summary>Malformed query syntax can cause the parsers to break out of scope</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ccw-morris</reporter><labels /><created>2013-07-04T14:19:28Z</created><updated>2014-07-23T13:08:23Z</updated><resolved>2014-07-23T13:08:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add the ability to ignore or fail on numeric fields when executing more-like-this or fuzzy-like-this queries.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3291</link><project id="" key="" /><description>More-like-this and fuzzy-like-this queries expect analyzers which are able to
generate character terms (CharTermAttribute), so unfortunately this doesn't
work with analyzers which generate binary-only terms (BinaryTermAttribute,
the default CharTermAttribute impl being a special BinaryTermAttribute) such as
our analyzers for numeric fields (byte, short, integer, long, float, double but
also date and ip).

To work around this issue, this commits adds a fail_on_unsupported_field
parameter to the more-like-this and fuzzy-like-this parsers. When this parameter
is false, numeric fields will just be ignored and when it is true, an error will
be returned, saying that these queries don't support numeric fields. By default,
this setting is true but the mlt API sets it to true in order not to fail on
documents which contain numeric fields.

Close #3252
</description><key id="16356830">3291</key><summary>Add the ability to ignore or fail on numeric fields when executing more-like-this or fuzzy-like-this queries.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-07-04T08:25:47Z</created><updated>2014-06-16T17:15:09Z</updated><resolved>2013-07-16T16:40:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>The top_children, has_child and has_parent query can cause error when cached.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3290</link><project id="" key="" /><description>In the case that the `top_children`, `has_child` or `has_parent` queries are cached via the `fquery` filter then an class cast exception error occurs. 
</description><key id="16336685">3290</key><summary>The top_children, has_child and has_parent query can cause error when cached.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-03T19:33:42Z</created><updated>2013-07-03T19:39:47Z</updated><resolved>2013-07-03T19:39:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Proposed fix for the issue #1284 - A new river may not be found immediate...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3289</link><project id="" key="" /><description>Proposed fix for the issue #1284 - A new river may not be found immediately. The proposed fix is to get the river metadata from the primary shard, so that it does not wait for the metadata to propagate.
</description><key id="16326251">3289</key><summary>Proposed fix for the issue #1284 - A new river may not be found immediate...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hshiraishi</reporter><labels /><created>2013-07-03T16:01:02Z</created><updated>2014-07-28T09:39:39Z</updated><resolved>2014-07-28T09:39:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-07-28T09:39:38Z" id="50317771">Good point, this fix was applied with #4864. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Partial fields filtering may return false matches and doesn't allow selecting complete objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3288</link><project id="" key="" /><description>To reproduce:

```
curl -XPUT "http://localhost:9200/test/type1/1" -d'
{
   "field": "value",
   "array" : [ 1 , 2],
   "obj" : {
       "field": "value"
   }
}'
```

Then search:

```
curl -XPOST "http://localhost:9200/test/_search" -d'
{
  "partial_fields": {
     "obj_selection": {
        "include": [ "obj" ]
     },
     "match_problem": {
        "include": [ "field_which_doesnt_exist" ]
     }
  }
}'
```

Results in:

```
{
  ...
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": [
         {
            ...
            "fields": {
               "obj_selection": {}, // not selected.
               "match_problem": {  // should be empty
                  "field": "value"
               }
            }
         }
      ]
   }
}
```
</description><key id="16322370">3288</key><summary>Partial fields filtering may return false matches and doesn't allow selecting complete objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-03T15:04:35Z</created><updated>2013-07-04T08:22:56Z</updated><resolved>2013-07-04T08:22:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Number of results from fuzzy_like_this_field influence "good" results being returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3287</link><project id="" key="" /><description>Hi,

Sorry for the lousy title, but let me explain the problem I'm having.  I'm currently working on a project, and a part of this includes a search engine for addresses, which I have in elasticsearch.  What I'm trying to do is use fuzzy_like_this_field queries when a new character is entered in my search bar to generate autocomplete results and try to "guess" which of the (~1 million) addresses the user is typing.

My issue is that I currently have a size limit on my query, as returning all of the results was both unnecessary and expensive, time-wise.  My issue, is that I often am not getting the "correct" result unless I return 1000 or more results from the query.  For example, if I enter "100 broad" in trying to search for "100 broadway" and I only return 200 results (about the max that I can do without it taking too long), 100 broadway is nowhere to be found, even though all of the returned results have a higher levenshtein distance than the result that I want.  I get "100 broadway" as the first result if I return 2000 results from my query, but it takes too long.  I can't even filter the results that got returned to bring the correct one to the top, because it's not being returned.  

Shouldn't putting a size limit of N on the query return the best N results, not a seemingly random subset of them?

Sorry if this is poorly worded or too vague.
</description><key id="16319566">3287</key><summary>Number of results from fuzzy_like_this_field influence "good" results being returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/markharwood/following{/other_user}', u'events_url': u'https://api.github.com/users/markharwood/events{/privacy}', u'organizations_url': u'https://api.github.com/users/markharwood/orgs', u'url': u'https://api.github.com/users/markharwood', u'gists_url': u'https://api.github.com/users/markharwood/gists{/gist_id}', u'html_url': u'https://github.com/markharwood', u'subscriptions_url': u'https://api.github.com/users/markharwood/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/170925?v=4', u'repos_url': u'https://api.github.com/users/markharwood/repos', u'received_events_url': u'https://api.github.com/users/markharwood/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/markharwood/starred{/owner}{/repo}', u'site_admin': False, u'login': u'markharwood', u'type': u'User', u'id': 170925, u'followers_url': u'https://api.github.com/users/markharwood/followers'}</assignee><reporter username="">benjamincohen1</reporter><labels /><created>2013-07-03T14:11:22Z</created><updated>2014-08-11T12:08:30Z</updated><resolved>2014-08-11T12:08:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="harlanmilkove" created="2013-07-03T18:33:46Z" id="20435953">I think I've this experienced too. To summarize: if you limit your results to 200, the best result is not in the list. But if you expand the size of the result set to a limit of 1000 where does it show up?

In the results I've seen the document is bumped up to the top of the list. 
</comment><comment author="benjamincohen1" created="2013-07-03T18:35:06Z" id="20436036">Correct.  And in accordance with what you saw, it was the top of my list too.
</comment><comment author="harlanmilkove" created="2013-07-03T18:37:28Z" id="20436215">Ok, no clue why this isn't selecting the best thing from the entire list of possible results regardless of what the query limit is.  Obviously expanding the search result size is somewhat of a workaround based on statistics. This is definitely dirty.  It sounds like a bug to me.
</comment><comment author="markharwood" created="2014-08-11T08:41:52Z" id="51754647">&gt; Shouldn't putting a size limit of N on the query return the best N results, not a seemingly random subset of them?

Completion of partially typed words is not the main objective of FuzzyLikeThis - ypu may want to look at the various suggester APIs to achieve your goal (see [1] and [2]).

FuzzyLikeThis has a different goal in mind and it may not make sense to tweak it for your case - there are different algorithms for different use cases for the same reason there are different kinds of bike for different kinds of terrain: it's all about tuning for a particular objective.

FuzzyLikeThis is tuned for the case where there are several (complete) words in your query, all of them potentially useful but not all of them are as interesting as each other, An example might be a search in a business directory for "John Caruso and sons Inc".  A match on the rare word "Caruso" would be highly valued but a match on the common "Inc" less so. We also want to match "Carusso" but a fuzzy match on "sons" that becomes "sony" is obviously bad.
The way we achieve this is we take a selection of the "root" terms (in this case probably all of them as there are only 5 words) and for each we generate a set of fuzzy-spelt variants. Each variant of the same root is scored using the same IDF (rareness) factor as the root form because we have to assume that the user knows what they are looking for. However if the root word doesn't exist we have no rareness score (e.g. likely in your "broadw" example) so then we score all the variants of the "broadw" root using an average of their IDFs. Levenshtein edit distance is always a factor applied along with IDF.  Another likely ranking factor is the length of the field in each doc if norms are enabled - Lucene will naturally favour short fields over longer ones.
You can use the "explain" api [3] to give a more detailed breakdown of the reasons for each match and that may be useful to add here if you think that the logic I outlined here is misbehaving in some way.

With these sorts of tuning issues it is often a challenge as fixing the ranking emphasis for one case can break many others. If you do chose to use the suggester APIs avoid sticking the house numbers at the front of the string as that will generate many duplicate chains of road names etc in the FST.

[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-suggesters.html
[2] http://www.elasticsearch.org/blog/you-complete-me/
[3] http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-explain.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>There is no official debian/ubuntu repository</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3286</link><project id="" key="" /><description>The benefits of having repository are:
1. Package manager will validate package signature for us 
2. It makes automated server configuration easier by using standard method rather then forcing us to write custom scripts and checks just for ES.
3. It makes updates fully automated (when we want them to be) 
4. It is the standard method of installing and managing software on debian based systems, so our admins are not surprised.
</description><key id="16319088">3286</key><summary>There is no official debian/ubuntu repository</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Fiedzia</reporter><labels /><created>2013-07-03T14:01:47Z</created><updated>2014-09-10T16:45:07Z</updated><resolved>2014-02-05T10:32:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-07-19T18:20:16Z" id="21267609">This is more of an inconvenience then a problem.  While it'd be great to have an elasticsearch deb in the Debian repositories it'd be a huge pain to make elasticsearch meet the packaging guidelines.  In the mean time the deb elasticsearch makes works fine and you can always add it to corporate repository.

It might be useful for have a PPA repository though.
</comment><comment author="deanmalmgren" created="2013-07-27T17:28:27Z" id="21669137">Just seeing this after I added [a similar comment](https://github.com/elasticsearch/elasticsearch/issues/2060#issuecomment-21668845) on #2060. A PPA would be fantastic for Debian-based systems. +1

Is there something similar for rpm-based systems? 
</comment><comment author="nikicat" created="2013-08-17T11:34:34Z" id="22810142">+1
</comment><comment author="nik9000" created="2013-08-30T11:27:24Z" id="23555104">This is getting popular.  Yesterday @Nikerabbit asked me for something like this as well.
</comment><comment author="nik9000" created="2013-08-30T11:48:17Z" id="23555971">Getting a PPA for Elasticsearch looks like it might be a pain because they expect to build source packages but Elasticsearch doesn't make source packages - it just makes a binary package.  It looks like both Debian and PPA policy would forbid Elasticsearch from downloading anything from maven central while building the source package.  Getting Elasticsearch building under those restriction would probably take some effort.

I'm not really sure what to do about this other than find someone else to host it.
</comment><comment author="fuwaneko" created="2013-08-30T13:04:29Z" id="23559411">@nik9000 You're not forced to build package from sources, you can pack binaries just fine.
</comment><comment author="nik9000" created="2013-08-30T13:34:56Z" id="23561240">@fuwaneko let me take another look see then.
</comment><comment author="nik9000" created="2013-08-30T14:04:15Z" id="23563080">@fuwaneko so I had another look and it looks like you have to upload a source package.
</comment><comment author="fuwaneko" created="2013-08-30T16:02:37Z" id="23571431">@nik9000 I believe it's not as in "java source" but rather "files to be put inside package". We use [FPM](https://github.com/jordansissel/fpm) with reprepro and nginx for our custom private repository (which is another solution).
</comment><comment author="benmccann" created="2013-08-30T16:57:14Z" id="23575024">+1

I have not setup a repository before, but it looks like there are multiple ways of doing it. One is to create your own repository by picking from several tools to create the repository structure (debarchiver looked like possibly the easiest) and then put the repository structure in the serving path of a web server. The other would be the PPA route mentioned here. I agree that it looks like you should be able to use the already compiled jars.

These may be the best docs for setting up a PPA:
http://developer.ubuntu.com/packaging/html/getting-set-up.html
http://developer.ubuntu.com/packaging/html/packaging-new-software.html

Here's an example script for it:
https://github.com/jordansissel/fpm/issues/170#issuecomment-6007105

Later down on that thread it sounds like people were very happy with creating their own repository straight from .deb files with PRM. I would definitely try that route first as it looks like the easiest:
https://github.com/dnbert/prm
</comment><comment author="lrowe" created="2013-10-15T03:36:54Z" id="26307164">A quick search turned up https://launchpad.net/~nickstenning/+archive/elasticsearch which is relatively up to date and seems to install and run for me. Would be great to see an official PPA though.
</comment><comment author="thedrow" created="2013-10-17T15:56:40Z" id="26520143">+1
</comment><comment author="michelem09" created="2013-10-22T10:54:28Z" id="26793351">+1
</comment><comment author="paravoid" created="2013-11-04T16:30:56Z" id="27699250">Since elasticsearch.org already provides a .deb file, providing an apt repository under downloads.elasticsearch.org is the natural next step and very easy to do so.

reprepro is an excellent tool to do so; the config is something like ten lines, where you specify suites available (wheezy, precise etc.) and components. It's incredibly easy and might save the rest of us lots of time, plus increase our security with the cryptographically signed hashes :)
</comment><comment author="nik9000" created="2013-11-04T16:38:02Z" id="27699906">I'm with paravoid - we love cryptographically signed hashes!
</comment><comment author="JesperTerkelsen" created="2013-11-11T10:53:14Z" id="28189627">+1
</comment><comment author="prusswan" created="2013-11-13T03:02:43Z" id="28359173">+1
</comment><comment author="i-trofimtschuk" created="2013-11-18T01:34:09Z" id="28670248">+1
like paravoid said hosting a debian repo at downloads.elasticsearch.org would be the way to go since by hosting a .deb you're basically almost there.
</comment><comment author="boltronics" created="2013-11-18T01:37:27Z" id="28670339">+1
</comment><comment author="kimchy" created="2013-11-19T16:33:28Z" id="28805322">update, we are working on setting it up on our end on elaticsearch.org, we will keep this issue updated with details
</comment><comment author="thedrow" created="2013-11-19T18:07:30Z" id="28816189">Woot!
</comment><comment author="DracoBlue" created="2013-11-20T15:50:43Z" id="28900359">Nice! We currently want to run elasticsearch on debian sequeze and are searching for a way to stay up to date with the elasticsearch.deb-file updates. An own repository at elasticsearch.org would definately do the trick!
</comment><comment author="fredericosilva" created="2013-11-26T09:03:39Z" id="29276499">+1
</comment><comment author="thedrow" created="2013-11-26T09:47:57Z" id="29279144">Guys since it's already in progress please stop posting +1s.
</comment><comment author="thedrow" created="2014-02-05T10:17:03Z" id="34153122">Shouldn't this issue be closed now that http://www.elasticsearch.org/blog/apt-and-yum-repositories/ was announced?

ping @Fiedzia 
</comment><comment author="dadoonet" created="2014-02-05T10:32:38Z" id="34154183">Indeed! Thanks.
</comment><comment author="konklone" created="2014-03-06T17:08:18Z" id="36910701">This really should be a dedicated (and kept-current) flat page in the docs, though, not just a blog post. Is there one I'm missing?
</comment><comment author="spinscale" created="2014-03-07T09:39:18Z" id="37002425">Does this help: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html
</comment><comment author="konklone" created="2014-03-07T14:15:45Z" id="37027259">Yes, it does - thanks, I couldn't find that when I looked.
</comment><comment author="jmagnusson" created="2014-03-24T13:39:04Z" id="38444635">Why is it not hosted on the official PPA?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom_score query within has_child query does not recognize child document fields within the script property</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3285</link><project id="" key="" /><description>custom_score query within has_child query does not recognize child document fields within the score script.

I have two document types, "Forums" and "ForumStats". Forums is a parent type and ForumStats is a child type. I had set the mapping correctly.

When I issue query like the one below on forums type, ES complains that the field "avgPostsPerMinute" is not found in "Forums" type, which I feel is wrong as the script is supposed to look at child document fields.

Note: "avgPostsPerMinute" field belongs to ForumStats type.

{
  "size": 20,
  "sort": {
    "_score": "desc"
  },
  "query": {
    "has_child": {
      "type": "forumstats",
      "score_type": "sum",
      "query": {
        "custom_score": {
          "script": "doc['avgPostsPerMin'].value",
          "query": {
            "match_all": {}
          }
        }
      }
    }
  }
}

If I set the script to a value that doe snot refer to child document type, then the query runs and I get a score for the parent document correctly.

Please review this and let me know if this is by design or a bug. If it is by design, it looks odd as the custom score query is running on child documents and I would expect ES to recognize the child document fields within the script property.

Thanks &amp; Regards,
Aditya.
</description><key id="16309632">3285</key><summary>custom_score query within has_child query does not recognize child document fields within the script property</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">apasumarthi</reporter><labels /><created>2013-07-03T09:46:20Z</created><updated>2014-07-03T09:58:27Z</updated><resolved>2014-07-03T09:58:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-07-03T21:42:36Z" id="20446789">@apasumarthi this is [known issue](https://github.com/elasticsearch/elasticsearch/blob/906f278896a8bf12952434bb6aae33ee3df150b7/src/main/java/org/elasticsearch/index/query/HasChildQueryParser.java#L70). The script in the `custom_score` query inside `has_child` is working in context of the parent query. So, you need to use fully qualified field names:

```
    "custom_score": {
        "script": "doc['forumstats.avgPostsPerMin'].value",
```

See working example [here](https://github.com/imotov/elasticsearch-test-scripts/blob/master/relevant_children.sh).
</comment><comment author="apasumarthi" created="2013-07-04T05:59:54Z" id="20460326">Thanks Igor. 

I was able to figure out the same (with little hunch). Hopefully the actual issue will be fixed soon.
</comment><comment author="fanbiao" created="2014-07-03T03:47:03Z" id="47864061">i have the same problem abount has_child
</comment><comment author="clintongormley" created="2014-07-03T09:58:27Z" id="47887852">This has been fixed in #5838
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Debian package dependencies can result in java uninstallation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3284</link><project id="" key="" /><description>The discussion happened at https://github.com/elasticsearch/elasticsearch/pull/3105

The Java dependency was changed from "Depends" to "Suggest" - which sounded good, but could result in uninstallation of java, in case no other package depended on java.

Maybe the 'Enhances' flag can help here, need to investigate, see https://github.com/elasticsearch/elasticsearch/pull/3105#issuecomment-20370219
</description><key id="16306087">3284</key><summary>Debian package dependencies can result in java uninstallation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-03T08:08:02Z</created><updated>2013-07-15T14:09:46Z</updated><resolved>2013-07-15T14:09:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-15T14:09:46Z" id="20971711">Closed by https://github.com/elasticsearch/elasticsearch/commit/c59b0b22e26ffcb6fed799a36e69ea92ab31f7c1

We decided, that it is still allowed to have elasticsearch packages installed anywhere without a java installation, but the init script now returns a clear error. Also all suggests/depends in debian package were removed. For more information see the commit comment
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Incorrect response on concurrent indexing of the same document.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3283</link><project id="" key="" /><description>When doing concurrent re-indexing of an existing document with the version parameter, sometimes the response has status 200 and is neither a success response (with "ok": true), nor a conflict.

The following are the relevant requests/responses when keeping a counter in a document and running 10 parallel processes to increment it:

```
curl -XPUT 'http://localhost:9200/test_storage_index/storage/key?version=22158&amp;pretty=true' -d '{"value": "113"}'
Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22158,"exists":true, "_source" : {"value": "112"}}
...
curl -XPUT 'http://localhost:9200/test_storage_index/storage/key?version=22367&amp;pretty=true' -d '{"value": "322"}'
Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22367,"exists":true, "_source" : {"value": "321"}}
...
curl -XPUT 'http://localhost:9200/test_storage_index/storage/key?version=22500&amp;pretty=true' -d '{"value": "455"}'
Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22501,"exists":true, "_source" : {"value": "455"}}
...
curl -XPUT 'http://localhost:9200/test_storage_index/storage/key?version=22771&amp;pretty=true' -d '{"value": "726"}'
Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22771,"exists":true, "_source" : {"value": "725"}}
```

From the final value of the counter, it seems that in these cases sometimes the indexing operation goes through, sometimes it doesn't.
This happens on ES version 0.90.2.
</description><key id="16271945">3283</key><summary>Incorrect response on concurrent indexing of the same document.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">abelldh</reporter><labels /><created>2013-07-02T16:03:43Z</created><updated>2013-07-08T10:34:47Z</updated><resolved>2013-07-08T09:31:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-07-05T11:23:11Z" id="20514251">Hi @abelldh 

I'm not following your description of the problem.  Could you provide more information as to what you are doing and what incorrect results you are seeing?

thanks
</comment><comment author="abelldh" created="2013-07-05T11:56:50Z" id="20515512">Hi.
I'm trying to do atomic updates to a single ES document via the versioning feature. The update goes this way:
- get the document
- modify it
- try to re-index it specifying the version

I expect the last indexing to either succeed or detect a conflict. If it succeeds, all went well. If there is a conflict, I repeat the process (get, update, store).
I'm testing this with a document keeping a counter in the "value" field and starting 10 processes which atomically update the counter 100 times.
The problem is, sometimes the response is neither success nor a conflict, but has the form reported above.
Example:

```
Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22771,"exists":true, "_source" : {"value": "725"}}
```

If I treat those cases as conflicts (so re-try to increment the counter when they happen), in the end I usually get a value higher than 1000 (which shows sometimes the update went through in the first place).
I I treat them as success, I usually get a value lower than 1000 (which shows sometimes the update didn't go through in the first place).
This seems to indicate that when that "anomalous" response is received, you can't be sure whether the re-indexing was performed or not.
I'm using pyes 0.20.0 to communicate with ES and logging the requests and responses from within pyes. Not sure whether this can affect the problem.
If you think it can help, I can try to write down a small script exhibiting the problem.

Cheers
</comment><comment author="clintongormley" created="2013-07-05T12:04:19Z" id="20515749">Ah ok - I'm understanding more now.  So it seems like you're getting a `GET` response to what should have been a `PUT` request.

I remember seeing something similar a long time ago when `HEAD` requests were sending error bodies, but the HTTP client wasn't reading the body, so the body would appear in the next request.  I wonder if something similar is happening with pyes.

Could you paste your script? I'll try to replicate it in another language, so that we can see if it is pyes or ES
</comment><comment author="abelldh" created="2013-07-05T13:35:35Z" id="20519126">&gt; So it seems like you're getting a GET response to what should have been a PUT request.

Yes, this description nails it down in a more concise way :-)
This script shows the problem.

``` python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pyes
from multiprocessing import Process, Queue, queues


index_name = 'test_storage_index'
doc_type = 'storage'
key = 'key'

es = pyes.ES('localhost:9200', timeout=60)


def get_ES():
    global es
    return es


def get_doc():
    es = get_ES()
    res = es.get(index_name, doc_type, id=key)
    if 'value' not in res:
        raise Exception('Document not retrieved or wrong format: %s' % res)
    return res


def version_from_doc(doc):
    return doc._meta['version']


def get_value():
    return get_doc()['value']


def put(value, version=None, fail_in_doubt=True):
    """
    :param version: fail unless current version in ES matches
    :param fail_in_doubt: fail if response gives no guarantee on update
    """
    es = get_ES()
    res = es.index(
        {'value': value},
        index_name,
        doc_type, id=key,
        version=version)
    # Note: sometimes res contains no 'ok'
    #       and contains 'exists': True instead
    #       This usually gives no guarantee about the result
    if not res.get('ok', None):
        if res.get('exists', None):
            print 'Strange response: %s' % res
            # If we accept doubtful response, return
            if not fail_in_doubt:
                return
        raise Exception('Error saving version %s: %s' % (version, res))


def update_value():
    try:
        old_doc = get_doc()
        old_val = old_doc['value']
        new_val = old_val + 1
        put(
            new_val,
            version=version_from_doc(old_doc),
            fail_in_doubt=True)
        return new_val
    except Exception, e:
        raise Exception('Failure: %s' % repr(e))


def test_concurrent_increment():

    put(0)
    print get_value()
    assert(get_value() == 0)

    q = Queue()

    def runner(label, n):

        import time
        import random
        time.sleep(random.random())

        for i in xrange(n):
            while True:
                try:
                    v = update_value()
                    q.put(v)
                    break
                except Exception, e:
                    q.put(e)
                    import time
                    import random
                    time.sleep(random.random())

    procs = []
    for i in xrange(10):
        proc = Process(target=runner, args=(i + 1, 100,))
        proc.start()
        procs.append(proc)

    all_done = False

    while not all_done:
        for i in range(10):
            try:
                dat = q.get(True, 1)
                print dat
            except queues.Empty:
                pass
        all_done = all([not proc.is_alive() for proc in procs])

    for proc in procs:
        proc.join()

    print 'Final: %d' % get_value()


if __name__ == '__main__':
    test_concurrent_increment()

```
</comment><comment author="bleskes" created="2013-07-08T09:31:36Z" id="20594552">Hi @abelldh,

Thanks for the script. It was very insightful. 

The problem lies in the way python implements multi-processing. It basically uses unix's fork but _doesn't_ cleanup process memory afterwards. This means you inherit everything (i.e., all object, sockets, file handles and what have you) from the parent process. Although the code suggest you get a clean "run this in another process", you actually get lots of dependencies. You have to be really careful when you create &amp; run Process objects so they won't inherit too much.

Luckily it seems that in your case you are better off using threading which means you don't need to worry about all of this. I don't know what the rest of your application does, but if you can avoid processes and fall back to threading you life will be much simpler. Usually most of the heavy lifting is done by ES or a database which means that the python process is IO bound and threads scale.

To illustrate, I modified your script to make it easy to switch between threading and processes. I also made it only print the "Strange response" line which makes it easy to run and see whether it occurs. When you run in threading mode, everything work. If are using processes, it also works because of a couple of subtle modifications I made:
- the es global is only created on demand
- the main process doesn't use ES (I commented out the part which sets the id to 0)

Because of the changes, pyes is imported and initialised separately in each process which does the trick. I don't know exactly what shared global state gets pyes confused (or maybe one of the libraries it uses like urllib3) but perhaps you can report it here: https://github.com/aparo/pyes

Cheers,
Boaz

Modified script:

```
#!/usr/bin/env python
# -*- coding: utf-8 -*-


use_threading = False

if use_threading:
    from threading import Thread
    import Queue
    runner_class = Thread
    queue_class = Queue.Queue
    queue_empty_exception = Queue.Empty
else:
    from multiprocessing import Process, Queue, queues
    runner_class = Process
    queue_class = Queue
    queue_empty_exception = queues.Empty


index_name = 'test_storage_index'
doc_type = 'storage'
key = 'key'

es = None

def get_ES():
    global es
    if not es:
        import pyes
        pyes.connection_http.update_connection_pool(maxsize=20)
        es = pyes.ES('localhost:9200', timeout=60)

    return es


def get_doc():
    es = get_ES()
    res = es.get(index_name, doc_type, id=key)
    if 'value' not in res:
        raise Exception('Document not retrieved or wrong format: %s' % res)
    return res


def version_from_doc(doc):
    return doc._meta['version']


def get_value():
    return get_doc()['value']


def put(value, version=None, fail_in_doubt=True):
    """
    :param version: fail unless current version in ES matches
    :param fail_in_doubt: fail if response gives no guarantee on update
    """
    es = get_ES()
    res = es.index(
        {'value': value},
        index_name,
        doc_type, id=key,
        version=version)
    # Note: sometimes res contains no 'ok'
    #       and contains 'exists': True instead
    #       This usually gives no guarantee about the result
    if not res.get('ok', None):
        if res.get('exists', None):
            print 'Strange response: %s' % res
            # If we accept doubtful response, return
            if not fail_in_doubt:
                return
        raise Exception('Error saving version %s: %s' % (version, res))


def update_value():
    try:
        old_doc = get_doc()
        old_val = old_doc['value']
        new_val = old_val + 1
        put(
            new_val,
            version=version_from_doc(old_doc),
            fail_in_doubt=True)
        return new_val
    except Exception, e:
        raise Exception('Failure: %s' % repr(e))


def test_concurrent_increment():

    # put(0)
    # print get_value()   # Uncomment this line to enable bug in multi-process mode.
    # assert(get_value() == 0)

    q = queue_class()

    def runner(label, n):

        import time
        import random
        time.sleep(random.random())

        for i in xrange(n):
            while True:
                try:
                    v = update_value()
                    #q.put(v)
                    break
                except Exception, e:
                    #q.put(e)
                    import time
                    import random
                    #time.sleep(random.random())

    procs = []
    for i in xrange(10):
        proc = runner_class(target=runner, args=(i + 1, 100,))
        proc.start()
        procs.append(proc)

    all_done = False

    while not all_done:
        for i in range(10):
            try:
                dat = q.get(True, 1)
                print dat
            except queue_empty_exception:
                all_done = all([not proc.is_alive() for proc in procs])
                if all_done:
                    break
                pass
        all_done = all([not proc.is_alive() for proc in procs])

    for proc in procs:
        proc.join()

    print 'Final: %d' % get_value()


if __name__ == '__main__':
    test_concurrent_increment()

```
</comment><comment author="abelldh" created="2013-07-08T10:34:47Z" id="20597307">Thanks @bleskes and @clintongormley. I confirm that delaying pyes initialization and having it done in the subprocesses fixes the issue. That's great news for us, because it means we can use ES for tasks we were considering an additional data store for.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How disable remote access in elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3282</link><project id="" key="" /><description>When running a elasticsearch server. We can access www.example.com:9200

But i want block remote access only allow local access.

How to setting?
</description><key id="16254891">3282</key><summary>How disable remote access in elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">grantchen</reporter><labels /><created>2013-07-02T09:14:27Z</created><updated>2013-07-02T10:14:42Z</updated><resolved>2013-07-02T10:14:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-02T10:14:42Z" id="20337510">Please use the google group to ask questions like this, as way more people are looking there. This should be used for bugs/issues only.

Regarding your problem, you should read http://www.elasticsearch.org/guide/reference/modules/network/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using Upsert to index a child document in Bulk update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3281</link><project id="" key="" /><description>I'm trying to update a child document or upsert one if one doesn't exist already. I'm doing this on many documents through the bulk api. Can we make it so that the parent of the newly upserted document is the one specified in the parent option of the update api?
</description><key id="16241698">3281</key><summary>Using Upsert to index a child document in Bulk update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottc52</reporter><labels /><created>2013-07-01T23:50:10Z</created><updated>2013-07-17T17:35:49Z</updated><resolved>2013-07-17T17:35:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-17T15:07:47Z" id="21119495">Hi, I'm not sure I follow, could you be more specific about what you're asking? Maybe post an example too?
</comment><comment author="scottc52" created="2013-07-17T17:35:49Z" id="21129985">Hi. I decided to work around the issue by using a "create" request first and than an "update". Thank you. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minimum_master_node not working as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3280</link><project id="" key="" /><description>Elastic Search Version: 0.90.1
Number of servers: 16
Nodes: 32 (2 nodes per server)

When using N/2+1 to set the "discovery.zen.minimum_master_nodes":

**discovery.zen.minimum_master_nodes: 17**

The node will never join the cluster. I can set it to: 

**discovery.zen.minimum_master_nodes: 5**

And it joins, but setting it to any number above 5 and it all not join.

Currently I have 4 hosts in the "discovery.zen.ping.unicast.hosts: " 

**discovery.zen.ping.unicast.hosts: ["node1", "node6", "node11", "node16"]**

If I change "discovery.zen.minimum_master_nodes: 5" to "6" the node will not join the cluster. However if I add an additional host to "discovery.zen.ping.unicast.hosts:" the node will then join the cluster. This behavior is consistant, +1 to master_node and I need to add a host to unicast setting.
</description><key id="16239340">3280</key><summary>minimum_master_node not working as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wsfloyd</reporter><labels /><created>2013-07-01T22:42:08Z</created><updated>2013-07-02T07:27:46Z</updated><resolved>2013-07-02T07:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-02T07:27:46Z" id="20330657">Duplicate of #3262 I guess
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>(Java) Using primitive arrays instead of Object with map/builder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3279</link><project id="" key="" /><description>One might argue that not being able to handle primitive arrays is not a bug. I wanted to report this anyway because it took me a very long time to debug. When doing something like this

``` java
Map&lt;String, Object&gt; map = ...;
map.put(LON_LAT, new double[]{123, 321});

client.index(new IndexRequestBuilder(client, indexName())
                        .setId(...)
                        .setType(...)
                        .setSource(map)
                        .request());
```

I received 

```
Caused by: org.elasticsearch.ElasticSearchIllegalArgumentException: the character '[' is not a valid geohash character
    at org.elasticsearch.common.geo.GeoHashUtils.decode(GeoHashUtils.java:280)
    at org.elasticsearch.common.geo.GeoHashUtils.decodeCell(GeoHashUtils.java:320)
    at org.elasticsearch.common.geo.GeoHashUtils.decode(GeoHashUtils.java:297)
    at org.elasticsearch.common.geo.GeoHashUtils.decode(GeoHashUtils.java:286)
    at org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parseGeohash(GeoPointFieldMapper.java:456)
    at org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parseStringLatLon(GeoPointFieldMapper.java:382)
    at org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parse(GeoPointFieldMapper.java:367)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:599)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:467)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:515)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:457)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:507)
    ...
```

Changing `new double[]{}` to `new Double[]{}` fixes the problem. Looking at `XContentMapConverter.java`, I have found that `writeValue(...)` does not handle primitive arrays. 

Depending how you look at it, this can be a bug. :)
</description><key id="16228073">3279</key><summary>(Java) Using primitive arrays instead of Object with map/builder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amir20</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-01T18:24:11Z</created><updated>2013-07-01T19:51:58Z</updated><resolved>2013-07-01T19:48:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-01T19:45:36Z" id="20305426">a bug or not, we can fix it.
</comment><comment author="amir20" created="2013-07-01T19:51:43Z" id="20305827">Fix looks good. Thanks for the immediate follow up. Cheers!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Lookup by Query/Filter (aka. Join Filter)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3278</link><project id="" key="" /><description>This PR adds support for generating a terms filter based on the field values
of documents matching a specified lookup query/filter.  The value of the 
configurable "path" field is collected from the field data cache for each 
document matching the lookup query/filter and is then used to filter the main 
query.  This is can also be called a join filter.

This PR abstracts the TermsLookup functionality in order to support multiple
lookup methods.  The existing functionality is moved into FieldTermsLookup and
the new query based lookup is in QueryTermsLookup.  All existing caching 
functionality works with the new query based lookup for increased performance.

During testing of I found that one of the performance bottlenecks was 
generating the Lucene TermsFilter on large sets of terms (probably since
it sorts the terms).  I have created a FieldDataTermsFilter that uses the
field data cache to lookup value of the field being filtered and compare it to
the set of gathered terms.  This significantly increased performance at the 
cost of higher memory usage.  Currently a TermsFilter is used when the number
of filtering terms is less than 1024 and the FieldDataTermsFilter is used
for everything else.  This should eventually be configurable or we need to
perform some test to find the optimal value.

Examples:

Replicate a has_child query by joining on the child's "pid" field to the
parent's "id" field for each child that has the tag "something".

```
curl -XPOST 'http://localhost:9200/parentIndex/_search' -d '{
    "query": {
        "constant_score": {
            "filter": {
                "terms": {
                    "id": {
                        "index": "childIndex",
                        "type": "childType",
                        "path": "pid",
                        "query": {
                            "term": {
                                "tag": "something"
                            }
                        }
                    }
                }
            }
        }
    }
}'
```

Lookup companies that offer products or services mentioning elasticsearch.
Notice that products and services are kept in their own indices.

```
curl -XPOST 'http://localhost:9200/companies/_search' -d '{
    "query": {
        "constant_score": {
            "filter": {
                "terms": {
                    "company_id": {
                        "indices": ["products", "services"],
                        "path": "company_id",
                        "filter": {
                            "term": {
                                "description": "elasticsearch"
                            }
                        }
                    }
                }
            }
        }
    }
}'
```
</description><key id="16225138">3278</key><summary>Terms Lookup by Query/Filter (aka. Join Filter)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>:Query DSL</label><label>stalled</label></labels><created>2013-07-01T17:20:37Z</created><updated>2017-03-30T20:36:12Z</updated><resolved>2016-08-08T20:16:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-07-02T07:19:05Z" id="20330363">Just updated PR with significant improvements to lookup and filter on numeric fields.  Performing the lookup on a numeric field is now ~2x faster than the same lookup on a string field.
</comment><comment author="damienalexandre" created="2013-07-10T12:43:34Z" id="20739218">I love this functionality, I already got an use case for it! :+1: 
</comment><comment author="mattweber" created="2013-07-27T01:17:40Z" id="21656758">Updated PR to be current with the latest master changes and fixed a bug when executing across multiple nodes.  All commits have been squashed.    

Has anyone had a chance to review this PR?
</comment><comment author="mattweber" created="2013-08-15T05:03:11Z" id="22685306">Updated to work with latest changes in master.
</comment><comment author="martijnvg" created="2013-09-11T11:25:10Z" id="24232827">Sorry for getting involved so late... This feature is really cool! (equivalent of a subquery). 

The main concern I have with this feature, is that the amount of values being send over the wire between nodes is unbounded. Each time the terms filter with query is parsed a query is being executed on all shards if the routing option isn't used. In the case a query matches with millions of values (which is think is a common scenario), then all these values need to be send over the wire. This transport of values will occur for each search request with a terms filter being executed times the number of shards this search request is targeted for. 

I'm also wondering about the lookup cache here, if one document changes in the index that terms lookup query is being executed on, then this the cache entry for that query needs to be trashed and think a bit more about the lookup cache it doesn't make much sense in the case if terms lookup query is used, since it is really hard to find out what has changed in perspective with the terms lookup query.

Also wondering If the terms lookup query would only be executed on shards to are locally available on the node that is executing the terms filter, would this feature still be useful? We can make for example the routing option required.
</comment><comment author="mattweber" created="2013-10-08T22:35:58Z" id="25933623">Hey @martijnvg,

&gt; The main concern I have with this feature, is that the amount of values being send over the wire between nodes is unbounded. Each time the terms filter with query is parsed a query is being executed on all shards if the routing option isn't used.

I have the same concern about the amount of data being sent, but I guess that  could be documented as a known limitation.  I imagine people will find it acceptable given the benefits a query like this provide.  

The request per shard (since it happens during parsing) is a problem.  I actually didn't realize that the json was parsed per-shard.  I would really like to gather the terms once then send the to the shards.  Maybe creating a new phase for this?  Not sure.  A while back we talked about field collapsing and you mentioned that to do it correctly some things will need to be moved around internally (the phases if I remember correctly).  Would that change make implementing this feature any easier/better?

&gt; I'm also wondering about the lookup cache here

Yea, I did this to try and piggyback on the terms lookup functionality/caching.  I don't think caching the lookup makes much sense unless we can do it per-segment, but then we still have the cost of sending over the transport.  My original idea was to have this as a new JoinQuery and JoinFilter that only caches the resulting terms fitler.  I would probably move to that vs. being part of the lookup if we were to move forward on this. 

&gt; Also wondering If the terms lookup query would only be executed on shards to are locally available on the node that is executing the terms filter, would this feature still be useful? 

I don't think so.  You might as well use parent/child functionality if that was the case.

At any rate, the move from trove to hppc (088e05b3) caused some issues with my current patch due to hppc ObjectOpenHashSet not implementing the standard Collection interface.  Before I spend some time getting everything fixed up, let's figure out the best approach to get this moving forward.

Thanks for taking the time to look at this!
</comment><comment author="martijnvg" created="2013-10-11T09:05:20Z" id="26123735">&gt; I have the same concern about the amount of data being sent, but I guess that could be documented as a known limitation. I imagine people will find it acceptable given the benefits a query like this provide.

Perhaps there can be an option that controls the amount of terms being fetched per shard? Something like `shard_size` in terms facet. We can set it to a default value based on `size`? It can be set to unlimited, but then someone who does this is aware that it can generate a lot of traffic between nodes.

About the request per shard, we can do something smart and have a helper service that just bundles the shard level field value retrieval in one request. I don't think there is a need to introduce an additional distributed phase here. (it is possible via an additional phase, but no necessary and this makes this feature much bigger then it should?)

About hppc, in what case do you need bridge to the JCF world?
</comment><comment author="mattweber" created="2013-10-11T16:39:31Z" id="26151855">Yea, I think having a `shard_size` makes perfect sense especially if there is a way to set it to unlimited.

That helper service sounds great, is there anything like that being used somewhere in the code that I can have a look at?  I prefer that over the new distributed phase for sure!  

I had relied on JCF by abstracting TermsLookup to have a getTerms method that returns a collection of terms.  The existing field based implementation returns a list of strings, my query implementation returned THashSet of either BytesRef or Number depending on the field type being looked up.  Hppc doesnt implment Collection interface, so I can't just swap Trove for Hppc easily.  This won't be a problem if I didn't try to make this part of terms lookup.  What do you think about pulling this back out into a `JoinFilter` and `JoinQuery`?
</comment><comment author="martijnvg" created="2013-10-13T23:20:48Z" id="26229556">I don't know of a good example right now, but it should be something simple. It should just keep the query result around for the duration of the query phase (maybe as a ThreadLocal?), so that unnecessary requests are avoided and drop the results after the query phase has been completed.  

I think it is best just to completely move over to Hppc. The TermsLookup#getTerms should return Iterator instead of Collection. Both implementations would return a simple wrapper that delegate to the actual implementation. For the QueryTermsLookup you can just make TermsByQueryAction work with ObjectOpenHashSet and just wrap the result in an anonymous Iterator impl. For FieldTermsLookup you can just keep using XContentMapValues#extractRawValues as is and just return the list's iterator.

Looking at XContentMapValues#extractRawValues usages, I think it can be moved to use Hppc nativly instead of JCF, but that is an unrelated to this change.
</comment><comment author="mattweber" created="2013-10-14T18:54:19Z" id="26279437">Sounds good, let me see what I can do.  I am going to work on Hppc fix first as that should be pretty easy.  Thanks.
</comment><comment author="martijnvg" created="2013-10-14T21:20:46Z" id="26289840">@mattweber great that you can work on this! Lets move this forward and get this in.
</comment><comment author="mattweber" created="2013-10-15T05:20:33Z" id="26309981">@martijnvg Got this updated for hppc but I am stuck trying to figure out how to execute only a single `TermsByQueryAction` or re-using the response.  The filter parser runs in multiple threads (one per shard) with nothing tying it back to the original `TransportSearchTypeAction` that actually submitted the search request to the threadpool and ultimately merges responses.  A `ThreadLocal` won't work because we will have X threads parsing the filter and thus triggering a lookup.  I need to figure out a way to tie each individual shard request to the original query request...
</comment><comment author="martijnvg" created="2013-10-15T15:37:32Z" id="26345463">Yes, a ThreadLocal won't help then... Maybe for now keep the TermsByQueryAction as is. The `IndicesTermsFilterCache` actually does the caching on a node level, so that should prevent unnecessary requests if a node holds multiple shards. Only the app integrating with the ES should clear the cache when the data that matched with the lookup query has changed. Perhaps the app can use the percolate api for this.
</comment><comment author="mattweber" created="2013-10-15T16:00:55Z" id="26347589">PR updated with latest changes from master and the switch to hppc.
</comment><comment author="mattweber" created="2013-10-15T17:09:05Z" id="26353275">@s1monw The tests I am writing for this keep failing.  Just wondering if this is because I am running it on a mac with the new randomized testing?  Doesn't appear to be anything related to my actual test.

https://gist.github.com/mattweber/28e400ec2d33090fd5e0
</comment><comment author="s1monw" created="2013-10-15T17:41:51Z" id="26356009">it tells you that you are missing to release the searcher you are acquired:

```
Caused by: java.lang.RuntimeException: Unreleased Searcher, source [termsByQuery]
    at org.elasticsearch.test.engine.MockRobinEngine$AssertingSearcher.&lt;init&gt;(MockRobinEngine.java:104)
    at org.elasticsearch.test.engine.MockRobinEngine.newSearcher(MockRobinEngine.java:92)
    at org.elasticsearch.index.engine.robin.RobinEngine.acquireSearcher(RobinEngine.java:689)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:609)
    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:603)
    at org.elasticsearch.action.terms.TransportTermsByQueryAction.shardOperation(TransportTermsByQueryAction.java:183)
    at org.elasticsearch.action.terms.TransportTermsByQueryAction.shardOperation(TransportTermsByQueryAction.java:73)
    at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$2.run(TransportBroadcastOperationAction.java:225)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    ... 1 more
```

it even tells you where this happened: `TransportTermsByQueryAction.java:183`  - test framework awesomeness :)
</comment><comment author="mattweber" created="2013-10-15T17:46:01Z" id="26356364">I do release it...

```
} finally {
            // this will also release the index searcher
            context.release();
            SearchContext.removeCurrent();
}
```

It looks like something with the MockDirectory.  I have had this test complete successfully, but most the time it fails.  If I delete all the TEST cluster data manually and run it, it tends to pass.
</comment><comment author="s1monw" created="2013-10-15T17:53:22Z" id="26356967">The mock dir failure is a side-effect of this not being released properly! 

This seems spooky:

```
 FieldMapper fieldMapper = context.smartNameFieldMapper(request.field());
 if (fieldMapper == null) {
    throw new SearchContextException(context, "field not found");
  }
```

seems like you don't release it if there is no field mapper? Maybe you don't create the mapping propperly and the mapping is not available on the shard? do you use dynamic mapping?
</comment><comment author="mattweber" created="2013-10-15T18:05:47Z" id="26357981">I use dynamic mapping, but this is only executing over a single shard so it should have the mapping once documents are indexed.  Let me make sure the context gets released even if there is a fieldMapper exception.  What get's me is that the test passes sometimes...
</comment><comment author="mattweber" created="2013-10-15T18:16:15Z" id="26358870">Still getting random fails... I can see that on tests that fail there is this:

```
[2013-10-15 11:09:54,077][INFO ][org.elasticsearch.plugins] [transport_client_node_0] loaded [], sites []
```

On tests that pass, that is not there.  
</comment><comment author="s1monw" created="2013-10-15T20:57:38Z" id="26371546">I pulled that PR in and did some modifications to get a more clear error message. I don't see any pending searchers with that one anymore and it fails all the time with that seed. check this out: https://gist.github.com/s1monw/6998524 

this should give you a better idea why you are seeing the nullpointer exceptions all the time

with this cmd it fails consistently for me:

```
 mvn test -Dtests.seed=6181805A6A5BDD1 -Dtests.class=org.elasticsearch.test.integration.action.terms.SimpleTermsByQueryActionTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Dtests.cluster_seed=132D5734748D2F80
```
</comment><comment author="mattweber" created="2013-10-15T20:59:47Z" id="26371697">Thanks Simon, I think I found the problem in the serialization code.  It only happens when using a different transport.    Randomized testing did it's job!
</comment><comment author="mattweber" created="2013-10-15T21:14:56Z" id="26372895">Ohh I see you fixed the serialization as well in that gist.  Thanks!  
</comment><comment author="mattweber" created="2013-10-16T04:36:21Z" id="26392374">Pushed up the latest changes.  The bug in the tests was due to missing serialization of the `field` variable in the `TermsByQueryRequest` which would only get triggered using a Transport client. I would have never caught this if it wasn't for the randomized testing!  Thanks for the help and awesome test framework @s1monw.

While fixing this, I removed some of the unnecessary multi-shard and multi-node tests since that is all handled by the randomized tests.  I updated my tests to pick a random number of shards, documents to index, and range query.  

BTW, I force pushed the latest update to this PR so you should do a new checkout.
</comment><comment author="mattweber" created="2013-10-18T21:17:41Z" id="26630493">@martijnvg I addressed your comments.  I am going to work on exposing a choosing between a regular `TermsFilter` and the `FieldDataTermsFilter` then push up the changes.  Thanks! 
</comment><comment author="mattweber" created="2013-10-21T01:34:54Z" id="26688490">@martijnvg 

Pushed latest code with all your suggestions.  I added a `buildAsBytes` method to the `FilterBuilder` interface to be consistent with `QueryBuilder`, is this ok and should I pull this into its own PR?  

Please take a look and let me know what you think of the current code.  I will get started on some documentation, more + better tests, and I do some more profiling to see if I can find any hotspots, etc.

Thanks!
</comment><comment author="mattweber" created="2013-10-22T16:55:35Z" id="26821071">@martijnvg Once @s1monw pushes the fielddata refactor in #3943 it should be pretty trivial to do the terms vs docs iteration optimization you talked about since `setDocId` will return the number of values for a given field.
</comment><comment author="mattweber" created="2013-10-25T00:37:37Z" id="27046525">I just pushed up a change with support for using a `BloomFilter` for terms lookup.  So instead of passing around a the terms gathered from each shard we just pass a serialized `BloomFilter` which is significantly faster.  On top of that, we also avoid the `copyShared` which creates a new `BytesRef` for each term pulled from the field data.  The `BloomFilter` can be configured with with a minimum size and false positive probability so the user can control the lookup accuracy to their needs.  Initial testing shows this is ~5x faster than passing than passing the terms.

The bloom filter is not supported for numeric fields as testing showed that the conversion from the primitive type to a BytesRef in the field data is slower than actually passing the terms.
</comment><comment author="martijnvg" created="2013-10-28T17:51:02Z" id="27236967">@mattweber The bloom filter is a nice idea! Still need to look at it properly. Since it is optional we still need the `shard_size` option, right?
</comment><comment author="mattweber" created="2013-10-28T18:00:43Z" id="27238067">Yes, we still need the shard_size option I just have not put it in yet.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for parent in multi get request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3277</link><project id="" key="" /><description>When specifying the docs to be returned in a multi get request, a parent
field could not be specified, so that some docs seemingly did not exist,
even though they did.

This fix behaves like the normal GetRequest and simply overwrites the
routing value if it has not yet been set.

Also a test for routing with mget has been added.

Closes #3274
</description><key id="16214614">3277</key><summary>Support for parent in multi get request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-07-01T13:42:42Z</created><updated>2014-06-19T02:36:31Z</updated><resolved>2013-07-02T08:02:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-07-01T14:18:55Z" id="20284561">+1, looks good, I would also backport it to 0.90.
</comment><comment author="spinscale" created="2013-07-02T08:02:00Z" id="20332011">Landed in master in https://github.com/elasticsearch/elasticsearch/commit/2dcc66431033bf9da22cf7b591bef90e1583c005 and in 0.90 https://github.com/elasticsearch/elasticsearch/commit/d339d6801aa7cf1ed82ba13e16fc266ed29d4ad1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent is ignored in exists request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3276</link><project id="" key="" /><description>The `parent` param is ignored in `HEAD` requests on `/index/type/id`:

```
curl -XPUT 'localhost:9200/test_1?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_parent" : {
            "type" : "foo"
         }
      }
   }
}
'

curl -XGET 'localhost:9200/_cluster/health?wait_for_status=yellow&amp;pretty=1'

curl -XPUT 'localhost:9200/test_1/test/1?parent=5&amp;pretty=1'  -d '
{
   "foo" : "bar"
}
'

curl -XHEAD 'localhost:9200/test_1/test/1?parent=5&amp;pretty=1'

# [Mon Jul  1 14:50:27 2013] ERROR: Not Found (404)
#
```
</description><key id="16212710">3276</key><summary>Parent is ignored in exists request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-07-01T12:55:10Z</created><updated>2013-07-01T15:27:52Z</updated><resolved>2013-07-01T15:27:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update HighlightBuilder.Field API, it should allow for the same API as S...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3275</link><project id="" key="" /><description>...earchConstextHighlight.Field. In other words, what is possible to setup using DSL in highlighting at the field level is also possible via the Java API.
BTW we would like to use this :-)
</description><key id="16212202">3275</key><summary>Update HighlightBuilder.Field API, it should allow for the same API as S...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-07-01T12:41:05Z</created><updated>2014-06-27T23:43:12Z</updated><resolved>2013-08-02T20:21:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-07-02T08:43:12Z" id="20333647">Just to add more context about this issue. Currently, it is perfectly possible to use JSON DSL to override global highlighting at the field level, for example to change pre/post_tags for specific field but there is no way of doing the same via Java API in HighlightBuilder.Field

This PR enhances Highlighter.Field API. Basically, I added the same features that are now allowed when parsing highlighted field JSON here: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/highlight/HighlighterParseElement.java#L140

Both SearchContextHighlight.Field and HighlightBuilder.Field should have the same API, right? But why do we need to keep two similar POJOs? (that is what I call dichotomy in #3269 )
</comment><comment author="s1monw" created="2013-08-02T20:21:50Z" id="22033270">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mget: no support for "parent"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3274</link><project id="" key="" /><description>The `parent` param is not supported by `mget` either at the top-level or at the per-document level  (while `routing` is):

```
curl -XPUT 'localhost:9200/test_1?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_parent" : {
            "type" : "foo"
         }
      }
   }
}
'

curl -XGET 'localhost:9200/_cluster/health?wait_for_status=yellow&amp;pretty=1'

curl -XPUT 'localhost:9200/test_1/test/1?parent=4&amp;pretty=1'  -d '
{
   "foo" : "bar"
}
'

curl -XGET 'localhost:9200/test_1/test/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "parent" : "4",
         "_id" : "1"
      }
   ]
}
'

# {
#    "docs" : [
#       {
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "test",
#          "exists" : false
#       }
#    ]
# }


curl -XGET 'localhost:9200/test_1/test/_mget?parent=4&amp;pretty=1'  -d '
{
   "docs" : [
      {
         "_id" : "1"
      }
   ]
}
'

# {
#    "docs" : [
#       {
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "test",
#          "exists" : false
#       }
#    ]
# }
```
</description><key id="16208476">3274</key><summary>Mget: no support for "parent"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2013-07-01T10:44:06Z</created><updated>2013-07-02T07:03:47Z</updated><resolved>2013-07-02T07:03:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-07-01T11:10:41Z" id="20275501">Should probably just be supported at the `docs` level, not the top level
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parsing fields in 0.90.2 is changed.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3273</link><project id="" key="" /><description>Something have changed in the way search requests are parsed. Haven't had time to look into the code for it. Can do that later tonight if no one is jumps on this.
This query works as expected

``` json
{    
    "fields": [
        "Title"
    ],
    "query": {
        "query_string": {
            "query": "Banana"
        },
     "language":"en"
    }
}
```

This query ignores the field param and returns `_source`

``` json
{   
    "query": {
        "query_string": {
            "query": "Banana"
        },
     "language":"en"
    }, 
    "fields": [
        "Title"
    ],
}
```

Should be the same query. Have previously been the same query. 
</description><key id="16205837">3273</key><summary>Parsing fields in 0.90.2 is changed.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">pecke01</reporter><labels /><created>2013-07-01T09:23:58Z</created><updated>2013-07-01T09:47:40Z</updated><resolved>2013-07-01T09:47:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-01T09:39:39Z" id="20271957">Hey,

something must be missing/different in your description, as my dumb test worked:

```
curl -X DELETE localhost:9200/foo
curl -X PUT localhost:9200/foo

curl -X PUT 'localhost:9200/foo/bar/1?refresh=1' -d '{ "foo" : "Banana"}'


curl localhost:9200/foo/bar/_search -d '{   
    "fields": [
        "foo"
    ],
    "query": {
        "query_string": {
            "query": "Banana"
        }
    }
}'

curl localhost:9200/foo/bar/_search -d '{   
    "query": {
        "query_string": {
            "query": "Banana"
        }
    },
    "fields": [
        "foo"
    ]
}'
```

Getting the same reply which only contains the foo field in both cases.

So I guess your are doing something different, than I did.
</comment><comment author="pecke01" created="2013-07-01T09:39:54Z" id="20271976">Seems to be with the extended `language` param in the query. Usually it is a custom query that I have that handles the query. But the error occurs with `query_string` as well.
</comment><comment author="clintongormley" created="2013-07-01T09:41:20Z" id="20272032">`language` is not a recognised param to `query_string`. Don't you mean `analyzer`?
</comment><comment author="pecke01" created="2013-07-01T09:42:26Z" id="20272083">My own custom parser have a param called language. And it works with the param but not when moving fields after the query. The param is parsed and used but fields ignored. Using query_string is just so it can be reproduced.
</comment><comment author="clintongormley" created="2013-07-01T09:45:04Z" id="20272178">So most (many?) constructs in ES throw an error on a bad param. Some still don't - they just abort pull parsing. I'm guessing that `query_string` is one of those.  So the real issue in `query_string` is not reporting bad params.

Pull parsing complicates things, as evidenced by #3250 

Perhaps your custom query makes a similar mistake?
</comment><comment author="pecke01" created="2013-07-01T09:47:40Z" id="20272289">@clintongormley Could be. I will look into it. Thanks for your time. Closing the issue and will reopen if I needed. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stop aborting of multiget requests in case of missing index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3272</link><project id="" key="" /><description>The MultiGet API stops with a IndexMissingException, if only one of all
requests tries to access a non existing index. This patch creates a
failure for this item without failing the whole request.

Closes #3267
</description><key id="16202755">3272</key><summary>Stop aborting of multiget requests in case of missing index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-07-01T07:47:09Z</created><updated>2014-07-01T23:41:36Z</updated><resolved>2013-07-01T08:35:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Supports mget fields parameter given as string.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3271</link><project id="" key="" /><description>Closes #3270
</description><key id="16193189">3271</key><summary>Supports mget fields parameter given as string.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-06-30T20:53:54Z</created><updated>2014-06-23T14:20:32Z</updated><resolved>2013-07-01T12:08:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-01T12:08:22Z" id="20277692">Committed in master 9ce0156d395784bc67f0fc306d4f59e4cfc88d46 and 0.90 branches 15c81cd784b182712273ec1b66911e14649d212f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mget: the fields parameter should accept a string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3270</link><project id="" key="" /><description>The fields parameter in the `mget` API only accepts an array of field names - it should accept a single string, like it does for the `search` API:

```
curl -XPUT 'localhost:9200/test_1/test/1?pretty=1'  -d '
{
   "foo" : "bar"
}
'

curl -XGET 'localhost:9200/_cluster/health?wait_for_status=yellow&amp;pretty=1'

curl -XGET 'localhost:9200/test_1/test/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "fields" : "foo",
         "_id" : "1"
      },
      {
         "fields" : [
            "foo"
         ],
         "_id" : "1"
      }
   ]
}
'

# {
#    "docs" : [
#       {
#          "_source" : {
#             "foo" : "bar"
#          },
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "test",
#          "exists" : true,
#          "_version" : 1
#       },
#       {
#          "fields" : {
#             "foo" : "bar"
#          },
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "test",
#          "exists" : true,
#          "_version" : 1
#       }
#    ]
# }
```
</description><key id="16192125">3270</key><summary>Mget: the fields parameter should accept a string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-30T19:33:41Z</created><updated>2013-07-02T06:51:08Z</updated><resolved>2013-07-02T06:51:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-02T06:51:07Z" id="20329472">Committed in master 9ce0156d395784bc67f0fc306d4f59e4cfc88d46 and 0.90 branches 15c81cd784b182712273ec1b66911e14649d212f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve camelCase/underscore_case handling</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3269</link><project id="" key="" /><description>This code frequently occurs in the ES source:

``` java
if ("foo_bar".equals(fieldName) || "fooBar".equals(fieldName) {
...
}
```

It is certainly nice to handle both camelCase and underscore_case but the camelCase related test can be easily forgotten which leads to sometimes inconsistent behavior.

What about adding some method in common Strings to handle this:

``` java
boolean equalsUnderscoreOrCamelCase(String s1, String s2) {
  // "foo_bar", "foo_bar" =&gt; true
  // "foo_bar", "fooBar" =&gt; true
  // "fooBar", "fooBar" =&gt; true
  // "fooBar", "foo_bar" =&gt; true
 // otherwise =&gt; false
}
```

Or maybe to avoid adding too much overhead underscore_case can be used everywhere in the code and this method can be added to common Strings:

``` java
boolean equalsOrCamelCaseEquals(String s1, String s2) {
...   
}
```

The code would become:

``` java
if (Strings.equalsOrCamelCaseEquals(fieldName, "foo_bar")) {
...
}
```

This would just add the overhead of applying toCamelCase() to s2 if s1 and s2 are different.

If you think it is a good idea I'm willing to work on it and change it in the code where it is relevant.
</description><key id="16191194">3269</key><summary>Improve camelCase/underscore_case handling</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Paikan</reporter><labels /><created>2013-06-30T18:19:34Z</created><updated>2013-07-02T08:43:13Z</updated><resolved>2013-07-02T00:08:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-01T08:57:36Z" id="20270120">I like it!

no need for a bigbang migration IMO, providing the string method and telling the engineers to use them is sufficient, I guess.
</comment><comment author="uboness" created="2013-07-01T09:29:25Z" id="20271470">I personally don't see a big added value here, first it will potentially introduce redundant string manipulation for each param in each request (with potentially redundant object creations) and second, ppl will need to remember/know they need to use this util method instead of the traditional equals... and then just like they can forget to add a special check for camel casing they can forget to use this method. 

I do think it can be useful to do this for the automated api testing though
</comment><comment author="Paikan" created="2013-07-01T09:41:49Z" id="20272052">@uboness yeah I understand your point of view and I tend to agree after more thinking the overhead isn't justified by a sufficient added value.

The automated api testing should make sure that camelCase and underscoreCase is consistently supported everywhere.

Feel free to close this one.
</comment><comment author="dadoonet" created="2013-07-01T10:27:26Z" id="20273922">Just a naive question. Why do we need to support many different formats for naming options?
I thought that this kind of test was only here because of deprecated naming. Did I miss something here?
</comment><comment author="clintongormley" created="2013-07-01T10:28:32Z" id="20273958">It's not deprecated. We still support camel case.  But I do wonder if we SHOULD deprecate it.  I'd love to know how many people are actually using it. 
</comment><comment author="uboness" created="2013-07-01T10:35:23Z" id="20274213">@clintongormley +1
</comment><comment author="lukas-vlcek" created="2013-07-01T12:15:57Z" id="20277970">On a related note - is there any reason why all the String constants are not defined on central place/places? For example there are many places where the code rely on the developer to correctly type constant on two or more places through the code (like in highlighting parsing and builder parts).
If I understand it correctly all those constants are interned internally anyway so it might be beneficial to have something like `Highlight.Field.POST_TAGS` instead of repeating "post_tags" on several places (reducing issues introduced by typos and naming unification).
My 2 cents.
</comment><comment author="uboness" created="2013-07-01T21:18:24Z" id="20311102">@lukas-vlcek TBH, introducing these constants ends up cluttering the code and makes it harder to navigate and read. As for the java clients, these parameters are exposed as methods in the builders, so they don't need to be exposed externally. Indeed, this comes with a price of double checking there are no typos, but in most cases, these literals are only checked and used in the parsers. Like with @Paikan suggestion above, it's all about finding the right balance and carefully considering the pros/cons of each approach and in many of the cases we base the decisions on experiments (I tried to move a lot of these param names to constants in the past and the code became much less descriptive and harder to read). From performance perspective, these string literals are indeed interned, so performance is not factorized in the "price".

My 2 cents ;)
</comment><comment author="uboness" created="2013-07-02T00:08:20Z" id="20318982">(re-closing) based on the discussion above. to sum it up, while having a method `equalsOrCamelCaseEquals` makes perfect sense from logical perspective, in practice it doesn't add much value as we don't really mitigate the risk of having the developer forgetting to support camel casing (the dev will still need to remember to use this method), furthermore, having this method means more string manipulation and potential object creations per multi-word param per request. 

That said, we should incorporate this approach in the api testing to make sure both the under_score &amp; camelCase versions of the params are properly tested.

@lukas-vlcek if you want to continue the discussion on the constants, perhaps create another issue dedicated to it and i'll be happy to continue it there, this issue is dedicated to a different discussion.
</comment><comment author="lukas-vlcek" created="2013-07-02T08:31:35Z" id="20333170">@uboness no problem. I understand this POW. Normally, I wouldn't bore much about this if I haven't been digging into #3275 at the same time where this situation leads to dichotomy.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix search shards count method when targeting concrete and aliased indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3268</link><project id="" key="" /><description>This is related to the same example as issue #2682.

This bug can lead to a forced query_and_fetch search type which might not be the expected behavior.

I propose to fix it this way because it is a quick 2 lines patch. But maybe we should refactor a bit and add a shared method for searchShards and searchShardsCount to avoid divergent behavior in the future.

Reading the code it seems like searchShardsCount is mainly implemented to test if we are hitting 1 shard or more than 1 shards so maybe it could be renamed and optimized for that specific use case.

Feel free to fix the problem otherwise or to tell me what do you prefer and I will be happy to change this pull request.
</description><key id="16190335">3268</key><summary>Fix search shards count method when targeting concrete and aliased indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-06-30T17:23:06Z</created><updated>2014-06-20T15:15:04Z</updated><resolved>2013-08-19T15:52:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-07-30T15:34:13Z" id="21799584">Thanks for pointing this out! Let me just check if I got it right. The searchShards method has been fixed (I see that #2682 is closed and the related #2683 has been merged) but the same change has not been applied to the searchShardsCount method, right? And that would lead to erroneously forcing query_and_fetch if the searchShardsCount returns 1?

I guess it would be nice to share the same code between the two methods then and maybe write a test that reproduces the bug that we are trying to address. What do you think?
</comment><comment author="Paikan" created="2013-07-30T17:26:01Z" id="21807505">@javanna yeah you got it perfectly right!

Yeah I agree I can work on it and update my pull request hopefully before the end of the week if you want. If you want it done before it is ok if you prefer to implement it yourself
</comment><comment author="javanna" created="2013-07-31T09:52:53Z" id="21851405">No rush, you can go ahead and update your pull request when you have time.
</comment><comment author="Paikan" created="2013-07-31T09:59:24Z" id="21851730">@javanna great thanks I will
</comment><comment author="kimchy" created="2013-08-17T17:47:28Z" id="22816364">hey, I suggest we pull this fix first, just so we have a fix in master and 0.90, and think about optimization / refactoring in a separate issue?

One note regarding potentially sharing code, its important that we don't cause the round robin iterators be invoked when calling searchShardsCount, cause if they will, it will mean that with the common case, of 1 replica, the same copy of the data will be hit each time (a single search request will increment the round robin index by 1 for the count, and then the next one will increment it again for the search).
</comment><comment author="Paikan" created="2013-08-17T22:22:23Z" id="22820748">@kimchy sure we can push this fix and work on the refactoring in a separate issue.

I was about to update the pull request with the refactoring tomorrow (I am still missing the unit tests for it). Basically what I have done is create a new private method Set&lt;IndexShardRoutingTable&gt; computeTargetedShards(ClusterState clusterState, String[] indices, String[] concreteIndices, @Nullable Map&lt;String, Set&lt;String&gt;&gt; routing) with the shared logic in it. It is called by searchShardsCount and searchShards.
</comment><comment author="Paikan" created="2013-08-17T22:29:25Z" id="22820861">@kimchy sorry a part is missing there the computeTargetedShards would return a set of IndexShardRoutingTable
</comment><comment author="kimchy" created="2013-08-17T22:30:43Z" id="22820886">@Paikan I am ok with the current pull request, then @javanna, lets pull this in once you double check it. Then, lets open a new pull request with the suggested refactoring, and discuss it there.
</comment><comment author="Paikan" created="2013-08-17T22:32:18Z" id="22820910">@kimchy ok I am going to open another issue with my proposed pull request tomorrow and we will be able to discuss more there about the suggested refactoring and maybe ways to optimize / improve it
</comment><comment author="javanna" created="2013-08-19T15:52:06Z" id="22882078">Merged into 8e137b1450dde3776747e3637799ff817cbb41f9 and backported to 0.90. Thanks @Paikan !!
I've also added a test for it...have a look at 7f7f79d622e84bae079a7dda92721cb36f808916 if you're interested ;)
</comment><comment author="Paikan" created="2013-08-19T18:10:07Z" id="22892107">@javanna thanks for pushing this and for adding the missing unit test it was quite interesting indeed.

@kimchy I have opened and updated PR #3530 to discuss about refactoring.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mget aborting request if index missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3267</link><project id="" key="" /><description>`mget` returns an error for each doc if the type or id is not found, but throws a top-level error if the index is not found.  This seems inconsistent:

```
curl -XPUT 'localhost:9200/test_1/test/1?pretty=1'  -d '
{
   "foo" : "bar"
}
'

curl -XGET 'localhost:9200/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "_index" : "test_1",
         "_id" : "2",
         "_type" : "test"
      },
      {
         "_index" : "test_1",
         "_id" : "1",
         "_type" : "none"
      },
      {
         "_index" : "test_1",
         "_id" : "1",
         "_type" : "test"
      }
   ]
}
'

# {
#    "docs" : [
#       {
#          "_index" : "test_1",
#          "_id" : "2",
#          "_type" : "test",
#          "exists" : false
#       },
#       {
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "none",
#          "exists" : false
#       },
#       {
#          "_source" : {
#             "foo" : "bar"
#          },
#          "_index" : "test_1",
#          "_id" : "1",
#          "_type" : "test",
#          "exists" : true,
#          "_version" : 1
#       }
#    ]
# }

curl -XGET 'localhost:9200/_mget?pretty=1'  -d '
{
   "docs" : [
      {
         "_index" : "test_1",
         "_id" : "2",
         "_type" : "test"
      },
      {
         "_index" : "test_2",
         "_id" : "1",
         "_type" : "test"
      },
      {
         "_index" : "test_1",
         "_id" : "1",
         "_type" : "none"
      },
      {
         "_index" : "test_1",
         "_id" : "1",
         "_type" : "test"
      }
   ]
}
'

# {
#    "status" : 404,
#    "error" : "IndexMissingException[[test_2] missing]"
# }
```
</description><key id="16190311">3267</key><summary>Mget aborting request if index missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-30T17:21:30Z</created><updated>2013-07-01T08:34:53Z</updated><resolved>2013-07-01T08:34:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Don't ignore doc_as_upsert value and simplify. Closes #3265</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3266</link><project id="" key="" /><description>This simplifies the code and should close #3265
</description><key id="16188600">3266</key><summary>Don't ignore doc_as_upsert value and simplify. Closes #3265</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-06-30T14:53:22Z</created><updated>2014-06-14T11:27:56Z</updated><resolved>2013-06-30T17:29:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-06-30T17:29:59Z" id="20250951">Closing as it has been pushed by @martijnvg 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The behavior of doc_as_upsert is incorrect</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3265</link><project id="" key="" /><description>Here is the gist recreation
- First some cleaning

``` bash
$ curl -XDELETE 'localhost:9200/index1'
```
- This insert the document where it shouldn't

``` bash
$ curl -XPOST 'localhost:9200/index1/test1/1/_update' -d '
{
  doc : { field : "value" },
  doc_as_upsert : false 
}'
```
- Changing the order change the behavior of the param

``` bash
$ curl -XPOST 'localhost:9200/index1/test1/2/_update' -d '
{
  doc_as_upsert : false,
  doc : { field : "value" }
}'
```
</description><key id="16188566">3265</key><summary>The behavior of doc_as_upsert is incorrect</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">Paikan</reporter><labels><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-30T14:50:00Z</created><updated>2013-06-30T17:34:41Z</updated><resolved>2013-06-30T16:51:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-30T16:54:51Z" id="20250344">@paikan Thanks for fixing this!
</comment><comment author="Paikan" created="2013-06-30T17:25:09Z" id="20250862">@martijnvg wow thanks it was fast! Shouldn't we push the fix in 0.90 branch too as I think the doc_as_upsert feature has been added in 0.90.2?
</comment><comment author="martijnvg" created="2013-06-30T17:32:54Z" id="20251003">@paikan Just pushed it also to the 0.90 branch. Took a while, the wifi on the train is a bit dodgy. 
</comment><comment author="Paikan" created="2013-06-30T17:34:41Z" id="20251112">@martijnvg cool thanks and have a good end of trip then :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom query parser registration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3264</link><project id="" key="" /><description>Hi,

IndicesQueriesModule's addQuery and addFilter method's signatures seems weird..
I can not register my custom parser's class..

They should be changed to  Class&lt;? extends QueryParser&gt; and Class&lt;? extends FilterParser&gt; respectively..

```
public synchronized IndicesQueriesModule addQuery(Class&lt;QueryParser&gt; queryParser) {
    queryParsersClasses.add(queryParser);
    return this;
}


public synchronized IndicesQueriesModule addFilter(Class&lt;FilterParser&gt; filterParser) {
    filterParsersClasses.add(filterParser);
    return this;
}
```

thanks.
</description><key id="16187515">3264</key><summary>custom query parser registration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rkarakaya</reporter><labels /><created>2013-06-30T13:11:47Z</created><updated>2013-07-01T12:11:00Z</updated><resolved>2013-07-01T12:11:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pecke01" created="2013-06-30T13:54:08Z" id="20247436">Hi,

You register it like this in your plugins processModule

``` java
@Override public void processModule(Module module) {  
      if(module instanceof IndexQueryParserModule){       
            ((IndexQueryParserModule) module).addQueryParser("custom_query", YourQueryParser.class);
     }
}
```

Hope that helps.
</comment><comment author="rkarakaya" created="2013-06-30T13:58:51Z" id="20247479">Thank you very much Marcus..
</comment><comment author="spinscale" created="2013-07-01T12:11:00Z" id="20277782">If you do not want to cast, you can use

``` java
public void onModule(IndexQueryParserModule module) {
    module.addQueryParser("custom_query", YourQueryParser.class);
}
```

also, please use the google group for questions like this, as getting answers is more likely due to a magnitude of more people looking at such questions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleaning up nodenames</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3263</link><project id="" key="" /><description>This is somewhat frivolous, but some of the nodenames were not very name-like. This makes the formatting better.

ex:
Banner, Betty Ross
is now simply
Betty Ross Banner

Closes #3600 
</description><key id="16183468">3263</key><summary>Cleaning up nodenames</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ohnorobo</reporter><labels /><created>2013-06-30T03:47:34Z</created><updated>2014-06-14T07:49:07Z</updated><resolved>2014-01-08T18:25:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2014-01-08T18:15:38Z" id="31861750">:metal: 
</comment><comment author="HonzaKral" created="2014-01-08T18:25:13Z" id="31862651">Merged via https://github.com/elasticsearch/elasticsearch/commit/b7a5537d8397e6bb525988569d05976702547a1e

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>unicast.hosts has to seed enough nodes to satisfy minimum_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3262</link><project id="" key="" /><description>If you have `minimum_master_nodes` set and you don't supply enough nodes in `d.z.p.unicast.hosts`, a node can fail to join the cluster. Described at http://thread.gmane.org/gmane.comp.search.elasticsearch.user/740.
</description><key id="16160150">3262</key><summary>unicast.hosts has to seed enough nodes to satisfy minimum_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/drewr/following{/other_user}', u'events_url': u'https://api.github.com/users/drewr/events{/privacy}', u'organizations_url': u'https://api.github.com/users/drewr/orgs', u'url': u'https://api.github.com/users/drewr', u'gists_url': u'https://api.github.com/users/drewr/gists{/gist_id}', u'html_url': u'https://github.com/drewr', u'subscriptions_url': u'https://api.github.com/users/drewr/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/6202?v=4', u'repos_url': u'https://api.github.com/users/drewr/repos', u'received_events_url': u'https://api.github.com/users/drewr/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/drewr/starred{/owner}{/repo}', u'site_admin': False, u'login': u'drewr', u'type': u'User', u'id': 6202, u'followers_url': u'https://api.github.com/users/drewr/followers'}</assignee><reporter username="">drewr</reporter><labels><label>bug</label></labels><created>2013-06-28T20:32:08Z</created><updated>2014-08-08T12:38:25Z</updated><resolved>2014-08-08T12:38:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-07-03T18:36:39Z" id="20436139">I can reproduce on `0.90.2`.

First node:

```
bin/elasticsearch -f \
  -Des.network.host=localhost \
  -Des.discovery.zen.ping.multicast.enabled=false
```

Second node:

```
bin/elasticsearch -f \
  -Des.network.host=localhost \
  -Des.discovery.zen.ping.multicast.enabled=false \
  -Des.discovery.zen.ping.unicast.hosts=localhost:9300
```

Third node:

```
bin/elasticsearch -f \
  -Des.network.host=localhost \
  -Des.discovery.zen.ping.multicast.enabled=false \
  -Des.discovery.zen.ping.unicast.hosts=localhost:9300 \
  -Des.discovery.initial_state_timeout=5s \
  -Des.discovery.zen.minimum_master_nodes=3
```

The first two nodes will cluster together, but the third, even though there are 3 total master-eligible nodes which should satisfy `discovery.zen.minimum_master_nodes=3`, fails:

```
[Emma Frost] {0.90.2}[29960]: initializing ...
[Emma Frost] loaded [], sites []
[Emma Frost] {0.90.2}[29960]: initialized
[Emma Frost] {0.90.2}[29960]: starting ...
[Emma Frost] bound_address {inet[/127.0.0.1:9302]}, publish_address {inet[localhost/127.0.0.1:9302]}
[Emma Frost] waited for 5s and no initial state was set by the discovery
[Emma Frost] elasticsearch/aSlF6NZnRJK_VVx591J6sw
[Emma Frost] bound_address {inet[/127.0.0.1:9202]}, publish_address {inet[localhost/127.0.0.1:9202]}
[Emma Frost] {0.90.2}[29960]: started
] [Emma Frost] {0.90.2}[29960]: stopping ...
[Emma Frost] {0.90.2}[29960]: stopped
[Emma Frost] {0.90.2}[29960]: closing ...
[Emma Frost] {0.90.2}[29960]: closed
```
</comment><comment author="alistar79" created="2013-08-09T14:53:55Z" id="22399562">Shouldn't es.discovery.zen.minimum_master_nodes = 2.  I thought it was the number of other master eligable nodes?
</comment><comment author="clintongormley" created="2014-08-08T12:38:25Z" id="51595133">Duplicate of #2572
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Facet Min Count Parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3261</link><project id="" key="" /><description>I would like to be able to add a facet query feature that uses a mincount parameter to return only facets back that have at least that number of counts. The mincount parameter was a piece of functionality we lost in facet queries going from SOLR to ElasticSearch.
</description><key id="16159979">3261</key><summary>Facet Min Count Parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">skawata1</reporter><labels /><created>2013-06-28T20:28:07Z</created><updated>2014-01-22T10:50:13Z</updated><resolved>2014-01-22T10:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jackjames" created="2013-09-26T17:05:11Z" id="25184949">This would also be a useful feature for me.  I'm looking to only return errors that have a count over a certain threshold. I'm using a terms facet on the error message field. 
</comment><comment author="amir20" created="2013-12-03T21:41:37Z" id="29754382">This would be helpful. Right now I have to do a lot of in memory processing to get ride of "low" values. Is this on the road plan? 
</comment><comment author="jpountz" created="2014-01-22T10:50:13Z" id="33011077">We added this feature to terms and histogram aggregations: #4662.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>suggest api options enhancement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3260</link><project id="" key="" /><description>Hi,

is it possible to extend suggest API's Option class so that custom suggesters return arbitrary values rather than just text and score?

For example I want to create a custom suggester that returns:
- suggested text
- highligted version of the suggested text
- A few concrete products (promotions) for each suggested text.

My purposed solution:
- Define an interface (StreamableToXContent) that extends both Streamable and ToXContent
- Modify Option.class as fallows:
  
  private Text text;
  private Text label; //this is presented to the user
  private float score;
  private StreamableToXContent data; //this is for arbitrary data

any thoughts?

regards..
</description><key id="16159105">3260</key><summary>suggest api options enhancement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">rkarakaya</reporter><labels /><created>2013-06-28T20:06:33Z</created><updated>2013-08-05T06:53:17Z</updated><resolved>2013-08-05T06:53:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-28T20:08:25Z" id="20211350">one simple option is to add a `Map` of additional data, and serialize it (both stream and xcontent) when its not null. We support Map with simple values (string, numbers, and map/arrays), which are probably enough in most cases?
</comment><comment author="rkarakaya" created="2013-06-28T20:28:54Z" id="20212442">Hi Shay,

Map will be enough for arbitrary data. 
But adding 'label' property will be very handy to integrate with existing auto-complete components. They usually takes two arguments. One for label presented to the user and the other for programming purposes.

thanks.  
</comment><comment author="spinscale" created="2013-07-23T08:22:32Z" id="21399642">Hey, 

maybe I got your requirement wrong, but you can already customize your options. Take the `TermSuggestion.Option` class as an example

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/suggest/term/TermSuggestion.java#L180-L197

It implements its own `readFrom()`, `writeTo()` and `innerToXContent()` methods.

Is there anything you are missing?
</comment><comment author="rkarakaya" created="2013-08-04T18:46:00Z" id="22076551">Hi Alexander,

You are absolutely right..Thank you. 

my stupidity.. Next time I'll be more carefull reading ES code..

regards..
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update with fields param doesn't return metadata fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3259</link><project id="" key="" /><description>When specifying `fields` in the query string, it should be possible to retrieve the metadata fields, just like in `GET`

```
curl -XPUT 'localhost:9200/test_1?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_parent" : {
            "type" : "foo"
         },
         "_timestamp" : {
            "store" : "yes",
            "enabled" : "1"
         },
         "_ttl" : {
            "store" : "yes",
            "default" : "10s",
            "enabled" : "1"
         }
      }
   }
}
'

curl -XGET 'localhost:9200/_cluster/health?wait_for_status=yellow&amp;pretty=1'

curl -XPOST 'localhost:9200/test_1/test/1/_update?parent=5&amp;fields=_parent%2C_routing%2C_timestamp%2C_ttl&amp;pretty=1'  -d '
{
   "doc" : {
      "foo" : "baz"
   },
   "upsert" : {
      "foo" : "bar"
   }
}
'

# {
#    "ok" : true,
#    "_index" : "test_1",
#    "_id" : "1",
#    "get" : {
#       "exists" : true
#    },
#    "_type" : "test",
#    "_version" : 1
# }
```

Compare with `GET`:

```
curl -XGET 'localhost:9200/test_1/test/1?parent=5&amp;fields=_parent%2C_routing%2C_timestamp%2C_ttl&amp;pretty=1'

# {
#    "fields" : {
#       "_timestamp" : 1372441128082,
#       "_parent" : "5",
#       "_ttl" : 9992,
#       "_routing" : "5"
#    },
#    "_index" : "test_1",
#    "_id" : "1",
#    "_type" : "test",
#    "exists" : true,
#    "_version" : 1
# }
```
</description><key id="16152782">3259</key><summary>Update with fields param doesn't return metadata fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-06-28T17:39:37Z</created><updated>2014-07-03T20:00:22Z</updated><resolved>2014-07-03T20:00:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T20:00:22Z" id="47977648">Closed in favour of #4095
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a score_mode to the rescore</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3258</link><project id="" key="" /><description>I started working on adding a score_mode parameter to the rescore query, something like :

``` json
{
    "rescore" : {
      "window_size" : 50,
      "query" : {
         "rescore_query" : {
            "match" : {
               "field1" : {
                  "query" : "the quick brown",
                  "type" : "phrase",
                  "slop" : 2
               }
            }
         },
         "query_weight" : 0.7,
         "rescore_query_weight" : 1.2,
         "score_mode" : "multiply"
      }
   }
}
```

Default is "total", possible values are : avg, max, min, total and multiply.

My use case is for the "multiply", because I want to do something like:

``` json
{
   "query" : {
      // lot of complex queries
      // default scoring
   },
   "rescore" : {
      "window_size" : 500,
      "query" : {
         "rescore_query" : {
            "custom_score" : {
              "query" : { "match_all" : { } },
              "script" : "complex_scoring",
              "lang" : "native"
            }
          },
         "query_weight" : 1.0,
         "rescore_query_weight" : 1.2
      }
   }
}
```

So rather than having to duplicate my complex query in the rescore query, I just multiply the first score with the second one.

What do you think? I will post my code.
</description><key id="16148966">3258</key><summary>Add a score_mode to the rescore</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">hc</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-28T16:09:27Z</created><updated>2014-01-15T15:19:04Z</updated><resolved>2013-07-26T10:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hc" created="2013-06-28T19:23:13Z" id="20208866">Here my commit: https://github.com/hc/elasticsearch/commit/c37a2b739b9f48281ee698ef53d73e8b292470ba
</comment><comment author="s1monw" created="2013-07-15T08:12:03Z" id="20956420">This looks very useful. I will look into this asap... catching up right now after 3 weeks of absence :)
</comment><comment author="s1monw" created="2013-07-15T12:02:55Z" id="20965196">I added a comment, can you update the PR?
</comment><comment author="s1monw" created="2013-07-20T06:16:05Z" id="21288963">any news on the PR?
</comment><comment author="hc" created="2013-07-20T13:58:12Z" id="21293887">I was also in holidays the past weeks :) I will push it later today.
</comment><comment author="s1monw" created="2013-07-20T17:50:47Z" id="21297465">cool man I will look soon / tomorrow thanks :)
</comment><comment author="s1monw" created="2013-07-20T17:54:59Z" id="21297521">this looks really good. One minor thing, since those functions are idempotent can we only create one instance for them and keep it around and then do `ScoreMode.Min.INSTANCE` instead of `new ScoreMode.Min()`?

PS: I hope you had a good vacation :) 
</comment><comment author="hc" created="2013-07-20T20:20:13Z" id="21299798">You are definitely right, I added a commit: https://github.com/elasticsearch/elasticsearch/pull/3360
I will merge in a single commit once everything is ok.

PS: thanks! You too if that's also the reason of your absence :)
</comment><comment author="kimchy" created="2013-07-21T11:06:21Z" id="21308386">how about making the score functions simply an enum?
</comment><comment author="hc" created="2013-07-21T12:47:05Z" id="21309498">Do you mean something like:

``` java
    private static enum ScoreMode {
        Avg("avg of:"),
        Max("max of:"),
        Min("min of:"),
        Multiply("product of:"),
        Total("sum of:");

        private final String description;

        ScoreMode(String description) {
            this.description = description;
        }
        public float combine(float primary, float secondary) {
            switch (this) {
                case Avg:
                    return (primary + secondary) / 2;
                case Max:
                    return Math.max(primary, secondary);
                case Min:
                    return Math.min(primary, secondary);
                case Multiply:
                    return primary * secondary;
                case Total:
                default:
                    return primary + secondary;
            }
        }
        public String description() {
            return description;
        }
    }
```
</comment><comment author="kimchy" created="2013-07-21T12:58:58Z" id="21309675">the combine method can be abstract on the enum, and each actual enum value can implement it, then you won't need the switch statement.
</comment><comment author="hc" created="2013-07-21T13:39:52Z" id="21310191">Oh thanks I didn't know about that. I pushed the modification.
</comment><comment author="s1monw" created="2013-07-23T18:41:43Z" id="21436409">@hc looks good can you squash the commits?
</comment><comment author="hc" created="2013-07-24T21:54:03Z" id="21519182">I updated the PR
</comment><comment author="s1monw" created="2013-07-26T10:31:47Z" id="21613070">@hc thanks man! would you mind opening an issue on the doc repo to update the features documentation?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The parent option is ignored in delete requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3257</link><project id="" key="" /><description>The `parent` option is ignored in the delete api (rest only) and for delete actions in the bulk api.
This bug occurs in the case that the `_parent` field enabled in a mapping, and only the `parent` option is used. This results in a situation that documents are deleted even if the specified parent value is incorrect. 

In the case that routing is required and no routing is specified for a delete request, then the delete is executed on all shards. The also applies when the `_parent` field is configured on a mapping for a type the delete is executed. The parent id also acts as a routing value.

Current work around is to not use the `parent` option and just use the `routing` option, which has the same effect.

Test case:

``` bash
curl -XDELETE 'localhost:9200/test%2A?pretty=1'

curl -XPUT 'localhost:9200/test_1?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_parent" : {
            "type" : "foo"
         }
      }
   }
}
'

curl -XGET 'localhost:9200/_cluster/health?wait_for_status=yellow&amp;pretty=1'

curl -XPUT 'localhost:9200/test_1/test/1?parent=3&amp;pretty=1'  -d '
{
   "foo" : "bar"
}
'

curl -XDELETE 'localhost:9200/test_1/test/1?parent=2&amp;pretty=1'

# {
#    "ok" : true,
#    "_index" : "test_1",
#    "_id" : "1",
#    "_type" : "test",
#    "found" : true,
#    "_version" : 2
# }

curl -XDELETE 'localhost:9200/test_1/test/1?parent=3&amp;pretty=1'

# {
#    "ok" : true,
#    "_index" : "test_1",
#    "_id" : "1",
#    "_type" : "test",
#    "found" : false,
#    "_version" : 0
# }
```
</description><key id="16139191">3257</key><summary>The parent option is ignored in delete requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-28T12:19:24Z</created><updated>2013-06-28T12:32:12Z</updated><resolved>2013-06-28T12:32:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Updating document with expired _ttl results in "TTL value must be &gt; 0. Illegal value provided" error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3256</link><project id="" key="" /><description>I'm using ES 0.90.1. I've run into a little race condition-ish problem concerning sending an update request for a document after its TTL has expired but before ES has run the TTL cleanup.

Here are the replication steps:

``` shell
% curl -XPUT localhost:9200/test-ttl -d '{
   "settings" : {
     "index.number_of_shards" : 1,
     "index.number_of_replicas" : 0
   },
   "mappings" : {
     "test-doc" : {
       "_ttl" : {
         "enabled" : true,
         "default" : "1m"
       }
     }
   }
 }'
% curl -XPUT 'localhost:9200/test-ttl/test-doc/1' -d '{ "title" : "test title" }'
% curl -XPOST 'localhost:9200/test-ttl/test-doc/1/_update' -d '{ "doc" : { "title" : "test update title" } }'
```

In another window, I have this running:

``` shell
% while true ; do date ; curl -XGET 'http://localhost:9200/test-ttl/test-doc/1?pretty=true&amp;fields=title,_ttl' ; sleep 1 ; done
```

While that's running, I can watch the `_ttl` field decrease. When it becomes negative, the document is in a state such that its TTL has expired, but ES hasn't yet run the cleanup step. I can still retrieve the document, even though technically speaking it's "expired".

When the document is in this state, if I do:

``` shell
% curl -XPOST 'localhost:9200/test-ttl/test-doc/1/_update' -d '{ "doc" : { "title" : "test update title" } }'
```

... I get the following error message:

``` javascript
{"error":"ElasticSearchIllegalArgumentException[TTL value must be &gt; 0. Illegal value provided [-209]]","status":400}
```

I raised this on the elasticsearch users mailing list and Benjamin Deveze suggested I open an issue for it, so here it is!
</description><key id="16113933">3256</key><summary>Updating document with expired _ttl results in "TTL value must be &gt; 0. Illegal value provided" error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">bradcavanagh</reporter><labels /><created>2013-06-27T20:59:45Z</created><updated>2014-12-01T10:02:29Z</updated><resolved>2014-12-01T10:02:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-06-27T21:09:32Z" id="20155133">I propose to fix this one if nobody in the ES team is working on it.

As I said on the ML I propose that an update on an expired but not already expunged document should behave like an update on a deleted document so in this particular case it should probably throws a DocumentMissingException.

Maybe the behavior should be the same in the get and mget APIs for the sake of consistency?

But maybe this is problematic if a to be expunged document is considered missing in the update get and mget APIs but can be accessible through the search APIs before being expunged?

Any thoughts on this?
</comment><comment author="bradcavanagh" created="2013-06-27T21:39:39Z" id="20156832">Keep in mind the solution you come up with needs to work for upserts as well. For an upsert, I would expect the document to be "recreated" with only the data that was supplied in the upsert request. That's how I originally found the problem; my Java client does

``` java
client.prepareUpdate(index, doctype, key).setDoc(json).setUpsertRequest(json).setRefresh(false)
                        .setRetryOnConflict(retry).execute().actionGet();
```

And this throws the `ElasticSearchIllegalArgumentException`, even though the `_ttl` field isn't included in the supplied JSON document.
</comment><comment author="Paikan" created="2013-06-27T21:42:00Z" id="20156968">@bradcavanagh yes of course it should behave as if the document has been deleted on every aspect of what is exposed by the update API
</comment><comment author="Kaidence" created="2014-01-28T19:53:46Z" id="33517231">Hey guys, I ran into the same issue as @bradcavanagh. Is someone taking a look at this? I completely agree that when upserting the old document should be deleted.
</comment><comment author="clintongormley" created="2014-10-17T08:57:00Z" id="59485672">Hi all

So expired documents are cleaned up in a bulk request run once every minute, at which time their ttl can be less than zero.  The question is, in this update case, what is the right thing to do?  Reset the TTL?Pretend that the document isn't there?  That would be inconsistent with GET and search.

You have access to the TTL and timestamp in a scripted update request.  I think the only viable solution here is to write the script to reflect the logic that you want implemented.  If the TTL is low (ie the document will be cleared up soon), then do $x, whatever $x is for your use case.
</comment><comment author="bradcavanagh" created="2014-10-17T16:30:44Z" id="59539055">In this update case the right thing to do is behave as if the document has been deleted because it's the only consistent answer. If you reset the TTL, then you can end up with documents being stored in ES in an arbitrary and non-deterministic manner. If I have supplied a TTL of 60 seconds, I am expecting ES to expire documents after 60 seconds, not (potentially) 119 seconds. If I try to update a document that I know was inserted into ES 90 seconds ago, then this update should always fail.

As of 0.90.1 this was the case, but the exception thrown was kind of misleading (ElasticSearchIllegalArgumentException shouldn't be the exception thrown, it should be some manner of DocumentMissingException).

Realistically GETs and searches shouldn't return records that have been expired either unless you explicitly ask for them (some kind of "ignore TTL" flag in the query maybe) because, well, they're expired! Unless I ask for them I shouldn't have to do client-side filtering because I've already explicitly told ES to expire documents, and I shouldn't have to remember that I'll actually get back more documents than I should.
</comment><comment author="clintongormley" created="2014-10-17T17:43:56Z" id="59548962">&gt; Realistically GETs and searches shouldn't return records that have been expired either unless you explicitly ask for them (some kind of "ignore TTL" flag in the query maybe) because, well, they're expired! Unless I ask for them I shouldn't have to do client-side filtering because I've already explicitly told ES to expire documents, and I shouldn't have to remember that I'll actually get back more documents than I should.

In the ideal world, yes.  However, doing this "just in case" for every query and GET adds significant overhead, eg we have to run a new range filter once every second, on every single query!  

This has never been the contract offered by `_ttl`. If a user wants this functionality, then they should implement it application side.  At least that way, they will understand the cost of what they are doing. Better that than impacting performance across the board.
</comment><comment author="jknewman3" created="2014-10-30T00:03:47Z" id="61026306">We've apparently run into this same problem with upserts. In the upsert we are also setting a new non-negative ttl. Rather than throwing the negative ttl error, couldn't elasticsearch just perform the upsert with the new ttl value and resurrect the document.
</comment><comment author="clintongormley" created="2014-10-30T10:33:23Z" id="61072054">@jknewman3 what version of Elasticsearch are you using, and what exactly does your request look like?
</comment><comment author="jknewman3" created="2014-10-30T20:44:12Z" id="61166269">We're using 1.2.1. I've requested the code from the developer and will include when I get it.
</comment><comment author="jknewman3" created="2014-10-31T17:39:20Z" id="61298489">Here's the code:

``` java
for(Map rec : v.upserts ){
                    bulkItem = conn.prepareIndex(idx,map,rec.id).setSource(rec.src)
                    if(rec.ttl) bulkItem.setTTL(rec.ttl)
                    bulkItem = conn.prepareUpdate(idx,map,rec.id).setDoc(rec.src).setUpsert(bulkItem.request())
                    if(rec.routing) bulkItem.setRouting(rec.routing)
                    bulkRequest.add(bulkItem)
                    records ++
                }
```

index {[apd-v2][accountRelDevice][5492e7ee-10a3-9f42-a458-a9b25b5b0af5], source[{"acctCode":"1001134378145","deviceId":282539788086975078,"subscriberId":586700,"txnTime":"2014-10-31T17:29:20.397Z"}]}

![image](https://cloud.githubusercontent.com/assets/6268028/4865569/d36eeeb8-6124-11e4-9402-3f74d4b95ef0.png)
</comment><comment author="jknewman3" created="2014-10-31T17:46:27Z" id="61299559">It's worth noting that I have a perl version that has not been encountering this issue:

``` perl
                        $bulk-&gt;update({
                            id            =&gt; $ard_uuid,
                            _ttl          =&gt; $ttl_ms,
                            _routing      =&gt; $deviceId,
                            doc           =&gt; {
                                subscriberId     =&gt; $subscriberId,
                                acctCode         =&gt; $acctCode,
                                deviceId         =&gt; $deviceId,
                                txnTime          =&gt; $txnTime
                            },
                            doc_as_upsert =&gt; "true"
                        });
```

Unfortunately it's the java that needs to work in prod here.
</comment><comment author="clintongormley" created="2014-11-29T15:43:11Z" id="64955580">@dadoonet Please could you take a look at the Java code in https://github.com/elasticsearch/elasticsearch/issues/3256#issuecomment-61298489
</comment><comment author="dadoonet" created="2014-11-29T20:00:55Z" id="64963409">@clintongormley I can reproduce the issue.

When an upsert request contains a `_ttl`, the ttl is ignored.

Full reproduction:

``` java
@Test
public void upsertWithTtlTest() throws IOException, InterruptedException {

    ImmutableSettings.Builder settings = ImmutableSettings.builder()
            .put("indices.ttl.interval", "200ms")
            .put("indices.ttl.bulk_size", 1);
    Client client = nodeBuilder().local(true).settings(settings).node().client();

    // Remove index
    try {
        client.admin().indices().prepareDelete("ttl").get();
    } catch (IndexMissingException e) {
    }

    // Create a mapping with ttl
    client.admin().indices().prepareCreate("ttl")
            .addMapping("doc", jsonBuilder().startObject().startObject("doc")
                    .startObject("_ttl").field("enabled", true).endObject()
                    .endObject().endObject())
            .get();

    IndexRequest indexRequest = new IndexRequest("ttl", "doc", "1")
            .source(jsonBuilder().startObject().field("foo", "bar").endObject())
            .ttl(1000L);

    // We just index one document with ttl = 1s
    BulkRequestBuilder bulk = client.prepareBulk();
    bulk.add(indexRequest);
    bulk.setRefresh(true).get();

    SearchResponse response = client.prepareSearch("ttl").get();
    System.out.println("Just after insert, we have " + response.getHits().getTotalHits() + " hit");

    Thread.sleep(2000L);

    response = client.prepareSearch("ttl").get();
    System.out.println("2s after insert, we have " + response.getHits().getTotalHits() + " hit");

    // We create a new bulk
    bulk = client.prepareBulk();
    indexRequest = new IndexRequest("ttl", "doc", "2")
            .source(jsonBuilder().startObject().field("foo", "bar").endObject())
            .ttl(1000L);
    UpdateRequest updateRequest = new UpdateRequest("ttl", "doc", "2")
            .doc(jsonBuilder().startObject().field("fooz", "baz").endObject())
            .upsert(indexRequest);
    bulk.add(updateRequest);
    bulk.setRefresh(true).get();

    response = client.prepareSearch("ttl").get();
    System.out.println("Just after upsert, we have " + response.getHits().getTotalHits() + " hit");

    Thread.sleep(2000L);

    response = client.prepareSearch("ttl").get();
    System.out.println("2s after upsert, we have " + response.getHits().getTotalHits() + " hit");
}
```

It gives:

```
Just after insert, we have 1 hit
2s after insert, we have 0 hit
Just after upsert, we have 1 hit
2s after upsert, we have 1 hit
```
</comment><comment author="dadoonet" created="2014-11-30T18:12:43Z" id="64993847">@clintongormley I opened #8715 to fix the ttl problem with upsert.
</comment><comment author="clintongormley" created="2014-12-01T10:02:29Z" id="65042675">thanks @dadoonet, then I'll close this issue in favour of #8715
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update maven shade plugin to latest version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3255</link><project id="" key="" /><description /><key id="16098649">3255</key><summary>Update maven shade plugin to latest version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splatch</reporter><labels /><created>2013-06-27T16:09:27Z</created><updated>2014-07-11T11:46:33Z</updated><resolved>2013-07-04T13:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch uses outdated version of maven-shade-plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3254</link><project id="" key="" /><description>Starting from version 1.6 maven-shade-plugin [MSHADE-105](http://jira.codehaus.org/browse/MSHADE-105) do not share constant pool for relocated classes. Currently, even if elasticsearch distro have relocated package org.elasticsearch.common.jackson constant pool points to com.fasterxml.jackson constant pool in bytecode.

Update of this would be great.
</description><key id="16098076">3254</key><summary>Elasticsearch uses outdated version of maven-shade-plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splatch</reporter><labels /><created>2013-06-27T16:03:26Z</created><updated>2013-07-04T13:58:15Z</updated><resolved>2013-07-04T13:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NPE in PluginManager when asking for list on non existing dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3253</link><project id="" key="" /><description>Asking for list of installed plugins with no existing plugin dir:

``` sh
$ bin/plugin --list
```

It causes a NPE in PluginManager.
</description><key id="16089394">3253</key><summary>NPE in PluginManager when asking for list on non existing dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.3</label></labels><created>2013-06-27T14:11:12Z</created><updated>2013-06-27T14:13:21Z</updated><resolved>2013-06-27T14:13:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Error on MoreLikeThis API with Non Stored Numeric Fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3252</link><project id="" key="" /><description>According to the documentation:

Note: In order to use the mlt feature a mlt_field needs to be either be stored, store term_vector or source needs to be enabled.

But,running this:

```
curl -XPOST http://localhost:9200/foo
curl -XPUT http://localhost:9200/foo/bar/_mapping -d '{ "bar": { "dynamic": "strict", "properties": { "id": { "type": "integer", "index": "not_analyzed" }, "content": { "type": "string", "analyzer": "standard" }}}}'

curl -XPUT http://localhost:9200/foo/bar/1 -d '{"id":1, "content":"foo bar foo2 bar2 foo3 bar3"}'
curl -XPUT http://localhost:9200/foo/bar/2 -d '{"id":2, "content":"foo3 bar3 foo4 bar4"}'


curl -XGET 'http://localhost:9200/foo/bar/1/_mlt?mlt_fields=content&amp;min_term_freq=1&amp;min_doc_freq=1'
curl -XGET 'http://localhost:9200/foo/bar/1/_mlt?min_term_freq=1&amp;min_doc_freq=1'
```

fails(second query) with:
{"error":"MapperParsingException[failed to parse [id]]; nested: ElasticSearchIllegalStateException[Field should have either a string, numeric or binary value]; ","status":400}

This is basically because here(for example):
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java#L356-L360

The numeric value is not actually used unless the field is stored.

Then here:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java#L293-L303

if you can't read it, it will just thrown an exception.
</description><key id="16077367">3252</key><summary>Error on MoreLikeThis API with Non Stored Numeric Fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-27T09:39:05Z</created><updated>2013-08-28T18:57:38Z</updated><resolved>2013-07-16T16:38:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-07-02T08:18:22Z" id="20332611">any comments on that? I could try going around and store the fields(even though its not the best scenario...), but would be nice also having that without the need to reindex...
</comment><comment author="clintongormley" created="2013-07-02T09:33:11Z" id="20335733">@lmenezes @jpountz says that he will have a look at it
</comment><comment author="lmenezes" created="2013-07-02T09:41:36Z" id="20336119">@clintongormley cool :)
</comment><comment author="jpountz" created="2013-07-03T09:57:21Z" id="20405814">@lmenezes You are right about why you got this error but unfortunately setting the value of the field instance even when the field is not stored won't work. The reason is that Lucene's MoreLikeThis can only work on top on character token streams and numeric fields are encoded as binary token streams.

This issue is very similar to #3211, where we decided to ignore numeric fields when performing highlighting in order to match Elasticsearch 0.20 behavior. Maybe we should do the same here? @clintongormley what do you think?
</comment><comment author="clintongormley" created="2013-07-03T10:02:22Z" id="20406049">My feeling is that the `more_like_this` functionality is about finding string terms in common, rather than numeric similarity, so I would agree with you on ignoring non-string fields.  Numeric similarity implies a different type of comparison, which would be usually be better handled by a specific clause outside the `mlt` query.

If you want to treat numbers as "full text" then you can always use a `multi_field` to index them both as numbers and as strings.

So ++ for ignoring non-strings, I'd say.
</comment><comment author="lmenezes" created="2013-07-03T10:23:15Z" id="20406916">@jpountz @clintongormley I don't really agree, since if the numbers are ids for some kind of relation, they represent similarity as well or even better than matching tokens. But, if it's a lucene limitation, ignoring is definitely better than failing. Still, would be nice having that working on numeric fields(I guess that affects everything that internally is stored as a number, like ips?).
But yeah, ignoring is ok.
</comment><comment author="jpountz" created="2013-07-03T13:37:42Z" id="20415565">@lmenezes This is correct, the limitation is in Lucene and this affects everything which is stored as a number, so byte, short, integer, long, float and double but also ips and dates. There might be options to support numbers in the future but right now I think the best fix to apply is to ignore numeric data from the mlt fields.
</comment><comment author="lmenezes" created="2013-07-03T13:46:17Z" id="20416070">@jpountz cool, waiting for the fix then :)
</comment><comment author="jpountz" created="2013-07-15T18:04:39Z" id="20989599">The mlt API uses the mlt query, so I updated to pull request:
- the mlt API doesn't fail even if one of the fields of the document is numeric,
- mlt and flt queries fail if any of the fields is numeric,
- the new fail_on_unsupported_field parameter (defaults to true) allows for ignoring numeric fields instead of raising an error when set to false.
</comment><comment author="lmenezes" created="2013-07-16T08:01:14Z" id="21026870">sounds good :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make index.warmer.enabled setting dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3251</link><project id="" key="" /><description>Even though proposed in the documentation, the realtime enabling/disabling of
index warmers was not supported. This commit adds support for
index.warmer.enabled as a dynamic setting.

Closes #3246
</description><key id="16075637">3251</key><summary>Make index.warmer.enabled setting dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-06-27T08:50:51Z</created><updated>2014-07-01T17:37:22Z</updated><resolved>2013-06-28T09:42:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-27T18:01:06Z" id="20143332">+1 Looks good, lets get this in.
</comment><comment author="spinscale" created="2013-06-28T09:42:19Z" id="20179091">Closed in master with https://github.com/elasticsearch/elasticsearch/commit/71d5148b1c454cd4749a90e8b4d6224e90e0dc39 and in 0.90 with https://github.com/elasticsearch/elasticsearch/commit/44d881b593e0166b1f4c4fbdc261fb8bc1f234bf
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dont execute suggest before parsing the full request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3250</link><project id="" key="" /><description>The current implementation of parsing suggestions executed inside of the
the pull parser - which resulted in being reliable of the order of the
elements in the request. This fix changes the behaviour to parse the
relevant parts of the request first and then execute all the suggestions
afterwards, so we can be sure that every information has been extracted
from the request before execution.

I did not create a test, as I believe this can only be tested against the REST interface (where we do not have any tests at the moment)

Closes #3247
</description><key id="16073832">3250</key><summary>Dont execute suggest before parsing the full request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-06-27T08:02:27Z</created><updated>2014-07-03T21:55:54Z</updated><resolved>2013-06-28T10:54:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-27T18:39:52Z" id="20145988">+1 Looks good!
</comment><comment author="spinscale" created="2013-06-28T10:54:18Z" id="20182001">Landed in master https://github.com/elasticsearch/elasticsearch/commit/0a50ed0a272e13d78f8329b2b1e469cab1fc6b82 and in 0.90 https://github.com/elasticsearch/elasticsearch/commit/bf435234e3a3dccd852f329ec018f8f3bd25863f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgradera till elasticsearch-0.90.2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3249</link><project id="" key="" /><description>PLZ DELETE
</description><key id="16073673">3249</key><summary>Upgradera till elasticsearch-0.90.2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kristiany</reporter><labels /><created>2013-06-27T07:57:37Z</created><updated>2013-06-27T07:59:42Z</updated><resolved>2013-06-27T07:59:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kristiany" created="2013-06-27T07:59:28Z" id="20102732">Sry, wrong repo!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ignore live docs when loading field data.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3248</link><project id="" key="" /><description>Relying on deleted documents when loading field data is dangerous because a
field data instance might be loaded for a given generation of a segment and
then loaded from the cache by an older generation of the same segment which
has fewer deleted documents. This could, for example, lead to under-estimated
facet counts.
</description><key id="16057157">3248</key><summary>Ignore live docs when loading field data.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-06-26T21:26:42Z</created><updated>2014-06-13T10:01:07Z</updated><resolved>2013-06-29T10:14:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Invalid "ElasticSearchIllegalArgumentException[The required text option is missing]" Error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3247</link><project id="" key="" /><description>If I precisely follow the docs &amp; send the following, I get a successful request (200).

```
curl -XPOST 'localhost:9200/_search' -d '{
  "query": {
    "query_string": {
        "query": "hello"
    }
  },
  "suggest": {
    "suggest": {
      "text": "*:*",
      "term": {
        "field": "_all"
      }
    }
  }
}'
```

However, if I just change the ordering of the `text` &amp; `term` keys, I get the "Required text option is missing error" (&amp; a resulting HTTP 500), even though it is present in the JSON. AFAICT, the ordering within the JSON object shouldn't matter.

```
curl -XPOST 'localhost:9200/_search' -d '{
  "query": {
    "query_string": {
        "query": "hello"
    }
  },
  "suggest": {
    "suggest": {
      "term": {
        "field": "_all"
      },
      "text": "*:*"
    }
  }
}'
```
</description><key id="16053976">3247</key><summary>Invalid "ElasticSearchIllegalArgumentException[The required text option is missing]" Error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">toastdriven</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-26T20:23:55Z</created><updated>2013-06-28T10:17:43Z</updated><resolved>2013-06-28T10:17:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index Warmer Setting is not dynamic anymore on 0.90.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3246</link><project id="" key="" /><description>According to the documentation; Index warmup can be disabled by setting index.warmer.enabled to false. It is supported as a realtime setting using update settings API.

When I try to set it back to true;

```
{"index.warmer.enabled":"true"}
```

I get to following exception;

```
{"error":"RemoteTransportException[[xxx][inet[/xx.x.xx.xx:9300]][indices/settings/update]]; nested: ElasticSearchIllegalArgumentException[Can't update non dynamic settings[[index.warmer.enabled]] for open indices[[public_20120701]]]; ","status":400}
```
</description><key id="16046201">3246</key><summary>Index Warmer Setting is not dynamic anymore on 0.90.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">jgagnon1</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-26T17:42:47Z</created><updated>2013-06-28T08:39:42Z</updated><resolved>2013-06-28T08:39:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PluginManager fails with unknown command when passing url or verbose parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3245</link><project id="" key="" /><description>To reproduce try:

&gt; bin/plugin -v

[...]

Message:
   Command [-v] unknown.

The problem is that in the main function the commands are cycled twice; the first time to check for url and verbose, the second time to check for the others. If the second time a command that is not install, remove, list or help is found, the application exits (but url and verbose were never filtered out from the list of commands).
</description><key id="16037860">3245</key><summary>PluginManager fails with unknown command when passing url or verbose parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">sunriis</reporter><labels><label>bug</label><label>v0.90.3</label></labels><created>2013-06-26T15:08:23Z</created><updated>2014-09-23T16:00:44Z</updated><resolved>2013-06-26T16:50:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-06-26T15:19:12Z" id="20054837">Thanks for catching it! Working on a fix.
</comment><comment author="vpernin" created="2013-07-10T16:00:10Z" id="20752664">Are you going to release a 0.90.2.1 or 0.90.3 quickly with this fix ?
</comment><comment author="dadoonet" created="2013-07-10T16:48:06Z" id="20755994">You can install plugin manually by downloading and unzipping it in `./plugins/yourpluginname` dir.
So even it's annoying, it's not a blocking issue and we don't need to release a new version just for that problem.
</comment><comment author="vpernin" created="2013-07-10T16:56:11Z" id="20756570">I know, I know ...
</comment><comment author="missinglink" created="2013-07-10T17:30:09Z" id="20758859">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove artifact shading from ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3244</link><project id="" key="" /><description>Artifact shading causes lots of troubles for these who carefully manage dependencies. We have troubles with previous versions of elasticsearch under OSGi environment.
</description><key id="16036140">3244</key><summary>Remove artifact shading from ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">splatch</reporter><labels /><created>2013-06-26T14:39:58Z</created><updated>2014-06-25T21:21:12Z</updated><resolved>2013-06-27T14:59:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-06-27T14:59:03Z" id="20125452">The shading is important to keep our dependencies (notably netty, lucene, guava) close to our code so that we can fix an issue even if the upstream provider lags behind.  It's possible we will distributed modularized versions of the code, which would help with your particular issue (#2091 for example), but we can't simply remove the shaded dependencies at this time.  You can build a local version of ES for your purposes until there's a better solution.
</comment><comment author="splatch" created="2013-06-27T16:18:28Z" id="20135112">Well, shading is always devil sign that something goes wrong - your project or its dependencies.. If you have to do that without publishing own versions of artifacts, than please at least [update maven shade plugin](https://github.com/elasticsearch/elasticsearch/pull/3255) to version which do full relocation and do not keep _any_ references to source packages.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix ShapeFetchService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3243</link><project id="" key="" /><description>Fix parsing the field names in the `ShapeFetchService` 

closes #3242
</description><key id="16024435">3243</key><summary>Fix ShapeFetchService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-26T10:21:05Z</created><updated>2014-06-13T10:07:19Z</updated><resolved>2013-06-26T11:15:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geoshape filter can't handle multiple shapes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3242</link><project id="" key="" /><description>The `geo_shape` filter seems to be unable to handle multiple `geo_shape` fields in a single document if this document is used as indexed filter.

Assume a mapping with multiple `geo_shape` fields:

```
{
    "type1" : {
        "properties" : {
            "location1" : {
                 "type" : "geo_shape"
            },
            "location2" : {
                "type" : "geo_shape"
            }
        }
    }
}
```

and a document

```
{
    "location1" : {
        "type":"polygon",
        "coordinates":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]
    },
    "location2" : {
        "type":"polygon",
        "coordinates":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]
    }
}
```

If a `geo_shape` filter is applied to the `location2` field

```
{
    "geo_shape": {
        "location2": {
            "indexed_shape": { 
                "id": "1",
                "type": "type1",
                "index": "test",
                "shape_field_name": "location2"
            }
        }
    }
}
```

parsing fails with

```
ElasticSearchIllegalStateException[Shape with name [1] found but missing location2 field];
```
</description><key id="16024086">3242</key><summary>Geoshape filter can't handle multiple shapes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>bug</label><label>v0.90.4</label><label>v1.0.0.Beta1</label></labels><created>2013-06-26T10:10:52Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-09-07T20:16:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="alexeiemam" created="2013-09-07T19:25:30Z" id="24007940">Is this fix included in the 0.90.3 release?
I am currently experiencing the same error when both a geo_shape and a geo_point field are present in the same document.

**Edit: using 0.90.3**
</comment><comment author="s1monw" created="2013-09-07T19:42:33Z" id="24008233">this has never been backported... I will backport!
</comment><comment author="s1monw" created="2013-09-07T20:16:56Z" id="24008833">pushed to `0.90` branch. This will be part of `0.90.4`
</comment><comment author="alexeiemam" created="2013-09-08T00:04:22Z" id="24012292">When is the, approximate, anticipated release of 0.90.4?
</comment><comment author="s1monw" created="2013-09-08T05:52:21Z" id="24015630">&gt; When is the, approximate, anticipated release of 0.90.4?

we plan on a release early next week  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms Filter Lookup: Allow to disable caching of lookup terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3241</link><project id="" key="" /><description>Allow to disable per execution of terms filter the caching of the lookup terms (which still retaining control over caching the filter results).

The paramater is named `cache` and is at the same level as the lookup parameters (index, type, path).

Relates to #3236 
</description><key id="16018989">3241</key><summary>Terms Filter Lookup: Allow to disable caching of lookup terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-26T07:44:58Z</created><updated>2013-06-26T07:46:04Z</updated><resolved>2013-06-26T07:46:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms Filter Lookup: When on cache key defined, use terms values as key to filter cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3240</link><project id="" key="" /><description>When no cache key is defined, use the terms values as keys to filter cache (bitsets), compared to using the same key we use to cache the lookup document terms. 

Relates to #3236
</description><key id="16018672">3240</key><summary>Terms Filter Lookup: When on cache key defined, use terms values as key to filter cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-26T07:33:27Z</created><updated>2013-06-26T07:34:29Z</updated><resolved>2013-06-26T07:34:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>plugin version in admin api / feature request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3239</link><project id="" key="" /><description>when running _nodes?plugin=true it would be great to be able to get the version of the installed plugins
</description><key id="16005228">3239</key><summary>plugin version in admin api / feature request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">locojay</reporter><labels /><created>2013-06-25T22:23:32Z</created><updated>2013-09-14T06:47:44Z</updated><resolved>2013-09-14T06:47:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-06-25T23:01:54Z" id="20014413">Right. I first thought about it (see commit https://github.com/dadoonet/elasticsearch/commit/591145766ba6903a42beb6523071a7791c66af20 ) but it introduced a breakable backward compatibility. That's the reason we did not merge it.
I'm not sure if we want to push it to master (1.0 version). cc @kimchy 
</comment><comment author="dadoonet" created="2013-09-14T06:47:44Z" id="24438167">Closing this one as duplicate of #2784 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix field number attribution to _version.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3238</link><project id="" key="" /><description>IndexUpgraderMergePolicy assumed that field numbers were dense and that
fieldInfos.size() was a free field number. This can however be wrong for a
segment which doesn't have one or more fields that some older segments have.
</description><key id="15989344">3238</key><summary>Fix field number attribution to _version.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-06-25T17:01:31Z</created><updated>2014-07-16T21:53:02Z</updated><resolved>2013-06-26T20:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>IndexUpgraderMergePolicy doesn't assign a field number to _version correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3237</link><project id="" key="" /><description>Martijn found a bug in IndexUpgraderMergePolicy which attributes fieldInfos.size() as a field number although this field number might already be taken in the current segment.
</description><key id="15985688">3237</key><summary>IndexUpgraderMergePolicy doesn't assign a field number to _version correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v1.0.0.Beta1</label></labels><created>2013-06-25T15:52:16Z</created><updated>2013-06-26T15:07:25Z</updated><resolved>2013-06-26T15:07:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make caching of TermsLookup and normal Filter Caching Independent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3236</link><project id="" key="" /><description>Following up on https://github.com/elasticsearch/elasticsearch/issues/3219,
it would be nice controlling if the Terms Lookup should be cached or not, and also making the caching key of the resulting bitset based on the content of the lookup rather than its definition(or on a given key). 
Currently you can only control if the resulting terms filter is cached or not. 

I imagine the situation where this could be useful:

Having a constantly changing index which is used as the source for a lookup. In this case, caching the lookup will lead to inconsistent results when the index changes(of course you can invalidate the cache entry, but that is just extra work for a possibly common use case).

So, here I would like to set something like _cache_lookup = false. This way, lets say that the lookup is not cached... But then, since the bit set cache key is based on the definition of the lookup(idx+type+path+id), it wont even get to execute the lookup... And of course, if the lookup result changed, you will still get an inconsistent result. 

You could set _cache to false, and then this should work, but then you are always repeating the transformation from the result of the lookup to the resulting bit set, which is the most expensive part(on my tests, for terms lookup of 35K-60K terms, the lookup took on average less than 40ms. the actual execution of the filter, was around 700ms-1s).

Finally, if the bitset cache key was based on the result of the lookup, you can not cache the lookup and still benefit of the global filter cache, even though you would have to execute the lookup(but, i guess the time spent on this phase is irrelevant comparing the benefit of having the global filter).

On the case where you want to cache the lookup, you can still use the lookup definition as the cache key for the global filter.

Sounds reasonable? And sorry if its not that clear the explanation, still the best I could get.
</description><key id="15977435">3236</key><summary>Make caching of TermsLookup and normal Filter Caching Independent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-06-25T13:20:53Z</created><updated>2013-06-26T14:50:13Z</updated><resolved>2013-06-26T14:50:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-06-26T14:50:13Z" id="20052686">looks good now!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String value won't search on specific term</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3235</link><project id="" key="" /><description>I have a lot of string values in my index containing `yes`, or `no`, and I use these for filters to filter out results based on parameters passed in by a querystring.

I am finding that when a filter is set to `yes` it correctly filters those records that have a corresponding value, but when set to `no` it returns no results at all.

I have tried this with `no`, `not` and a `null` value, and none return results.

I have tried this via the rest api, and also in the head plugin directly, and it is the same result each time.

Either this is a bug, or I am doing something that isn't allowed by elasticsearch.
</description><key id="15973227">3235</key><summary>String value won't search on specific term</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">designermonkey</reporter><labels /><created>2013-06-25T11:22:54Z</created><updated>2013-06-25T22:29:57Z</updated><resolved>2013-06-25T22:29:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-06-25T15:40:46Z" id="19985384">Please ask the mailing list before raising an issue: http://www.elasticsearch.org/help/
Also, provide a full curl recreation as we can reproduce it.

My bet here is that you are using default analyzer which remove stopwords or may be elasticsearch recognize values as boolean? You can try to set index: not_analyzed for this field and use a termFilter to filter your query.

Let's discuss about it on the mailing list. If it's not an issue but a question, you should close it.
</comment><comment author="designermonkey" created="2013-06-25T22:29:57Z" id="20012447">Sorry if its not the right place, thanks for the pointer. I think it may be stop word related. I've altered the index value to circumvent it, and will read more about stop words.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a "routing" parameter to the terms lookup filter. This resolves issue 3233</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3234</link><project id="" key="" /><description>https://github.com/elasticsearch/elasticsearch/issues/3233

This allows a custom routing parameter for the document lookup when using the "terms lookup" filter. Previously, only the document_id could be used.
</description><key id="15954831">3234</key><summary>Add a "routing" parameter to the terms lookup filter. This resolves issue 3233</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daveey</reporter><labels /><created>2013-06-25T00:00:06Z</created><updated>2014-07-16T21:53:03Z</updated><resolved>2013-06-26T02:01:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-25T09:44:35Z" id="19963714">agreed, we need to add it, there are a few things missing in the pull request, and the builder is not there. Also, teh handling if fieldMapper might be null was already fixed differently in the code.

I really want to push 0.90.2 quickly (possibly today), so I will work on the fix, apologies for not working with you properly on this one to get this in.
</comment><comment author="daveey" created="2013-06-25T17:24:55Z" id="19992633">No problem! I am just excited for this to be fixed. &lt;3

On Tue, Jun 25, 2013 at 2:45 AM, Shay Banon notifications@github.comwrote:

&gt; agreed, we need to add it, there are a few things missing in the pull
&gt; request, and the builder is not there. Also, teh handling if fieldMapper
&gt; might be null was already fixed differently in the code.
&gt; 
&gt; I really want to push 0.90.2 quickly (possibly today), so I will work on
&gt; the fix, apologies for not working with you properly on this one to get
&gt; this in.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/3234#issuecomment-19963714
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lookup Terms Filter ignores the routing parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3233</link><project id="" key="" /><description>https://gist.github.com/anonymous/5854123

I believe that the looked up document uses the "id" for routing instead of the "routing" parameter. I can see wanting an explicit "_routing" field in the lookup filter.
</description><key id="15951864">3233</key><summary>Lookup Terms Filter ignores the routing parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">daveey</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T22:49:28Z</created><updated>2013-06-25T09:54:29Z</updated><resolved>2013-06-25T09:54:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Deleting or closing an index doesn't clean the memory properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3232</link><project id="" key="" /><description>Related to #3130, just with a dedicated issue of the problem of not properly cleaning leftover memory after an index is deleted or closed.

The problem relates to thread locals not properly handling self references, and the fact that in general they take time to clean up.
</description><key id="15951523">3232</key><summary>Deleting or closing an index doesn't clean the memory properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T22:40:03Z</created><updated>2013-06-24T22:44:14Z</updated><resolved>2013-06-24T22:44:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add Arabic/PersianNormalizationFilters from Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3231</link><project id="" key="" /><description>Tracking pull request https://github.com/elasticsearch/elasticsearch/pull/3227 with this bug report (for changelog).

Commits
Master: https://github.com/elasticsearch/elasticsearch/commit/c561b1bbcfd8ae016921a0277a40d1aad41b47c5
0.90: https://github.com/elasticsearch/elasticsearch/commit/3aaf31159bba25bb4d7eab02a17409145a2d1115
</description><key id="15949185">3231</key><summary>Add Arabic/PersianNormalizationFilters from Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T21:41:08Z</created><updated>2013-06-24T21:41:13Z</updated><resolved>2013-06-24T21:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geohash filter enhancement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3230</link><project id="" key="" /><description>The `geohash_cell` filter now adapts the format of other geo-filters. The oject fieldnames match the fieldnames document names automatically. This invalidates the `field` field in previeous versions. The value these fields value is a `geo_point` value (all formats supported) which is internally translated to a geohash. Since those points alway have a maximum precision (level 12) a `precision` definition has been included. This precision can either be defined as _length_ of the geohash-string or as _distance_. It's assumed the a distance without any unit is a geohash-length.

```
GET 'http://127.0.0.1:9200/locations/_search?pretty=true' -d '{
    "query": {
        "match_all":{}
    },
    "filter": {
        "geohash_cell": {
            "pin": {
                "lat": 13.4080,
                "lon": 52.5186
            },
            "precision": 3,
            "neighbors": true
        }
    }
}'
```

Closes #3229
</description><key id="15949130">3230</key><summary>Geohash filter enhancement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-24T21:39:45Z</created><updated>2014-07-04T12:40:46Z</updated><resolved>2013-06-25T11:22:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geohash filter format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3229</link><project id="" key="" /><description>The format of the geohash filter does not match the format other filters in the REST API. The fieldnames within the filter should match the fieldnames within the mapping.
Also the  it should be possible to define the geohash_cell by a point in lat/lon format. Such a point can than transformed into a geohash.
In some corner cases the geohash_cell filter fails. i.e. the southern neighbor of cell `12b`.
</description><key id="15948240">3229</key><summary>Geohash filter format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>bug</label><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T21:20:28Z</created><updated>2013-06-25T11:22:29Z</updated><resolved>2013-06-25T11:22:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Reproducible UnavailableShardsException on type-creation via POST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3228</link><project id="" key="" /><description>I'm trying to put together some tests on a module I've been writing with ElasticSearch and as a result I've been trying to create and delete types and indices in reliable reproducible ways. I've run into an error though that seems to make it very difficult to truly decouple index settings and type mappings. Process for reproducing bug is as follows:

Startup ES server
PUT request to http://localhost:9200/test-index w/ data: 
{
    "settings": {
        "number_of_shards": 10,
        "number_of_replicas": 5
    }
}
response:
{
   "ok": true,
   "acknowledged": true
}
POST request to http://localhost:9200/test-index/test-type w/ data:
`{
    "mappings": {
        "properties": {
            "test-string": {
                "type": "string",
                "store": "yes",
                "index": "analyzed",
                "term_vector": "with_positions_offsets"
            },

```
        "test-float": {
            "type": "float",
            "store": "yes"
        }
    }
}
```

}`

Response:
{
   "error": "UnavailableShardsException[[test-index][3] [6] shardIt, [1] active : Timeout waiting for [1m], request: index {[test-index][test-type][S_RZkZekR2iiFBnhAhVSpQ], source[{\n    \"mappings\": {\n\t\t\"properties\": {\n\t\t\t\"test-string\": {\n\t\t\t\t\"type\": \"string\",\n\t\t\t\t\"store\": \"yes\",\n\t\t\t\t\"index\": \"analyzed\",\n\t\t\t\t\"term_vector\": \"with_positions_offsets\"\n\t\t\t},\n\n\t\t\t\"test-float\": {\n\t\t\t\t\"type\": \"float\",\n\t\t\t\t\"store\": \"yes\"\n\t\t\t}\n\t\t}\n\t}\n}]}]",
   "status": 503
}

I know I could simple hard-code my mapping and settings into the same file with type names and PUT that to the index, but I feel like that makes for a significantly less clean implementation. Do I have to close the index down, then POST, then reopen it? Or is this a bug with a known workaround? Sorry if I'm doing something silly here, but I'm fairly new to ElasticSearch, though I've really been loving it.

Update: Tried on a closed index and got a ClusterBlockException, which is understandable.

Update: This seems to be the result of the settings used in the index. Is there some reason for that? Is 10 shards with 5 replicas far larger than I think it is?
</description><key id="15947663">3228</key><summary>Reproducible UnavailableShardsException on type-creation via POST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Slater-Victoroff</reporter><labels /><created>2013-06-24T21:08:37Z</created><updated>2013-06-25T18:41:22Z</updated><resolved>2013-06-25T18:41:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-25T13:01:37Z" id="19974119">Hey,

can you please provide the exact curl calls you executed? The following sample worked for me

```
curl -X DELETE 'http://localhost:9200/test-index'
curl -X PUT 'http://localhost:9200/test-index' -d '{
  "settings": {
    "number_of_shards": 10,
    "number_of_replicas": 5
  }
}'


curl -v -X POST http://localhost:9200/test-index/test-type/_mapping -d '{
  "mappings": {
    "properties": {
      "test-string": {
        "type": "string",
        "store": "yes",
        "index": "analyzed",
        "term_vector": "with_positions_offsets"
      },
      "test-float": {
        "type": "float",
        "store": "yes"
      }
    }
  }
}'
```

Is it maybe that you executed the mapping request against `/test-index/test-type` instead of `/test-index/test-type/_mapping`?

Also you might want to use the google group for requests like this, as their are many many more eyes in order to help you with requests.
</comment><comment author="Slater-Victoroff" created="2013-06-25T18:41:22Z" id="19997767">Ah, many thanks, I will definitely ask questions there in the future. That said, I didn't execute these requests with curl, but I reproduced them using both Python's requests module and the Sense app for Chrome. Also the request did in fact work when the POST was made to `_mapping`, but it's still not clear to me why the ES instance should behave differently when I define index settings.

To recap:
When I define the settings for an index I CANNOT create a mapping by posting directly to the type
When I do not define the settings for an index I CAN create a mapping by posting directly to the type 

This behaviour still baffles me, but as there is a workaround for this strange behaviour I'm going to close this issue, might ask a related question on other more appropriate forums.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added Arabic/PersianNormalizationFilters from Lucene</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3227</link><project id="" key="" /><description>added two simple wrapping factories for both normalization filters... no options needed...
</description><key id="15945064">3227</key><summary>Added Arabic/PersianNormalizationFilters from Lucene</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-06-24T20:13:46Z</created><updated>2014-06-26T08:59:09Z</updated><resolved>2013-06-24T21:03:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-06-24T20:46:54Z" id="19934601">Looks good to me!
</comment><comment author="kimchy" created="2013-06-24T20:58:52Z" id="19935379">+1, lets get it into 0.90 and master?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Netty 4.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3226</link><project id="" key="" /><description>I'm currently working on this. Writing the issue here so we could discuss.

Netty 4 is not yet stable, so I won't merge until everything is tested and Netty 4 has a stable release.
</description><key id="15935303">3226</key><summary>Upgrade to Netty 4.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jasontedor/following{/other_user}', u'events_url': u'https://api.github.com/users/jasontedor/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jasontedor/orgs', u'url': u'https://api.github.com/users/jasontedor', u'gists_url': u'https://api.github.com/users/jasontedor/gists{/gist_id}', u'html_url': u'https://github.com/jasontedor', u'subscriptions_url': u'https://api.github.com/users/jasontedor/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/4744941?v=4', u'repos_url': u'https://api.github.com/users/jasontedor/repos', u'received_events_url': u'https://api.github.com/users/jasontedor/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jasontedor/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jasontedor', u'type': u'User', u'id': 4744941, u'followers_url': u'https://api.github.com/users/jasontedor/followers'}</assignee><reporter username="">cravergara</reporter><labels><label>:Network</label><label>enhancement</label><label>high hanging fruit</label><label>upgrade</label></labels><created>2013-06-24T16:59:27Z</created><updated>2016-07-23T02:26:36Z</updated><resolved>2016-07-23T02:26:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-24T22:36:31Z" id="19941129">cool, btw, I have been looking into this as well, its quite a bit of work, might request some changes in other parts of ES to properly use the new buffering model (that I still need to fully understand how its implemented).
</comment><comment author="cravergara" created="2013-06-25T04:32:02Z" id="19952624">I haven't looked into how exactly it's implemented, but the API is the same in that regard (other than changing the name from ChannelBuffer to ByteBuf). The ChannelBuffer class was only used in a few pieces, namely to support the HTTP server. Other than that, it's very modular, and swapping out the HTTP servers should be fairly trivial.

What's NOT trivial is that the API is very much streamlined in 4.0, whereas it's not so much in 3.6. I'm spending quite a bit of time trying to decipher the logic in 3.6, but very little time actually translating it to 4.0 once understood.

For your reference, these were the packages impacted:

org.elasticsearch.bulk.udp
org.elasticsearch.common.byte
org.elasticsearch.common.compress
org.elasticsearch.common.compress.lzf
org.elasticsearch.common.netty
org.elasticsearch.common.http.netty
org.elasticsearch.transport.netty

The byte and compress packages were only slightly modified to act as utilities for the other packages. the org.elasticsearch.common.http package was completely generic. I think someone had the foresight to believe that the HTTP functionality would eventually be swapped. org.elasticsearch.common.netty were basically utility functions for the other packages.

The hardest to change will be org.elasticsearch.common.http.netty and org.elasticsearch.transport.netty, which is what I'm up to right now. I won't be able to test until I can resolve the dependencies, so the most time consuming part will be testing and refactoring.

In all, it's a real challenge. But I'm down :)

I'm actually mainly doing this so I can hopefully build in Websocket support. I want to be able to stream searches ;)

-Cris
</comment><comment author="spinscale" created="2013-06-25T06:29:53Z" id="19955741">Hey,

there was just a post about a major refactoring from one netty 4 CR to another CR version. See http://netty.io/news/2013/06/18/4-0-0-CR5.html

Just wanted to ensure, that one cannot be totally confident, that this is the last breaking netty 4 change by the netty team (which I like personally, as performance is a first class citizen here).

--Alex
</comment><comment author="cravergara" created="2013-06-25T06:51:31Z" id="19956457">Thanks for the heads up! I've been on their IRC, but I don't think I've managed to talk with anyone on their team (or at least I'm not sure :) ). Right now, I'm working off of their current branch, so this is actually 4.0.0-CR7-SNAPSHOT. I am expecting it to change, but I figured now would be a good time to at least start. It's drastic enough from 3.6 that I think going from 4.0.0-CR7 to release would be easier.

Also I want this done for my own uses sooner than waiting :)
</comment><comment author="kimchy" created="2013-06-25T11:49:11Z" id="19970717">The way I saw it, to properly use the new netty 4 features, we will need to make use of the new buffering mechanisms to reduce buffer copies. Currently, we heavily rely on how buffers are handled in netty 3 in the IO reader and writer threads, which we will probably need to properly understand to use with netty 4. I think (haven't looked closely) that we will need to change things like how REST actions serialize data to make sure we reduce buffer copies.
</comment><comment author="cravergara" created="2013-06-26T03:03:59Z" id="20024665">I've just been replacing the data types with the Netty 4 equivalents, but I'm at the point where I have to rewrite the pipelining. I'll go back and take a look at buffering, because I didn't exactly take a close look under the hood.
</comment><comment author="icedfish" created="2013-07-16T08:12:29Z" id="21027322">Hi all,
Netty 4.0 Stable has been released :
https://github.com/netty/netty/releases/tag/netty-4.0.0.Final
</comment><comment author="cravergara" created="2013-07-23T13:25:41Z" id="21413290">I'll take a look again soon.
</comment><comment author="GrantGochnauer" created="2013-11-06T16:48:13Z" id="27890428">+1 - http://www.infoq.com/news/2013/11/netty4-twitter
</comment><comment author="kimchy" created="2013-11-07T00:22:05Z" id="27927689">I had another look, and I am not too happy with some of the netty 4 behavior.Note, the behavior people see (less GC) happens with "stock" netty, we do quite a few tricky to reduce the buffer copies already.  The migration will require quite a bit of work in order to maintain those optimizations... (we will do it anyhow). Also need to find the time to raise some of the issues with netty...
</comment><comment author="GrantGochnauer" created="2013-11-07T00:30:07Z" id="27928136">Thanks for the update Shay!
</comment><comment author="normanmaurer" created="2013-11-13T09:56:31Z" id="28382152">@kimchy what "issues" you are talking about? Maybe I can give you some pointers.
</comment><comment author="kimchy" created="2013-11-13T09:57:50Z" id="28382226">@normanmaurer I will start respective questions properly on the netty mailing list.
</comment><comment author="normanmaurer" created="2013-11-14T06:02:29Z" id="28461661">@kimchy alright..
</comment><comment author="kimchy" created="2013-11-14T12:23:52Z" id="28479746">@normanmaurer cool, we are focused on getting a new version of ES out, so not focusing on netty 4 now, especially with the problematic aspects I saw there and the fact that it will be a bigger change than expected. And trust me, the "issues" there should not have quotation marks as in your comment, we use it in quite an advance manner make sure we don't allocate buffers needlessly, and the refactoring done in netty 4 is, well, tricky.... (admittedly, it does help naive handlers).
</comment><comment author="hhoffstaette" created="2014-02-14T22:15:38Z" id="35130363">@kimchy Mind if I take this? I'll gladly collect any concerns (sort of doing it already) and write up/research/do the work. I'm also now on the Netty list.
</comment><comment author="normanmaurer" created="2014-02-15T08:27:01Z" id="35150245">Feel free to ping me if any questions come up

&gt; Am 14.02.2014 um 23:16 schrieb Holger Hoffst&#228;tte notifications@github.com:
&gt; 
&gt; @kimchy Mind if I take this? I'll gladly collect any concerns (sort of doing it already) and write up/research/do the work.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="jabley" created="2014-07-01T15:14:00Z" id="47668726">Ohai.

https://github.com/jabley/elasticsearch/compare/enhancement/netty_4_upgrade

I think this is quite close, I'll try to get the 5 test failures passing soon.

```
Tests with failures (first 3 out of 5):
  - org.elasticsearch.options.jsonp.JsonpOptionDisabledTest.testThatJSONPisDisabled
  - org.elasticsearch.options.jsonp.JsonpOptionEnabledTest.testThatJSONPisEnabled
  - org.elasticsearch.transport.netty.SimpleNettyTransportTests.testVersion_from0to0
```

Those tests pass in isolation, so I'll have to dig a bit.
</comment><comment author="jabley" created="2014-07-14T15:31:25Z" id="48914784">```
mvn test -Dtests.seed=2025BC3DED852C8E -Dtests.class=org.elasticsearch.cluster.ClusterServiceTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/London -Des.logger.level=INFO -Des.node.local=1 -Dtests.heap.size=512m -Dtests.bwc.path=/Users/jabley/git/elasticsearch/backwards -Dtests.processors=8
```

that is failing on my branch.

```
Tests with failures:
  - org.elasticsearch.cluster.ClusterServiceTests.testListenerCallbacks
  - org.elasticsearch.cluster.ClusterServiceTests.testPendingUpdateTask
```

Those tests pass in isolation when I used `-Dtests.method="testPendingUpdateTask"`. I haven't figured it out yet; I'm presuming there's some shared state that I've not spotted.
</comment><comment author="jasontedor" created="2015-11-18T16:47:13Z" id="157774439">&gt; Feel free to ping me if any questions come up

@normanmaurer As [Netty 3.x is deprecated](http://stackoverflow.com/a/30477520) but is still currently maintained, do the Netty Project maintainers have a rough timeframe for when it will no longer be the case that it is maintained?
</comment><comment author="philnate" created="2016-03-21T13:45:21Z" id="199286055">Any news on when this will be done? It's causing some trouble with newer nettys, which use the same artefact names, but different packages. Could at least netty be shaded like the other dependencies are?
</comment><comment author="jasontedor" created="2016-03-21T13:48:51Z" id="199288691">&gt; Any news on when this will be done?

Not in the near future.

&gt; It's causing some trouble with newer nettys, which use the same artefact names, but different packages. 

What's the issue, since they are in different packages (`org.jboss.netty` versus `io.netty`)?

&gt; Could at least netty be shaded like the other dependencies are?

We don't shade dependencies after the 1.x series, and are not going to make a change like that for the 1.x series.
</comment><comment author="normanmaurer" created="2016-03-21T13:53:20Z" id="199291834">Just to be clear netty 4.x and 3.x use different groupId's and different packages so it should be no problem to use both versions in the same app.
</comment><comment author="philnate" created="2016-03-21T21:49:45Z" id="199501646">Yeah, I got lost in dependencies. Somehow some old org.jetty.netty got pulled in which didn't fit elasticsearchs needs. I think what got me confused is that mvnrepository tells  that ES 2.2 uses io.netty:netty (which in fact has org.jetty.netty packages) http://mvnrepository.com/artifact/org.elasticsearch/elasticsearch/2.2.0, this package would conflict with never nettys. I've pulled in io.netty:netty:3.x and io.netty:netty-all:4.y and it worked out.
</comment><comment author="jasontedor" created="2016-06-08T12:42:31Z" id="224576919">Relates netty/netty#5361 which is a proposal to discontinue support for Netty 3.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added support for PatternReplaceCharFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3225</link><project id="" key="" /><description>PatternReplaceCharFilter allows the use of a regex to manipulate the characters in a string before analysis

Closes #3197
</description><key id="15934758">3225</key><summary>Added support for PatternReplaceCharFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-06-24T16:47:24Z</created><updated>2014-07-15T21:18:58Z</updated><resolved>2013-06-26T13:47:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-06-24T19:44:40Z" id="19930704">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IndexFieldData should ignore an AtomicReader's liveDocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3224</link><project id="" key="" /><description>IndexFieldData instances only build field data for live documents. This can trigger inconsistencies if a collector consumes a field data instance which has been built for a newer point-in-time AtomicReader which might have additional deleted documents.

For example, in the following scenario, Q1 will miss some data:
- Q1 (slow query) starts executing
- refresh
- Q2 (fast query) starts executing
- Q2 requires field data for segment S (preserved through the refresh, but has additional deleted documents) -&gt; load and cache
- Q1 requires data for segment S -&gt; get from cache (same cache key)

This problem would be solved by ignoring live documents when loading field data.
</description><key id="15932921">3224</key><summary>IndexFieldData should ignore an AtomicReader's liveDocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels><label>bug</label></labels><created>2013-06-24T16:08:29Z</created><updated>2013-06-29T10:14:05Z</updated><resolved>2013-06-29T10:14:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-06-26T21:47:42Z" id="20082660">@martijnvg just told me that there is the same problem in SimpleIdCache. We should fix this class as well...
</comment><comment author="jpountz" created="2013-06-28T08:41:23Z" id="20176686">... and WeightedFilterCache!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo Bulktest to Slow</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3223</link><project id="" key="" /><description>The gzipped data file which is used in the geo bulk test needs a lot of time to load. The number of datasets in this file should be reduced to an accurate amount.
</description><key id="15928392">3223</key><summary>Geo Bulktest to Slow</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-24T14:45:31Z</created><updated>2013-06-24T15:06:45Z</updated><resolved>2013-06-24T15:06:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Merge integer field data implementations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3222</link><project id="" key="" /><description>This commit merges field data implementations for byte, short, int and long
data into PackedArrayAtomicFieldData which uses Lucene's PackedInts API to
store data.

I wrote a small benchmarking utility: LongFieldDataBenchmark, included in the
commit, to make sure that this patch didn't make field data larger or slower to load.
It reported similar loading times and memory usage reduction was between 0%
when values were hardly compressible (random) and/or ordinals took most of the
space and 50% for the single-valued case when Lucene's packed ints allow for
better compression than byte-aligned storage.
</description><key id="15927844">3222</key><summary>Merge integer field data implementations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-06-24T14:36:12Z</created><updated>2014-07-16T21:53:06Z</updated><resolved>2013-06-26T20:47:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Reduced geobulk data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3221</link><project id="" key="" /><description>Reduced the data in world map to europe to fasten up geo bulktest.

Closes #3223
</description><key id="15927467">3221</key><summary>Reduced geobulk data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-24T14:29:02Z</created><updated>2014-07-16T21:53:06Z</updated><resolved>2013-06-24T15:06:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Merge integer field data implementations together</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3220</link><project id="" key="" /><description>Elasticsearch has 4 similar field data implementations for its integer types: byte, short, int and long. These implementations could be merged together and even be made a little more memory-efficient by using Lucene's PackedInts API.
</description><key id="15926785">3220</key><summary>Merge integer field data implementations together</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T14:16:02Z</created><updated>2013-07-09T16:38:43Z</updated><resolved>2013-06-26T20:22:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-06-26T20:33:40Z" id="20077869">With the help of @martijnvg , I ran a few benchmarks to compare the new implementation against the old ones. Loading time are similar, memory usage is between 1x and 2x smaller and faceting runs at similar speeds (there are little differences based on the dataset due to CPU caching effects). For example, here are the results of HistogramFacetSearchBenchmark on a 20m documents index for fields of type byte (b_value), short (s_value), int (i_value) and long (l_value):

```
Without this commit:
--&gt; Histogram Facet (b_value) 599ms
--&gt; Histogram Facet (b_value/b_value) 819ms
--&gt; Histogram Facet (s_value) 681ms
--&gt; Histogram Facet (s_value/s_value) 813ms
--&gt; Histogram Facet (i_value) 668ms
--&gt; Histogram Facet (i_value/i_value) 804ms
--&gt; Histogram Facet (l_value) 670ms
--&gt; Histogram Facet (l_value/l_value) 815ms
With this commit:
--&gt; Histogram Facet (b_value) 604ms
--&gt; Histogram Facet (b_value/b_value) 752ms
--&gt; Histogram Facet (s_value) 637ms
--&gt; Histogram Facet (s_value/s_value) 738ms
--&gt; Histogram Facet (i_value) 637ms
--&gt; Histogram Facet (i_value/i_value) 737ms
--&gt; Histogram Facet (l_value) 640ms
--&gt; Histogram Facet (l_value/l_value) 743ms
```

And here are the results on a 5m index:

```
Without this commit:
--&gt; Histogram Facet (b_value) 150ms
--&gt; Histogram Facet (b_value/b_value) 166ms
--&gt; Histogram Facet (i_value) 141ms
--&gt; Histogram Facet (i_value/i_value) 164ms
--&gt; Histogram Facet (i_value) 140ms
--&gt; Histogram Facet (i_value/i_value) 164ms
--&gt; Histogram Facet (l_value) 140ms
--&gt; Histogram Facet (l_value/l_value) 164ms
With this commit:
--&gt; Histogram Facet (b_value) 152ms
--&gt; Histogram Facet (b_value/b_value) 195ms
--&gt; Histogram Facet (i_value) 147ms
--&gt; Histogram Facet (i_value/i_value) 169ms
--&gt; Histogram Facet (i_value) 146ms
--&gt; Histogram Facet (i_value/i_value) 169ms
--&gt; Histogram Facet (l_value) 145ms
--&gt; Histogram Facet (l_value/l_value) 169ms
```
</comment><comment author="jpountz" created="2013-06-27T08:02:40Z" id="20102850">About memory and loading time, here are reports from LongFieldDataBenchmark on 1M documents.

```
Without this commit
Data    Loading time    Implementation  Actual size     Expected size
SINGLE_VALUES_DENSE_ENUM    65  Single  976.6 KB    976.6 KB
SINGLE_VALUED_DENSE_DATE    200 Single  7.6 MB  7.6 MB
MULTI_VALUED_DATE   233 WithOrdinals    15.4 MB 15.4 MB
MULTI_VALUED_ENUM   48  WithOrdinals    7.8 MB  7.8 MB
SINGLE_VALUED_SPARSE_RANDOM 30  WithOrdinals    3.6 MB  3.6 MB
MULTI_VALUED_SPARSE_RANDOM  71  WithOrdinals    8.1 MB  8.1 MB
MULTI_VALUED_DENSE_RANDOM   428 WithOrdinals    27.1 MB 27.1 MB

With this commit
Data    Loading time    Implementation  Actual size     Expected size
SINGLE_VALUES_DENSE_ENUM        86      Single  488.4 KB        488.3 KB
SINGLE_VALUED_DENSE_DATE        191     Single  4.3 MB  4.3 MB
MULTI_VALUED_DATE       224     WithOrdinals    10.5 MB 10.5 MB
MULTI_VALUED_ENUM       46      WithOrdinals    7.8 MB  7.8 MB
SINGLE_VALUED_SPARSE_RANDOM     30      WithOrdinals    3.5 MB  3.5 MB
MULTI_VALUED_SPARSE_RANDOM      76      WithOrdinals    7.7 MB  7.7 MB
MULTI_VALUED_DENSE_RANDOM       448     WithOrdinals    23.7 MB 23.7 MB
```

More information about the data sets:
- SINGLE_VALUES_DENSE_ENUM assigns a single long between 0 and 15 to each document
- SINGLE_VALUED_DENSE_DATE assigns a single date between 2010 and 2012 to each document
- MULTI_VALUED_DATE assigns 0, 1 or 2 dates between 2010 and 2012 to every document
- MULTI_VALUED_ENUM assigns 0, 1 or 2 longs between 3 and 10 to every document
- SINGLE_VALUED_SPARSE_RANDOM assigns 1 random long to 10% of documents
- MULTI_VALUED_SPARSE_RANDOM assigns between 1 and 5 random longs to 10% of documents
- MULTI_VALUED_DENSE_RANDOM assigns between 1 and 3 random longs to all documents

More information about the columns:
- Loading time is the time to load field data from the directory into memory.
- Implementation is the class name of the AtomicFieldData instance class which has been loaded
- Actual size is the memory usage reported by RamUsageEstimator
- Expected size is the memory usage reported by AtomicFieldData.getMemorySizeInBytes

Explanation of the memory reduction:
- On single-valued fields, the new implementation performs better when the number of required bits per value is not right below 8, 16, 32 or 64, for example for small enums or dates.
- On multi-valued fields with low cardinality, most of the memory usage is taken by the ordinals map, so memory usage doesn't change much.
- On multi-valued fields with high cardinality, the fact that we use MonotonicAppendingLongBuffer to encode values (this class compresses efficiently sequences of monotonically increasing longs) gives a memory reduction again (for example 13% on MULTI_VALUED_DENSE_RANDOM even though values have been chosen completely randomly).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Lookup Terms Filter _cache parameter not being taken into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3219</link><project id="" key="" /><description>The parameter is taken into account for normal terms filter(when you pass the list of terms), but not when you are using the look up mechanism.

I won't paste a test case since it has to be long enough as for the response time to be meaningful, but I guess in this line there should be a check on wether or not to cache:

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java#L172

same way there is in: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java#L193-L195
</description><key id="15919596">3219</key><summary>Lookup Terms Filter _cache parameter not being taken into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T11:39:07Z</created><updated>2013-06-25T13:24:19Z</updated><resolved>2013-06-24T13:23:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-06-24T12:48:08Z" id="19904595">Thinking about it, its also not possible to NOT cache the result of the lookup... 
Maybe that should also be possible? Or at least, when explicitly not caching the total filter, the lookup shouldn't be cached as well(it just makes no sense in this case).
</comment><comment author="kimchy" created="2013-06-24T13:21:48Z" id="19906162">I will fix the cache flag to be taken into account, I think caching the lookup makes sense almost all times, and its bounded by size by default, so you should be ok with defining a small size if you are concerned by it?
</comment><comment author="lmenezes" created="2013-06-24T13:37:51Z" id="19907054">Not really concerned about size as much as with consistency. 
I do think it might be useful to not cache the lookup. It's not that expensive of an operation(at least in my use case), and this way I don't have to worry about keeping the cache up to date(afaik, the only way to guarantee that is assigning it a key and clearing when needed?)
</comment><comment author="kimchy" created="2013-06-24T13:41:11Z" id="19907235">in that case, you can simply set `indices.cache.filter.terms.size` to `0`, or are you looking to do it per query call? If so, then we probably need another flag, cause the cache flag always means caching the filter itself (the bitsets per segment).
</comment><comment author="lmenezes" created="2013-06-24T13:47:58Z" id="19907616">I agree this should be possible to set per call, since setting that to 0 basically would interfere with other needs as well.
But I do think if you explicitly say _cache -&gt; false, but then, the "content" of the filter is cached, makes it kind of pointless not caching in the first place, since the resulting filter will always be the same. Or am I missing something?
If its so, then I think its okay just using the global _cache flag as a hint for not caching the result of the lookup. I just dont see the use case where you dont want to cache the total filter, but want to cache the lookup...
</comment><comment author="lmenezes" created="2013-06-25T08:57:33Z" id="19961632">@kimchy 
As a follow up on that...
I ran some tests on this feature, where the lookup would fetch around 35K-60K terms, and the average lookup time was below 40ms(the lookup index has one single shard replicated across all nodes). Applying the filter was actually much more expensive, ranging on 700ms-1s. 

This way, maybe it even makes sense caching the total filter using as key the hashed content of the lookup? This way if you are not caching the lookup, you might still get some benefits. Does it make sense?
</comment><comment author="lmenezes" created="2013-06-25T13:24:19Z" id="19975402">I creatd a ticket for that on https://github.com/elasticsearch/elasticsearch/issues/3236
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geohash filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3218</link><project id="" key="" /><description>Geohashes provide a structure for geospatial subdivisions by defining a hierarchical grid. This data structure supports an efficient way to filter data in a hierarchical way which should be supported by the REST API to filter points of certain cell.
Such a filter just needs a geohash that defines a cell within the grid and will be used as prefix. In result of filtering data with a geohash filter all geopoints sharing the same prefix as the defined cell must be returned. Also such a filter should allow filtering adjacent cells of the defined cell. For example the cell defined by the geohash `u30` has eight adjacent cells `u1r`, `u32`, `u33`, `u1p`, `u31`, `u0z`, `u2b` and `u2c`. So this kind of filter can be used for a more efficient filtering-process than the simple bounding box filter does.
# Example

The most simple setup for such a filter uses two arguments:
- the field of the document which holds the geohash
- the actual geohash

```
curl -XGET 'http://127.0.0.1:9200/locations/_search?pretty=true' -d '{
    "query": {
        "match_all":{}
    },
    "filter": {
        "geohash_cell": {
            "field": "pin",
            "geohash": "u30",
        }
    }
}'
```

Since each geopoint corresponds to a single geohash it must be possible to index all prefixes of this geohash. For example the geohash `u30` also corresponds to the terms `u3` and `u`. The upper bound of the geohash length is defined by the precision defined in the mapping.

```
curl -XPUT 'http://127.0.0.1:9200/locations/?pretty=true' -d '{
    "mappings" : {
        "location": {
            "properties": {
                "pin": {
                    "type": "geo_point",
                    "geohash": true,
                    "geohash_prefix": true,
                    "geohash_precision", 12
                }
            }
        }
    }
}'
```
</description><key id="15916399">3218</key><summary>Geohash filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-24T09:58:27Z</created><updated>2013-06-24T10:48:39Z</updated><resolved>2013-06-24T10:42:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Close/Open Index API to support multiple indices and wildcard on index names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3217</link><project id="" key="" /><description>Close and Open index APIs should follow other indices APIs where they should support wildcard notation and multiple indices.
</description><key id="15892025">3217</key><summary>Close/Open Index API to support multiple indices and wildcard on index names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-06-23T01:45:42Z</created><updated>2013-09-05T08:55:55Z</updated><resolved>2013-07-16T13:23:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms Filter Lookup: Failure when no mappings for the terms field exists (no data indexed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3216</link><project id="" key="" /><description>It should simply not match any data on that index, not fail (the mappings are required when building the terms list from the lookup data).
</description><key id="15887530">3216</key><summary>Terms Filter Lookup: Failure when no mappings for the terms field exists (no data indexed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-22T17:40:42Z</created><updated>2013-06-22T17:41:14Z</updated><resolved>2013-06-22T17:41:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Remove reference to a ThreadLocal instance which causes it not to be GCed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3215</link><project id="" key="" /><description>This is because thread holds a hard reference to the QueryParseContext value, that one holds a reference
to this IndexQueryParserService  instance which holds a link to the `cache` (ThreadLocal instance).
This a hard reference which comes on top of the usual WeakReference in Thread.threadLocals.

We need to break the last part of this chain so that `cache` will only be weakly referenced by the Thread.threadLocals, which can be cleaned.
Once cleaned, Thread.threadLocals will pick it up and clean the reference to the QueryParseContext and this IndexQueryParserService as well.

Closes #3130
</description><key id="15869118">3215</key><summary>Remove reference to a ThreadLocal instance which causes it not to be GCed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-06-21T20:16:59Z</created><updated>2014-07-04T01:37:16Z</updated><resolved>2013-06-25T08:10:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="usmanm" created="2013-06-25T00:38:56Z" id="19946063">@kimchy committed a patch (cbe18608ef88be34b8018f76a866f84365808284) which fixes this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix NumericTokenizer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3214</link><project id="" key="" /><description>NumericTokenizer is a simple wrapper aroung a NumericTokenStream. However, its
implementations had a few issues: its reset() method was not idempotent,
causing exceptions if reset() was called twice (causing #3211) and it had no
attributes, meaning that the only thing it allowed to do is counting the number
of generated tokens. The reason why indexing numeric data worked is that
the mapper's parseCreateField directly generates a NumericTokenStream and
by-passes the analyzer.

This commit makes NumericTokenizer.reset idempotent and makes consuming a
NumericTokenizer behave the same way as consuming the underlying
NumericTokenStream.
</description><key id="15853978">3214</key><summary>Fix NumericTokenizer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-06-21T14:34:49Z</created><updated>2014-06-12T08:27:50Z</updated><resolved>2013-06-24T14:11:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add PATCH-based update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3213</link><project id="" key="" /><description>This simplifies the way a document can be updated. It changes the syntax to, for example:

``` sh
$ curl -XPATCH ":9200/foo/user/1" -d '{"first_name":"Stephen"}'
```

Instead of the more verbosely-nested:

``` sh
$ curl -XPOST ":9200/foo/user/1/_update" -d '{"doc":{"first_name":"Stephen"}}'
```

Adding PATCH support is a relatively big change for the project, but I wanted to at least start a discussion. I think the request format for the update example is cleaner than nesting the update in a "doc" node (and more predictable/consistent with the PUT method).
</description><key id="15830867">3213</key><summary>Add PATCH-based update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephencelis</reporter><labels><label>discuss</label></labels><created>2013-06-21T00:46:03Z</created><updated>2014-07-25T08:58:31Z</updated><resolved>2014-07-25T08:58:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-06-21T06:09:18Z" id="19799995">I modified your issue description and added _update in your last query and change PUT to POST as described in http://www.elasticsearch.org/guide/reference/api/update/
</comment><comment author="stephencelis" created="2013-06-21T06:53:04Z" id="19800940">Ah, thanks! Was buried in the code all day and missed that :)
</comment><comment author="stephencelis" created="2013-07-26T03:45:28Z" id="21599875">So...any interest in this?
</comment><comment author="qraynaud" created="2014-02-24T10:18:01Z" id="35872878">I would rather have a native support of RFC 6902 on PATCH. It would help me a lot here.
</comment><comment author="stephencelis" created="2014-02-24T15:35:47Z" id="35898058">You mean JSON patch? That's different (but very cool). It builds upon the more general RFC 5789.
</comment><comment author="qraynaud" created="2014-02-24T15:40:35Z" id="35898565">Yes. I would like to keep update where it is so PATCH could be implemented in accordance to RFC 6902.

You could argue that RFC 6902 introduces a specific Content-Type header (application/json-patch+json) that could be matched to know if we should use the update API or a PATCH 6902.

I believe it would be kind of dirty and hard to explain in the documentation. I would rather have both Content-Type application/json &amp; application/json-patch+json do the same thing on PATCH.

What do you think about this?
</comment><comment author="clintongormley" created="2014-07-23T13:29:25Z" id="49873044">@qraynaud i like the idea of supporting something like RFC 6092 http://tools.ietf.org/html/rfc6902
but I find their proposal quite limited, eg it would be good to support incrementing, numeric ops etc as well.
</comment><comment author="qraynaud" created="2014-07-23T15:30:22Z" id="49890376">This could obviously be added with non standard operators inspired from the spec. But at least it would be compliant with the spec ^^.
</comment><comment author="clintongormley" created="2014-07-25T08:58:31Z" id="50124520">Closing in favour of #7030 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize parent/child queries and filters if possible</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3212</link><project id="" key="" /><description>This is for #3190.  The main optimization is in has_child queries and filters by short-circuiting the parent processing once all known parents have been.  Other small updates to has_child query/filter to not execute the internal children filter when we know there are going to be no hits (no parents found).
</description><key id="15824591">3212</key><summary>Optimize parent/child queries and filters if possible</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-06-20T21:38:24Z</created><updated>2014-06-23T05:33:00Z</updated><resolved>2013-07-18T16:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-25T20:47:42Z" id="20005969">The counter seems to have a good impact in some cases, but I think it only beneficial in the case the matching docs are in the first segments / beginning of a segment. Would be cool if it also improve the case when the matching docs are in any segment / any place inside a segment. I think we can do by replacing the parentsFilter by a term/terms filter.
</comment><comment author="martijnvg" created="2013-06-25T20:52:51Z" id="20006354">I found out that this way of changing the parentFilter with a TermFilter or TermsFilter improves the query time:
https://github.com/martijnvg/elasticsearch/commit/bcc01732a32392fa85d9a93f614722a72f94f4d7#L0R197
</comment><comment author="mattweber" created="2013-06-25T21:15:08Z" id="20007934">Thats awesome.  Why did you choose a to use the filter when only 128 or less uid's are found?  Would it improve or degrade performance if we increase that number?
</comment><comment author="martijnvg" created="2013-06-25T21:29:47Z" id="20008916">At some points it will degrade performance, I'm not sure yet at what point that is. I picked 128 without really finding the sweet spot. I think a value like 1M doesn't work, but it feels like this can be increased to something like between 1000 and 2000, but maybe this should be a ratio (maybe configurable) between number of matching docs and the total number of parent docs.

The terms filter actually access the  index on disk, and too many terms in the terms filter will slow down the query. If the default approach is chosen then for each parent doc it does a lookup in a map and performing this many times (millions of map lookups) is also slow. 
</comment><comment author="mattweber" created="2013-07-01T21:38:04Z" id="20312349">Rebased to current master.
</comment><comment author="mattweber" created="2013-07-18T16:50:48Z" id="21197542">Closed via 4d05c9c, see #3190 for more information.  Thanks @martijnvg!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch 0.90 fails when "highlight" contains a field of type "long"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3211</link><project id="" key="" /><description>If the "highlight" section of a searh query contains a field of type "long", an error occurs. That didn't happen with the old version.
This is a sample script I used to test this behaviour:

``` bash
curl -s -o /dev/null -X DELETE "http://${hostname}:9200/test?pretty"

curl -s -o /dev/null -X PUT "http://${hostname}:9200/test/?pretty"  -d '
{
   "mappings" : {
      "test1" : {
        "properties" : {
            "text" : {
                "store": "yes",
                "type": "string"
            }
         }
      },
      "test2" : {
        "properties" : {
            "text" : {
                "store": "yes",
                "type": "string"
            },
            "number" : {
                "store": "yes",
                "type": "long"
            }
         }
      }
   }
}
'

curl -s -o /dev/null -X POST "http://${hostname}:9200/test/test1?pretty"  -d '
{
   "text" : "test one"
}
'

curl -s -o /dev/null -X POST "http://${hostname}:9200/test/test2?pretty"  -d '
{
   "text" : "test two",
   "number" : 100
}
'

curl -s -o /dev/null -X POST "http://${hostname}:9200/test1/_refresh"
curl -s -o /dev/null -X POST "http://${hostname}:9200/test2/_refresh"

sleep 3

curl -s -X GET "http://${hostname}:9200/test/_search?pretty" -d '
{
  "query": {
        "prefix": {
          "text": "test"
        }
  },
  "highlight": {
    "number_of_fragments": 0,
    "fields": {
        "text": {},
        "number": {}
    }
  }

}'
```

If you run it against an elasticsearch 0.20.6 server, it returns the two hits, correctly highlighted.
However, if you run it against an elasticsearch 0.90 server, this happens:

``` bash
{
  "took" : 6,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 4,
    "failed" : 1,
    "failures" : [ {
      "index" : "test",
      "shard" : 0,
      "status" : 500,
      "reason" : "FetchPhaseExecutionException[[test][0]: query[text:test*],from[0],size[10]: Fetch Failed [Failed to highlight field [number]]]; nested: StringIndexOutOfBoundsException[String index out of range: -1]; "
    } ]
  },
  "hits" : {
    "total" : 2,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "test1",
      "_id" : "y51XzuuaQ6uYnigzhq5EtA",
      "_score" : 1.0, "_source" :
{
   "text" : "test one"
}
,
      "highlight" : {
        "text" : [ "&lt;em&gt;test&lt;/em&gt; one" ]
      }
    } ]
  }
}
```
</description><key id="15786292">3211</key><summary>ElasticSearch 0.90 fails when "highlight" contains a field of type "long"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">ariasdelrio</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-20T08:13:29Z</created><updated>2013-07-03T09:57:21Z</updated><resolved>2013-06-24T14:12:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-20T08:36:04Z" id="19737495">thanks for opening this issue! We will look into it soon
</comment><comment author="jpountz" created="2013-06-20T12:41:45Z" id="19749587">@quecksilber Thanks for the detailed steps, I could reproduce the problem. Can you confirm that with 0.20 you didn't expect the numbers to be highlighted even if they were part of the query? (0.20 doesn't look able to do so)
</comment><comment author="ariasdelrio" created="2013-06-20T13:32:41Z" id="19752404">@jpountz I didn't expect the numbers to be highlighted in the query. To
say the truth, we only realized we were automatically including numeric
fields in the "highlight" section after this error came up (it is an ID in
the real scenario). Maybe this should be an error, but with a different
message, like "numeric values cannot be highlighted" or something like that.
</comment><comment author="jpountz" created="2013-06-21T14:36:08Z" id="19819527">I discussed this issue with Simon and we decided to make the behavior match 0.20: highlighting numeric terms is not supported but doesn't raise errors.
</comment><comment author="ariasdelrio" created="2013-06-21T14:40:28Z" id="19819791">Thanks :)

On Fri, Jun 21, 2013 at 4:36 PM, Adrien Grand notifications@github.comwrote:

&gt; I discussed this issue with Simon and we decided to make the behavior
&gt; match 0.20: highlighting numeric terms is not supported but doesn't raise
&gt; errors.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3211#issuecomment-19819527
&gt; .
</comment><comment author="smtlaissezfaire" created="2013-06-23T00:21:09Z" id="19867037">+1 
</comment><comment author="smtlaissezfaire" created="2013-06-24T19:49:22Z" id="19930982">Awesome!  Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception if the URL is too long</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3210</link><project id="" key="" /><description>Netty throws an exception if a URL is longer than 4096 bytes.

The problem is simple to reproduce, try to execute a search on a super long list of indexes, you should get the following exception:

```
[2013-06-19 10:16:31,273][WARN ][http.netty               ] [Hardnose] Caught exception while handling client http traffic, closing connection [id: 0xc6261414, /127.0.0.1:63015 :&gt; /127.0.0.1:9200]
org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: An HTTP line is larger than 4096 bytes.
    at org.elasticsearch.common.netty.handler.codec.http.HttpMessageDecoder.readLine(HttpMessageDecoder.java:642)
    at org.elasticsearch.common.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:182)
    at org.elasticsearch.common.netty.handler.codec.http.HttpMessageDecoder.decode(HttpMessageDecoder.java:101)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:500)
    at org.elasticsearch.common.netty.handler.codec.replay.ReplayingDecoder.cleanup(ReplayingDecoder.java:554)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:336)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:779)
    at org.elasticsearch.common.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:54)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendDownstream(DefaultChannelPipeline.java:784)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.closeRequested(SimpleChannelHandler.java:334)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleDownstream(SimpleChannelHandler.java:260)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:591)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:582)
    at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
    at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
    at org.elasticsearch.http.netty.NettyHttpServerTransport.exceptionCaught(NettyHttpServerTransport.java:306)
    at org.elasticsearch.http.netty.HttpRequestHandler.exceptionCaught(HttpRequestHandler.java:49)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.exceptionCaught(SimpleChannelHandler.java:156)
    at org.elasticsearch.common.netty.channel.SimpleChannelHandler.handleUpstream(SimpleChannelHandler.java:130)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.exceptionCaught(SimpleChannelUpstreamHandler.java:153)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:680)
```

Is there any setting to control the maximum size of a URL ? I found nothing in the documentation.
</description><key id="15785777">3210</key><summary>Exception if the URL is too long</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jeromer</reporter><labels /><created>2013-06-20T07:58:27Z</created><updated>2014-09-02T12:55:51Z</updated><resolved>2013-06-20T13:45:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-20T08:04:10Z" id="19736255">can't you use an alias for those indices?
</comment><comment author="jeromer" created="2013-06-20T08:50:16Z" id="19738034">That would actually be an acceptable solution for the majority of use cases.
But I have a list of dynamic index names, so maintaining some aliases would be quite hard, unless Elasticsearch can use some kind of patterns (regexes ?) in aliases definition which I could use to solve my problem.

I agree that could sound weir to get a super long URL but I tend to think I am not the only one in this situation.

In case using aliases is not possible, do you think providing a configuration variable for netty's max_header_size (If I am not mistaken, this setting is available in netty)) would be an acceptable trade-off ?

https://docs.jboss.org/netty/3.2/api/org/jboss/netty/handler/codec/http/HttpMessageDecoder.html
</comment><comment author="hc" created="2013-06-20T09:01:16Z" id="19738582">It looks like it already exists but it's not documented: https://github.com/elasticsearch/elasticsearch/issues/1174
So there is "http.max_initial_line_length" defaulting at 4kb
</comment><comment author="jeromer" created="2013-06-20T13:45:03Z" id="19753141">Cool, thanks for the tip :)
</comment><comment author="s1monw" created="2013-06-20T14:26:17Z" id="19756168">hey, do you wanna open an issue on the reference documentation to document that? https://github.com/elasticsearch/elasticsearch.github.com

thanks
</comment><comment author="dadoonet" created="2013-06-20T15:01:57Z" id="19759007">Option added to documentation: https://github.com/elasticsearch/elasticsearch.github.com/commit/e6fd4d044fd659738401d149717f42b737de262c
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Statistical facet ignores nested documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3209</link><project id="" key="" /><description>To reproduce, create a mapping:

```
curl -XPUT "http://localhost:9200/bar/" -d'
{
   "mappings": {
      "foo": {
         "properties": {
            "accounts": {
               "type": "nested"
            }
         }
      }
   }
}'
```

Index an object:

```
curl -XPOST "http://localhost:9200/bar/foo" -d'
{
   "accounts": [
      {
         "amount": 1
      }
   ]
}'
```

And run a search:

```
curl -XPOST "http://localhost:9200/bar/_search" -d'
{
   "facets": {
      "test": {
         "statistical": {
            "field": "accounts.amount"
         }
      }
   },
   "size":0
}'
```

This is the output (note that it did find one document, but not stats on it):

```
{
   "took": 90,
   "timed_out": false,
   "_shards": {
      "total": 5,
      "successful": 5,
      "failed": 0
   },
   "hits": {
      "total": 1,
      "max_score": 1,
      "hits": []
   },
   "facets": {
      "test": {
         "_type": "statistical",
         "count": 0,
         "total": 0,
         "min": "Infinity",
         "max": "-Infinity",
         "mean": 0,
         "sum_of_squares": 0,
         "variance": "NaN",
         "std_deviation": "NaN"
      }
   }
}
```
</description><key id="15785312">3209</key><summary>Statistical facet ignores nested documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-06-20T07:43:52Z</created><updated>2013-06-20T18:37:18Z</updated><resolved>2013-06-20T18:37:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-06-20T18:28:24Z" id="19773569">you forgot to specify "nested" on the facet:

```

curl -XPOST "http://localhost:9200/bar/_search" -d'
{
   "facets": {
      "test": {
         "statistical": {
            "field": "accounts.amount"
         },
         "nested" : "accounts"

      }
   },
   "size":0
}'
```
</comment><comment author="bleskes" created="2013-06-20T18:37:18Z" id="19774139">@uboness thx. Indeed. For future record - this works too and conforms to the documentation (note the `field` option of the facet):

```
curl -XPOST "http://localhost:9200/bar/_search" -d'
{
   "facets": {
      "test": {
         "statistical": {
            "field": "amount"
         },
         "nested": "accounts"
      }
   },
   "size":0
}'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elastic Search Random Node High Load</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3208</link><project id="" key="" /><description>We have a problem with our Elastic Search cluster in every environment. The cluster will sometimes get into an unstable state, wherein certain ES nodes will have load that is several times greater than the load on other nodes.

This can be reproduced every time very quickly by hitting the cluster with about 40 concurrent threads while running data indexing, and much less quickly by simply running constant searches.

Our cluster setup:
- 4 no-data client nodes that service search requests
- 1 no-data client node that sends data indexing requests.
- 10 data nodes that are called by the 5 for indexing and searching.

We have verified that all the boxes have:
- the same hardware
  - CPU Intel Xeon X5550 (2.67GHz, 64-bit, 16 core)
  - 96GB high-end physical RAM (64GB JVM heap, 20 GB mem baseline).
- the same OS
  - Red Hat 4.1.2-54 (Linux version 2.6.18-348.1.1.el5)
- the same JVM
  - Sun/Oracle 1.7.0_09 (64-bit)
- the same ES version
  - Currently on 0.20.4
  - Problem existed since 0.19.8, when we started with ES
- no other software running
- the same number of shards
  - 1 product shard per node
  - 13M product documents
  - (product schema has thousands of fields).
  - 1 entity shard per node
  - 135M entity documents
  - (dozens of entity data types, each with several fields).
- approximately the same amount of data
  - product index is 25GB on disk.
  - entity index is 12GB on disk.
  - All data loads into about 20GB baseline RAM.
- the exact same logical configuration
  {
      product: {
          settings: {
              index.translog.flush_threshold_size: 500mb
              index.refresh_interval: 30s
              index.number_of_replicas: 1
              index.translog.disable_flush: false
              index.version.created: 190999
              index.number_of_shards: 5
              index.routing.allocation.total_shards_per_node: 1
              index.translog.flush_threshold_period: 60m
              index.translog.flush_threshold_ops: 5000
          }
      }
      entity: {
          settings: {
              index.translog.flush_threshold_size: 500mb
              index.refresh_interval: 30s
              index.number_of_replicas: 1
              index.translog.disable_flush: false
              index.version.created: 190999
              index.number_of_shards: 5
              index.routing.allocation.total_shards_per_node: 1
              index.translog.flush_threshold_period: 60m
              index.translog.flush_threshold_ops: 5000
          }
      }
  }

Here is what our load graphs look like.
Notice the high load ES node. Odd.
![graphite load test graphs](https://f.cloud.github.com/assets/4742707/678545/6ba6f212-d93a-11e2-9d67-abca2035f3e9.gif)

Here is our cluster distribution.
All the shards are similar in size and evenly distributed.
![es head server profile](https://f.cloud.github.com/assets/4742707/678547/72aebffe-d93a-11e2-9a9c-466ca4ab93ed.gif)
</description><key id="15774834">3208</key><summary>Elastic Search Random Node High Load</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">djovic</reporter><labels /><created>2013-06-19T23:47:52Z</created><updated>2013-09-16T23:40:41Z</updated><resolved>2013-06-20T15:59:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-06-20T05:00:50Z" id="19731019">Could you please ask your question on the mailing list and close this issue? http://www.elasticsearch.org/help/
</comment><comment author="djovic" created="2013-06-20T15:59:39Z" id="19762928">Closing issue, as requested.
</comment><comment author="dadoonet" created="2013-06-20T16:21:46Z" id="19764340">Thanks! Feel free to open the discussion on the mailing list. We will be happy to help.
</comment><comment author="djovic" created="2013-06-20T23:37:03Z" id="19790593">Thanks. Reposted on the mailing list here:
https://groups.google.com/forum/#!topic/elasticsearch/Bs3O2cmP84E
</comment><comment author="djovic" created="2013-09-16T23:37:14Z" id="24553654">This problem was resolved with the upgrade to 0.90.x.

DJ
</comment><comment author="kimchy" created="2013-09-16T23:40:41Z" id="24553802">@djovic great!, happy 0.90 resolved it and thanks for taking the time and reporting it back. 0.90 was a quite the major version with many improvements.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Lucene CommonGrams/CommonGramsQuery token fiter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3207</link><project id="" key="" /><description>Both filters merged in a single "common_grams" tokenfilter.

Usage:

``` json
{
    "index":{
        "analysis":{
            "filter":{
                "common_grams_inline":{
                    "type":"common_grams",
                    "common_words":[
                        "brown",
                        "fox"
                    ]
                },
                "common_grams_file":{
                    "type":"common_grams",
                    "common_words_path":"common_words.txt"
                },
                "common_grams_caseless":{
                    "type":"common_grams",
                    "ignore_case":true,
                    "common_words_path":"common_words.txt"
                },
                "common_grams_query_mode":{
                    "type":"common_grams",
                    "query_mode":true,
                    "common_words_path":"common_words.txt"
                }
            }
        }
    }
}
```

The "query_mode" enable the CommonGramsQueryFilter: it removes the single words not already part of a shingle.

Closes #3202
</description><key id="15752591">3207</key><summary>Add Lucene CommonGrams/CommonGramsQuery token fiter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hc</reporter><labels /><created>2013-06-19T15:53:09Z</created><updated>2014-07-16T21:53:08Z</updated><resolved>2013-06-19T16:09:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Thread pools incorrectly rejecting requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3206</link><project id="" key="" /><description>I have two problems that appear to be caused by thread pools incorrectly rejecting threads.

For context, my [cluster settings](https://gist.github.com/peterbourgon/c01be2b7f7ff7b3e8766) and [node stats](https://gist.github.com/peterbourgon/242c579b17ebd2a0a589). Important bit is that all of my {bulk,index,search} threadpools are type: fixed and queue_size: -1. My understanding from the [guide](http://www.elasticsearch.org/guide/reference/modules/threadpool/) is that -1 means unbounded. 

(Side note: the documentation is apparently wrong here, unbounded is _not_ the default behavior, when I had queue_size unset, node stats reported capacity: 1k)
1. First symptom is during shard balancing. Shards sometimes get into a state where they are continuously "initializing" or "relocating", and cluster therefore remaining stuck in "yellow" stage for hours/indefinitely. Checking the logs, I see "Failed to perform [bulk/shard] on replica" due to EsRejectedExecutionException ([gist here](https://gist.github.com/peterbourgon/6da550f8458a1e03708f)). So I assume bulk or index requests are getting rejected at the threadpool stage. But how can it be, if both have unbounded queues?
2. Second symptom is more general. My search threadpool reports steadily rising number of rejected requests. Compare [this node stats thread_pool.search.rejected count](https://gist.github.com/peterbourgon/242c579b17ebd2a0a589#file-gistfile1-txt-L78) with [the same output a few minutes later](https://gist.github.com/peterbourgon/1a18d63444c52aaad1a1#file-gistfile1-txt-L78). Checking my application logs, I count a lot of 50X's coming back from ES: 74% of all 50X's are EsRejectedExecutionExceptions, and a further 22% are ReduceSearchPhaseException due to EsRejectedExecutionException ([gist here](https://gist.github.com/peterbourgon/e25e328459f90247ea85)). So, seems like same error(s) as above, except against search threadpool.

**tl;dr**: are EsRejectedExecutionExceptions always from thread pool queue rejection? If so, why are unbounded thread pools rejecting requests (should never happen AFAIK)? If not, what else could be causing my symptoms?
</description><key id="15743006">3206</key><summary>Thread pools incorrectly rejecting requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">peterbourgon</reporter><labels /><created>2013-06-19T12:59:40Z</created><updated>2014-08-08T12:53:02Z</updated><resolved>2014-08-08T12:32:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hc" created="2013-06-19T20:48:49Z" id="19713804">If I read [the threadpool source code](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L482-L492) right, the doc seems wrong about the "-1" value. Setting "-1" is exactly like setting "0", so it would be like having no queue at all.
</comment><comment author="peterbourgon" created="2013-06-24T08:00:31Z" id="19893665">Well, that would certainly explain my symptoms.
</comment><comment author="kimchy" created="2013-06-25T11:45:39Z" id="19970565">actually, if you set it to -1 it will mean an unbounded queue, so I am unsure why you get rejected failures. Can you share the nodes info output with the thread_pool flag and your thread pool configuration?
</comment><comment author="peterbourgon" created="2013-06-25T12:06:32Z" id="19971482">I believe I linked to both of those outputs in my original post. Are you looking for something different?
</comment><comment author="hc" created="2013-06-25T12:47:20Z" id="19973370">Are you sure about the unbounded queue? To me it seems to be a SynchronousQueue when value is less than 1
</comment><comment author="kimchy" created="2013-06-25T12:55:28Z" id="19973799">ahh, I confused unbounded threads with unbounded queue in this thread, sorry... . Yea, it means that the queue will be bounded and size of 1. I think we can support queueSize &lt; 0 and then use unbounded one.
</comment><comment author="clintongormley" created="2014-08-08T12:32:45Z" id="51594686">I don't think we want unbounded queues at all, as it would just overwhelm the node.  Closing this issue. Please reopen if you can think of a good reason.
</comment><comment author="peterbourgon" created="2014-08-08T12:53:02Z" id="51596298">At a minimum, it should be possible to choose between immediately dropping the request (which, unless things have changed since I filed the issue, is the current behavior) and blocking the request until a queue slot is available.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose fielddata "fields" param in standard in indicesStatsRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3205</link><project id="" key="" /><description>The standard `/_stats/` request accepts `?fielddata` in the query string, but not 
`?fielddata&amp;fields=foo,bar`
</description><key id="15739348">3205</key><summary>Expose fielddata "fields" param in standard in indicesStatsRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-19T11:18:33Z</created><updated>2013-06-19T11:22:19Z</updated><resolved>2013-06-19T11:22:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>"fielddata" qs param setting idCache, not fieldData</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3204</link><project id="" key="" /><description /><key id="15737512">3204</key><summary>"fielddata" qs param setting idCache, not fieldData</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-19T10:32:09Z</created><updated>2013-06-19T10:33:08Z</updated><resolved>2013-06-19T10:33:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>min_score does not work with HasChild queries (0.90.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3203</link><project id="" key="" /><description>how to replicate:

Create index test
curl -XPOST &#8216;http://localhost:9200/test&#8217;

Parent docs
curl -XPOST 'http://localhost:9200/test/testP/1' -d '{"yes" : "1"}'
curl -XPOST 'http://localhost:9200/test/testP/2' -d '{"yes" : "2"}'

Create child mapping
curl -XPUT 'http://localhost:9200/test/testC/_mapping' -d '{"testC" : {"type" : "object", "_parent" : {"type" : "testP"}}}'

Child docs
curl -XPOST 'http://localhost:9200/test/testC?parent=1' -d '{"yes" : "2"}'
curl -XPOST 'http://localhost:9200/test/testC?parent=1' -d '{"yes" : "2"}'
curl -XPOST 'http://localhost:9200/test/testC?parent=1' -d '{"yes" : "2"}'
curl -XPOST 'http://localhost:9200/test/testC?parent=2' -d '{"yes" : "2"}'

Query returns empty:
{
  "min_score": 1,
  "query": {
    "has_child": {
      "query": {
        "constant_score": {
          "filter": {
            "match_all": {}
          }
        }
      },
      "child_type": "testC",
      "score_type": "sum"
    }
  }
}
</description><key id="15716254">3203</key><summary>min_score does not work with HasChild queries (0.90.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">gukjoon</reporter><labels><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-18T22:19:02Z</created><updated>2013-06-25T22:16:51Z</updated><resolved>2013-06-25T11:38:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-25T10:25:20Z" id="19966941">I can reproduce this issue. I will fix it soon.

The reason it breaks is that in the context rewrite phase the minimum score is also active, but it shouldn't, the minimum score should be checked when the scores of the child scores have been aggregated.
</comment><comment author="gukjoon" created="2013-06-25T22:16:51Z" id="20011670">Awesome. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for Lucene Common Grams token filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3202</link><project id="" key="" /><description>It's like the Shingle tokenfilter but only for common words.
</description><key id="15712603">3202</key><summary>Add support for Lucene Common Grams token filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">hc</reporter><labels><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-18T21:06:05Z</created><updated>2013-06-20T22:13:01Z</updated><resolved>2013-06-19T16:09:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hc" created="2013-06-18T21:13:10Z" id="19642569">Here a first commit: https://github.com/hc/elasticsearch/commit/7c99cf4a93e4b49cf7316d0896529f5b96a210b9

I merged CommonGrams and CommonGramsQuery in a single filter with a "query_mode" parameter to switch between them. I did some refactoring in Analysis.java to avoid code duplication for words list loading.

I probably missed some things, let me know :)
</comment><comment author="jpountz" created="2013-06-19T07:30:47Z" id="19667231">Hi C&#233;dric. I left a couple of comments on your commit, it looks very good!
</comment><comment author="hc" created="2013-06-19T13:43:43Z" id="19684534">Hello, thanks for your comments.

I renamed the "words" parameter to "common_words", and made it mandatory. Also removed the unused method and fixed the package name: https://github.com/hc/elasticsearch/commit/e623ccd17d6d9fc434d6e441053848fabc5d5080 (https://github.com/hc/elasticsearch/commits/common_grams)
</comment><comment author="jpountz" created="2013-06-19T15:11:02Z" id="19690256">This looks ready! Can you squash your commits and add "Closes #3202" at the end of the commit message?
</comment><comment author="jpountz" created="2013-06-19T19:44:53Z" id="19709487">Thank you C&#233;dric!
</comment><comment author="clintongormley" created="2013-06-20T18:15:06Z" id="19772545">Hi @hc 

Any chance we can get some documentation for this? when/how to use it? 

https://github.com/elasticsearch/elasticsearch.github.com/

many thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoHash Filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3201</link><project id="" key="" /><description>Previous versions of the `GeoPointFieldMapper` just stored the actual geohash
of a point. This commit changes the behavior of storing geohashes by storing
the geohash and _all_ its prefixes in decreasing order in the same field. To
enable this functionality the option `geohash_prefix` must be set in the mapping.

This behavior allows to filter `GeoPoints` by their _geohashes_. Basically a
geohash prefix is defined by the filter and all geohashes that match this
prefix will be returned. The `neighbors` flag allows to filter geohashes
that surround the given geohash cell. In general the neighborhood of a
geohash is defined by its eight adjacent cells.

To enable this, the type of filtered fields must be `geo_point` with `geohashes`
and `geohash_prefix` enabled.

For example:

```
curl -XPUT 'http://127.0.0.1:9200/locations/?pretty=true' -d '{
    "mappings" : {
        "location": {
            "properties": {
                "pin": {
                    "type": "geo_point",
                    "geohash": true,
                    "geohash_prefix": true
                }
            }
        }
    }
}'
```

This example defines a mapping for a type `location` in an index `locations`
with a field `pin`. The option `geohash` arranges storing the geohash of
the `pin` field.

To filter the results by the geohash a `geohash_cell` must to be defined.
For example

```
curl -XGET 'http://127.0.0.1:9200/locations/_search?pretty=true' -d '{
    "query": {
        "match_all":{}
    },
    "filter": {
        "geohash_cell": {
            "field": "pin",
            "geohash": "u30",
            "neighbors": true
        }
    }
}'

```

This filter will match all geohashes that start with one of the following
prefixes: `u30`, `u1r`, `u32`, `u33`, `u1p`, `u31`, `u0z`, `u2b` and `u2c`.

Internally the `GeoHashFilter` is either a simple `TermFilter`, in case no
neighbors should be filtered or a `BooleanFilter` combining the `TermFilters`
of the geohash and all its neighbors.

Closes #2778
Closes #3218
</description><key id="15690152">3201</key><summary>GeoHash Filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-18T14:13:34Z</created><updated>2014-07-16T21:53:08Z</updated><resolved>2013-06-19T12:45:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-23T20:03:21Z" id="19880145">@chilling can you open a corresponding issue describing this change and tagging it with the relevant version tag?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch Highlighting problem (0.90.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3200</link><project id="" key="" /><description>I am working on  upgradion of elasticsearch from 0.20.2 to 0.90.1 and come across the following issue.
Elasticsearch Highlighting was working fine(getting results as we expected) in 0.20.2, but the same doesn&#8217;t work in ES 0.90.1 "type": "pattern",

Steps to reproduce the issue

Environment:
JDK 1.7,Windows 7, elasticsearch 0.90.1, used elasticsearch head plugin to create/query documents

Step 1:-
Defined mappings and settings for index(test_hightlight)/type(hightlight).

http://localhost:9200/test_hightlight   [POST]

{
  "settings": {
    "index": {
      "number_of_shards": 6,
      "number_of_replicas": 2,
      "analysis": {
        "analyzer": {
          "CommaAnalyzer": {
            "type": "pattern",
            "flags": "DOTALL",
            "lowercase": "true",
            "pattern": "\,",
            "stopwords": "_none_"
          }
        }
      }
    }
  },
  "mappings": {
    "hightlight": {
      "properties": {
        "documentName": {
          "analyzer": "CommaAnalyzer",
          "type": "string"
        },
        "description": {
          "analyzer": "CommaAnalyzer",
          "type": "string"
        }
      }
    }
  }
}

Step2: 
Indexed following documents to newly created Index.

```
http://localhost:9200/test_hightlight/hightlight/1001  [POST]
{
    "documentName":"business Contract JSON business vendor and rep credentialing program ensures that Kutti Kumar and reps you are doing business with meet your requirements and are sound business partners With the business Small Business Package you can be where the buyers business are Not only do you get Business access to more than Business 1800 business hospitals you can BUSINESS promote your business in the only credentialed supplier sourcing tool used by Business healthcare organizations across business the country",
    "description":"Manage Kutti Kumar access and influence permissions Monitor Kutti Kumar sanction and financial details"

}

http://localhost:9200/test_hightlight/hightlight/1002  [POST]
{
    "documentName":"business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for you Kutti Kumar",
    "description":"notifications and management enable you to have better insight to your business Kutti Kumar"

}

http://localhost:9200/test_hightlight/hightlight/1003  [POST]
{
  "documentName": "Kutti Kumar business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for Kutti Kumar",
  "description": "Kutti Kumar notifications and management enable Kutti Kumar to have better insight to your business"
}
```

Step3:
Executed the following query 

URL:http://localhost:9200/test_hightlight/  
Query:

{
  "timeout": 60000,
  "query": {
    "bool": {
      "must": {
        "query_string": {
          "query": "business",
          "default_operator": "and"
        }
      }
    }
  },
  "explain": false,
  "highlight": {
    "pre_tags": [
      "&lt;em&gt;"
    ],
    "post_tags": [
      "&lt;/em&gt;"
    ],
    "fields": {
      "documentName": {
        "fragment_size": 20,
        "number_of_fragments": 5,
        "fragment_offset": 0
      }
    }
  }
}

Got the following error

{

```
took: 20
timed_out: false
_shards: {
    total: 6
    successful: 3
    failed: 3
    failures: [
        {
            index: test_hightlight
            shard: 3
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][3]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
        {
            index: test_hightlight
            shard: 4
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][4]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
        {
            index: test_hightlight
            shard: 5
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][5]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
    ]
}
hits: {
    total: 3
    max_score: 0.12557761
    hits: [ ]
}
```

}

Note: The same query is working as we expected with Elasticsearch 0.20.2
</description><key id="15682276">3200</key><summary>Elasticsearch Highlighting problem (0.90.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">kuttiKumarv</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-18T11:38:36Z</created><updated>2013-07-08T13:21:04Z</updated><resolved>2013-07-08T12:49:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-18T12:39:57Z" id="19608364">can you provide a full gist that reproduces this problem?
</comment><comment author="kuttiKumarv" created="2013-06-19T05:23:17Z" id="19663698">Elasticsearch Highlighting problem (0.90.1)

I am working on  upgradion of elasticsearch from 0.20.2 to 0.90.1 and come across the following issue.
Elasticsearch Highlighting was working fine(getting results as we expected) in 0.20.2, but the same doesn&#8217;t work in ES 0.90.1 "type": "pattern",

Steps to reproduce the issue

Environment:
JDK 1.7,Windows 7, elasticsearch 0.90.1, used elasticsearch head plugin to create/query documents

Step 1:-
Defined mappings and settings for index(test_hightlight)/type(hightlight).

http://localhost:9200/test_hightlight   [POST]

{
  "settings": {
    "index": {
      "number_of_shards": 6,
      "number_of_replicas": 2,
      "analysis": {
        "analyzer": {
          "CommaAnalyzer": {
            "type": "pattern",
            "flags": "DOTALL",
            "lowercase": "true",
            "pattern": "\,",
            "stopwords": "_none_"
          }
        }
      }
    }
  },
  "mappings": {
    "hightlight": {
      "properties": {
        "documentName": {
          "analyzer": "CommaAnalyzer",
          "type": "string"
        },
        "description": {
          "analyzer": "CommaAnalyzer",
          "type": "string"
        }
      }
    }
  }
}

Step2: 
Indexed following documents to newly created Index.

```
http://localhost:9200/test_hightlight/hightlight/1001  [POST]
{
    "documentName":"business Contract JSON business vendor and rep credentialing program ensures that Kutti Kumar and reps you are doing business with meet your requirements and are sound business partners With the business Small Business Package you can be where the buyers business are Not only do you get Business access to more than Business 1800 business hospitals you can BUSINESS promote your business in the only credentialed supplier sourcing tool used by Business healthcare organizations across business the country",
    "description":"Manage Kutti Kumar access and influence permissions Monitor Kutti Kumar sanction and financial details"

}

http://localhost:9200/test_hightlight/hightlight/1002  [POST]
{
    "documentName":"business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for you Kutti Kumar",
    "description":"notifications and management enable you to have better insight to your business Kutti Kumar"

}

http://localhost:9200/test_hightlight/hightlight/1003  [POST]
{
  "documentName": "Kutti Kumar business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for Kutti Kumar",
  "description": "Kutti Kumar notifications and management enable Kutti Kumar to have better insight to your business"
}
```

Step3:
Executed the following query 

URL:http://localhost:9200/test_hightlight/  
Query:

{
  "timeout": 60000,
  "query": {
    "bool": {
      "must": {
        "query_string": {
          "query": "business",
          "default_operator": "and"
        }
      }
    }
  },
  "explain": false,
  "highlight": {
    "pre_tags": [
      "&lt;em&gt;"
    ],
    "post_tags": [
      "&lt;/em&gt;"
    ],
    "fields": {
      "documentName": {
        "fragment_size": 20,
        "number_of_fragments": 5,
        "fragment_offset": 0
      }
    }
  }
}

Got the following error

{

```
took: 20
timed_out: false
_shards: {
    total: 6
    successful: 3
    failed: 3
    failures: [
        {
            index: test_hightlight
            shard: 3
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][3]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
        {
            index: test_hightlight
            shard: 4
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][4]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
        {
            index: test_hightlight
            shard: 5
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][5]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
    ]
}
hits: {
    total: 3
    max_score: 0.12557761
    hits: [ ]
}
```

}

Note: The same query is working as we expected with Elasticsearch 0.20.2
</comment><comment author="kuttiKumarv" created="2013-06-19T13:52:04Z" id="19685065"> Hi s1monw,
Windows Environment:
JDK 1.7,Windows 7, elasticsearch 0.90.1, used elasticsearch head plugin to create/query documents
</comment><comment author="jpountz" created="2013-06-21T07:42:02Z" id="19802374">The JSON looks invalid: "pattern": "\," should be replaced by either "pattern": "\," or "pattern": ",".

I tried with both options and didn't manage to reproduce the issue with 0.90.1. Could you provide us with a bash script containing a set of curl commands that always reproduce the problem?
</comment><comment author="kuttiKumarv" created="2013-06-26T10:07:37Z" id="20037459"># Delete previous tests

curl -XDELETE 'http://127.0.0.1:9200/test_hightlight/?pretty=1'

# Step 1:-

# Defined mappings and settings for index(test_hightlight)/type(hightlight).

curl -XPUT 'http://127.0.0.1:9200/test_hightlight/?pretty=1' -d '
{
  "settings": {
    "index": {
      "number_of_shards": 6,
      "number_of_replicas": 2,
      "analysis": {
        "analyzer": {
          "CommaAnalyzer": {
            "type": "pattern",
            "flags": "DOTALL",
            "lowercase": "true",
            "pattern": "\\,",
            "stopwords": "_none_"
          }
        }
      }
    }
  },
  "mappings": {
    "hightlight": {
      "properties": {
        "documentName": {
          "analyzer": "CommaAnalyzer",
          "type": "string"
        },
        "description": {
          "analyzer": "CommaAnalyzer",
          "type": "string"
        }
      }
    }
  }
}
'

# Step2:

# Indexed following documents to newly created Index.

curl -X POST 'http://localhost:9200/test_hightlight/hightlight/1001' -d '
{
        "documentName":"business Contract JSON business vendor and rep credentialing program ensures that Kutti Kumar and reps you are doing business with meet your requirements and are sound business partners With the business Small Business Package you can be where the buyers business are Not only do you get Business access to more than Business 1800 business hospitals you can BUSINESS promote your business in the only credentialed supplier sourcing tool used by Business healthcare organizations across business the country",
        "description":"Manage Kutti Kumar access and influence permissions Monitor Kutti Kumar sanction and financial details"
}'
curl -X POST 'http://localhost:9200/test_hightlight/hightlight/1002' -d '
{
        "documentName":"business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for you Kutti Kumar",
        "description":"notifications and management enable you to have better insight to your business Kutti Kumar"
}'
curl -X POST 'http://localhost:9200/test_hightlight/hightlight/1003' -d '
{
      "documentName": "Kutti Kumar business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for Kutti Kumar",
      "description": "Kutti Kumar notifications and management enable Kutti Kumar to have better insight to your business"
}'

#Step3:
#Executed the following query 
curl -XGET 'http://127.0.0.1:9200/test_hightlight/hightlight/_search?pretty=1' -d '
{
  "timeout": 60000,
  "query": {
    "bool": {
      "must": {
        "query_string": {
          "query": "business",
          "default_operator": "and"
        }
      }
    }
  },
  "explain": false,
  "highlight": {
    "pre_tags": [
      "&lt;em&gt;"
    ],
    "post_tags": [
      "&lt;/em&gt;"
    ],
    "fields": {
      "documentName": {
        "fragment_size": 20,
        "number_of_fragments": 5,
        "fragment_offset": 0
      }
    }
  }
}'

Got the following error

{

```
took: 20
timed_out: false
_shards: {
    total: 6
    successful: 3
    failed: 3
    failures: [
        {
            index: test_hightlight
            shard: 3
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][3]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
        {
            index: test_hightlight
            shard: 4
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][4]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
        {
            index: test_hightlight
            shard: 5
            status: 500
            reason: FetchPhaseExecutionException[[test_hightlight][5]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; 
        }
    ]
}
hits: {
    total: 3
    max_score: 0.12557761
    hits: [ ]
}
```

}
</comment><comment author="jpountz" created="2013-07-08T13:21:04Z" id="20604531">@kuttiKumarv Thanks for the detailed steps, I managed to reproduce the issue and fix it!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>suggest api enhancement</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3199</link><project id="" key="" /><description>Hi,

I want to build a custom suggester and plug it into the existing suggest API.
The problem I am facing is that the current suggester extension api dont let me to do my job.

I need to access:
- searcher 
- index 
- shardId 

Current API expose parameters only needed by built-in phrase and term suggester :-(

thanks.
  .   
</description><key id="15676683">3199</key><summary>suggest api enhancement</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rkarakaya</reporter><labels><label>enhancement</label><label>v0.90.6</label><label>v1.0.0.Beta1</label></labels><created>2013-06-18T09:07:19Z</created><updated>2013-09-27T18:21:26Z</updated><resolved>2013-06-18T13:30:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-18T10:15:49Z" id="19602312">At this point we already have an extension mechanism for this in place added in #3089 You have access to the searcher or rather to the index reader which is what you want at the end of the day. I am happy to expose the shard and the index as well, can you explain the usecase a bit more?
</comment><comment author="rkarakaya" created="2013-06-18T11:00:17Z" id="19604145">Current Suggester.java execute API as fallows:

public Suggestion&lt;? extends Entry&lt;? extends Option&gt;&gt; execute(String name, T suggestion, IndexReader indexReader, CharsRef spare) throws IOException;

For example, at this point, I need searcher rather than reader.

My customer wanted me to build a suggester that supports:
- highlight matched tokens
- phrase suggestion. 
- a few concrete product suggestions at the end of the suggested items list. 

As far as I understand, current phrase suggester does not support AND search. 
I mean, for instance  it prompted "word1 word2" but no such document exists in my index that contains both words suggested. May be I am missing something:-)

Also, my customer wants to see more words other than what user typed and simple corrections. ,
So, I decided to build my own suggester using faceting on a field that contains shingles. 

I can create totaly a new rest endpoint for my suggester but it would be awesome to plug it into the existing ES architecture. 
</comment><comment author="s1monw" created="2013-06-18T12:24:26Z" id="19607623">&gt; &gt; For example, at this point, I need searcher rather than reader.

you can simply call new IndexSearcher(reader); The seacher is a lightweight wrapper around the reader.
</comment><comment author="rkarakaya" created="2013-06-18T12:32:17Z" id="19607979">Thank you  Simon.. I didn't know that..

is it possible for you to extend SuggestionContext so that it contains index and shard info as well?
</comment><comment author="s1monw" created="2013-06-18T12:36:48Z" id="19608223">&gt; &gt;  is it possible for you to extend SuggestionContext so that it contains index and shard info as well?

yeah I think this makes sense! I will do that
</comment><comment author="rkarakaya" created="2013-06-18T12:40:46Z" id="19608402">thank you very much.. 
</comment><comment author="s1monw" created="2013-06-18T13:37:09Z" id="19611421">I have't ported this to 0.90 yet since I am waiting for  #3089 to get ported... I will revisit this once we are done portin
</comment><comment author="s1monw" created="2013-09-27T18:21:26Z" id="25265928">backported to `0.90`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for "high_freq" and "low_freq" parameters for Common Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3198</link><project id="" key="" /><description>Add "high_freq"/"low_freq" parameters for "minimum_should_match" parameter.

Closes #3188
</description><key id="15648255">3198</key><summary>Add support for "high_freq" and "low_freq" parameters for Common Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hc</reporter><labels /><created>2013-06-17T18:33:36Z</created><updated>2014-07-09T16:39:29Z</updated><resolved>2013-06-17T18:47:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add support for PatternReplaceCharFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3197</link><project id="" key="" /><description>The PatternReplaceCharFilter allows the use of a regex to manipulate the characters in a string before analysis:

http://lucene.apache.org/core/4_0_0-ALPHA/analyzers-common/org/apache/lucene/analysis/pattern/PatternReplaceCharFilter.html
</description><key id="15630198">3197</key><summary>Add support for PatternReplaceCharFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-17T12:40:38Z</created><updated>2013-06-26T13:47:07Z</updated><resolved>2013-06-26T13:47:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>IndexOutOfBoundsException while call suggest api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3196</link><project id="" key="" /><description>java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.rangeCheck(ArrayList.java:571)
        at java.util.ArrayList.get(ArrayList.java:349)
        at org.elasticsearch.search.suggest.Suggest$Suggestion.reduce(Suggest.java:251)
        at org.elasticsearch.search.suggest.Suggest.reduce(Suggest.java:179)
        at org.elasticsearch.action.suggest.TransportSuggestAction.newResponse(TransportSuggestAction.java:145)
        at org.elasticsearch.action.suggest.TransportSuggestAction.newResponse(TransportSuggestAction.java:60)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.finishHim(TransportBroadcastOperationAction.java:369)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.onOperation(TransportBroadcastOperationAction.java:306)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:265)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:242)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:218)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
</description><key id="15621909">3196</key><summary>IndexOutOfBoundsException while call suggest api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">piaolingxue</reporter><labels><label>bug</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-06-17T08:41:09Z</created><updated>2013-12-16T14:35:53Z</updated><resolved>2013-12-16T14:35:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-17T08:51:26Z" id="19532924">can you provide an example request?
</comment><comment author="piaolingxue" created="2013-06-20T09:24:13Z" id="19739599">On 06/17/2013 04:51 PM, Simon Willnauer wrote:

&gt; can you provide an example request?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elasticsearch/elasticsearch/issues/3196#issuecomment-19532924.
&gt; 
&gt; for example
&gt; run this script will throw Exception 
&gt; like:_java.lang.IndexOutOfBoundsException: Index: 1, Size: 1_
&gt; but if I compile elasticsearch locally, then run the es server and this 
&gt; script, it will not throw exception.
&gt; /#!/bin/bash//
&gt; //
&gt; //echo "delete index[products]"//
&gt; //curl -X DELETE 0:9200/products//
&gt; //echo "create index[products]"//
&gt; //curl -X PUT 0:9200/products -d '//
&gt; //{//
&gt; //    "settings" : {//
&gt; //        "analysis" : {//
&gt; //            "tokenizer" : {//
&gt; //                "ngram_min_1" : {//
&gt; //                    "type" : "nGram",//
&gt; //                    "min_gram" : "1",//
&gt; //                    "max_gram" : "10"//
&gt; //                }//
&gt; //            },//
&gt; //            "analyzer" : {//
&gt; //                "ngram_min_1" : {//
&gt; //                    "type" : "custom",//
&gt; //                    "tokenizer" : "ngram_min_1"//
&gt; //                }//
&gt; //            }//
&gt; //        }//
&gt; //    },//
&gt; //    "mappings" : {//
&gt; //        "product" : {//
&gt; //            "properties" : {//
&gt; //                "ProductId":    { "type": "string", "index": 
&gt; "not_analyzed" },//
&gt; //                "ProductName" : {//
&gt; //                    "type": "string", //
&gt; //                    "index_analyzer": "whitespace",//
&gt; //                    "search_analyzer" : "ngram_min_1"//
&gt; //                }//
&gt; //            }//
&gt; //        }//
&gt; //    }//
&gt; //}'//
&gt; //echo//
&gt; //echo "add docs"//
&gt; //for i in 1 2 3 4 5 6 7 8 9 10 100 101 1000; do//
&gt; //    json=$(printf '{"ProductId": "%s", "ProductName": "%s" }', $i, "My 
&gt; Product $i")//
&gt; //    echo $json//
&gt; //    curl -X PUT 0:9200/products/product/$i -d "$json"//
&gt; //    echo//
&gt; //done//
&gt; //
&gt; //echo//
&gt; //echo 'seggestion with:prodct'//
&gt; //curl -XPOST '0:9200/_suggest' -d '{//
&gt; //    "text" : "prodct",//
&gt; //    "my-seggestion1" : {//
&gt; //        "term" : {//
&gt; //            "field" : "ProductName",//
&gt; //            "suggest_mode" : "always"//
&gt; //        }//
&gt; //    }//
&gt; //}'/
</comment><comment author="martijnvg" created="2013-06-20T12:04:41Z" id="19746926">So you build ES from source based on master or based on the 0.90 branch? What Elasticsearch version did initially use when you ran the shell script?
</comment><comment author="piaolingxue" created="2013-06-21T01:54:43Z" id="19794634">On 06/20/2013 08:05 PM, Martijn van Groningen wrote:

&gt; So you build ES from source based on master or based on the 0.90 
&gt; branch? What Elasticsearch version did initially use when you ran the 
&gt; shell script?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elasticsearch/elasticsearch/issues/3196#issuecomment-19746926.
&gt; 
&gt; I build ES based on the 0.90.1 branch (refs/tags/v0.90.1)
&gt; and I firstly ran the shell script use the 0.90.1 version.

My OS is Ubuntu 12.04&#65292;have you met this problem?
</comment><comment author="piaolingxue" created="2013-06-21T01:56:34Z" id="19794680">On 06/20/2013 08:05 PM, Martijn van Groningen wrote:

&gt; So you build ES from source based on master or based on the 0.90 
&gt; branch? What Elasticsearch version did initially use when you ran the 
&gt; shell script?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub 
&gt; https://github.com/elasticsearch/elasticsearch/issues/3196#issuecomment-19746926.
&gt; 
&gt; and my JVM info as follows:
&gt; java version "1.6.0_27"
&gt; OpenJDK Runtime Environment (IcedTea6 1.12.3) (6b27-1.12.3-0ubuntu1~12.04.1)
&gt; OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)
</comment><comment author="martijnvg" created="2013-06-24T15:48:12Z" id="19915755">@piaolingxue I tried to run your script, but I don't run into the IndexOutOfBoundsException error.
</comment><comment author="gromgull" created="2013-07-24T10:01:37Z" id="21474871">I've also had this exception (v. 0.90.2), it is unfortunately not consistently reproducable, the same query will sometimes work ok, and sometimes break.

I can offer two hints though: 
- I think it only happens when there are at least two terms in the suggest text
- I would guess it's related to using a different field/analyzer for the query and for the suggestions? My content field is a multi_field, once with a snowball stemmer (this one is queried), and once with a standard analyzer for the suggestions. 

I will keep trying to make a contained test that breaks at least sometimes :)
</comment><comment author="s1monw" created="2013-07-24T16:24:13Z" id="21497073">@gromgull is it the same stacktrace as above?
</comment><comment author="gromgull" created="2013-07-25T07:36:17Z" id="21537868">Close, the underlying exception is nearly identical:

``` java
2013-07-24 11:30:03,532][DEBUG][action.search.type       ] [Ghazikhanian, Carter] failed to reduce search
org.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce]
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:177)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:155)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:149)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:346)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:149)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:136)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.elasticsearch.search.suggest.Suggest$Suggestion.reduce(Suggest.java:246)
        at org.elasticsearch.search.suggest.Suggest.reduce(Suggest.java:174)
        at org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:404)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:190)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThen&#63273;FetchAction.java:175)
        ... 8 more
```
</comment><comment author="s1monw" created="2013-12-15T19:49:45Z" id="30618544">Eventually I was able to reproduce this! The reason here seems to be that there is more than one index and the indices share a field with the same name but different analysis chains that produce different number of tokens. It would be great if anybody could confirm that their setup also has more than one index were at least 2 indices share the same field with different analysis chains. I added a commit to a branch with a test that reproduces this issue.
</comment><comment author="s1monw" created="2013-12-15T20:05:42Z" id="30618883">Added a fix for this in the commit above. IMO we should throw an exception and fail the reduce if the size is not the same. @martijnvg @imotov what do you think?
</comment><comment author="imotov" created="2013-12-15T21:45:18Z" id="30622324">Looks good to me. 
</comment><comment author="martijnvg" created="2013-12-16T09:16:30Z" id="30645761">LGTM as well.
</comment><comment author="s1monw" created="2013-12-16T13:06:57Z" id="30659002">@martijnvg @imotov I force pushed another change that also checks the entry keys since we also need to make sure we can merge them. There is also a test that checks it. Can you take another look?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add `doc_as_upsert` option to update api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3195</link><project id="" key="" /><description>Issue for PR #3153.
</description><key id="15620383">3195</key><summary>Add `doc_as_upsert` option to update api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-17T07:55:44Z</created><updated>2013-06-17T12:10:19Z</updated><resolved>2013-06-17T08:28:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added version support to update requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3194</link><project id="" key="" /><description>Moved version handling from RobinEngine into VersionType. This avoids code re-use and makes it cleaner and easier to read.

Closes #3111
</description><key id="15618876">3194</key><summary>Added version support to update requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-06-17T06:56:44Z</created><updated>2014-06-27T02:51:34Z</updated><resolved>2013-06-20T11:48:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>encount a java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3193</link><project id="" key="" /><description>Hi ther,  I post this problem first at google group to found reason of problem.

and now,I think I know where the problem is.

This happes, when I delete all data and changed mappings .

insert data is right.but when search again. ES throws cast exception. 

eg. my time filed is string first time. and delete all data and change time to date.

after that search again will cause exception.

so restart server will solve problem. this is a small bug.u can ignore:).
</description><key id="15618141">3193</key><summary>encount a java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lxbzmy</reporter><labels /><created>2013-06-17T06:21:35Z</created><updated>2015-05-06T16:21:47Z</updated><resolved>2014-07-04T09:50:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-06-17T08:59:35Z" id="19533263">Thanks for the report, I'll look into it.
</comment><comment author="jpountz" created="2013-06-17T10:10:24Z" id="19536332">I couldn't manage to reproduce this issue. Can you make sure you are using Elasticsearch 0.90.1 and provide a test case that demonstrates the problem?
</comment><comment author="lxbzmy" created="2013-06-17T10:33:17Z" id="19537222">I couldn't  reproduce this issue too. But I remember
produce

1 100,000 rows insert  with time field : {time:'Mon Jun 17 2013 08:00:00
GMT+0800'} it is recongize as a string
1. XPUT mapping and tell me
   {"error":"MergeMappingException[Merge failed with failures {[mapper [time]
   of different type, current_type [string], merged_type
   [date]]}]","status":400}
2. XDELETE index.
3. change time format  to {"time":"2013-06-15T16:52:34"}

5 100,000 rows insert.

6  curl -XGET http://localhost:9200/syslog/log1/_search?pretty= -d '
{
  "facets": {
    "time": {
      "date_histogram": {
        "field": "time",
        "interval": "hour"
      }
    }
  }
}'

throw exception.

I will keep in touch .

&gt; I couldn't manage to reproduce this issue. Can you make sure you are using
&gt; Elasticsearch 0.90.1 and provide a test case that demonstrates the problem?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3193#issuecomment-19536332
&gt; .
</comment><comment author="s1monw" created="2013-06-17T10:51:52Z" id="19537948">are you creating the index with the first request or do you explicitly set a mapping?
</comment><comment author="lxbzmy" created="2013-06-17T13:47:03Z" id="19545742">When I found time field is wrong I have already insert 100000 test data.

at this time I can not change field type.

so I delete index. and insert a data. and put a mapping. and insert 100000
data again.

Some one report same problem at google groups.

2013/6/17 Simon Willnauer notifications@github.com

&gt; are you creating the index with the first request or do you explicitly set
&gt; a mapping?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3193#issuecomment-19537948
&gt; .
</comment><comment author="jpountz" created="2013-06-21T07:44:30Z" id="19802435">By any chance, do you have a full stack trace of the problem? (should be in Elasticsearch logs)
</comment><comment author="lxbzmy" created="2013-06-21T08:18:16Z" id="19803521">It seems that I miss some log before this one.
I remember put /syslog/syslog and delte it and reput /syslog/syslog again.
different is I found time filed type is string and date

&gt; [2013-06-15 14:25:03,609][WARN ][bootstrap                ] jvm uses the
&gt; client vm, make sure to run `java` with the server vm for best performance
&gt; by adding `-server` to the command line
&gt; [2013-06-15 14:25:03,609][INFO ][node                     ] [Ape]
&gt; {0.90.1}[5708]: initializing ...
&gt; [2013-06-15 14:25:03,625][INFO ][plugins                  ] [Ape] loaded
&gt; [], sites []
&gt; [2013-06-15 14:25:05,828][INFO ][node                     ] [Ape]
&gt; {0.90.1}[5708]: initialized
&gt; [2013-06-15 14:25:05,828][INFO ][node                     ] [Ape]
&gt; {0.90.1}[5708]: starting ...
&gt; [2013-06-15 14:25:06,062][INFO ][transport                ] [Ape]
&gt; bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/
&gt; 192.168.11.107:9300]}
&gt; [2013-06-15 14:25:09,156][INFO ][cluster.service          ] [Ape]
&gt; new_master [Ape][yn3er67mTN2lIGO7r8uXjw][inet[/192.168.11.107:9300]],
&gt; reason: zen-disco-join (elected_as_master)
&gt; [2013-06-15 14:25:09,203][INFO ][discovery                ] [Ape]
&gt; elasticsearch/yn3er67mTN2lIGO7r8uXjw
&gt; [2013-06-15 14:25:09,234][INFO ][http                     ] [Ape]
&gt; bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/
&gt; 192.168.11.107:9200]}
&gt; [2013-06-15 14:25:09,234][INFO ][node                     ] [Ape]
&gt; {0.90.1}[5708]: started
&gt; [2013-06-15 14:25:09,312][INFO ][gateway                  ] [Ape]
&gt; recovered [0] indices into cluster_state
&gt; [2013-06-15 14:28:19,937][INFO ][cluster.metadata         ] [Ape] [syslog]
&gt; creating index, cause [auto(index api)], shards [5]/[1], mappings []
&gt; [2013-06-15 14:28:20,421][INFO ][cluster.metadata         ] [Ape] [syslog]
&gt; update_mapping [syslog](dynamic)
&gt; [2013-06-15 14:28:22,328][WARN ][http.netty               ] [Ape] Caught
&gt; exception while handling client http traffic, closing connection [id:
&gt; 0x33c77d0c, /192.168.11.110:4421 =&gt; /192.168.11.107:9200]
&gt; java.io.IOException: &#36828;&#31243;&#20027;&#26426;&#24378;&#36843;&#20851;&#38381;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#36830;&#25509;&#12290;
&gt; at sun.nio.ch.SocketDispatcher.read0(Native Method)
&gt; at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)
&gt; at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:202)
&gt; at sun.nio.ch.IOUtil.read(IOUtil.java:169)
&gt; at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
&gt; at
&gt; org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
&gt; at
&gt; org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; at java.lang.Thread.run(Thread.java:662)
&gt; [2013-06-15 14:50:28,000][WARN ][http.netty               ] [Ape] Caught
&gt; exception while handling client http traffic, closing connection [id:
&gt; 0x2c33bfc8, /192.168.11.110:4428 =&gt; /192.168.11.107:9200]
&gt; java.io.IOException: &#36828;&#31243;&#20027;&#26426;&#24378;&#36843;&#20851;&#38381;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#36830;&#25509;&#12290;
&gt; at sun.nio.ch.SocketDispatcher.read0(Native Method)
&gt; at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:25)
&gt; at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:202)
&gt; at sun.nio.ch.IOUtil.read(IOUtil.java:169)
&gt; at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:243)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:64)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
&gt; at
&gt; org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
&gt; at
&gt; org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
&gt; at
&gt; org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; at java.lang.Thread.run(Thread.java:662)
&gt; [2013-06-15 16:55:43,531][INFO ][cluster.metadata         ] [Ape]
&gt; [[syslog]] remove_mapping [syslog]
&gt; [2013-06-15 17:01:10,187][INFO ][cluster.metadata         ] [Ape] [syslog]
&gt; create_mapping [syslog]
&gt; [2013-06-15 17:09:05,906][DEBUG][action.search.type       ] [Ape]
&gt; [syslog][2], node[yn3er67mTN2lIGO7r8uXjw], [P], s[STARTED]: Failed to
&gt; execute [org.elasticsearch.action.search.SearchRequest@1ca9565]
&gt; org.elasticsearch.search.SearchParseException: [syslog][2]:
&gt; query[ConstantScore(_:_)],from[0],size[100]: Parse Failure [Failed to parse
&gt; source
&gt; [{"fields":["facility","hostname","id","message","serverity","tag","time","type"],"from":0,"size":100,"query":{"match_all":{}},"facets":{"facility":{"terms":{"field":"facility","size":7}},"hostname":{"terms":{"field":"hostname","size":7}},"id":{"terms":{"field":"id","size":7}},"message":{"terms":{"field":"message","size":7}},"serverity":{"statistical":{"field":"serverity"}},"tag":{"terms":{"field":"tag","size":7}},"time":{"date_histogram":{"field":"time","interval":"hour","post_zone":-8}},"type":{"terms":{"field":"type","size":7}}}}]]
&gt; at
&gt; org.elasticsearch.search.SearchService.parseSource(SearchService.java:573)
&gt; at
&gt; org.elasticsearch.search.SearchService.createContext(SearchService.java:484)
&gt; at
&gt; org.elasticsearch.search.SearchService.createContext(SearchService.java:469)
&gt; at
&gt; org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:462)
&gt; at
&gt; org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:234)
&gt; at
&gt; org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
&gt; at
&gt; org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
&gt; at
&gt; org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:206)
&gt; at
&gt; org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:193)
&gt; at
&gt; org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:179)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
&gt; at
&gt; java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
&gt; at java.lang.Thread.run(Thread.java:662)
&gt; Caused by: java.lang.ClassCastException:
&gt; org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be
&gt; cast to org.elasticsearch.index.fielddata.IndexNumericFieldData
&gt; at
&gt; org.elasticsearch.search.facet.datehistogram.DateHistogramFacetParser.parse(DateHistogramFacetParser.java:162)
&gt; at
&gt; org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:92)
&gt; at
&gt; org.elasticsearch.search.SearchService.parseSource(SearchService.java:561)
&gt; ... 12 more

2013/6/21 Adrien Grand notifications@github.com

&gt; By any chance, do you have a full stack trace of the problem? (should be
&gt; in Elasticsearch logs)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3193#issuecomment-19802435
&gt; .
</comment><comment author="s1monw" created="2013-06-21T08:28:17Z" id="19803897">Yeah I ran into this as well. I think you indexed your field as a string field but then you try to run a DataHistorgramFacet on it, it that possible? We need to throw better exceptions in such a case - this can accidentially load a massive amout of data if you do this on a tokenized string field.
</comment><comment author="lxbzmy" created="2013-06-21T08:53:58Z" id="19804851">I used this web page to see my data :
https://github.com/OlegKunitsyn/elasticsearch-browser
This application automatic query _mapping from ES server.
so I'm sure at that moment es return _mapping show time field is date.(u
konw, I updated mapping )

2013/6/21 Simon Willnauer notifications@github.com

&gt; Yeah I ran into this as well. I think you indexed your field as a string
&gt; field but then you try to run a DataHistorgramFacet on it, it that
&gt; possible? We need to throw better exceptions in such a case - this can
&gt; accidentially load a massive amout of data if you do this on a tokenized
&gt; string field.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3193#issuecomment-19803897
&gt; .
</comment><comment author="jpountz" created="2014-07-04T09:50:27Z" id="48026423">The issue is very likely due to the fact that the field is mapped as a string on one type and as a date on another one, which plays badly with field data.
</comment><comment author="artemredkin" created="2014-10-21T12:26:04Z" id="59919282">&gt; The issue is very likely due to the fact that the field is mapped as a string on one type and as a date on another one, which plays badly with field data
&gt; This seems very strange to me, one cannot have different field mappings between types? Why? Is it because of Lucene? What about doc values?

I've just encountered this issue, some of my types have "id" of type "string", some &#8211; "long". Sometimes (but not always) sorting on this field breaks. And "sort" field in response looks very strange:
`"sort" : [ "P\u0002\u0000\u0000" ]`, where `"id": 97`.
</comment><comment author="haight6716" created="2015-05-05T21:27:40Z" id="99228064">+1  Is it really true we can't use different mappings with different definitions for field names in the same index?

No fix is planned for this?  Could we at least generate an error in this situation (i.e. if I try to add a mapping with a different field definition than another mapping in the same index, I would get an error at that point, not later when I'm trying to do queries on my data).
</comment><comment author="rjernst" created="2015-05-06T08:33:35Z" id="99380243">There is no "fix" possible.  It is simply impossible in Lucene to use the same field name for more than one data type.

An error will be added in #8871.
</comment><comment author="haight6716" created="2015-05-06T16:21:47Z" id="99528206">Thanks for the response @rjernst , we will have to work around it in our application.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose timeout for nodes_info requests in the REST interface</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3192</link><project id="" key="" /><description>The nodes_info action accepts a timeout parameter, but it isn't exposed in the REST interface. It is useful to have to avoid long request when sniffing for nodes.

Closes #3191
</description><key id="15594146">3192</key><summary>Expose timeout for nodes_info requests in the REST interface</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-06-15T17:08:23Z</created><updated>2014-07-16T21:53:10Z</updated><resolved>2013-06-15T17:28:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose timeout in nodes_info REST API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3191</link><project id="" key="" /><description>The nodes_info action accepts a `timeout` parameter, but it isn't exposed in the REST interface.  It is useful to have to avoid long request when sniffing for nodes.
</description><key id="15594024">3191</key><summary>Expose timeout in nodes_info REST API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-15T16:59:41Z</created><updated>2013-06-15T17:28:42Z</updated><resolved>2013-06-15T17:28:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>optimize has_child query when matching parent count is low</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3190</link><project id="" key="" /><description>Currently the has_child query loops over every single document of the parent type looking for the parents matched in child query.  In situations where the child query only matches few parents, this loop is expensive.  

I feel this loop can be eliminated or short-circuited early since we already know the matching parent ids (the keys of the uidToScore map).

I am currently testing a few approaches and will submit a PR when ready.

/cc @martijnvg @s1monw 
</description><key id="15591963">3190</key><summary>optimize has_child query when matching parent count is low</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-15T14:02:34Z</created><updated>2015-06-03T09:50:41Z</updated><resolved>2013-07-18T16:27:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-30T23:07:13Z" id="20257374">I added a benchmark (ChildSearchShortCircutBenchmark) to test the short circuit mechanisms @mattweber came up with and the results look promising. The benchmark inserts 10M parent docs and a bunch of child docs. Child docs are inserted in groups from a group containing 1 doc to 1M docs (each group has double amount of child docs as previous group), this allows to show the performance impact between short circuits in place and no short circuits in place.

Test results without the short circuits:

```
--&gt; has_child filter with 1 parent hits, query Avg: 139ms
--&gt; has_child filter with 2 parent hits, query Avg: 287ms
--&gt; has_child filter with 4 parent hits, query Avg: 448ms
--&gt; has_child filter with 8 parent hits, query Avg: 641ms
--&gt; has_child filter with 16 parent hits, query Avg: 849ms
--&gt; has_child filter with 32  parent hits, query Avg: 1063ms
--&gt; has_child filter with 64 parent hits, query Avg: 1268ms
--&gt; has_child filter with 128 parent hits, query Avg: 1469ms
--&gt; has_child filter with 256 parent hits, query Avg: 1660ms
--&gt; has_child filter with 512 parent hits, query Avg: 1851ms
--&gt; has_child filter with 1024 parent hits, query Avg: 2048ms
--&gt; has_child filter with 2048 parent hits, query Avg: 2236ms
--&gt; has_child filter with 4096 parent hits, query Avg: 2428ms
--&gt; has_child filter with 8192 parent hits, query Avg: 2612ms
--&gt; has_child filter with 16384 parent hits, query Avg: 2803ms
--&gt; has_child filter with 32768 parent hits, query Avg: 3007ms
--&gt; has_child filter with 65536 parent hits, query Avg: 3230ms
```

Test results with the short circuit:

```
--&gt; has_child filter with 1 parent hits, query Avg: 3ms
--&gt; has_child filter with 2 parent hits, query Avg: 6ms
--&gt; has_child filter with 4 parent hits query Avg: 9ms
--&gt; has_child filter with 8 parent hits query Avg: 13ms
--&gt; has_child filter with 16 parent hits query Avg: 17ms
--&gt; has_child filter with 32 parent hits query Avg: 20ms
--&gt; has_child filter with 64 parent hits query Avg: 24ms
--&gt; has_child filter with 128 parent hits query Avg: 28ms
--&gt; has_child filter with 256 parent hits query  Avg: 33ms
--&gt; has_child filter with 512 parent hits query  Avg: 37ms
--&gt; has_child filter with 1024 parent hits query  Avg: 49ms
--&gt; has_child filter with 2048 parent hits query  Avg: 62ms
--&gt; has_child filter with 4096 parent hits query  Avg: 80ms
--&gt; has_child filter with 8192 parent hits query  Avg: 111ms
--&gt; has_child filter with 16384 parent hits query  Avg: 172ms
--&gt; has_child filter with 32768 parent hits query  Avg: 275ms
--&gt; has_child filter with 65536 parent hits query  Avg: 453ms
```
</comment><comment author="mattweber" created="2013-07-01T21:40:31Z" id="20312493">Wow this is even better than what I observed in my tests.  Very cool.
</comment><comment author="martijnvg" created="2013-07-18T16:28:10Z" id="21195953">Thanks @mattweber for bringing this optimisations up!
</comment><comment author="kaliseo" created="2015-06-03T09:50:41Z" id="108278711">i there,

Very interesting.

Do you have and example of your has_child request ?

Thanks a lot !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field data should support more than 2B ordinals per segment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3189</link><project id="" key="" /><description>Field data currently uses integers to store ordinals although a Lucene index can have more than 2B unique values per index. We should use longs instead in the APIs and fix implementations to actually support more than 2B unique values.
</description><key id="15589350">3189</key><summary>Field data should support more than 2B ordinals per segment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-15T09:32:48Z</created><updated>2013-08-09T04:03:44Z</updated><resolved>2013-07-19T07:29:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-07-09T16:38:43Z" id="20687500">I opened a pull request (#3306) which tries to fix this issue. In addition to that, it has some nice memory improvements. Here is the memory usage reported by `LongFieldDataBenchmark` for the whole field data instance (ordinals + data) without and with this commit (you can have a look at #3220 too to see how much memory usage was before we started using the PackedInts API to store values):

&lt;pre&gt;
                               Before      After
SINGLE_VALUED_DENSE_ENUM     488.3 KB   488.3 KB
SINGLE_VALUED_DENSE_DATE       4.3 MB     4.3 MB
MULTI_VALUED_DATE             10.5 MB     5.9 MB
MULTI_VALUED_ENUM              7.8 MB     1.2 MB 
SINGLE_VALUED_SPARSE_RANDOM    3.5 MB     1.5 MB
MULTI_VALUED_SPARSE_RANDOM     7.7 MB     3.4 MB
MULTI_VALUED_DENSE_RANDOM     23.7 MB    17.8 MB
&lt;/pre&gt;

Nothing changes for the single-valued case (as expected) but there are some nice savings for the multi-valued case, especially when the values don't require much space.

I also ran `TermsFacetSearchBenchmark` to see how this impacts faceting, here are the results:

&lt;pre&gt;
Before:
                     name      took    millis
                  terms_s      6.1s        30
              terms_map_s     20.7s       103
                  terms_l     13.8s        69
              terms_map_l       14s        70
                 terms_sm       22s       110
             terms_map_sm      3.3m      1009
                 terms_lm      1.3m       391
             terms_map_lm      1.3m       390
          terms_stats_s_l     31.9s       159
         terms_stats_s_lm        1m       322
         terms_stats_sm_l      4.3m      1319
After:
                  terms_s      5.4s        27
              terms_map_s     20.7s       103
                  terms_l     12.7s        63
              terms_map_l     12.7s        63
                 terms_sm     40.1s       200
             terms_map_sm      3.3m      1015
                 terms_lm      1.6m       486
             terms_map_lm      1.6m       486
          terms_stats_s_l     28.8s       144
         terms_stats_s_lm      1.3m       415
         terms_stats_sm_l      4.3m      1300
&lt;/pre&gt;

In some cases, faceting is slower. I ran the benchmark under a profiler and MonotonicAppendingLongBuffer and AppendingLongBuffer, which are used to store the ordinals, were among the most hot spots. Since they are also the reason why we have these memory savings, maybe it is not that bad?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a minimum_should_match parameter when Common query has only high frequent terms</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3188</link><project id="" key="" /><description>When a Common query has only high frequent terms, they are combined in a boolean MUST query.

I'd like to add the possibility to specify a minimum_should_match for this specific case. Because when I use a very low cutoff_frequency, the boolean fallback query is too restrictive in this case.

It requires to override the buildQuery method in the Lucene CommonTermsQuery class. I called the parameter "high_freq_only_minimum_should_match", but that might be a bit far-fetched... what do you think?
</description><key id="15575722">3188</key><summary>Add a minimum_should_match parameter when Common query has only high frequent terms</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">hc</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-14T20:06:28Z</created><updated>2013-06-28T19:19:18Z</updated><resolved>2013-06-17T18:47:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-14T20:09:15Z" id="19478824">hey man, great to see you are using this feature. I agree this could be tricky if you have a low cutoff. Yet, I am not sure if we should add yet another parameter or just use a disjunction operator if `minimum_should_match` is set. This would make it a bit easier? I'd like to push all the changes we make here upstream as well I think this is good input. Does this query work well for you so far?
</comment><comment author="hc" created="2013-06-14T20:45:26Z" id="19480663">I used to manually export the top terms of my index and do exactly what the CommonTerm query does but in my frontend application. So this new query is a great addition for me.

Do you mean using the same minimum_should_match parameter for both the "main" query and the fallback one? Because I would probably want a higher minimum_should_match for the query with only high terms.

For example if I have a query with only high frequent terms: "How to live your life with her?" I want a more restrictive minimum_should_match than for this query: "A chromosome is an organized structure of DNA and protein"

Something like:

``` json
{ "query" : "...",
"cutoff_frequency" : 0.0002,
"minimum_should_match" : "2/50%",
"high_freq_only_minimum_should_match" : "4/75%" }
```

But it might be too specific to my case.
</comment><comment author="hc" created="2013-06-15T19:44:21Z" id="19502311">Here an attempt at what I meant: https://github.com/hc/elasticsearch/commit/cc4414bc98b7efae6a286d173bd0174ad4e5c005
</comment><comment author="s1monw" created="2013-06-16T11:45:53Z" id="19511269">hey cedric, I see what you mean though. I think this makes sense and I think I am ok with the change on `buildQuery()` - yet I don't think we should introduce yet another parameter on the top level. What do you think about this:

```
{ 
  "query" : "...",
  "cutoff_frequency" : 0.0002,
  "minimum_should_match" : { "low_freq" :  "2/50%", "high_freq" :  "4/75%" }
}
```

and we can also accept this:

```
{ 
  "query" : "...",
  "cutoff_frequency" : 0.0002,
  "minimum_should_match" : "2/50%"
}
```

this is equivalent to: 

```
{ 
  "query" : "...",
  "cutoff_frequency" : 0.0002,
  "minimum_should_match" : { "low_freq" :  "2/50%" }
}
```

which makes this elegant and backwards compatible, makes sense?
</comment><comment author="hc" created="2013-06-17T10:10:58Z" id="19536355">It makes more sense yes, makes it cleaner. I did the changes there : 
https://github.com/hc/elasticsearch/commit/277f999c476796ae16f509ec0f2c566fe1e28251

I don't know if I did it "the right way", let me know.
</comment><comment author="s1monw" created="2013-06-17T11:00:52Z" id="19538294">I commented on the commit... looks good :)
</comment><comment author="hc" created="2013-06-17T13:14:05Z" id="19543685">Commented too, I pushed the changes there: https://github.com/hc/elasticsearch/tree/common_terms_minimum_high
</comment><comment author="s1monw" created="2013-06-17T13:42:49Z" id="19545477">cool looks awesome. I think it's ok to break BW compat on such a new "experimental" query in the Java API. I just thought about this for a while and given those changes I think it would make sense to apply the `highFreqMinimumShouldMatch` also if we have low freq terms. Since the special case we try to solve here is really a common case at the end of the day this would make it consistent and easier to document, does this make sense?
</comment><comment author="hc" created="2013-06-17T14:13:37Z" id="19547480">In my mind, setting the highFreqMinimumShouldMatch is to avoid returning no result. But if the same parameter is also applied to the main query, it might reduce the number of results, no?

In your suggestion, if I am looking for "a lot like protein lipid", with "4/75%" as earlier, I am ending with a query where all terms are required? protein, lipid (low freq) + a, lot, like (high freq)

Or I didn't get what you mean :)
</comment><comment author="s1monw" created="2013-06-17T14:43:26Z" id="19549478">&gt; &gt; In my mind, setting the highFreqMinimumShouldMatch is to avoid returning no result. But if the same parameter is also applied to the main query, it might reduce the number of results, no?

not really. so lets take your example `a lot like protein lipid`  you will end up with two queries just like this:

```
{ 
  "query" : "a lot like protein lipid ",
  "cutoff_frequency" : 0.0002,
  "minimum_should_match" : { "low_freq" :  "2/50%" , "high_freq" : "4/75%" }
}
```

which is somewhat equivalent to:

```
bool : {
  must : [ { query: "protein lipid",  "minimum_should_match" :   "2/50%"  } ],
 should : [ {query : "a lot like" ,  "minimum_should_match" :   "4/70%"  }]
}
```

so the high freq part is executed as a `should` while if you don't have any low freq terms it's executed as a `must` so in that case the high freq part will only contribute to the score iff the `minimum_should_match` criteria is met on the should part otherwise you only get the score from the must part, does this make sense now?
</comment><comment author="hc" created="2013-06-17T14:51:46Z" id="19550077">Ah yes you are right, I had my head so much in the special case with only high terms I forgot there were inner boolean queries.

Now that makes sense to me :)

By the way, I just noticed the "highFreqBoost" and "lowFreqBoost" variables, they could also be exposed? Well maybe in another ticket.
</comment><comment author="s1monw" created="2013-06-17T15:53:32Z" id="19554451">&gt; &gt; Now that makes sense to me :)

awesome, can you update the PR? if you updated it can you squash the commits into one so I can pull it in?

&gt; &gt; By the way, I just noticed the "highFreqBoost" and "lowFreqBoost" variables, they could also be exposed? Well maybe in another ticket.

I'd be happy to help you doing this and I agree lets do it in another ticket!

thanks!
</comment><comment author="hc" created="2013-06-17T16:31:08Z" id="19556874">I merged my commits : https://github.com/hc/elasticsearch/commit/d4cd1aaf4bbab958d05412324d0269f7907cc4ef
</comment><comment author="s1monw" created="2013-06-17T18:16:40Z" id="19563724">this looks very very cool! thanks man for doing this. I will pull this in soon today or tomorrow. 
Can I convince you that it would be awesome to have this folded back into Lucene as well? I'd be happy to help you working on a port to lucene. We try in general to push all our extension / fixes upstream so 1. everybody benefits from them, 2. we don't have to maintain too much code and 3. we don't diverge from the lucene functionallity. 

If you need help how to get started here is a [guide](http://wiki.apache.org/lucene-java/HowToContribute) or just go ahead and ask me :)
</comment><comment author="s1monw" created="2013-06-17T18:26:51Z" id="19564402">oh one more thing,  can you add a `Closes #3188` as the last line in the commit message and open a pull-request for this so I can easily pull it in! 

thanks!
</comment><comment author="s1monw" created="2013-06-17T18:50:00Z" id="19565988">thanks man!
</comment><comment author="kimchy" created="2013-06-23T20:00:35Z" id="19880096">can this be properly documented on how it can be used in the issue description, so we can update the docs once 0.90.2 is released?
</comment><comment author="hc" created="2013-06-27T07:46:00Z" id="20102187">Added it there: https://github.com/elasticsearch/elasticsearch.github.com/pull/463
</comment><comment author="hc" created="2013-06-28T19:19:18Z" id="20208654">It's backward compatible, no breaking I think
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compress PagedBytesAtomicFieldData's termOrdToBytesOffset.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3187</link><project id="" key="" /><description>Using MonotonicAppendingLongBuffer instead of a GrowableWriter should help
save several bits per value, especially when the bytes to store have similar
lengths.
</description><key id="15568951">3187</key><summary>Compress PagedBytesAtomicFieldData's termOrdToBytesOffset.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-06-14T17:26:14Z</created><updated>2014-07-16T21:53:11Z</updated><resolved>2013-07-23T09:33:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Compress PagedBytesAtomicFieldData's termOrdToBytesOffset</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3186</link><project id="" key="" /><description>MonotonicAppendingLongBuffer is an in-memory append-only data-structure which compresses efficiently monotonically increasing sequence of longs. We would save memory by using it in PagedBytesAtomicFieldData to store the term ordinal -&gt; offset mapping.
</description><key id="15562462">3186</key><summary>Compress PagedBytesAtomicFieldData's termOrdToBytesOffset</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-14T15:04:02Z</created><updated>2013-06-15T07:50:46Z</updated><resolved>2013-06-15T07:50:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Pack the ordinals in field data for single valued fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3185</link><project id="" key="" /><description>In field data ordinals for singe valued fields are always represented as plain integer arrays. Using packed integers to store those ordinals will reduce the memory usage.
</description><key id="15547050">3185</key><summary>Pack the ordinals in field data for single valued fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-14T08:13:56Z</created><updated>2013-06-14T08:17:40Z</updated><resolved>2013-06-14T08:17:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>how to implement Chinese spellchecker?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3184</link><project id="" key="" /><description>for example: 
query: &#30041;&#24471;&#21326;
after spell checker: &#21016;&#24503;&#21326;
</description><key id="15542478">3184</key><summary>how to implement Chinese spellchecker?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">piaolingxue</reporter><labels /><created>2013-06-14T04:58:16Z</created><updated>2013-06-15T08:03:19Z</updated><resolved>2013-06-15T08:03:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pecke01" created="2013-06-14T08:47:04Z" id="19445794">I would look at this plugin: https://github.com/elasticsearch/elasticsearch-analysis-smartcn it brings the Lucene Smart Chinese analyzer into ES

I believe that it is a bit better then the built in support altho I have no experience from comparing them
</comment><comment author="spinscale" created="2013-06-15T08:03:19Z" id="19492945">Please ask questions like this on the mailinglist/google group, as there are **much** more people participating. Most of the people working on issues and bugs here do not speak chinese.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Too many open files issue even after increasing limit to 65K on Ubuntu</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3183</link><project id="" key="" /><description>Still getting a stack trace when I try to copy an index with just 143 entries over to another index.  ulimit -a shows 65K set for the max open files.d

Caused by: java.io.FileNotFoundException: /usr/local/share/elasticsearch/data/elasticsearch/nodes/0/indices/contacts-new/
1/index/_5.fdx (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:233)
        at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.&lt;init&gt;(SimpleFSDirectory.java:71)
        at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.&lt;init&gt;(SimpleFSDirectory.java:98)
        at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.&lt;init&gt;(NIOFSDirectory.java:92)
        at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:79)
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:537)
        at org.apache.lucene.index.FieldsReader.&lt;init&gt;(FieldsReader.java:133)
        at org.apache.lucene.index.SegmentCoreReaders.openDocStores(SegmentCoreReaders.java:234)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:118)
        at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:696)
        at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:654)
        at org.apache.lucene.index.DirectoryReader.&lt;init&gt;(DirectoryReader.java:142)
        at org.apache.lucene.index.ReadOnlyDirectoryReader.&lt;init&gt;(ReadOnlyDirectoryReader.java:36)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:451)
        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:399)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:296)
        at org.apache.lucene.search.SearcherManager.&lt;init&gt;(SearcherManager.java:82)
        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1428)
        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:271)
        ... 6 more
[2013-06-14 04:45:28,671][WARN ][cluster.action.shard     ] [Emplate] sending failed shard for [contacts-new][1], node[gbLiKgx3T0qcVQpmsmj3gQ], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[contacts-new][1] failed recovery]; nested: EngineCreationFailureException[[contacts-new][1] failed to open reader on writer]; nested: FileNotFoundException[/usr/local/share/elasticsearch/data/elasticsearch/nodes/0/indices/contacts-new/1/index/_5.fdx (Too many open files)]; ]]
</description><key id="15542367">3183</key><summary>Too many open files issue even after increasing limit to 65K on Ubuntu</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">samonderous</reporter><labels /><created>2013-06-14T04:50:07Z</created><updated>2013-06-14T06:31:40Z</updated><resolved>2013-06-14T06:31:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="samonderous" created="2013-06-14T05:01:00Z" id="19439390">Ran into this even with 128K max open file limit.
</comment><comment author="spinscale" created="2013-06-14T05:48:08Z" id="19440356">Can you please check via `http://localhost:9200/_cluster/nodes?process` if the elasticsearch process is really allowed to open that much file descriptors. Check the `max_file_descriptors` parameter.

Maybe your operating system sets the resource limits differently (for example via PAM on linux)
</comment><comment author="samonderous" created="2013-06-14T06:31:40Z" id="19441393">Absolutely correct.  The max descriptor counts were not increasing.  Went through a lot of pain to get that to update.  I had to specify root in limits.conf since wildcard \* doesn't include root and I was running as root.  I also added ulimit -n 125000 in the elasticsearch.in.sh file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add isolation support for plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3182</link><project id="" key="" /><description>When loaded from /plugins directory, plugins main jars and libraries are all added to main ClassLoader during the PluginsService initialisation. As a consequence, plugins share same resources and classes and thus could not use different, maybe conflicting versions of the same dependency.

This pull request adds support for loading each plugins in its own isolated ClassLoader that uses local class definitions first. This feature may be activated using the "plugin.isolate" boolean setting.

More details on the discovery case and on a working demo here : http://lbroudoux.wordpress.com/2013/06/13/plugin-isolation-support-in-elasticsearch/

Regards, 
</description><key id="15530451">3182</key><summary>Add isolation support for plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lbroudoux</reporter><labels /><created>2013-06-13T21:12:01Z</created><updated>2014-06-13T17:11:41Z</updated><resolved>2014-03-26T08:29:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-22T17:51:25Z" id="19861764">hey, actually, its intentional that there is no isolation in terms of class loaders in plugins. Not to say that its impossible, its just something that is going to be quite complex to implement properly. Such an isolation will need to deal with a lot of aspects, that your solution for example doesn't solve, for example, in your solution, if a Lucene class is used, it will be loaded twice (so a solution similar to web containers need to be provided, with shared class loaders, ...), or if a plugin introduces a class that is serialized over the network, the thread local class loader needs to be set based on the plugin (and if there are 2 contributing to the same request, its practically impossible).

It ends up being quite complicated to support, I don't think (at least not now) that this is something we should tackle... . Specifically since its the first time I heard of it (it might happen, agreed, but seems to be rare with the type of plugins developed for ES).
</comment><comment author="lbroudoux" created="2013-06-25T22:25:50Z" id="20012178">Hello, 

I also thought my solution was a little quick for such a problem ... Indeed I've forgot the thread local class loader stuffs during plugin invocation and come back today with a fix for this. The idea here is to proxy plugin and switch current thread class loader based on the plugin one, during plugin execution and then reset back to origin. I've used this trick on another container related project where we met some compatibility issues during loaded plugins invocation. This one may be optimized, let me know if you have any track to follow ...

On the first topic you mention, I'm not sure to understand what you're saying about 

&gt; ... in you solution, if a Lucene class is used, it will be loaded twice ...

because that was the main point of my proposition : use plugin classloader for plugin related classes but delegate to parent classloader for jvm, elasticsearch and its dependencies classes (each plugin classloader having the same parent as shared classloader). Thus, I addded some logs on 2 plugins and it actually seems that the same definition of a core Lucene class is available in both. Here are some logs that outputs a Lucene class hashCode and its classloader :

``` javascript
[2013-06-25 23:40:12,774][DEBUG][com.github.lbroudoux.elasticsearch.river.drive.river.DriveRiver] [Sleepwalker] [google-drive][drivedocs] Lucene core class is 593224478
[2013-06-25 23:40:12,774][DEBUG][com.github.lbroudoux.elasticsearch.river.drive.river.DriveRiver] [Sleepwalker] [google-drive][drivedocs] Lucene core class CL is sun.misc.Launcher$AppClassLoader@6d6f0472
...
[2013-06-25 23:40:15,782][DEBUG][com.github.lbroudoux.elasticsearch.river.s3.river.S3River] [Sleepwalker] [amazon-s3][s3docs] Lucene core class is 593224478
[2013-06-25 23:40:15,782][DEBUG][com.github.lbroudoux.elasticsearch.river.s3.river.S3River] [Sleepwalker] [amazon-s3][s3docs] Lucene core class CL is sun.misc.Launcher$AppClassLoader@6d6f0472
```

Would you explain me if I missing something ?

That said, I am not a guru on classloader stuffs and have already spent days at debugging JEE apps to see it can be quite a mess to have proper support. However, I think that the case I mention is not that uncommon or **will not be** that uncommon when companies with a big legacy and various sources (such as the one I work for) will try to set up more and more rivers to fed up indices.

Maybe a compromise would be to allow plugin isolation on a plugin basis (default is no isolation) and marking this feature has _experimental_ during a period. This will involve a little more refactoring of the PluginsService class... What do you think ?

Regards,
</comment><comment author="dadoonet" created="2014-03-26T08:29:07Z" id="38659605">Hi @lbroudoux 

@costin opened a new issue for this #5261 
Closing this one now.
</comment><comment author="lbroudoux" created="2014-03-26T08:59:19Z" id="38661562">Hi @dadoonet 

happy to see that what seems to be complicated 10 months ago is now included in 1.x roadmap !!
Just hope my initial contributation has helped a little bit - at least on finding the enabler/disabler property name `plugin.isolate` ;-)
</comment><comment author="jprante" created="2014-04-22T07:59:43Z" id="41013316">Happy to see this coming to ES plugins officially. This is really encouraging plugin development!

I started plugin classloader isolation with maven artifact resolution back in December 2012 https://github.com/jprante/elasticsearch-apps 

And now I hope to finally reuse jars in plugins without class loading issues in the core code. 

Even if Maven artifiacts are still not able to resolve, this is a big step for clean ES production environment setups.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Pull request #3002 creates issue when getting array fields from _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3181</link><project id="" key="" /><description>Pull request #3002 creates issue when getting array fields from _source.

If a field in _source is an array with just one element, the field gets extracted as the element's type and not an array with the element inside.

So, instead of getting this:

``` javascript
{
    "_index": "someindex",
    "_type": "sometype",
    "_id": "someid",
    "_version": 2,
    "exists": true,
    "fields": {
        "tags": [
            {
                "id": "someid",
                "value": "somevalue"
            }
        ]
    }
}
```

I get this:

``` javascript
{
    "_index": "someindex",
    "_type": "sometype",
    "_id": "someid",
    "_version": 2,
    "exists": true,
    "fields": {
        "tags": {
            "id": "someid",
            "value": "somevalue"
        }
    }
}
```

This causes, for example, de-serialization exceptions when trying to convert the JSON "fields" object into a strongly typed class.

Do let me know if I you need any further information or assistance.
</description><key id="15528442">3181</key><summary>Pull request #3002 creates issue when getting array fields from _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">nicolasgarfinkiel</reporter><labels /><created>2013-06-13T20:30:36Z</created><updated>2013-08-01T18:08:11Z</updated><resolved>2013-07-30T12:12:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nicolasgarfinkiel" created="2013-06-17T17:25:36Z" id="19560448">Hey guys, any news on this issue? This is preventing me to upgrade to 0.90.

I would really appreciate if you can give me any feedback on it. Also, if you give me any pointers as to how to tackle this issue, I might try to fix it myself.

Thanks a lot in advance!
</comment><comment author="bleskes" created="2013-06-17T18:13:41Z" id="19563518">Hi Nicolas,

I scheduled some time to look at it tomorrow (European time zone). I'll keep you posted on how it goes.

Cheers,
Boaz
</comment><comment author="nicolasgarfinkiel" created="2013-06-17T18:27:40Z" id="19564450">Thanks Boaz, let me know if I can be of any assistance. Cheers!
</comment><comment author="bleskes" created="2013-06-18T15:24:27Z" id="19618782">Hi Nicolas,

I've looked at this and as simple as it may sound, it has some quite deep ramifications. I will have to flush it out internally a bit here to make sure we do they right thing.

Will keep you posted here.

Cheers,
Boaz
</comment><comment author="nicolasgarfinkiel" created="2013-06-18T15:31:12Z" id="19619235">Yes, that's what I figured and the reason why I didn't attempt to fix it
myself in the first place. :-)

Thanks for your efforts and help!

Cheers,
Nicolas.

On Tue, Jun 18, 2013 at 12:24 PM, Boaz Leskes notifications@github.comwrote:

&gt; Hi Nicolas,
&gt; 
&gt; I've looked at this and as simple as it may sound, it has some quite deep
&gt; ramifications. I will have to flush it out internally a bit here to make
&gt; sure we do they right thing.
&gt; 
&gt; Will keep you posted here.
&gt; 
&gt; Cheers,
&gt; Boaz
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3181#issuecomment-19618782
&gt; .
</comment><comment author="bleskes" created="2013-07-30T12:12:03Z" id="21786430">I've just pushed a new feature that allows you to directly specify whether and what parts of the `_source` should be returned ( see : #3301 ). We needed a dedicated new option as the `fields` one was built to work with lucene stored fields which is why you see those subtle discrepancies. 
</comment><comment author="nicolasgarfinkiel" created="2013-08-01T18:08:10Z" id="21957537">Excelent! Thanks a lot for your help!

On Tue, Jul 30, 2013 at 9:12 AM, Boaz Leskes notifications@github.comwrote:

&gt; I've just pushed a new feature that allows you to directly specify whether
&gt; and what parts of the _source should be returned ( see : #3301https://github.com/elasticsearch/elasticsearch/issues/3301). We needed a dedicated new option as the
&gt; fields one was built to work with lucene stored fields which is why you
&gt; see those subtle discrepancies.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3181#issuecomment-21786430
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update ttl after update document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3180</link><project id="" key="" /><description>Hi everyone. I have a question
why when i update the document, ttl is not update?
logically, if the document was updated, TTL must be changed, too, or not?

``` bash
#!/bin/bash
#
# Demonstrates the use of _ttl
#
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient": {
        "indices.ttl.interval": "1s"
    }
}'
echo
curl -XDELETE localhost:9200/test
echo
curl -XPUT localhost:9200/test -d '{
    "settings": {
    "index.number_of_shards": 1,
    "index.number_of_replicas": 0
    },
    "mappings": {
        "doc": {
            "_ttl" : { "enabled" : true, "default": "10s" },
            "properties": {
                "title": { "type": "string" }
            }
        }
    }
}'
echo
curl -XPUT localhost:9200/test/doc/1 -d '{"title": "rec 1 (to be updated)"}'
echo
curl -XPUT localhost:9200/test/doc/2 -d '{"title": "rec 2 (should expire)"}'
echo
curl -XPUT localhost:9200/test/doc/3 -d '{"title": "rec 3 (should expire)"}'
echo
curl -XPOST localhost:9200/test/_refresh
echo
echo "sleep 5"
sleep 5
echo
echo "Search should return all"
curl -XGET "localhost:9200/test/_search?pretty=true"
echo
echo "Update 1"
curl -XPOST 'localhost:9200/test/doc/1/_update' -d '{"doc" : {"title" : "new title"}}'
curl -XPOST localhost:9200/test/_refresh
echo
echo "Search should return all"
curl -XGET "localhost:9200/test/_search?pretty=true&amp;fields=_source,_ttl"
echo
echo "sleep 10"
sleep 10
echo
echo "Search should return only 1"
curl -XGET "localhost:9200/test/_search?pretty=true&amp;fields=_source,_ttl"

```
</description><key id="15519851">3180</key><summary>Update ttl after update document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">max-inetgiant</reporter><labels /><created>2013-06-13T18:02:09Z</created><updated>2013-07-23T20:44:03Z</updated><resolved>2013-07-23T08:32:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-06-13T21:21:19Z" id="19425902">I  think you should close this one and ask this kind of question in the mailing list.

Have a look at update API (http://www.elasticsearch.org/guide/reference/api/update/) documentation at the end of the page.

If you don't explicitly update the TTL value, just updating the document won't update its TTL. 
</comment><comment author="max-inetgiant" created="2013-06-14T00:13:18Z" id="19432889">why ttl does not update automatically after update? where is logic?
who can explain to me?
</comment><comment author="spinscale" created="2013-07-23T08:32:08Z" id="21400035">Imagine you are running an auction based site and index auctions. If you just found a typo, you may not want to update the TTL of your document, but rather fix the typo. This prevents you from doing the calculation when this document is supposed to be expired again.

As elasticsearch cannot know what kind of data you indexed and how your expiration policy is, it has to leave this decision to the user.
</comment><comment author="max-inetgiant" created="2013-07-23T20:33:24Z" id="21443854">ok,  can you add parameter "autoupdate" : true|false?

``` json
{
  "product": {
    "dynamic": true,
    "_ttl": {
      "autoupdate": true,
      "enabled": true,
      "default": 864000000
    },
    "properties": {
      "title": {
        "type": "string",
        "index": "not_analyzed"
      }
    }
  }
}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change default values for "low/high_freq_operator" to "or" for "common" queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3179</link><project id="" key="" /><description>It seems more logical to me rather than keep "and"

Closes #3178
</description><key id="15517498">3179</key><summary>Change default values for "low/high_freq_operator" to "or" for "common" queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hc</reporter><labels /><created>2013-06-13T17:18:00Z</created><updated>2014-06-19T00:37:23Z</updated><resolved>2013-06-14T09:09:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-06-14T09:09:36Z" id="19446740">Merged!  Many thanks for the commit
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Common terms query low/high_freq_operator is "and" by default, but the doc says it's "or"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3178</link><project id="" key="" /><description>I guess it should be as the doc says, "or" by default, as for MatchQuery. The examples are also implying the default value is "or"
</description><key id="15515318">3178</key><summary>Common terms query low/high_freq_operator is "and" by default, but the doc says it's "or"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hc</reporter><labels /><created>2013-06-13T16:39:09Z</created><updated>2013-06-14T09:09:04Z</updated><resolved>2013-06-14T09:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NPE in query execution of boolean filter in 0.90.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3177</link><project id="" key="" /><description>Attempting to upgrade to 0.90.1 (from 0.90.0) and seeing a new NPE for certain queries (specific generated boolean query with `not` section).

Details below -- happy to provide more information to help track down but haven't had any luck so far reducing this to simpler case.

The error is:

```
[2013-06-13 11:04:12,411][DEBUG][action.search.type       ] [Mop Man] [product-development-1][0], node[2njk8xyQQUSqmjVuG51JiA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4e299813]
org.elasticsearch.search.query.QueryPhaseExecutionException: [product-development-1][0]: query[filtered(ConstantScore(+QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(properties.name.id:Product Category) +cache(properties.value.id:13351)))-&gt;cache(_type:__properties))) QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(categories.name.id:Product Category) +cache(categories.value.id:13351)))-&gt;cache(_type:__categories))) +NotFilter(QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(properties.name.id:ProductName) +cache(properties.value.id:drive converter)))-&gt;cache(_type:__properties))) QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(categories.name.id:ProductName) +cache(categories.value.id:drive converter)))-&gt;cache(_type:__categories))))))-&gt;cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@953b6c6c)],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:206)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:193)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:179)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.lucene.docset.NotDocIdSet$IteratorBasedIterator.cost(NotDocIdSet.java:168)
    at org.elasticsearch.common.lucene.docset.AndDocIdSet$IteratorBasedIterator.&lt;init&gt;(AndDocIdSet.java:140)
    at org.elasticsearch.common.lucene.docset.AndDocIdSet.iterator(AndDocIdSet.java:82)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:135)
    at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:538)
    at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:133)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:609)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:161)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:482)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:438)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:134)
    ... 9 more
```

Query is:

``` json
{
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "and": [
          {
            "or": [
              {
                "nested": {
                  "path": "properties",
                  "query": {
                    "filtered": {
                      "filter": {
                        "and": [
                          {
                            "term": {
                              "properties.name.id": "Product Category"
                            }
                          },
                          {
                            "term": {
                              "properties.value.id": "13351"
                            }
                          }
                        ]
                      }
                    }
                  }
                }
              },
              {
                "nested": {
                  "path": "categories",
                  "query": {
                    "filtered": {
                      "filter": {
                        "and": [
                          {
                            "term": {
                              "categories.name.id": "Product Category"
                            }
                          },
                          {
                            "term": {
                              "categories.value.id": "13351"
                            }
                          }
                        ]
                      }
                    }
                  }
                }
              }
            ]
          },
          {
            "not": {
              "or": [
                {
                  "nested": {
                    "path": "properties",
                    "query": {
                      "filtered": {
                        "filter": {
                          "and": [
                            {
                              "term": {
                                "properties.name.id": "ProductName"
                              }
                            },
                            {
                              "term": {
                                "properties.value.id": "drive converter"
                              }
                            }
                          ]
                        }
                      }
                    }
                  }
                },
                {
                  "nested": {
                    "path": "categories",
                    "query": {
                      "filtered": {
                        "filter": {
                          "and": [
                            {
                              "term": {
                                "categories.name.id": "ProductName"
                              }
                            },
                            {
                              "term": {
                                "categories.value.id": "drive converter"
                              }
                            }
                          ]
                        }
                      }
                    }
                  }
                }
              ]
            }
          }
        ]
      }
    }
  },
  "fields": [
    "id"
  ]
}
```
</description><key id="15510842">3177</key><summary>NPE in query execution of boolean filter in 0.90.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jredburn</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-13T15:17:15Z</created><updated>2013-06-14T14:18:04Z</updated><resolved>2013-06-14T08:01:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-13T15:55:17Z" id="19401157">thanks for reporting this.... I will get this fixed very soon, it's obvious but I need to add a test first.
</comment><comment author="s1monw" created="2013-06-14T08:10:00Z" id="19444446">the only workaround I can see is to use a Query for your NOT part instead of a filter by wrapping it into a

```
"fquery" : { "query" : { "bool" : { "not" : ... } } }
```

simon
</comment><comment author="jredburn" created="2013-06-14T14:18:04Z" id="19459405">Thanks much for the quick turnaround! We'll likely stick with 0.90.0 for now and look forward to the next release.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch JVM options</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3176</link><project id="" key="" /><description>Hello!

I'm trying to increase elasticsearch indexing performance for logstash. here are my tech specs:

three servers with:
each with two E5520 CPU and 24Gb RAM, RAID10 (4HDD)

Here are java opts:
ES_HEAP_SIZE=12g
ES_JAVA_OPTS="-XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:CMSInitiatingOccupancyFraction=70 -XX:-UseGCOverheadLimit -XX:NewSize=256m -XX:InitialTenuringThreshold=10 -Djava.net.preferIPv4Stack=true -Dnetworkaddress.cache.ttl=7200 -Dnetworkaddress.cache.negative.ttl=2"

Here are configs. master:

cluster.name: "logstash"
node.name: "search-1"
node.master: true
node.data: true
index.number_of_shards: 3
bootstrap.mlockall: true
indices.memory.index_buffer_size: 50%
index.translog.flush_threshold_ops: 50000
threadpool.search.type: fixed
threadpool.search.size: 20
threadpool.search.queue_size: 100

threadpool.index.type: fixed
threadpool.index.size: 60
threadpool.index.queue_size: 200

slave:
cluster.name: "logstash"
node.name: "search-2"
node.master: false
node.data: true
index.number_of_shards: 3
bootstrap.mlockall: true
indices.memory.index_buffer_size: 50%
index.translog.flush_threshold_ops: 50000
threadpool.search.type: fixed
threadpool.search.size: 20
threadpool.search.queue_size: 100

threadpool.index.type: fixed
threadpool.index.size: 60
threadpool.index.queue_size: 200

The maximum amount of logs I reached are 3000 logs per second and lots of gaps approximately 30-60 seconds (garbage collector I guess).

Could anyone help me to reach at least 10000 logs per second? Each log is approximately 170 bytes.
</description><key id="15506730">3176</key><summary>Elasticsearch JVM options</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kayrus</reporter><labels /><created>2013-06-13T14:03:53Z</created><updated>2013-06-13T14:07:20Z</updated><resolved>2013-06-13T14:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-13T14:07:20Z" id="19393895">please ask these kind of questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/elasticsearch)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ElasticSearch connection issues from Pyes client</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3175</link><project id="" key="" /><description>I'm seeing an intermittent connection issue with ElasticSearch from the Pyes client.  I don't think this is a problem with Pyes because I have an ES visualization plugin that also timesout the connection.  Here's the stacktrace I get from Pyes:

[Thu Jun 13 02:48:20 2013] [error]   File "/usr/local/lib/python2.6/dist-packages/pyes/es.py", line 583, in _send_request
[Thu Jun 13 02:48:20 2013] [error]     response = self.connection.execute(request)
[Thu Jun 13 02:48:20 2013] [error]   File "/usr/local/lib/python2.6/dist-packages/pyes/connection_http.py", line 85, in execute
[Thu Jun 13 02:48:20 2013] [error]     self._local.server = server = self._get_server()
[Thu Jun 13 02:48:20 2013] [error]   File "/usr/local/lib/python2.6/dist-packages/pyes/connection_http.py", line 121, in _get_server
[Thu Jun 13 02:48:20 2013] [error]     raise NoServerAvailable
[Thu Jun 13 02:48:20 2013] [error] NoServerAvailable

Restarting ES fixes the problem, only to reappear intermittently again but only after ES has been running for a while.
</description><key id="15492722">3175</key><summary>ElasticSearch connection issues from Pyes client</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">samonderous</reporter><labels /><created>2013-06-13T07:55:45Z</created><updated>2013-06-13T17:32:30Z</updated><resolved>2013-06-13T17:32:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="samonderous" created="2013-06-13T08:08:32Z" id="19377710">And this is what is in the elasticsearch.log:

[2013-06-13 08:05:57,899][WARN ][netty.channel.socket.nio.AbstractNioSelector] Failed to accept a connection.
java.io.IOException: Too many open files
</comment><comment author="samonderous" created="2013-06-13T08:10:36Z" id="19377786">More detailed:

[2013-06-13 08:05:57,899][WARN ][netty.channel.socket.nio.AbstractNioSelector] Failed to accept a connection.
java.io.IOException: Too many open files
        at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
        at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:163)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.process(NioServerBoss.java:100)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:313)
        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
</comment><comment author="pecke01" created="2013-06-13T08:10:53Z" id="19377797">Need to raise the amount of file descriptors on your machine then. Recommended is 64k. Read more about it here: http://www.elasticsearch.org/guide/reference/setup/installation/
</comment><comment author="samonderous" created="2013-06-13T17:32:17Z" id="19409082">Yes, this was the issue.  Ubuntu Lucid only comes with 1024 as default max fd's.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timestamp index settings incorrectly stored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3174</link><project id="" key="" /><description>When setting a timestamp field as not indexed, the value is not saved correctly in the cluster state.

Using the following index template:

```
{
    "foo_template": {
        "template": "foo-*",
        "settings": {
            "index.number_of_shards": 1,
            "index.number_of_replicas": 0
        },
        "mappings": {
            "type1": {
                "_timestamp" : { "enabled" : true, "index": "no", "store": "yes"},
                "properties": {
                    "test": {"type" : "long", "index" : "no"},
                    "baz": {"type" : "boolean", "index" : "no"}
                }
            }
        }
    }
}
```

Creating a new index (foo-test) with that template works correctly. The mapping returned is as follows:

```
{
  "foo-test": {
    "type1": {
      "_timestamp": {
        "enabled": true,
        "index": false,
        "store": true
      },
      "properties": {
        "baz": {
          "type": "boolean",
          "index": "no"
        },
        "test": {
          "type": "long",
          "index": "no"
        }
      }
    }
  }
}
```

Notice that the values for the timestamp field now uses boolean values instead of yes/no. 
Whenever the cluster state is recovered, the boolean values are still used, causing an exception

```
[2013-06-12 16:34:43,481][INFO ][gateway                  ] [searchnode] recovered [1] indices into cluster_state
[2013-06-12 16:34:43,482][DEBUG][indices.cluster          ] [searchnode] [foo-test] adding mapping [type1], source [{"type1":{"_timestamp":{"enabled":true,"index":false,"store":true},"properties":{"baz":{"type":"boolean","index":"no"},"test":{"type":"long","index":"no"}}}}]
[2013-06-12 16:34:43,482][WARN ][indices.cluster          ] [searchnode] [foo-test] failed to add mapping [type1], source [{"type1":{"_timestamp":{"enabled":true,"index":false,"store":true},"properties":{"baz":{"type":"boolean","index":"no"},"test":{"type":"long","index":"no"}}}}]
org.elasticsearch.index.mapper.MapperParsingException: Wrong value for index [false] for field [_timestamp]
```

The index setting on the timestamp field works correctly on 0.20.0RC1. The biggest change is the toXContent method. The build sets the fields as a boolean: builder.field("index", fieldType.indexed()), which works on other field mappers.

I have not ran the test in a debugger yet, but will do so shortly.

The timestamp field in 0.20.0RC1

```
_timestamp: {
    enabled: true
    index: no
    store: yes
}
```
</description><key id="15480235">3174</key><summary>Timestamp index settings incorrectly stored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brusic</reporter><labels><label>bug</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T23:42:10Z</created><updated>2013-07-15T16:03:31Z</updated><resolved>2013-07-15T16:03:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-06-13T00:09:57Z" id="19363888">The fix should be setting the field as:
builder.field("index", indexTokenizeOptionToString(fieldType.indexed(), fieldType.tokenized()));

Can submit a pull request, but it's a one line change. :) Will do so anyways later on. This issue probably affects the RoutingFieldMapper as well.
</comment><comment author="s1monw" created="2013-06-13T08:27:33Z" id="19378459"> a PR would be awesome maybe including a testcase?
</comment><comment author="brusic" created="2013-06-13T16:21:48Z" id="19402938">This issue is one of those times when the test case will be much longer than the fix.:)

I will attempt do have a PR shortly. This feature is not essential in my system and I am in the middle of a Lucene 4.3/elasticsearch 0.90 upgrade. However, if 0.90.2 will be released soon, I will work on it first. Any word on its release?
</comment><comment author="brusic" created="2013-07-05T18:23:48Z" id="20532585">I finally had time to fix this issue today (day after a holiday is slow around here), and you already fixed it. The fix is easy, but will the current tests simulate the serialization/deserialization of the index settings? Most tests skip this step.
</comment><comment author="brusic" created="2013-07-05T19:16:00Z" id="20534738">The routing field also needs to be fixed:
https://github.com/brusic/elasticsearch/commit/a660fcc53c97f1b47c8dcde08c8edb6dbf4d9e34

(I accidentally created a git branch off another branch, if not I would submit a pull request)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Distributed percolator engine</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3173</link><project id="" key="" /><description>## Background

Redesigning the percolate engine is targeted for version 1.0. The main reason why the rewrite is necessary is that the current perculate engine doesn't scale. The idea is that perculating a document should be executed in the same manner as a distributed search request. 

In the current approach queries are stored in a single primary shard index, that is auto replicated to each data node. This allows the percolation to happen locally. In the case that large amount of queries are index into this `_percolator` index, percolating document just start to take to long. Also all queries are loaded into memory (Map: query uid -&gt; Lucene Query), so in this case heap space issues can occur. On top of this with the current api the query always need to get index into the `_percolator` index and the type is the name of the index the query is percolated for. So scaling out the percolator feature is needed for sharing the percolator execution and memory load.

Because of the fact that percolation will be a distributed request, the perculate option in the index api is scheduled to be removed. The main reason behind this is that we can't block and wait in the index api for a distributed percolate request to complete. The perculate request may take longer to complete then the actual index request (we currently perculate during replication) and thus slowing down the actual index request.

To substitute the percolate while indexing option, one just needs to run percolate api directly after the index api returned. The percolate api will remain to be a realtime api.
## Implementation plan

The percolator index type approach stores the percolate queries in a special `_percolator` type with its own mapping in the same index where the actual data is or in a different index (dedicated percolation index, which might require different sharding behavior compared to the index that holds actual data and being search on). This approach also allows percolator to scale beyond the single shard exection we have today, meaning we both partition the percolated queries, and distribute the percolate execution.

Store a query in the twitter index:

```
curl -XPUT 'localhost:9200/twitter/_percolator/p_es' -d '{
    "query" : {
        "match" : {
            "message" : "elasticsearch"
        }
    }
}'
```

Percolating a document uses the same rest end point:

```
curl -XGET 'localhost:9200/twitter/tweet/_percolate' -d '{
    "doc" : {
        "message" : "Bonsai tree in elasticsearch office"
    }
}'
```

The response initially doesn't change. The rest endpoint will also support a routing query string parameter, to allow documents to only be percolated on queries in specific shards. 

During regular searches, we will automatically filter out documents with the `_percolator` type (only if it exists, so its only added as an overhead if explicitly used). We won't filter `_percolator` type if explciitly specified in the search request since users might still want to search and get back the percolated queries.
## Backwards compatibility

The plan is not to keep backwards compatibility with the current percolate implementation. Percolate queries indexed via the old infrastructure will need to be migrated into the new planned infrastructure. The 'old' `_percolate` index won't be removed, so the queries can easily be copied to the new infrastructure by using a scan search request.
## Post redesign

After the redesign has been implemented adding more features to the percolator is next. One of them is to highlight what parts of the query matched with the document. 

The idea is have different response modes. For example:
- `count` - A count of how many queries matched with the document.
- `compact` - Returns a list of query ids that have matched with the document. (just like we do today)
- `verbose` - Returns a body per matched query. This body can for example hold a query highlight in the future.

Here are a few thoughts on post features for percolator:
- Support additional operations such as highlighting
- Allow to do bulk percolation. Two options, simple bulk and use the MemoryIndex to do it one by one, or somehow bulk index the docs into an in memory index (MemoryIndex does't support more than one index, possible RAM based dir?), and then execute the queries against it. Bulk percolation will still need to be distributed and broken down into shard level bulks.
- Support percolating an existing document, by specifying an index, type and id and optionally an version instead of an actual document.
</description><key id="15474471">3173</key><summary>Distributed percolator engine</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T21:12:22Z</created><updated>2014-01-28T06:03:10Z</updated><resolved>2013-07-18T15:10:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="itsadok" created="2013-06-13T03:53:34Z" id="19370718">Currently the queries are executed against the MemoryIndex sequentially, in a loop. Has any thought been given to somehow combining queries, in case there are common sub-queries that are needlessly executed over and over?
</comment><comment author="martijnvg" created="2013-06-13T12:27:35Z" id="19388297">@itsadok I haven't thought about it, but perhaps percolate queries can be structured in a tree like structure, so that only a part of the percolate queries have to be evaluated when percolating a document. 
</comment><comment author="skade" created="2013-07-18T16:42:39Z" id="21196963">Following up on:

https://twitter.com/Argorak/statuses/357893281193017344

I really miss the feature to somehow attach the result of percolation into the document itself. This doesn't necessarily be in the document itself, e.g. a child document would be useful as well. I often use the percolator to categorize incoming data. This data is given by external services and is messy, though fixable by simple "search and clean". We use the percolator to register (sometimes user-created) queries that map those entries to our internal values.

Currently, this works like this:
- Percolate the document
- Write the result to the `categories` field
- Index the document

Allowing this within the percolation step itself would vastly reduce our network overhead in this case and (if bulk percolation happens) also allow us to do bulk actions in one step.
</comment><comment author="martijnvg" created="2013-07-19T10:12:08Z" id="21241897">So It is like a percolate post write operation? That would update a specific part of the percolated documents based on the percolate matches and then index this updated document.

There're no plans for this kind of feature. You can create an issue for this feature if you want, then this idea doesn't get lost.
</comment><comment author="missinglink" created="2013-07-24T14:18:35Z" id="21487757">Was this merged to master? if so what version?
</comment><comment author="martijnvg" created="2013-07-26T15:23:42Z" id="21627701">@missinglink Yes, this is now on master. Will be part of Elasticsearch when 1.0 beta will be released.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make get mapping response consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3172</link><project id="" key="" /><description>The get mapping api response wraps the mappings in a type object and the type objects in an index object. If the mapping for only one index and type is requested the index object is omitted, for the sake of brevity, but this can also be confusing for application parsing it. The top level index object should always be included
</description><key id="15469533">3172</key><summary>Make get mapping response consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T19:34:37Z</created><updated>2013-06-13T08:11:26Z</updated><resolved>2013-06-13T08:11:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve get mapping and warmers api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3171</link><project id="" key="" /><description>Improve the way the get mapping and the get warmer api get their data from the master's copy of the cluster state.

Relates to #3100 
</description><key id="15466722">3171</key><summary>Improve get mapping and warmers api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T18:42:38Z</created><updated>2013-06-12T19:04:55Z</updated><resolved>2013-06-12T19:04:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use CFS in any case if index.compound_format is set to true</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3170</link><project id="" key="" /><description>Lucenes MergePolicies support a noCFSRatio. This commit introduces
support for this ratio via `index.compound_format`. This setting
can parse a boolean value or a value in the interval [0..1] that
is equivalent to the noCFSRatio. The setting `1`, `1.0` and `true`
are equivalent as well as `0`, `0.0` and `false`.

Closes #3166
</description><key id="15463931">3170</key><summary>Use CFS in any case if index.compound_format is set to true</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-06-12T17:46:35Z</created><updated>2014-07-16T21:53:12Z</updated><resolved>2013-06-12T18:46:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NPE in IndexRequest, unguarded method index(String index) (0.90.1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3169</link><project id="" key="" /><description>Just stumbled upon an NPE in 0.90.1. It looks like index(String index) does not check against a null value for `index` parameter.

org.elasticsearch.action.support.replication.ShardReplicationOperationRequest:107

```
    @SuppressWarnings("unchecked")
public final T index(String index) {
    this.index = index;
    return (T) this;
}
```

which yields later an NPE in a bulk request

```
 Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.io.stream.HandlesStreamOutput.writeString(HandlesStreamOutput.java:55)
    at org.elasticsearch.action.support.replication.ShardReplicationOperationRequest.writeTo(ShardReplicationOperationRequest.java:174)
    at org.elasticsearch.action.index.IndexRequest.writeTo(IndexRequest.java:628)
    at org.elasticsearch.action.bulk.BulkRequest.writeTo(BulkRequest.java:466)
    at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:539)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:184)
```

Fix should be trivial.

J&#246;rg
</description><key id="15459873">3169</key><summary>NPE in IndexRequest, unguarded method index(String index) (0.90.1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-06-12T16:24:14Z</created><updated>2014-08-08T12:30:57Z</updated><resolved>2014-08-08T12:30:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-12T16:25:36Z" id="19337456">strange, we are validating it in the validate method..., requires closer look...
</comment><comment author="s1monw" created="2013-06-12T17:02:15Z" id="19339777">@kimchy r u looking into this?
</comment><comment author="javanna" created="2013-11-04T15:54:35Z" id="27695748">@jprante is this still valid? Are you able to reproduce it? As Shay previously said, we usually validate the requests before their serialization, and the `IndexRequest#validate` does contain a null check for `index`
</comment><comment author="jprante" created="2013-12-02T20:14:38Z" id="29653285">@javanna on 0.90.7, I am no longer able to reproduce it. I think it was a spurious exception.

But, as a related issue on 0.90.7, I have created four different test cases for BulkRequest throwing NPE. The test cases are somewhat pathological but maybe there is space for error message improvement.

AbstractNodeTest
https://gist.github.com/jprante/7757319

NPETest
https://gist.github.com/jprante/7757335

NPETest log
https://gist.github.com/jprante/7757406
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MVEL infinite loop in its error handling causing cluster to degrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3168</link><project id="" key="" /><description>Recently I've seen requests trigger a failure, where MVEL gets into an infinite loop trying to report the exception.

This failure causes service to degrade across the entire cluster, causing timeouts on requests handled by all nodes. These may be just for indexes with a shard on the affected machine, but it feels bigger than that.

Shared some logs and a repro with @kimchy.
</description><key id="15457209">3168</key><summary>MVEL infinite loop in its error handling causing cluster to degrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">nz</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T15:37:03Z</created><updated>2013-06-19T10:14:27Z</updated><resolved>2013-06-19T10:14:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-06-19T09:45:57Z" id="19672900">Based on @nz 's report, I've submitted a patch to MVEL to solve this issue (http://jira.codehaus.org/browse/MVEL-292 ) . It is limited in scope to a single line script that start with a new line and trigger an error while executing/parsing (this example, a bad field data access). 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>custom_score could support a filter directly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3167</link><project id="" key="" /><description>custom_score only supports a query, but lot's of scripts probably don't use the `_score` variable anyway. So it makes sense to use a filter (no scoring overhead and better caching).

It would be nice if you could use a `filter` directly instead of wrapping it in a `filtered` query (see an example in #3165), it looks better and could be a little bit more optimized (maybe).
</description><key id="15455584">3167</key><summary>custom_score could support a filter directly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">q42jaap</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T15:07:26Z</created><updated>2013-06-12T20:43:16Z</updated><resolved>2013-06-12T20:43:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Merge Settings are misleading</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3166</link><project id="" key="" /><description>today we have a settings `index.compound_format` that if set to `true` tells lucene to use a compound index format. Yet, this might be confusing to many folks since what lucene does is it only uses CFS if the segment that is written is &lt; 10% of the rest of the index. This is an impl detail of lucene and ES should manage user expectation and use CFS all the time if set to true.
</description><key id="15451380">3166</key><summary>Merge Settings are misleading</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-12T13:44:38Z</created><updated>2013-06-24T10:26:19Z</updated><resolved>2013-06-12T18:46:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-06-13T05:09:31Z" id="19372377">@s1monw Do I understand it correctly that is it possible to update CFS ratio on the fly? Thus switching from/to CFS completely as needed? If yes, how does it work then, it apply only on the new segments? Or it generally influences how segments are merged? In other words, if CFS is turned off and I turn it on, it will probably take some time for the change to take effect - until the next merge?

Anyway, thanks for looking at this!
</comment><comment author="s1monw" created="2013-06-13T13:22:13Z" id="19391145">@lukas-vlcek you can update that ratio in realtime via the API. Yet, the merge policy that is in use at this point will not pick up the changes. You need to execute a flush via the API to enforce a new indexwirter to be opened. Like  this: `curl -XPOST 'http://localhost:9200/index_name/_flush?full=true'` this will cause a new writer to be opened and a new MergePolicy will be created with the new settings. If you want to enforce CFS then you can just run an optimize.
</comment><comment author="lukas-vlcek" created="2013-06-24T10:26:19Z" id="19899229">Thanks, it seems to work fine for me. (Looking forward to the new release).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_parent query with automatic filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3165</link><project id="" key="" /><description>I this poc I'm doing, I'm generating this kind of has_parent filter:

``` json
{
  "facet_filter": {
    "and": {
      "filters": [
        {
          "type": { "value": "prices" }
        },
        {
          "term": { "careType": "LO" }
        },
        {
          "has_parent": { // &lt;== this is the filter I'm talking about
            "type": "accos",
            "query": {
              "filtered": {
                "filter": {
                  "term": {
                    "locationCode": "0512"
                  }
                }
              }
            }
          }
        }
      ]
    }
  }
}
```

I want to filter on parents instead of doing a query (which imposes scoring overhead, which is strange because it's a has_parent filter anyway).

It would be nice to be able to use a filter directly in the has_parent filter:

``` json
{
  "has_parent": {
    "type": "accos",
    "filter": {
      "term": {
        "locationCode": "0512"
      }
    }
  }
}
```

I actually think this would also be useful for the has_parent query or the has_child query.

Martijn's favourite has_child/custom_score to sort on a numeric property of child records, looks like this:

``` json
{
 "query": {
    "filtered": {
      "query": {
        "has_child": {
          "type": "prices",
          "score_type": "max",
          "query": {
            "custom_score": {
              "script": "-doc['pricePerPerson'].value",
              "query": {
                "filtered": {
                  "filter": {
                    "term": { "careType": "LO" }
                  }
                }
              }
            }
          }
        }
      },
      "filter": {
        "term": { "locationCode": "0512" }
      }
    }
  }
}
```

custom_score's query being a filtered query could collapse the filter part into the custom_score query itself, this is another way to write a more consise (and probably also more optimal) query.
</description><key id="15450342">3165</key><summary>has_parent query with automatic filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q42jaap</reporter><labels /><created>2013-06-12T13:22:36Z</created><updated>2013-06-12T15:07:27Z</updated><resolved>2013-06-12T15:04:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Mpdreamz" created="2013-06-12T13:48:08Z" id="19326378">I've often wondered about this too. 
</comment><comment author="martijnvg" created="2013-06-12T13:58:54Z" id="19327147">@q42jaap @Mpdreamz Both the `has_child` and `has_parent` filter already support a filter this was added via:
https://github.com/elasticsearch/elasticsearch/issues/2588

I will update the documentation.
</comment><comment author="q42jaap" created="2013-06-12T14:03:48Z" id="19327481">How about custom_score?
</comment><comment author="martijnvg" created="2013-06-12T14:13:42Z" id="19328147">I think it makes to add a filter option to `custom_score` query, but then the `_score` variable in the script is always 1.
</comment><comment author="q42jaap" created="2013-06-12T15:04:00Z" id="19331747">That's true, but could could combine the `query` and the `filter` in the `custom_score`, that way the _score var would be different from 1.0.

I'll close this one and make another.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase suggest run 100% CPU when giving long sentences</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3164</link><project id="" key="" /><description>Hi,

I run into an issue with the Suggester, using "phrase". Here is a gist, but I have to warn you: your CPU will melt if you do not stop ES :fire:!

https://gist.github.com/damienalexandre/5763504

You can test with:

``` json
{"query": {
    "match" : {
      "_all": {
        "query": "queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen "
      }
    }
   },
  "suggest": {
    "text": "queen",
    "my-suggest": {
      "phrase": {
        "field": "name",
        "analyzer" : "francais"
      }
    }
  }
}
```

And that will works. So the issue is only when the `text` is long. 

To prevent serious issues in production, I have to implement a truncate on the client side, avoiding long user contributed sentences.

I do not know if this is an ES bug or a bad usage from me - using a suggester on a phrase is not clever BUT as this is a public search engine, I can't expect user to use simple search terms.

ES 0.90.1 on Ubuntu 12.04 (but same issue on FreeBSD).
</description><key id="15439688">3164</key><summary>Phrase suggest run 100% CPU when giving long sentences</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">damienalexandre</reporter><labels /><created>2013-06-12T08:25:59Z</created><updated>2013-06-13T13:33:29Z</updated><resolved>2013-06-13T13:33:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-12T10:29:22Z" id="19317537">as a simple workaround you can use a [limit token filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/limit-token-count-tokenfilter/) for now but I agree this is bad! I think we should add a hard limit you can adjust to prevent stuff like this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-field query_string with auto_generate_phrase_queries = true when one of the fields is indexed w/o positions using word delimiter filter [0.90.1]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3163</link><project id="" key="" /><description>When running query_string query on multiple fields, one of which is indexed w/o positions (index_options: docs) and the query string contains a token which is split by word delimiter, if auto_generate_phrase_queries is true, the query fails with `IllegalStateException` (`field was indexed without position data; cannot run PhraseQuery`).

It was working fine in 0.20.x (just checked in 0.20.5), but doesn&#8217;t work in 0.90.1

Looks like it splits a term into sub-terms and tries to form a PhraseQuery from the tokens and apparently fails to run it. In 0.20.x it seems it was ignoring auto_generate_phrase_queries for fields w/o positions data, so it was getting the results from the other fields.

Example:

```
curl -XPOST "http://localhost:9200/phrases-test/" -d '
{
   "settings":{
      "index":{
         "analysis":{
            "analyzer":{
                "generic_text":{
                    "type": "custom",
                    "tokenizer": "whitespace",
                    "filter": ["short_word_delimiter", "lowercase"]
                }
            },
            "filter":{
               "short_word_delimiter":{
                    "type": "word_delimiter",
                    "generate_word_parts": true,
                    "generate_number_parts": true,
                    "catenate_words": true,
                    "catenate_numbers": true,
                    "catenate_all": true,
                    "split_on_case_change": true,
                    "preserve_original": true
               }
            }
         }
      }
   }
}
'

curl -XPOST "http://localhost:9200/phrases-test/test/_mapping" -d '
{
   "test": {
        "properties": {
            "title": {
                "type": "multi_field",
                "fields": {
                    "regular": {
                        "type":"string",
                        "analyzer": "generic_text"
                    },
                    "wo_positions": {
                        "type":"string",
                        "analyzer": "generic_text",
                        "index_options": "docs"
                    }
                }
            }
        }
   }
}
'

curl -XPOST "http://localhost:9200/phrases-test/test/" -d '
{
   "title": "plain box"
}
'
curl -XPOST "http://localhost:9200/phrases-test/test/" -d '
{
   "title":"box 10x20"
}
'
curl -XPOST "http://localhost:9200/phrases-test/test/" -d '
{
   "title":"box 10x30"
}
'
```

When you search in 0.90.1:

```
curl -XPOST "http://localhost:9200/phrases-test/test/_search?pretty=true" -d '
{
   "query":{
        "query_string" : {
            "fields" : ["title.regular", "title.wo_positions"],
            "query" : "10x30",
            "auto_generate_phrase_queries": true
        }
   }
}
'
```

You get the exception:

```
"status" : 500,
"reason" : "QueryPhaseExecutionException[[phrases-test][0]: query[filtered((title.regular:\"(10x30 10) x (30 10x30)\" | title.wo_positions:\"(10x30 10) x (30 10x30)\"))-&gt;cache(_type:test)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalStateException[field \"title.wo_positions\" was indexed without position data; cannot run PhraseQuery (term=10x30)]; "
```

In 0.20.5 you get the expected results:

```
"hits" : {
    "total" : 1,
    "max_score" : 0.76713204,
    "hits" : [ {
      "_index" : "phrases-test",
      "_type" : "test",
      "_id" : "Oa8-V__fRjO6UrfwRZWmDg",
      "_score" : 0.76713204, "_source" :
{
   "title":"box 10x30"
}
    }
```
</description><key id="15424950">3163</key><summary>Multi-field query_string with auto_generate_phrase_queries = true when one of the fields is indexed w/o positions using word delimiter filter [0.90.1]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kostiklv</reporter><labels><label>:Query DSL</label><label>adoptme</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-06-11T22:00:51Z</created><updated>2015-09-19T17:48:04Z</updated><resolved>2015-09-19T17:48:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abaddononion" created="2013-08-27T20:15:40Z" id="23367027">I'm still running into this issue on Elasticsearch 0.90.3, as well. Queries that were working for me on the 0.20.x line involving hyphens (particularly in the logsource field where I'm storing hostnames) are now breaking and throwing this error. =/ 

Is there some way to restructure affected queries to step around this issue that anyone has found?
</comment><comment author="clintongormley" created="2014-11-29T15:32:44Z" id="64955313">Perhaps the solution here is to extend the `lenient` parameter so that phrase queries on fields without positions are rewritten as bool-must instead.
</comment><comment author="clintongormley" created="2015-09-19T17:48:04Z" id="141693199">Revisiting this issue: I don't think we should extend lenient - this can be solved easily on the client side with a bool-should query.

Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>add Dockerfile to easily build elasticsearch into a runnable linux container</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3162</link><project id="" key="" /><description>It would be nice to have a Dockerfile for this project, to be able to start an ElasticSearch container easily.

1) Instal docker `http://docker.io`
2) Download the Dockerfile `wget https://raw.github.com/elasticsearch/elasticsearch/dockerfile/Dockerfile`
3) Build the container using the Dockerfile `docker build - &lt; Dockerfile`
4) Start ElasticSearch with `docker run &lt;imageid&gt;`

That's it!
</description><key id="15404715">3162</key><summary>add Dockerfile to easily build elasticsearch into a runnable linux container</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/ESamir/following{/other_user}', u'events_url': u'https://api.github.com/users/ESamir/events{/privacy}', u'organizations_url': u'https://api.github.com/users/ESamir/orgs', u'url': u'https://api.github.com/users/ESamir', u'gists_url': u'https://api.github.com/users/ESamir/gists{/gist_id}', u'html_url': u'https://github.com/ESamir', u'subscriptions_url': u'https://api.github.com/users/ESamir/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/9035864?v=4', u'repos_url': u'https://api.github.com/users/ESamir/repos', u'received_events_url': u'https://api.github.com/users/ESamir/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/ESamir/starred{/owner}{/repo}', u'site_admin': False, u'login': u'ESamir', u'type': u'User', u'id': 9035864, u'followers_url': u'https://api.github.com/users/ESamir/followers'}</assignee><reporter username="">vieux</reporter><labels><label>:Packaging</label></labels><created>2013-06-11T15:26:29Z</created><updated>2016-03-08T13:00:02Z</updated><resolved>2016-03-08T13:00:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-11T16:41:34Z" id="19274095">hey,

I have never worked with docker at all, but it has to be linux distribution bound, I guess? If it has to be, any reason not to use the debian package?
</comment><comment author="vieux" created="2013-06-11T17:31:43Z" id="19278029">I asked myself the same question, I chose the tar.gz because:

1) the debian package uses init.d which daemonize the process, as the container will live with the process, it's easier to have the forground option

2) if you don't like ubuntu, you can change the FROM to use centos for exemple, and it'll still work

But if you prefer I could use the .deb, let me know.
</comment><comment author="kimchy" created="2013-06-11T17:35:46Z" id="19278305">I wonder if this is the right place for this? should it be in our repo? also, there isn't a lot of configuration options that I woudl love to be able to pass to it (unsure if its possible with docker).
</comment><comment author="shykes" created="2013-06-11T18:33:47Z" id="19282994">Hi @kimchy and @spinscale, jumping in here:
- (re @spinscale) A docker container is actually _not linux distribution bound_. The same container will run on any distro, as long as its kernel is supported. Of course, a container may _include_ system libraries, binaries and configuration from the developer's distro of choice. But it may also be built on nothing more than a busybox stub. Regardless of that choice (which should be opaque to the end-user and sysadmin), the resulting container will run on any distro. In other words, the choice of intra-container distro, if any, should matter no more than the choice of compiler (and indeed, docker treats the 2 decisions in exactly the same way).
- (re @kimchy) When a repository includes a Dockerfile, docker can automatically built it into a container with no additional context. This allows the software maintainers to specify a recommended way of building and configuring your code, down to the last byte. Docker containers are basically just tarballs, so it's very easy to convert them to your appliance or package format of choice.
- (re @kimchy) It doesn't have to be _your_ repository, but as an elastic-search user, if given the choice between 'docker build github.com/shykes/elasticsearch' and 'docker build github.com/elasticsearch/elasticsearch', I prefer the original :)
- (re @spinscale) unlike deb/rpms, docker doesn't impose a particular versioning and dependency system. This is particularly nice when you want to build a particular git commit, or even a locally changed repo for testing before committing. You can still benefit from a standardized build process (since you know exactly which bytes will be different between dev and production containers, down to the last system library and config file), without the heavyweight process of building and releasing deb/rpms.

Hope this helps.
</comment><comment author="shykes" created="2013-06-11T18:39:17Z" id="19283362">- (re @kimchy) you can think of docker containers as static binaries on steroids. So you can configure a container at runtime in every way you could configure a regular binary command: commandline argument, environment variable, stdin, and network messages. For file-based configuration (eg. custom configuration files, scripting), the end-user can add extra filesystem layers to the container (essentially extending the build process on top of your Dockerfile). For persistent data there is a notion of data volumes which can be passed and shared between containers.
</comment><comment author="kimchy" created="2013-06-11T18:46:02Z" id="19283780">I see, so maybe we can do something similar with it to what we do with deb/rpm? I want them then to be properly configurable (to the same level the dev ones allow in init.d) and no problem with building them as part of our build and providing downloads for them.
</comment><comment author="spinscale" created="2013-07-04T10:30:05Z" id="20470125">Hey,

I have created a branch, which creates a Dockerfile on build from a template (which is roughly like your file), which we can put on downloads.elasticsearch.org and users can simply call `docker build http;//download.es.org/path.to/Dockerfile`. You can check it out here: https://github.com/spinscale/elasticsearch/commit/4c47c9652ec25c315bfc3348beba1b3d570684de

@vieux do you think that is a good idea?

Anyway, there are two points, which I am not too happy with yet (likely because I haven"t grasped the concepts of docker it I assume).
- Handling configuration. The workflow for a docker user is now to build the image first, but what about configuration in the next step? If I want to change the config file, I will have to change the configuration inside the container and save it again? Also setting the HEAP would result in changing the CMD line in the docker file. Maybe there is something simpler in docker I have simply overlooked until now (I guess so).
- Creating own custom builds will force one to change the Dockerfile template. But I think I can live with that.

Thx for your help!
</comment><comment author="vieux" created="2013-07-11T18:11:18Z" id="20830735">@spinscale yes it looks good.
The point to ass the Dockerfile in the github repo is that we can do `docker build github.com/elasticsearch/elasticsearch` and use sources without having to checkout the code inside the Dockerfile (you can use `ADD`) do you see what I mean ?

Even if you specify a CMD in the Dockerfile, you can override it with the `docker run` command.
</comment><comment author="spinscale" created="2013-07-16T14:54:44Z" id="21047431">Hey @vieux 

I'd rather stick with the stable version of elasticsearch (therefore using the external download link) instead of using master, which is not our stable branch - that's the reason for not going the github way. Makes sense?

My other open point is, how configuration file changes can be handled in the most simple way? Can you point me to some documentation regarding that, I am still a bit puzzled here (most likely because of docker misunderstandings on my side).

Thanks for the `docker run` hint (and your help in general)!
</comment><comment author="vieux" created="2013-07-18T00:31:37Z" id="21155472">Yes it makes sense.

The best way would be to `ADD` the configuration file during build.
If you use `docker build .` it's build with the current directory as context, and you will be able to add files from the context directory to the container.

Does this help you ?
</comment><comment author="lgs" created="2013-07-29T08:20:27Z" id="21705269">+1  
... any news on here ?

Until now it seems we haven't official version yet ...

&lt;pre&gt;
lsoave@base:~$ docker search elasticsearch
Found 8 results matching your query ("elasticsearch")
NAME                             DESCRIPTION
fgrehm/elasticsearch             ElasticSearch 0.90.0, built with https://gist.github.com/fgrehm/5711068. Usage: docker run -d fgrehm/elastic...
vieux/elasticsearch              
ehazlett/elasticsearch           ElasticSearch 0.90.1. Specify the following environment variable to change the config:  JAVA_OPTS: extra opt...
ehazlett/logstash                Logstash 1.1.13 &amp; Kibana 0.2.0.  Specify the following environment variable to config:  CONFIG: URL to custo...
firmasaga/elasticsearch          
paulczar/elasticsearch-zk        requires zookeeper server.   see full description for detailed instructions on running.
paulczar/elasticsearch           docker run -d paulczar/elasticsearch /opt/elasticsearch/bin/elasticsearch.sh
bacongobbler/elasticsearch       Elasticsearch as a Service. Run this image with 'docker run -d -p 9200 bacongobbler/elasticsearch'.
lsoave@base:~$ 
&lt;/pre&gt;
</comment><comment author="lgs" created="2013-11-24T19:20:00Z" id="29163345">UPDATE:

&lt;pre&gt;
lsoave@basenode:~$ docker search elasticsearch | grep OK
andrewh/kibana3     Elasticsearch log viewer     0                    [OK]
felix/docker-elasticsearch                       0                    [OK]
lsoave@basenode:~$ 
&lt;/pre&gt;
</comment><comment author="hamiltont" created="2014-09-06T03:26:23Z" id="54701049">As an end user not involved in either project, here's some thoughts:
- Docker has been a wonderful way for me to try out new tech. Having zero experience, I got the elk stack up and running on a machine in about 15 minutes and am currently playing with panels--I'm hooked on this stack, can't wait to try out some new ideas like mixing this with http://speedof.me/ to track if my ISP is really providing me the service I pay for  ;-) I would have never done this on my "play" machine before having Docker, because I would not want to risk days of fixing my working setup (of 4-5 different servers). So big vote in favor of you guys adding a Dockerfile

&gt; given the choice between 'docker build github.com/shykes/elasticsearch' and 'docker build github.com/elasticsearch/elasticsearch', I prefer the original
- 100% agree, that's why I'm here :-) The other non-official Dockerfiles have various confusions (some include kibana, some don't, some turn on different things, they are all configured differently, etc)

&gt; I'd rather stick with the stable version of elasticsearch (therefore using the external download link) instead of using master, which is not our stable branch - that's the reason for not going the github way. Makes sense?

I'd advocate for the opposite, for two reasons 1) If you use ADD instead of a static tarball, it makes getting involved in development much easier ;-) I have had good luck with projects where one Dockerfile sets up a base environment for production use, and then eventually a second Dockerfile is contributed to extend this base environment with a set of development tools. The end result is `git clone`+`docker build` and you're inside a development-ready environment
2) [the standard docker registry](registry.hub.docker.com) supports tagging (e.g. versioning), and I think most docker users mostly rely on using `docker pull` to download specific versions of pre-built tarballs. If you want to easily create sharable docker containers for past/future versions of elasticsearch, using ADD means you can just checkout the proper commit in this repository, put the Dockerfile into the working directory and run `docker build` to get a container for that version of elasticsearch (assuming the launch process stays fairly constant). The numerous Docker team members in this thread could help you setup tarballs for past elasticsearch versions if you wish, it would basically involve a one-time process of checking out the proper commit, running a build, and pushing a tarball for each past version. 

&gt;  A docker container is actually not linux distribution bound

I can report some initial success with this. I was able to move ~10 different dockerized projects from being hosted on an Ubuntu 12 machine to a CentOS machine -- it worked out of the box, literally copy the files and 10x`docker run` and I was back to production state. If the OS supports docker, then the containers seem to be cross-platform. 

&gt; how configuration file changes can be handled in the most simple way? 

The most common solution I've seen is to have a wrapper script. Currently @vieux calls `CMD elasticsearch-0.90.1/bin/elasticsearch -f` at the end of the Dockerfile. If you add a script like `bootstrap_docker.sh` to your repository, you can use `CMD bootstrap_docker.sh` instead. This allows you to check for environment variables that have been passed in, or wget some default configuration options, etc, and then call elasticsearch when you're ready. For my current elk stack, I pass configuration to logstash by mounting a directory containing logstash.conf (e.g. `docker run -v /path/on/host/holding/conf:/opt/logstash/conf logstash`). The Dockerfile always launches logstash to look in `/opt/logstash/conf` for my configuration (Also, my container start script will look for `/opt/logstash/conf/logstash.conf` and create a default configuration if one was not mounted during `docker run`). 

There is one other way - if you use `ENTRYPOINT` instead of `CMD`, then you can pass arguments at the end of `docker run`. (e.g. `docker run elasticsearch --additional --arguments --toES --here`). Personally I find the wrapper scripts more helpful, but it there are a ton of arguments possible then perhaps using entrypoint is better. 

PS - it may be worth noting you can also "mount" a specific file e.g. `docker run -v /path/on/host/logstash.conf:/opt/logstash.conf logstash`, but this can cause some tricky situations with inodes (things like `sed --inline` will stop working)
</comment><comment author="s1monw" created="2014-12-05T09:57:08Z" id="65768602">@ESamir can you maybe take a look at this. The point here is that there is demand for a feature / config file like this but we need to make sure that if we add this it's a first class citizen and it's maintained in the main build file etc. and published with our releases just like RPM and .deb IMO. I think we can maybe build on top of this one? 
</comment><comment author="bleskes" created="2015-03-27T09:42:54Z" id="86881890">@ESamir did you have a chance to look at this?
</comment><comment author="ESamir" created="2015-03-27T10:02:27Z" id="86886935">@bleskes still discussing the best approach to achieve it 
</comment><comment author="clintongormley" created="2015-06-19T07:23:11Z" id="113405898">@ESamir just a ping to remind you about this PR
</comment><comment author="electrical" created="2015-07-14T13:48:17Z" id="121242175">Just to give an update on the thread. We will be having an internal discussion around this subject and hopefully have a more clearer way forward with this.
</comment><comment author="dakrone" created="2015-12-09T18:42:11Z" id="163353175">@ESamir @electrical any update on this? I believe this PR might be so old that it should be re-done with the latest ES changes
</comment><comment author="clintongormley" created="2016-03-08T13:00:02Z" id="193777048">This PR is very old.  Currently there are up to date dockerfiles available from https://github.com/dockerfile/elasticsearch and from Docker Hub Registry, so I'm going to close this for now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread pool: rename `capacity` to `queue_size`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3161</link><project id="" key="" /><description>The `queue_size` is currently returned as `capacity` in the nodes info:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes?thread_pool=true&amp;pretty=1'
```

Rename to `queue_size` for consistency
</description><key id="15392483">3161</key><summary>Thread pool: rename `capacity` to `queue_size`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-11T10:42:56Z</created><updated>2013-06-11T11:07:21Z</updated><resolved>2013-06-11T11:07:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Facets builders &amp; co do not allow a JSON string/binary data to be used as  source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3160</link><project id="" key="" /><description>Sometimes it's useful to create facets based on the JSON/binary data already at hand. The option is available for filters (through WrapperFilterBuilder) but not for FacetBuilders.
</description><key id="15391384">3160</key><summary>Facets builders &amp; co do not allow a JSON string/binary data to be used as  source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">costin</reporter><labels /><created>2013-06-11T10:09:40Z</created><updated>2014-08-08T12:30:07Z</updated><resolved>2014-08-08T12:30:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T12:30:07Z" id="51594483">Facets are deprecated. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting not working when assigning a "min_similarity"?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3159</link><project id="" key="" /><description>I am fiddling with fuzzy search in elasticsearch-0.20.6 and elasticsearch-0.90.1. I am using highlighting to help me tune the fuzzyness. The highlighting shows fine when I use fuzzy until I add "min_similarity". I use head to post and check the results.

Shows highlighting:

``` javascript
{
 "query": {
   "fuzzy": {
     "title": "bambi"
   }
 },
 "highlight": {
   "fields": {
     "title": {}
   }
 }
```

Doesn't show highlighting:

``` javascript
{
 "query": {
   "fuzzy": {
     "title": "bambi",
     "min_similarity": 0.8
   }
 },
 "highlight": {
   "fields": {
     "title": {}
   }
 }
```

Is this not possible or is it a bug?
</description><key id="15389569">3159</key><summary>Highlighting not working when assigning a "min_similarity"?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Timmel</reporter><labels><label>:Highlighting</label></labels><created>2013-06-11T09:15:45Z</created><updated>2014-08-08T12:29:38Z</updated><resolved>2014-08-08T12:29:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T12:29:38Z" id="51594443">This is a syntax error. Your query should look like this:

```
GET /_search
{
  "query": {
    "fuzzy": {
      "title": {
        "value": "bambi",
        "min_similarity": 0.8
      }
    }
  },
  "highlight": {
    "fields": {
      "title": {}
    }
  }
}
```

In current versions of ES, your previous query now throws an exception
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing array elements in facets and script filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3158</link><project id="" key="" /><description>Arrays of integers seem to be showing incorrect behavior when subjected to facets and script filters.  

Here is a reproducer for the facet behavior.  We index two arrays with 16 entries, but the facet count 3 and 6 respectively.  This can be reproduced with numbers other than 0 filling most of the array.  

https://gist.github.com/superfuego/5ebfc33431edddf4b387

Edit:  Drew Raines started looking into this when I first posed this question in IRC, and said he found more inconsistent behavior than I had described.
</description><key id="15377393">3158</key><summary>Missing array elements in facets and script filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">superfuego</reporter><labels><label>discuss</label></labels><created>2013-06-11T01:49:31Z</created><updated>2014-08-08T13:10:40Z</updated><resolved>2014-08-08T13:10:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T12:27:06Z" id="51594250">This comes down to the fact that duplicate terms are all counted just once, eg:

```
POST /test/jdbc
{
  "idv_bitfield": [
    1048576,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    0,
    8
  ],
  "name": "bar"
}

POST /test/_search
{
  "query": {
    "match_all": {}
  },
  "script_fields": {
    "foo": {
      "script": "doc['idv_bitfield'].values.length"
    }
  }, 
  "aggs": {
    "stat1": {
      "value_count": {
        "field": "idv_bitfield"
      }
    }
  }
}
```

Number of values: 3

Is this something that we can change?
</comment><comment author="clintongormley" created="2014-08-08T13:10:40Z" id="51597865">After discussing with @jpountz - this is expected behaviour.  Only unique values are returned in a single field.  To do what you need, you would need to use nested documents.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_ttl listed in index template is ignored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3157</link><project id="" key="" /><description>0.90.1

https://gist.github.com/unthingable/5708136 &#8212; the mapping and the template that produced it. The mapping has no _ttl.
</description><key id="15372080">3157</key><summary>_ttl listed in index template is ignored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">unthingable</reporter><labels /><created>2013-06-10T22:25:34Z</created><updated>2013-06-17T18:49:08Z</updated><resolved>2013-06-17T18:48:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-11T09:53:38Z" id="19252069">Hey,

I just tried this

My `template.json` file

```
{
    "template" : "te*",
    "mappings" : {
        "type" : {
            "_ttl" : { "enabled" : true, "default": "5m" },
            "_source" : { "enabled" : false }
        }
    }
}
```

```
curl -XPUT localhost:9200/_template/template_1 -d @template.json
{"ok":true,"acknowledged":true}

curl -X PUT localhost:9200/test/type/1 -d '{"foo":"bar"}'
{"ok":true,"_index":"test","_type":"type","_id":"1","_version":1}

curl 'localhost:9200/test/_mapping?pretty'
{
  "test" : {
    "type" : {
      "_ttl" : {
        "enabled" : true,
        "default" : 300000
      },
      "_source" : {
        "enabled" : false
      },
      "properties" : {
        "foo" : {
          "type" : "string"
        }
      }
    }
  }
}
```

Can you test if the above works for you as well?
Can you provide your curl commands you used to create the issue, so we can reproduce? Maybe it is a bug only occuring in combination with other features, not sure yet.

Thanks for reporting!
</comment><comment author="unthingable" created="2013-06-11T19:31:54Z" id="19286609">Interesting. I can't test that yet, but I forgot to mention: the template json file sits in `/etc/elasticsearch/templates/`, I don't do anything to load it manually.

Having now said that, I think the template file is not being loaded at all, as the generated mapping looks very default.
</comment><comment author="spinscale" created="2013-06-12T15:07:17Z" id="19331998">I need some more input from you, as I just tested and it works again.

```
cat config/templates/template_1.json
{
  "template_1" : {
    "template" : "to*",
    "mappings" : {
        "type" : {
            "_ttl" : { "enabled" : true, "default": "5m" },
            "_source" : { "enabled" : false }
        }
    }
  }
}
```

then I delete the index and recreate it by adding a document, and getting the mapping shows the TTL

```
curl -X DELETE localhost:9200/toast
curl -X PUT localhost:9200/toast/type/1 -d '{"foo":"bar"}'
curl 'localhost:9200/toast/_mapping?pretty'
{
  "toast" : {
    "type" : {
      "_ttl" : {
        "enabled" : true,
        "default" : 300000
      },
      "_source" : {
        "enabled" : false
      },
      "properties" : {
        "foo" : {
          "type" : "string"
        }
      }
    }
  }
}
```

Can you recheck your sample? Maybe some naming problem? Can you try and reproduce step-by-step?
</comment><comment author="unthingable" created="2013-06-17T18:48:19Z" id="19565867">The problem was traced to incorrect permission on /etc/elasticsearch/templates/, sorry about that. The templates were not being loaded at all.
</comment><comment author="s1monw" created="2013-06-17T18:49:07Z" id="19565936">thanks for bringing up the solution!! 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup String to UTF-8 conversion</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3156</link><project id="" key="" /><description>Currently we have many different places that convert String to UTF-8
bytes and back. We shouldn't maintain more code than necessary to
do this conversion and rather use Lucene's support for it.
</description><key id="15365787">3156</key><summary>Cleanup String to UTF-8 conversion</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-06-10T20:01:57Z</created><updated>2014-07-16T21:53:13Z</updated><resolved>2013-06-11T09:43:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added created flag to index related request classes.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3155</link><project id="" key="" /><description>The flag is set to true when a document is new, false when replacing an existing object.

Other minor changes:
Fixed an issue with dynamic gc deletes settings update
Added an assertThrows to ElasticsearchAssertion

Closes #3084 , Closes #3154
</description><key id="15324144">3155</key><summary>Added created flag to index related request classes.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-06-09T19:00:48Z</created><updated>2014-07-01T17:37:22Z</updated><resolved>2013-06-13T07:25:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-06-13T07:25:20Z" id="19376196">merged.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add a created flag to IndexResponse &amp; UpdateResponse</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3154</link><project id="" key="" /><description>The new flag indicates whether the operation added the document to the index (true) or replaced an existing one (false).
</description><key id="15324099">3154</key><summary>Add a created flag to IndexResponse &amp; UpdateResponse</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-06-09T18:57:58Z</created><updated>2013-06-13T07:31:32Z</updated><resolved>2013-06-13T07:23:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-06-13T07:25:48Z" id="19376220">Any chance to push this to 0.90 as well?
</comment><comment author="bleskes" created="2013-06-13T07:31:32Z" id="19376407">That's tricky because it breaks backward compatibility on the wire level (internal communication protocols between nodes), which needs a major version change. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Upsert shouldn't require an extra document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3153</link><project id="" key="" /><description>When posting a document for Update should be possible to tell the command to do an upsert of the same document. Something like upsert_doc: true. Reduces the amount of data sent over the wire significantly. This scenario happens a lot when having several data stores for the same document in the index. 
</description><key id="15319624">3153</key><summary>Upsert shouldn't require an extra document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">pecke01</reporter><labels /><created>2013-06-09T12:51:58Z</created><updated>2014-06-18T10:49:09Z</updated><resolved>2013-06-17T09:00:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pecke01" created="2013-06-09T12:52:15Z" id="19165594">I am working on this and will provide a pull request for this.
</comment><comment author="pecke01" created="2013-06-10T21:42:10Z" id="19228480">Added implementation for shouldUpsertDoc. Using the REST api it is "upsert_doc" : true. Not sure if the rest api need/should have the same name. should_upsert_doc or shouldupsertdoc seems flawed to me.
</comment><comment author="martijnvg" created="2013-06-14T12:42:28Z" id="19454834">Hi @pecke01, nice idea! 
What about using the name `doc_as_upsert` for the `upsert_doc` option? 

Also I see that there are now a lot of if statements in the UpdateRequest class, I think the intrusiveness of this option in the code base can be reduced by just handling it in the UpdateHelper#prepare() method:

``` java
if (!getResult.isExists()) {
   if (request.upsertRequest() == null &amp;&amp; !request.shouldUpsertDoc()) {
       throw new DocumentMissingException(...);
   }
   IndexRequest indexRequest = request.shouldUpsertDoc() ? request.doc() : request.upsertRequest();
   ....
```

I think the above code is less intrusive.
</comment><comment author="pecke01" created="2013-06-15T11:36:21Z" id="19495216">Hi @martijnvg 

Thank you for the feedback. I have adjusted it to be less intrusive.

Also like the `doc_as_upsert` and adjusted that as well.
</comment><comment author="martijnvg" created="2013-06-16T18:47:02Z" id="19516839">@pecke01 Looks good! Can you squash the two commits into one commit? Then I can pull your changes in.
</comment><comment author="pecke01" created="2013-06-16T19:34:34Z" id="19517591">Great. Squashed them together.
</comment><comment author="martijnvg" created="2013-06-17T08:37:37Z" id="19532354">@pecke01 Thanks for submitting this pull request!
</comment><comment author="pecke01" created="2013-06-17T08:45:06Z" id="19532669">@martijnvg thank you for guiding and holding my hand.
</comment><comment author="s1monw" created="2013-06-17T08:56:45Z" id="19533132">@pecke01 any reason why you reopend it?
</comment><comment author="pecke01" created="2013-06-17T09:00:20Z" id="19533307">Hmm no. I must have done it by misstake. Closed again
</comment><comment author="F21" created="2013-06-29T13:44:54Z" id="20230070">Sorry for commenting on a closed issue. Does this option also work for the bulk API? If so, the docs would need some updating: http://www.elasticsearch.org/guide/reference/api/bulk/
</comment><comment author="Paikan" created="2013-06-30T13:35:27Z" id="20247213">@F21 you are right. The "doc_as_upsert" option works also for the bulk API. I have opened a pull request to add it to the bulk API documentation (https://github.com/elasticsearch/elasticsearch.github.com/pull/466). Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return matched children per hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3152</link><project id="" key="" /><description>In has_child queries, we can only get the list of parents back. It would be great if we can also set an option to have the children (related using a parent-child association) returned.

There's an issue to implement this for nested documents: https://github.com/elasticsearch/elasticsearch/issues/3022?source=cc, but there wasn't one for parent-child, so here it is :smile: 
</description><key id="15314250">3152</key><summary>Return matched children per hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">F21</reporter><labels><label>:Search</label></labels><created>2013-06-09T03:31:16Z</created><updated>2014-12-02T11:02:24Z</updated><resolved>2014-12-02T11:02:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-09T12:44:23Z" id="19165516">See @martijnvg last comment here: #3022, the refactoring needed there is aimed at also supporting also returning matched children. This require quite significant work, mainly around trying to figure out how the refactoring should look like. No actual work started around this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoHashes in GeoPointFieldMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3151</link><project id="" key="" /><description>Fixed the `GeoPointFieldMapper` to parse `geohashes` correctly.

Closes #3073
</description><key id="15279370">3151</key><summary>GeoHashes in GeoPointFieldMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-07T16:08:31Z</created><updated>2014-07-06T08:32:29Z</updated><resolved>2013-06-13T13:01:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-13T13:01:49Z" id="19389978">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow date format to supported group of built-in patterns</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3150</link><project id="" key="" /><description>Until now 'named dates' like `dateOptionalTime` could not be used as a group
of dates. This patch allows it to group it arbitrarily like this:
- yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||dateOptionalTime
- dateOptionalTime||yyyy/MM/dd HH:mm:ss||yyyy/MM/dd
- yyyy/MM/dd HH:mm:ss||dateOptionalTime||yyyy/MM/dd
- date_time||date_time_no_millis

Closes #2132
</description><key id="15276045">3150</key><summary>Allow date format to supported group of built-in patterns</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-06-07T14:59:14Z</created><updated>2014-07-07T06:20:52Z</updated><resolved>2013-06-24T22:07:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-10T07:25:10Z" id="19183983">@s1monw thx for your input, worked a bit on the patch (after my force push, your comments seem to have vanished though).

Stil not sure about exception handling. The currently provided error message is poor. I'll try to come up with something better (at least mentioning what part of the list of dates could not be formatted would be worth something IMO).
</comment><comment author="spinscale" created="2013-06-10T08:22:43Z" id="19185966">Added a more verbose message to the illegal argument exception, in case the date format cannot be parsed (the format is now put into the message).
</comment><comment author="spinscale" created="2013-06-24T22:07:07Z" id="19939550">Committed in master and 0.90

https://github.com/elasticsearch/elasticsearch/commit/9d3e34b9f98ffbe0f40781f34669841035cb8953
https://github.com/elasticsearch/elasticsearch/commit/d78ace936f40af7976b0b6a14014881496a02fdc
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Geo-Refactoring</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3149</link><project id="" key="" /><description>The code handling geo-shapes is not centralized and creating points takes place at different places. Also the collection of supported geo_shapes is not complete regarding to the GEOJSon specification. This commit centralizes the code related to GEO calculations and extends the old API by a set of new shapes.
## Null-Shapes

The latest implementation of geo-shapes allows to index _null-shapes_. This means a field that is defined to hold a geo-shape can be set to `null`. In example:

```
{
    "shape": null
}
```
## New Shapes

The geo-shapes _multipoint_ and _multilinestring_ have been added to the _geo_shape_ types. Also _geo_circle_ is introduced by this commit.
## Dateline wrapping

A major issue of geo-shapes is the spherical geometry. Since ElasticSearch works on the _Geo-Coordinates_ by wrapping the Earths surface to a plane, some shapes are hard to define if it's crossing the _+180&#176;_, _-180_ longitude. To solve this issue ElasticSearch offers the possibility to define geo-shapes crossing this borders and decompose these shapes and automatically re-compose them in a spherical manner.
This feature may change the indexed shape-type. If for example a _polygon_ is defined, that crosses the dateline, it will be re-assembled to a set of polygons. This causes indexing a _multipolygon_. Also _linestrings_ crossing the dateline might be re-assembled to _multilinestrings_.
## Builders

The API has been refactored to use builders instead of using shapes. So parsing geo-shapes will result in builder objects. These builders can be parsed and serialized without generating any shapes. this causes shape generation only on the nodes executing the actual operation.
Also the baseclass `ShapeBuilder` implements the `ToXContent` interface which allows to set fields of `XContent` directly.
## TODO's
- The _geo-circle_ will not work, if it's crossing the dateline
- The _envelope_ also needs to wrapped

Closes #1997 #2708 
</description><key id="15261394">3149</key><summary>Geo-Refactoring</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-06-07T08:02:04Z</created><updated>2014-06-29T12:35:16Z</updated><resolved>2013-07-03T20:04:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-06-10T07:45:43Z" id="19184654">overall this looks awesome. you mentioned that this branch introduces geo_circle above in the commit message: `Also geo_circle is introduced by this branch.` is this still correct? I also wonder what the `null` shape does, can you elaborate on this?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>match_phrase_prefix missing results in some circumstances</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3148</link><project id="" key="" /><description>I am seeing this issue in 0.90.1 and 0.20.5 (I have not tested on any release in between or before 0.20.5).

First, I create an index called `test`.

Then I insert 2 documents:

``` json
POST http://localhost:9200/test/people/1

{
    "user": "Anna Citizen",
    "post_date": "2009-11-15T14:12:12",
    "message": "ES is great!"
}
```

``` json
POST http://localhost:9200/test/people/2

{
    "user": "Jerry Smith",
    "post_date": "2009-11-15T14:12:12",
    "message": "Testing ES!"
}
```

Then I run a simple `match_phrase_prefix` query:

``` json
POST http://localhost:9200/test/_search/

{
   "query":{
      "match":{
         "people.user":{
            "type":"phrase_prefix",
            "query":"j"
         }
      }
   }
}
```

The correctly finds the `Jerry Smith` document:

``` json
{
   "took":2,
   "timed_out":false,
   "_shards":{
      "total":5,
      "successful":5,
      "failed":0
   },
   "hits":{
      "total":1,
      "max_score":0.19178301,
      "hits":[
         {
            "_index":"test",
            "_type":"people",
            "_id":"2",
            "_score":0.19178301,
            "_source":{
               "user":"Jerry Smith",
               "post_date":"2009-11-15T14:12:12",
               "message":"ES is great!"
            }
         }
      ]
   }
}
```

However, if I search for `a`, I get nothing:

``` json
POST http://localhost:9200/test/_search/

{
   "query":{
      "match":{
         "people.user":{
            "type":"phrase_prefix",
            "query":"a"
         }
      }
   }
}
```

``` json
{
   "took":1,
   "timed_out":false,
   "_shards":{
      "total":5,
      "successful":5,
      "failed":0
   },
   "hits":{
      "total":0,
      "max_score":null,
      "hits":[

      ]
   }
}
```

The only way to get the `Anna Citizen` document is to search for `ann`:

``` json
POST http://localhost:9200/test/_search/

{
   "query":{
      "match":{
         "people.user":{
            "type":"phrase_prefix",
            "query":"ann"
         }
      }
   }
}
```

``` json
{
   "took":1,
   "timed_out":false,
   "_shards":{
      "total":5,
      "successful":5,
      "failed":0
   },
   "hits":{
      "total":1,
      "max_score":0.19178301,
      "hits":[
         {
            "_index":"test",
            "_type":"people",
            "_id":"1",
            "_score":0.19178301,
            "_source":{
               "user":"Anna Citizen",
               "post_date":"2009-11-15T14:12:12",
               "message":"trying out Elastic Search"
            }
         }
      ]
   }
}
```

Why is this happening? Could this possibly be a bug?
</description><key id="15253640">3148</key><summary>match_phrase_prefix missing results in some circumstances</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">F21</reporter><labels /><created>2013-06-07T01:25:25Z</created><updated>2013-06-07T11:58:15Z</updated><resolved>2013-06-07T11:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-07T11:58:15Z" id="19102860">Hey,

when typing the `a` using the standard analzyer it actually gets removed from the query, as it is a stopword (this is the reason that `an` also gets removed an only `ann` is working). There are two possibilties to solve this.
- Do not use the standard analyzer (which might make sense, as you do not want to remove stopwords in name fields most of the time) for the field
- Use the `analyzer` parameter of the query in order to change it, see http://www.elasticsearch.org/guide/reference/query-dsl/match-query/

Note that the first solutions applies at index time, where the second solution applies at query time.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolator registered on an alias is never triggered </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3147</link><project id="" key="" /><description>Actually, adding a percolator is always successful, regardless whether the targeted index exists or not. Currently, the targeted index should be a real index, and cannot be an alias. I don't know if this is the wanted behavior, but it's confused me; the percolate documentation is silent about this use case.

To reproduce:

``` bash
# Create an index (idx), and an alias pointing to it (idxAlias).
curl -XPUT localhost:9200/idx
curl -XPOST localhost:9200/_aliases -d ' { "actions" : [ { "add" : { "index" : "idx", "alias" : "idxAlias" } } ] }'

# Create a percolator on the index it-self.
# Percolating a document via the index or the alias works in both cases. This is fine.
curl -XPUT localhost:9200/_percolator/idx/kuku -d '{ "query" : { "term" : { "field1" : "value1" } } }'
curl -XGET localhost:9200/idx/type1/_percolate -d '{ "doc" : { "field1" : "value1" } }'
    ~&gt; {"ok":true,"matches":["kuku"]}
curl -XGET localhost:9200/idxAlias/type1/_percolate -d '{ "doc" : { "field1" : "value1" } }'
    ~&gt; {"ok":true,"matches":["kuku"]}

# Create a percolator on the alias.
# Percolating a document via the index or the alias fails in both cases.
curl -XPUT localhost:9200/_percolator/idxAlias/kiki -d '{ "query" : { "term" : { "field2" : "value2" } } }'
    ~&gt; {"ok":true,"_index":"_percolator","_type":"idxAlias","_id":"kiki","_version":1}
curl -XGET localhost:9200/idx/type1/_percolate -d '{ "doc" : { "field2" : "value2" } }'
    ~&gt; {"ok":true,"matches":[]}
curl -XGET localhost:9200/idxAlias/type1/_percolate -d '{ "doc" : { "field2" : "value2" } }'
    ~&gt; {"ok":true,"matches":[]}
```
</description><key id="15241622">3147</key><summary>percolator registered on an alias is never triggered </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-06-06T19:49:34Z</created><updated>2013-08-01T08:50:47Z</updated><resolved>2013-08-01T08:50:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="julianhille" created="2013-07-26T13:14:43Z" id="21619708">This is a very heavy issue for me, cause im relying on percolator and aliases.
- Percolators will be created with the name of the alias as a type.
- Percolators querying will be with the real index name.
</comment><comment author="martijnvg" created="2013-07-31T11:35:18Z" id="21855741">The percolate api just doesn't support resolving aliases. The redesigned percolate api does support resolving aliases to concrete indices.
</comment><comment author="martijnvg" created="2013-08-01T08:50:47Z" id="21923048">Now fully supported via #3420
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>To delete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3146</link><project id="" key="" /><description>Sorry, this issue must be destroyed
</description><key id="15241214">3146</key><summary>To delete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vpuech</reporter><labels /><created>2013-06-06T19:41:06Z</created><updated>2013-06-06T19:43:20Z</updated><resolved>2013-06-06T19:41:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Replication fails with indeterminate error in basic configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3145</link><project id="" key="" /><description>Officially filing what I've seen here: https://groups.google.com/forum/#!topic/elasticsearch/LPv_wPPVTJg

In short, I'm attempting to configure a basic 2-node cluster with ES 0.90.0 on JRE 1.7.0_17, dedicated hardware, slightly different Linux distros. The baseline configuration isn't too complicated, simply 16 shards with predefined mappings and one index. Regardless of discovery mechanism, the nodes seem to discover/elect fine. If I run a single node and load it with the exact same harness, everything works fine. When I have two nodes, system errors in replication begin to happen but I'm not able to tell what the errors are due to what appears to be errors in the Netty transport layer being unable to parse the serialized exception. Stack traces appear similar to:

```
[2013-06-05 09:35:20,480][WARN ][action.index             ] [es-1] Failed to perform index on replica [rules][4]
org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.StreamCorruptedException: unexpected end of block data
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1369)
    at java.io.ObjectInputStream.access$300(ObjectInputStream.java:205)
    at java.io.ObjectInputStream$GetFieldImpl.readFields(ObjectInputStream.java:2132)
    at java.io.ObjectInputStream.readFields(ObjectInputStream.java:537)
    at java.net.InetSocketAddress.readObject(InetSocketAddress.java:282)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1872)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1894)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1894)
    at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)
    at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)
    ... 23 more
[2013-06-05 09:35:20,483][WARN ][cluster.action.shard     ] [es-1] sending failed shard for [rules][4], node[Ed7LBQdzQMae69IzlCm-Dg], [R], s[STARTED], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
[2013-06-05 09:35:20,483][WARN ][cluster.action.shard     ] [es-1] received shard failed for [rules][4], node[Ed7LBQdzQMae69IzlCm-Dg], [R], s[STARTED], reason [Failed to perform [index] on replica, message [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]]
```

As additional context, I originally started with compression enabled and then disabled it. I've tested with both ZooKeeper and Zen discovery (unicast and multicast), same results. I've deleted the index as well as starting completely fresh, no change. 

I'll follow up with config files after I sanitize them. Haven't made any progress asking on the mailing list.
</description><key id="15237717">3145</key><summary>Replication fails with indeterminate error in basic configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eonnen</reporter><labels><label>high hanging fruit</label></labels><created>2013-06-06T18:30:10Z</created><updated>2015-09-19T17:44:37Z</updated><resolved>2015-09-19T17:44:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eonnen" created="2013-06-06T18:46:54Z" id="19065919">One additional data point I've discovered, this issue is only reproducible when generating _concurrent_ load with multiple clients. If I drop my clients down to a single thread, no problems. When I step up to two threads executing index calls (using TransportClient), I can reproduce the issue.
</comment><comment author="s1monw" created="2013-06-07T07:41:43Z" id="19093249">hi eric, 

this is interesting though, is it possible to reproduce this error with a certain document you could share or does the document not matter at all here? as far as I understand you are indexing into 2 nodes with transport client and as soon as you switch to TransportClient and concurrent requests you see the problems happening, right? Yet, do you get exceptions in your client as well or do you see failing index requests? Is there any chance that your client uses a different es version by any chance?
</comment><comment author="jprante" created="2013-06-07T08:35:06Z" id="19095177">Duplicate older thread here (no solution yet) https://groups.google.com/d/msg/elasticsearch/MSrKvfgKwy0/Tfk6nhlqYxYJ
</comment><comment author="eonnen" created="2013-06-07T16:18:27Z" id="19117099">Hey Simon! You're correct, I see no exceptions at the client layer and all is fine. I'm indexing a stream of very similar documents and when I only run a single indexer thread, I encounter no problems. As soon as I change to multiple threads on the client I can reproduce the issue. Given that I have no problems single threaded, I'm fairly certain this isn't a document-specific issue. I can't really pinpoint a specific problematic document as the exception doesn't make it back to the client.
</comment><comment author="eonnen" created="2013-06-07T16:19:02Z" id="19117139">Oh and yes, I am using the same client version as the server assuming the maven version of the client is correct.
</comment><comment author="eonnen" created="2013-06-07T16:24:30Z" id="19117420">With further testing, the issue does not seem to present itself when I disable the geo_point field from my mappings.
</comment><comment author="s1monw" created="2013-06-07T18:12:59Z" id="19123677">man, that is good input! I will check this soonish! thanks man... will report back once I have something
</comment><comment author="jprante" created="2013-06-10T07:25:25Z" id="19183990">A popular reason of this kind of error is a server-side exception generated by an optional jar, which is not present at client side. Geo uses the optional spatial4j jar so my recommendation is to add the spatial4j jar to the client classpath.
This also holds for all kind of plugins, if plugin jars are not present in client classpath and throw exceptions, they might fail while decoding.
Another option is to add more logging statements on server side so it is easier to track runtime execptions in the server logs.
</comment><comment author="eonnen" created="2013-06-11T16:25:47Z" id="19273166">@jprante, doesn't the TransportClient simply submit a JSON representation of a document with nested fields? I'm not sure I understand why the client needs to have the ability to create a geospatial representation of what is effectively a text document. Regardless, the client's classpath was that of the POM which includes lucene-spatial and as a dependency spatial4j.
</comment><comment author="s1monw" created="2013-06-11T16:54:35Z" id="19274809">@eonnen I had no luck reproducing this one yesterday but I will give it another go tomorrow. Yet, the transport client still talks binary and not json via HTTP
</comment><comment author="s1monw" created="2013-06-13T14:55:52Z" id="19397133">@eonnen I used 0.90 and started 2 nodes on localhost:9200 and localhost:9201 (transport 9300 / 9301 respectively) Then I ran  https://gist.github.com/s1monw/5774329 and I don't see any issues. Can you try to run this against your setup or extend it so that it reproduces?

simon
</comment><comment author="tkurki" created="2013-10-15T13:24:10Z" id="26333352">I am seeing something similar here, but my case is really different: client throws 

```
Caused by: java.io.EOFException
    at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2597)
    at java.io.ObjectInputStream.skipCustomData(ObjectInputStream.java:1942)
    at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1916)
...
    at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:301)
```

when doing simple search.

Digging strings from the data in the buffer just previous to trying to read it with ThrowableObjectInputStream shows stuff like

```
4org.elasticsearch.transport.RemoteTransportExceptionxr
4org.elasticsearch.transport.ActionTransportExceptionxr
.org.elasticsearch.transport.TransportExceptionxr
(org.elasticsearch.ElasticSearchExceptionxr
java.lang.RuntimeExceptionxr
java.lang.Throwablexpsr
/org.elasticsearch.indices.IndexMissingExceptionxr
&amp;org.elasticsearch.index.IndexExceptionxq
```

so in my case it looks like a simple case of missing index, but reporting the error is breaking. 

Only it isn't, because the index is there and you can search it via http json...

I have been previously burned by trying to return serialized Exceptions over the wire  and the client failing because of some incompatibility between server and client side classpaths. I think the prudent way of handling server side errors is to return String based well defined Error objects and logging stacktraces on the server side. Otherwise this type of "sorry, we failed while failing" can crop up. 

Haven't looked at the code yet though, it is possible I'm barking up the wrong tree.
</comment><comment author="fizx" created="2013-10-31T01:02:57Z" id="27453710">This error can easily be duplicated by starting a cluster with some nodes running JRE 1.7.0_17, and other nodes running JRE 1.7.0_45.
</comment><comment author="kimchy" created="2013-10-31T01:55:22Z" id="27455667">This happens because sadly, Java broke backward compatibility between 1.7.0_17 and 1.7.0_2x in how InetAddress gets serialized over the network. The only place in ES where we use Java serialization is when we serialize exceptions (since we don't want to write custom serializers for each one), and we use exception for state management (i.e. try to perform an operation on a node that is not ready). Sadly, part of those exception is an InetAddress, which causes this serialization problem...

There is nothing much we can do with current versions, except for asking to make sure not to run on mixed &lt;=1.7.0_17 and &gt;= 1.7.0_2x versions in the same cluster....
</comment><comment author="JacekLach" created="2013-11-21T19:18:24Z" id="29014479">FYI: this also happens on java6 between 1.6.0_43 and 1.6.0_45.
</comment><comment author="kimchy" created="2013-11-29T23:35:14Z" id="29541505">grrr, so annoying..., though we don't really have control over it, apologies....
</comment><comment author="josegonzalez" created="2014-01-27T19:35:04Z" id="33411884">I just ran into this issue. Shards are constantly reallocating themselves and failing :(
</comment><comment author="kimchy" created="2014-01-27T19:46:19Z" id="33413004">@josegonzalez make sure you run the same Java version across nodes?
</comment><comment author="josegonzalez" created="2014-01-27T19:50:26Z" id="33413467">Yeah I fixed it. Here's what happened in my case:

We brought up a new node for our api elasticsearch cluster, and this new node had Java `1.7.0_51`. The old nodes ran `1.7.0_16` (I believe). I ended up updating java everywhere.

Once java is installed, elasticsearch is started. Installing java does not automatically restart elasticsearch, and neither does upgrading. So we ended up running two different versions for in the cluster, even though `java -version` produced `1.7.0_51` everywhere.

I had to restart all older nodes in the cluster, which then allowed replication to proceed as advertised.

Java should really have a sticker on it "please don't touch me, ok thx bye" :)

Might be worth mentioning this in the docs somewhere, maybe an FAQ.
</comment><comment author="matiwinnetou" created="2014-02-03T20:49:24Z" id="33998261">Would it make sense to replace standard Java Serialization with another serialization protocol?
</comment><comment author="kimchy" created="2014-02-03T22:43:52Z" id="34009813">@mati1979 we only use standard Java serialization for serializing exceptions, for most cases where the request/response are not exception driven, we use our own custom serialization
</comment><comment author="aphyr" created="2014-04-12T07:41:52Z" id="40274254">Chiming in that this also breaks _clients_ which use different JVM patchlevels, and I imagine it makes doing any kind of rolling upgrade something of a headache. Might consider serializing exceptions as a [classname-str message-str [stacktrace-line stacktrace-line ...]] structure, under the premise that some information is better than no information about the underlying error. If you're relying on try/catch dispatch for Elasticsch exception types, you can always define a custom deserializer for those; it's only the underlying exceptions you don't control that need a fallback mechanism.
</comment><comment author="kimchy" created="2014-04-12T07:47:54Z" id="40274372">there is a thread on the mailing list about it. This is the first time Java broke serialization for exceptions in the 4.5 years the codebase exists. While annoying, the question is if we should now write custom logic to serialize exceptions. 

Exceptions hold more than just what you described, since they also hold variables and the class type itself, so your solution won't work. One option is to use something like Jackson object mapper to serialize the exceptions in a generic fashion. If we see this pattern repeating in java (at least one more time), then we will do it (which comes with other costs obviously, you still rely on _a_ serialization).

Side note, you are running your servers on 1.7.0_03, make sure you don't do that as it is known to be a problematic java version (in many different aspects). Use _25.
</comment><comment author="aphyr" created="2014-04-12T07:55:27Z" id="40274505">I am not really an expert in these sorts of things, and of course defer to your judgement and expertise, but might I suggest defining a taxonomy of the errors which occur in your system, and having a formally specified, platform-independent representation of those errors on the wire--such that clients running on other JVMs or even clients from other runtimes could interpret those errors in a common fashion?
</comment><comment author="kimchy" created="2014-04-12T08:48:14Z" id="40275408">that would be nice!, and something that potentially will be worth the investment to do. 

The taxonomy of external errors today for external clients follows the rest status codes, as most of our clients talk to ES through HTTP. In JVM based languages using the java client, each error can be cast to an http response code.

For internal errors, or node (client) to node erros the idea behind using Java serialization for exceptions is to try and preserve the full information of the exception and propagate it through the chain of nodes that handles the request. Those exceptions many times come from low level operations like network or file system. This allows for example to give detailed information on why something failed not necessarily on the node that it actually failed on (this helps a lot when it comes to debugging). 

For example, if a shard failed, the master node will log that information without needing to go and chase where that shard failed to try and understand why it failed.

Last, the other aspect that makes it super hard is the fact that Java doesn't have a good taxonomy of exceptions. Just check IOException :). At least in that case, by not loosing the type and info, we can potentially do better not only where the actual type is around...
</comment><comment author="ppuschmann" created="2014-04-30T08:23:41Z" id="41771784">Hi, I also ran in the same issue when applying security fixes for one server (1.6.0_18 vs. 1.6.0_31) running an ES 0.19.12 cluster (with only http-clients).

Very interesting to see that the one node with the wrong (higher) version-number had no entries in its log-files while others that want to replicate shards to "the one" had a lot of exceptions.

Is there a plan how to get this fixed? And how should we do an update of the JRE?
</comment><comment author="clintongormley" created="2015-09-19T17:44:37Z" id="141693038">This has been implemented in #11910
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child &amp; has_parent queries don't take deletes into account</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3144</link><project id="" key="" /><description>If score mode: `max`, `avg` or `sum` is specified for `has_child` and if score mode `score` is specified for `has_parent` query then deletes aren't taken into account.

This affects all version from 0.90.0.Beta1
</description><key id="15227676">3144</key><summary>has_child &amp; has_parent queries don't take deletes into account</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-06T15:06:38Z</created><updated>2013-06-06T15:14:00Z</updated><resolved>2013-06-06T15:14:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>MultiGetRequestBuilder toString implementation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3143</link><project id="" key="" /><description>I reported this in the Google group about a month ago.  Unlike other builders, MultiGetRequestBuilder does not log the JSON used to query the server when the toString method is called.

David Pilato responded:

&gt; I think we just need to add:
&gt; 
&gt; ```
&gt; @Override
&gt; public String toString() {
&gt;     return internalBuilder().toString();
&gt; }
&gt; ```
&gt; 
&gt; In MultiGetRequestBuilder and MultiSearchRequestBuilder as well.
</description><key id="15227148">3143</key><summary>MultiGetRequestBuilder toString implementation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">mmindenhall</reporter><labels /><created>2013-06-06T14:58:23Z</created><updated>2013-09-12T08:14:40Z</updated><resolved>2013-09-12T08:14:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-06-06T16:34:22Z" id="19057569">Sounds like I forget to add it! :-(
</comment><comment author="javanna" created="2013-08-26T18:09:10Z" id="23282583">This might sound weird but most of the request builders don't actually translate back to json. That's because the java api classes are actually used internally for node-to-node communication, using our own optimized binary protocol instead of json. That means that in most of the cases we create the request builders given the json on the node that receives the request, and we never need to translate requests back to json. There are exceptions, like the `SearchRequestBuilder`, and you can identify the classes that are easily representable in json format as those that implement the `ToXContent` interface. In general we do that only when needed by elasticsearch itself internally.
</comment><comment author="javanna" created="2013-09-12T08:14:39Z" id="24301782">Closing this one since as mentioned before `MultiGetRequestBuilder` doesn't easily map back to json and it doesn't make much sense to change this only to have toString method that returns json.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FVH can result in massive CPU &amp; RAM usage if MultPhraseQuery is large</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3142</link><project id="" key="" /><description>in the case of a MultiPhraseQuery (multiple terms on the same position) the highlighter can result in a very big cpu and memory cosumption. We should cut off if there are too many terms and just highlight term by term instead which might result in slightly different highlights but doesn't potentially kill a node.
</description><key id="15212412">3142</key><summary>FVH can result in massive CPU &amp; RAM usage if MultPhraseQuery is large</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-06T09:09:11Z</created><updated>2013-06-06T10:41:51Z</updated><resolved>2013-06-06T10:41:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>missing/exists filters should also work for objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3141</link><project id="" key="" /><description>The missing filter is not working, when the field expected to be missing is an object. Consider this sample

``` sh
curl -X DELETE http://localhost:9200/test

curl -XPUT 'http://localhost:9200/test/test/3?refresh=true' -d '{
    "foo" : {
        "bar" : "baz"
    }
}'

curl -XPOST "http://localhost:9200/test/_search?pretty" -d '{
    "filter" : {
        "missing" : {
            "field" : "foo"
        }
    }
}'
```

This could be implemented by checking the mapping for the type to be queried and create a bool filter full of missing filters, when the mapping shows an object type.

This feature also supports wildcards notation, for example `obj1.obj2.field_pre*`.
</description><key id="15211782">3141</key><summary>missing/exists filters should also work for objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-06T08:53:21Z</created><updated>2013-07-25T10:26:38Z</updated><resolved>2013-06-12T02:42:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-12T02:41:36Z" id="19304040">Going to push the change to this one soon, note, this improvement will only work for objects that have fields that are indexed within it to check if it exists or missing...
</comment><comment author="philayres" created="2013-06-12T19:24:32Z" id="19349230">I'm pleased I searched before raising an issue of my own. You say this will only work for "objects that have fields that are indexed within". Am I correct in understanding that the only case this does not cover is if all fields within an object are marked as 'not indexed' in mappings? Or will it also exclude an object only containing other objects (i.e. no discrete indexed fields).

Looking forward to trying this out on my test server soon.
</comment><comment author="kimchy" created="2013-06-12T19:26:33Z" id="19349368">@philayres yes, if all fields within the object are marked as not indexed, it will regard it as missing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>FVH produces StringArrayIndexOutOfBounds if stored field is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3140</link><project id="" key="" /><description>We still use Lucene's SimpleFragmentsBuilder which doesn't try to correct the offsets is a broken analysis chain is used like the old NGramTokenFilter. SimpleFragmentsBuilder also needs to call into FragmentBuilderHelper.fixWeightedFragInfo() to detect broken chains and do best effort to resort the tokens.
</description><key id="15210589">3140</key><summary>FVH produces StringArrayIndexOutOfBounds if stored field is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-06T08:20:34Z</created><updated>2013-06-06T09:01:52Z</updated><resolved>2013-06-06T09:01:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>has_parent query returning no results with multi level child docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3139</link><project id="" key="" /><description>Reported via the ML: 
https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/gto9da4fq3M

In short: 
With types the grand_parent, parent, child
Then the `has_parent` query and filter can't find any docs if parent_type is set to parent.

For example when the following query is executed: 

```
"has_parent" : {
   "type" : "parent",
   "query" : match_all : {}
}
```

Then no results are returned.
</description><key id="15191104">3139</key><summary>has_parent query returning no results with multi level child docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-06-05T20:08:53Z</created><updated>2013-06-05T20:16:30Z</updated><resolved>2013-06-05T20:16:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-06-05T20:16:30Z" id="19005461">Closed by: https://github.com/elasticsearch/elasticsearch/commit/82ff1c6802c085201b6a0643a954fd112cd4a88e
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias to an alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3138</link><project id="" key="" /><description>We are potentially interested in exposing parts of our Elasticsearch cluster to the Internet. The idea is to do authentication in Varnish before we hit the actual backend. With the authentication done we do want to limit the accessible documents on a per customer basis.

With [Filtered Aliases](http://www.elasticsearch.org/guide/reference/api/admin-indices-aliases/) it is possible to create an alias for an existing index and add additional filters to each request. It is possible to create such an index alias for every customer in our system. When a request comes in we use Varnish to rewrite the original request URL to include the customer id returned by the authentication backend. Simplified example:

```
Client
   | /jobs/123
   v
Varnish
   | /{customerId}/jobs/123
   v
Elasticsearch
```

In our current setup we already use aliases in order to do live reindexing. We have an alias `production_jobs` pointing to an index `production_jobs_1368699241`. We can swap the alias to another index without noticeable downtime for our users. It would be great if the setup described above could also leverage this pattern, but now by pointing the alias for a customer to the production index alias. In that case we would only have to update the alias as we do now, instead of doing it for every customer specific alias.
</description><key id="15158649">3138</key><summary>Alias to an alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">asm89</reporter><labels><label>:Aliases</label><label>discuss</label><label>enhancement</label></labels><created>2013-06-05T08:13:29Z</created><updated>2016-04-22T14:20:48Z</updated><resolved>2015-11-13T10:16:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sumangurung" created="2014-02-16T13:33:49Z" id="35196759">I have a very similar use case. I have index aliases set up for zero downtime reindexing. And for a particular search, I search across a couple of indexes(via their aliases). It would be ideal if I could create an alias which points to a group of indexes. 

At the moment, the choices are:
1. maintain the list of aliases that I want to search against.
2. create an alias which points to the indexes. And update the alias to use the new index name when we perform live reindexing.

This would be a nice feature addition.
</comment><comment author="nik9000" created="2014-02-18T15:01:50Z" id="35392179">I'd use it as well.  This is my setup:

```
              foo
  +------------+------------+
  |                         |
foo_a                     foo_b
  |                         |
foo_a_1368699241          foo_b_1368699241
```

Where foo, foo_a, and foo_b are aliases.  Right now, foo points to foo_a_138699241 and foo_b_136899241 so it can include both.  It'd be more convenient if foo count point to foo_a and foo_b.  The swaps can still be atomic because because you can swap both in the same call but there would be fewer moving parts to juggle with aliases to aliases.
</comment><comment author="zwrss" created="2014-03-13T14:07:27Z" id="37536358">I'd love to see that feature. That would allow me to handle reindexing while not bothering about all the aliases to that index but one "main" alias that all others should point to. Definitely a nice feature to have. 
</comment><comment author="bhaskarvk" created="2014-05-01T14:07:47Z" id="41912642">I would love to see this feature too.
</comment><comment author="RomanShestakov" created="2014-07-23T09:07:00Z" id="49849645">+1 would be very useful feature 
</comment><comment author="jaytaylor" created="2014-07-24T21:55:10Z" id="50083695">+1 this would be very valuable.
</comment><comment author="clintongormley" created="2014-07-25T08:50:24Z" id="50123679">@javanna any thoughts on this?
</comment><comment author="nik9000" created="2014-07-25T12:08:45Z" id="50140628">If no once gets a chance to look at this in the next few days I can pick it
up.  Its something I've wanted for a while but not enough to build it....

On Fri, Jul 25, 2014 at 4:50 AM, Clinton Gormley notifications@github.com
wrote:

&gt; @javanna https://github.com/javanna any thoughts on this?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3138#issuecomment-50123679
&gt; .
</comment><comment author="clintongormley" created="2014-07-25T12:16:13Z" id="50141215">I don't believe anybody is working on it.  it's still in the discussion phase though, so perhaps let us mull over it a bit before starting? I don't want you to do work that we can't use because it interferes with something we're not aware of yet.
</comment><comment author="nik9000" created="2014-07-25T12:17:40Z" id="50141339">Sure!  I'm not going to have time for the next few days anyway.

On Fri, Jul 25, 2014 at 8:16 AM, Clinton Gormley notifications@github.com
wrote:

&gt; I don't believe anybody is working on it. it's still in the discussion
&gt; phase though, so perhaps let us mull over it a bit before starting? I don't
&gt; want you to do work that we can't use because it interferes with something
&gt; we're not aware of yet.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3138#issuecomment-50141215
&gt; .
</comment><comment author="javanna" created="2014-07-25T12:45:49Z" id="50143832">I like the idea too, we need to find a good way to resolve aliases to indices though if we allow for it, as aliases might point to other aliases...
</comment><comment author="clintongormley" created="2014-07-25T12:50:23Z" id="50144270">... and to ensure that we don't loop:  `foo` -&gt; `bar` -&gt; `foo` -&gt; &#8734;
</comment><comment author="nik9000" created="2014-07-25T12:58:29Z" id="50145076">Yeah - loops should cause 400 errors when you attempt to build them.  Maybe
500 errors if you are able to sneak them in and try to use them.  Certainly
not spin forever.....

On Fri, Jul 25, 2014 at 8:50 AM, Clinton Gormley notifications@github.com
wrote:

&gt; ... and to ensure that we don't loop: foo -&gt; bar -&gt; foo -&gt; &#8734;
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3138#issuecomment-50144270
&gt; .
</comment><comment author="stevemer" created="2014-08-06T21:49:31Z" id="51402430">Has it been decided yet whether this is being implemented?
</comment><comment author="nik9000" created="2014-08-06T22:26:17Z" id="51406280">I have bigger fish to fry at the moment.  If I were to implement it the
first thing is do is look at how hard loop detection would be. Like how
hard inserting it into the right spots would be.
On Aug 6, 2014 10:49 PM, "Steve Merritt" notifications@github.com wrote:

&gt; Has it been decided yet whether this is being implemented?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3138#issuecomment-51402430
&gt; .
</comment><comment author="perryn" created="2015-01-13T05:53:40Z" id="69699018">Another vote for this feature.

(I'm looking at combining mutli-tenancy via filtered aliases with index-per-timeframe and I'd rather not have to manage a combinatorial explosion of aliases)
</comment><comment author="jaytaylor" created="2015-01-14T18:18:32Z" id="69963602">FWIW, my use case was similar.  This would be a very useful feature imo!

On Mon, Jan 12, 2015 at 9:54 PM, Perryn Fowler notifications@github.com
wrote:

&gt; Another vote for this feature.
&gt; 
&gt; (I'm looking at combining mutli-tenancy via filtered aliases with
&gt; index-per-timeframe and I'd rather not have to manage a combinatorial
&gt; explosion of aliases)
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/3138#issuecomment-69699018
&gt; .
</comment><comment author="ghost" created="2015-01-27T09:31:41Z" id="71615726">+1
</comment><comment author="jannemann" created="2015-02-03T14:13:39Z" id="72656514">+1
</comment><comment author="Osiriz" created="2015-02-03T14:22:13Z" id="72657908">+1
</comment><comment author="kubz" created="2015-03-03T09:31:01Z" id="76913289">+1
</comment><comment author="bradvido" created="2015-03-16T19:11:38Z" id="81878879">I have many use cases for this as well, especially since aliases can be filtered -- it would be nice to "nest" those filtered aliases instead of repeating the same common filters in each alias.

For those not aware, you can specify an alias when you create an alias, but it probably isn't working as you'd expect (just adds the alias to the "root" index) as discussed here: https://github.com/elastic/elasticsearch/issues/10106
</comment><comment author="lewchuk" created="2015-04-09T22:00:13Z" id="91367199">+1 for aliases of aliases, my usecase is multiple streams of time based data, I want to window alias each stream individually and have a combined alias for all my streams.
</comment><comment author="j0hnsmith" created="2015-06-25T11:37:53Z" id="115216941">+1 for aliases of aliases
</comment><comment author="mattaylor" created="2015-08-05T16:25:46Z" id="128058266">+1
</comment><comment author="nik9000" created="2015-08-05T16:33:44Z" id="128061959">So I took a look at this a while ago and its not a simple thing to implement which is why no one's done it.
</comment><comment author="pcerioli" created="2015-09-08T17:20:03Z" id="138639097">+1 I would love to see this feature added as well.
</comment><comment author="clintongormley" created="2015-09-19T17:43:19Z" id="141692955">We've talked about this a lot internally and nobody is particularly keen on it.  For a start, there is nowhere to store an alias which points to other aliases (an alias is stored with the index it points to).  Then you have the problems of cycles.  Plus there is a practical limit to the number of aliases you can use, as they are stored in the cluster state.

I think we are unlikely to implement this feature.
</comment><comment author="colings86" created="2015-11-13T10:16:36Z" id="156387920">Agreed, closing this issue
</comment><comment author="yinrong" created="2016-04-05T07:14:23Z" id="205690558">+1 need this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch highlights entire word instead of just the query when ngram filter is used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3137</link><project id="" key="" /><description>when using an nGram filter on a field or index, if you try to highlight said field (or a field in an index that has an nGram filter defined on it, in search results, elasticsearch highlights the entire word instead of just the query.

so if I have the text "American" and I search for "rican"
highlighting should look like this ----&gt; Ame **rican**
but instead it does this ---&gt; **American**

To see this in action just follow the instructions here http://stackoverflow.com/a/15005321/141822
you get this output, which is clearly wrong

```
{
  "took" : 11,
  "timed_out" : false,
    "_shards" : {
      "total" : 1,
      "successful" : 1,
      "failed" : 0
    },
    "hits" : {
      "total" : 3,
      "max_score" : 0.71231794,
      "hits" : [ {
        "_index" : "myindex",
        "_type" : "product",
        "_id" : "0KyaIB8xRmqE-g0hl0ky6g",
        "_score" : 0.71231794,
        "fields" : {
         "code" : "Samsung Galaxy i7500"
        },
        "highlight" : {
          "code.ngram" : [ "&lt;em&gt;Samsung Galaxy i7500&lt;/em&gt;" ],
          "code" : [ "&lt;em&gt;Samsung Galaxy i7500&lt;/em&gt;" ]
        }
      }, {
        "_index" : "myindex",
        "_type" : "product",
       "_id" : "vZwpcBu0QAyGmP9LHz1hUA",
       "_score" : 0.71231794,
        "fields" : {
          "code" : "Samsung Galaxy 5 Europa"
        },
        "highlight" : {
          "code.ngram" : [ "&lt;em&gt;Samsung Galaxy 5 Europa&lt;/em&gt;" ],
          "code" : [ "&lt;em&gt;Samsung Galaxy 5 Europa&lt;/em&gt;" ]
        }
      }, {
        "_index" : "myindex",
        "_type" : "product",
        "_id" : "7sNkZAlxSlmuLZA9S68bvg",
        "_score" : 0.71231794,
        "fields" : {
          "code" : "Samsung Galaxy Mini"
        },
        "highlight" : {
          "code.ngram" : [ "&lt;em&gt;Samsung Galaxy Mini&lt;/em&gt;" ],
          "code" : [ "&lt;em&gt;Samsung Galaxy Mini&lt;/em&gt;" ]
        }
      } ]
    }
  }
```

With the whitespace tokenizer (vs keyword tokenizer in this case), it highlights just the word with the match in it, which is still not expected behavior
</description><key id="15152398">3137</key><summary>elasticsearch highlights entire word instead of just the query when ngram filter is used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ike-bloomfire</reporter><labels /><created>2013-06-05T03:32:08Z</created><updated>2016-03-08T17:58:34Z</updated><resolved>2013-06-06T06:52:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ike-bloomfire" created="2013-06-05T20:45:29Z" id="19007221">This problem won't exhibit itself if you do an upgrade and continue using an old index. It only happens on new indexes created in 0.90.1 or 0.90.0 ... 
</comment><comment author="s1monw" created="2013-06-06T06:52:58Z" id="19027931">I am afraid this is a feature not a bug. We had to drop the behaviour in the standard ngram filter since it produces broken offsets that can lead to massive offset indices and exceptions in the highlighter. If you want to have the same behaviour as 0.20 etc. you need to map the token filter like this:

```
  "old_ngram": {
               "type": "ngram",
               "min_gram": 2,
               "max_gram": 2,
               "version": "4.1"
            }
```

we are currently working on a fix in lucene that makes this possible without tricks

simon
</comment><comment author="ike-bloomfire" created="2013-06-06T08:50:46Z" id="19032296">Thanks for the explanation @s1monw! This is good to know.

I'm a little confused by the alternative you provided, specifically the ngram settings. If the min_gram and max_gram are the same number how will elasticsearch know what the real min_gram and max_gram are?
</comment><comment author="ike-bloomfire" created="2013-06-06T09:10:21Z" id="19033208">I just figured out that it was a typo, and that you only have to specify the "version" in there. I appreciate having this fallback. Thank you very much.

So I imagine when I'm using version 4.1 of this ngram filter, all the problems of "massive offset indices and exceptions in the highlighter" will still remain?
</comment><comment author="s1monw" created="2013-06-06T14:55:10Z" id="19050720">&gt; So I imagine when I'm using version 4.1 of this ngram filter, all the problems of "massive offset indices and exceptions in the highlighter" will still remain?

kind of, we try to prevent them with some internal reordering but ideally you should only use the deprecated stuff if you really need to. I think it's ok at this point but once we have the improved ngram support we work on you should likely switch. BTW. word delimiter has similar problems. 

&gt; I'm a little confused by the alternative you provided, specifically the ngram settings. If the min_gram and max_gram are the same number how will elasticsearch know what the real min_gram and max_gram are?

oh sorry I was only pointing you towards the version. :) I'd personally use min==max if I'd use it but it's up to you
</comment><comment author="ike-bloomfire" created="2013-06-11T19:37:36Z" id="19286960">&gt; kind of, we try to prevent them with some internal reordering but ideally you should only use the deprecated stuff if you really need to. 

Yeah. I tried using the version 4.1 stuff and I was still running into the **out of bounds** exceptions that I had with 19.2 :\

&gt; I think it's ok at this point but once we have the improved ngram support we work on you should likely switch. BTW.

I was curious about this. Are you saying that the old way of highlighting will make a come back in a future version of elasticsearch?
</comment><comment author="s1monw" created="2013-06-11T20:23:47Z" id="19289668">&gt; Yeah. I tried using the version 4.1 stuff and I was still running into the out of bounds exceptions that I had with 19.2 :\

do you use stored field instead of source? I fixed this lately and this will be in the upcoming release, can you check 0.90 branch on github if that fixes your problem to make sure we have it fixed?

&gt; I was curious about this. Are you saying that the old way of highlighting will make a come back in a future version of elasticsearch?

Yes, it will likely come in the next release. Given the fact that this all relies on a bug / broken behaviour this was the safest choice. 
</comment><comment author="ike-bloomfire" created="2013-06-11T20:33:31Z" id="19290236">&gt; do you use stored field instead of source?

So just basically store a every field that I highlight with the version 4.1 stuff, yes?

&gt; I fixed this lately and this will be in the upcoming release, can you check 0.90 branch on github if that fixes your problem to make sure we have it fixed?

okay. So this is stuff that will be in 0.90.2 correct?

&gt; Given the fact that this all relies on a bug / broken behaviour this was the safest choice.

Can you explain this a bit for me, I wasn't sure what you were talking about here.

&gt; Yes, it will likely come in the next release

We rely heavily on elasticsearch highlighting for our typeahead stuff, so this is a big relief. I thought the current behavior was going to be permanent.
</comment><comment author="s1monw" created="2013-06-11T20:40:05Z" id="19290667">&gt; So just basically store a every field that I highlight with the version 4.1 stuff, yes?

yeah so this should be fixed

&gt; okay. So this is stuff that will be in 0.90.2 correct?

YES!

&gt; Can you explain this a bit for me, I wasn't sure what you were talking about here.

Well NGram Filter and Tokenizer where entirely broken causing all these StringIndexOOB Exception and bloated term vectors etc. so fixing the broken behaviour was the only way to go here and deprecate the behaviour basically only exposing it via `version` 

&gt; We rely heavily on elasticsearch highlighting for our typeahead stuff, so this is a big relief. I thought the current behavior was going to be permanent.

We will allow this with NGramTokenizer but not with NGramTokenFilter we are working on a `pre-tokenization` step that splits on whitespaces for instance. The problem here is that only tokenizer can reliably modify token offsets without breaking the TokenStream contract in lucene and produce broken offsets so we can only allow this with Tokenizers. If you need to have this in a filter you need to pull it in via `version` which I discourage once NGramTokenizer if fixed. I hope this helps you moving forward.
</comment><comment author="BowlingX" created="2013-08-05T16:03:57Z" id="22116327">I ran into the same issue, is there something else i can or should use? (so instead of setting the version parameter)
</comment><comment author="s1monw" created="2013-08-05T16:05:50Z" id="22116469">@BowlingX you should use NGramTokenizer instead that should give you the partial highlighting capabilities you need. see http://www.elasticsearch.org/guide/reference/index-modules/analysis/ngram-tokenizer/
</comment><comment author="ariasdelrio" created="2014-02-25T14:20:36Z" id="36011279">@s1monw I was using the 4.1 stuff for edge_ngram, but now I want to change to an NGramTokenizer. My problem is that we were using an ICU tokenizer before. Is there a way to keep this? The only option that the NGramTokenizer seems to allow is to separate words based on character classes.
</comment><comment author="s1monw" created="2014-02-25T14:22:10Z" id="36011445">no you can't really do the ICU tokenization together with the NGramTokenizer
</comment><comment author="mcuelenaere" created="2015-10-08T23:25:04Z" id="146714735">@s1monw what's the status on partial highlighting with NGramTokenFilter (not NGramTokenizer)? Is there an issue to track this?
</comment><comment author="wkiser" created="2016-03-08T03:06:27Z" id="193579374">+1 would also like to know the status of partial highlighting using an (edge)NGramTokenFilter. Or if there are any workarounds for 2.2.0. 
</comment><comment author="clintongormley" created="2016-03-08T08:59:12Z" id="193670947">@mcuelenaere @wkiser The contract of token filters is that they can't change positions or offsets, so an ngram (or edge ngram) token filter will always use the offsets of the whole original word.

For partial highlighting, look at using the ngram and edge ngram tokenizers instead.  
</comment><comment author="wkiser" created="2016-03-08T16:50:12Z" id="193860434">@clintongormley one of the requirements that I am working with is supporting typeahead search for name synonyms. I have this working by chaining synonym and edgengram token filters. Switching to ngramtokenizer seems like I would lose typeahead on all the synonyms.

Am I out of luck with accurate highlighting here? Is there no slower, non offset based highlighter?
</comment><comment author="clintongormley" created="2016-03-08T17:58:34Z" id="193891715">@wkiser you are out of luck, as far as i'm aware
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlight is not working on _source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3136</link><project id="" key="" /><description>I am trying to perform a query on all fields.
I would like to know which fields contain the matched string.

I am using the default mapping for ES. I am passing a json object when putting the documents into ES.

I am then searching on all fields. It works, but I cannot manage to highlight the matched string and/or the field containing the matched string.
</description><key id="15143816">3136</key><summary>Highlight is not working on _source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">lucabelluccini</reporter><labels /><created>2013-06-04T22:03:31Z</created><updated>2013-08-26T19:34:45Z</updated><resolved>2013-08-26T19:34:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pecke01" created="2013-06-05T15:06:16Z" id="18982636">When you say searching on all fields I assume that you are searching using the _all field and not specifying "all fields".

When you want to highlight something it needs to be a field that is stored or part of the _source. Since the _all field isn't any of those the highlightning is not supported by default.
</comment><comment author="lucabelluccini" created="2013-06-05T17:51:04Z" id="18994828">Hi, thanks for the reply.
There's a way to search on all fields (single fields are extracted from source) without specifying all of them?
And if I search on _source, I'll get the sub-fields of the source?
</comment><comment author="roytmana" created="2013-06-13T12:37:51Z" id="19388781">if you search _all but specify fields you are interested in in highlights section, they will be highlighted
</comment><comment author="javanna" created="2013-08-26T18:15:36Z" id="23283092">Hi @lucabelluccini, may I ask if you solved the problem you encountered?

Anyway, for this kind of issues/questions, our [google group](https://groups.google.com/forum/#!forum/elasticsearch) is probably a better fit, give it a try next time if you haven't don't it yet ;)
</comment><comment author="lucabelluccini" created="2013-08-26T18:29:48Z" id="23284312">I was on my first steps into ES.
So turned out that you need to pass the list of fields to be highlighted...
</comment><comment author="javanna" created="2013-08-26T19:34:45Z" id="23288643">Ok thanks for your quick feedback.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>OOM causes data loss on 0.20.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3135</link><project id="" key="" /><description>Hi there,

Last week we had a major crash in our production cluster. Due to some faulty configuration the machines when out of memory and the whole cluster crashed.

After a reconfiguration and a restart we discovered that shard 8 wasn't recovering. We checked the data in disk and found out that the shard had half of the expected size.
We ran the command: "java -cp lucene-core-3.6.1.jar -ea:org.apache.lucene... org.apache.lucene.index.CheckIndex /chroot/pr/elasticsearch/data/PR/nodes/0/indices/spc/8/index/ -fix" but no data was recovered. Both the master and the replica of shard 8 had the exact same size.

We are running ES 0.20.6. Our index has 20 shards, each one with a replica. We have 6 machines and our cluster size is nearing the 250GB (500GB with the replicas). Our configuration is this: http://pastie.org/8006818 . 

I published our logs in dropbox: https://dl.dropboxusercontent.com/u/53354942/PR-logs.zip . Let me know if you need more logs.
</description><key id="15143533">3135</key><summary>OOM causes data loss on 0.20.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MagmaRules</reporter><labels /><created>2013-06-04T21:57:03Z</created><updated>2014-08-08T12:15:29Z</updated><resolved>2014-08-08T12:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="MagmaRules" created="2013-06-06T13:03:07Z" id="19043417">Yesterday we had another crash, same reason. This time there was no data loss.
</comment><comment author="clintongormley" created="2013-06-18T10:56:18Z" id="19603958">@imotov @s1monw any ideas on this one?
</comment><comment author="MagmaRules" created="2013-06-18T11:07:17Z" id="19604455">The cluster crashed again and we lost a full shard. The shard disappeared. It is not even in disk. I will make the logs available ASAP.
</comment><comment author="s1monw" created="2013-06-18T12:34:00Z" id="19608089">Are you always getting OOM when your cluster crashes? I wonder if the shard disappears because it can't be recovered since you hit OOM during recovery? Is it possible that your nodes are pretty much at the limits memory wise? I'd be interested if you hit a too many open files exception at some point? 
</comment><comment author="MagmaRules" created="2013-06-18T13:34:27Z" id="19611239">"Are you always getting OOM when your cluster crashes?"
Yes.

"I'd be interested if you hit a too many open files exception at some point?"
We are not seeing too many open files in the logs. I know about this issue: https://github.com/elasticsearch/elasticsearch/issues/2812 . Unfortunately it doesn't seem the same. Although it seems similar.

"Is it possible that your nodes are pretty much at the limits memory wise"
Yes its possible. I'm working on that. The problem is just the data loss.
</comment><comment author="MagmaRules" created="2013-06-18T14:57:49Z" id="19616828">Just to add some more info. This time the shard disappeared from the filesystem. The only way we were able to recover was by copying an empty shard from a dev machine.
</comment><comment author="spinscale" created="2014-02-21T16:08:29Z" id="35744604">do you still hit this with a current elasticsearch version or might it make sense to close this one?
</comment><comment author="MagmaRules" created="2014-02-21T16:14:57Z" id="35745293">I guess =). I haven't tried to replicate it lately.
The only environment i had these problems was in production and a daily reboot was implemented to prevent the crashes. 
We did upgrade to 0.90 but the reboot was not removed so I can't say for sure the problem was fixed. On the other hand I can't say it wasn't =).
</comment><comment author="clintongormley" created="2014-08-08T12:15:29Z" id="51593384">Please reopen if this is still an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Static analysis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3134</link><project id="" key="" /><description>Fixes some problems discovered by findbugs, metrics projects.
</description><key id="15141311">3134</key><summary>Static analysis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">rhryciuk</reporter><labels /><created>2013-06-04T21:08:54Z</created><updated>2014-09-03T09:48:03Z</updated><resolved>2014-09-03T09:47:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2014-08-11T08:38:59Z" id="51754388">@rhryciuk Your PR got missed and some refactorings don't apply anymore, yet some of them do and I would be interested in applying them, would you be ok with that? If yes, could I ask you to sign the CLA so that I can get it merged in?
http://www.elasticsearch.org/contributor-agreement/
</comment><comment author="jpountz" created="2014-09-03T09:47:58Z" id="54274082">Closing due to lack of feedback. Please feel free to reopen when you sign it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change comments to reflect getTotalCount and getOtherCount return document counts, not facet counts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3133</link><project id="" key="" /><description>See also: https://github.com/elasticsearch/elasticsearch.github.com/pull/429
</description><key id="15130328">3133</key><summary>Change comments to reflect getTotalCount and getOtherCount return document counts, not facet counts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jredburn</reporter><labels /><created>2013-06-04T17:19:40Z</created><updated>2014-07-16T21:53:16Z</updated><resolved>2013-08-26T18:30:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-26T18:30:11Z" id="23284348">Closing since the original comments seem correct. Have a look at https://github.com/elasticsearch/elasticsearch.github.com/issues/429 to know more.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API privison to delete index level settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3132</link><project id="" key="" /><description>Use case - Routing settings are made to not use a machine , but later we need to use that machine. 

We should be able to delete settings related to routing.
</description><key id="15121784">3132</key><summary>API privison to delete index level settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">Vineeth-Mohan</reporter><labels /><created>2013-06-04T14:35:43Z</created><updated>2013-10-05T14:53:44Z</updated><resolved>2013-10-05T14:53:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-26T18:20:59Z" id="23283600">Hi @Vineeth-Mohan , not sure but this sounds similar to what has been asked in #3572 . What do you think?

If that's not the case, could you please elaborate a bit more and help me out understanding what you meant?
</comment><comment author="javanna" created="2013-10-05T14:53:44Z" id="25749926">As far as I understood this is about shards allocation and filtering settings. Closing as duplicate of #3572 which seems to address the same. @Vineeth-Mohan feel free to reopen if you meant something else.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Feature Request: meta per property/field in mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3131</link><project id="" key="" /><description>Would like the ability to have meta data per property in the mapping, and have GET mapping return it.

``` javascript
{
  theytype: {
    properties: {
      field1: {
        type: "long",
        _meta: {
          info:"field1 info"
        }
      },
      field2: {
        _meta: {
          info:"urls mutli field info"
        },
        type: "multi_field",
        path: "just_name",
        omit_norms: true,
        fields: {
          field2: {type: "string", analyzer: "snowball"},
          rawfield2: {type: "string", index: "not_analyzed"}
        }
      }
    }
  }
}
```

It looks like I could use fielddata for this too, however seems like a hack, and I wasn't sure if it would hurt performance.  Feature might be useful for folks who want to add comments.
</description><key id="15120927">3131</key><summary>Feature Request: meta per property/field in mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2013-06-04T14:20:21Z</created><updated>2013-08-28T10:08:00Z</updated><resolved>2013-08-28T10:08:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-06-23T02:17:59Z" id="19867939">Same here https://github.com/elasticsearch/elasticsearch/issues/2949
</comment><comment author="javanna" created="2013-08-28T10:08:00Z" id="23403649">Makes sense, closing as duplicate of #2857 though.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch not responding. Possible GC issue.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3130</link><project id="" key="" /><description>I am trying to upgrade elasticsearch from 0.19.12 to 0.90.x. After upgrading elasticsearch on the workers of my testing environment I found out that elasticsearch server started to become unresponsive on every distinct worker. The workers are regular PCs with one elasticsearch instance with its own cluster name. They are not related at all.

I managed to figure out that each of the elasticsearch instances were run out of heap memory.

I wrote a script that does the same thing that the tests of our application perform to the elasticsearch server(creating the proper indexes before the start of the suite, performing some queries and truncating the contents of each index after each run).

I compared the heap memory usage between the 0.19.12 and 0.90.1 versions with this script. I found out that in the case of the version 0.19.12, the garbage collector was applied and the heap memory was freed, and the server continued to run without any problem.

But in the case of 0.90.1, although the garbage collector was applied, the memory was not freed.

I attach the screenshots of bigdesk for both versions and the script that I used for conducting this comparison.

```
#!/bin/bash

while true; do

  echo "Deleting all indexes"
  curl -XDELETE 'http://localhost:9200/_all/?pretty' &gt; /dev/null  2&gt;&amp;1

  echo "Creating index"
  curl -XPUT 'http://localhost:9200/test/?pretty' -d '{
   "index" : true,
   "settings": {
     "number_of_shards" : 1
   },
   "_default_": {
     "include_in_all" : true
   },
   "mappings" : {
     "blog_post" : {
       "type" : "object",
       "_all" : {"enabled" : true },
       "_source": {"enabled" : true, "compress" : true}
     }
   }
  }' &gt; /dev/null  2&gt;&amp;1

  for i in {1..10}
  do
    echo "Truncating $i times"
    curl -XDELETE 'http://localhost:9200/test/blog_post/_query?pretty' -d '{
      "match_all" : {}
    }' &gt; /dev/null  2&gt;&amp;1

    echo "Refreshing $i times"
    curl -XPOST 'http://localhost:9200/test/_refresh?pretty' &gt; /dev/null  2&gt;&amp;1
  done

done
```

Elasticsearch 0.19.12:
![heap-memory-0-19-12](https://f.cloud.github.com/assets/827828/605783/f4e58206-cd1d-11e2-8b58-962d5b6790c8.png)

Elasticsearch 0.90.1:
![heap-memory-0-90-1](https://f.cloud.github.com/assets/827828/605784/faecf986-cd1d-11e2-9cb8-0a030bdbea39.png)

I have mentioned this issue at the google group of elasticsearch: [Elasticsearch becomes unresponsive after updating to 0.90.x](https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/yqhCO-NNOXs)
</description><key id="15119536">3130</key><summary>Elasticsearch not responding. Possible GC issue.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">astathopoulos</reporter><labels /><created>2013-06-04T13:54:19Z</created><updated>2013-06-25T08:00:12Z</updated><resolved>2013-06-25T07:18:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="usmanm" created="2013-06-20T22:41:27Z" id="19788259">We are experiencing a similar problem. In our case, the nodes are heavy duty with 15GB of memory (8GB mlocked by ES) and are part of a single cluster. Previously we used to observe the heap memory dropping significantly (maybe to 2-3GB) once it got close to 8GB (probably because the GC kicked in). However after upgrading to 0.90, we see that the heap memory keeps on increasing and plateaus around 7-8GB. When that happens CPU utilization suddenly spikes and the GC also starts happening much more frequently. We've tried restarting the cluster but to no effect.
</comment><comment author="astathopoulos" created="2013-06-21T08:28:03Z" id="19803886">@usmanm Does your application performs frequent index creations and destructions?
</comment><comment author="usmanm" created="2013-06-21T08:55:50Z" id="19804950">Not on a daily basis but yes we do create and delete indices regularly (maybe once a week). We usually do it when we need to make some changes to our index schema which might conflict with the schema of the existing index.
</comment><comment author="astathopoulos" created="2013-06-21T09:11:45Z" id="19805627">I believe that the GC cannot delete objects that were created during the creation/destruction of an index that's why the heap memory is not freed. But, I haven't fully investigated it. It's just a guess based on what I see. That's what I reproduced in the description of this issue.
</comment><comment author="usmanm" created="2013-06-21T19:40:13Z" id="19836524">We restarted the entire cluster following such an outage a few days back and we haven't deleted/created new indices since then. But we still continue to face this issue. Your hypothesis might be correct, but probably doesn't explain it for us.
</comment><comment author="bleskes" created="2013-06-25T07:18:06Z" id="19957426">@usmanm kimchy's push doe solves the issues which is shown in the reproduction script in this issue. I'm not sure though you are having the same issue as you are only once a week. It will take a long time before this leak has an effect.  I'm going to close this issue as it is solved as stated. If you are still seeing memory issues, can you please open a new issue describing your situation?
</comment><comment author="kimchy" created="2013-06-25T07:26:38Z" id="19957782">quick question, can you share your settings of the index you create and the elastucsearch.yml? are you by any chance using soft cache for the field data?
</comment><comment author="astathopoulos" created="2013-06-25T08:00:12Z" id="19959133">@kimchy Yes, we are using soft cache:

```
index.cache.field.expire: 10m
index.cache.field.type: soft
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timeout is not respected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3129</link><project id="" key="" /><description>On this issue https://github.com/elasticsearch/elasticsearch/issues/3128
if I add a timeout to the query(1ms, for example), the query instantly returns with no results, which is what I would expect. 
But if the timeout is big enough(as to allow finding the document), then the query doesn't return at all,which is not what I would expect. 
I guess this is the "desired" behavior, since the idea is to return whatever documents could be found before the timeout, and then highlight it and etc.
But if there are such cases where the highlighting is actually the "heavy" part of the query, doesn't it make sense to apply the timeout globally?
</description><key id="15108802">3129</key><summary>Timeout is not respected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-06-04T09:13:14Z</created><updated>2014-07-18T11:12:24Z</updated><resolved>2014-07-18T11:12:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-04T12:42:03Z" id="18906185">yea, this is the current behavior, where the timeout applies to the query execution part, and it is the intended behavior. It does make sense to allow for timeouts to apply to other aspects, but it gets tricky in terms of definition since what will end up being returned in some of those cases is tricky to define.
</comment><comment author="lmenezes" created="2013-06-04T13:19:14Z" id="18908178">@kimchy true... But on the other hand, if I set a timeout of 3000ms for example, it just means I'm not willing to wait for more than that, and will timeout to the end client anyway. 
Even though I can see all the benefits of timing out and still retrieving maybe a "partial result set", I think its even more useful to know that I wont have long queries running using resources when I don't care about their result anymore. 
Maybe an "abort_on_timeout" option could do the trick and still maintaing the partial result timeout? 
</comment><comment author="clintongormley" created="2014-07-18T11:12:24Z" id="49419893">Closed in favour of #4586 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unusual CPU/Memory Usage while Highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3128</link><project id="" key="" /><description>So, trying to run this on ES 0.90 results in an OOM...
Its quite an unusual query I agree, but still the behavior seems wrong. 

```
curl -XPOST http://localhost:9200/test_hl -d '{ "index": { "number_of_shards": "1", "number_of_replicas": "0", "analysis": { "filter": { "wordDelimiter": { "type": "word_delimiter", "split_on_numerics": "false", "generate_word_parts": "true", "generate_number_parts": "true", "catenate_words": "true", "catenate_numbers": "true", "catenate_all": "false" } }, "analyzer": { "custom_analyzer": { "tokenizer": "whitespace", "filter": [ "lowercase", "wordDelimiter" ] } } } } }'

curl -XPUT http://localhost:9200/test_hl/profile/_mapping -d '{ "profile": { "dynamic": "strict", "properties": { "id": { "type": "integer", "index": "not_analyzed", "store": "yes" }, "content": { "type": "string", "index_analyzer": "custom_analyzer", "search_analyzer": "custom_analyzer", "store": "yes", "term_vector": "with_positions_offsets" } } } }'

curl -XPUT http://localhost:9200/test_hl/profile/2 -d '{"content": "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature", "id": 2}'

curl -XGET http://localhost:9200/test_hl/profile/_search -d '{ "from": 0, "size": 10, "query": { "match": { "content": { "query": "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature", "type": "phrase" } } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }'
```
</description><key id="15064592">3128</key><summary>Unusual CPU/Memory Usage while Highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels /><created>2013-06-03T12:35:20Z</created><updated>2014-04-17T03:35:47Z</updated><resolved>2013-06-06T09:36:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kroepke" created="2013-06-04T12:42:18Z" id="18906208">I've been looking at this problem and it basically consumes all its time in &lt;code&gt;org.apache.lucene.search.vectorhighlight.FieldQuery()&lt;/code&gt; trying to expand the flat queries in to PhraseQueries for later matching it against the Hits.

Put another way: It's extremely easy to break the fast vector highlighter by asking it to highlight the exact field content, provided that the query terms and stored field terms are the same. In this case the implementation degenerates to a cartesian product of the terms, leading to memory exhaustion.

Moral of the story, beware of automatically generated search queries that come from field contents when also highlighting.
Maybe a fail safe mode could be added somewhere, stopping the iteration during construction of the FieldQuery object to prevent it bringing down the cluster in seconds.
</comment><comment author="s1monw" created="2013-06-05T15:42:20Z" id="18985381">This in-fact a bug in Lucene or maybe a really bad situation. not sure if it's a bug per-se. Yet, the problem is that due to the settings `"generate_number_parts": "true", "catenate_words": "true",` this creates a MultiPhraseQuery (multiple token on the same position) and  if this one gets too long it kind runs in a very expensive loop. Setting those to "false" makes it fast and it seems to produce the right thing too I will look into this more in detail in lucene land
</comment><comment author="kroepke" created="2013-06-05T15:58:25Z" id="18987143">This definitely is a corner case, I agree, unfortunately our system sometimes generates queries like this based on user action, and these are tricky to filter out (although we will try to, of course).
This specific example will generate 4095 queries internally when constructing the FieldQuery object, and so far I haven't quite found a way to have it bail out earlier. Needless to say you run out of memory rather quickly when this happens.
I agree it's not really a bug, but rather a unfortunate worst-case scenario for the highlighter, but it has very dire consequences once you hit it.
Let me know if you need anything more from us.
</comment><comment author="s1monw" created="2013-06-05T18:27:38Z" id="18997206">Yeah I know it's tricky though. I wonder if you can just not use phrase for higlighting here. I mean it's pretty intense though. if you use `generate_number_parts": "false", "catenate_words": "false"` you will not have the problems. you might want to just do this for the search here as a search analyzer or so? I think we will go forward and add a cutoff for this to make sure it doesn't blow up but rather return a not so correct highlighthing result ;), what do you think?
</comment><comment author="s1monw" created="2013-06-06T09:24:52Z" id="19033913">I got a workaround for this in this patch. I think this is the only reasonable thing to-do here really. The highlights will be different if you get the crazy query but it should return quickly. Let me know what you think?
</comment><comment author="lmenezes" created="2013-06-06T09:36:04Z" id="19034441">Fine for me. I mean, I wonder if it even makes sense highlighting this kind of query. I was actually more concerned with cluster stability rather then "correct highlighting" behavior. 
Will this make into 0.90.2?
</comment><comment author="synhershko" created="2013-06-06T10:05:16Z" id="19035734">@lmenezes It does - think about phrase queries where each word passes through a SynonymFilter. You still want to be able to highlight such queries. We were seeing similar issues, thx @s1monw for the fix, I'll test it early next week
</comment><comment author="lmenezes" created="2013-06-06T10:09:27Z" id="19035905">@synhershko I just meant for the case where the query is actually the exact content of an indexed document field.
</comment><comment author="s1monw" created="2013-06-06T10:11:10Z" id="19035972">I think it makes sense to highlight MultiPhraseQueries, I am just concerened about crazy ones that are somewhat generated. I am also all for preventing cluster stability problems and give somewhat bad highlighting instead. I will push this soon
</comment><comment author="synhershko" created="2013-06-06T10:24:04Z" id="19036571">Speaking of cluster stability problems, and sorry for hijacking the thread, the most imminent problem today is the lack of a timeout concept in Lucene. Once ES hands over control to any Lucene component, you are at risk of that thread running forever. This makes ES timeouts be on a "best effort basis" to quote Shay's words, and in practice timeouts are almost never enforced.

This is basically what happened with this FVH issue, but can also happen with complex searches, and we are seeing nodes come unresponsive after a while in our cluster, presumably because of such issues

If only Lucene could respect timeouts, that would be great

/cc @bleskes 
</comment><comment author="s1monw" created="2013-06-06T10:49:17Z" id="19037613">&gt; If only Lucene could respect timeouts, that would be great

when you think about it, how would you implement it? this is a really tough problem and during searches lucene respects it. Each search with a timeout checks if it has timed out after each collected document so this is pretty accurate. Not sure what you are referring to here.

@lmenezes I pushed the fix it will be in 0.90.2
</comment><comment author="synhershko" created="2013-06-06T10:53:37Z" id="19037771">Let me get back to you with more concrete details
</comment><comment author="clintongormley" created="2013-06-06T11:18:53Z" id="19038814">@synhershko Rather move the timeout conversation to issue #3129 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggest Doesn't work with latest .deb package (0.91)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3127</link><project id="" key="" /><description>I installed elastic search on a vm (Ubuntu 12.04) (from scratch) using the .deb package

I followed the instructions here:

http://elasticsearchserverbook.com/elasticsearch-0-90-using-suggester/

I am getting no options back for suggestions.

  "suggest" : {
    "check1" : [ {
      "text" : "crume",
      "offset" : 0,
      "length" : 5,
      "options" : [ ]
    } ]
  }

I should be getting:

"suggest" : {
  "check1" : [ {
    "text" : "crume",
    "offset" : 0,
    "length" : 5,
    "options" : [ {
      "text" : "crime",
      "score" : 0.8,
      "freq" : 1 
     } ] 
   } ] 
 }

There is a general paucity of info about this feature. I tried this as a last measure (i.e install from scratch with online examples) when I couldn't get suggest to work on a current install.

Anyone have any idea why it might be behaving like this?
</description><key id="15042409">3127</key><summary>Suggest Doesn't work with latest .deb package (0.91)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">chokosabe</reporter><labels /><created>2013-06-02T17:16:59Z</created><updated>2013-06-24T12:46:07Z</updated><resolved>2013-06-24T12:46:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-06T14:07:22Z" id="19047289">Hey,

we had no bug reports so far for the suggester so I think it works. A quick peek at the link you provided shows, that there is a typo near `search_type=count` at one query, as the ampersand is escaped. Maybe that is the issue.

If it is not, can you please provide a complete gist for us to reproduce with curl commands (index creation, document indexing and queries), so we can see, where the problems are?
</comment><comment author="spinscale" created="2013-06-24T12:46:07Z" id="19904495">Closing. Assuming that no further information (and no further bugreports) mean it works. If not, please reopen with more information attached. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changing version semantics to be more readable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3126</link><project id="" key="" /><description>The Version class had hard to understand semantics when two versions were
compared against each other.

Sample of the new logic (which is simply the old logic inverted):
- V_0_20_0.before(V_0_90_0) =&gt; true
- V_0_90_0.after(V_0_20_0)  =&gt; true

Closes #3124
</description><key id="14998002">3126</key><summary>Changing version semantics to be more readable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-05-31T15:12:16Z</created><updated>2014-06-18T10:40:08Z</updated><resolved>2013-06-02T13:01:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-05-31T17:33:22Z" id="18759510">I agree this is more readable! A good side-effect is that this PR makes Elasticsearch's Version behavior consistent with Lucene's oal.util.Version.
</comment><comment author="kimchy" created="2013-05-31T17:34:17Z" id="18759569">+1, the whole idea was to be similar to Lucene .)
</comment><comment author="spinscale" created="2013-06-02T13:01:13Z" id="18806774">Closed by 609ad0e57257a838b8f711ed766c9440ecbb565a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fuzzy query ranks typos over exact matches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3125</link><project id="" key="" /><description>Using this query:

"query": {
    "fuzzy": {
      "_all": "excellent"
    }

The top results are all misspellings of excellent, likely as the result of IDF.
This is surely behaviour that typical end users would see as a bug?
If nothing else the docs for fuzzy query should reference FuzzyLikeThis query as an alternative.

The ideal of behaviour of fuzzy was discussed here:
https://issues.apache.org/jira/browse/LUCENE-329
</description><key id="14993461">3125</key><summary>Fuzzy query ranks typos over exact matches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels><label>:Query DSL</label><label>adoptme</label></labels><created>2013-05-31T13:36:50Z</created><updated>2014-12-30T16:33:38Z</updated><resolved>2014-12-30T16:33:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-31T13:40:28Z" id="18745275">@markharwood w00t a 3 digit lucene issue ;) wanna take a look into this issue? ie. provide a patch?
</comment><comment author="markharwood" created="2013-05-31T14:59:31Z" id="18750101">Ugh. This is a Lucene problem with FuzzyQuery. I reduced my test back down to a pure Lucene query opening the ES  index and it reproduced the problem. This is odd behaviour as I thought Lucene had addressed that IDF problem in FuzzyQuery.

It looks like ES's FuzzyQueryParser.java has some options for passing in a custom rewrite method so it could pick a saner default but I expect the fix is better managed "upstream" in Lucene's FuzzyQuery.

Want to close this issue?

Cheers
Mark
</comment><comment author="clintongormley" created="2014-03-01T12:21:00Z" id="36423361">I'd love to see saner scoring for the fuzzy query, presumably with a custom rewrite method. Any advance made here?
</comment><comment author="mericano1" created="2014-07-28T09:27:35Z" id="50316621">+1 we have the same issue. It make fuzzy not useful at all.
</comment><comment author="clintongormley" created="2014-07-28T10:03:35Z" id="50320635">@mericano1 see #6932 
</comment><comment author="mericano1" created="2014-07-28T10:17:29Z" id="50321797">@clintongormley would that work for the suggestion auto complete queries?
</comment><comment author="clintongormley" created="2014-07-28T10:24:06Z" id="50322332">@mericano1 i believe fuzzy in the completion suggester doesn't use scores at all, just the weight specified in the suggestions

/cc @areek 
</comment><comment author="mericano1" created="2014-07-28T10:35:54Z" id="50323227">Yes, it is only specified when indexing. I think the same should apply really, I have no way to make exact matches show at the top.

Should I raise a new issue?
</comment><comment author="clintongormley" created="2014-07-28T10:44:24Z" id="50323914">@mericano1 yeah, you may have a point there.  yes, please open another issue for it
</comment><comment author="mericano1" created="2014-07-28T13:22:38Z" id="50336869">@clintongormley done :+1:  #7060  
</comment><comment author="areek" created="2014-07-28T15:16:13Z" id="50352076">I think this is a know issue, the fuzziness in the completion suggester is achieved by using Levenstein Automata. To give exact matches (distance = 0) higher rank than that of fuzzy suggestion, a lucene change is required, that will report the edit distance and hence could be incorporated in the scoring (this may not be a trivial change unfortunately). I also think there are existing issues that is somewhat relevant to this (#4441, #4759).
</comment><comment author="clintongormley" created="2014-11-29T15:23:55Z" id="64955052">Related to #8352
</comment><comment author="clintongormley" created="2014-12-30T16:33:38Z" id="68371518">Closed in favour of #9103
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change Version methods to be more readable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3124</link><project id="" key="" /><description>Current behaviour

```
Version.V_0_90_1.before(Version.V_0_90_0) == true
```

This is confusing and more readable the other way around.

**Note**: This is a breaking change! Test with care

Also all official plugins need to checked, in order to make sure, we use it right apart from elasticsearch core.
</description><key id="14985417">3124</key><summary>Change Version methods to be more readable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>breaking</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-05-31T09:30:08Z</created><updated>2013-06-02T12:59:26Z</updated><resolved>2013-06-02T12:59:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>rpm upgrade to 0.90.1 overwrote /etc/sysconfig/elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3123</link><project id="" key="" /><description>rpm -Uvh https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.1.noarch.rpm from 0.90.0 overwrites /etc/sysconfig/elasticsearch without warning. It doesn't overwrite /etc/elasticsearch/elasticsearch.yml however.
</description><key id="14983622">3123</key><summary>rpm upgrade to 0.90.1 overwrote /etc/sysconfig/elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">stonith</reporter><labels /><created>2013-05-31T08:37:56Z</created><updated>2013-05-31T14:27:09Z</updated><resolved>2013-05-31T14:27:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-31T09:13:57Z" id="18734297">I'll take a look... both files are marked as configuration in the `pom.xml` when the package is build. I will check if there is a bug.

Thanks for reporting!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Proposal to fix #3079 (span_near query should accept slop = -1)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3122</link><project id="" key="" /><description>This pull request changes the default slop value from -1 to Integer.MIN_VALUE (in SpanNearQueryParser.java).

The bug remains (i.e. you can specify slop = -2147483648, and the SpanNearQueryParser will raise error "span_near must include [slop]"), but unlike -1, I'm not aware of any case where such a slop value is useful.
</description><key id="14958993">3122</key><summary>Proposal to fix #3079 (span_near query should accept slop = -1)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-05-30T20:00:50Z</created><updated>2014-07-16T21:53:16Z</updated><resolved>2013-05-31T08:03:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-30T20:09:30Z" id="18704849">can we just make it an Integer and initialise it with `null` and check for null to raise the exception?
</comment><comment author="iksnalybok" created="2013-05-30T22:52:22Z" id="18713624">Updated to use null check as suggested.
</comment><comment author="s1monw" created="2013-05-31T08:03:39Z" id="18730113">merged, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolator requests return inconsistent/empty results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3121</link><project id="" key="" /><description>We're experience an issue where calls to _percolate return inconsistent results based on which node we hit. Unfortunately I don't have a fully reproducible case; this seems to have happened in two cases, (1) a node fails and leaves/re-enters the cluster (2) a percolator shard goes in to a 'recovering' state.

Here's an example of what we see:

```
~$ curl -XGET 'node1.endpoint:9200/abc/def/_percolate' -d '{"doc":{"foo":"bar"}}'
{"ok":true,"matches":["some-result"]}
~$ curl -XGET 'node2.endpoint:9200/abc/def/_percolate' -d '{"doc":{"foo":"bar"}}'
{"ok":true,"matches":[]}
```

Furthermore, we'll see for the "docs" entry in the below calls:

```
~$ curl -XGET endpoint.node1:9200/_percolator/_status
"docs" : { "num_docs" : 8, "max_doc" : 8, "deleted_docs" : 0 }
~$ curl -XGET endpoint.node2:9200/_percolator/_status
"docs" : { "num_docs" : 8, "max_doc" : 11, "deleted_docs" : 3 }
```

Some notes on the occurrence: 
- We're running 5 nodes, and we will see "matches": [] across some number of them (2 during the most recent event).
- The results returned from each node are consistent, meaning that we will receive empty matches repeatedly from the same bad node.
- Flushing or refreshing do not resolve this.
- The appropriate documents exist on disk in the machines (maybe expected):
  `/location/elasticsearch/nodes/0/indices/_percolator/0/index/_0.fdt`
  contained the same document on both machines.

Resolution:
The only way we can resolve this at the moment is by deleting the _percolator index and re-submitting entries to it.

This is running on Ubuntu with java version 1.6.0_27. We have 5 nodes, 3 indices. ES: 0.20.4 with s3 as the gateway.type.
</description><key id="14953450">3121</key><summary>Percolator requests return inconsistent/empty results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">olimcc</reporter><labels /><created>2013-05-30T18:03:22Z</created><updated>2014-08-08T12:14:07Z</updated><resolved>2014-08-08T12:14:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="olimcc" created="2013-05-30T18:05:26Z" id="18697297">Potentially the same issue as
https://github.com/elasticsearch/elasticsearch/issues/2531

(edited to point to correct bug id)
</comment><comment author="36degrees" created="2013-05-31T08:22:49Z" id="18731592">I think you meant #2531, but yes!
</comment><comment author="clintongormley" created="2014-08-08T12:14:07Z" id="51593290">Old percolators have been replaced.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make it easier to get started with Eclipse.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3120</link><project id="" key="" /><description>I'm proposing a few changes to make life of Eclipse users a little easier. This pull request makes `mvn eclipse:eclipse` generate additional eclipse configuration
files so that Eclipse:
- uses Java 1.6 compliance level,
- truncates lines after 140 chars,
- uses 4 spaces for indentation,
- automatically adds a license header when creating a new class file,
- organizes imports the same way as Intellij Idea (which makes sense I guess
  since most of the code bas has been written with Intellij, this will prevent
  from having large diffs due to the fact that the order of imports has
  changed).

This should make getting started coding on Elasticsearch as easy as:
- `mvn eclipse:eclipse`
- `File &gt; Import &gt; Existing project into workspace` from Eclipse
- code!

What do you think?
</description><key id="14945142">3120</key><summary>Make it easier to get started with Eclipse.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-05-30T15:07:20Z</created><updated>2014-07-11T10:37:39Z</updated><resolved>2013-05-30T15:21:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-30T15:14:15Z" id="18686823">w00t push it!
</comment><comment author="kimchy" created="2013-05-30T15:15:36Z" id="18686920">+1.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolation of big documents fails in 0.90.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3119</link><project id="" key="" /><description>Percolation of documents larger than 32766 bytes fails in ES 0.90.0 with the following Exception. While doing the same with ES 0.20.5 still worked.

```
2013-05-30 00:04:37,275][DEBUG][action.percolate         ] [auto] failed to execute [org.elasticsearch.action.percolate.PercolateRequest@1c3fcd07]
java.lang.RuntimeException: org.apache.lucene.util.BytesRefHash$MaxBytesLengthExceededException: bytes can be at most 32766 in length; got 39747
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:466)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:372)
        at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:450)
        at org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:422)
        at org.elasticsearch.index.percolator.PercolatorService.percolate(PercolatorService.java:111)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:93)
        at org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:41)
        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.apache.lucene.util.BytesRefHash$MaxBytesLengthExceededException: bytes can be at most 32766 in length; got 39747
        at org.apache.lucene.util.BytesRefHash.add(BytesRefHash.java:323)
        at org.apache.lucene.util.BytesRefHash.add(BytesRefHash.java:271)
        at org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:440)
        ... 10 more
```
</description><key id="14943640">3119</key><summary>Percolation of big documents fails in 0.90.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fxh</reporter><labels /><created>2013-05-30T14:40:09Z</created><updated>2013-07-15T23:45:45Z</updated><resolved>2013-05-30T15:04:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-05-30T14:47:21Z" id="18684915">Looks like you hit the maximum term length supported by Lucene. Is it expected that your document has such long terms?
</comment><comment author="jpountz" created="2013-05-30T15:00:01Z" id="18685805">I'm suspecting that one of your fields is marked as not analyzed although it should be?
</comment><comment author="fxh" created="2013-05-30T15:00:33Z" id="18685843">Thanks for the quick response. You are right, I have a multi_field mapping where I store the orig additionally as not_analyzed which for this document has a too large value on it. Apologies, I just got confused as this was somehow working fine (or at least without an exception) in the older version of ES.
</comment><comment author="s1monw" created="2013-05-30T15:04:41Z" id="18686137">@jpountz that is true, mem index has not the limitation of BytesRefHash (2GB and 32KB term length)
@fxh I close this since it seems rather a problem on your side?
</comment><comment author="ajhalani" created="2013-07-15T23:40:40Z" id="21011865">We are facing the same issue. We have multi field with both analyzed and not analyzed and if I percolate with a document that has a large value for the analyzed field it fails. This happens with ES v0.90.1 while older 0.20.x versions worked. 
Gist at  (https://gist.github.com/ajhalani/6004466)

Why is it failing even though the large field in the document is an analyzed one? I apologize in advance if I should have created a new issue instead of commenting on a closed one. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Add NGramTokenizer and NGramTokenFilter to broken chains</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3118</link><project id="" key="" /><description>NgramTokenizer and NGramTokenFilter are broken with a version &lt; 4.2
We should still support these filters but should prevent the StringIOOB
exceptions. Adding these fitlers for the FragmentBuilderHelper will
allow seamless highlighting on fields indexed with those tokenizers or
tokenfilters
</description><key id="14930643">3118</key><summary>Highlighting: Add NGramTokenizer and NGramTokenFilter to broken chains</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-30T08:46:00Z</created><updated>2013-05-30T09:00:32Z</updated><resolved>2013-05-30T08:47:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-30T08:47:05Z" id="18668430">Closed by a89230945f464ab57b404dff7002560af53dd706
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"index_options" cannot be changed by deleting and recreating type mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3117</link><project id="" key="" /><description>Field "index_options" cannot be changed from "docs" to "positions"
by deleting and recreating type mapping if you've already indexed
some docs.

Tried version: 0.20.2, 0.20.6

The problem is caused by the fact that mapping deletion does not 
delete lucene index file on disk, and lucene will merge new field 
info with the ElasticSearch already _deleted_ field info. 
The code is as follows:

void org.apache.lucene.index.FieldInfo.update(...)

```
  if (this.indexOptions != indexOptions) {
    // downgrade
    this.indexOptions = this.indexOptions.compareTo(indexOptions) &lt; 0 ? this.indexOptions : indexOptions;
    this.storePayloads = false;
  }
```

and the index_options will always be "docs". The result is that no
position info is stored and phrase query simply not work.

curl script to reproduce the bug as follows:

curl -XDELETE localhost:9200/index1?pretty

curl -XPUT localhost:9200/index1?pretty -d '
{
  "index.number_of_shards" : 1
}
'
curl localhost:9200/index1/_settings?pretty

curl -XPUT localhost:9200/index1?pretty

curl -XPOST 'localhost:9200/index1/type1/_mapping?pretty' -d '
{
  "type1" : {
    "properties" : {
        "beisen" : {
         "type" : "string",
         "analyzer" : "standard",
         "index_options" : "docs"
       }
     }
  }
}'

curl localhost:9200/index1/_mapping?pretty

curl localhost:9200/index1/type1/111?pretty -d '
{
  "beisen" : "&#21271;&#26862;&#27979;&#35780;"
}
'

curl localhost:9200/index1/type1/_search?pretty -d '
{
  "query" : {
    "text" : {
      "beisen" : {
        "query" : "&#21271;&#26862;",
        "type" : "phrase"
      }
    }
  }
}
'

The above query will find nothing, because the "index_options" in the mapping 
is "docs", no position info is store, the result is expected.

---

curl -XDELETE localhost:9200/index1/type1/_mapping?pretty

curl -XPOST 'localhost:9200/index1/type1/_mapping?pretty' -d '
{
  "type1" : {
    "properties" : {
        "beisen" : {
         "type" : "string",
         "analyzer" : "standard",
         "index_options" : "positions"
       }
     }
  }
}'

curl localhost:9200/index1/_mapping?pretty

curl localhost:9200/index1/type1/111?pretty -d '
{
  "beisen" : "&#21271;&#26862;&#27979;&#35780;"
}
'

curl localhost:9200/index1/type1/_search?pretty -d '
{
  "query" : {
    "text" : {
      "beisen" : {
        "query" : "&#21271;&#26862;",
        "type" : "phrase"
      }
    }
  }
}
'

Delete the mapping and recreate with "index_options" : "positions",
now position info should be stored according to the mapping, so we 
expect the above query returns our doc, but still nothing returned, disappointing...

---

curl -XDELETE localhost:9200/index1?pretty

curl -XPUT localhost:9200/index1?pretty -d '
{
  "index.number_of_shards" : 1
}
'

curl -XPOST 'localhost:9200/index1/type1/_mapping?pretty' -d '
{
  "type1" : {
    "properties" : {
        "beisen" : {
         "type" : "string",
         "analyzer" : "standard",
         "index_options" : "positions"
       }
     }
  }
}'

curl localhost:9200/index1/_mapping?pretty

curl localhost:9200/index1/type1/111?pretty -d '
{
  "beisen" : "&#21271;&#26862;&#27979;&#35780;"
}
'

curl localhost:9200/index1/type1/_search?pretty -d '
{
  "query" : {
    "text" : {
      "beisen" : {
        "query" : "&#21271;&#26862;",
        "type" : "phrase"
      }
    }
  }
}
'

Delete the index (lucene index files on disk got cleared), and 
recreate with "index_options" : "positions", now it works the 
way we expected.
</description><key id="14926871">3117</key><summary>"index_options" cannot be changed by deleting and recreating type mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mashudong</reporter><labels /><created>2013-05-30T06:25:32Z</created><updated>2013-11-04T18:34:31Z</updated><resolved>2013-11-04T18:34:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-30T08:18:08Z" id="18667258">This actually works for me just fine on `0.90` did you run _refresh in between?
</comment><comment author="mashudong" created="2013-05-30T08:42:09Z" id="18668208">No, but I tried just now, nothing changed.
</comment><comment author="mashudong" created="2013-05-31T07:40:28Z" id="18729096">@s1monw could you check this issue on 0.20.2 or 0.20.6?
</comment><comment author="javanna" created="2013-11-04T16:55:30Z" id="27701540">Just tested this. I can confirm it happens on 0.20.6 but doesn't happen anymore starting from 0.90.0.
In the first step, trying to execute a phrase query without positions in the index, 0.90 returns an error. The second query returns results with 0.90 as expected, given that you recreated the mapping with the proper `index_options`. I would definitely recommend to upgrade!
</comment><comment author="javanna" created="2013-11-04T18:34:31Z" id="27710146">Closing as the solution here is just to upgrade to 0.90 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add more information in PluginManager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3116</link><project id="" key="" /><description>New option -list display list of existing plugins
Catch ArraysOutOfBoundException when no arg given to install or remove option
Add description on plugin name structure:
- elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)
- groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
- username/repository          for site plugins (download from github master)
  PR for #3112.
</description><key id="14908926">3116</key><summary>Add more information in PluginManager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-05-29T19:52:54Z</created><updated>2014-07-16T21:53:17Z</updated><resolved>2013-05-30T20:35:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-30T07:49:59Z" id="18666254">Maybe add a `-h` option (no need to document it in the `displayHelp` method), as this is known from many unix commands?
</comment><comment author="dadoonet" created="2013-05-30T07:52:21Z" id="18666327">@spinscale agreed!
</comment><comment author="dadoonet" created="2013-05-30T08:27:27Z" id="18667609">Documentation update commit is here: https://github.com/dadoonet/elasticsearch.github.com/commit/016eda565d04b83f68e2b615d2ab883da07ab14f
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>term vector request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3115</link><project id="" key="" /><description># 

Returns information and statistics on terms in the fields of a particular document as stored in the index.

```
    curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true'
```

Three types of values can be requested: term information, term statistics and field statistics.
By default, all term information and field statistics are returned for all fields but no term statistics.

Optionally, you can specify the fields for which the information is retrieved either with a parameter in the url

```
curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?fields=text,...'
```

or adding by adding the requested fields in the request body (see example below).
## Term information
- term frequency in the field (always returned)
- term positions ("positions" : true)
- start and end offsets ("offsets" : true)
- term payload ("payloads" : true)

If the requested information wasn't stored in the index, it will be omitted without further warning.
See [mapping](http://www.elasticsearch.org/guide/reference/mapping/core-types/) on how to configure your index to store term vectors.
## Term statistics

Setting "term_statistics" to "true" (default is "false") will return
- total term frequency (how often a term occurs in all documents)
- document frequency (the number of documents containing the current term)

By default these values are not returned since term statistics can have a serious performance impact.
## Field statistics

Setting "field_statistics" to "false" (default is "true") will omit
- document count (how many documents contain this field)
- sum of document frequencies (the sum of document frequencies for all terms in this field)
- sum of total term frequencies (the sum of total term frequencies of each term in this field)

Here is a sample request that returns everything for the field "text":

```
    curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true' -d
    '{
            "fields" : ["text"],
            "offsets" : true,
            "payloads" : true,
            "positions" : true,
            "term_statistics" : true,
            "field_statistics" : true
    }'
```

This will return all the information described above for the field "text" in document "1" of type "tweet" in index "twitter".
## Behavior

The term and field statistics are not accurate. Deleted documents are not taken into account. The information is only retrieved for the shard the requested document resides in. The term and field statistics are therefore only useful as relative measures whereas the absolute numbers have no meaning in this context.
## Next steps

A term vector request for more than one document similar to [multi get](http://www.elasticsearch.org/guide/reference/api/multi-get/)

Closes #3114
</description><key id="14904074">3115</key><summary>term vector request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brwe</reporter><labels /><created>2013-05-29T18:12:06Z</created><updated>2014-06-12T21:14:44Z</updated><resolved>2013-06-13T07:42:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-29T19:10:17Z" id="18639128">I think it would be nice to see the response format. The description only show the request format. Britta can you add  that to the commit?
</comment><comment author="clintongormley" created="2013-05-30T07:45:00Z" id="18666080">Very nice indeed - this will be useful!
</comment><comment author="brwe" created="2013-05-30T10:57:29Z" id="18673551">I added an example to the commit message, see below. Is that what you meant?

   Example
    -------------------------

First, we create an index that stores term vectors, payloads etc. :

```
    curl -s -XPUT 'http://localhost:9200/twitter/' -d '{
        "mappings": {
            "tweet": {
                "properties": {
                    "text": {
                                "type": "string",
                                "term_vector": "with_positions_offsets_payloads",
                                "store" : "yes",
                                "index_analyzer" : "fulltext_analyzer"
                         },
                     "fullname": {
                                "type": "string",
                                "term_vector": "with_positions_offsets_payloads",
                                "index_analyzer" : "fulltext_analyzer"
                         }
                 }
            }
        },
        "settings" : {
            "index" : {
                "number_of_shards" : 1,
                "number_of_replicas" : 0
            },
            "analysis": {
                    "analyzer": {
                        "fulltext_analyzer": {
                            "type": "custom",
                            "tokenizer": "whitespace",
                            "filter": [
                                "lowercase",
                                "type_as_payload"
                            ]
                        }
                    }
            }
         }
    }'
```

Second, we add some documents:

```
    curl -XPUT 'http://localhost:9200/twitter/tweet/1?pretty=true' -d '{
      "fullname" : "John Doe",
      "text" : "twitter test test test "

    }'

    curl -XPUT 'http://localhost:9200/twitter/tweet/2?pretty=true' -d '{
      "fullname" : "Jane Doe",
      "text" : "Another twitter test ..."

    }'
```

The following request returns all information and statistics for firld "text" in document "1" (John Doe):

```
     curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true' -d '{
                    "fields" : ["text"],
                    "offsets" : true,
                    "payloads" : true,
                    "positions" : true,
                    "term_statistics" : true,
                    "field_statistics" : true
            }'
```

Response:

```
    {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_version" : 1,
      "exists" : true,
      "term_vectors" : {
        "text" : {
          "field_statistics" : {
            "sum_doc_freq" : 6,
            "doc_count" : 2,
            "sum_ttf" : 8
          },
          "terms" : {
            "test" : {
              "doc_freq" : 2,
              "ttf" : 4,
              "term_freq" : 3,
              "pos" : [ 1, 2, 3 ],
              "start" : [ 8, 13, 18 ],
              "end" : [ 12, 17, 22 ],
              "payload" : [ "word", "word", "word" ]
            },
            "twitter" : {
              "doc_freq" : 2,
              "ttf" : 2,
              "term_freq" : 1,
              "pos" : [ 0 ],
              "start" : [ 0 ],
              "end" : [ 7 ],
              "payload" : [ "word" ]
            }
          }
        }
      }
    }
```
</comment><comment author="s1monw" created="2013-05-30T11:34:20Z" id="18674884">good stuff! yes that is what I was talking about!
</comment><comment author="jpountz" created="2013-05-30T12:42:39Z" id="18677524">I'm worried that this API exposes top-level terms statistics because it forces this API to perform one random seek for term vectors and another one in the terms dictionary. So maybe top-level field statistics should be an opt-in option or a different API?
</comment><comment author="s1monw" created="2013-05-30T12:44:12Z" id="18677599">@jpountz you are talking about `term_statistics` like docFreq? this is disabled by default so you have to opt in.
</comment><comment author="jpountz" created="2013-05-30T12:45:43Z" id="18677676">Sorry, I completely missed the options in the query!
</comment><comment author="s1monw" created="2013-05-30T12:54:09Z" id="18678085">no worries!
</comment><comment author="s1monw" created="2013-06-07T19:06:28Z" id="19126815">+1 on the latest commit version! LGTM push it to master and backport to 0.90
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>return term vectors and some statistics for a document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3114</link><project id="" key="" /><description>This feature seems to be useful as can be seen by typing "term vectors elasticsearch" in google. 

Here is how it should work: 

Returns information and statistics on terms in the fields of a particular document as stored in the index.

```
    curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true'
```

Tree types of values can be requested: term information, term statistics and field statistics.
By default, all term information and field statistics are returned for all fields but no term statistics.

Optionally, you can specify the fields for which the information is retrieved either with a parameter in the url

```
curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?fields=text,...'
```

or adding by adding the requested fields in the request body (see example below).
## YOU MUST ENABLE TERM VECTOR STORING FOR USING THE API

See [mapping](http://www.elasticsearch.org/guide/reference/mapping/core-types/) doc and the example below on how to do that.
## Term information
- term frequency in the field (always returned)
- term positions ("positions" : true)
- start and end offsets ("offsets" : true)
- term payloads ("payloads" : true), as base64 encoded bytes

If the requested information wasn't stored in the index, it will be omitted without further warning.
See [mapping](http://www.elasticsearch.org/guide/reference/mapping/core-types/) on how to configure your index to store term vectors.
## Term statistics

Setting "term_statistics" to "true" (default is "false") will return
- total term frequency (how often a term occurs in all documents)
- document frequency (the number of documents containing the current term)

By default these values are not returned since term statistics can have a serious performance impact.
## Field statistics

Setting "field_statistics" to "false" (default is "true") will omit
- document count (how many documents contain this field)
- sum of document frequencies (the sum of document frequencies for all terms in this field)
- sum of total term frequencies (the sum of total term frequencies of each term in this field)
## Behavior

The term and field statistics are not accurate. Deleted documents are not taken into account. The information is only retrieved for the shard the requested document resides in. The term and field statistics are therefore only useful as relative measures whereas the absolute numbers have no meaning in this context.
## Example

First, we create an index that stores term vectors, payloads etc. :

```
    curl -s -XPUT 'http://localhost:9200/twitter/' -d '{
        "mappings": {
            "tweet": {
                "properties": {
                    "text": {
                                "type": "string",
                                "term_vector": "with_positions_offsets_payloads",
                                "store" : "yes",
                                "index_analyzer" : "fulltext_analyzer"
                         },
                     "fullname": {
                                "type": "string",
                                "term_vector": "with_positions_offsets_payloads",
                                "index_analyzer" : "fulltext_analyzer"
                         }
                 }
            }
        },
        "settings" : {
            "index" : {
                "number_of_shards" : 1,
                "number_of_replicas" : 0
            },
            "analysis": {
                    "analyzer": {
                        "fulltext_analyzer": {
                            "type": "custom",
                            "tokenizer": "whitespace",
                            "filter": [
                                "lowercase",
                                "type_as_payload"
                            ]
                        }
                    }
            }
         }
    }'
```

Second, we add some documents:

```
    curl -XPUT 'http://localhost:9200/twitter/tweet/1?pretty=true' -d '{
      "fullname" : "John Doe",
      "text" : "twitter test test test "

    }'

    curl -XPUT 'http://localhost:9200/twitter/tweet/2?pretty=true' -d '{
      "fullname" : "Jane Doe",
      "text" : "Another twitter test ..."

    }'
```

The following request returns all information and statistics for field "text" in document "1" (John Doe):

```
     curl -XGET 'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true' -d '{
                    "fields" : ["text"],
                    "offsets" : true,
                    "payloads" : true,
                    "positions" : true,
                    "term_statistics" : true,
                    "field_statistics" : true
            }'
```

Equivalently, all parameters can be passed as URI parameters:
     curl -GET 'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true&amp;fields=text&amp;offsets=true&amp;payloads=true&amp;positions=true&amp;term_statistics=true&amp;field_statistics=true'

Response:

```
  {
    "_index" : "twitter",
    "_type" : "tweet",
    "_id" : "1",
    "_version" : 1,
    "exists" : true,
    "term_vectors" : {
      "text" : {
        "field_statistics" : {
          "sum_doc_freq" : 6,
          "doc_count" : 2,
          "sum_ttf" : 8
        },
        "terms" : {
          "test" : {
            "doc_freq" : 2,
            "ttf" : 4,
            "term_freq" : 3,
            "pos" : [ 1, 2, 3 ],
            "start" : [ 8, 13, 18 ],
            "end" : [ 12, 17, 22 ],
            "payload" : [ "d29yZA==", "d29yZA==", "d29yZA==" ]
          },
          "twitter" : {
            "doc_freq" : 2,
            "ttf" : 2,
            "term_freq" : 1,
            "pos" : [ 0 ],
            "start" : [ 0 ],
            "end" : [ 7 ],
            "payload" : [ "d29yZA==" ]
          }
        }
      }
    }
  }
```

This is similar to Issue #2691
</description><key id="14903231">3114</key><summary>return term vectors and some statistics for a document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/brwe/following{/other_user}', u'events_url': u'https://api.github.com/users/brwe/events{/privacy}', u'organizations_url': u'https://api.github.com/users/brwe/orgs', u'url': u'https://api.github.com/users/brwe', u'gists_url': u'https://api.github.com/users/brwe/gists{/gist_id}', u'html_url': u'https://github.com/brwe', u'subscriptions_url': u'https://api.github.com/users/brwe/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/4320215?v=4', u'repos_url': u'https://api.github.com/users/brwe/repos', u'received_events_url': u'https://api.github.com/users/brwe/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/brwe/starred{/owner}{/repo}', u'site_admin': False, u'login': u'brwe', u'type': u'User', u'id': 4320215, u'followers_url': u'https://api.github.com/users/brwe/followers'}</assignee><reporter username="">brwe</reporter><labels><label>feature</label><label>v1.0.0.Beta1</label></labels><created>2013-05-29T17:53:27Z</created><updated>2016-10-04T19:58:51Z</updated><resolved>2013-06-10T10:03:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="BitcoinKing" created="2016-10-04T19:58:51Z" id="251495578">Could anyone point me on how to enable termvectors with django-haystack (Python lib that works with Elasticsearch) https://django-haystack.readthedocs.io
No such data in docs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Don't fail highlighting if the field to highlight is missing.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3113</link><project id="" key="" /><description>PlainHighlighter fails at highlighting with a hard exception in case the field
to highlight is missing. This patch fixes this issue by
- making FieldsVisitor.fields() return an empty list instead of null when no
  stored field was found,
- replacing the fields to highlight with an empty list in case they are absent.

Closes: #3109
</description><key id="14903077">3113</key><summary>Don't fail highlighting if the field to highlight is missing.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jpountz</reporter><labels /><created>2013-05-29T17:50:49Z</created><updated>2014-07-16T21:53:18Z</updated><resolved>2013-05-30T09:19:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update plugin manager</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3112</link><project id="" key="" /><description>I searched using Google and found nothing.

I tried this and got nothing:

```
$ ./plugin -help
```

I tried this and got nothing:

```
$ ./plugin -nonsense
```

I tried this and got incompete information:

```
$ ./plugin
Usage:
    -url     [plugin location]   : Set exact URL to download the plugin from
    -install [plugin name]       : Downloads and installs listed plugins
    -remove  [plugin name]       : Removes listed plugins
    -verbose                     : Prints verbose messages
```

I tried this and got nothing:

```
$ ./plugin -verbose
```

I tried this and got nothing:

```
$ ./plugin -verbose -nonsense
```

Just to make my own sense of frustrated indignation complete, I tried this:

```
$ ./plugin -this "is bullshit"
$ echo $?
```

The result was _0_.

And the icing on the cake:

```
$ ./plugin -install
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 1
        at org.elasticsearch.plugins.PluginManager.main(PluginManager.java:321)
```

I see lots of examples around the web where _plugin name_ is something like **mobz/elasticsearch-head** or **elasticsearch/mapper-attachments/1.6.0** which clearly has some semantically important syntax embedded in it.  Nowhere is it documented what the components of this syntax are or what valid values are.
</description><key id="14894497">3112</key><summary>Update plugin manager</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">inkblot</reporter><labels><label>enhancement</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-05-29T15:03:50Z</created><updated>2013-05-30T20:39:24Z</updated><resolved>2013-05-30T20:35:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-05-29T15:25:10Z" id="18624162">Yeah. You're right.

Plugin could be used as follow:
- site plugin (download master ZIP from github): `bin/plugin -install username/reponame`
- elasticsearch official plugin (download from download.elasticsearch.org):  `bin/plugin -install elasticsearch/pluginname/version`
- community plugins (download from maven central):  `bin/plugin -install groupId/artifactId/version`

We should fix that in documentation and as a default help as suggested.
</comment><comment author="dadoonet" created="2013-05-30T20:39:24Z" id="18706763">Now Plugin Manager does not send Exception and display documentation in case of error.
New options have been added:
- -l, --list displays list of existing plugins
- -h, --help displays help

Deprecated options:
- -install is now -i, --install
- -remove is now -r, --remove
- -url is now -u, --url

Add description on plugin name structure:
- elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)
- groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)
- username/repository          for site plugins (download from github master)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update api doesn't support versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3111</link><project id="" key="" /><description>To reproduce

``` sh
curl -XPOST http://localhost:9200/test/test/1 -d'{
    "field": "value1"
}'
```

The internal version is now 1.

Now index again:

``` sh
curl -XPOST http://localhost:9200/test/test/1 -d'{
    "field": "value2"
}'
```

Internal version is now 2. 

Try to update using version=1 (should fail)

``` sh
curl -XPOST "http://localhost:9200/test/test/1/_update?version=1" -d'{
    "doc": { "field": "value3" }
}'
```

Which doesn't fail with a version conflict but returns:

``` json
{"ok":true,"_index":"test","_type":"test","_id":"1","_version":3,"_previous_version":2}
```

PS. The java api's UpdateRequestBuilder doesn't have a setVersion method
</description><key id="14887904">3111</key><summary>Update api doesn't support versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v1.0.0.Beta1</label></labels><created>2013-05-29T12:48:24Z</created><updated>2013-09-13T15:03:56Z</updated><resolved>2013-06-20T11:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index/Update/Create/Delete now report the previous version of the docume...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3110</link><project id="" key="" /><description>...nt they have touched (new version already) reported.

Added an assertThrows to ElasticsearchAssertions. Added a shortcut function to UpdateRequest to quickly update a single doc field.

Closes #3066 . Closes #3084.
</description><key id="14887015">3110</key><summary>Index/Update/Create/Delete now report the previous version of the docume...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-05-29T12:23:33Z</created><updated>2014-07-16T21:53:18Z</updated><resolved>2013-06-09T19:09:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Highlighter exception (0.90.0)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3109</link><project id="" key="" /><description>When applying highlight on fields that are declared as stored at index mapping but do not actually exists the following exception occurs (in version 0.90.x)

``` javascript
{
   "took":6,
   "timed_out":false,
   "_shards":{
      "total":3,
      "successful":2,
      "failed":1,
      "failures":[
         {
            "index":"myindex",
            "shard":1,
            "status":500,
            "reason":"FetchPhaseExecutionException[[myindex][1]: query[filtered(name:p*)-&gt;cache(_type:myindextype)],from[0],size[10]: Fetch Failed [Failed to highlight field [surname]]]; nested: NullPointerException; "
         }
      ]
   },
   "hits":{
      "total":1,
      "max_score":1.0,
      "hits":[

      ]
   }
}.
```

You can reproduce the error by doing the following on an empty cluster

``` bash
 # create index 

curl -XPUT 'http://192.168.56.150:9200/myindex/' -d '{
    "settings" : {
        "number_of_shards" : 3,
        "number_of_replicas" : 0
    }
}'

# create the mapping

curl -XPUT http://192.168.56.150:9200/myindex/myindextype/_mapping -d '
    { "myindextype" : {"properties" : { "name":{"type":"string", "store":"yes", "analyzer":"simple"}, "surname":{"type":"string", "store":"yes", "analyzer":"simple"} } } }
'

# add a record 
curl -XPUT http://192.168.56.150:9200/myindex/myindextype/1 -d '{ "name":"panagiotis" }'

# query with highlighting

curl -XGET 'http://192.168.56.150:9200/myindex/myindextype/_search' -d '
{
  "query": {
    "query_string": {
      "query": "name:p*"
    }
  },
  "highlight": {
    "order": "score",
    "fields": {
      "name": {},
      "surname": {}
    }
  }
}
'
```

I think that ES should not throw an exception on that case (as did in 0.20.X and earlier versions) due to the schema-free philosophy that is built on. 

The problematic code is **HighlightPhase.java:191** and one easy patch is to to replace that line with the following lines

``` java
if (fieldVisitor.fields() == null)
    textsToHighlight = new ArrayList&lt;Object&gt;();
else
    textsToHighlight = fieldVisitor.fields().get(mapper.names().indexName());
```

I think that the same logic where in previous versions.

Thank you,
Alex
</description><key id="14886058">3109</key><summary>Highlighter exception (0.90.0)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">aantoniadis</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-29T11:54:50Z</created><updated>2013-05-30T12:00:06Z</updated><resolved>2013-05-30T08:58:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jpountz" created="2013-05-29T13:06:23Z" id="18614768">Thanks for the comprehensive report! Indeed, this kind of query shouldn't fail, I'll look into it.
</comment><comment author="s1monw" created="2013-05-30T12:00:06Z" id="18675829">thanks adrien!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing Filter DOES NOT work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3108</link><project id="" key="" /><description>elasticsearch version: 0.20.6

Run search:
{'query': {'filtered': {'filter': {'missing': {'field': 'thumbnail_pic'}}, 'query': {'match_all': {}}}}, 'from': 0, 'size': 10}

1655697282 None
2256313614 ww1_867c9d0ejw1dpilhahhadj.jpg
2077179667 ww1_7bcf3f13jw1dop8lrcayij.jpg
2010910661 None
2010910661 None
2390283571 ww4_8e78d533jw1dpd9mbncn0j.jpg
1655697282 None
1655697282 None
1773689615 None
1776722807 None

The result will return documents that the filtered field with value.
</description><key id="14876593">3108</key><summary>Missing Filter DOES NOT work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">liujuncn</reporter><labels /><created>2013-05-29T07:15:53Z</created><updated>2013-05-29T07:18:57Z</updated><resolved>2013-05-29T07:18:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Missing Filter DOES NOT work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3107</link><project id="" key="" /><description>elasticsearch version: 0.20.6

Run search:
{'query': {'filtered': {'filter': {'missing': {'field': 'thumbnail_pic'}}, 'query': {'match_all': {}}}}, 'from': 0, 'size': 10}

1655697282 None
2256313614 ww1_867c9d0ejw1dpilhahhadj.jpg
2077179667 ww1_7bcf3f13jw1dop8lrcayij.jpg
2010910661 None
2010910661 None
2390283571 ww4_8e78d533jw1dpd9mbncn0j.jpg
1655697282 None
1655697282 None
1773689615 None
1776722807 None

The result will return documents that the filtered field with value.
</description><key id="14876551">3107</key><summary>Missing Filter DOES NOT work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">liujuncn</reporter><labels /><created>2013-05-29T07:14:09Z</created><updated>2013-05-29T07:34:16Z</updated><resolved>2013-05-29T07:34:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Facet on different numerical types causes exceptions even when resticting by type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3106</link><project id="" key="" /><description>Given this script:

``` sh
#!/usr/bin/env zsh

curl -s -XDELETE localhost:9200/ftest
curl -s -XPOST localhost:9200/ftest -d'
{
    "mappings": {
        "doc": {
            "_source": {
                "enabled": true
            },
            "properties": {
                "id": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "size": {
                    "type": "integer"
                }
            }
        },
        "doc2": {
            "_source": {
                "enabled": true
            },
            "properties": {
                "id": {
                    "index": "not_analyzed",
                    "type": "string"
                },
                "size": {
                    "type": "long"
                }
            }
        }
    },
    "settings": {
        "number_of_replicas": 0,
        "number_of_shards": 1
    }
}'
echo

curl -XPUT localhost:9200/ftest/doc/1 -d'{"id":"1","size":10}'
curl -XPUT localhost:9200/ftest/doc2/1 -d'{"id":"1","size":5}'

curl -XPOST localhost:9200/ftest/_refresh

echo "\n\nall"
curl -XPOST http://localhost:9200/ftest/_search\?search_type\=count -d '{
    "facets": {
        "size_stats": {
            "statistical": {
                "field": "size"
            }
        }
    },
    "query": {
        "match_all": {}
    }
}
'

echo "\n\njust doc"
curl -XPOST http://localhost:9200/ftest/doc/_search\?search_type\=count -d '{
    "facets": {
        "size_stats": {
            "statistical": {
                "field": "size"
            }
        }
    },
    "query": {
        "match_all": {}
    }
}
'

echo "\n\njust doc2"
curl -XPOST http://localhost:9200/ftest/doc2/_search\?search_type\=count -d '{
    "facets": {
        "size_stats": {
            "statistical": {
                "field": "size"
            }
        }
    },
    "query": {
        "match_all": {}
    }
}
'

echo
```

When run:

```
&#8756; ./facet-on-different-types.zsh 
{"ok":true,"acknowledged":true}{"ok":true,"acknowledged":true}
{"ok":true,"_index":"ftest","_type":"doc","_id":"1","_version":1}{"ok":true,"_index":"ftest","_type":"doc2","_id":"1","_version":1}{"ok":true,"_shards":{"total":1,"successful":1,"failed":0}}

all
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[gNdiQCO0TQSxijotnwBMUQ][ftest][0]: QueryPhaseExecutionException[[ftest][0]: query[ConstantScore(*:*)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticSearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }]","status":500}

just doc
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[gNdiQCO0TQSxijotnwBMUQ][ftest][0]: QueryPhaseExecutionException[[ftest][0]: query[ConstantScore(cache(_type:doc))],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticSearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }]","status":500}

just doc2
{"error":"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[gNdiQCO0TQSxijotnwBMUQ][ftest][0]: QueryPhaseExecutionException[[ftest][0]: query[ConstantScore(cache(_type:doc2))],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticSearchException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: UncheckedExecutionException[java.lang.NumberFormatException: Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; nested: NumberFormatException[Invalid shift value in prefixCoded bytes (is encoded value really an INT?)]; }]","status":500}
```

Which I'm guessing is caused by ES seeing "integer" as the type for the `size` field in the `doc` type and assuming everything else will be an integer. What I don't understand is that specifying `doc` or `doc2` as part of the URL to limit the query to a particular ES type doesn't work around this.

Another interesting this is that if the types are swapped (put "long" for doc and "integer" for doc2), the facet works correctly, so the order seems to matter.
</description><key id="14859404">3106</key><summary>Facet on different numerical types causes exceptions even when resticting by type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-05-28T20:25:18Z</created><updated>2014-07-23T13:35:08Z</updated><resolved>2014-07-23T13:35:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-28T20:33:15Z" id="18578023">the size field should have the same type on both doc "types" and there is no check for that in the code, you can't use String and double and run a stats facet on it either. Yet, I agree it would be nice to automatically upgrade but I think that is not a low hanging fruit. For you as a workaround use long on both.
</comment><comment author="dakrone" created="2013-05-28T20:37:10Z" id="18578271">Right, I understand you can't facet across both types. What I don't understand is why limiting the search by type by using `localhost:9200/ftest/doc/_search` and `localhost:9200/ftest/doc2/_search` still complains about the number mismatch.

Does ES load all the values for the entire index (regardless of ES type) for a statistical facet?
</comment><comment author="s1monw" created="2013-05-28T20:38:07Z" id="18578317">&gt; Does ES load all the values for the entire index (regardless of ES type) for a statistical facet?

YES
</comment><comment author="dakrone" created="2013-05-28T20:39:53Z" id="18578440">Okay, is that by design, or is there a reason why that has to happen? I'll change our types to work around the issue in the meantime.
</comment><comment author="s1monw" created="2013-05-28T20:41:42Z" id="18578543">that is by design though. We might be able to upgrade the values on the fly but I don't think this is going to be simple due to the nature of how they are stored in the index.
</comment><comment author="dakrone" created="2013-05-28T20:42:48Z" id="18578617">Okay, thanks for the info, I'll update our mappings accordingly!
</comment><comment author="s1monw" created="2013-05-28T21:03:42Z" id="18579997">I am trying to look into this a bit, maybe we can just fallback to int / long parsing if we hit an exception. Yet, that might not work for searches though but for faceting. not sure if we should?
</comment><comment author="jordansissel" created="2014-04-30T19:17:04Z" id="41837778">A user reported this a similar problem with script_score as well: https://gist.github.com/jordansissel/5ceaf0bd01d815d66323

I made attempts to workarounds in mvel, but it is the field value lookup that seems to be failing, so there's nothing a user can do in mvel.
</comment><comment author="clintongormley" created="2014-07-23T13:35:08Z" id="49873733">Closing in favour of #4081 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed Java dependency from Depends to Suggest for Debian. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3105</link><project id="" key="" /><description>Since people are also using the Oracle JAVA distribution and not the OpenJDK.

Now the installation will at least continue, and it will not give dependency errors when installing other packages after the installation of ElasticSearch.
</description><key id="14843829">3105</key><summary>Changed Java dependency from Depends to Suggest for Debian. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">webpatser</reporter><labels /><created>2013-05-28T15:00:12Z</created><updated>2014-06-17T23:39:11Z</updated><resolved>2013-06-02T13:13:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-01T19:09:29Z" id="18795142">+1, I think it makes sense, @spinscale what do you think?
</comment><comment author="spinscale" created="2013-06-02T13:03:04Z" id="18806807">I like it. I'll get it in
</comment><comment author="spinscale" created="2013-06-02T13:13:06Z" id="18806915">Thanks a lot for your contribution! If you have any other recommendations about packaging, feel free to drop me a mail or create other github issues. Happy for any feedback!

Pushed into master at 9f43814a867b4cb348521744dcbf1004eceb64a7
</comment><comment author="kimchy" created="2013-06-02T15:07:20Z" id="18807585">@spinscale I think it makes sense to back port this to 0.90 as well, what do you think?
</comment><comment author="spinscale" created="2013-06-02T16:41:36Z" id="18809168">@kimchy pushed it as well into 0.90
</comment><comment author="ctrochalakis" created="2013-07-01T13:40:48Z" id="20282105">Actually, there is a big problem with that.

After installing the 0.90.2 package the whole jre was removed. Since no package depends on the jre anymore, apt decides to remove it!

```
$ aptitude install elasticsearch
The following packages will be REMOVED:    
  ca-certificates-java{u} java-common{u} openjdk-7-jre-headless{u} openjdk-7-jre-lib{u} tzdata-java{u} 
The following packages will be upgraded:
  elasticsearch 
```

Since elasticsearch obviously depends on java it should be stated as a dependency. The only oracle java package i found (https://launchpad.net/~webupd8team/+archive/java) correctly [provides](http://www.debian.org/doc/debian-policy/ch-relationships.html#s-virtual) java*-runtime that is required by elasticsearch so this shouldn't be an issue.

We shouldn't break packaging for those who install oracle java by hand. Those users could "trick" apt by installing a dummy package that satisfies the dependency. This can by done with equivs.

```
apt-get install equivs
equivs-control oracle-java-dummy
vim oracle-java-dummy (modify Package name, and add a Provides: java7-runtime line)
equivs-build oracle-java-dummy
dpkg -i oracle-java-dummy*deb
```
</comment><comment author="spinscale" created="2013-07-02T07:09:01Z" id="20330019">This sounds like an issue to fix, I'll check it out.

@webpatser do you agree or see another solution to this?
</comment><comment author="webpatser" created="2013-07-02T19:30:43Z" id="20369770">Sure. If there is no other way.. Maybe add it to the debian installation how to? 
</comment><comment author="webpatser" created="2013-07-02T19:38:17Z" id="20370219">http://www.debian.org/doc/debian-policy/ch-relationships.html

Maybe the Enhances flag is better then? Reading the description it should not remove the java packages if present "This field is similar to Suggests but works in the opposite direction. It is used to declare that a package can enhance the functionality of another package.".. 

(but then again, suggest also doesn't mention this auto-removal)..
</comment><comment author="ctrochalakis" created="2013-07-03T08:37:18Z" id="20402411">I don't think that enhances will solve the auto removal issue. It is also not appropriate because it states that elasticsearch enhances java's functionality which is not the case.

Personally I think the equivs solution is the "correct way" to handle such cases. To make it easier you could provide a java-dummy package from the elasticsearch download page.
</comment><comment author="spinscale" created="2013-07-04T12:13:23Z" id="20473962">@ctrochalakis I was not yet able to get to the status with apitude like you reached above. Are you using ubuntu as well? Which packages did you delete beforehand in order to get this error?

Apart from that I am also coming to the conclusion that neither `Enhances` nor `Recommends` is a good match, after reading the package guidelines.. so the equivs solution might be the best one.
</comment><comment author="webpatser" created="2013-07-08T14:39:11Z" id="20609780">You can also just have no dependency set. I will complain about JAVA when it's not present during startup... Maybe provide a 'apt-get install java..' message in the init script.
</comment><comment author="ctrochalakis" created="2013-07-09T06:23:08Z" id="20655603">@spinscale Sorry for taking so long to answer. Here are the steps to reproduce the java removal behaviour:

```
# First install an elasticsearch that depends on java
$ sudo apt-get install elasticsearch=0.19.8
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following extra packages will be installed:
  ca-certificates-java java-common openjdk-7-jre-headless openjdk-7-jre-lib tzdata-java
Suggested packages:
  default-jre libnss-mdns sun-java6-fonts fonts-ipafont-gothic fonts-ipafont-mincho ttf-wqy-microhei ttf-wqy-zenhei ttf-indic-fonts
Recommended packages:
  icedtea-7-jre-jamvm
The following NEW packages will be installed:
  ca-certificates-java elasticsearch java-common openjdk-7-jre-headless openjdk-7-jre-lib tzdata-java
0 upgraded, 6 newly installed, 0 to remove and 7 not upgraded.
Need to get 0 B/54.6 MB of archives.
After this operation, 72.0 MB of additional disk space will be used.
Do you want to continue [Y/n]?

....

# Try to update with the latest version (uses suggests) and java gets removed
$ aptitude install elasticsearch
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?
luke ~ sudo aptitude install elasticsearch
The following packages will be REMOVED:
  ca-certificates-java{u} java-common{u} openjdk-7-jre-headless{u} openjdk-7-jre-lib{u} tzdata-java{u}
The following packages will be upgraded:
  elasticsearch
1 packages upgraded, 0 newly installed, 5 to remove and 7 not upgraded.
Need to get 0 B/17.1 MB of archives. After unpacking 52.1 MB will be freed.
Do you want to continue? [Y/n/?]

```
</comment><comment author="paravoid" created="2013-12-11T17:00:14Z" id="30339106">I concur with @ctrochalakis. Enhances is definitely very wrong for this, and Suggests is also wrong and can result into apt doing the wrong thing because of a misstated dependency, as it has already been the case for some people. (I'm a Debian Developer).

These are Debian packages; coexisting with locally-installed Java defeats the purpose of that. If people really want to install Java locally instead of using make-jpkg or the PPA, they can either use equivs or not use the Elasticsearch .deb at all.

Moreover, on issue #3286 it has been mentioned that Elasticsearch is going to provide an apt repository. When that arrives, "apt-get install elasticsearch" should do the right thing and pull the JRE in, instead of just providing a broken package until the sysadmin installs it manually.
</comment><comment author="spinscale" created="2013-12-11T17:40:15Z" id="30342792">Hey

`Enhances` and `Suggests` have been removed quite some time ago - did you see it somewhere in our current packages and this is why you bring it up again? If so, lets try to fix it ASAP, as I agree we should not use them.

We are preparing repositories, which will not include a java distribution. Yet, at the same time, we want people to use the newest java version available and preferrably the oracle JDK at the moment. As there problems with having those available (especially having an up-to-date JDK when using the stable debian release and of course the licensing issues for the oracle one) and we do not want people to use equivs, we decided to go this way.

We still warn people in the init script if no JDK exists and show how to install the oracle JDK using apt-get on the documentation.

You still think, that this is not sufficient to provide a good user experience under the given circumstances or that it is not the debian way? Interested in opinions here.
</comment><comment author="paravoid" created="2013-12-11T23:23:11Z" id="30375198">I didn't see either of the two, I brought it up because right now there's no dependency whatsoever and none of these two alternatives sounded appealing either.

Honestly, I think that a Depends at the virtual package provides the best user experience. But, obviously, I'm a bit biased. I'd also wouldn't use the Oracle JDK either, so I guess I'm doubly biased :) This is off-topic, but why are you recommending that, if I may ask? It should matter less with OpenJDK 7 onwards, no?

In any case, most admins that care about a .deb for ElasticSearch would care about a .deb for the JDK as well &#8212; on the flip side, I can't imagine why someone would install Java by hand, but wouldn't do the same for Elasticsearch.

Besides the OpenJDK, Debian stable provides the "java-package" package for working with Oracle Java (`apt-get install java-package; make-jpkg &lt;tarball&gt;; dpkg -i oracle-java7-jre*deb`), and many Ubuntu users use the installer packages from the webupd8team PPA. Both of these provide the right virtual packages that would satisfy the dependency, so I think that a proper Depends: line + the recommendation of those two should be enough for the "installing in Debian/Ubuntu" section of the install guide.
</comment><comment author="spinscale" created="2013-12-12T08:03:00Z" id="30395552">Offtopic answer: Yes, matters less with OpenJDK7, but it is not yet completely gone.

Ontopic answer: Actually, it is exactly opposite from our experiences in the fields. Admins care a lot about having a special version of the JDK installed and available and care less about this being a debian package or not (especially in times of puppet/chef automation). A part of the reason for this lies in elasticsearch itself: You have to have the same java version installed, not only between all nodes of an elasticsearch cluster (which most likely runs the same OS anyway), but also between clients, if you use the java API (as the JDK folks tend to change serialization of objects between minor releases). This means, that even across operating system boundaries you want to have the same java version down to the minor version.

Java does not have init scripts or needs log file configurations, it is just binary - the elasticsearch debian package comes with these included. So yes, there is a difference between having elasticsearch as a package compared to java.

Didnt know the `make-jpkg` trick, should add that to our docs.
</comment><comment author="nik9000" created="2013-12-12T14:03:58Z" id="30424036">&gt; Offtopic answer: Yes, matters less with OpenJDK7, but it is not yet completely gone.

That last I heard on the mailing list was that this is gone now.  If it is isn't it needs to be.  All the problems with the OpenJDK need to be filed as issues, tags, and linked prominently somewhere.  I'd be happy to contribute fixes if I knew what was broken.

&gt; Ontopic answer:...

Elasticsearch has a tradition of installing cleanly and working reasonably well without tuning.  Installing the deb package and not getting Java is somewhat against that.  In my ideal world it'd install Java and we'd tell folks if they wanted a specific version of Java then to `make-jpkg`, run your own apt repository with the version of java you want, or don't use the deb.

As far as the specific JDK version thing, that is really an argument against using any sort of Java serialization in the Java api.  We are going to want to do rolling restarts to upgrade the JDK and I don't want Elasticsearch to blow up when I do it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store _version as a numeric doc values field.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3104</link><project id="" key="" /><description>Doc values can be expected to be more compact than payloads and should provide
better flexibility since doc values formats can be picked on a per-field basis.
This patch:
- makes _version stored as a numeric doc values field,
- manages backwards compatibility: if a version is not found in doc values,
  then it will look into payloads,
- uses background merges to upgrade old segments and move _version from
  payloads to doc values.

Closes #3103
</description><key id="14840545">3104</key><summary>Store _version as a numeric doc values field.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jpountz</reporter><labels /><created>2013-05-28T13:55:44Z</created><updated>2014-06-29T01:14:10Z</updated><resolved>2013-05-30T09:51:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Store _version as a numeric doc values field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3103</link><project id="" key="" /><description>_version is today stored as a payload alongside the postings of the _uid field.

Storing it as a numeric doc values field would help save space, since _uid could be indexed with IndexOptions.DOCS_ONLY and numeric doc values formats can be very compact since they don't need to be byte-aligned. For example, all doc values formats in Lucene but SimpleText store numbers into blocks of packed integers.
</description><key id="14839331">3103</key><summary>Store _version as a numeric doc values field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">jpountz</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-05-28T13:30:39Z</created><updated>2013-05-30T09:51:45Z</updated><resolved>2013-05-30T09:51:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-29T10:44:08Z" id="18608370">I left a couple of minor comments on the PullRequest aside of those here is my +1

LGTM push it!
</comment><comment author="brusic" created="2013-05-29T17:47:32Z" id="18633863">Would this change help in making the _version field indexable?  I can contribute a patch if so, since I would love that functionality.
</comment><comment author="jpountz" created="2013-05-29T18:06:25Z" id="18635056">What is your use-case for querying the _version field?
</comment><comment author="brusic" created="2013-05-29T18:10:32Z" id="18635317">Simple. Give me any document that has been updated (version &gt;= 2).

I actually started looking into creating a VersionFieldMapper a couple of weeks ago, but I have not had time to look into it more. This change could potentially simplify things or at least not have to rewrite things twice.
</comment><comment author="jpountz" created="2013-05-29T18:58:22Z" id="18638432">I'm not sure this is a good idea to try to expand the scope of _version. For example, if you delete a document and add again soon after a document with the same _uid, its _version won't be 1 although it is a new document which has never been updated?
</comment><comment author="s1monw" created="2013-05-29T19:02:29Z" id="18638690">I'd agree with adrien this is pretty internal and the behaviour might be unexpected. Searching  on a version should be external I guess that way you can index the actual version as a field and search for it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Root search analyzer doesn't act as default for fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3102</link><project id="" key="" /><description>Recently, we moved from version `0.19.11` to last stable `0.90.0` and found one very strange behavior that looks like an issue.

If on the item root level we have specified our custom `index_analyzer`, `search_analyzer` (or just `analyzer`), then `index_analyzer` works well, but not the `search_analyzer`. Also, fif we update existing mapping with explicitly specifying `search_analyzer` on the field level, then it still doesn't seem to work and ES uses standard one.
### To reproduce:

Create a new index and define our custom analyzer `de_stem`: 

```
curl -XPUT 'http://localhost:9200/issue/' -d '{"index": {"number_of_shards": 1,"analysis": {"filter": {"de_snowball": {"type": "snowball","language": "German"}},"analyzer": {"de_stem": {"type": "custom","tokenizer": "standard","filter": ["lowercase", "de_snowball"]}}}}},"number_of_replicas": 0}}'
```

Put mapping with specified `index_analyzer` and `search_analyzer` :

```
curl -XPUT 'http://localhost:9200/issue/item/_mapping' -d '{"item": {"index_analyzer" : "de_stem","search_analyzer" : "de_stem","properties": {"content": {"dynamic": false,"properties": {"body": {"type": "string"}}}}}}}'
```

Try `search_analyzer` for the field `content.body` with Analyze API

```
curl -XGET 'localhost:9200/issue/_analyze?pretty=true&amp;field=content.body' -d 'Apple'
```
#### Actual result:

```
{
  "tokens" : [ {
    "token" : "apple",
    "start_offset" : 0,
    "end_offset" : 5,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  } ]
}
```
#### Expected result:

```
{
  "tokens" : [ {
    "token" : "appl",
    "start_offset" : 0,
    "end_offset" : 5,
    "type" : "&lt;ALPHANUM&gt;",
    "position" : 1
  } ]
}
```

The right (expected) result still possible to get, but with explicitly specified `search_analyzer`:

```
curl -XGET 'localhost:9200/issue/_analyze?pretty=true&amp;field=content.body&amp;analyzer=de_stem' -d 'Apple'
```
### Index Analyzer is set well

As we see above, `search_analyzer` seems wasn't set, but `index_analyzer`  works well.

Let's index a document:

```
curl -PUT 'http://localhost:9200/issue/item/1' -d '{"content" : {"body": "10 Things We Hate About Apple"}}'
```

If `index_analyzer` was set well to `de_stem` the word `Apple` should be indexed as `appl`, but not `apple` (as `standard` analyzer does).

Let's search for `appl` first:

```
curl -XGET 'http://localhost:9200/issue/_search?search_type=count&amp;pretty=true' -d '{"query":{"query_string":{"fields":["content.body"],"query":"appl"}}}'
```

It works! We get back 1 result:

```
  "hits" : {
    "total" : 1,
    "max_score" : 0.0,
    "hits" : [ ]
  }
```

For the word `apple`, as expected, it doesn't work since `search_analyzer` is standard, but  `index_analyzer` is `de_stem` (so, actual search term will stay `apple`, but indexed is `appl`):

```
curl -XGET 'http://localhost:9200/issue/_search?search_type=count&amp;pretty=true' -d '{"query":{"query_string":{"fields":["content.body"],"query":"apple"}}}'
```

```
  "hits" : {
    "total" : 0,
    "max_score" : 0.0,
    "hits" : [ ]
  }
```
### Specifying search analyzer with Put Mapping API doesn't help

Ok, i try to update mapping and specify explicitly `search analyzer` for `content.body` field on the existing index we created above:

```
curl -PUT 'http://localhost:9200/issue/item/_mapping' -d '{"item": {"properties": {"content": {"dynamic": false,"properties": {"body": {"type": "string", "search_analyzer": "de_stem"}}}}}}'
```

Response is `ok`, but the all problems described above stay the same. So, it seems the `search_analyzer` for the field `content.body` is still `standard`.
</description><key id="14835678">3102</key><summary>Root search analyzer doesn't act as default for fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gakhov</reporter><labels><label>:Mapping</label></labels><created>2013-05-28T11:55:06Z</created><updated>2014-12-24T15:59:59Z</updated><resolved>2014-12-24T15:59:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gakhov" created="2013-05-31T10:19:58Z" id="18737043">The general issue stays the same in new release `0.90.1`.  Only the updating `search_analyzer` on runtime now seems to work.
</comment><comment author="s1monw" created="2013-05-31T20:35:11Z" id="18770046">hey @gakhov we will look into this soon hopefully. Thanks for reporting it!
</comment><comment author="kimchy" created="2013-06-01T20:57:21Z" id="18796880">I have quickly looked into it, and I can see why it happens. The reason is that the analyzer (index/search) are associated with the types, so when using APIs that are not directly using a type, the analyzer can't be derived automatically (just based on the field name).

In the above test case, the document gets indexed properly (because it has the type). When using the analyze API, the type information is not there, so it can't derive the analyzer (and we don't expose the ability to provide a type in the analyze API).

In the search case, the search is executed on the index as a whole, so again the analyzer can't be derived based on the type, on the other hand, if its executes explicitly on the type (`item`), things will work well:

```
curl -XGET 'http://localhost:9200/issue/item/_search?search_type=count&amp;pretty=true' -d '{"query":{"query_string":{"fields":["content.body"],"query":"apple"}}}'
```

This behavior mainly comes from the fact that it gets tricky supporting multiple types with different definitions (like analysis) when executing across all types (such as executing against the index without specifying the type explicitly). But I admit its confusing. Requires some thinking into how to improve the behavior if even possible, but just wanted to post it here to explain the logic of how things work now.
</comment><comment author="gakhov" created="2013-06-02T09:05:50Z" id="18804296">Ah, thank you @kimchy! I didn't figure it out. This behaviour was a big problem for our application and we temporary solved it with specifing the `search_analyzer` on the quering.

It make fully sense for me to specify the `item type` on indexing, but I expected that `ElasticSearch` will guess the type on searching phase since type isn't required in fact for searching and I have only one type at all, so it should be easy to guess. 

Actually, i thought when i update mapping, `ElasticSearch` goes through all fields and set analyzers from default if they don't set explicitely in mapping. So, on search phase every field has an explicit analyzer (exactly as it would be if i manually set analyzers in mapping) and then `ElasticSearch` doesn't need to guess the type at all since every field has it's own analyzer.

P.S. In such situation, I would expect either clean exception from `ElasticSearch` or right resolving item type based on field, since silence makes debug very hard.

One more comment.  It seems in analyze API i can't specify the `item` name, so no way to check how query is analyzed (if i don't specify the analyzer explicitly) and this feels like an issue too.
</comment><comment author="clintongormley" created="2014-10-22T15:12:59Z" id="60100764">This should be fixable once we have #4081 done.
</comment><comment author="clintongormley" created="2014-11-29T15:19:22Z" id="64954920">As part of #4081, we should remove the type-level analysis settings.
</comment><comment author="clintongormley" created="2014-12-24T15:59:59Z" id="68060389">Closing in favour of #8874
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>esfile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3101</link><project id="" key="" /><description>ss
</description><key id="14819321">3101</key><summary>esfile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jkhhuse</reporter><labels /><created>2013-05-28T04:36:53Z</created><updated>2014-07-16T21:53:19Z</updated><resolved>2013-05-28T05:28:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aliases: Add indices aliases exists api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3100</link><project id="" key="" /><description>Add indices aliases exists api that allows to check to existence of an index alias. This api redirects to the master to check for the existence of one or multiple index aliases.

Possible options:
- `index` - The index name to check index aliases for. Partially names are supported via wildcards, also multiple index names can be specified separated with a comma. Also the alias name for an index can be used.
- `alias` - The name of alias to check the existence for. Like the index option, this option supports wildcards and the option the specify multiple alias names separated by a comma. This is a required option.
- `ignore_indices` - What to do is an specified index name doesn't exist. If set to `missing` then those indices are ignored.

The rest head endpoint is: `/{index}/_alias/{alias}`

Examples:
Check existence for any aliases with the name 2013 in any index:

```
curl -XHEAD 'localhost:9200/_alias/2013
```

Check existence for any aliases that start with 2013_01 in any index

```
curl -XHEAD 'localhost:9200/_alias/2013_01*
```

Check existence for any aliases in the users index.

```
curl -XHEAD 'localhost:9200/users/_alias/*
```
</description><key id="14807164">3100</key><summary>Aliases: Add indices aliases exists api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-27T18:36:31Z</created><updated>2013-06-12T18:42:38Z</updated><resolved>2013-05-27T18:41:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Changed faceting behavior from 0.20.6 to 0.90.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3099</link><project id="" key="" /><description>When faceting on a field that does not exist on the mapping raises SearchPhaseExecutionException on 0.90.0. This behavior was different in previous elasticsearch versions where the faceting was just not applied.

```
curl -XPUT 'http://localhost:9200/test/'

{"ok":true,"acknowledged":true}

curl -XPUT 'http://localhost:9200/test/blog_post/1' -d '{
  "title" : "Some interesting title",
  "body" : "Some interesting body"
}'

{"ok":true,"_index":"test","_type":"blog_post","_id":"1","_version":1}

curl -XGET 'http://localhost:9200/test/blog_post/_search?pretty' -d '{
"query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag",
                "size" : 10
            }
        }
    }
}'

{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[Frj_DRabTvabCbjN1rim-w][test][0]: SearchParseException[[test][0]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\n\"query\" : {\n        \"match_all\" : {  }\n    },\n    \"facets\" : {\n        \"tag\" : {\n            \"terms\" : {\n                \"field\" : \"tag\",\n                \"size\" : 10\n            }\n        }\n    }\n}]]]; nested: FacetPhaseExecutionException[Facet [tag]: failed to find mapping for [tag]]; }]",
  "status" : 500
}
```
</description><key id="14800136">3099</key><summary>Changed faceting behavior from 0.20.6 to 0.90.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">astathopoulos</reporter><labels /><created>2013-05-27T14:23:13Z</created><updated>2013-06-03T09:08:59Z</updated><resolved>2013-06-03T09:08:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pecke01" created="2013-05-27T14:27:20Z" id="18501114">There is already code in the upcoming release that will handle terms facet for unmapped fields in another way. Look at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/facet/terms/unmapped/UnmappedFieldExecutor.java for more information
</comment><comment author="kimchy" created="2013-06-01T19:10:24Z" id="18795162">This has been fixed in 0.90.1, can you please test it and see if it solves your problem?
</comment><comment author="astathopoulos" created="2013-06-03T09:01:43Z" id="18829208">Yes, this has solved my problem! Thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Autocomplete</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3098</link><project id="" key="" /><description>Adding autocomplete to elasticsearch.
</description><key id="14796106">3098</key><summary>Autocomplete</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukasz20p</reporter><labels /><created>2013-05-27T13:33:05Z</created><updated>2013-06-10T09:27:38Z</updated><resolved>2013-06-10T09:27:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-05-27T15:39:46Z" id="18504329">You can already have `autocomplete` feature in elasticsearch using the edgengrams / facets / phrase-prefix match queries.
What are you looking for exactly?
</comment><comment author="lukasz20p" created="2013-05-28T07:51:50Z" id="18535941">Can you write any example json query ?
</comment><comment author="spinscale" created="2013-06-10T09:27:38Z" id="19188711">Please ask questions like this on the google group and use this for issues/bugs only (the main advantage is that many users are reading the mailinglist compared to github issues, so chances are higher for a good reply). Googling for `elasticsearch autocomplete` should give you a few hints on how to start as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>No matched_filters being returned when using named filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3097</link><project id="" key="" /><description>See the request and response here:

https://gist.github.com/Mpdreamz/5656950

I'm not seeing a matched_filters property on any of the hits, am I doing something wrong here?
</description><key id="14794753">3097</key><summary>No matched_filters being returned when using named filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>enhancement</label><label>v0.90.3</label><label>v1.0.0.Beta1</label></labels><created>2013-05-27T13:03:23Z</created><updated>2016-03-05T02:23:08Z</updated><resolved>2013-08-02T15:14:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-02T00:13:11Z" id="21979028">Named filters are not currently supported on top-level filters, but only on filters under the main query (using filtered query).

We are most likely going to add soon support for them in the top-level filters too.
</comment><comment author="JoshMcCullough" created="2016-03-05T02:23:08Z" id="192552527">Documentation for this needs to be updated: http://nest.azurewebsites.net/nest/search/named-filters.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Allow to configure position_offset_gap for all core types mappings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3096</link><project id="" key="" /><description>This has already been done for the string type (#1813) and for Custom analyzer (#1812).

Is there a reason why it is not supported for the other core types ?
</description><key id="14787670">3096</key><summary>Mapping: Allow to configure position_offset_gap for all core types mappings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-05-27T09:06:41Z</created><updated>2014-07-28T12:44:27Z</updated><resolved>2014-07-08T19:28:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-28T15:07:39Z" id="18556705">Non string fields don't have positions.
</comment><comment author="iksnalybok" created="2013-05-30T10:31:54Z" id="18672576">Ok. Sorry for the stupid question. Closing.

NB: this question comes from the following use case:
Given doc1 = { str:foo, int:10 }, and doc2 = [ { str:foo, int:20 }, { str:bar, int:10 } ]
the query SpanNear( [ FieldMaskingSpan(str, foo), FieldMaskingSpan(int, 10) ] , slop=-1, false)
selects only doc1. This works fine when int is a TextField, but fails when is an IntField.
</comment><comment author="clintongormley" created="2013-05-30T12:15:04Z" id="18676379">You can't use positional queries with non-string fields, so you would have to make your "int" field a string field.
</comment><comment author="iksnalybok" created="2013-05-30T12:35:40Z" id="18677224">(Sorry, previously reopened without the comment)

Reopen: digging further, non string fields _can_ have positions.

Example: IntField (cf http://lucene.apache.org/core/4_3_0/core/org/apache/lucene/document/IntField.html) has a constructor allowing to customize the FieldType: IntField(String name, int value, FieldType type).
We can then specify: type.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS).
NB: LongField, FloatField, and DoubleField also get this constructor.

Here is a small test (lucene code):

``` java
    FieldType MY_TYPE = new FieldType();
    MY_TYPE.setIndexed(true);
    MY_TYPE.setStored(true);
    MY_TYPE.setTokenized(true);
    MY_TYPE.setOmitNorms(false);
    MY_TYPE.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS);
    MY_TYPE.setNumericType(FieldType.NumericType.INT);
    MY_TYPE.freeze();

    StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_43);
    Directory index = new RAMDirectory();
    IndexWriterConfig config = new IndexWriterConfig(Version.LUCENE_43, analyzer);
    IndexWriter w = new IndexWriter(index, config);

    Document doc;
    doc = new Document(); // doc1
    doc.add(new TextField("str", "foo", Field.Store.YES)); doc.add(new IntField("int", 10, MY_TYPE));
    w.addDocument(doc);
    doc = new Document(); // doc2
    doc.add(new TextField("str", "foo", Field.Store.YES)); doc.add(new IntField("int", 20, MY_TYPE));
    doc.add(new TextField("str", "bar", Field.Store.YES)); doc.add(new IntField("int", 10, MY_TYPE));
    w.addDocument(doc);
    w.close();

    BooleanQuery q1 = new BooleanQuery();
    q1.add(new TermQuery(new Term("str", "foo")), Occur.MUST);
    q1.add(NumericRangeQuery.newIntRange("int", 20, 20, true, true), Occur.MUST);
    // =&gt; find doc2

    Query q2 = new SpanNearQuery(new SpanQuery[] {
          new FieldMaskingSpanQuery(new SpanTermQuery(new Term("str", "foo")), "span")
        , new FieldMaskingSpanQuery(new SpanMultiTermQueryWrapper&lt;NumericRangeQuery&gt;(NumericRangeQuery.newIntRange("int", 10, 10, true, true)), "span")
    }, -1, false);
    // =&gt; find doc1
```

Especially for use cases like q2 above (that emulates kind of nesting), it gives the possibility to make range queries instead of string comparisons.

Would it be possible to include this in ES ?
Thanks.
</comment><comment author="iksnalybok" created="2013-05-31T22:01:28Z" id="18773985">I just tested again today (git/master branch), and ... you can specify positions for numeric fields (not only in lucene as said in my previous comment, but also in elasticsearch). Cf example 1 below.

However, position_offset_gap is not taken into account: specifying "position_offset_gap" : 100 has no effect for numeric types (cf examples 2a and 2b).
According to the fact that positions are supported, it would be nice to benefit from the position_offset_gap option. Could you please re-open the issue (I can't; github does not allow me to reopen it a second time) ?

---

Example 1 (positions for numeric fields):
In the following example, if you specify "num" : { "type" : "long" } without "index_options" : "positions" during the index creation, the last queries (span queries) will fail with an IllegalStateException ([field ... was indexed without position data). With it, they are successful. Also successfully tested for types integer and date.

``` bash
# to replay the test (with/without "index_options" : "positions)
curl -XDELETE "http://localhost:9200/spans"

# create the index
curl -XPOST "http://localhost:9200/spans" -d '
{
  "settings" : {
    "index" : {
      "number_of_shards" : 1,
      "number_of_replicas" : 0
    }
  },
  "mappings" : {
    "doc" : {
      "properties" : {
        "blocks" : {
          "properties" : {
            "str" : { "type" : "string", "index_options" : "positions" },
            "num" : { "type" : "long", "index_options" : "positions" }
          }
        }
      }
    }
  }
}
'

# create a first document (doc1)
curl -XPOST "http://localhost:9200/spans/doc/1" -d '
{ "blocks" : [
  { "str" : "foo", "num" : "10" },
  { "str" : "bar", "num" : "20" }
] }
'

# create a second document (doc2)
curl -XPOST "http://localhost:9200/spans/doc/2" -d '
{ "blocks" : [
  { "str" : "foo", "num" : "20" }
] }
'

# normal query
curl -XPOST "http://localhost:9200/spans/_search?format=yaml" -d '
{
  "query" : {
    "bool" : {
      "must" : [
         { "term" : { "str" : "foo" } },
         { "term" : { "num" : "20" } }
      ]
    }
  }
}
'

# span query
curl -XPOST "http://localhost:9200/spans/_search?format=yaml" -d '
{
  "query" : {
    "span_near" : {
      "clauses" : [
        { "field_masking_span" : { "query" : { "span_term" : { "str" : "foo" } }, "field" : "block" } },
        { "field_masking_span" : { "query" : { "span_term" : { "num" : "20"  } }, "field" : "block" } }
      ],
      "slop" : -1,
      "in_order" : false
    }
  }
}
'

# span query (using span_multi)
curl -XPOST "http://localhost:9200/spans/_search?format=yaml" -d '
{
  "query" : {
    "span_near" : {
      "clauses" : [
        { "field_masking_span" : { "query" : { "span_term" : { "str" : "foo" } }, "field" : "block" } },
        { "field_masking_span" : { "query" : { "span_multi" : { "match" : { "range" : { "num" : { "from" : "15", "to" : "25", "include_lower" : true, "include_upper": true } } } } }, "field" : "block" } }
      ],
      "slop" : -1,
      "in_order" : false
    }
  }
}
'
```

Now, if you add position_offset_gap of 100 in the index defintion, and run a span query with a slop of 10, the document in incorrectly selected.
Example 2a:

``` bash
# index (position_offset_gap on str)
          "properties" : {
            "str" : { "type" : "string", "index_options" : "positions", "position_offset_gap" : "100" },
            "num" : { "type" : "long", "index_options" : "positions" }
          }
# query (between a str and a num)
curl -XPOST "http://localhost:9200/spans/_search?format=yaml" -d '
{
  "query" : {
    "span_near" : {
      "clauses" : [
        { "field_masking_span" : { "query" : { "span_term" : { "str" : "foo" } }, "field" : "block" } },
        { "field_masking_span" : { "query" : { "span_term" : { "num" : "20"  } }, "field" : "block" } }
      ],
      "slop" : 10,
      "in_order" : false
    }
  }
}
'
# result: select both docs, while doc1 should not be selected.
```

Example 2b:

``` bash
# index (position_offset_gap on num)
          "properties" : {
            "str" : { "type" : "string", "index_options" : "positions" },
            "num" : { "type" : "long", "index_options" : "positions", "position_offset_gap" : "100" }
          }
# query (between two str)
curl -XPOST "http://localhost:9200/spans/_search?format=yaml" -d '
{
  "query" : {
    "span_near" : {
      "clauses" : [
        { "field_masking_span" : { "query" : { "span_term" : { "str" : "foo" } }, "field" : "block" } },
        { "field_masking_span" : { "query" : { "span_term" : { "str" : "bar"  } }, "field" : "block" } }
      ],
      "slop" : 10,
      "in_order" : false
    }
  }
}
'
# result: select doc1, while it should not.
```
</comment><comment author="clintongormley" created="2013-06-01T11:57:41Z" id="18787832">@s1monw What's your take on this? Do positions on numbers make sense?
</comment><comment author="iksnalybok" created="2013-06-01T20:47:35Z" id="18796753">If I may so, for my part, it does, with the slop=-1 trick described in https://lucene.apache.org/core/4_3_0/core/org/apache/lucene/search/spans/FieldMaskingSpanQuery.html. I don't know any other use case, but this one is quite interesting: it emulates simple nesting, without the need of using nested documents.

For example: let's say you want to index issues of github. A document can be the project name, a title, the issue number, and a list of tuples (author, date, comment). NB: date is a numeric field. The document is re-indexed each time someone updates the issue (a new tuple is added). To retrieve the issues a specific person has worked on during the last seven days, I can issue a span query, with slop -1, first clause being "span_term : { author : somebody }", and the second one  "span_multi : { match : { range : { date : { from : today, to : today-7days } } } }". Without positions, you retrieve issues for which the person has worked on and somebody (can be someone else) has worked on during the last seven days. Using a string instead of a date requires some custom code to correctly handle range queries.
Not disclosing real-life business, but I have exactly this need to monitor events happening to an entity.

That's about positions, and ES currently supports them. To be honest, it's what I need for the moment; I can't remember why I focused on the offset_gap and not positions.
The example does not actually make use of the position_offset_gap, but at least for consistency, it makes sense to support it. A possible use of position_offset_gap I'm thinking of may be when a element of the tuple is it-self a bounded list of sth (ex: comment is a list of maximum 100 paragraphs), so using position_offset_gap=220 and searching with a slop of 104 allows to limit the span query to a paragraph instead of the full comment.
</comment><comment author="clintongormley" created="2014-07-08T19:28:22Z" id="48388354">Reading through this ticket again, it seems like nested docs are a much better solution for this, and likely will perform a lot better than span queries. Either way, it hasn't received any traction in the last year, so I'm going to close this.  If you feel that there is a use case here that I'm missing, please could you open another ticket with the details.  thanks
</comment><comment author="iksnalybok" created="2014-07-28T12:44:27Z" id="50333116">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested sorting "missing": "_first" assumes wrong position</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3095</link><project id="" key="" /><description>```
curl -XPUT 'http://localhost:9200/test/type1/_mapping' -d '{
    "type1" : {
        "properties" : {
            "message" : {"type" : "nested"}
        }
    }
}'


curl -XPUT localhost:9200/test/type1/1 -d '{
    "message": { "counter" : 0 },
    "tags" : ["first"]
}'

curl -XPUT localhost:9200/test/type1/2 -d '{
    "message": { "counter" : 1 },
    "tags" : ["second"]
}'

curl -XPUT localhost:9200/test/type1/3 -d '{
    "tags" : ["third"]
}'


curl 'localhost:9200/test/type1/_search?pretty=true' -d '{
    "sort" : [
        { "message.counter" : {
              "order" : "asc",
              "missing" : "_first"
            }
        }
    ],
    "query" : {
        "match_all" : {}
    }
}'
```

The query gives me:

```
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 3,
    "max_score" : null,
    "hits" : [ {
      "_index" : "test",
      "_type" : "type1",
      "_id" : "1",
      "_score" : null, "_source" : {
    "message": { "counter" : 0 },
    "tags" : ["first"]
},
      "sort" : [ 0 ]
    }, {
      "_index" : "test",
      "_type" : "type1",
      "_id" : "3",
      "_score" : null, "_source" : {
    "tags" : ["third"]
},
      "sort" : [ 0 ]
    }, {
      "_index" : "test",
      "_type" : "type1",
      "_id" : "2",
      "_score" : null, "_source" : {
    "message": { "counter" : 1 },
    "tags" : ["second"]
},
      "sort" : [ 1 ]
    } ]
  }
}
```

while I expected the order to be "third", "first", "second".

```
curl localhost:9200
{
  "ok" : true,
  "status" : 200,
  "name" : "Wolverine",
  "version" : {
    "number" : "0.90.0",
    "snapshot_build" : false
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="14775586">3095</key><summary>Nested sorting "missing": "_first" assumes wrong position</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schnittchen</reporter><labels /><created>2013-05-26T21:36:47Z</created><updated>2014-09-06T08:29:56Z</updated><resolved>2014-07-23T12:29:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="schnittchen" created="2013-05-26T21:38:55Z" id="18470175">I'm only guessing here, but the `"sort" : [ 0 ]` made me suspect the "missing" feature is implemented by replacing missing values with an extremal value, and the bug would come from taking 0 instead of MINLONG or along those lines...
</comment><comment author="clintongormley" created="2014-07-23T12:29:09Z" id="49866755">This appears to have been fixed. Closing.
</comment><comment author="schnittchen" created="2014-09-06T08:29:56Z" id="54706363">Confirmed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>IllegalAccessError when using an MVEL script filter in the query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3094</link><project id="" key="" /><description>Hi,

We have a 2-node ES cluster running 0.90.0 release in our QA environment.  When we started using a script filter in our query, we got the following stack trace after a few queries to ES.  After looking into the issue a bit, the issue seemed to be caused by the use of MVEL in compiled or accelerated mode.  Further research seems to indicate that disabling JIT for MVEL worked around the problem (via -Dmvel2.disable.jit=true).

Disabling MVEL JIT would also mean a performance concern for us.  What's root cause of this issue?  How should we address it?

Our QA environment.

&lt;b&gt;OS&lt;/b&gt;
Ubuntu 12.10 (GNU/Linux 3.5.0-17-generic x86_64)

&lt;b&gt;Java version, vendor&lt;/b&gt;
java version "1.7.0_21"
OpenJDK Runtime Environment (IcedTea 2.3.9) (7u21-2.3.9-0ubuntu0.12.10.1)
OpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)

&lt;b&gt;Script&lt;/b&gt;

&lt;pre&gt;
if(doc['targeting.keys'] == null || doc['targeting.keys'].values.length == 0)
      return true;
foreach (kv : doc['targeting.keys'].values) {
       var found = 0;
       foreach (qkey : qkeys) {
            if (kv == qkey) {
                 found = 1;
            }
       }
       if(found == 0){
            return false;
       }
 }
return true;
&lt;/pre&gt;


&lt;b&gt;Stack Trace&lt;/b&gt;

&lt;pre&gt;
[2013-05-23 21:04:55,650][DEBUG][action.search.type       ] [Omen] [indexedbuyitem][0], node[-3mMO7JjTKKhIk92AJt_xQ], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1d5e469d]

java.lang.IllegalAccessError: org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1
        at ASMAccessorImpl_1471145441369339522430.getValue(Unknown Source)
        at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)
        at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
        at org.elasticsearch.common.mvel2.ast.BinaryOperation.getReducedValueAccelerated(BinaryOperation.java:108)
        at org.elasticsearch.common.mvel2.ast.Or.getReducedValueAccelerated(Or.java:34)
        at org.elasticsearch.common.mvel2.ast.Or.getReducedValueAccelerated(Or.java:34)
        at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:85)
        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
        at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:192)
        at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter$ScriptDocSet.matchDoc(ScriptFilterParser.java:184)
        at org.elasticsearch.common.lucene.docset.MatchDocIdSet.get(MatchDocIdSet.java:67)
        at org.apache.lucene.search.FilteredDocIdSet$1.get(FilteredDocIdSet.java:65)
        at org.elasticsearch.common.lucene.docset.AndDocIdSet$AndBits.get(AndDocIdSet.java:106)
        at org.elasticsearch.common.lucene.docset.AndDocIdSet$AndBits.get(AndDocIdSet.java:106)
        at org.elasticsearch.common.lucene.docset.BitsDocIdSetIterator$FilteredIterator.match(BitsDocIdSetIterator.java:59)
        at org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:60)
        at org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:59)
        at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:185)
        at org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorScorer.nextDoc(FiltersFunctionScoreQuery.java:283)
        at org.apache.lucene.search.Scorer.score(Scorer.java:63)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:605)
        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:156)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:572)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:501)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:281)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:213)
        at org.elasticsearch.search.action.SearchServiceTransportAction$2.handleException(SearchServiceTransportAction.j
&lt;/pre&gt;
</description><key id="14766652">3094</key><summary>IllegalAccessError when using an MVEL script filter in the query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">connieyang</reporter><labels><label>non-issue</label></labels><created>2013-05-26T05:21:25Z</created><updated>2014-03-28T08:33:22Z</updated><resolved>2013-08-21T13:59:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-26T19:35:23Z" id="18468310">hey connie,  thanks for opening the issue! First of all I can't reproduce the problem on my mac but I will try a icetea JVM tomorrow. While doing this can I ask you to try to track down what exactly raises the issue ie. can you run the script without this part: 

```
if(doc['targeting.keys'] == null || doc['targeting.keys'].values.length == 0)
      return true;
```

this should not be needed anyway since we return an `empty` instance anyway instead of `null` then go on and remove the body of the loop ie:

```
foreach (kv : doc['targeting.keys'].values) {

}
return true;
```

just to figure out where I need to look closer :) can you report this back to me? The exception is pretty odd since it usually only happens if something is entirely wrong ie. you try to access a protected field from a class in the same package but located in a different jar. Not sure what causes this here in this script but I can miss the illegal access so I'd love to hear back what you can find out.
</comment><comment author="connieyang" created="2013-05-27T07:13:36Z" id="18485436">Sure.  We can give that a try.

You are right that this issue didn't exist on mac.  We didn't discover this issue until we rolled this feature to our QA environment where icetea JVM is used.

Could this be a class loader issue?
</comment><comment author="s1monw" created="2013-05-27T08:35:02Z" id="18488124">I ran this on 1.6 and 1.7 IceTea versions with no luck reproducing the issue. Is it possible that you have 2 ES jars in the classpath somehow?
</comment><comment author="connieyang" created="2013-05-27T17:18:01Z" id="18507871">I can double check, but our ES installation is based on the 0.90 release zip file + JSW + the Head plugin +the paramedics plugin.  We have not added or introduced any new libraries in the installation.

Just want to double check, you ran the test with JIT enabled and MVEL in its compiled or accelerated mode, which should be the default,  right?  Also, this issue does not occur immediately, it takes a few queries for this to happen.
</comment><comment author="saibaskaran" created="2013-05-30T19:57:27Z" id="18704119">Hi,
    I had the same issue and when I replaced .length with .size() it worked. I am not sure why. Please try that and let me know if it solves the issue. Thanks,
</comment><comment author="connieyang" created="2013-05-30T23:58:26Z" id="18716046">Hmm, the documentation seems to indicates that it's an array and hence we used .length.

As suggested in the earlier thread, we removed the null and empty check seems to address the problem.  However, I just want to point out that the above script works (aka no IllegelAccessError) during the first 10 minutes of our load and performance run (with 200 concurrent users running in 200 loops).  We started seeing the IllegalAccessError after that.  This seems to suggest a GIT related issue.

Thanks for your reply.
</comment><comment author="s1monw" created="2013-05-31T07:32:44Z" id="18728626">I guess this is due to the fact that length is a member variable of the returned iterable and not a method while size() is one. I think we should also add length() which can be addressed as values.length to help jit a bit here? this seems very odd though but I guess that is a mvel problem?
</comment><comment author="s1monw" created="2013-08-21T13:59:36Z" id="23018843">moving out - seems like a JIT issue for some reason - can't really fix this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add {path} keyword to dynamic templates to be replaced with the full path of the field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3093</link><project id="" key="" /><description>For a project I've been working on I have json documents that contain multiple inner objects, which contain similar fields, as in the following example:

```
{
    "inner1" : {
        "title" : "title inner1" 
    },
    "inner2" : {
        "title" : "title inner2"
    }
}
```

I want to keep all the titles in separate fields `inner1.title` and `inner2.title`, but at the same time I want to have a catch all field that contains all the titles, possibly as a top level field called `title`. I achieved this with a multi_field using the `just_name` path like this:

```
"inner1" : {
    "type" : "object",
    "properties" : {
        "title" : {
            "type" : "multi_field",
            "path" : "just_name",
            "fields" : {
                "title" : {"type":"string"},
                "inner1.title" : {"type":"string"}
            }
        }
    }
}
```

It is tricky because it's exactly the opposite of what a normal multi_field would do, since the first title (without path)  becomes the top level catch all, while the other `inner1.title` is the usual `inner1.title` field. Since I have to repeat in the mapping the same multi_field for every level where I want to take the title from, I decided to use dynamic templates to keep the mapping to submit when creating the index more concise. Unfortunately the fact that the second field in the multi_field contains the path is a problem and not supported through a placeholder like {name} or {dynamic_type}.

The pull request adds the support for a new {path} placeholder that makes it possible to do the following:

```
"template":{
    "path_match":"*.title",
    "mapping":{
        "type": "multi_field",
        "path":"just_name",
        "fields":{
            "{name}": {"type": "{dynamic_type}"},
            "{path}": {"type": "{dynamic_type}"}
        }
    }
}
```

This way with a single dynamic template I can achieve what I want for multiple fields taken from multiple levels in the json tree.
</description><key id="14764361">3093</key><summary>Add {path} keyword to dynamic templates to be replaced with the full path of the field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-05-25T23:49:18Z</created><updated>2014-07-28T09:31:26Z</updated><resolved>2014-07-28T09:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-28T09:31:26Z" id="50316956">Solved with `copy_to`, no?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Compression using a common LZW dictionary for the whole index or document types</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3092</link><project id="" key="" /><description>Hello,

We are storing massive amount of data which is quite redundant (documents do not vary greatly). For this reason I have been looking into the compression option of ES and see that individual documents or document in a bulk can be indexed together.

The drawback of the bulk method is that the whole bulk would needed to be uncompressed if a single document in the bulk is needed (as I understand it).

An interesting option for us would be the ability to use a single, growing, dictionary to compress all the _source documents. This would make result in a bigger LZW dictionary, but it would also mean a better compression ratio and the ability to uncompress single documents.

Is this something which has been considered?

Very best regards,
  -Stefan Baxter
</description><key id="14755074">3092</key><summary>Compression using a common LZW dictionary for the whole index or document types</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">acmeguy</reporter><labels /><created>2013-05-25T08:37:55Z</created><updated>2013-05-25T20:52:21Z</updated><resolved>2013-05-25T10:29:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-25T10:29:14Z" id="18444985">Hi Stefan

Single document or bulk indexing is no different.  Also, the default codec that we use in 0.90 to write new segments compresses stored fields and term vectors by default.

You don't need to enable anything for this to work - it's the default now.
</comment><comment author="acmeguy" created="2013-05-25T10:46:33Z" id="18445168">Hi,

So ES is using a single common LZW dictionary for all _source documents already?

Regards,
 -Stfan
</comment><comment author="clintongormley" created="2013-05-25T10:48:51Z" id="18445200">For all _source documents in a segment, yes.  If you want to make that a single dictionary for a shard, then optimize the shard down to one segment. Of course, if you keep indexing, then it is pointless to do that - new segments will be created, but smaller segments get merged into a bigger segment automatically, so you can just leave it up to ES to handle.
</comment><comment author="acmeguy" created="2013-05-25T10:49:25Z" id="18445209">Great, thank you!
</comment><comment author="jpountz" created="2013-05-25T14:11:33Z" id="18447757">I'll try to give a little more information on how stored fields compression works under the cover.

The stored fields file is compressed into blocks of 16K or more, and Lucene maintains a very compact in-memory data-structure that maps every document ID to the start offset of the block that contains it (a document cannot span across several blocks). All blocks are independent, meaning there is no shared dictionary, and are compressed using LZ4 (https://code.google.com/p/lz4/), an LZ77-based compression codec which trades CPU for compression ratio.

At reading time, there is an optimization that allows to decompress as little data as needed. For example, if you are looking for the 2nd document of a block that contains 7 documents, it is very likely that the block will only be partially decoded.

There are a few optimizations that would allow to improve the compression ratio, but it is likely that they will bring a few drawbacks too. For example, the doc-&gt;offset mapping that Lucene stores in memory is very compact because it only actually stores only the address for the first document of a block (and uses a binary-search like approach to find the block offset for a given doc ID). If documents were not stored in large blocks anymore, this will likely require more memory.

Additionally, the fact that there is no shared state across segments is great for merging, since in a few common cases Lucene can directly copy the compressed data without having to decompress and then compress it again.
</comment><comment author="acmeguy" created="2013-05-25T14:21:25Z" id="18447886">Thank you.

This also means that the _real answer_ to my question is no :). (if I understand you correctly).

I know I'm nitpicking here (this is more a curiosity rather than a requirement) but the data I'm working with would really benefit from being compresses using a single, common, compression dictionary.

I understand the logic behind the current approach and am in no position to state that a common dictionary has enough benefits to warrant an inquiry.

Very best regards,
  -Stefan
</comment><comment author="clintongormley" created="2013-05-25T15:46:22Z" id="18449160">OK, so I fudged it a bit :)
</comment><comment author="acmeguy" created="2013-05-25T20:52:21Z" id="18453724">NP.

Dealing effectively with highly redundant data should be quite valuable.
I hope an opportunity will present it self for you guys to investigate it further :)

Regards,
 -Stefan
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Dates passed to the script terms facet are now in the default time zone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3091</link><project id="" key="" /><description>This may be on purpose, but it's a breaking change from 0.20.6, where dates were always UTC. See discussion at  https://groups.google.com/d/topic/elasticsearch/a_Jj7d4Uk6c/discussion
</description><key id="14750172">3091</key><summary>Dates passed to the script terms facet are now in the default time zone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">ejain</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-25T00:32:28Z</created><updated>2013-11-20T14:22:45Z</updated><resolved>2013-05-25T20:44:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-25T06:41:43Z" id="18440657">From the comment thread:

&gt; Actually both of those are the same millisecond value stored in the date field displayed differently (In the Summer 8 AM in Seattle is the same moment in time as 3 PM in London) Therefore, it depends on the definition of what the script aDateField.date is supposed to correspond to in the Joda date library and thus what toString method is used to convert the results back to the caller.  It is not a matter of honoring the original TZ, because it can't, the field stores a millisecond value; that's it.  On the way out nothing knows how it got that millisecond value into the field.   
&gt; 
&gt; As Eric as observed
&gt; it WAS converting the millisecond value to the result string using UTC.
&gt; but now
&gt; it IS converting to the millisecond value to the result string using the DEFAULT VM timezone.

This sounds like a bug to me...
</comment><comment author="kimchy" created="2013-05-25T18:30:43Z" id="18451813">It is a bug, it should have remained UTC, will fix shortly...
</comment><comment author="CodingFabian" created="2013-11-20T14:16:04Z" id="28892046">~~I am observing that dates in from and to fields of range facets for date field are using local time zone instead of UTC~~
~~For example, I have an entry in the index which says:~~
~~upload_date: 2013-10-02T13:40:40.681Z~~
~~When I do facetting on that field, the from value is for example
~~2012-11-20T15:06:04.770+01:00~~
~~The field is specified in index just as "date", no special formatting given.~~
~~When I query the field from ES, the field value is also in UTC.~~

~~So it seems that the generation of from and to bounds for range facets on date fields somehow does not utc, but a local time.~~

~~Shall I create a new issue, is it a new issue or the same?~~

Usual case of: I can see the error after writing the issue.
We are giving in in our date facet range query time stamps with +1 and it seems ES honors them when returning them back to us.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title> all_terms = true for (Date) Histogram facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3090</link><project id="" key="" /><description>Not sure if this is possible but it would be great to have the option of setting all_terms = true for (date) Histogram facets.
</description><key id="14734721">3090</key><summary> all_terms = true for (Date) Histogram facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gebrits</reporter><labels /><created>2013-05-24T17:01:20Z</created><updated>2014-02-13T09:44:59Z</updated><resolved>2014-01-13T15:40:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ecandino" created="2013-06-05T21:08:24Z" id="19008711">+1
</comment><comment author="zhoutuo" created="2013-07-03T22:31:52Z" id="20449229">I will really appreciate this if it could be implemented, since i'm doing it manually right now.
</comment><comment author="spinscale" created="2013-07-04T10:03:07Z" id="20469098">What is your goal of this? Getting the fist and the last available dates in your time based data? If so, a statistical facet could help you as well. Take this sample:

```
curl -X PUT 'localhost:9200/foo/bar/1?refresh' -d '{ "date":"2013-07-04"}'
curl -X PUT 'localhost:9200/foo/bar/2?refresh' -d '{ "date":"2013-07-03"}'
curl -X PUT 'localhost:9200/foo/bar/3?refresh' -d '{ "date":"2013-07-02"}'

curl -X POST 'localhost:9200/foo/bar/_search?size=0' -d '{"facets" : { "d" : { "statistical": { "field":"date" }  } } }'
{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":3,"max_score":1.0,"hits":[]},
"facets":{"d":{"_type":"statistical","count":3,"total":4.1184288E12,"min":1.3727232E12,"max":1.372896E12,"mean":1.3728096E12,
"sum_of_squares":5.6538186084864E24,"variance":4.976639809159168E15,"std_deviation":7.054530323954365E7}}}

# now extract the max and min values from the response and get back max/min dates
# divide by 1000 because of the millisecond resolution
date -r $(($(printf '%.0f\n' 1.3727232E12)/1000))
date -r $(($(printf '%.0f\n' 1.372896E12)/1000))
```
</comment><comment author="dadoonet" created="2013-07-08T09:32:19Z" id="20594582">@spinscale I think he wants to have intermediate values even if there are no documents. Something like: 
- 2013-07-01: 10
- 2013-07-02: 0
- 2013-07-03: 5
- 2013-07-04: 0
- 2013-07-05: 10
</comment><comment author="dgouyette" created="2013-07-08T09:51:58Z" id="20595511">any advice to make it works ?
</comment><comment author="gebrits" created="2013-07-08T09:57:01Z" id="20595749">Correct that's the need.

It allows for trivial updating of things as sliders, etc. in which no
client-side transform is needed to supply the missing values.
It's no real dealbreaker, but with this option a lot of clientside widgets
that expect a non-sparse dataset just work.
</comment><comment author="dgouyette" created="2013-07-08T10:00:09Z" id="20595873">I&#160;want to make charts with DateHistogramFacet.
it's won't work out of the box
</comment><comment author="SeyZ" created="2014-01-13T15:12:40Z" id="32177272">+1  for missing values. Feedback for this issue?
</comment><comment author="jpountz" created="2014-01-13T15:40:00Z" id="32180074">I am closing this issue as this feature is going to be supported by the new `date_histogram` aggregation. Passing `min_doc_count: 0` to this aggregation will also render date buckets that match no hit.
</comment><comment author="dgouyette" created="2014-01-13T15:41:25Z" id="32180205">Great, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make suggest API implementations pluggable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3089</link><project id="" key="" /><description>In order to create a foundation for more possible suggesters, we need to open up the suggest API and support the possibility of registering suggester types (similar to registering facets for example).
</description><key id="14731304">3089</key><summary>Make suggest API implementations pluggable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-05-24T15:45:46Z</created><updated>2013-06-24T10:41:28Z</updated><resolved>2013-05-28T07:00:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-27T12:35:02Z" id="18496473">pull request looks awesome! maybe we can get a simple test for a "dummy" suggester that serves as an impl example for others?
</comment><comment author="spinscale" created="2013-05-27T17:00:16Z" id="18507274">Hey, I have pushed another version including tests and a dummy suggester

One more thing: Do you think it makes sense to create another kind of `SuggestContext` class as the only parameter for the `Suggester.suggest()` method?
</comment><comment author="s1monw" created="2013-05-27T17:12:19Z" id="18507668">@spinscale I think the current commit looks good! LGTM... push it!
</comment><comment author="s1monw" created="2013-06-18T13:36:22Z" id="19611367">@spinscale any reason why this is not in 0.90? Can you backport it?
</comment><comment author="spinscale" created="2013-06-24T10:41:28Z" id="19899818">@s1monw done
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"no index mapper found for field" bulk indexing in 0.90</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3088</link><project id="" key="" /><description>Today I've been getting horrible performance on 0.90 server I upgraded last week. Running on Ubuntu, Ruby 1.9.3, Rails 3.2.3, with the Stretcher gem.

The issue is in bulk indexing Twitter data, with JSON very similar to the response shown at: https://dev.twitter.com/docs/api/1.1/get/statuses/user_timeline, except the user node of each item is removed and is instead a parent document (although I don't think that is the issue).

My mappings are intentionally kept simple, using dynamic mappings to add fields. Everything has been working fine to this point. Now I'm getting the following error, bringing the server to its knees.

The error is:
[2013-05-23 22:13:01,792][WARN ][index.merge.scheduler    ] [Rocket Racer] [twitter_history_76816072][1] failed to merge
org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]
        at org.elasticsearch.index.codec.PerFieldMappingPostingFormatCodec.getPostingsFormatForField(PerFieldMappingPostingFormatCodec.java:52)
        at org.apache.lucene.codecs.lucene42.Lucene42Codec$1.getPostingsFormatForField(Lucene42Codec.java:59)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.addField(PerFieldPostingsFormat.java:102)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:71)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3693)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3296)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:401)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:478)
[2013-05-23 22:13:01,939][WARN ][index.engine.robin       ] [Rocket Racer] [twitter_history_76816072][1] failed engine
org.apache.lucene.index.MergePolicy$MergeException: org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]
        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:100)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:514)
Caused by: org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]
        at org.elasticsearch.index.codec.PerFieldMappingPostingFormatCodec.getPostingsFormatForField(PerFieldMappingPostingFormatCodec.java:52)
        at org.apache.lucene.codecs.lucene42.Lucene42Codec$1.getPostingsFormatForField(Lucene42Codec.java:59)
        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.addField(PerFieldPostingsFormat.java:102)
        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:71)
        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3693)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3296)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:401)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:478)
[2013-05-23 22:13:04,187][WARN ][cluster.action.shard     ] [Rocket Racer] sending failed shard for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[INITIALIZING], reason [engine failure, message [MergeException[org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]]; nested: ElasticSearchIllegalStateException[no index mapper found for field: [entities.description.urls.display_url]]; ]]
[2013-05-23 22:13:04,234][WARN ][cluster.action.shard     ] [Rocket Racer] received shard failed for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[INITIALIZING], reason [engine failure, message [MergeException[org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]]; nested: ElasticSearchIllegalStateException[no index mapper found for field: [entities.description.urls.display_url]]; ]]
[2013-05-23 22:13:07,433][WARN ][indices.cluster          ] [Rocket Racer] [twitter_history_76816072][1] master [[Rocket Racer][_1cPivHASUOzVcgQ2fK0_A][inet[/10.178.13.201:9300]]] marked shard as started, but shard have not been created, mark shard as failed
[2013-05-23 22:13:07,446][WARN ][cluster.action.shard     ] [Rocket Racer] sending failed shard for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[STARTED], reason [master [Rocket Racer][_1cPivHASUOzVcgQ2fK0_A][inet[/10.178.13.201:9300]] marked shard as started, but shard have not been created, mark shard as failed]
[2013-05-23 22:13:07,446][WARN ][cluster.action.shard     ] [Rocket Racer] received shard failed for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[STARTED], reason [master [Rocket Racer][_1cPivHASUOzVcgQ2fK0_A][inet[/10.178.13.201:9300]] marked shard as started, but shard have not been created, mark shard as failed]

The best guess I have is that some random data is breaking the creation of new mappings. The same is happening on the same index in both production and test environments. Other indexes with the same initial mappings but different Twitter data content are not exhibiting the same error. 

I'm trying to capture the data, although there is a mass of it. Therefore before I go too far, has anybody got any clue how I can start to identify the source of the problem?
</description><key id="14719267">3088</key><summary>"no index mapper found for field" bulk indexing in 0.90</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">philayres</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-24T12:22:06Z</created><updated>2014-01-23T19:42:06Z</updated><resolved>2013-05-24T15:40:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="philayres" created="2013-05-24T13:07:44Z" id="18403595">An additional note. I found this similar issue for another user: http://www.marshut.com/kpnht/problems-with-rabbitmq-river-90rc2.html that I think is unrelated to their RabbitMQ river, but is the same issue as I am seeing. No resolution over there apart from to unplug the river.
</comment><comment author="s1monw" created="2013-05-24T14:19:52Z" id="18407584">no matter why this happens it's bad to fail with a corrupt index (which is essentially what an exception during merge is) We should track down the race but first of all I think we should return the default postings format instead of failing with an exception
</comment><comment author="philayres" created="2013-05-25T15:58:13Z" id="18449338">Thanks for the incredibly fast response. 

I see from the commits we are now logging the issue rather than raising an exception. What effect does this have on the automatic creation of a new element in the mapping if we hit this condition? Do we just skip it, or is the mapping now updated accordingly? My concern is just that we would be arbitrarily not indexing elements in a document based on this error. Or is this not really an error condition and I'm misinterpreting?
</comment><comment author="s1monw" created="2013-05-25T16:46:36Z" id="18450136">the condition you are seening is certainly not correc, the mapper for a given field is not present in the mapping which can have multiple reasons which I am trying to figure out at the moment. My suspicion is that due to some index deleting or type deletion the mappings are pruned and therefor a concurrently running merge can't access this mapping. So what happens if we return the default postings format rather than a per field format is in the worst case the default field format for the current lucene version you are using (which in your case is very likely identical). That won't be a problem since we pruned the mapper anyways so this field will likely not be accessed anymore. I am still trying to reproduce this but your docs will be updated correctly.
</comment><comment author="philayres" created="2013-05-25T21:29:18Z" id="18454227">Simon, I'd be happy to spin up a copy of this server so you can take a look directly at the mappings and data if that would help to reproduce this situation. Just let me know.

Just in case it is useful, I spotted the field in question (entities.description.urls.display_url) potentially having its mapping set when the data being passed included a 'special' character. I don't know if the actual field value could affect the mapping creation. The data was a regular ascii string ending with an elipsis (hex E2 80 A6). This may be unrelated, it just seemed unusual.
</comment><comment author="ajhalani" created="2013-05-30T15:07:06Z" id="18686285">This issue affected us for regular indexing as well. We suspect this was caused when in 0.20.x, we overwrote existing mapping with new mapping which had fewer fields. The data somehow still had the old fields and merge failed with 0.90.0.
With 0.90.1, we are now getting warnings but doesn't fail. thanks!
</comment><comment author="s1monw" created="2013-05-30T15:15:14Z" id="18686897">cool thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>update delete fields appearing several times in the document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3087</link><project id="" key="" /><description>Let's explain with an example.

First create the index:

``` bash
curl -XPUT "http://127.0.0.1:9200/index/" -d '
{
    "settings" : {
        "index" : {
            "number_of_shards" : 1,
            "number_of_replicas" : 0,
            "analysis" : {
                "analyzer" : {
                    "A" : {
                        "tokenizer" : "whitespace",
                        "filter" : [ "asciifolding", "lowercase" ]
                    }
                }
            }
        }
    },
    "mappings" : {
        "type" : {
            "_all"       : { "enabled" : "true", "analyzer": "A" },
            "_source"    : { "enabled" : "true" },
            "_timestamp" : { "enabled" : "true" },
            "properties" : {
                "firstname" : { "type" : "string", "analyzer" : "A" },
                "lastname"  : { "type" : "string", "analyzer" : "A" }
            }
        }
    }
}'
```

Then add a document:

``` bash
curl -XPOST "localhost:9200/index/type/1" -d '
{
    "lastname"  : "Doe"   , "firstname" : "John",
    "lastname"  : "Martin", "firstname" : "Jean", "firstname" : "Albert",
    "lastname"  : "Smith" , "firstname" : "Adam"
}'
```

Here firstname and lastname are keys that appears many times.

When searching for the document, we see all values:

``` bash
curl -XGET "http://127.0.0.1:9200/index/_search?pretty=true&amp;format=yaml" -d '{
 "query": { "match_all": {} }
}'
=&gt;  _source:
      lastname: "Doe"
      firstname: "John"
      lastname: "Martin"
      firstname: "Jean"
      firstname: "Albert"
      lastname: "Smith"
      firstname: "Adam"
```

After updating nothing, we see only the last values of each field:

``` bash
curl -XPOST 'localhost:9200/index/type/1/_update' -d '{
   "script" : ""
}'
curl -XGET "http://127.0.0.1:9200/index/_search?pretty=true&amp;format=yaml" -d '{
   "query": { "match_all": {} }
}'
=&gt;  _source:
      lastname: "Smith"
      firstname: "Adam"
```

I know that using an array works

``` bash
curl -XPOST "localhost:9200/index/type/1" -d '
{
    "lastname"  : [ "Doe", "Martin", "Smith" ],
    "firstname" : [ "John", "Jean", "Albert", "Adam" ]
}'
```

but then I loose the mapping between lastnames and firstnames.
</description><key id="14718287">3087</key><summary>update delete fields appearing several times in the document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-05-24T11:48:57Z</created><updated>2013-05-24T14:07:51Z</updated><resolved>2013-05-24T12:49:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-24T12:49:37Z" id="18402782">Repeating key names is not valid JSON.  It works in places in ES because it uses a stream parser, but this behaviour should not be relied upon.  Passing your `_source` via almost any other JSON parser will remove the repeated keys, as is happening when you update.

I suggest you use an array with: 

```
[
   { firstname: "John", lastname: "Doe"},
   { firstname: "Jean", lastname: "Martin"}
]
```
</comment><comment author="iksnalybok" created="2013-05-24T13:56:57Z" id="18406285">Thanks for the suggestion !

Tried. It works perfectly. I feared this would generate nested documents, but no. Thanks again.
</comment><comment author="iksnalybok" created="2013-05-24T14:07:50Z" id="18406889">For anyone interested, the mapping to manage such structure looks like this:

```
            "properties" : {
                "persons" : {
                    "properties": {
                        "lastname"  : { "type" : "string" },
                        "firstname" : { "type" : "string" }
                    }
                }
            }
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add 'ignore_unmapped' to faceted search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3086</link><project id="" key="" /><description>Similar in scope to the current 'ignore_unmapped' (issue #1558) used for sorting search results, this parameter would prevent an exception from being generated when a given term is not mapped within an index during a faceted search.

This would be most helpful when generating faceted results across multiple indexes and a certain term does not exist in a given index within the search. The results for the indexes where a term does exist would still be created.

An example:
indexA,indexB,indexC/blogs/_search

```
{
    "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "tag" : {
            "terms" : {
                "field" : "tag",
                "size" : 10,
                "ignore_missing": true
            }
        }
    }
}
```

This should be applied across all facet queries: term, range, histogram, date histogram, statistical, etc.
This ticket could be modified to apply the flag via the uri.
</description><key id="14717893">3086</key><summary>Add 'ignore_unmapped' to faceted search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels /><created>2013-05-24T11:35:05Z</created><updated>2013-10-22T07:48:24Z</updated><resolved>2013-10-22T07:48:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-22T07:48:24Z" id="26783215">Closing as duplicate of #2569 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ignore_unmapped'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3085</link><project id="" key="" /><description /><key id="14717891">3085</key><summary>Add ignore_unmapped'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bryangreen</reporter><labels /><created>2013-05-24T11:35:03Z</created><updated>2013-05-24T11:45:34Z</updated><resolved>2013-05-24T11:45:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index endpoint doesn't return a 201 Created status code when using an external version type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3084</link><project id="" key="" /><description /><key id="14715000">3084</key><summary>Index endpoint doesn't return a 201 Created status code when using an external version type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels /><created>2013-05-24T10:01:35Z</created><updated>2013-06-13T07:23:30Z</updated><resolved>2013-06-13T07:23:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-26T18:27:53Z" id="18467266">cool stuff I will look into this soon.
</comment><comment author="s1monw" created="2013-05-26T18:32:26Z" id="18467340">after a quick review, I am worried about backwards compatibility. I'd like to push this to 0.90 aswell and if somebody upgrades from 0.90.0 we break the wire format. can you try to leverage StreamInput.getVersion() to conditionally read the boolean / long? I think a "found" flag is still useful so we might wanna keep that?
</comment><comment author="bleskes" created="2013-05-27T07:29:26Z" id="18485912">Answers in reverse order:
- the found flag (I assume you mean the DeleteResponse one) is still supported from the class interface perspective (isNotFound() checks for previousVersion == -1L; ) 
- we could use StreamInput.getVersion to detect we're speaking to a node of an older version. Doing the right thing with it is trickier. If the sender is 0.90.1 and the client 0.90.0 we could just write a boolean instead of a long and things should work. The reverse is trickier because we don't enough info to support the previousVersion functionality.  Last - I looked at the code and I can't find the place where this version actually gets serialised to the wire. Can you help and point it out to me? 
- It was my understanding that we don't support multi-versioned cluster at the moment and therefore don't have to worry about wire level backward compatibility. Did I get this wrong?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>path.data not working in windows 0.9</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3083</link><project id="" key="" /><description>I am setting Q:/esdata for path.data and I get the following startup error.
C:\ddapplications\elasticsearch-0.90.0\bin&gt;elasticsearch.bat
{0.90.0}: Setup Failed ...
- SettingsException[Failed to load settings from [file:/C:/ddapplications/elasti
  csearch-0.90.0/config/elasticsearch.yml]]
      ElasticSearchParseException[malformed, expected settings to start with '
  object', instead was [VALUE_STRING]]
</description><key id="14697666">3083</key><summary>path.data not working in windows 0.9</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dynamicdeploy</reporter><labels /><created>2013-05-23T21:53:27Z</created><updated>2013-05-23T21:55:08Z</updated><resolved>2013-05-23T21:55:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dynamicdeploy" created="2013-05-23T21:55:06Z" id="18374412">Ok, there needs to be a space between path.data: and Q:/. Works now. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast Cluster on Azure Virtual Machines not working (0.90)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3082</link><project id="" key="" /><description>I am trying to build a unicast cluster on Azure Virtual machines and it does not seem to work. I don't see any errors. I have enabled debug mode and here are the logs.
Also, both the machines can talk to each other. I looked at the source and the hosts array (using initial hosts [],) during initialization seems to be empty even though I am specifying 2 hosts in it.
## Node 1: Config

``` yml
#################################### Node #####################################

# Node names are generated dynamically on startup, so you're relieved
# from configuring them manually. You can tie this node to a specific name:
#
 node.name: "xES1"

################################## Discovery ##################################

# Discovery infrastructure ensures nodes can be found within a cluster
# and master node is elected. Multicast discovery is the default.

# Set to ensure a node sees N other master eligible nodes to be considered
# operational within the cluster. Set this option to a higher value (2-4)
# for large clusters (&gt;3 nodes):
#
# discovery.zen.minimum_master_nodes: 1

# Set the time to wait for ping responses from other nodes when discovering.
# Set this option to a higher value on a slow or congested network
# to minimize discovery failures:
#
 discovery.zen.ping.timeout: 10s

# See &lt;http://elasticsearch.org/guide/reference/modules/discovery/zen.html&gt;
# for more information.

# Unicast discovery allows to explicitly control which nodes will be used
# to discover the cluster. It can be used when multicast is not present,
# or to restrict the cluster communication-wise.
#
#1. Disable multicast discovery (enabled by default):
#
 discovery.zen.ping.multicast.enabled: false
#
#2. Configure an initial list of master nodes in the cluster
#    to perform discovery when new nodes (master or data) are started:
#discovery.zen.ping.unicast.hosts: 
discovery.zen.ping.unicast.hosts:["elasticsearch3","rnynjpxyhcfhxdm"]
#discovery.zen.ping.unicast.hosts:["10.78.76.39:9300","10.78.26.64:9300"]
```
## Node 1 logs:

```
[2013-05-23 19:35:12,433][INFO ][node                     ] [xES1] {0.90.0}[3136]: initializing ...
[2013-05-23 19:35:12,435][DEBUG][node                     ] [xES1] using home [C:\ddapplications\elasticsearch-0.90.0], config [C:\ddapplications\elasticsearch-0.90.0\config], data [[C:\ddapplications\elasticsearch-0.90.0\data]], logs [C:\ddapplications\elasticsearch-0.90.0\logs], work [C:\ddapplications\elasticsearch-0.90.0\work], plugins [C:\ddapplications\elasticsearch-0.90.0\plugins]
[2013-05-23 19:35:12,454][INFO ][plugins                  ] [xES1] loaded [], sites [head]
[2013-05-23 19:35:12,524][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder
[2013-05-23 19:35:12,567][DEBUG][env                      ] [xES1] using node location [[C:\ddapplications\elasticsearch-0.90.0\data\elasticsearch\nodes\0]], local_node_id [0]
[2013-05-23 19:35:15,035][DEBUG][threadpool               ] [xES1] creating thread_pool [generic], type [cached], keep_alive [30s]
[2013-05-23 19:35:15,057][DEBUG][threadpool               ] [xES1] creating thread_pool [index], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:35:15,059][DEBUG][threadpool               ] [xES1] creating thread_pool [bulk], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:35:15,060][DEBUG][threadpool               ] [xES1] creating thread_pool [get], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:35:15,065][DEBUG][threadpool               ] [xES1] creating thread_pool [search], type [fixed], size [2], queue_size [1k], reject_policy [abort], queue_type [linked]
[2013-05-23 19:35:15,066][DEBUG][threadpool               ] [xES1] creating thread_pool [percolate], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:35:15,067][DEBUG][threadpool               ] [xES1] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2013-05-23 19:35:15,069][DEBUG][threadpool               ] [xES1] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:35:15,070][DEBUG][threadpool               ] [xES1] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:35:15,071][DEBUG][threadpool               ] [xES1] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:35:15,072][DEBUG][threadpool               ] [xES1] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:35:15,073][DEBUG][threadpool               ] [xES1] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:35:15,147][DEBUG][transport.netty          ] [xES1] using worker_count[2], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1], receive_predictor[512kb-&gt;512kb]
[2013-05-23 19:35:15,166][DEBUG][discovery.zen.ping.unicast] [xES1] using initial hosts [], with concurrent_connects [10]
[2013-05-23 19:35:15,169][DEBUG][discovery.zen            ] [xES1] using ping.timeout [10s], master_election.filter_client [true], master_election.filter_data [false]
[2013-05-23 19:35:15,171][DEBUG][discovery.zen.elect      ] [xES1] using minimum_master_nodes [-1]
[2013-05-23 19:35:15,174][DEBUG][discovery.zen.fd         ] [xES1] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-05-23 19:35:15,185][DEBUG][discovery.zen.fd         ] [xES1] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-05-23 19:35:15,264][DEBUG][monitor.jvm              ] [xES1] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name='ParNew', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name='ConcurrentMarkSweep', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]
[2013-05-23 19:35:15,782][DEBUG][monitor.os               ] [xES1] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@4ec48e7] with refresh_interval [1s]
[2013-05-23 19:35:15,805][DEBUG][monitor.process          ] [xES1] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@26d6221b] with refresh_interval [1s]
[2013-05-23 19:35:15,825][DEBUG][monitor.jvm              ] [xES1] Using refresh_interval [1s]
[2013-05-23 19:35:15,827][DEBUG][monitor.network          ] [xES1] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@2d923a8f] with refresh_interval [5s]
[2013-05-23 19:35:16,019][DEBUG][monitor.network          ] [xES1] net_info
host [rnynjpxyhcfhxdm]
lo  display_name [Software Loopback Interface 1]
        address [/127.0.0.1] [/0:0:0:0:0:0:0:1] 
        mtu [-1] multicast [true] ptp [false] loopback [true] up [true] virtual [false]
net0    display_name [WAN Miniport (L2TP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net1    display_name [WAN Miniport (SSTP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net2    display_name [WAN Miniport (IKEv2)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net3    display_name [WAN Miniport (PPTP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
ppp0    display_name [WAN Miniport (PPPOE)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth0    display_name [WAN Miniport (IP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth1    display_name [WAN Miniport (IPv6)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth2    display_name [WAN Miniport (Network Monitor)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth3    display_name [Microsoft Kernel Debug Network Adapter]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
ppp1    display_name [RAS Async Adapter]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth4    display_name [Microsoft Hyper-V Network Adapter]
        address [/10.78.76.39] [/fe80:0:0:0:4ccd:6139:c38f:e027%12] 
        mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
eth5    display_name [WAN Miniport (IP)-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth6    display_name [WAN Miniport (IPv6)-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth7    display_name [WAN Miniport (Network Monitor)-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth8    display_name [Microsoft Hyper-V Network Adapter-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth9    display_name [Microsoft Hyper-V Network Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth10   display_name [Microsoft Hyper-V Network Adapter-WFP Native MAC Layer LightWeight Filter-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net4    display_name [Microsoft ISATAP Adapter]
        address [/fe80:0:0:0:0:5efe:a4e:4c27%19] 
        mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]

[2013-05-23 19:35:16,105][DEBUG][monitor.fs               ] [xES1] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@e1b054c] with refresh_interval [1s]
[2013-05-23 19:35:16,703][DEBUG][indices.store            ] [xES1] using indices.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2013-05-23 19:35:16,718][DEBUG][cache.memory             ] [xES1] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]
[2013-05-23 19:35:16,744][DEBUG][script                   ] [xES1] using script cache with max_size [500], expire [null]
[2013-05-23 19:35:16,836][DEBUG][cluster.routing.allocation.decider] [xES1] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2013-05-23 19:35:16,840][DEBUG][cluster.routing.allocation.decider] [xES1] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2013-05-23 19:35:16,841][DEBUG][cluster.routing.allocation.decider] [xES1] using [cluster_concurrent_rebalance] with [2]
[2013-05-23 19:35:16,846][DEBUG][gateway.local            ] [xES1] using initial_shards [quorum], list_timeout [30s]
[2013-05-23 19:35:17,100][DEBUG][indices.recovery         ] [xES1] using max_size_per_sec[0b], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2013-05-23 19:35:17,254][DEBUG][http.netty               ] [xES1] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb-&gt;512kb]
[2013-05-23 19:35:17,263][DEBUG][indices.memory           ] [xES1] using index_buffer_size [101.5mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2013-05-23 19:35:17,279][DEBUG][indices.cache.filter     ] [xES1] using [node] weighted filter cache with size [20%], actual_size [203.1mb], expire [null], clean_interval [1m]
[2013-05-23 19:35:17,282][DEBUG][indices.fielddata.cache  ] [xES1] using size [-1] [-1b], expire [null]
[2013-05-23 19:35:17,297][DEBUG][gateway.local.state.meta ] [xES1] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2013-05-23 19:35:17,358][DEBUG][gateway.local.state.meta ] [xES1] took 60ms to load state
[2013-05-23 19:35:17,359][DEBUG][gateway.local.state.shards] [xES1] took 0s to load started shards state
[2013-05-23 19:35:17,367][DEBUG][bulk.udp                 ] [xES1] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]
[2013-05-23 19:35:17,370][INFO ][node                     ] [xES1] {0.90.0}[3136]: initialized
[2013-05-23 19:35:17,371][INFO ][node                     ] [xES1] {0.90.0}[3136]: starting ...
[2013-05-23 19:35:17,484][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500
[2013-05-23 19:35:17,485][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false
[2013-05-23 19:35:17,588][DEBUG][transport.netty          ] [xES1] Bound to address [/0:0:0:0:0:0:0:0:9300]
[2013-05-23 19:35:17,659][INFO ][transport                ] [xES1] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.78.76.39:9300]}
[2013-05-23 19:35:27,756][DEBUG][discovery.zen            ] [xES1] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2013-05-23 19:35:27,767][DEBUG][cluster.service          ] [xES1] processing [zen-disco-join (elected_as_master)]: execute
[2013-05-23 19:35:27,769][DEBUG][cluster.service          ] [xES1] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]
[2013-05-23 19:35:27,772][INFO ][cluster.service          ] [xES1] new_master [xES1][QVFMXbh8Tt6A25sY8eZauA][inet[/10.78.76.39:9300]], reason: zen-disco-join (elected_as_master)
[2013-05-23 19:35:27,845][DEBUG][transport.netty          ] [xES1] connected to node [[xES1][QVFMXbh8Tt6A25sY8eZauA][inet[/10.78.76.39:9300]]]
[2013-05-23 19:35:27,851][DEBUG][cluster.service          ] [xES1] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state
[2013-05-23 19:35:27,852][INFO ][discovery                ] [xES1] elasticsearch/QVFMXbh8Tt6A25sY8eZauA
[2013-05-23 19:35:27,875][DEBUG][cluster.service          ] [xES1] processing [local-gateway-elected-state]: execute
[2013-05-23 19:35:27,897][DEBUG][cluster.service          ] [xES1] cluster state updated, version [2], source [local-gateway-elected-state]
[2013-05-23 19:35:27,982][INFO ][gateway                  ] [xES1] recovered [0] indices into cluster_state
[2013-05-23 19:35:27,984][DEBUG][cluster.service          ] [xES1] processing [local-gateway-elected-state]: done applying updated cluster_state
[2013-05-23 19:35:27,985][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: execute
[2013-05-23 19:35:27,985][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-05-23 19:35:27,987][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: execute
[2013-05-23 19:35:27,988][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-05-23 19:35:28,059][INFO ][http                     ] [xES1] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.78.76.39:9200]}
[2013-05-23 19:35:28,061][INFO ][node                     ] [xES1] {0.90.0}[3136]: started
[2013-05-23 19:35:37,849][DEBUG][cluster.service          ] [xES1] processing [routing-table-updater]: execute
[2013-05-23 19:35:37,851][DEBUG][cluster.service          ] [xES1] processing [routing-table-updater]: no change in cluster_state
[2013-05-23 19:40:51,172][TRACE][action.admin.cluster.health] [xES1] Calculating health based on state version [2]
```
## Node2 Config

``` yml
#################################### Node #####################################

# Node names are generated dynamically on startup, so you're relieved
# from configuring them manually. You can tie this node to a specific name:
#
 node.name: "xES2"

################################## Discovery ##################################

# Discovery infrastructure ensures nodes can be found within a cluster
# and master node is elected. Multicast discovery is the default.

# Set to ensure a node sees N other master eligible nodes to be considered
# operational within the cluster. Set this option to a higher value (2-4)
# for large clusters (&gt;3 nodes):
#
# discovery.zen.minimum_master_nodes: 1

# Set the time to wait for ping responses from other nodes when discovering.
# Set this option to a higher value on a slow or congested network
# to minimize discovery failures:
#
 discovery.zen.ping.timeout: 10s

# See &lt;http://elasticsearch.org/guide/reference/modules/discovery/zen.html&gt;
# for more information.

# Unicast discovery allows to explicitly control which nodes will be used
# to discover the cluster. It can be used when multicast is not present,
# or to restrict the cluster communication-wise.
#
#1. Disable multicast discovery (enabled by default):
#
 discovery.zen.ping.multicast.enabled: false
#
#2. Configure an initial list of master nodes in the cluster
#    to perform discovery when new nodes (master or data) are started:
#
# discovery.zen.ping.unicast.hosts: ["host1", "host2:port", "host3[portX-portY]"]
discovery.zen.ping.unicast.hosts:["rnynjpxyhcfhxdm","elasticsearch3"]
```
## Node 2 Logs

```
====================================================================
[2013-05-23 19:58:28,732][INFO ][node                     ] [xES2] {0.90.0}[3604]: initializing ...
[2013-05-23 19:58:28,734][DEBUG][node                     ] [xES2] using home [C:\ddapplications\elasticsearch-0.90.0], config [C:\ddapplications\elasticsearch-0.90.0\config], data [[C:\ddapplications\elasticsearch-0.90.0\data]], logs [C:\ddapplications\elasticsearch-0.90.0\logs], work [C:\ddapplications\elasticsearch-0.90.0\work], plugins [C:\ddapplications\elasticsearch-0.90.0\plugins]
[2013-05-23 19:58:28,751][INFO ][plugins                  ] [xES2] loaded [], sites [head]
[2013-05-23 19:58:28,821][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder
[2013-05-23 19:58:28,863][DEBUG][env                      ] [xES2] using node location [[C:\ddapplications\elasticsearch-0.90.0\data\elasticsearch\nodes\0]], local_node_id [0]
[2013-05-23 19:58:31,712][DEBUG][threadpool               ] [xES2] creating thread_pool [generic], type [cached], keep_alive [30s]
[2013-05-23 19:58:31,742][DEBUG][threadpool               ] [xES2] creating thread_pool [index], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:58:31,743][DEBUG][threadpool               ] [xES2] creating thread_pool [bulk], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:58:31,744][DEBUG][threadpool               ] [xES2] creating thread_pool [get], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:58:31,749][DEBUG][threadpool               ] [xES2] creating thread_pool [search], type [fixed], size [2], queue_size [1k], reject_policy [abort], queue_type [linked]
[2013-05-23 19:58:31,750][DEBUG][threadpool               ] [xES2] creating thread_pool [percolate], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]
[2013-05-23 19:58:31,751][DEBUG][threadpool               ] [xES2] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2013-05-23 19:58:31,753][DEBUG][threadpool               ] [xES2] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:58:31,754][DEBUG][threadpool               ] [xES2] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:58:31,755][DEBUG][threadpool               ] [xES2] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:58:31,756][DEBUG][threadpool               ] [xES2] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:58:31,757][DEBUG][threadpool               ] [xES2] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]
[2013-05-23 19:58:31,827][DEBUG][transport.netty          ] [xES2] using worker_count[2], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1], receive_predictor[512kb-&gt;512kb]
[2013-05-23 19:58:31,852][DEBUG][discovery.zen.ping.unicast] [xES2] using initial hosts [], with concurrent_connects [10]
[2013-05-23 19:58:31,855][DEBUG][discovery.zen            ] [xES2] using ping.timeout [10s], master_election.filter_client [true], master_election.filter_data [false]
[2013-05-23 19:58:31,857][DEBUG][discovery.zen.elect      ] [xES2] using minimum_master_nodes [-1]
[2013-05-23 19:58:31,860][DEBUG][discovery.zen.fd         ] [xES2] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-05-23 19:58:31,871][DEBUG][discovery.zen.fd         ] [xES2] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-05-23 19:58:31,962][DEBUG][monitor.jvm              ] [xES2] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name='ParNew', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name='ConcurrentMarkSweep', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]
[2013-05-23 19:58:32,483][DEBUG][monitor.os               ] [xES2] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@4ec48e7] with refresh_interval [1s]
[2013-05-23 19:58:32,495][DEBUG][monitor.process          ] [xES2] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@26d6221b] with refresh_interval [1s]
[2013-05-23 19:58:32,515][DEBUG][monitor.jvm              ] [xES2] Using refresh_interval [1s]
[2013-05-23 19:58:32,526][DEBUG][monitor.network          ] [xES2] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@2d923a8f] with refresh_interval [5s]
[2013-05-23 19:58:32,711][DEBUG][monitor.network          ] [xES2] net_info
host [elasticsearch3]
lo  display_name [Software Loopback Interface 1]
        address [/127.0.0.1] [/0:0:0:0:0:0:0:1] 
        mtu [-1] multicast [true] ptp [false] loopback [true] up [true] virtual [false]
net0    display_name [WAN Miniport (L2TP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net1    display_name [WAN Miniport (SSTP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net2    display_name [WAN Miniport (IKEv2)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net3    display_name [WAN Miniport (PPTP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
ppp0    display_name [WAN Miniport (PPPOE)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth0    display_name [WAN Miniport (IP)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth1    display_name [WAN Miniport (IPv6)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth2    display_name [WAN Miniport (Network Monitor)]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth3    display_name [Microsoft Kernel Debug Network Adapter]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
ppp1    display_name [RAS Async Adapter]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth4    display_name [Microsoft Hyper-V Network Adapter]
        address [/10.78.26.64] [/fe80:0:0:0:e024:1b29:9f61:eda%12] 
        mtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]
eth5    display_name [WAN Miniport (IP)-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth6    display_name [WAN Miniport (IPv6)-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth7    display_name [WAN Miniport (Network Monitor)-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth8    display_name [Microsoft Hyper-V Network Adapter-QoS Packet Scheduler-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth9    display_name [Microsoft Hyper-V Network Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
eth10   display_name [Microsoft Hyper-V Network Adapter-WFP Native MAC Layer LightWeight Filter-0000]
        address 
        mtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]
net4    display_name [Microsoft ISATAP Adapter]
        address [/fe80:0:0:0:0:5efe:a4e:1a40%19] 
        mtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]

[2013-05-23 19:58:32,786][DEBUG][monitor.fs               ] [xES2] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@e1b054c] with refresh_interval [1s]
[2013-05-23 19:58:33,377][DEBUG][indices.store            ] [xES2] using indices.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2013-05-23 19:58:33,391][DEBUG][cache.memory             ] [xES2] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]
[2013-05-23 19:58:33,418][DEBUG][script                   ] [xES2] using script cache with max_size [500], expire [null]
[2013-05-23 19:58:33,515][DEBUG][cluster.routing.allocation.decider] [xES2] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2013-05-23 19:58:33,517][DEBUG][cluster.routing.allocation.decider] [xES2] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2013-05-23 19:58:33,518][DEBUG][cluster.routing.allocation.decider] [xES2] using [cluster_concurrent_rebalance] with [2]
[2013-05-23 19:58:33,523][DEBUG][gateway.local            ] [xES2] using initial_shards [quorum], list_timeout [30s]
[2013-05-23 19:58:33,775][DEBUG][indices.recovery         ] [xES2] using max_size_per_sec[0b], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2013-05-23 19:58:33,931][DEBUG][http.netty               ] [xES2] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb-&gt;512kb]
[2013-05-23 19:58:33,941][DEBUG][indices.memory           ] [xES2] using index_buffer_size [101.5mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2013-05-23 19:58:33,954][DEBUG][indices.cache.filter     ] [xES2] using [node] weighted filter cache with size [20%], actual_size [203.1mb], expire [null], clean_interval [1m]
[2013-05-23 19:58:33,957][DEBUG][indices.fielddata.cache  ] [xES2] using size [-1] [-1b], expire [null]
[2013-05-23 19:58:33,973][DEBUG][gateway.local.state.meta ] [xES2] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2013-05-23 19:58:34,033][DEBUG][gateway.local.state.meta ] [xES2] took 59ms to load state
[2013-05-23 19:58:34,034][DEBUG][gateway.local.state.shards] [xES2] took 0s to load started shards state
[2013-05-23 19:58:34,049][DEBUG][bulk.udp                 ] [xES2] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]
[2013-05-23 19:58:34,051][INFO ][node                     ] [xES2] {0.90.0}[3604]: initialized
[2013-05-23 19:58:34,052][INFO ][node                     ] [xES2] {0.90.0}[3604]: starting ...
[2013-05-23 19:58:34,259][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500
[2013-05-23 19:58:34,260][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false
[2013-05-23 19:58:34,263][DEBUG][transport.netty          ] [xES2] Bound to address [/0:0:0:0:0:0:0:0:9300]
[2013-05-23 19:58:34,340][INFO ][transport                ] [xES2] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.78.26.64:9300]}
[2013-05-23 19:58:44,430][DEBUG][discovery.zen            ] [xES2] filtered ping responses: (filter_client[true], filter_data[false]) {none}
[2013-05-23 19:58:44,439][DEBUG][cluster.service          ] [xES2] processing [zen-disco-join (elected_as_master)]: execute
[2013-05-23 19:58:44,440][DEBUG][cluster.service          ] [xES2] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]
[2013-05-23 19:58:44,442][INFO ][cluster.service          ] [xES2] new_master [xES2][Nss1G-bwT0aU3UYeF-A0Lg][inet[/10.78.26.64:9300]], reason: zen-disco-join (elected_as_master)
[2013-05-23 19:58:44,592][DEBUG][transport.netty          ] [xES2] connected to node [[xES2][Nss1G-bwT0aU3UYeF-A0Lg][inet[/10.78.26.64:9300]]]
[2013-05-23 19:58:44,599][DEBUG][cluster.service          ] [xES2] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state
[2013-05-23 19:58:44,600][INFO ][discovery                ] [xES2] elasticsearch/Nss1G-bwT0aU3UYeF-A0Lg
[2013-05-23 19:58:44,629][DEBUG][cluster.service          ] [xES2] processing [local-gateway-elected-state]: execute
[2013-05-23 19:58:44,640][DEBUG][cluster.service          ] [xES2] cluster state updated, version [2], source [local-gateway-elected-state]
[2013-05-23 19:58:44,722][INFO ][gateway                  ] [xES2] recovered [0] indices into cluster_state
[2013-05-23 19:58:44,723][DEBUG][cluster.service          ] [xES2] processing [local-gateway-elected-state]: done applying updated cluster_state
[2013-05-23 19:58:44,727][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: execute
[2013-05-23 19:58:44,728][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-05-23 19:58:44,729][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: execute
[2013-05-23 19:58:44,730][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: no change in cluster_state
[2013-05-23 19:58:44,796][INFO ][http                     ] [xES2] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.78.26.64:9200]}
[2013-05-23 19:58:44,797][INFO ][node                     ] [xES2] {0.90.0}[3604]: started
```
</description><key id="14692647">3082</key><summary>Unicast Cluster on Azure Virtual Machines not working (0.90)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dynamicdeploy</reporter><labels /><created>2013-05-23T20:00:37Z</created><updated>2013-09-09T14:47:33Z</updated><resolved>2013-05-28T07:17:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="renaudboutet" created="2013-05-24T10:56:23Z" id="18398489">Hi,

Are you deploying your VMs in the same virtual network?
I am a user of Azure and ES (obviously :)) and by experience if you use DNS names only you have to go through the Azure's load balancer which can intercept and cut your connections.

This is working for us as all our machines are deployed under virtual networks.
</comment><comment author="dynamicdeploy" created="2013-05-24T14:11:36Z" id="18407091">Hello! Renaud,

I am deploying both the VMs in the same cloud service and therefore they should be able to talk to each other. The cloud service creates a network boundary (similar to virtual network). When I deploy, I am able to get to the http://[name of the other machine]:9200 directly without going through the load-balancer. Somehow, from the debug logs, it feels like its not even trying to ping the machines listed in the Config file.

If you are interested, I can setup a quick environment for you and you can try it out.

Thanks for your response.

Tejaswi
</comment><comment author="dynamicdeploy" created="2013-05-24T14:13:13Z" id="18407190">I also tried using ipaddress, but with the same result. 
</comment><comment author="dynamicdeploy" created="2013-05-24T16:19:56Z" id="18414996">If I hit 9300 in the browser, I get the following error on the other machine. That means, the connection can reach. Then why is it not detecting during startup?

[2013-05-24 16:15:34,176][WARN ][transport.netty          ] [Celestial Madonna] exception caught on transport layer [[id: 0xa028c0e0, /10.78.26.64:50376 :&gt; /10.78.76.39:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format
 at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)
 at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
 at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
 at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
 at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
 at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
 at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
 at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:336)
 at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
 at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:574)
 at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
 at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
 at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:505)
 at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:227)
 at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
 at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
 at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
 at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
 at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
 at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:658)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:566)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)
 at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)
 at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)
 at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
 at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
 at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
 at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
 at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
 at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
 at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
 at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
 at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
 at java.lang.Thread.run(Thread.java:722)
</comment><comment author="dadoonet" created="2013-05-27T07:58:29Z" id="18486868">Could you try to add a space just before:

``` yml
 discovery.zen.ping.unicast.hosts:["elasticsearch3","rnynjpxyhcfhxdm"]
```

This log entry should not be empty AFAIK:

```
[2013-05-23 19:58:31,852][DEBUG][discovery.zen.ping.unicast] [xES2] using initial hosts [], with concurrent_connects [10]
```

BTW, we are currently building an azure discovery plugin that will ease your elasticsearch azure discovery setup.
</comment><comment author="dynamicdeploy" created="2013-05-27T20:46:56Z" id="18514196">Thanks David. That worked. I would love to test the Azure Discovery plugin.
</comment><comment author="mikelazell" created="2013-09-09T14:47:33Z" id="24082098">I had exactly the same issue. A missing space.. Thanks guys
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Limited facet search OutOfMemoryError[Java heap space]; ( core dump link attached )</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3081</link><project id="" key="" /><description>Loaded 300mil data points to do some statistical analysis.  ES java process is only using 1.05 GB on a 12GB server with 6 GB free memory.
The search is available below.  Other searches work fine even after the error.

Core dump available here
https://ten10.box.com/s/hynkizn4qtfiyryh434a

  "error" : "SearchPhaseExecutionException[Failed to execute phase [dfs], total failure; shardFailures {[QwoFm6gqSi6TEFjefRkKaw][touches][6]: SearchParseException[[touches][6]: query[ConstantScore(_:_)],from[0],size[100]: Parse Failure [Failed to parse source [{ 
"from":0,
"size": 100,
   "query" : {
        "match_all" : {  }
    },
    "facets" : {
        "sp_user_distinct" : {
            "terms" : {
                "field" : "sp_user",
                "all_terms" : true,
                                "size": 100
            }
        }
    }

}]]]; nested: ElasticSearchException[java.lang.OutOfMemoryError: Java heap space]; nested: ExecutionError[java.lang.OutOfMemoryError: Java heap space]; nested: OutOfMemoryError[Java heap space];
</description><key id="14683318">3081</key><summary>Limited facet search OutOfMemoryError[Java heap space]; ( core dump link attached )</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">matte</reporter><labels><label>non-issue</label></labels><created>2013-05-23T16:29:19Z</created><updated>2013-05-29T19:59:45Z</updated><resolved>2013-05-29T19:59:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-23T19:09:01Z" id="18365085">which version are you using?
</comment><comment author="matte" created="2013-05-29T06:41:20Z" id="18598637">ElasticSearch 0.90
 java -version
java version "1.6.0_43"
Java(TM) SE Runtime Environment (build 1.6.0_43-b01-447-11M4203)
Java HotSpot(TM) 64-Bit Server VM (build 20.14-b01-447, mixed mode)
</comment><comment author="s1monw" created="2013-05-29T10:09:21Z" id="18606969">you are running with`"all_terms" = true` I don't think this is going to work though. do you really need all terms here? Can you afford to not load low frequent terms like users that only occur once?
</comment><comment author="matte" created="2013-05-29T18:54:02Z" id="18638113">Yes  I am running with all_term and have 16gb of ram on a 16gb maching. Elastic search's  java is only taking 1gb and I get a heap exception?  I only have 5mil unique ints for the id which is in the order of megabytes in size.

Also this simple query gives a heap exception
{
          "from": 0,
          "size": 1000,
         "query" : {
              "match" : { "sp_user": 21884751 }
          },
          "sort" : [
                  { "at" : {"order" : "asc" } }
          ]

  }

if I remove the sort get 14 results back.

any ideas?

[2013-05-29 11:51:16,903][DEBUG][action.search.type       ] [ap_master_a] [touches][0], node[ekcir6kyRJOeC3K6AoQ9uA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@63630762]
org.elasticsearch.search.query.QueryPhaseExecutionException: [touches][0]: query[filtered(sp_user:[21884751 TO 21884751])-&gt;cache(_type:touch)],from[0],size[1000],sort[&lt;custom:"at": org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorSource@5d29be53&gt;]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:680)
Caused by: org.elasticsearch.ElasticSearchException: java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.index.fielddata.plain.LongArrayIndexFieldData.load(LongArrayIndexFieldData.java:81)
    at org.elasticsearch.index.fielddata.fieldcomparator.LongValuesComparatorBase.setNextReader(LongValuesComparatorBase.java:68)
    at org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:602)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:161)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:572)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:501)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)
    ... 9 more
Caused by: org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space
    at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2261)
    at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:4000)
    at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)
    at org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:144)
    at org.elasticsearch.index.fielddata.plain.LongArrayIndexFieldData.load(LongArrayIndexFieldData.java:76)
    ... 18 more
Caused by: java.lang.OutOfMemoryError: Java heap space
</comment><comment author="s1monw" created="2013-05-29T18:58:19Z" id="18638429">do you configure the heap for ES? how much heap are you giving it via ES_HEAP_SIZE ?
</comment><comment author="matte" created="2013-05-29T19:56:51Z" id="18641910">Thanks for the quick response!  I'm at idiot and did not set it for the test server!

Sorry to waste your time.

Keep being awesome!
</comment><comment author="s1monw" created="2013-05-29T19:59:27Z" id="18642043">don't worry! thanks for bringing closure to this. It's important for us to get this feedback and things like this make us think about the defaults and keeps on improving the system. Just hypothetically we could issue a warning to prevent these kind of errors in the future. just another example of how tricky this stuff still is!

simon
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>template index name prefix bug</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3080</link><project id="" key="" /><description>As posted in this thread:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/rlONWRp_iXU

The following template:

```
{
  "template": "logs*",
  "settings": {
    "query.default_field": "@message",
    "action.auto_create_index": "+logs*",
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "routing.allocation.total_shards_per_node": 1,
    "auto_expand_replicas": false,
    "index": {
      "analysis": {
        "analyzer": {
          "my_log_analyser": {
            "tokenizer": "whitespace",
            "filter": [
              "standard",
              "lowercase"
            ],
            "type": "custom"
          }
        }
      }
    }
  },
  "mappings": {
    "_default_": {
      "_all": {
        "enabled": false
      },
      "_source": {
        "compress": false
      },
      "dynamic_templates": [
        {
          "fields_template": {
            "mapping": {
              "type": "string",
              "index": "not_analyzed"
            },
            "path_match": "@fields.*"
          }
        }
      ],
      "properties": {
        "@time": {
          "type": "date",
          "index": "not_analyzed"
        },
        "@message": {
          "type": "string",
          "index": "analyzed",
          "analyzer": "my_log_analyser"
        },
        "@fields": {
          "type": "object",
          "dynamic": true,
          "path": "full"
        }
      }
    }
  }
}
```

Will cause queries issued with lowercase words to produce no hits whatsoever.
It does seem that 
- logs*
- logs-*
  is maybe a reserved word and hence the lowercase filter is not applied for some reason.

Changing the prefix name to ANY other value will produce the desired results.
</description><key id="14673611">3080</key><summary>template index name prefix bug</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">achselschweisz</reporter><labels /><created>2013-05-23T13:22:14Z</created><updated>2014-03-17T11:26:02Z</updated><resolved>2014-03-17T11:26:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-23T09:47:33Z" id="21403604">Hey,

can you help me to reproduce this correctly? I could not with 0.90.2. This is what I did:

```
curl -X DELETE localhost:9200/_template/tmpl1
curl -X PUT localhost:9200/_template/tmpl1 -d '{
  "template": "logs*",
  "settings": {
    "query.default_field": "@message",
    "action.auto_create_index": "+logs*",
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "routing.allocation.total_shards_per_node": 1,
    "auto_expand_replicas": false,
    "index": {
      "analysis": {
        "analyzer": {
          "my_log_analyser": {
            "tokenizer": "whitespace",
            "filter": [
              "standard",
              "lowercase"
            ],
            "type": "custom"
          }
        }
      }
    }
  },
  "mappings": {
    "_default_": {
      "_all": {
        "enabled": false
      },
      "_source": {
        "compress": false
      },
      "dynamic_templates": [
        {
          "fields_template": {
            "mapping": {
              "type": "string",
              "index": "not_analyzed"
            },
            "path_match": "@fields.*"
          }
        }
      ],
      "properties": {
        "@time": {
          "type": "date",
          "index": "not_analyzed"
        },
        "@message": {
          "type": "string",
          "index": "analyzed",
          "analyzer": "my_log_analyser"
        },
        "@fields": {
          "type": "object",
          "dynamic": true,
          "path": "full"
        }
      }
    }
  }
}'


curl -X PUT 'localhost:9200/logs-foo/bar/1?refresh=true' -d '{
    "@time" : "2013-01-01",
    "@message":"This is such a wonderful test",
    "@fields" : { "a":"this is not sparta, but a", "b":"oh, look, that is b"} 
}'

curl -X GET 'localhost:9200/logs-foo/_mapping'

curl -X GET 'localhost:9200/logs-foo/bar/_search' -d '{ "query" : { "match" : { "@message" : "wonderful" } } }'
curl -X GET 'localhost:9200/logs-foo/bar/_search?q=wonderful'
```

Both queries at the end are returning the document, so I seem to have missed something. Would be glad if you could point me to the missing piece!

Thanks!
</comment><comment author="achselschweisz" created="2013-08-20T13:38:54Z" id="22945215">Hi,
sorry for getting back to you so late (I was on an extended holiday). You missed something indeed. Actually the lowercase filter does not seem to work when using a template for +logs*.

Using the EXACT SAME template that you posted above, I inserted another message like so:

curl -X PUT 'localhost:9200/logs-20130820/bar/2?refresh=true' -d '{
    "@time" : "2013-01-01",
    "@message":"This is such a Rude Message my Dear Donny",
    "@fields" : { "a":"this is not sparta, but a", "b":"oh, look, that is b"} 
}'
{"ok":true,"_index":"logs-20130820","_type":"bar","_id":"2","_version":1}

Then:

curl -X GET 'localhost:9200/logs-20130820/bar/_search?q=rude'
{"took":5,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}

The lowercase filter should have been applied, apparently this is not the case.

When I do this:

curl -X GET 'localhost:9200/logs-20130820/bar/_search?q=Rude'
{"took":4,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":0.3125,"hits":[{"_index":"logs-20130820","_type":"bar","_id":"2","_score":0.3125, "_source" : {
    "@time" : "2013-01-01",
    "@message":"This is such a Rude Message my Dear Donny",
    "@fields" : { "a":"this is not sparta, but a", "b":"oh, look, that is b"} 
}}]}}

... we do get an answer.

Note however, that I am using Elasticsearch 0.90.0. Tomorrow I will try the same thing using the latest version (0.90.3).

Note also, that changing the template name for the created indexes to "+lgs*" produces the desired results.

Thanks for your support,
- Chris
</comment><comment author="spinscale" created="2013-10-15T16:06:59Z" id="26348131">hey,

did you have any success reproducing this with 0.90.5? I did not so far... search works as expected
</comment><comment author="spinscale" created="2013-12-02T09:13:43Z" id="29603266">@achselschweisz any chance for you to test this with a recent 0.90 release in your environment?
</comment><comment author="achselschweisz" created="2013-12-06T14:32:29Z" id="29998101">Hi spin,

I just tested this with the current stable 0.90.7 and the issue seems to have gone.
Also deployed the new version on our test cluster and everythings seems fine (and better)

thanks,
 -chris
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>span_near query should accept slop = -1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3079</link><project id="" key="" /><description>Currently, specifying slop = -1 in a span_near query ends up with an error:
`(QueryParsingException[[index] span_near must include [slop]]; }]`.

Looking at /src/main/java/org/elasticsearch/index/query/SpanNearQueryParser.java, -1 is the value meaning "no slop".

This is preventing from using the "hack" documented here: 
https://lucene.apache.org/core/4_3_0/core/org/apache/lucene/search/spans/FieldMaskingSpanQuery.html
</description><key id="14667656">3079</key><summary>span_near query should accept slop = -1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-05-23T10:35:51Z</created><updated>2013-09-12T11:04:39Z</updated><resolved>2013-05-31T07:57:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="iksnalybok" created="2013-05-30T20:03:39Z" id="18704518">pull request #3122
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>String sorting incorrect after reindex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3078</link><project id="" key="" /><description>After reindexing a doc, it is not being returned in the correct sort order (when sorting on a string field)

First, index docs 1..100 with a string field `user`:

```
curl -XPOST 'http://127.0.0.1:9200/test/test/_bulk?pretty=1'  -d '
{"index" : {"_id" : "1"}}
{"user" : "1"}
{"index" : {"_id" : "2"}}
{"user" : "2"}
{"index" : {"_id" : "3"}}
{"user" : "3"}
{"index" : {"_id" : "4"}}
{"user" : "4"}
{"index" : {"_id" : "5"}}
{"user" : "5"}
{"index" : {"_id" : "6"}}
{"user" : "6"}
{"index" : {"_id" : "7"}}
{"user" : "7"}
{"index" : {"_id" : "8"}}
{"user" : "8"}
{"index" : {"_id" : "9"}}
{"user" : "9"}
{"index" : {"_id" : "10"}}
{"user" : "10"}
{"index" : {"_id" : "11"}}
{"user" : "11"}
{"index" : {"_id" : "12"}}
{"user" : "12"}
{"index" : {"_id" : "13"}}
{"user" : "13"}
{"index" : {"_id" : "14"}}
{"user" : "14"}
{"index" : {"_id" : "15"}}
{"user" : "15"}
{"index" : {"_id" : "16"}}
{"user" : "16"}
{"index" : {"_id" : "17"}}
{"user" : "17"}
{"index" : {"_id" : "18"}}
{"user" : "18"}
{"index" : {"_id" : "19"}}
{"user" : "19"}
{"index" : {"_id" : "20"}}
{"user" : "20"}
{"index" : {"_id" : "21"}}
{"user" : "21"}
{"index" : {"_id" : "22"}}
{"user" : "22"}
{"index" : {"_id" : "23"}}
{"user" : "23"}
{"index" : {"_id" : "24"}}
{"user" : "24"}
{"index" : {"_id" : "25"}}
{"user" : "25"}
{"index" : {"_id" : "26"}}
{"user" : "26"}
{"index" : {"_id" : "27"}}
{"user" : "27"}
{"index" : {"_id" : "28"}}
{"user" : "28"}
{"index" : {"_id" : "29"}}
{"user" : "29"}
{"index" : {"_id" : "30"}}
{"user" : "30"}
{"index" : {"_id" : "31"}}
{"user" : "31"}
{"index" : {"_id" : "32"}}
{"user" : "32"}
{"index" : {"_id" : "33"}}
{"user" : "33"}
{"index" : {"_id" : "34"}}
{"user" : "34"}
{"index" : {"_id" : "35"}}
{"user" : "35"}
{"index" : {"_id" : "36"}}
{"user" : "36"}
{"index" : {"_id" : "37"}}
{"user" : "37"}
{"index" : {"_id" : "38"}}
{"user" : "38"}
{"index" : {"_id" : "39"}}
{"user" : "39"}
{"index" : {"_id" : "40"}}
{"user" : "40"}
{"index" : {"_id" : "41"}}
{"user" : "41"}
{"index" : {"_id" : "42"}}
{"user" : "42"}
{"index" : {"_id" : "43"}}
{"user" : "43"}
{"index" : {"_id" : "44"}}
{"user" : "44"}
{"index" : {"_id" : "45"}}
{"user" : "45"}
{"index" : {"_id" : "46"}}
{"user" : "46"}
{"index" : {"_id" : "47"}}
{"user" : "47"}
{"index" : {"_id" : "48"}}
{"user" : "48"}
{"index" : {"_id" : "49"}}
{"user" : "49"}
{"index" : {"_id" : "50"}}
{"user" : "50"}
{"index" : {"_id" : "51"}}
{"user" : "51"}
{"index" : {"_id" : "52"}}
{"user" : "52"}
{"index" : {"_id" : "53"}}
{"user" : "53"}
{"index" : {"_id" : "54"}}
{"user" : "54"}
{"index" : {"_id" : "55"}}
{"user" : "55"}
{"index" : {"_id" : "56"}}
{"user" : "56"}
{"index" : {"_id" : "57"}}
{"user" : "57"}
{"index" : {"_id" : "58"}}
{"user" : "58"}
{"index" : {"_id" : "59"}}
{"user" : "59"}
{"index" : {"_id" : "60"}}
{"user" : "60"}
{"index" : {"_id" : "61"}}
{"user" : "61"}
{"index" : {"_id" : "62"}}
{"user" : "62"}
{"index" : {"_id" : "63"}}
{"user" : "63"}
{"index" : {"_id" : "64"}}
{"user" : "64"}
{"index" : {"_id" : "65"}}
{"user" : "65"}
{"index" : {"_id" : "66"}}
{"user" : "66"}
{"index" : {"_id" : "67"}}
{"user" : "67"}
{"index" : {"_id" : "68"}}
{"user" : "68"}
{"index" : {"_id" : "69"}}
{"user" : "69"}
{"index" : {"_id" : "70"}}
{"user" : "70"}
{"index" : {"_id" : "71"}}
{"user" : "71"}
{"index" : {"_id" : "72"}}
{"user" : "72"}
{"index" : {"_id" : "73"}}
{"user" : "73"}
{"index" : {"_id" : "74"}}
{"user" : "74"}
{"index" : {"_id" : "75"}}
{"user" : "75"}
{"index" : {"_id" : "76"}}
{"user" : "76"}
{"index" : {"_id" : "77"}}
{"user" : "77"}
{"index" : {"_id" : "78"}}
{"user" : "78"}
{"index" : {"_id" : "79"}}
{"user" : "79"}
{"index" : {"_id" : "80"}}
{"user" : "80"}
{"index" : {"_id" : "81"}}
{"user" : "81"}
{"index" : {"_id" : "82"}}
{"user" : "82"}
{"index" : {"_id" : "83"}}
{"user" : "83"}
{"index" : {"_id" : "84"}}
{"user" : "84"}
{"index" : {"_id" : "85"}}
{"user" : "85"}
{"index" : {"_id" : "86"}}
{"user" : "86"}
{"index" : {"_id" : "87"}}
{"user" : "87"}
{"index" : {"_id" : "88"}}
{"user" : "88"}
{"index" : {"_id" : "89"}}
{"user" : "89"}
{"index" : {"_id" : "90"}}
{"user" : "90"}
{"index" : {"_id" : "91"}}
{"user" : "91"}
{"index" : {"_id" : "92"}}
{"user" : "92"}
{"index" : {"_id" : "93"}}
{"user" : "93"}
{"index" : {"_id" : "94"}}
{"user" : "94"}
{"index" : {"_id" : "95"}}
{"user" : "95"}
{"index" : {"_id" : "96"}}
{"user" : "96"}
{"index" : {"_id" : "97"}}
{"user" : "97"}
{"index" : {"_id" : "98"}}
{"user" : "98"}
{"index" : {"_id" : "99"}}
{"user" : "99"}
{"index" : {"_id" : "100"}}
{"user" : "100"}
'
```

Search, sorting on `user`:

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "sort" : {
      "user" : "asc"
   },
   "fields" : [],
   "size" : 10
}
'
```

Results show that `user:1` is in first position:

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "sort" : [
#                "1"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "10"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "10",
#             "_type" : "test"
#          },
# ....
```

Now reindex the first doc, with the same values:

```
curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1'  -d '
{
   "user" : "1"
}
'
```

And search again:

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "sort" : {
      "user" : "asc"
   },
   "fields" : [],
   "size" : 10
}
'
```

Doc with `user:1` no longer appears in the correct position, in fact it doesn't appear anywhere in the first 10 results:

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "sort" : [
#                "10"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "10",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "100"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "100",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "11"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "11",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "12"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "12",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "13"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "13",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "14"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "14",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "15"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "15",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "16"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "16",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "17"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "17",
#             "_type" : "test"
#          },
#          {
#             "sort" : [
#                "18"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "18",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : null,
#       "total" : 100
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```

However, if you return all 100 docs, then it appears in the first position again (correctly):

```
curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "sort" : {
      "user" : "asc"
   },
   "fields" : [],
   "size" : 100
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "sort" : [
#                "1"
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "test"
#          },
```

which leads me to think that it is the shard level sorting which is incorrect.
</description><key id="14665837">3078</key><summary>String sorting incorrect after reindex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-23T09:43:59Z</created><updated>2013-07-17T20:11:14Z</updated><resolved>2013-05-24T06:38:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sarmiena" created="2013-05-23T17:39:40Z" id="18359401">Thanks for the ticket, @clintongormley . Sadly this bug is causing many people to yell at me :( They update a record and it's removed from their UI.
</comment><comment author="s1monw" created="2013-05-23T18:52:12Z" id="18364063">what version does this reproduce on? does this still happen on master?
</comment><comment author="s1monw" created="2013-05-23T18:59:04Z" id="18364487">@martijnvg this is fixed it seems. this caused by #2991 and fixed in master and 0.90
</comment><comment author="sarmiena" created="2013-05-23T19:45:41Z" id="18367186">@s1monw this issue is definitely happening on 0.90.0 release. I also just pulled down the repo and ran it against  v 1.0.0 beta1. The issue still exists there as well.

Please verify and reopen. 

@clintongormley can you confirm?
</comment><comment author="clintongormley" created="2013-05-23T19:47:31Z" id="18367305">@sarmiena For me it is broken in 0.90.0, but fixed in master and in the 0.90.1 branch.

Unless you have a different test to show otherwise?
</comment><comment author="sarmiena" created="2013-05-23T19:54:54Z" id="18367693">@clintongormley I'm building from master using:

```
mvn clean package -DskipTests
```

However, this is building elasticsearch-1.0.0.Beta1-SNAPSHOT.

I'm not sure how to build 0.90.1 since there is no tag or branch in the repo that I can see.

Let me know if you want me to show you (live) how to reproduce it using 1.0.0.Beta1
</comment><comment author="sarmiena" created="2013-05-23T20:02:24Z" id="18368117">@clintongormley ok I just ran the same scenario on 0.90.1 branch and it's definitely still happening. Not sure why yours isn't showing the same issue.
</comment><comment author="sarmiena" created="2013-05-23T20:13:14Z" id="18368671">@clintongormley Sorry to keep bothering :) However I have good news and bad news:

Good news: Your test case does work in 0.90.1
Bad news: An alternative test case produces same problem

You used bulk upload, while I simply added 1 record at a time (100 times).

https://gist.github.com/sarmiena/d945848fd683f39d212c

I used Ruby to iterate 100 POST requests in that gist, but you can use whatever you'd like.

The issue doesn't appear to be resolved. Can we reopen the ticket?
</comment><comment author="clintongormley" created="2013-05-23T20:18:32Z" id="18368960">@s1monw ?
</comment><comment author="s1monw" created="2013-05-23T21:21:33Z" id="18372516">i added a testcase that mirrors your ruby test in java and it doesn't fail. I can't reproduce your problem I am sorry. Are you sure you build 0.90.1?
</comment><comment author="sarmiena" created="2013-05-23T21:24:52Z" id="18372722">@s1monw I'm sure I can reproduce this in 0.90.1. Perhaps the test isn't producing the same problems since the JSON api is being used and the test is using the interfaces directly?

I can to a teamviewer if you'd like. Otherwise you can just pop open irb and copy/paste the ruby code in there. 

gchat me sarmiena@gmail.com if you want to get ahold of me. otherwise i'm on IRC in #elasticsearch as sarmiena_ (notice the underscore)
</comment><comment author="sarmiena" created="2013-05-23T22:41:44Z" id="18376453">Ok looks like 0.90.1 does fix this issue. The formatting was a little off and I missed the record:

Please close.

```
# ruby code (irb)
(1).upto(100) do |i|
    `curl -XPUT 'http://localhost:9200/twitter/tweet/#{i}' -d '{ "user" : "#{i}"}'`
end

# command line
$ curl -X POST "http://localhost:9200/twitter/tweet/1" -d '
&gt; {"user":"1"}
&gt; '

$ curl -X GET 'http://localhost:9200/twitter/tweet/_search?pretty' -d '
{
  "sort": [
    {
      "user": "asc"
    }
  ],
  "size": 10,
  "from": 0
}
'
{
  "took" : 8,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 100,
    "max_score" : null,
    "hits" : [ {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "1",
      "_score" : null, "_source" :
{"user":"1"}
,
      "sort" : [ "1" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "10",
      "_score" : null, "_source" : { "user" : "10"},
      "sort" : [ "10" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "100",
      "_score" : null, "_source" : { "user" : "100"},
      "sort" : [ "100" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "11",
      "_score" : null, "_source" : { "user" : "11"},
      "sort" : [ "11" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "12",
      "_score" : null, "_source" : { "user" : "12"},
      "sort" : [ "12" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "13",
      "_score" : null, "_source" : { "user" : "13"},
      "sort" : [ "13" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "14",
      "_score" : null, "_source" : { "user" : "14"},
      "sort" : [ "14" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "15",
      "_score" : null, "_source" : { "user" : "15"},
      "sort" : [ "15" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "16",
      "_score" : null, "_source" : { "user" : "16"},
      "sort" : [ "16" ]
    }, {
      "_index" : "twitter",
      "_type" : "tweet",
      "_id" : "17",
      "_score" : null, "_source" : { "user" : "17"},
      "sort" : [ "17" ]
    } ]
  }
}
```
</comment><comment author="s1monw" created="2013-05-24T06:38:09Z" id="18388883">thanks for bringing clarification! good to work with you last night!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aliases: Add delete index alias api for deleting a single alias.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3077</link><project id="" key="" /><description>Add delete index alias api for deleting a single alias.

Options:
- `index` - The index the alias is in, the needs to be deleted. This is a required option.
- `alias` - The name of the alias to delete. This is a required option.

The rest endpoint is: /{index}/_alias/{alias}

Example:
curl -XDELETE 'localhost:9200/users/_alias/user_12'
</description><key id="14660789">3077</key><summary>Aliases: Add delete index alias api for deleting a single alias.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-23T07:09:13Z</created><updated>2013-05-30T09:01:11Z</updated><resolved>2013-05-23T07:19:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aliases: Add endpoint to add one specific index alias</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3076</link><project id="" key="" /><description>Add put index alias api for adding a single index alias.

Options:
- `index` - The index to alias refers to. This is a required option.
- `alias` - The name of the alias. This is a required option.
- `routing` - An optional routing that can be associated with an alias.
- `filter` - An optional filter that can be associated with an alias. 

The rest endpoint is: `/{index}/_alias/{alias}`

Examples:
Adding time based alias:

```
curl -XPUT 'localhost:9200/logs_201305/_alias/2013'
```

Adding user alias:

```
curl -XPUT 'localhost:9200/users/_alias/user_12' -d '{
    "routing" : "12",
    "filter" : {
        "term" : {
            "user_id" : 12
        }
    }   
}'
```
</description><key id="14660765">3076</key><summary>Aliases: Add endpoint to add one specific index alias</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-23T07:08:25Z</created><updated>2013-05-30T09:01:19Z</updated><resolved>2013-05-23T07:19:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Aliases: Add get index alias api that allows get specific aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3075</link><project id="" key="" /><description>Add get index alias api that allows to filter by alias name and index name. This api redirects to the master and fetches the requested index aliases, if available. This api only serialises the found index aliases.

Possible options:
- `index` - The index name to get aliases for. Partially names are supported via wildcards, also multiple index names can be specified separated with a comma. Also the alias name for an index can be used.
- `alias` - The name of alias to return in the response. Like the index option, this option supports wildcards and the option the specify multiple alias names separated by a comma. This is a required option.
- `ignore_indices` - What to do is an specified index name doesn't exist. If set to `missing` then those indices are ignored.

The rest endpoint is: `/{index}/_alias/{alias}`

Examples:
All aliases for the index users:

```
curl -XGET 'localhost:9200/users/_alias/*
 {
  "users" : {
    "aliases" : {
      "user_13" : {
        "filter" : {
          "term" : {
            "user_id" : 13
          }
        },
        "index_routing" : "13",
        "search_routing" : "13"
      },
      "user_14" : {
        "filter" : {
          "term" : {
            "user_id" : 14
          }
        },
        "index_routing" : "14",
        "search_routing" : "14"
      },
      "user_12" : {
        "filter" : {
          "term" : {
            "user_id" : 12
          }
        },
        "index_routing" : "12",
        "search_routing" : "12"
      }
    }
  }
}
```

All aliases with the name 2013 in any index:

```
curl -XGET 'localhost:9200/_alias/2013
{
  "logs_201304" : {
    "aliases" : {
      "2013" : { }
    }
  },
  "logs_201305" : {
    "aliases" : {
      "2013" : { }
    }
  }
}
```

All aliases that start with 2013_01 in any index

```
curl -XGET 'localhost:9200/_alias/2013_01*
{
  "logs_20130101" : {
    "aliases" : {
      "2013_01" : { }
    }
  }
}
```
</description><key id="14660723">3075</key><summary>Aliases: Add get index alias api that allows get specific aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-23T07:06:22Z</created><updated>2013-05-30T10:17:58Z</updated><resolved>2013-05-23T07:19:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Common terms query parameters inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3074</link><project id="" key="" /><description>The `common` terms query uses `disable_coords` instead of `disable_coord`, which the `bool` query uses.  Also, no support for camelCased parameters.
</description><key id="14626853">3074</key><summary>Query DSL: Common terms query parameters inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-22T14:50:48Z</created><updated>2013-05-22T17:02:40Z</updated><resolved>2013-05-22T14:56:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-22T15:37:31Z" id="18286967">@clintongormley can you set the labels for this and maybe backport it to 0.90?
</comment><comment author="clintongormley" created="2013-05-22T17:02:40Z" id="18292601">@s1monw done, ta
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regresion: geo distance filter - filters out proper geohashes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3073</link><project id="" key="" /><description>Mapping of searched field:

```
location: {
   type: geo_point
}
```

I tried also with:

```
location: {
   lat_lon: true,
   type: geo_point,
   geohash: true,
   geohash_precision: 24
}
```

Query:

```
{
  "from": 0,
  "fields": [
    "_id",
    "_parent",
    "_routing",
    "_source"
  ],
  "filter": {
        "geo_distance": {
          "distance": "50mi",
          "optimize_bbox": "memory",
          "location": {
            "lat": 40.720611,
            "lon": -73.998776
          }
        }
  },
  "query": {
    "match_all": {}
  },
  "size": 20
}
```

Data:

```
{
 "location": "dr5rshgwz81eqnfrrrhz"
}
```

It works for 0.20.2 and don't work for current release 0.90.0

EDIT: Sorry I post it to fast, data attached :)
</description><key id="14621628">3073</key><summary>Regresion: geo distance filter - filters out proper geohashes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kosz85</reporter><labels><label>bug</label><label>regression</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-05-22T13:02:28Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-06-10T07:22:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-05-22T13:06:25Z" id="18276626">Hi @kosz85, can you also post a simple example of your indexed data, filters and results?
</comment><comment author="kosz85" created="2013-05-22T13:15:39Z" id="18277111">I may add that 0.90.0 has no matches even for 5000mi

```
{
  "filter": {
        "geo_distance": {
          "distance": "50mi",
          "optimize_bbox": "memory",
          "location": {
            "lat": 40.720611,
            "lon": -73.998776
          }
        }
  },
  "query": {
    "match_all": {}
  },
  "size": 20
}
```

Almost the same data in both instances

0.90.0

```
{
    "took": 2,
    "timed_out": false,
    "_shards": {
        "total": 3,
        "successful": 3,
        "failed": 0
    },
    "hits": {
        "total": 0,
        "max_score": null,
        "hits": []
    }
}
```

0.20.2

```
{
    "took": 3,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "failed": 0
    },
    "hits": {
        "total": 220,
        "max_score": 1,
        "hits": [

(...)
              {
                "_id": "105",
                "_score": 1,
                "_source": {
                    "_id": 105,
                    "location": "dr72hbxdbzk3npz479cy"
               }
(...)
```

Explain on 0.90.0:

```
{
  "query": {
    "filtered": {
      "filter": {
          "geo_distance": {
            "distance": "50mi",
            "location": {
              "lat": 40.720611,
              "lon": -73.998776
            }
          }
       }
   }
}
-------
{
    "ok": true,
    "_index": "business_index",
    "_type": "business",
    "_id": "105",
    "matched": false,
    "explanation": {
        "value": 0,
        "description": "ConstantScore(GeoDistanceFilter(location, ARC, 50.0, 40.720611, -73.998776)) doesn't match id 7"
    }
}
```
</comment><comment author="chilling" created="2013-06-07T16:14:20Z" id="19116841">@kosz85 thanks for open this issue. I think my commit will fix this
</comment><comment author="kosz85" created="2013-06-10T07:22:09Z" id="19183882">Thanks, fix worked after reindexing data. (We checked it on version 0.90.1 patched with this commit)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inverting logic of parsing timestamp fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3072</link><project id="" key="" /><description>The current implementation tries to parse every timestamp as a long first
and uses it as a unix timestamp.
This fails if the configured timestamp is like 'YYYYMMDD' and also
resembles a long by coincidence.

This PR tries to fix this issue by trying to parse it as a date first and
only it fails is tried to be parsed as a long from unix timestamp.

Possible problems:
- Performance: Reversing the order might make things slower.
- Wrong parsing: An additional check was added to make sure no negative
  unix timestamps can be generated.

Closes #2937
</description><key id="14619130">3072</key><summary>Inverting logic of parsing timestamp fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-05-22T11:53:02Z</created><updated>2014-08-08T11:19:51Z</updated><resolved>2014-06-10T12:24:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T11:19:51Z" id="51589544">heya - what happened with this PR?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>percolate requires action.auto_create_index and index.mapper.dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3071</link><project id="" key="" /><description>When disabling automatic creation of indexes/mappings (action.auto_create_index:false and index.mapper.dynamic:false in elasticsearch.yml), percolation fails.

Percolation is a feature provided by elasticsearch, and as such, should not depend on the fact it's implemented with an index.

To reproduce, just run a server with this configuration, and execute the examples in the percolate documentation (guide/reference/api/percolate/). When trying to register a query in the percolator, error is: 
`{"error":"IndexMissingException[[_percolator] missing]","status":404}`
Ok, there is a workaround: set action.auto_create_index to +_\* or +_percolator. Then,
`{"error":"TypeMissingException[[_percolator] type[test] missing: trying to auto create mapping, but dynamic mapping is disabled]","status":404}`

Is there any way to use percolation while preventing indexing from creating mappings on the fly?
</description><key id="14589713">3071</key><summary>percolate requires action.auto_create_index and index.mapper.dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">iksnalybok</reporter><labels /><created>2013-05-21T20:39:51Z</created><updated>2013-09-13T14:27:25Z</updated><resolved>2013-09-13T14:27:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="iksnalybok" created="2013-05-28T08:38:43Z" id="18537758">NB: Though not exactly what I was looking for (i.e. prevent creation of _new_ mappings), here is a way to disable dynamic _changes_ in a mapping. Use, within the mapping you want immutable, "dynamic : strict" (documented in http://www.elasticsearch.org/guide/reference/mapping/object-type/).
</comment><comment author="kimchy" created="2013-06-01T19:12:34Z" id="18795197">sadly, this is the case today. We are thinking on a new design of percolator that will not require it, expect an issue describing the new design to come out shortly (its still very much up in the air), and hopefully we will start and work it relatively soon.
</comment><comment author="martijnvg" created="2013-08-27T19:35:06Z" id="23364175">The redesigned percolator in master, all the queries are stored in the _percolator type, so no need for enable dynamic mapping if you chose to disable it, just make sure that you create the _percolator type yourself before indexing the queries. 
</comment><comment author="iksnalybok" created="2013-09-13T14:27:25Z" id="24398188">New percolator design is really cleaver! I'll close this issue as it does not make sense any more.

Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API feature for ES snapshots/backups</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3070</link><project id="" key="" /><description>As stated in [issue 2458](https://github.com/elasticsearch/elasticsearch/issues/2458), the  Shared Gateways are getting deprecated and we will need a new API in order to snapshot ES clusters/indices.
</description><key id="14585543">3070</key><summary>API feature for ES snapshots/backups</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">quentins</reporter><labels /><created>2013-05-21T19:12:22Z</created><updated>2014-04-25T14:17:04Z</updated><resolved>2014-04-25T14:17:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-01T19:13:11Z" id="18795212">agreed, this is the plan to replace the need of shared gateways.
</comment><comment author="kavinxavier" created="2013-08-29T19:34:09Z" id="23517209">Will this be accommodated in v1.0.0? This is an important feature for huge scale systems. Backup and Restore with index level granularity would be great. At least the ability to restore only selected index to a point in time. 

We have TBs of data stored in Elastic Search and very recently we faced the "Split Brain" couple of times and couple of indices became inconsistent and wouldn't get assigned in any of the replicas we had and hence the cluster was in Red. I had to CURL delete those huge indices and reindex all the data back which took days for me.

I already snapshot the disks, but again, the restore would have taken more time than the reindexing option because of the huge volume and moreover restoring the disk would have required a lot of down time.

Please make this a higher priority because this would help a lot of souls.
</comment><comment author="imotov" created="2013-10-03T00:06:42Z" id="25587768">I added the new issue #3826 to track work on the first phase of the snapshot/restore API.
</comment><comment author="javanna" created="2014-04-25T14:17:04Z" id="41397044">I think we can close this issue as snapshot and restore api was released with 1.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>mo1lo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3069</link><project id="" key="" /><description /><key id="14580517">3069</key><summary>mo1lo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">srhanson</reporter><labels /><created>2013-05-21T17:36:47Z</created><updated>2013-05-21T17:37:18Z</updated><resolved>2013-05-21T17:37:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="srhanson" created="2013-05-21T17:37:18Z" id="18224332">Whoops
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>conflict between alias routing and parent/child routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3068</link><project id="" key="" /><description>I was trying to create aliases with routing on an index that includes parent/child docs. Posting to an alias endpoint while specifying "parent=X" causes an error. I was thinking it shouldn't, because the parent/child routing is to ensure the docs will wind up on the same shard, but this is already guaranteed by the routed alias.

Curl recreation:

https://gist.github.com/erikcameron/5621421

As noted, it looks like it works if you give parent and routing explicitly. 

Thanks!
-E
</description><key id="14579219">3068</key><summary>conflict between alias routing and parent/child routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">erikcameron</reporter><labels><label>:Parent/Child</label><label>bug</label><label>low hanging fruit</label><label>v5.0.0-alpha1</label></labels><created>2013-05-21T17:10:27Z</created><updated>2016-07-29T09:23:49Z</updated><resolved>2015-12-21T08:58:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-29T13:29:53Z" id="18616086">Makes sense, ES shouldn't throw an error when indexing into an alias and `parent` is set. Assuming that someone wants to override the parent routing (which is automatically set when `parent` is present in a index request) when indexing into an index alias, ES should use the routing specified in the index alias.
</comment><comment author="jkogara" created="2014-08-24T23:15:04Z" id="53212482">Just ran into this same issue, is it likely to be fixed soon?
</comment><comment author="bakura10" created="2014-10-21T17:13:33Z" id="59962730">Just had this issue too, I've lost lot of time finding out why it failed like this :).
</comment><comment author="bakura10" created="2014-11-28T18:06:52Z" id="64918308">Any news on this issue @martijnvg ?
</comment><comment author="aewhite" created="2016-04-28T11:18:20Z" id="215392346">Any chance of this getting back-ported to ES 2.x? This is a rather high priority bug for us since we can't determine the routing of the parent without investigating the alias directly before every insert (which is very expensive).
</comment><comment author="jondeandres" created="2016-07-29T09:23:49Z" id="236134970">any news on this? This bug is happening still in 2.3.3, has been ported to any 2.x tag?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing percolation of documents with TTL set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3067</link><project id="" key="" /><description>When a type is configured with a TTL, percolation of documents of this type
was not possible and threw an exception. This fix ignores the TTL for percolation instead of
throwing an exception that the document is already expired.

Closes #2975
</description><key id="14574598">3067</key><summary>Fixing percolation of documents with TTL set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-05-21T15:42:13Z</created><updated>2014-07-16T21:53:21Z</updated><resolved>2013-05-27T07:18:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-27T07:18:11Z" id="18485550">Closed with https://github.com/elasticsearch/elasticsearch/commit/2e4d18b519a4074d2b0c25f51147f334bfc9c6a1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Report previous version in IndexResponse when external version type used</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3066</link><project id="" key="" /><description>When indexing, you get back in the response the version number that was assigned to the document you put. This makes sense, and quite useful in many scenarios - for example in ours, where we check to see if it == 1 and if it does we know its a new entry and can do perform some extra actions.

When specifying a version yourself by using the external version type, you already know what version to expect in the response if the indexing operation succeeds. Hence, the version returned is quite meaningless. In this scenario, I would really like to get the PREVIOUS version. For us this would mean knowing whether this is a new entry or not.

I'll be happy to provide a pull request with this change if this helps :)
</description><key id="14560088">3066</key><summary>Report previous version in IndexResponse when external version type used</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-05-21T10:07:02Z</created><updated>2013-07-16T08:14:05Z</updated><resolved>2013-07-16T08:14:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bleskes" created="2013-06-09T19:17:38Z" id="19171646">Hi Itamar,

I have tried to implement the previous_version you requested but I run into complications that have made it unuseful  in practice and _not_ a good solution to indicate document creation.

Instead, I've implemented a created flag (see case #3154), which gives an explicit indication on the IndexResponse class. This should solve your problem.

Cheers,
Boaz
</comment><comment author="synhershko" created="2013-06-10T08:22:19Z" id="19185940">Cool, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>QueryStringQuery overwrites parsed boost value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3065</link><project id="" key="" /><description>Set the query boost of a parsed query string query to the product of
the parsed query boost and the boost value specified in the "boost"
query string parameter.

fixes #3024
</description><key id="14548361">3065</key><summary>QueryStringQuery overwrites parsed boost value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mattweber</reporter><labels /><created>2013-05-21T02:35:26Z</created><updated>2014-07-16T21:53:21Z</updated><resolved>2013-05-21T09:03:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-21T09:03:14Z" id="18196904">pushed thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Regex on terms facet doesn't work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3064</link><project id="" key="" /><description>I have objects with region_ids with mapping:

```
"region_ids" : {
          "type" : "integer"
        },
```

For example:

```
{
'_id': 52,
'region_ids': [3, 4, 12, 13, 14, 18, 30, 31, 30067]
}
```

Facet Query:

```
{'facets': {'regions': {'terms': {'field': 'region_ids',
    'regex': '62191|75|30',
    'regex_flags': 'DOTALL',
    'size': 10}}},
 'fields': ['_id', '_parent', '_routing', '_source'],
 'from': 0,
 'query': {'match_all': {}},
 'size': 10}
```

And it returns facet terms not filtered by regex:

```
{'regions': {'_type': 'terms',
  'missing': 5,
  'other': 570,
  'terms': [{'count': 52, 'term': 30067},
   {'count': 48, 'term': 4},
   {'count': 44, 'term': 31},
   {'count': 43, 'term': 29902},
   {'count': 43, 'term': 75},
   {'count': 40, 'term': 30},
   {'count': 32, 'term': 13},
   {'count': 23, 'term': 12},
   {'count': 23, 'term': 3},
   {'count': 19, 'term': 29398}],
  'total': 937}}
```

It works with script and exlude, but not with regex, so I assume it's a bug.
I try regexs:

```
62191|75|30
^(62191|75|30)$
75
[75]*
```

It just doesn't work.

ES newest build 0.90
</description><key id="14519605">3064</key><summary>Regex on terms facet doesn't work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">kosz85</reporter><labels /><created>2013-05-20T13:27:19Z</created><updated>2014-07-22T19:16:50Z</updated><resolved>2014-02-21T16:06:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-20T16:19:25Z" id="18156588">Regex are only supported on term facets ie. if you field is of type `string` but I agree we should rather throw an exception...
</comment><comment author="spinscale" created="2014-02-21T16:06:41Z" id="35744400">closing this, supported with aggregations when using terms and stats aggregations. facets are deprecated and in bugfix-only mode
</comment><comment author="rayward" created="2014-06-04T03:41:37Z" id="45048644">The terms aggregator has the same restriction. 

Its quite restrictive not being able to exclude numeric terms.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: External terms doesn't work with _id field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3063</link><project id="" key="" /><description>```
curl -XPUT http://localhost:9200/index1/t1/123 -d '{ "name": "123" }'
curl -XPUT http://localhost:9200/index1/t1/456 -d '{ "name": "456" }'
curl -XPUT http://localhost:9200/index1/t2/1 -d '{ "ids": ["123", "456"] }'
```

Query with external terms returns no results:

```
curl http://localhost:9200/index1/t1/_search?pretty -d '{ "query": { "filtered": { "filter": { "terms": { "_id": { "index": "index1", "type": "t2", "id": "1", "path": "ids" } } } } } }'
```

Query with listed terms works:

```
curl http://localhost:9200/index1/t1/_search ?pretty -d '{ "query": { "filtered": { "filter": { "terms": { "_id": ["123", "456"] } } } } }'
```

External terms on `name` field works:

```
curl http://localhost:9200/index1/t1/_search?pretty -d '{ "query": { "filtered": { "filter": { "terms": { "name": { "index": "index1", "type": "t2", "id": "1", "path": "ids" } } } } } }'
```

Side issue: unmapped field throws NPE:

```
curl http://localhost:9200/index1/t1/_search?pretty -d '{ "query": { "filtered": { "filter": { "terms": { "XXX": { "index": "index1", "type": "t2", "id": "1", "path": "ids" } } } } } }'
```
</description><key id="14516183">3063</key><summary>Query DSL: External terms doesn't work with _id field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/kimchy/following{/other_user}', u'events_url': u'https://api.github.com/users/kimchy/events{/privacy}', u'organizations_url': u'https://api.github.com/users/kimchy/orgs', u'url': u'https://api.github.com/users/kimchy', u'gists_url': u'https://api.github.com/users/kimchy/gists{/gist_id}', u'html_url': u'https://github.com/kimchy', u'subscriptions_url': u'https://api.github.com/users/kimchy/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/41300?v=4', u'repos_url': u'https://api.github.com/users/kimchy/repos', u'received_events_url': u'https://api.github.com/users/kimchy/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/kimchy/starred{/owner}{/repo}', u'site_admin': False, u'login': u'kimchy', u'type': u'User', u'id': 41300, u'followers_url': u'https://api.github.com/users/kimchy/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-20T11:32:11Z</created><updated>2013-05-30T09:01:41Z</updated><resolved>2013-05-20T13:17:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Pull request to close issue 3061</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3062</link><project id="" key="" /><description>This pull request closes the following issue:

https://github.com/elasticsearch/elasticsearch/issues/3061
</description><key id="14509211">3062</key><summary>Pull request to close issue 3061</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">stephane-bastian</reporter><labels /><created>2013-05-20T06:56:18Z</created><updated>2014-07-01T17:04:17Z</updated><resolved>2014-04-07T12:13:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-04-07T12:13:09Z" id="39722580">Same change was made in #5470, but making a copy of the items. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MultiGetRequest does not expose its list of items</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3061</link><project id="" key="" /><description>The class MultiGetRequest does not provide a way to access the list of get request. However, it's sometimes useful to look at the individual item before executing the request. Note that the class MultiSearchRequest provides a method called request() to expose the list of individual request.
</description><key id="14508756">3061</key><summary>MultiGetRequest does not expose its list of items</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">stephane-bastian</reporter><labels><label>enhancement</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-05-20T06:32:10Z</created><updated>2014-03-20T09:30:22Z</updated><resolved>2014-03-20T09:29:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="stephane-bastian" created="2013-05-20T06:54:56Z" id="18134474">The following patch closes the issue:

https://github.com/stephane-bastian/elasticsearch/commit/31f5133fadcd195e824bb1cd07807f7cb298f879
</comment><comment author="stephane-bastian" created="2013-05-24T08:32:13Z" id="18392716">Hey Shay,

I noticed that you rolled-back the MultiGet changes you recently made (=&gt;  https://github.com/elasticsearch/elasticsearch/commit/e0825686f323bcb9bb0348ce61068e4af1763316). 
Quick question: Do you plan to accept the pull request and expose the list of items for 0.90.1?
</comment><comment author="s1monw" created="2014-03-12T20:22:52Z" id="37459415">@kimchy any reason why we can't add a getter for the items?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Wrong lucene-core version being pulled in by maven.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3060</link><project id="" key="" /><description>Using ES 0.90.0 Java API, Maven pulls in lucene-core 3.6 instead of 4.2.1
Here's the snipplet from the output of mvn dependency:tree for a project using ES API 0.90.0

```
 789 [INFO] +- org.elasticsearch:elasticsearch:jar:0.90.0:compile
 790 [INFO] |  +- org.apache.lucene:lucene-core:jar:3.6.1:compile (version managed from 4.2.1)
 791 [INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:4.2.1:compile
 792 [INFO] |  +- org.apache.lucene:lucene-codecs:jar:4.2.1:compile
 793 [INFO] |  +- org.apache.lucene:lucene-queries:jar:4.2.1:compile
 794 [INFO] |  +- org.apache.lucene:lucene-memory:jar:4.2.1:compile
 795 [INFO] |  +- org.apache.lucene:lucene-highlighter:jar:4.2.1:compile
 796 [INFO] |  +- org.apache.lucene:lucene-queryparser:jar:4.2.1:compile
 797 [INFO] |  |  \- org.apache.lucene:lucene-sandbox:jar:4.2.1:compile
 798 [INFO] |  +- org.apache.lucene:lucene-suggest:jar:4.2.1:compile
 799 [INFO] |  +- org.apache.lucene:lucene-join:jar:4.2.1:compile
 800 [INFO] |  |  \- org.apache.lucene:lucene-grouping:jar:4.2.1:compile
 801 [INFO] |  \- org.apache.lucene:lucene-spatial:jar:4.2.1:compile
```

lucene-core should be 4.2.1 , but 3.6.1 is being pulled in, which results in  error when loading org.elasticsearch.Version class.
</description><key id="14505230">3060</key><summary>Wrong lucene-core version being pulled in by maven.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">bhaskarvk</reporter><labels /><created>2013-05-20T02:35:51Z</created><updated>2015-09-02T03:00:03Z</updated><resolved>2013-06-28T11:32:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-20T07:01:41Z" id="18134647">can you run a `mvn clean` and see if 4.2.1 is not pulled? What kind of error are you getting?
</comment><comment author="bhaskarvk" created="2013-05-21T03:11:29Z" id="18186648">Yep, I did do mvn clean.
Here's the exact error I got

```
java.lang.NoSuchFieldError: LUCENE_41
    at org.elasticsearch.Version.&lt;clinit&gt;(Version.java:116)
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:119)
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166)

```

To get around it, I had to manually add lucene-core 4.2.1 dependency.
</comment><comment author="spinscale" created="2013-05-21T07:19:22Z" id="18192866">Hey,

do you maybe have another dependency in your pom.xml which relies on an older version of lucene-core, try this command to find out

```
mvn dependency:tree  -Dverbose -Dincludes=org.apache.lucene
```

please paste the results. Also you could run the above command with `-Dincludes=org.elasticsearch` in order to check if there is more than one elasticsearch version.
</comment><comment author="spinscale" created="2013-06-28T11:32:56Z" id="20183341">Closing. Happy to reopen with more information provided.
</comment><comment author="renhai" created="2015-09-02T03:00:03Z" id="136919423">thx, this solved my problem
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Integrate forbiddenAPIs checks into ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3059</link><project id="" key="" /><description>https://code.google.com/p/forbidden-apis/ integrates with maven and checks Java byte code against a list of "forbidden" API signatures. It fails the builds if deprecated JDK methods are used or if Streams are created with default charsets, if default locales or timezones are used or if encodings are missing on String formatting etc. 

This helps to prevent issues that occur on the users systems if locals are different or charsets are oddly set.
</description><key id="14489371">3059</key><summary>Integrate forbiddenAPIs checks into ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-18T22:12:34Z</created><updated>2013-05-19T21:28:30Z</updated><resolved>2013-05-19T21:28:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uschindler" created="2013-05-18T23:36:05Z" id="18109774">I added a comment to your commit :-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query norm doesn't apply to custom_score queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3058</link><project id="" key="" /><description>Should query norm apply to custom_score queries? Currently it doesn't - it just returns the "pure" _score returned from the script.  This can make for inconsistencies, eg:

```
curl -XDELETE 'http://127.0.0.1:9200/test/?pretty=1'

curl -XPUT 'http://127.0.0.1:9200/test/product/1?pretty=1'  -d '
{
   "foo" : 1
}
'

curl -XPUT 'http://127.0.0.1:9200/test/product/2?pretty=1&amp;refresh=1'  -d '
{
   "foo" : 2
}
'
```

I would assume that both `should` clauses would return a score of 3, but the `constant_score` clause has its boost normalised by the query norm, and the `custom_score` doesn't:

```
curl -XGET 'http://127.0.0.1:9200/test/product/_search?pretty&amp;format=yaml&amp;explain=true'  -d '
{
   "query" : {
      "bool" : {
         "disable_coord" : 1,
         "should" : [
            {
               "custom_score" : {
                  "script" : "3",
                  "query" : {
                     "constant_score" : {
                        "filter" : {
                           "term" : {
                              "foo" : 1
                           }
                        }
                     }
                  }
               }
            },
            {
               "constant_score" : {
                  "boost" : 3,
                  "filter" : {
                     "term" : {
                        "foo" : 2
                     }
                  }
               }
            }
         ]
      }
   }
}
'
```

Explain output:

```
took: 2
timed_out: false
_shards:
  total: 5
  successful: 5
  failed: 0
hits:
  total: 2
  max_score: 3.0
  hits:
  - _shard: 2
    _node: "iGrI9-xiQQ2zn6aozlwmZQ"
    _index: "test"
    _type: "product"
    _id: "1"
    _score: 3.0
    _source:
      foo: 1
    _explanation:
      value: 3.0
      description: "sum of:"
      details:
      - value: 3.0
        description: "custom score, product of:"
        details:
        - value: 3.0
          description: "script score function: composed of:"
          details:
          - value: 0.31622776
            description: "ConstantScore(cache(foo:[1 TO 1])), product of:"
            details:
            - value: 1.0
              description: "boost"
            - value: 0.31622776
              description: "queryNorm"
        - value: 1.0
          description: "queryBoost"
  - _shard: 3
    _node: "iGrI9-xiQQ2zn6aozlwmZQ"
    _index: "test"
    _type: "product"
    _id: "2"
    _score: 0.94868326
    _source:
      foo: 2
    _explanation:
      value: 0.94868326
      description: "sum of:"
      details:
      - value: 0.94868326
        description: "ConstantScore(cache(foo:[2 TO 2]))^3.0, product of:"
        details:
        - value: 3.0
          description: "boost"
        - value: 0.31622776
          description: "queryNorm"
```
</description><key id="14482256">3058</key><summary>Query norm doesn't apply to custom_score queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-05-18T11:30:32Z</created><updated>2014-07-03T19:40:42Z</updated><resolved>2014-07-03T19:40:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-08-16T14:07:11Z" id="22767528">This may have something to do with the implementation of

`ConstantScoreQuery.ConstantWeight#getValueForNormalization` 

vs 

`FunctionScoreQuery.CustomBoostFactorWeight#getValueForNormalization`
</comment><comment author="btiernay" created="2013-08-16T21:14:11Z" id="22794352">Perhaps query norms should be configurable by way of a custom Lucene `Similarity` class used by ElasticSearch. This could allow applications that rely on this "feature" to continue working.
</comment><comment author="s1monw" created="2013-08-17T06:29:59Z" id="22806592">I don't really understand the problem here. Why can't people that need an exact score vs. a `constant_score` just use a `custom_score`? I'd also be interested in the usecase of this vs. `constant_score`
</comment><comment author="btiernay" created="2013-08-17T13:54:53Z" id="22812233">@s1monw I think @clintongormley is pointing out an inconsistency from an API perspective. Nonetheless, it should be documented.

One reason someone may want to use `constant_score` over `custom_score` is purely to avoid the performance hit of involving scripting.
</comment><comment author="s1monw" created="2013-08-17T19:13:01Z" id="22817855">&gt; One reason someone may want to use constant_score over custom_score is purely to avoid the performance hit of involving scripting.

hmm I think `custom_boost_factor` is the way to go.... http://www.elasticsearch.org/guide/reference/query-dsl/custom-boost-factor-query/ we fixed all this in the new function score API that will come with `0.90.4`

I think we should close this issue @clintongormley ?
</comment><comment author="btiernay" created="2013-08-17T21:10:15Z" id="22819725">@s1monw thanks for the reference to `custom_boost_factor`. 

I can't speak for @clintongormley, but I think it would be useful to mention in the docs that query norms don't apply to `custom_score` queries. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query fails when nested multi_fields share the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3057</link><project id="" key="" /><description>I've an index which features two fields (`pages` and `categories`) with a similar `path` property , which I've defined as multi-field. Initially the index only featured `categories`, and I was able to query without issue. However after adding the `pages` property I stopped getting results when querying against `categories`, while `pages` queries were fine. Removing the `pages` property meant the problematic queries returned to working as normal.

I tracked the problem down to the two fields sharing a multi-field field with the same name (again, `path`). If I rename that field in the `pages` section, everything works as normal.

I've a gist here: https://gist.github.com/craigmarvelley/9f7dad9ce4a57a4dba60 with two files that demonstrate this: one where the fields share the same name, which demonstrates a search failing, and a second where I've renamed the fields and the search returns results as expected. I've renamed both fields in that latter example, but so long as they're different it works.

I'm using version 0.19.8 (client restrictions means I can't upgrade yet). I couldn't see from the docs any reason why two fields should not be able to have multi-field subfields with the same name, but apologies if this isn't the case and this is a bogus issue.
</description><key id="14468088">3057</key><summary>Query fails when nested multi_fields share the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">craigmarvelley</reporter><labels /><created>2013-05-17T19:32:53Z</created><updated>2014-03-04T01:15:26Z</updated><resolved>2013-05-17T19:45:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-17T19:45:37Z" id="18082271">Hi @craigmarvelley 

You can get around this by specifying the full field name, eg `categories.path` or even `content.categories.path``
</comment><comment author="craigmarvelley" created="2013-05-17T19:54:33Z" id="18082694">@clintongormley That works perfectly, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accessing fields of nested doc in custom score script may cause documents missing in query result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3056</link><project id="" key="" /><description>I recently ran into a problem of missing documents in query result when custom score script is used. After some testing, I found that the problem seems occur when the script tries to access a field in a nested doc where a particular root document does not contain any such nested doc.

To reproduce the problem, test data and queries can be found here: http://goo.gl/iHOc5. The example may not make much sense in real world, but the idea is to sort products by average rate from users' review. One particular requirement is to always treat anonymous user's rate as 3 and assign rate as 3 for products with no reviews.

We can determine whether a user is anonymous or not by checking review.user.member_id field is empty or not: doc['review.user.member_id'].empty, this seems work fine except that products with no reviews are dropped out in the result as the first query example shows. Is this a bug? As there is no query/filter that excludes documents, shouldn't all documents be returned?

Also, there seems no way to determine whether a review exists or not. The second query example shows doc['review'].empty does not work, this makes sense because indeed, there is not such field as 'review' under the 'product' index, 'review' is a nested document. However, the question remains: is there a way to determine the existence of a nested doc?
</description><key id="14467277">3056</key><summary>Accessing fields of nested doc in custom score script may cause documents missing in query result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">junjun-zhang</reporter><labels /><created>2013-05-17T19:12:09Z</created><updated>2014-08-08T12:11:39Z</updated><resolved>2014-08-08T12:11:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-18T11:06:05Z" id="18099013">Hiya

This is a really interesting question (and thanks for the runnable gist!)

The issue is that the nested match_all is only matching docs which have
nested docs. Really, you just care about whether a doc has any review by a
registered member, because all other products get the score of 3. In order
to do that, we need to expose member.id in the parent doc as well, which we
can do by adding `include_in_parent: true` to the nested mapping.

Then we write the query to match docs with member reviews, and calculate
the score using your script, and docs without member reviews, which get a
score of 3. For this we use a `bool` query with two clauses. Also, set
`disable_coord` to true, so that the "pure" score from each clause comes
through. Otherwise the bool query would divide the score by the number of
clauses (ie 2).

```
  "bool" : {
     "disable_coord" : 1,
     "should" : [
        { clause to match docs without member reviews },
        { clause to match docs with member reviews }
     ]
  }
```

So the clause WITH member reviews looks like the following:

{
   "filtered" : {
      "filter" : {
         "exists" : {
            "field" : "review.user.member_id"
         }
      },
      "query" : {
         "nested" : {
            "query" : {
               "custom_score" : {
                  "script" : "doc[\u0027review.user.member_id\u0027].empty
? 3 : doc[\u0027review.rate\u0027].value",
                  "query" : {
                     "match_all" : {}
                  }
               }
            },
            "score_mode" : "avg",
            "path" : "review"
         }
      }
   }
}

Then, the clause to match docs WITHOUT reviews.  Initially, I wrote this:

{
   "constant_score" : {
      "boost" : 3,
      "filter" : {
         "missing" : {
            "field" : "review.user.member_id"
         }
      }
   }
}

But the score of `3` was being combined with the query norm, and so
returning values like 0.9xxxx. Weirdly, the custom_score doesn't get
combined with the query norm.  I'm not sure if that is intentional or not,
but that's the way it is.  So the way to get a pure score of `3` from the
above is to wrap it in a custom_score query, and have the script just
return `3`:

{
   "custom_score" : {
      "script" : "3",
      "query" : {
         "constant_score" : {
            "filter" : {
               "missing" : {
                  "field" : "review.user.member_id"
               }
            }
         }
      }
   }
},

The full query is here: https://gist.github.com/clintongormley/5604037

IMPORTANT: you're paying the cost of this calculation at query time, but
all the information is known at index time.  A much better approach would
be to just calculate the avg rating when you index a document and store it
as a field: `avg_rating`.  Then you can sort by that field.

clint

On 17 May 2013 21:12, Junjun Zhang notifications@github.com wrote:

&gt; I recently ran into a problem of missing documents in query result when
&gt; custom score script is used. After some testing, I found that the problem
&gt; seems occur when the script tries to access a field in a nested doc where a
&gt; particular root document does not contain any such nested doc.
&gt; 
&gt; To reproduce the problem, test data and queries can be found here:
&gt; http://goo.gl/iHOc5. The example may not make much sense in real world,
&gt; but the idea is to sort products by average rate from users' review. One
&gt; particular requirement is to always treat anonymous user's rate as 3 and
&gt; assign rate as 3 for products with no reviews.
&gt; 
&gt; We can determine whether a user is anonymous or not by checking
&gt; review.user.member_id field is empty or not:
&gt; doc['review.user.member_id'].empty, this seems work fine except that
&gt; products with no reviews are dropped out in the result as the first query
&gt; example shows. Is this a bug? As there is no query/filter that excludes
&gt; documents, shouldn't all documents be returned?
&gt; 
&gt; Also, there seems no way to determine whether a review exists or not. The
&gt; second query example shows doc['review'].empty does not work, this makes
&gt; sense because indeed, there is not such field as 'review' under the
&gt; 'product' index, 'review' is a nested document. However, the question
&gt; remains: is there a way to determine the existence of a nested doc?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3056
&gt; .
</comment><comment author="btiernay" created="2013-08-16T13:49:06Z" id="22767008">For the future reader, https://github.com/elasticsearch/elasticsearch/issues/3058 was created to address:

&gt; Weirdly, the custom_score doesn't get combined with the query norm.  I'm not sure if that is intentional or not, but that's the way it is. 
</comment><comment author="clintongormley" created="2014-08-08T12:11:39Z" id="51593095">Closed in favour of #3495
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ClasscastException in GetResponse for binary data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3055</link><project id="" key="" /><description>Someone on the mailinglist mentioned a seemingly random occuring issue when executing a GetRequest...

I managed to track down the issue, but the exception still occurs randomly (my assumption is, that it depends whether the data needs to be fetched remotely).

This is the test case

``` java
public class SearchClassCastTest extends AbstractNodesTests {

    private Client client;

    @BeforeClass
    public void createNodes() throws Exception {
        startNode("node1");
        startNode("node2");
        client = node("node1").client();
    }

    @AfterClass
    public void closeNodes() {
        client.close();
        closeAllNodes();
    }

    @Test
    public void flakyAssertion() throws Exception {
        try {
            client.admin().indices().prepareDelete("test").execute().actionGet();
        } catch (IndexMissingException e) {}
        client.admin().indices().prepareCreate("test").execute().actionGet();

        String mapping = jsonBuilder().startObject()
                .startObject("test")
                    .startObject("properties")
                        .startObject("data")
                            .field("type","binary")
                        .endObject()
                    .endObject()
                .endObject()
                .string();
        client.admin().indices().preparePutMapping("test").setType("test").setSource(mapping).execute().actionGet();

        client.prepareIndex("test", "test", "0").setSource(jsonBuilder().startObject()
                .field("data", "dm9vZG9v")
            .endObject())
            .setRefresh(true)
            .execute().actionGet();

        GetResponse g = client.prepareGet("test", "test", "0").setFields("data").execute().actionGet();
        assertThat(g.isExists(), is(true));

        Object data= g.getField("data").getValue();
        // FLAKY ASSERTION HERE
        assertThat(data, instanceOf(BytesArray.class));
    }


```

The assertion error is this

```
java.lang.AssertionError: 
Expected: an instance of org.elasticsearch.common.bytes.BytesArray
     but: &lt;org.elasticsearch.common.bytes.ChannelBufferBytesReference@cfac04ae&gt; is a org.elasticsearch.common.bytes.ChannelBufferBytesReference
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
    at org.elasticsearch.test.integration.search.simple.SearchClassCastTest.flakyAssertion(SearchClassCastTest.java:87)
```

My assumption is, that we just forgot to call `ChannelBufferBytesReference.toBytesArray()` somewhere when getting the data.

Also, the test fails never when ES_LOCAL_MODE is set. And you need more than one node.
</description><key id="14451125">3055</key><summary>ClasscastException in GetResponse for binary data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">spinscale</reporter><labels><label>bug</label></labels><created>2013-05-17T13:02:19Z</created><updated>2013-05-17T14:32:43Z</updated><resolved>2013-05-17T14:32:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-17T14:04:24Z" id="18063125">Update on this one, I think it is invalid and can be closed, I just need to verify.
</comment><comment author="spinscale" created="2013-05-17T14:32:43Z" id="18064862">My fault. Confused interfaces (`BytesReference`) with implementations, when a get request comes back from a local shard it is `BytesArray`, remote it is `ChannelBufferBytesReference`. Closing therefore.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error while using wildcard in multifield query_string query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3054</link><project id="" key="" /><description>I've indexed some data with a field named "observed" that is an object with two other fields named "filename" and "pathname".  These fields are analyzed and have multiple values.

The following query executed without problems:

``` json
"query_string" : { 
  "fields" : ["observed.filename", "observed.pathname"], 
  "query" : "Autorun", 
  "use_dis_max" : true 
}
```

but when I change it to the following it fails:

``` json
"query_string" : {
  "fields" : ["observed.*"],
  "query" : "Autorun",
  "use_dis_max" : true
}
```

with the error:

"SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[jHW46VKNS_G6Cnrdzko_WA][file_analysis][0]: SearchParseException[[file_analysis][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\t\"query\": {\t    \"query_string\" : {\t        \"fields\" : [\"observed._\"],\t        \"query\" : \"Autorun\",\t        \"use_dis_max\" : true\t    }\t}}]]]; nested: NumberFormatException[For input string: \"Autorun\"]; }{[jHW46VKNS_G6Cnrdzko_WA][file_analysis][1]: SearchParseException[[file_analysis][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\t\"query\": {\t    \"query_string\" : {\t        \"fields\" : [\"observed._\"],\t        \"query\" : \"Autorun\",\t        \"use_dis_max\" : true\t    }\t}}]]]; nested: NumberFormatException[For input string: \"Autorun\"]; }]

Why is that?  The docs indicate it should work.

Also odd is that the following works:

``` json
"query_string" : {
  "fields" : ["observed.f*"],
  "query" : "Autorun",
  "use_dis_max" : true
}
```

but this fails:

``` json
"query_string" : {
  "fields" : ["observed.p*"],
  "query" : "Autorun",
  "use_dis_max" : true
}
```

I am Running 0.90.0.  The mapping for the fields looks like:

``` json
"observed" : {
  "type": "object",
  "properties" : {
    "pathname"  : { "type": "string", "analyzer": "path"              },
    "filename"  : {
      "type": "multi_field",
      "fields": {
        "filename": { "type": "string", "analyzer": "path"              },
        "whole"   : { "type": "string", "analyzer": "lowercase_keyword" }
      }
    }
  }
}
```

There are some other fields in there as well.
</description><key id="14450214">3054</key><summary>Error while using wildcard in multifield query_string query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eliaslevy</reporter><labels /><created>2013-05-17T12:37:29Z</created><updated>2013-05-21T09:03:45Z</updated><resolved>2013-05-21T09:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-05-20T17:15:07Z" id="18159630">This is not a wildcard issue, it is due to attempting to search a field with the wrong format.  You must have a numeric field that begins with a p inside of the observed object.  When using your wildcard, this field get's included in the searchable fields and thus throws and error when you attempt to search it using a query string.  Set the `lenient` parameter to true to ignore these types of errors as documented here:  http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query/
</comment><comment author="eliaslevy" created="2013-05-21T09:03:44Z" id="18196930">Doh.  Makes sense.  Thanks for pointing it out Matt.  Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Missing LF (0xA) in output of (all?) ES API requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3053</link><project id="" key="" /><description>Dear all,

I encoutered the output of (all?) API requests, e.g. 

  curl -XGET 'http://byyd30:9200/_cluster/health?pretty=true' 

are missing the last LF character (ran the commands on Unix/Linux). I use nxlog as data collector for several data collections out of ES. In result, the last curly bracket of the JSON output is missing and kills the processing.

I double checked the output by writing the data into a file and hexedited that data:

![missing-lf](https://f.cloud.github.com/assets/4394913/517728/babab694-bee7-11e2-9d54-1f3ed9ef991d.png)

I checked the same on Windows (to eleminate any terminal influeneces).

Do I'm wrong or is that "normal" - is it a feature/error? Has anyone else similiar issues with that?

Any comment/help is appreciated.

Thanks a lot,
Juergen
</description><key id="14448930">3053</key><summary>Missing LF (0xA) in output of (all?) ES API requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jueadams</reporter><labels /><created>2013-05-17T11:56:08Z</created><updated>2013-10-18T09:35:28Z</updated><resolved>2013-10-18T09:35:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-17T12:12:34Z" id="18057976">We intentionally do not return it, its not really needed as a HTTP response with json
</comment><comment author="jueadams" created="2013-05-17T12:44:58Z" id="18059200">Many thanks for the quick answer. I'm not a HTTP specialist - I wasn't aware about that. I'm using a collector that is NOT typically dealing with HTTP responses (over JSON). I expect that some other people "leave that world" and use other interfaces to overtake that data to feed non-web applications. Would be worth for an enhancement request to add an additional output format (like "pretty=true") that adds that missing LF (and not break the Standard HTTP response)?

If so, where can a place such an request?

Many thanks in advance,
Juergen
</comment><comment author="javanna" created="2013-10-18T09:35:28Z" id="26583031">Hi @jueadams ,
have a look at https://github.com/elasticsearch/elasticsearch/issues/3748 . From 0.90.6 we'll add a line feed will be added when using `pretty=true`.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow Delete by Query for _percolator</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3052</link><project id="" key="" /><description>Just a suggestion to allow delete by query, especially wildcard queries against the _id, instead of just delete by _id.  It would be much faster and easier than one by one.
</description><key id="14448571">3052</key><summary>Allow Delete by Query for _percolator</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kwloafman</reporter><labels /><created>2013-05-17T11:45:05Z</created><updated>2013-05-22T08:11:40Z</updated><resolved>2013-05-22T08:11:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-22T08:11:40Z" id="18263320">Thanks for taking the time to report. I am closing this one as it is a duplicate of #1712
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArrayIndexOutOfBoundsException in org.elasticsearch.index.fielddata.ScriptDocValues.Strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3051</link><project id="" key="" /><description>getValues() causes an ArrayIndexOutOfBoundsException in case the list is larger than 10 elements because the SlicedObjectList.grow() method is never called. ES-version 0.90.0
</description><key id="14442568">3051</key><summary>ArrayIndexOutOfBoundsException in org.elasticsearch.index.fielddata.ScriptDocValues.Strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pgrebien</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-17T08:32:51Z</created><updated>2013-11-28T08:43:43Z</updated><resolved>2013-05-17T13:18:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-17T12:46:58Z" id="18059282">thanks for reporting... I will fix this in a bit.
</comment><comment author="rosejn" created="2013-11-27T16:31:20Z" id="29398342">We are still getting this error when trying to use a function_score script that accesses array values where the array has more than 10 values.  This is with 0.90.5, so it seems the bug was not fixed?
</comment><comment author="spinscale" created="2013-11-28T08:43:43Z" id="29447420">@rosejn dont hesitate to create a new bug report, it is easy to get lost in the old closed ones. Also, Simon added a test, which tries to reproduce the issue in the commit above. Can you find out, what is different with your setup compared to the test, so we can check on it? Or provide us a reproducible test-case?

Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MapperParsingException is DEBUG and not ERROR</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3050</link><project id="" key="" /><description>I have installed raven-java and am seeing MapperParsingException's coming through to my sentry instance.  

I have looked at the elasticsearch logs and these exceptions are coming up as DEBUG and so they are coming through to sentry as DEBUG.

Is this right? Should it not come up as an ERROR instead?
</description><key id="14424973">3050</key><summary>MapperParsingException is DEBUG and not ERROR</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">eamonnfaherty</reporter><labels /><created>2013-05-16T20:25:18Z</created><updated>2013-05-16T20:34:09Z</updated><resolved>2013-05-16T20:34:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-16T20:30:57Z" id="18027060">Its intentional, we return an error in the API call, a single API failing is not considered to be an error in elasticsearch if we properly report it back on the API layer. An error in elasticsearch is something that has gone back in the cluster, configuration, creation of a shard, ...
</comment><comment author="eamonnfaherty" created="2013-05-16T20:33:56Z" id="18027239">Ahh, I see that makes sense.

Thanks for your quick reply.  Sounds like the consumer of the api needs to bubble this exception instead.

Thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Changed daemon command to use $ES_USER</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3049</link><project id="" key="" /><description>I just installed `elasticsearch-0.90.0-1.noarch.rpm` and have not been able to start/stop elasticsearch using `service elasticsearch start`.

At line 67 where it tries to exec the `daemon` command, it is using a variable `$USER`, which is not set anywhere in the script, which causes that command to fail.

This should be changed to `$ES_USER`.
</description><key id="14416549">3049</key><summary>Changed daemon command to use $ES_USER</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">abohne</reporter><labels /><created>2013-05-16T17:13:18Z</created><updated>2014-07-16T21:53:22Z</updated><resolved>2013-06-28T11:31:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-17T07:40:57Z" id="18047826">Hey Andy,

the USER variable is set in `/etc/sysconfig/elasticsearch`

I agree it makes sense to rename it to ES_USER (in the file) in order to be more consistent, but still this is not the source of your problem, I think.

Can give some more information?
</comment><comment author="spinscale" created="2013-06-28T11:31:46Z" id="20183288">Changed (while being backwards compatible) with 2485c4890ce82124089062be4b0ee5622b8d54b8
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>JMX connector not created</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3048</link><project id="" key="" /><description>Hello,

JMX connector is not created even though it's configured to be. 
I have upgraded ES from 0.19.8 to 0.90.0-1 RPM package. With 0.19.8 it worked.

Configured according to the [documentation](http://www.elasticsearch.org/guide/reference/modules/jmx/):

```
jmx.create_connector: true
jmx.port: 9400-9500
jmx.domain: elasticsearch
```

```
# netstat -tplnu | grep java
tcp        0      0 0.0.0.0:9200                0.0.0.0:*                   LISTEN      21204/java          
tcp        0      0 0.0.0.0:9300                0.0.0.0:*                   LISTEN      21204/java          
udp        0      0 0.0.0.0:54328               0.0.0.0:*                               21204/java  
```

Bests,
Andor
</description><key id="14413514">3048</key><summary>JMX connector not created</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tothandor</reporter><labels /><created>2013-05-16T16:06:24Z</created><updated>2013-07-17T12:34:07Z</updated><resolved>2013-07-17T12:34:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-05-16T16:09:32Z" id="18011139">See #2728
</comment><comment author="spinscale" created="2013-07-17T12:34:07Z" id="21109529">JMX support has been dropped in 0.90 - you should switch to the JSON APIs
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date parsing is locale dependant with no way to configure</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3047</link><project id="" key="" /><description>Some date formats are locale dependent, e.g. the one given in the enron emails (http://www.cs.cmu.edu/~enron/). The format is:

"E, d MMM yyyy HH:mm:ss Z"
e.g.
"Wed, 06 Dec 2000 02:55:00 -0800"

E is locale dependent, in the German locale, it is "Mi" (afaik, its certainly not "Wed"). Parsing this date fails with `de_DE.UTF-8` and succeeds with `en_US.UTF-8` as environment setting. Sadly, this is not settable for date parsing, e.g.:

```
"type" : "date",
"format" : "E, d MMM yyyy HH:mm:ss Z",
"locale" : "US"
```

(where locale is a java.util.Locale)

This means that elasticsearch can only ever parse dates in one format per cluster. (or worse, depending on the locale of the node).
</description><key id="14412607">3047</key><summary>Date parsing is locale dependant with no way to configure</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">skade</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-16T15:48:34Z</created><updated>2013-05-19T21:28:30Z</updated><resolved>2013-05-19T21:28:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-17T14:53:45Z" id="18066185">that sucks, I agree. lets fix this with a local in the mapping! I will come up with something...
</comment><comment author="s1monw" created="2013-05-17T15:03:39Z" id="18066766">I will need to write some more tests but here is a start
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allowing pluggable highlighter implementations.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3046</link><project id="" key="" /><description>Currently elasticsearch ships with the plain and the fast-vector highlighter.
In order to support arbitrary highlighters via plugins, you only need to
implement a Highlighter interface and register your implementation in your
plugin at the HighlightModule.

In addition you can also add arbitrary options via the 'options' field in
the highlight request, which can be parsed in the highlighter implementation.

In order to find out how to write add your own analyzer, check out the tests
classes (CustomHighlighterSearchTests and CustomHighlighter).

Closes #2828
</description><key id="14404790">3046</key><summary>Allowing pluggable highlighter implementations.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-05-16T13:08:31Z</created><updated>2014-06-27T16:28:36Z</updated><resolved>2013-05-17T07:25:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dongaihua" created="2013-05-17T06:23:44Z" id="18045717">@spinscale , I checked your code, it can meet my requirement. Thank you very much for your effort. Will you merge this change into the next elasticsearch version? Thank you.
</comment><comment author="spinscale" created="2013-05-17T07:25:11Z" id="18047354">@dongaihua I have pushed it to master and the 0.90 branch, so it will be in the next elasticsearch version.

Closed by https://github.com/elasticsearch/elasticsearch/commit/2e07af63ba908b4d73c723068fe47eac898f7088
</comment><comment author="dongaihua" created="2013-05-17T07:37:14Z" id="18047717">@spinscale , it is awesome. Thank you very much!
</comment><comment author="s1monw" created="2013-05-17T14:06:54Z" id="18063282">very cool Alex! thank you!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API for retrieving information about a single alias (i.e., faster and transfer less data than _aliases)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3045</link><project id="" key="" /><description>Scenario: we have ~20K aliases on a single index (one for each user). These aliases are used 1) to ensure no cross-talk between users (via a filter in the alias) and 2) to provide indirection so that we can seamlessly migrate users between backwards-incompatible type mappings by creating a new index, backfilling a user's data into the new index, and then atomically repointing their alias to the new index.

We routinely need to look up what underlying index a user's alias is pointing to.

The current API for querying alias information is via an index's _aliases endpoint. This is fine when you want to know "what are all the aliases pointing to this index?"

What we want to know is "what is the underlying index for this particular alias?", which can _also_ be answered via the existing API, but becomes an expensive operation when a large number of aliases are in use.

For example, the _aliases response in our setup outlined above is ~160K and takes anywhere from 300-700ms.

All we're looking for is the alias information for a single alias, but querying `http://localhost:9200/alias/_aliases` is the same as requesting `http://localhost:9200/index/_aliases`, and returns a large payload with mostly data we're not interested in.

Making a HEAD request can tell us whether or not an alias or index exists, but we need to know the underlying index for the alias, not just whether or not it exists.

It would be great to have an API for querying individual aliases to avoid transferring all the extra data we're not interested in.

Some ideas:
1. Like some of the other info/stats APIs, make it possible to filter the _aliases response to only include the information you're interested in via the query string/path:

`curl -XGET 'http://localhost:9200/[index or alias]/_aliases/alias_name_of_interest`
1. Currently if you make a GET request to just an index or alias you get the following:

`curl -XGET 'http://localhost:9200/[index or alias]'
No handler found for uri [/index_name] and method [GET]`

We could instead have this return some high-level information about the index/alias. For aliases, I would want the alias's definition (including filtering, routing, etc.) as a response.

Mostly, we need to make querying for information about a single alias faster and have less extraneous information in the response.
</description><key id="14401962">3045</key><summary>API for retrieving information about a single alias (i.e., faster and transfer less data than _aliases)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbertram</reporter><labels /><created>2013-05-16T11:45:08Z</created><updated>2013-05-30T10:17:58Z</updated><resolved>2013-05-30T10:17:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-16T13:26:29Z" id="18000701">Hi @dbertram, This makes sense. We are working on improving the way specific index aliases are retrieved. Hopefully we get this in soon.
</comment><comment author="martijnvg" created="2013-05-30T10:17:58Z" id="18672090">I'm closing this issue. What you want has recently been implemented via issue #3075 
Thanks for bringing this up!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting: Also support nested sorting for sorting by script and geo distance sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3044</link><project id="" key="" /><description /><key id="14400361">3044</key><summary>Sorting: Also support nested sorting for sorting by script and geo distance sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-16T10:57:12Z</created><updated>2013-05-30T09:01:55Z</updated><resolved>2013-05-16T16:53:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>bool query with error in query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3043</link><project id="" key="" /><description>I was wondering: what if one of the queries in the array "should" contained errors.
For example:

```
{
    "bool" : {       
        "should" : [
            {
                "query_string" : { "query" : "\"Learning elasticsearch" }
            },
            {
                "term" : { "_all" : "\"Learning elasticsearch" }
            }
        ],
        "minimum_number_should_match" : 1,
        "boost" : 1.0
    }
}
```

The lucene query contains an unmatched double quote at the begin. Is the first
query discarded from the list "should" while searching? I'm trying to create a
fallback for bad user input, while trying to support the old Lucene syntax.

Thanks in advance
</description><key id="14395985">3043</key><summary>bool query with error in query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nicolasfranck</reporter><labels /><created>2013-05-16T08:48:16Z</created><updated>2013-05-16T13:38:38Z</updated><resolved>2013-05-16T13:38:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-05-16T13:38:38Z" id="18001383">@nicolasfranck if query_string query contains errors, the entire query fails. A better fallback would be to check if elasticsearch returned SearchPhaseExecutionException and resend user's query using [match](http://www.elasticsearch.org/guide/reference/query-dsl/match-query/) query instead. 

By the way, elasticsearch mailing list is much better place to ask questions like this. See http://www.elasticsearch.org/help/ for more details.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_msearch api, search_type or just type?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3042</link><project id="" key="" /><description>The documentation at http://www.elasticsearch.org/guide/reference/api/multi-search/
says **search_type**, but the server always returns
 {"error":"No search type for [mytype]"}

on the other hand just **type** instead of **search_type** seems to be working.
</description><key id="14395651">3042</key><summary>_msearch api, search_type or just type?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kul</reporter><labels /><created>2013-05-16T08:39:01Z</created><updated>2013-05-16T11:26:29Z</updated><resolved>2013-05-16T10:50:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-16T09:38:02Z" id="17991037">@kul `search_type` and `type` are different.  Could you provide a gist of what you are doing?
</comment><comment author="kul" created="2013-05-16T10:40:00Z" id="17993688">I have a index **library** with types **book** and **journal** in it.
Now i want to query both, for which the data i send to localhost:9200/library/_msearch is
{"search_type": "book"}
{"query":{"match_all":{}}}
{"search_type": "journal"}
{"query":{"match_all":{}}}

Now is search_type not a **type** of index , is what you are saying? Sorry i must be misunderstood then.
</comment><comment author="clintongormley" created="2013-05-16T10:50:15Z" id="17994097">Correct.  `search_type` indicates the type of search to perform, not which `type` to search in.  So it can be:
- `count`
  -  `query_then_fetch`
  - `dfs_query_then_fetch`
  - etc, etc

You're looking for the `type` parameter instead.
</comment><comment author="kul" created="2013-05-16T11:26:29Z" id="17995413">Thanks for clarifying.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simple query logging for debugging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3041</link><project id="" key="" /><description>Hi Elasticsearch team,
some users want simple query logging, so I added some debug statements. By adding a line "search: DEBUG" in logging.yml, the node requested should output some information about query execution and the JSON source.
</description><key id="14385873">3041</key><summary>Simple query logging for debugging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels><label>discuss</label></labels><created>2013-05-16T00:21:04Z</created><updated>2015-11-20T16:37:12Z</updated><resolved>2014-08-08T08:18:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nurikabe" created="2014-01-29T16:24:25Z" id="33600557">+1
</comment><comment author="dbaq" created="2014-01-30T19:09:33Z" id="33721558">:+1: 
</comment><comment author="thomax" created="2014-03-07T11:35:39Z" id="37016613">+1
</comment><comment author="sbourke" created="2014-03-11T08:49:40Z" id="37273997">+1
</comment><comment author="gugat" created="2014-03-17T22:34:57Z" id="37879636">+1
</comment><comment author="endofunky" created="2014-03-20T14:55:32Z" id="38176545">+1
</comment><comment author="mbeteta" created="2014-04-10T07:44:29Z" id="40051338">+1
</comment><comment author="bigerock" created="2014-05-16T20:18:59Z" id="43374961">where should we put that in the logging.yml file? and where does it log to?
</comment><comment author="brusic" created="2014-05-16T20:30:20Z" id="43375968">@bigerock This issue is a pull request, so it is not yet (if ever) incorporated into Elasticsearch.
</comment><comment author="bigerock" created="2014-05-16T20:35:13Z" id="43376404">thanks @brusic. i just had realized that. doh! if i understood compiling java i suppose i could download the changed files and merge and compile them?
</comment><comment author="brusic" created="2014-05-16T20:40:47Z" id="43376914">The pull request at this point a year old and based on the 1.0.0.Beta1-SNAPSHOT branch. There might be some merge conflicts. You could checkout Jorg's fork and try to merge the branch and see what happens!

Another option is to enable slow logs and basically set the threshold to 0ms: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-slowlog.html
</comment><comment author="bigerock" created="2014-05-16T21:00:32Z" id="43378919">thanks - that enable slow logs is much easier. very much appreciated!!
</comment><comment author="clintongormley" created="2014-08-08T08:18:12Z" id="51574969">Hi @jprante 

Thanks for this PR.  We've talked about it and, given that the slow log will log all queries if the threshold is zero, all of the clients provide client-side query logging, and the fact that we have a query profiler in the works #6699, we've decided not to include this change.
</comment><comment author="shikhar" created="2015-11-20T16:35:58Z" id="158452995">https://github.com/etsy/es-restlog is a plugin for logging requests that might be useful for folks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If doc isn't in nested filter, fall back on missing value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3040</link><project id="" key="" /><description>PR for #3020
</description><key id="14353958">3040</key><summary>If doc isn't in nested filter, fall back on missing value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-05-15T11:24:14Z</created><updated>2015-05-18T23:41:57Z</updated><resolved>2013-05-16T08:15:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Mlt api doesn't serialize routing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3039</link><project id="" key="" /><description>In some cases the mlt api redirects the request to a different node and because of the fact that the routing option isn't serialized during the redirecting it can end up going to the a node that doesn't hold the right shard (based on routing option). This results in a document missing error.
</description><key id="14350277">3039</key><summary>Mlt api doesn't serialize routing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-15T09:33:38Z</created><updated>2013-05-15T11:10:12Z</updated><resolved>2013-05-15T11:10:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Normalized handling of term values between string terms facet and terms stats facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3038</link><project id="" key="" /><description>This gives the term stats facets the script, exclude and regex features. Closes #3004 and #2109.
</description><key id="14346197">3038</key><summary>Normalized handling of term values between string terms facet and terms stats facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels /><created>2013-05-15T07:22:12Z</created><updated>2014-07-09T17:15:39Z</updated><resolved>2013-05-29T12:24:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kul" created="2013-05-16T08:42:12Z" id="17988647">:+1:  Thanks
</comment><comment author="roytmana" created="2013-05-17T15:28:36Z" id="18068215">also please look into https://github.com/elasticsearch/elasticsearch/issues/2617
it would be nice if terms_stats also supported missing and other in a way consistent with terms facet
</comment><comment author="bleskes" created="2013-05-29T12:24:14Z" id="18612645">replaced with a new
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase suggest direct generator possibly not obeying min_word_len 0.90 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3037</link><project id="" key="" /><description>I ran into an issue where the phrase suggester does not seem to be generating terms for words of length less than the default of four even with the min_word_len set to 0,1,2, or 3. When I run a term suggest, the term comes back as expected.

Here is a gist reproducing the issue:
https://gist.github.com/jtreher/5577747
</description><key id="14321284">3037</key><summary>Phrase suggest direct generator possibly not obeying min_word_len 0.90 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jtreher</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-14T17:24:41Z</created><updated>2013-06-07T19:07:55Z</updated><resolved>2013-05-15T13:21:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-15T10:08:45Z" id="17930139">The parameter is `prefix_len` not `prefix_leng`:
</comment><comment author="clintongormley" created="2013-05-15T10:11:32Z" id="17930258">```
    curl -XPOST '127.0.0.1:9200/test/_suggest?pretty' -d'{
      "did_you_mean": {
        "text": "ice",
        "term": {
          "field": "name",
          "max_edits":2,
          "suggest_mode": "always",
          "min_word_len": 0,
          "prefix_len": 0
        }
      }
    }'
    {
      "_shards" : {
        "total" : 1,
        "successful" : 1,
        "failed" : 0
      },
      "did_you_mean" : [ {
        "text" : "ice",
        "offset" : 0,
        "length" : 3,
        "options" : [ {
          "text" : "iced",
          "score" : 0.6666666,
          "freq" : 1
        } ]
      } ]
    }
```
</comment><comment author="jtreher" created="2013-05-15T11:36:11Z" id="17933359">@clintongormley While I did have a typo in the term suggest, the phrase suggest example is working and demonstrates the issue. Could you reopen?

I will clarify that the gist was demonstrating that the term suggest is providing the term "iced" but I believe the candidate generator in the phrase suggest is not provided the term "iced" for the phrase suggest to consider because of the word length. 
</comment><comment author="clintongormley" created="2013-05-15T12:02:07Z" id="17934296">@jtreher sorry - got that completely wrong. I'll reopen

I'm seeing the same thing you're seeing.

@s1monw ?
</comment><comment author="s1monw" created="2013-05-15T12:32:08Z" id="17935520">this seems to be a bug in the min_doc_freq smoothing. The good thing is that this only happens if your query term has a freq = 1 and the replacement has a freq = 1 as well. So in practice this might not be an issue. I will have a fix soon, in the meanwhile this should help:

```
curl -XPOST 'localhost:9200/test/_suggest?pretty=true' -d '{                                                                                                                              
  "text": "ice tea",
  "did_you_mean": {
    "phrase": {
      "field": "name_shingled",
      "gram_size": 3,
      "direct_generator": [
        {
          "field": "name",
          "max_edits": 2,
          "suggest_mode": "always",
          "min_word_len": 0,
          "prefix_len": 0,
          "min_doc_freq": 1.0
        }
      ]
    }
  }
}'
```
</comment><comment author="jtreher" created="2013-05-15T17:39:44Z" id="17954045">@s1monw Is there any chance that max_term_freq is not being obeyed as well with "always?" While this patch fixed this test issue, I actually have a situation where ice appears thousands of times and iced several hundred. I see "iced" appear from the term suggest, but it's like the phrase suggest never gets it.
</comment><comment author="s1monw" created="2013-05-15T21:01:17Z" id="17965924">`max_term_freq` is the maximum threshold (default: 0.01f) of documents a query term can appear in order to provide suggestions. Which means that if you won't even get a candidate for `ice` if the freq exceeds `max_term_freq`. Maybe I am not understanding you question right?
</comment><comment author="jtreher" created="2013-05-16T00:28:25Z" id="17974446">@s1monw  I have to set max_term_freq in the term suggest to 0.999 (99.9%) to have term show up. However, when I do this in phrase suggest, it's as though the candidate is not generated. 
</comment><comment author="jtreher" created="2013-06-07T17:08:44Z" id="19119915">@s1monw This did fix the issue I had. It seems to respect max_term_freq now with 0.9.1
</comment><comment author="s1monw" created="2013-06-07T19:07:55Z" id="19126888">@jtreher I think I did!  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Searching for different/strange signs causes SearchParseException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3036</link><project id="" key="" /><description>We have a series of integration tests for our Api. When testing against 0.90.0 we have encountered an issue that works on 0.20.x I have pasted the repro query here. It seems to stem from Lucene and when talking to Karmi on IRC he suggested I create an issue here.

```
curl -XPOST http://localhost.:9200/myindex/_search -d '{
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "\\!\\$1#&#164;%&amp;/\\(\\)=\\?\\+&#180;`\\^\\*_\\:\\;&gt;&lt;,.\\-&#168;\\~\\}\\]\\[\\{$&#163;@&#167;'\\\\&#8224;&#8225;&#8240;&#8249;&#8250;&#9824;&#9827;&#9829;&#9830;&#8254;&#8592;&#8593;&#8594;&#8595;&#8482;&#8217;&#8216;&#8218;&#8220;&#8221;&#8222;"
                }
            },
            "filter": {
                "term": {
                    "___types": "TestSupport.TestData.TypeWithAllNativePropertyTypes"
                }
            }
        }
    }
}'
```

I know there are a lot of strange signs in there and double escaping and so on but the same query works on 0.20.x with expected result.
</description><key id="14304083">3036</key><summary>Searching for different/strange signs causes SearchParseException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pecke01</reporter><labels /><created>2013-05-14T10:51:09Z</created><updated>2013-11-07T20:09:01Z</updated><resolved>2013-05-14T11:14:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-14T11:14:04Z" id="17869782">The query string parser in 0.90 supports regexes, which are marked with /

You have a /, but not followed by a valid regex, hence the error

clint

On Tue, May 14, 2013 at 12:51 PM, Marcus Granstr&#246;m &lt;notifications@github.com

&gt; wrote:
&gt; 
&gt; We have a series of integration tests for our Api. When testing against
&gt; 0.90.0 we have encountered an issue that works on 0.20.x I have pasted the
&gt; repro query here. It seems to stem from Lucene and when talking to Karmi on
&gt; IRC he suggested I create an issue here.
&gt; 
&gt; curl -XPOST http://localhost.:9200/myindex/_search -d '{
&gt; "query": {
&gt; "filtered": {
&gt; "query": {
&gt; "query_string": {
&gt; "query":
&gt; "!\$1#&#164;%&amp;/()=\?+&#180;`^*_:\;&gt;&lt;,.-&#168;~}][{$&#163;@&#167;'\&#8224;&#8225;&#8240;&#8249;&#8250;&#9824;&#9827;&#9829;&#9830;&#8254;&#8592;&#8593;&#8594;&#8595;&#8482;&#8217;&#8216;&#8218;&#8220;&#8221;&#8222;"
&gt; }
&gt; },
&gt; "filter": {
&gt; "term": {
&gt; "___types": "TestSupport.TestData.TypeWithAllNativePropertyTypes"
&gt; }
&gt; }
&gt; }
&gt; }
&gt; }'
&gt; 
&gt; I know there are a lot of strange signs in there and double escaping and
&gt; so on but the same query works on 0.20.x with expected result.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/3036
&gt; .
</comment><comment author="pecke01" created="2013-05-14T11:17:11Z" id="17869886">thx @clintongormley that makes sense. Works when I removed it.
</comment><comment author="kimchy" created="2013-05-14T11:18:42Z" id="17869939">btw, you might want to consider simply using the `match` query instead of the `query_string` one. Doesn't support the operators and additional syntax (like range) the query string does, but on the other hand, won't fail on parsing it...
</comment><comment author="pecke01" created="2013-05-14T11:21:37Z" id="17870046">This is just a test to support what already is done by customers we have using the query_string for this. The / didn't really test anything. Is the unicode characters that is the important ones. 

Also known as the Justin Bieber twitter test.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Config: Use Recovery Throttling by default in 0.90.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3035</link><project id="" key="" /><description>along the same lines as #3033 recovery throttling is very frequently used in production for cluster stability. we should use it by default as well to prevent usually unexpected impact on search performance due to recovery operations.
</description><key id="14303130">3035</key><summary>Config: Use Recovery Throttling by default in 0.90.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-14T10:19:17Z</created><updated>2013-05-30T09:02:22Z</updated><resolved>2013-05-14T13:10:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Parent-Child: Improve has_parent &amp; has_child filter execution</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3034</link><project id="" key="" /><description>Both `has_parent` and `has_child` filters are internally executed in two rounds. In the second round all documents are evaluated whilst only specific documents need to be checked. In the `has_child` case only documents belonging to a specific parent type need to be checked and in the `has_parent` case only child documents need to be checked.  
</description><key id="14300058">3034</key><summary>Parent-Child: Improve has_parent &amp; has_child filter execution</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-14T08:49:16Z</created><updated>2013-05-30T09:02:33Z</updated><resolved>2013-05-14T09:02:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Config: Use Merge Throttling by default in 0.90.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3033</link><project id="" key="" /><description>Merge Throttling is one of the most recommended settings and crucial in the RealTime indexing case. We should set the default to a  reasonable setting that allows folks to index in a production index and don't see large merge peaks by default. Yet, the default here is hard to calculate and solely relies on experience from production environments. The `right` settings depends on the actual hardware used and can't be easily predicted.

if somebody has experience with a good setting, sharing HW setup and throttle setting would be much appreciated.
</description><key id="14300013">3033</key><summary>Config: Use Merge Throttling by default in 0.90.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-14T08:48:20Z</created><updated>2013-05-30T09:02:53Z</updated><resolved>2013-05-14T13:10:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-14T09:01:30Z" id="17864540">I have been thinking about a startup script that checks if the cluster name is the default one, then offers to generate a config based on the local hardware, so eg it could:
- ask you to choose a cluster name
- recommend a heap size
- advise about swap
- do a disk test and advise about merge settings
- ???
</comment><comment author="s1monw" created="2013-05-14T09:06:53Z" id="17864775">@clintongormley can you create a different issue for this? I think this is unrelated - even with the HW spec we can't really make a better / worse choice
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Config: Raise Search ThreadPool Size to 3x availableProcessors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3032</link><project id="" key="" /><description>2x available processors seems kind of lowish and in practice 3x seems to be a sweetspot. We should raise the limit to 3x as a default value
</description><key id="14299043">3032</key><summary>Config: Raise Search ThreadPool Size to 3x availableProcessors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-14T08:27:48Z</created><updated>2014-10-15T19:44:38Z</updated><resolved>2013-05-14T13:16:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-14T13:16:18Z" id="17874984">closed via 076a1ed &amp; 09fb226
</comment><comment author="garyelephant" created="2014-10-09T09:05:31Z" id="58481806">What does `available processors` mean ?
</comment><comment author="clintongormley" created="2014-10-15T19:44:38Z" id="59264044">@garyelephant the number of processors/cores that the JVM detects on your server, or the setting of `processors` in your config file.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Config: Allow to disable allocation on the index level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3031</link><project id="" key="" /><description>Similar to the global cluster wide disable allocation flags, allow to set those on a specific index by updating its settings. The keys are the same as the cluster one, except they start with an index, for example: `index.routing.allocation.disable_allocation` set to `true`.
</description><key id="14298666">3031</key><summary>Config: Allow to disable allocation on the index level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-14T08:24:24Z</created><updated>2013-05-30T09:03:11Z</updated><resolved>2013-05-14T08:25:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>has_child fails with certain documents</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3030</link><project id="" key="" /><description>I currently have the following document structure:
- Organisation
- User (child of organisation)
- Sickness (child of user)
- Periods (nested document in sickness)

I'm trying to find User documents which have sickness documents that match a date range. However, the majority of User documents fail to be matched even though they should be returned - some documents do match though. If I search directly for Sickness documents, the correct ones are returned.

The query I'm using is like this:
query: {
  bool: {
    must: [
    {
      bool: {
        must: [
        {
          bool: {
            must: [
            {
              range: {
                periods.start: {
                  to: "2013-05-13 19:29:29"
                }
              }
            },
            {
              range: {
                periods.end: {
                  from: "2013-05-13 19:29:29"
                }
              }
            }
          ]
          }
      }
      ]
      }
    }
    ]
  }
}

If, however, I remove the Organisation document (and specify that User documents have no parent), then the query works absolutely fine and all the correct User documents are returned.

Is there some issue with using has_child on documents that also have a parent document?
</description><key id="14274838">3030</key><summary>has_child fails with certain documents</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nmpolo</reporter><labels /><created>2013-05-13T18:34:20Z</created><updated>2013-05-13T18:45:34Z</updated><resolved>2013-05-13T18:45:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-13T18:35:57Z" id="17831385">Is there a chance for a full curl recreation for this? Including setting up the mappings, indexing some sample data, and then executing the search request? Without it, its hard to figure out if its a user error or an actual problem in elasticsearch (and if it is, helps speed up the fix).
</comment><comment author="clintongormley" created="2013-05-13T18:39:42Z" id="17831722">Using two levels of parent-child relations can be tricky.  Parents and children need to be indexed on the same shard for the parent-child relationship to work.  This is usually achieved by setting the `parent` parameter when indexing the child, which in effect sets the `routing` parameter for the child.

However, your `user` docs are children of the `organization`, so they already have a custom `routing` value.  When you index your `sickness` docs, you are setting the `parent` value, but this no longer contains the correct `routing`. Instead you need to set the `parent=$parent_id` AND `routing=$Grandparent_id`.  That way, related organization, user and sickness docs are all stored on the same shard.
</comment><comment author="nmpolo" created="2013-05-13T18:45:34Z" id="17832084">@clintongormley Thanks for your reply. Setting routing = $grandparent_id solved my issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not load the ids of child documents into memory.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3029</link><project id="" key="" /><description>PR for #3028
</description><key id="14265032">3029</key><summary>Not load the ids of child documents into memory.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-05-13T15:08:44Z</created><updated>2015-05-18T23:41:57Z</updated><resolved>2013-05-14T08:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Parent-Child: Improve memory usage id cache</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3028</link><project id="" key="" /><description>The id cache loads the ids of all documents. If we only load the parent ids, then we can reduce the memory usage of the parent/child support.

Update: The amount of memory that will be reduced when upgrading to version `0.90.1` depends on the amount of child documents in an index. For example if you have one child doc for every parent doc, then it the memory usage of the id cache should be reduced by around half. The more child docs per parent doc the larger the difference. 
</description><key id="14264603">3028</key><summary>Parent-Child: Improve memory usage id cache</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-13T15:00:39Z</created><updated>2013-06-10T15:15:05Z</updated><resolved>2013-05-14T07:58:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Version check on update?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3027</link><project id="" key="" /><description>I'm trying to use version checking with document updates using the Java API, but I'm not sure how; there is no 'setVersion' option. This is a feature supported by HTTP, as http://www.elasticsearch.org/2011/02/08/versioning/ show an update example with a version parameter.

Also, Just to clarify, when I read http://www.elasticsearch.org/guide/reference/api/update/ :

"this operation still means full reindex of the document, it just removes some network roundtrips and reduces chances of version conflicts between the get and the index"

; I assume the 'get' and 'index' are between ES and lucene, rather than between my client and ES? Otherwise I can't see what roundtrips are saved.
</description><key id="14263098">3027</key><summary>Version check on update?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Chris2048</reporter><labels><label>non-issue</label></labels><created>2013-05-13T14:31:32Z</created><updated>2013-05-14T16:10:56Z</updated><resolved>2013-05-14T13:58:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-14T13:58:09Z" id="17877489">versions are checked automatically if specified. The java API provides a method for this in `IndexRequestBuilder`
something like `client.prepareIndex("test", "type1", "1").setVersion(1)`  should work. The existing version must be `&lt;` the specified version. Please report such questions to the mailing list first rather than creating an issue. It's not a big deal though we just try to separate questions from issues which are rather for bugs or features. I am closing this since you are discussing on the mailing list already. 
</comment><comment author="Chris2048" created="2013-05-14T16:10:56Z" id="17886616">Index and Delete request builders have 'setVersion' methods of this nature, but Update request builders don't. Do update builders contain Index request objects somewhere within?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent-Child: Make score mode naming consistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3026</link><project id="" key="" /><description>The `top_children`, `has_child`, `has_parent` and `nested` query have score modes that define how the scores are pushed to the parent or child hits. All these queries have different parameter names for the score mode behaviour. These queries should all support `score_mode` in addition to the score mode parameter name already supported. 
</description><key id="14249516">3026</key><summary>Parent-Child: Make score mode naming consistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-13T08:11:12Z</created><updated>2013-05-30T09:03:33Z</updated><resolved>2013-05-13T08:30:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Enabling offsets in MemoryIndex</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3025</link><project id="" key="" /><description>This allows for reusing the memory index pool for more advanced usages from consumer applications, for example through a highlighting percolator (see separate PR)
</description><key id="14233954">3025</key><summary>Enabling offsets in MemoryIndex</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-05-12T12:05:54Z</created><updated>2014-07-16T21:53:24Z</updated><resolved>2013-06-01T20:13:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-06-01T19:18:58Z" id="18795324">under which case do you need offsets with the current percolator?
</comment><comment author="synhershko" created="2013-06-01T19:35:29Z" id="18795594">Highlighting Percolator (tm), see separate pr

Though we are seeing some issues with the current impl and probably will be
moving away from it.

Any eta on the new impl?
On Jun 1, 2013 10:19 PM, "Shay Banon" notifications@github.com wrote:

&gt; under which case do you need offsets with the current percolator?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/3025#issuecomment-18795324
&gt; .
</comment><comment author="kimchy" created="2013-06-01T20:13:12Z" id="18796221">that's what I thought, so obviously this is not really needed for current features exposed by percolator, and it's not going to be enabled for highlighting which one can't really do, they do come with an overhead.

we will probably start to design the new implementation relatively soon, will open an issue with our thoughts about it once someone gets around to look at it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MatchQueryParser doesn't allow field boosting on query when included in a _GET request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3024</link><project id="" key="" /><description>In the following query, we try to boost the FirstLast field by 7 and it fails:

/_search?q=(FirstLast%3A"johnsmith")^7&amp;explain=true
...
_explanation": {

```
"value": 10,
"description": "weight(FirstLast:johnsmith in 0) [PerFieldSimilarity], result of:"
```

This behavior works correctly in the MultiMatchQueryParser:
_search?q=(FirstLast%3A"johnsmith")^7+(State%3A"wa")&amp;explain=true
...
"_explanation": {

```
"value": 80,
"description": "sum of:",
"details": [
    {
        "value": 70,
        "description": "weight(FirstLast:johnsmith^7.0 in 0) [PerFieldSimilarity], result of:",
```

Note that the boost does work when the request is a _POST as so:
_search" -d '{"query":{"term":{"State":{"value":"wa","boost":7.0}}}}'
{"took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":70.0,"hits":[{"_index":"3","_type":"people","_id":"XXXXXXXXXX","_score":70.0, "_source" : {"FirstLast":["johnsmith"],"State":["wa"]}}]}}

I'm just getting used to the code and have no debugger setup yet so forgive me if I'm mistaken or incorrectly using this.
</description><key id="14213752">3024</key><summary>MatchQueryParser doesn't allow field boosting on query when included in a _GET request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mikedangelo</reporter><labels><label>breaking</label><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-10T23:24:10Z</created><updated>2013-05-21T09:00:47Z</updated><resolved>2013-05-21T09:00:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-05-20T22:05:20Z" id="18177051">Tracked this down.  Looks like the parsed query boost is getting reset to the default of 1.0 on this line:

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java#L217

@kimchy @s1monw Should this line look something like the following?

```
query.setBoost(query.getBoost() * qpSettings.boost());
```

If yes, I can open a pull request.

Thanks,
Matt Weber
</comment><comment author="s1monw" created="2013-05-21T08:35:51Z" id="18195723">yeah so the reason why this fails is that the query that we boost with the qpSettings is the topLevelQuery. The fix looks good to me. I will add some more tests and pull it in! thanks matt
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow AnalyzeRequestBuilder to be given an arbitrary text array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3023</link><project id="" key="" /><description>The AnalyzeRequestBuilder is useful for seeing exactly how a text string value will be analyzed. But when a document can contain multiple values, it's often difficult to guess the tokens and token positions that result when ES is indexing a multi-valued field.

It would be very helpful to have the AnalyzeRequestBuilder accept an array of String objects, or perhaps even a JSON document with some restrictions, so that a real-world multi-valued document field is more accurately modeled and analyzed.
</description><key id="14205368">3023</key><summary>Allow AnalyzeRequestBuilder to be given an arbitrary text array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/johtani/following{/other_user}', u'events_url': u'https://api.github.com/users/johtani/events{/privacy}', u'organizations_url': u'https://api.github.com/users/johtani/orgs', u'url': u'https://api.github.com/users/johtani', u'gists_url': u'https://api.github.com/users/johtani/gists{/gist_id}', u'html_url': u'https://github.com/johtani', u'subscriptions_url': u'https://api.github.com/users/johtani/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/1142449?v=4', u'repos_url': u'https://api.github.com/users/johtani/repos', u'received_events_url': u'https://api.github.com/users/johtani/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/johtani/starred{/owner}{/repo}', u'site_admin': False, u'login': u'johtani', u'type': u'User', u'id': 1142449, u'followers_url': u'https://api.github.com/users/johtani/followers'}</assignee><reporter username="">brian-from-fl</reporter><labels><label>:Analysis</label><label>enhancement</label><label>v2.0.0-beta1</label></labels><created>2013-05-10T19:20:29Z</created><updated>2015-05-15T11:40:25Z</updated><resolved>2015-05-15T11:21:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-14T13:15:40Z" id="17874943">I referenced the wrong issue from this commit... reopening
</comment><comment author="clintongormley" created="2014-08-08T11:56:26Z" id="51591995">This could be supported when #5866 is implemented
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return matching nested inner objects per hit</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3022</link><project id="" key="" /><description>Add support for including the matching nested inner objects per hit element.
</description><key id="14194830">3022</key><summary>Return matching nested inner objects per hit</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>:Search</label><label>feature</label></labels><created>2013-05-10T15:05:28Z</created><updated>2016-08-10T07:51:44Z</updated><resolved>2014-12-02T11:02:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="eranid" created="2013-05-28T11:49:09Z" id="18545503">+1
</comment><comment author="roeena" created="2013-05-28T12:02:27Z" id="18546038">+1
</comment><comment author="btiernay" created="2013-05-28T14:28:27Z" id="18553979">I'm curious on the intended behaviour of this feature: 
- Will it be possible to do a global sort, offset, limit based on properties of the child?
- Will it be possible to return the matching child AND parent?

The answers to these questions will have implications in how we proceed in implementing our current application. 
Thanks!
</comment><comment author="brusic" created="2013-05-28T17:05:09Z" id="18564777">Sorting on nested documents has been supported since the 0.90 release: https://github.com/elasticsearch/elasticsearch/issues/2662

Nested queries always returns the parent so I am assuming the behavior will remain the same. Hopefully this feature will have many settings, similar to most other elasticsearch features.

And I hate sounding like a broken record, but can we please stop with the +1s? The elasticsearch team is not influenced by them and they only create noise.
</comment><comment author="btiernay" created="2013-05-28T17:29:50Z" id="18566459">&gt; Sorting on nested documents has been supported since the 0.90 release: #2662

By "global sort", a mean without regard to parent-nested relationship. That is, it is possible to return sorted children which may not be contiguous with respect to their parent. For example:

```
Hit 1. nested1,1 -&gt; parent1
Hit 2. nested2,1 -&gt; parent2
Hit 3. nested1,2 -&gt; parent1
```

Notice how different parents are interleaved.

&gt; Nested queries always returns the parent so I am assuming the behavior will remain the same. Hopefully this feature will have many settings, similar to most other elasticsearch features.

It would be nice to have flexibility here as you describe.

&gt; And I hate sounding like a broken record, but can we please stop with the +1s? The elasticsearch team is not influenced by them and they only create noise.

Message received, sorry about that.
</comment><comment author="brusic" created="2013-05-28T18:12:12Z" id="18569259">IMHO, your use case is better suited for parent/child documents and not nested documents. The way I see things is that inner/nested documents always form a single document with the outer/parent document. The inner/nested documents never appear separately. This feature breaks that model slightly by not returning certain nested documents, but the parent is always the same. Of course, I do not work for elasticsearch so my views and thoughts have no bearing on the issue. :)

BTW, there was nothing wrong with your comment. Adding discussion to an issue via a concrete use case provides value and is the type of comment we should be seeing. A comment with nothing but +1 does not provide value. Perhaps I should just create an email filter that ignores github messages with only +1.
</comment><comment author="eranid" created="2013-05-28T18:36:05Z" id="18570790">Parent-Child has the problem of using ALOT of in-memory for the joins.
I was using it at first, but as the index grew to hundreds of GB, it became a memory and CPU monster.
When most of my queries are "get me the photos that were tagged with certain tags with some value in a range of dates" (the nested document is the tag)
I have to use either parent-child or nested. 

Since there might be lots of tags per photo, I want to get just the relevant tags (don't care about getting the parent really, though I'd rather not).

Parent-Child just can't handle this. with 7GB of memory, The machine takes forever to do the joins, and sometimes crashes.

Also, I did not know the +1 was a bother. I thought it helped you guys prioritize features.
My apologies. Will spread the word.
</comment><comment author="brusic" created="2013-05-28T20:53:01Z" id="18579284">I never said parent-child was efficient, just that its functionality is better suited to your use case. :) Even if nested documents eventually supported your use case, the overhead of sorting will also be it grossly inefficient. Each parent document would need to be scored several times.

As far as +1 goes, there has been some discussion about them. There are a few issues that are 2-3 years old that have hundreds of +1s. You can make the judgement if they are effective or not. I am not on the elasticsearch team so everyone should follow their advice on proper github etiquette and not mine. :)
</comment><comment author="btiernay" created="2013-05-29T12:22:06Z" id="18612557">&gt; Even if nested documents eventually supported your use case, the overhead of sorting will also be it grossly inefficient. Each parent document would need to be scored several times.

This may be true given what lucene currently supports for `BlockJoinQuery` and `BlockJoinCollector`. This is a good article describing the basics: http://blog.mikemccandless.com/2012/01/searching-relational-content-with.html?m=1

&gt; The join can currently only go in one direction (mapping child docIDs to parent docIDs), but in some cases you need to map parent docIDs to child docIDs. For example, when searching songs, perhaps you want all matching songs sorted by their title. You can't easily do this today because the only way to get song hits is to group by album or band/artist. 
</comment><comment author="martijnvg" created="2013-05-30T20:13:05Z" id="18705079">@btiernay @brusic  The idea is that the nested inner objects hits are included in the root doc hit. Something like this:

```
"hits" : [ {
      "_index" : "test",
      "_type" : "type1",
      "_id" : "1",
      "_score" : 1.584377, "_source" : ....,
      "nested_hits" : {
        "total" : 2,
          "max_score" : 1.6391755,
          "hits" : [ {
            "_offset" : 1, 
            "_score" : 1.6391755, "_source" : ...
          }, {
            "_offset" : 0,
            "_score" : 1.5295786, "_source" : ...
          } ]
      }
}
```

In the above case `_offset` is nested field's array offset in the _source. 

It should be possible to specify a global sort and a sort inside the root document and what to show per nested hit (the complete inner object based on the source or just some fields). In addition supporting highlighting and other per hit features makes a lot of sense as well.

@eranid The memory usage of the parent/child have been reduced in the new `0.90.1` version. Hopefully parent/child queries can work now better in your environment.
</comment><comment author="brusic" created="2013-05-30T21:48:43Z" id="18710717">@martijnvg, so the full source will still be returned? The nested hits is a great idea in terms of flexibility and makes more sense than editing the source (which I referred to above in "breaking the model"), I just hope that it is efficient. I have some convoluted logic to deal with filtering nested documents on the client side, and the serialization/deserialization using Jackson is a bit of a performance hit.

Can scoring be avoid on the nested hits results? My use case calls for scoring using the fields in the parent document, but only filtering the nested documents. Not sure if you thought of this scenario, but a flexible scoring model would be a great feature.
</comment><comment author="martijnvg" created="2013-05-30T22:13:19Z" id="18711879">@brusic The full source can optionally returned if that is requested, but it isn't necessary. The source of the nested inner object will be separately returned, but is based on the source in the root document. The source can also be disabled and individual fields can be separately be set to stored in the mapping, these individual fields can then be requested instead of the source. 

The overhead of fetching inner nested objects should be small. This should be done in the fetch phase (so only for the competitive root docs) by re-executing the inner query of the nested query only on the nested docs of the root docs to be retrieved (a big filter).

Not sure what you mean with the avoiding the scoring on neste hits. Just use a field from the parent for scoring via sorting by script?
</comment><comment author="btiernay" created="2013-05-31T02:55:06Z" id="18720818">@martijnvg: Very nice proposal. A couple of clarifications:

&gt; It should be possible to specify a global sort and a sort inside the root document

When you say "global sort" do you mean global with respect to the root document, or with respect to nested documents? I could see how you might be implying the ability to do either. 

&gt; ...based on the sort or just some fields

I assume you mean "source" not "sort"?
</comment><comment author="btiernay" created="2013-05-31T02:58:13Z" id="18720892">@brusic: With respect to:

&gt; The nested hits is a great idea in terms of flexibility and makes more sense than editing the source (which I referred to above in "breaking the model"), I just hope that it is efficient.

I think this really depends on the size and structure of your documents. We have some very large documents (deep and wide) for which the ability to return the nested documents without "editing" the source would be _much_ more efficient.
</comment><comment author="clintongormley" created="2013-05-31T07:27:42Z" id="18728438">@eranid to add to what @martijnvg said: up until 0.90.1, parent-child relationships required the parent IDs and child IDs to be held in memory. From 0.90.1 onwards, only the parent IDs need to be held in memory.  This is a massive saving and should make parent-child much lighter.
</comment><comment author="martijnvg" created="2013-05-31T19:43:57Z" id="18767490">@btiernay The global sort is with respect to the root document. You could use nested sorting as global sorting which will base the ordering of root docs based on aggregate sort values from the nested inner objects.

&gt; I assume you mean "source" not "sort"?

 Yes, I meant source.
</comment><comment author="martijnvg" created="2013-05-31T20:15:26Z" id="18769045">We definitely want to get this feature in, but in order get in it in right, a refactoring is needed in the fetch phase.
The fetch phase needs to have "a hit in a hit" concept (inner hits), that should cover both nested hits and getting child hits as part of the parent hit. All features that currently work on normal hits like for example explain, highlighting, fields and partial fields should also work for inner hits (if applicable).
</comment><comment author="btiernay" created="2013-06-01T16:29:22Z" id="18792152">@martijnvg To be clear, I suppose there would be no way of inverting the relationship to sort globally based on nested docs (effectively ignoring the root-nested grouping) globally? If so, is this due to a Lucene imposed limitation?
</comment><comment author="martijnvg" created="2013-06-05T09:49:56Z" id="18965721">@btiernay You can sort globally based on the nested docs with the current nested sorting support. The global nested sorting won't be changed when inner hits are added that allows to sort nested hits per root / main document hit. Makes sense?
</comment><comment author="btiernay" created="2013-06-17T18:58:35Z" id="19566609">@martijnvg: Sorry for being so dense here, but it is still unclear if I can return nested docs as the root document using this approach. Then, I would be able to sort by the nested doc, without regard to parents, very similar to how parent-child relationships work.
</comment><comment author="martijnvg" created="2013-06-17T19:58:33Z" id="19570462">@btiernay No, with this approach the nested inner objects can't be a root document on its own. Nested inner objects are always part of the root document.
</comment><comment author="btiernay" created="2013-06-17T21:48:40Z" id="19577355">@martijnvg: Thanks again for the clarification. Much appreciated. I realize your answer / solution is consistent with the other aspects of nested docs (e.g whole part relationships). However, I'm very curious if my proposal is technically feasible since I think it could be very powerful and more performant than the alternative parent-child approach. 
</comment><comment author="martijnvg" created="2013-06-18T07:35:41Z" id="19595500">@btiernay I think your idea is technically possible. Right now the inner nested objects don't have a unique identifier like regular root document have. In theory we could use the path + the offset in the nested array as additional data to the root documents's unique identifier for the inner nested object's unique key.

Also inner objects are tightly coupled to the lifecycle of the root document. If a root document is removed all the nested inner objects (which are stored as separate Lucene documents) are removed as well. Updating or adding individual nested inner objects isn't possible without reindexing the root document and all other nested inner objects (Lucene document block). If nested inner objects were exposed as independent hits in the search result, I guess the fact that these hits have limitations would be confusing.
</comment><comment author="btiernay" created="2013-06-18T11:08:13Z" id="19604496">That gives me hope then :)

&gt; we could use the path + the offset in the nested array as additional data to the root documents's unique identifier for the inner nested object's unique key

That's an interesting idea. I hadn't thought about the `id` field. I like it :)

&gt; If nested inner objects were exposed as independent hits in the search result, I guess the fact that these hits have limitations would be confusing.

Perhaps, but consider "write once" applications in which the documents rarely, (if ever) change. Given the potential speedup / memory improvements that can be achieved using block documents (especially for deeply nested or wide documents), it would be a shame to not expose this functionality.
</comment><comment author="julianhille" created="2013-09-16T12:58:12Z" id="24507645">any progress on this one? cause i'd love to see this.
Otherwise any etimated time or any way to help out?
</comment><comment author="GabrielKast" created="2013-10-07T21:51:16Z" id="25848480">I would also like to know if there is any progress on that feature. Any way we could help out?
I have more or less the same use case as described. I wouls like to select some children in a tree of data where the chlildren have sense only when they are included in their parent. (The use case is : I have a company, with many establisments linked to that company, I would like to query/retrieve the establishment based on their geographical position. The position belongs to the children, but all the "good data" are linkde to the parent document ie the Company) 
I can manage to do something with parent/children, but I need to duplicate some data from the parent to the children and vice-versa.
Another way to avoid issues would be to be able to embed the parent in a query with a "has_parent"/embed the child in a query with a "has_child". I know it's not in the perimeter of that issue but maybe it's a simpler idea?
I have the intuition that nested_hits would be a faster solution.
Something might also be difficult (I am not familiar with ES internals..) : how do you compute the "nested_count" ? to know how many are the nested hits. Maybe it's more of a parent/children feature.
(please be kind if I'm a little clumsy I don't usually post comment on github :) ) 
</comment><comment author="gpstathis" created="2013-10-10T17:55:41Z" id="26075908">+1, will be great to have this, right now we are using nested queries and have to filter the sub-docs at the app layer.
</comment><comment author="martijnvg" created="2013-10-14T11:52:25Z" id="26250350">@julianhille @GabrielKast @gpstathis I'm working on this feature. I have an implementation that works for nested inner objects: https://github.com/martijnvg/elasticsearch/commits/nested_fields

The goal is to put this into a more generic framework (in FetchPhase), so that the notion of the inner hits also works with parent/child and that an inner hit has the same set of fetch features as a normal hit (for example highlighting and matched queries, explain etc).

If you take a look at the NestedHitsTests test in the mentioned branch, you can see how it can be used.

There is no ETA for this feature yet.
</comment><comment author="gpstathis" created="2013-10-14T15:19:46Z" id="26262801">@martijnvg this goes above and beyond what I was hoping to see. Since the nested hits are sorted, having their original offset in the source array makes total sense and is a nice touch.

The only thing I'm having a hard time following (and it's probably due to my lack of understanding of the internals) are the key values for the Map returned by nestedHits(). E.g.

``` java
[...]
assertThat(response.getHits().getHits()[1].nestedHits().get("1").getHits().length, equalTo(1));
[...]
```

Is the key string value "1" an internal field name? Apologies in advance if there is an obvious answer to this that I'm missing.
</comment><comment author="brusic" created="2013-10-14T17:15:01Z" id="26272093">"martijnvg authored 10 months ago"

You are breaking my heart! :) 

How does the new aggregation framework tie into this feature? Not an issue or are you designing around it?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MVEL VerifyError with update API after restarting elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3021</link><project id="" key="" /><description>Original report came from the mailing list at

https://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/verifyerror/elasticsearch/noCI7fBRsHw/t4rSML4D8jcJ

I'll put the gist here for readability (orig at https://gist.github.com/baxford/5551877)

```
curl -XDELETE 'http://localhost:9200/app'
curl -XPUT 'http://localhost:9200/app' -d '
{
    "mappings" : {
        "parent": {
            "properties" : {
                "children": {
                    "type" : "nested",
                    "properties" : {
                        "id" : {"store" : false, "type": "long", "include_in_all": false},
                        "sent" : {"store" : false, "type": "long", "include_in_all": false}
                    }
                }
            }
        }               
    }
}'

curl -XPUT 'http://localhost:9200/app/parent/2nested' -d '
{
    "children": [
        {
            "id": 1967,
            "sent": 1367561477819
        },
        {
            "id": 1968,
            "sent": 1367723988849
        }
    ]
}
'
curl -XPUT 'http://localhost:9200/app/parent/manyNested' -d '
{"children":[{"id":1970,"sent":1367557829592},{"id":1967,"sent":1367561477651},{"id":1963,"sent":1367566046098},{"id":2220,"sent":1367574093902},{"id":2221,"sent":1367574178118},{"id":1949,"sent":1367577441224},{"id":2230,"sent":1367578681929},{"id":2255,"sent":1367582445865},{"id":2229,"sent":1367582978773},{"id":2036,"sent":1367586060714},{"id":2181,"sent":1367588265555},{"id":2135,"sent":1367589621826},{"id":2040,"sent":1367590988082},{"id":2265,"sent":1367597111092},{"id":1961,"sent":1367597766420},{"id":2368,"sent":1367607084937},{"id":2325,"sent":1367604622108},{"id":2380,"sent":1367632844989},{"id":2449,"sent":1367637386597},{"id":2162,"sent":1367639335040},{"id":2457,"sent":1367640147955},{"id":2359,"sent":1367640976257},{"id":1995,"sent":1367642443645},{"id":2106,"sent":1367644004789},{"id":2483,"sent":1367645257597},{"id":2244,"sent":1367643998849},{"id":2070,"sent":1367645763061},{"id":1955,"sent":1367648027479},{"id":2413,"sent":1367647150696},{"id":2432,"sent":1367644680952},{"id":2236,"sent":1367645491674},{"id":2154,"sent":1367648593594},{"id":2017,"sent":1367645542935},{"id":2437,"sent":1367649600428},{"id":2447,"sent":1367648433362},{"id":2115,"sent":1367650722908},{"id":2147,"sent":1367649384788},{"id":2415,"sent":1367652386667},{"id":2490,"sent":1367660500351},{"id":2431,"sent":1367660240898},{"id":2565,"sent":1367661020928},{"id":2501,"sent":1367661971510},{"id":2078,"sent":1367663056515},{"id":2184,"sent":1367662922221},{"id":2583,"sent":1367664149452},{"id":2195,"sent":1367664839517},{"id":2562,"sent":1367664317481},{"id":2286,"sent":1367664384121},{"id":2462,"sent":1367667431564},{"id":2332,"sent":1367669188915},{"id":2143,"sent":1367674582587},{"id":1957,"sent":1367675023621},{"id":2238,"sent":1367685309996},{"id":2074,"sent":1367687406146},{"id":2374,"sent":1367686493566},{"id":2336,"sent":1367693189820},{"id":2317,"sent":1367693900534},{"id":2095,"sent":1367695156486},{"id":1947,"sent":1367699112244},{"id":2031,"sent":1367699831117},{"id":2356,"sent":1367699040377},{"id":2492,"sent":1367699156344},{"id":2407,"sent":1367688683169},{"id":2122,"sent":1367699317524},{"id":2049,"sent":1367699998534},{"id":2352,"sent":1367705893143},{"id":2611,"sent":1367704332534},{"id":2463,"sent":1367705247247},{"id":2295,"sent":1367705743731},{"id":2428,"sent":1367707924514},{"id":2713,"sent":1367711489640},{"id":1998,"sent":1367688901861},{"id":2218,"sent":1367715074441},{"id":2438,"sent":1367688967128},{"id":2668,"sent":1367715433513},{"id":2429,"sent":1367716384220},{"id":2081,"sent":1367717543664},{"id":2232,"sent":1367717580481},{"id":2405,"sent":1367718543453},{"id":2133,"sent":1367702124091},{"id":2185,"sent":1367709922527},{"id":2053,"sent":1367720072185},{"id":2409,"sent":1367711905080},{"id":2489,"sent":1367712328854},{"id":2402,"sent":1367713280879},{"id":2007,"sent":1367714124287},{"id":2704,"sent":1367714511892},{"id":2388,"sent":1367715713018},{"id":2360,"sent":1367716008115},{"id":2004,"sent":1367721618552},{"id":2508,"sent":1367717122370},{"id":2027,"sent":1367717322516},{"id":2502,"sent":1367721633478},{"id":2697,"sent":1367718063199},{"id":2525,"sent":1367722480411},{"id":1974,"sent":1367722629388},{"id":2165,"sent":1367718180288},{"id":2695,"sent":1367718702869},{"id":1976,"sent":1367719463806},{"id":1968,"sent":1367721068881},{"id":2030,"sent":1367722280029},{"id":2300,"sent":1367725603161}]}
'

# Stopping ES, restarting it, then executing the following seems to cause the VerifyError in script compilation 
# TransportUpdateActiong.shardOperation, line 310
# when executing script.run();
curl -XPOST 'http://localhost:9200/app/parent/manyNested/_update' -d '{
    "script" : 
        "if(ctx._source[\"children\"] != null) { for (int i = 0; i &lt; ctx._source.children.size(); i++){        if(ctx._source.children[i].id == child_id){ctx._source.children.remove(i);i--;}}}        if (ctx._source[\"children\"] == null) { ctx._source.children = [{\"id\": child_id, \"sent\": timestamp }] } else {ctx._source.children += [{\"id\": child_id, \"sent\": timestamp }] }"
    ,
    "params" : {
        "parent_id" : "1183146090417",
        "timestamp" : 1367722894963,
        "child_id" : 2030
    },
    "upsert" : {
        "children": [
            {
                "id": 2030,
                "sent": 1367722894963
            }
        ]
    }
}' &amp;&amp; echo

# However, the same issue doesn't seem to happen to a doc with less nested docs
curl -XPOST 'http://localhost:9200/app/parent/2nested/_update' -d '{
    "script" : 
        "if(ctx._source[\"children\"] != null) { for (int i = 0; i &lt; ctx._source.children.size(); i++){        if(ctx._source.children[i].id == child_id){ctx._source.children.remove(i);i--;}}}        if (ctx._source[\"children\"] == null) { ctx._source.children = [{\"id\": child_id, \"sent\": timestamp }] } else {ctx._source.children += [{\"id\": child_id, \"sent\": timestamp }] }"
    ,
    "params" : {
        "parent_id" : "1183146090417",
        "timestamp" : 1367722894963,
        "child_id" : 2030
    },
    "upsert" : {
        "children": [
            {
                "id": 2030,
                "sent": 1367722894963
            }
        ]
    }
}' &amp;&amp; echo
```

Haven't taken a deeper look at it, but can confirm the curl call hangs and there is this exception in the logs:

```
**** COMPILER BUG! REPORT THIS IMMEDIATELY AT http://jira.codehaus.org/browse/mvel2
Expression: if(ctx._source["children"] != null) { for (int i = 0; i &lt; ctx._source.children.size(); i++){        if(ctx._source.children[i].id == child_id){ctx._source.children.remove(i);i--;}}}        if (ctx._source["children"] == null) { ctx._source.children = [{"id": child_id, "sent": timestamp }] } else {ctx._source.children += [{"id": child_id, "sent": timestamp }] }
Exception in thread "elasticsearch[Acrobat][index][T#1]" java.lang.VerifyError: (class: ASMAccessorImpl_10448523421368185357780, method: getValue signature: (Ljava/lang/Object;Ljava/lang/Object;Lorg/elasticsearch/common/mvel2/integration/VariableResolverFactory;)Ljava/lang/Object;) Expecting to find integer on stack
    at java.lang.Class.getDeclaredConstructors0(Native Method)
    at java.lang.Class.privateGetDeclaredConstructors(Class.java:2398)
    at java.lang.Class.getConstructor0(Class.java:2708)
    at java.lang.Class.newInstance0(Class.java:328)
    at java.lang.Class.newInstance(Class.java:310)
    at org.elasticsearch.common.mvel2.optimizers.impl.asm.ASMAccessorOptimizer._initializeAccessor(ASMAccessorOptimizer.java:725)
    at org.elasticsearch.common.mvel2.optimizers.impl.asm.ASMAccessorOptimizer.compileAccessor(ASMAccessorOptimizer.java:859)
    at org.elasticsearch.common.mvel2.optimizers.impl.asm.ASMAccessorOptimizer.optimizeAccessor(ASMAccessorOptimizer.java:243)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.optimize(DynamicGetAccessor.java:90)
    at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:64)
    at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)
    at org.elasticsearch.common.mvel2.ast.BinaryOperation.getReducedValueAccelerated(BinaryOperation.java:108)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:38)
    at org.elasticsearch.common.mvel2.ast.IfNode.getReducedValueAccelerated(IfNode.java:73)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:38)
    at org.elasticsearch.common.mvel2.ast.ForNode.getReducedValueAccelerated(ForNode.java:67)
    at org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:38)
    at org.elasticsearch.common.mvel2.ast.IfNode.getReducedValueAccelerated(IfNode.java:74)
    at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:85)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)
    at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:113)
    at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelExecutableScript.run(MvelScriptEngineService.java:130)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:310)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:211)
    at org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:85)
    at org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$1.run(TransportInstanceSingleOperationAction.java:191)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:680)
```
</description><key id="14187782">3021</key><summary>MVEL VerifyError with update API after restarting elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-05-10T11:41:42Z</created><updated>2014-07-23T12:51:22Z</updated><resolved>2014-07-23T12:51:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-05-10T14:30:30Z" id="17723352">Here is the script that reproduces the error without restarting elasticsearch - https://gist.github.com/imotov/5554678. It looks like some really bizarre MVEL issue. The failure occurs when this expression `ctx._source.children[i].id == child_id` evaluates into true. But, here is an interesting part, if I run this [script](https://gist.github.com/imotov/55a99a2d2ea9fcd2cde0) on the same node once, the problem can be no longer reproduced.

My recommendation for @baxford would be to switch to javascript. 
</comment><comment author="clintongormley" created="2014-07-23T12:51:22Z" id="49868889">MVEL is deprecated.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting: Nested filter with nested sorting doesn't use missing value.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3020</link><project id="" key="" /><description>Reuse the missing support for fields (number based fields) inside nested object when `nested_filter` doesn't match a inner object. 
</description><key id="14183105">3020</key><summary>Sorting: Nested filter with nested sorting doesn't use missing value.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-10T08:46:39Z</created><updated>2013-05-30T09:03:46Z</updated><resolved>2013-05-16T08:14:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unable to leave TTL field blank so that some documents do not expire</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3019</link><project id="" key="" /><description>I am using the per document TTL. Each document has a different TTL, some document should have no TTL as they should not expire

```
curl -X POST "http://0.0.0.0:9200/feeds7" -d '{
  "id": 7,
  "title": "Images in Medicine",
  "url": "http:\/\/www.google.com",
  "state": "subscribed",
  "_ttl": null
}'
```

I get the following error: 

```
{
  "error": "MapperParsingException[Failed to parse]; nested: JsonParseException[Current token (VALUE_NULL) not numeric, can not use numeric value accessors\n at [Source: [B@5452edac; line: 1, column: 100]]; ",
  "status": 400
}
```

I have also tried, 0 and -1 but as you can see [here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/internal/TTLFieldMapper.java#L179) neither of those are allowed.
</description><key id="14175951">3019</key><summary>Unable to leave TTL field blank so that some documents do not expire</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brupm</reporter><labels /><created>2013-05-10T02:24:00Z</created><updated>2013-05-14T08:45:23Z</updated><resolved>2013-05-14T08:45:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-10T06:58:02Z" id="17706921">Hey,

if you do not set a default expiry in your mapping and do not set the _ttl field at all in you document, the document will not expire. See
http://www.elasticsearch.org/guide/reference/mapping/ttl-field/
</comment><comment author="brupm" created="2013-05-10T15:57:51Z" id="17728582">@spinscale the problem is when only some of the documents in the index do not expire while others do. I enabled TTL left the default blank and added a field to the mappings called _ttl which, when I try to leave the field blank for the documents I do not want to expire, I get the error.
</comment><comment author="imotov" created="2013-05-13T11:55:45Z" id="17807060">@brupm Instead of leaving the _ttl field blank, don't include this field at all. See https://github.com/imotov/elasticsearch-test-scripts/blob/master/ttl.sh for example.
</comment><comment author="brupm" created="2013-05-13T16:33:47Z" id="17823599">@imotov brillian, thank you. This isn&#8217;t very clear from the docs. 
</comment><comment author="spinscale" created="2013-05-14T08:45:23Z" id="17863840">@brupm Sorry for not being clear in my last comment.
Closing this one, as it is not an issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reject update request that has both script and doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3018</link><project id="" key="" /><description>See #2967.
</description><key id="14165634">3018</key><summary>Reject update request that has both script and doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">akahn</reporter><labels /><created>2013-05-09T20:56:57Z</created><updated>2014-07-16T21:53:26Z</updated><resolved>2013-05-10T14:45:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-10T14:45:25Z" id="17724243">Thanks! Pushed to 0.90 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Invalid internal transport message format between 0.20.6 master and client caused re-election</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3017</link><project id="" key="" /><description>A client connecting to a master resulted in 

```
java.io.StreamCorruptedException: invalid internal transport message format
```

Both client and server are running 0.20.6 (stack trace from the master below). This seems to be a truncated message (client died or network problem), but it's difficult to say. It looks to have badly hung the master, which became unresponsive on the 9300 port but was honouring requests on 9200, claiming the cluster was green.  In the meantime other nodes in the cluster were failing to get responses from it. The two other master eligible nodes in the cluster excised it and performed a re-election a few minutes afterwards. The original master was unable to join the cluster until it was restarted - up to that point it was reporting the cluster as green. 

What happened after the exception is a concern as the cluster seems to have cascaded into a bad state -
- The rest of cluster went yellow as a mass re-allocation of replicas occurred.
- This is an example of what showed up in the logs, which stopped after about 1 minute after the election - 

```
[2013-05-09 11:34:52,232][WARN ][indices.cluster          ] [ip-10-239-70-202] [profiles_0001][26] master [[ip-10-34-144-149][IsP0kjtRS6KJ-9R3hZehwQ][inet[/10.34.144.149:9300]]{data=false, master=true, zone=eu-west-1c}] marked shard as started, but shard have not been created, mark shard as failed
[2013-05-09 11:34:52,232][WARN ][cluster.action.shard     ] [ip-10-239-70-202] sending failed shard for [profiles_0001][26], node[nUOPQBwwTdihgBPosOdbxA], [P], s[STARTED], reason [master [ip-10-34-144-149][IsP0kjtRS6KJ-9R3hZehwQ][inet[/10.34.144.149:9300]]{data=false, master=true, zone=eu-west-1c} marked shard as started, but shard have not been created, mark shard as failed]
```
- After about 7hours the 3 replicas in progress remain unassigned. We concluded the allocation process had failed at this point. 

Some details -
- Cluster is running in AWS with ec2 discovery
- 6 data nodes, 3 master nodes 
- The masters are dedicated (`node.master: true`; `node.data: false`)
- The data nodes are dedicated (`node.master: false`; `node.data: true`)
- `discovery.zen.minimum_master_nodes: 2`

```
2013-05-09 10:32:42,015][WARN ][discovery.ec2            ] [ip-10-208-11-218] received a join request for an existing node [[ip-10-36-129-154][jaqf1e_AS0aE_6CXupb2IQ][inet[/10.36.129.154:9300]]{client=true, data=false}]
[2013-05-09 11:32:51,646][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xbab451d1, /10.36.129.154:37310 =&gt; /10.208.11.218:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format
    at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2013-05-09 11:32:51,648][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xbab451d1, /10.36.129.154:37310 :&gt; /10.208.11.218:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format
    at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:336)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:570)
    at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
    at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
    at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:505)
    at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:227)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:654)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:562)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2013-05-09 11:32:54,717][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xd9e316de, /10.36.129.154:37311 =&gt; /10.208.11.218:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format
    at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
[2013-05-09 11:32:54,720][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xd9e316de, /10.36.129.154:37311 :&gt; /10.208.11.218:9300]], closing connection
java.io.StreamCorruptedException: invalid internal transport message format
    at org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:336)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)
    at org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:570)
    at org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)
    at org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)
    at org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:505)
    at org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:227)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)
    at org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:654)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:562)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```
</description><key id="14161297">3017</key><summary>Invalid internal transport message format between 0.20.6 master and client caused re-election</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dehora</reporter><labels><label>bug</label></labels><created>2013-05-09T19:18:37Z</created><updated>2014-08-20T17:07:48Z</updated><resolved>2014-08-20T17:07:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dehora" created="2013-05-10T21:41:49Z" id="17746193">Updated to clarify the java.io.StreamCorruptedException resulted in the master becoming unresponsive and disconnected from the rest of the cluster.
</comment><comment author="jayv" created="2013-05-27T13:14:06Z" id="18497987">We experience the same issue with 0.20.1, did you get an answer or found a solution for this problem? 
I discovered this while performing various tests to assure failover is happening and recovery works.

We run 3 nodes, zen minimum master = 2, when I shut down the network on 1 machine, the remaining 2 keep working, when the network connection returns, it never joins the cluster and we see the same exception is in our logs.
</comment><comment author="stephanj" created="2013-05-28T11:30:20Z" id="18544773">Any plans on fixing this issue?
</comment><comment author="dehora" created="2013-05-28T11:54:06Z" id="18545708">*did you get an answer or found a solution for this problem? *

@jayv No. But we've seen it 2 times since this report. What causes the elected master to go awry has varied, but the resulting behaviour is consistent -
- previous master excised from cluster
- new master elected
- rest of cluster reporting the same state
- previous master reporting a different state
- previous master requires a restart to join the cluster

Requiring N/2+1 master eligible nodes to be involved in election I gather stops a split brain, but I've seen this behaviour enough times such that I'm concluding it's how ES fault tolerance actually works when it comes to master elections. It's possibly not expected or intended, but it seems to be part of the design (I'd love to see a formal model of ES election.)

The primary concern I have is the consequent level of replica motion, which we've seen in every case after this happens. It doesn't seem workable for large datasets. We had one node post this failure get stuck moving 2 replicas to another node for about 8 days (we let it run to see what would happen, the guidance was to blow away the data on the sending node). What fixed it eventually was another master failure that moved about 15% of the shard/replicas, possibly more.
</comment><comment author="imotov" created="2013-05-28T12:09:43Z" id="18546294">Could someone post or send us complete logs (from all nodes, preferably on DEBUG level) from one of the incidents? 
</comment><comment author="dehora" created="2013-06-04T18:13:47Z" id="18928235">We've seen this happen again (another invalid internal transport message). The knock-on this time included 3 of 6 data nodes having to be restarted as they were unable to join the cluster with the master post-election. As before a large part of the index was dropped and re-allocated - beyond invalid internal transport, master re-election plus re-allocation has happened about 4 times in 6w. 

_Could someone post or send us_

Let me know how you want them sent.
</comment><comment author="imotov" created="2013-06-18T13:23:15Z" id="19610579">You can, for example, upload them to dropbox or any other server and post here, email me or send me on IRC channel the link. Whatever works for you.
</comment><comment author="metacret" created="2013-06-22T01:15:04Z" id="19848309">I've seen this with 0.90.0 too, exactly same behavior.
</comment><comment author="kornypoet" created="2013-06-26T20:08:27Z" id="20076228">We're seeing this too; will send logs over later today. On 0.90.0 as released.
</comment><comment author="imotov" created="2013-11-21T23:30:58Z" id="29035310">I just reproduced this issue locally with master (1.0.0.Beta2-SNAPSHOT). I overloaded one of the nodes by moving about 1000 shards to a really small node and it caused this node to run out of memory, which was expected. But just before node got into out of memory state it threw a couple of "java.io.StreamCorruptedException: invalid internal transport message format" exceptions. In other words, it looks like this exception might be a symptom of pre-OOM node. There are might be other reasons for this error though. 
</comment><comment author="dehora" created="2013-11-22T21:15:00Z" id="29109909">@imotov Awesome you've seen this happen. I don't have access to the related logs anymore, but from what I recall about the gc logs, the master nodes in scope for the initial bugrep were not under memory pressure at the time. Again, the concern for me is the cascading failure rather than a node hanging.
</comment><comment author="imotov" created="2014-08-20T15:53:55Z" id="52798709">In the last year, I haven't seen any reports of this error that weren't related to nodes running out of memory. Considering this, should we close it?
</comment><comment author="clintongormley" created="2014-08-20T17:07:48Z" id="52809180">Seems reasonable to me. Please reopen if you see further related incidents on 1.3 or later.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix teh typos in javadocs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3016</link><project id="" key="" /><description>Kill those typos!
</description><key id="14127928">3016</key><summary>Fix teh typos in javadocs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blakesmith</reporter><labels /><created>2013-05-08T23:43:40Z</created><updated>2014-07-16T21:53:26Z</updated><resolved>2013-10-18T10:24:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T10:24:13Z" id="26585562">Merged, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make GetField behavior more consitent for multivalued fields.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3015</link><project id="" key="" /><description>Before this change, the GetField#getValue() method was returning a list of values of a multivalued fields if the field values were obtained from source or if the field was stored and real-time get was used. If the field was stored but non-realtime get was used, GetField#getValue() was returning only the first element and the GetField#getValues() was returning a list of elements. This change makes behavior consistent. GetField#getValue() now always returns only the first value of the field and GetField#getValues() returns the entire list.
</description><key id="14127907">3015</key><summary>Make GetField behavior more consitent for multivalued fields.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-08T23:43:06Z</created><updated>2014-11-22T06:31:16Z</updated><resolved>2013-05-09T16:51:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-08T23:43:54Z" id="17641151">Looks good!, lets backport to 0.90 as well.
</comment><comment author="btiernay" created="2013-06-06T15:52:52Z" id="19054917">For nested types, what should be the expected behavior here? After upgrading to 0.90.1 our ES client code broke semantically since now `getValue` only returns the first nested doc. However, there is now no way to determine at query time if the value should be single value or not without knowing the mapping. 

Even if the mapping is known, there is no way to distinguish an array object type from an "object" object type. This makes it very difficult to proxy elasticsearch queries dynamically (ex. dynamically created queries for which you wouldn't know until runtime which fields are returned ), and requires access to the mapping and manually maintained metadata to know which method to call for a single vs multivalue field. 
</comment><comment author="btiernay" created="2013-06-06T16:37:46Z" id="19057756">I thought about this a little more. Would it be possible to add a new method to `GetField` such as `isMultiValued` to return whether or not the field is mapped as having multiple values? This would be similar to `LongValues`, `DoubleValues`, `BytesValues`, etc. This would be a suitable compromise for both consistency and general usage.
</comment><comment author="btiernay" created="2013-07-02T19:26:00Z" id="20369448">@imotov: Just curious to your thoughts on this. We don't really have a good solution for working around this right now. Is there another mechanism I'm overlooking? Thank again.
</comment><comment author="imotov" created="2013-07-02T19:36:52Z" id="20370122">@btiernay I don't think I fully understand the problem. In elasticsearch any field can be multivalued. There is no "array" type because any field can accept and array of values. Are you trying to distinguish between an array that contains one element and a single element that wasn't passed to elasticsearch as an array? If not, could you provide some examples?
</comment><comment author="btiernay" created="2013-07-02T19:42:55Z" id="20370519">@imotov: I'm trying to distinguish `x` (non-nested) in the following document:

`{x: [1, 2, 3]}`

from:

`{x: 1}`

So that when a query requesting `x` be returned, it is known that `getValue` or  `getValues` should be called. At design time this is known, but at query time it is not. 

What I am getting from you is that since ES makes no distinction between the two, it is up to the application to manage this information.  Is that correct?
</comment><comment author="imotov" created="2013-07-02T19:55:03Z" id="20371283">There is a difference between `{"x": [1,2,3]}` and `{"x": 1}`, for the first record `GetField#getValues()` returns a list with 3 elements and for the second with only 1. However, there is currently no way to distinguish between `{"x": [1]}` and `{"x": 1}`.  
</comment><comment author="kimchy" created="2013-07-02T19:59:05Z" id="20371558">@btiernay there isn't a way today, but we are going to support it pretty soon. In search we actually support it with teh concept of partial fields, and we plan to add it to get, and actually simplify partial fields even more.
</comment><comment author="btiernay" created="2013-07-02T20:00:20Z" id="20371650">@imotov: Thanks for the explanation.
@kimchy: Thanks for the update. Is there a ticket I can follow to be sure that I don't miss it? 
</comment><comment author="kimchy" created="2013-07-02T20:07:00Z" id="20372092">@btiernay I don't think we have a ticket open yet, there should be one open pretty soon where we can brainstorm if needed.
</comment><comment author="btiernay" created="2013-07-02T20:13:34Z" id="20372537">@kimchy: Awesome thanks again for such a great product :)
</comment><comment author="btiernay" created="2014-11-21T14:56:29Z" id="63980597">@kimchy Was the feature you mentioned in your previous message ever added? We are struggling with this issue now that we are upgrading from 0.90.1 to 1.4. Cheers.
</comment><comment author="bleskes" created="2014-11-22T06:31:16Z" id="64071167">@btiernay, yeah- look for source filtering http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE Exception in Java client with nested query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3014</link><project id="" key="" /><description>When I switched over to the 90.0 version from 20.4, I started seeing some null pointer exceptions in the Java client from my integration tests.  The problem is not reproducible using the REST API.

It seems to occur when you do a nested query against multiple indices and one of the indices has no documents that match a query (but does have some documents in it).

The problem occurs in the AggregatedDfs.writeTo method because of the entries in the fieldStatistics map has an entry with a null key.  I think the root of the problem is wherever the fieldStatistics map gets created

A highly abbreviated call stack of where the exception occurs:
      at org.elasticsearch.common.io.stream.HandlesStreamOutput.writeString(HandlesStreamOutput.java:55)
      at org.elasticsearch.search.dfs.AggregatedDfs.writeTo(AggregatedDfs.java:106)
      at org.elasticsearch.search.query.QuerySearchRequest.writeTo(QuerySearchRequest.java:69)
      at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:546)

I've created a Gist that reproduces the problem:  https://gist.github.com/bartakj/5542153
</description><key id="14121557">3014</key><summary>NPE Exception in Java client with nested query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bartakj</reporter><labels /><created>2013-05-08T20:51:14Z</created><updated>2013-05-09T17:34:04Z</updated><resolved>2013-05-08T21:13:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-08T21:02:20Z" id="17633634">@bartakj I think the NPE has nothing to do with the Java client or the nested query. This error is related to #3012 
If you remove the following ling from your main class the error should disappear:

```
.setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
```
</comment><comment author="bartakj" created="2013-05-08T21:12:12Z" id="17634233">Yes, removing the setSearchType line makes the error go away.  So most likely your fix for the other defect will fix this one too.
</comment><comment author="martijnvg" created="2013-05-08T21:13:19Z" id="17634297">Yes, the fix in #3012 will make your error go away.
</comment><comment author="s1monw" created="2013-05-09T13:08:43Z" id="17663248">@bartakj I could never reproduce the NPE but I think it is caused by the fix in #3012 yet, if you could check if it fixes it that would be awesome. I can help with building a snapshot if you need help!

thanks
</comment><comment author="bartakj" created="2013-05-09T17:33:13Z" id="17677632">Yes it's fixed.  

I verified that I could reproduce the problem after building from the 0.90 branch with the change prior to the commit.  Then I built with the latest from the 0.90 branch and could not reproduce it.
</comment><comment author="s1monw" created="2013-05-09T17:34:04Z" id="17677678">@bartakj this made my day! thanks man!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analysis: Expose LimitTokenCountFilter in ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3013</link><project id="" key="" /><description>Expose Lucene's LimitTokenCountFilter to ElasticSeach so we can pick the number of tokens to index on multi-valued data (array fields).
</description><key id="14112633">3013</key><summary>Analysis: Expose LimitTokenCountFilter in ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">thesuaves22</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-08T17:30:04Z</created><updated>2013-05-30T09:04:11Z</updated><resolved>2013-05-14T07:59:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-05-14T02:34:56Z" id="17853568">I committed the code to my branch, but did not issue a pull request since I already have one open. Here is the commit:
https://github.com/brusic/elasticsearch/commit/a52ba66312bff81524f30183cfe31e9160e013e2

I do not have a clean master and I do not want to rollback just to commit the previous commit to a different branch. I wish the ElasticSearch team would approve or decline pull requests. Mine is over a month old without a single comment.
</comment><comment author="s1monw" created="2013-05-14T07:20:43Z" id="17860517">hey @brusic I pulled your commit in and I will push in a bit. I am afraid sometimes PRs slip through and we try hard to get everything in that we can. Can you please ping on the PR you mentioned again to get my or others attention, thanks!

regarding open PRs you can have as many as you want with as many branches as you want if you need help with git, let me know I am working on a "how to contribute" guide and I am happy to share.
</comment><comment author="brusic" created="2013-05-15T17:05:13Z" id="17952000">Thanks @s1monw for pushing out the change. My other pull request is a feature request that probably only affects myself and I have a workaround, so there is no urgency. 

Ironically, I have been a strong advocate of switching to a DVCS here at work (preferably git, but I would accept bzr or mercurial) in order to get lightweight branching, and here I am not using branches properly! I will clean up my fork and submit another pull request with proper branches. Working on some other features as well.

As far as this issue goes, my commit used default values that helped thesuaves22's issue (from the mailing list), but are probably incorrect for general usage. I will match Solr's factory's handling of the args and submit a fix. Once that is submitted, I can add the filter to the docs as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>DFS modes can cause undefined behaviour in 0.90 </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3012</link><project id="" key="" /><description>Currently some of the transport protocols are broken if Term/FieldStatistics don't provide all optional information. For instance we use `totalTermFrequency` in the TermStats but if you omit the term frequency the `totalTermFrequency` is a negative 1 (`-1`). This also happens if an index gets upgrade to 0.90 from a 0.20 version that runs lucene 3.6 and doesn't have these stats so they are -1 by default.  This seems to be the cause for #3008 as well as #2932 which eventually fail with NPE since the reading behavior of vlongs is undefined.
</description><key id="14108578">3012</key><summary>DFS modes can cause undefined behaviour in 0.90 </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-08T16:07:19Z</created><updated>2013-05-08T21:02:20Z</updated><resolved>2013-05-08T19:55:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add more informative toString method to StoreDirectory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3011</link><project id="" key="" /><description /><key id="14100544">3011</key><summary>Add more informative toString method to StoreDirectory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-08T13:34:39Z</created><updated>2014-06-17T07:12:59Z</updated><resolved>2013-05-08T16:13:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>TopChildrenQuery does not consider all children for score of parent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3010</link><project id="" key="" /><description>I expected The TopChildrenQuery to consider all Children while creating the score of 
the parent, especially when i use "score": "sum" but it seems that some sampling is being done. The only workaround i found here is to set the "factor" to a high number, but that slows down the query a lot. Is this a bug or should the documentation be changed to describe this behavior?
</description><key id="14097786">3010</key><summary>TopChildrenQuery does not consider all children for score of parent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nahap</reporter><labels /><created>2013-05-08T12:33:17Z</created><updated>2013-05-08T12:48:45Z</updated><resolved>2013-05-08T12:34:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-05-08T12:34:59Z" id="17602476">The docs explain this in some detail. That's the whole point of the 'factor'.

However, in 0.90, the has_children query now does examine all children - you should use that instead
</comment><comment author="martijnvg" created="2013-05-08T12:48:45Z" id="17603253">Actually from version 0.20.2 the has_child query can take all the child docs with their score into account.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixed parsing of track_scores in RestSearchAction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3009</link><project id="" key="" /><description>The `track_scores` parameter is now parsed by the REST handler

Closes #2986
</description><key id="14093957">3009</key><summary>Fixed parsing of track_scores in RestSearchAction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-05-08T10:49:28Z</created><updated>2014-06-29T14:31:06Z</updated><resolved>2013-05-08T10:54:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-08T10:50:29Z" id="17598102">Looks good!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when using java client with DFS_QUERY_THEN_FETCH</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3008</link><project id="" key="" /><description>here's the request.toString() content, this request works fine with REST API, but not work with java client.
{
  "from" : 0,
  "size" : 5000,
  "query" : {
    "bool" : {
      "must" : [ {
        "match" : {
          "tag" : {
            "query" : "Custom",
            "type" : "phrase"
          }
        }
      }, {
        "range" : {
          "timestamp" : {
            "from" : 1367251200000,
            "to" : 1367942400000,
            "include_lower" : true,
            "include_upper" : true
          }
        }
      } ]
    }
  },
  "explain" : false,
  "fields" : [ "tag", "buildId", "timestamp", "day", "count" ],
  "sort" : [ {
    "timestamp" : {
      "order" : "desc"
    }
  } ]
}
below is the response:
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 0,
    "failed" : 5,
    "failures" : [ {
      "status" : 500,
      "reason" : "SendRequestTransportException[[es-node-6][inet[/10.217.41.206:9300]][search/phase/query/id]]; nested: NullPointerException; "
    }, {
      "status" : 500,
      "reason" : "SendRequestTransportException[[es-node-5][inet[/10.8.199.3:9300]][search/phase/query/id]]; nested: NullPointerException; "
    }, {
      "status" : 500,
      "reason" : "SendRequestTransportException[[es-node-5][inet[/10.8.199.3:9300]][search/phase/query/id]]; nested: NullPointerException; "
    }, {
      "status" : 500,
      "reason" : "SendRequestTransportException[[es-node-7][inet[/10.143.135.88:9300]][search/phase/query/id]]; nested: NullPointerException; "
    }, {
      "status" : 500,
      "reason" : "SendRequestTransportException[[es-node-7][inet[/10.143.135.88:9300]][search/phase/query/id]]; nested: NullPointerException; "
    } ]
  },
  "hits" : {
    "total" : 0,
    "max_score" : 0.0,
    "hits" : [ ]
  }
}
</description><key id="14079108">3008</key><summary>NPE when using java client with DFS_QUERY_THEN_FETCH</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">waterdh</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-08T00:46:11Z</created><updated>2013-05-14T07:10:42Z</updated><resolved>2013-05-14T07:10:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="waterdh" created="2013-05-08T00:58:25Z" id="17580554">actually, it's using DFS_QUERY_THEN_FETCH, builder.setSearchType(SearchType.DFS_QUERY_THEN_FETCH);
when changed to SearchType.QUERY_THEN_FETCH, it works fine.
</comment><comment author="s1monw" created="2013-05-08T10:28:52Z" id="17597330">hey, I am curious do you see more infos about this in the logs? ie. the root cause of the exception?
I am also curious about your setup ie. if you upgraded from 0.20 to 0.90 and the index was essentially created with a previous version of elasticsearch?
</comment><comment author="s1monw" created="2013-05-08T16:12:48Z" id="17616334">it seems #3012 is the reason for this...
</comment><comment author="s1monw" created="2013-05-14T07:10:41Z" id="17860218">fixed by #3012
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: field_masking_span query parser not registered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3007</link><project id="" key="" /><description>field_masking_span  has been introduced via (now closed) issue #471.

The FieldMaskingSpanQueryParser is not registred in org/elasticsearch/indices/query/IndicesQueriesModule.java (version 0.90.0). I guess that adding

``` java
qpBinders.addBinding().to(FieldMaskingSpanQueryParser.class).asEagerSingleton();
```

fixes the issue.
</description><key id="14052598">3007</key><summary>Query DSL: field_masking_span query parser not registered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">iksnalybok</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-07T14:48:10Z</created><updated>2013-05-30T09:04:31Z</updated><resolved>2013-05-07T20:26:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-07T17:33:31Z" id="17557301">I will look... thanks for reporting
</comment><comment author="s1monw" created="2013-05-07T20:27:09Z" id="17568125">fixed, thanks again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting: Highlighter still fails if broken analysis chains are  used with fast vector highlighter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3006</link><project id="" key="" /><description>TokenFilters like `word_delimiter_filter` might produce broken term_vectors and mess up highlighting. We still fail with `StringIndexOutOfBoundsException` which is no good. Even if those filters are broken we should try at least best effort to not fail with a SIOOBException
</description><key id="14049980">3006</key><summary>Highlighting: Highlighter still fails if broken analysis chains are  used with fast vector highlighter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-07T13:56:01Z</created><updated>2013-05-30T09:04:39Z</updated><resolved>2013-05-07T14:29:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Wrong mapping using a nested object with same name as its type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3005</link><project id="" key="" /><description>When I index a document in elasticsearch 0.90.0 and I have nested objects, one of which has the same name as its type, the automatically submitted mapping is wrong and basic queries don't work anymore.

Here is the curl recreation: https://gist.github.com/javanna/5531326 .

Indexing an object like this under the "type" type, with empty mapping:

```
{
  "type": {
    "id" : "test id",
    "title" : "test title"  
  },
  "type2" : {
    "id" : "test"
  }
}
```

The updated mapping becomes the following:

```
{
  "type" : {
    "properties" : {
      "id" : {
        "type" : "string"
      },
      "title" : {
        "type" : "string"
      }
    }
  }
}
```

I would expect to see a type2 nested object but I can't find it, and any query on the nested type2 object doesn't return any result.
The problem seems to be caused by the name of the type used to index the document, which is the same as the name of one the nested objects. If I change that I don't see any problem.
</description><key id="14040018">3005</key><summary>Wrong mapping using a nested object with same name as its type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">javanna</reporter><labels /><created>2013-05-07T09:15:45Z</created><updated>2014-01-13T11:39:28Z</updated><resolved>2013-12-17T15:12:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-07T17:37:26Z" id="17557510">The problem is that the top level element in the doc is called `type`, which matches the mapping name `type`, so when providing a doc with a top level object name as the mapping name, it will just treat that object. This is to support the following case and not have "double" `type1` mapping:

```
curl -XPUT localhost:9200/index/type1/1 - d '{
   "type1" : {
        ....
   }
}
```

So, in your case, make sure the mapping name is not the same as the top level element if you don't provide your json docs wrapped with the type name.
</comment><comment author="clintongormley" created="2013-05-07T19:54:29Z" id="17566219">Does anybody use that double type (ie url + json)?  This seems like an alternative which could be removed fairly painlessly.
</comment><comment author="javanna" created="2013-05-08T09:30:42Z" id="17595143">Cool! I didn't know this was supposed to happen. I already changed the type name. 

On the other hand I'm not quite sure if anybody is actually using this feature. I would expect elasticsearch not to care too much about the content of my documents and give me a mapping like this:

```
{
  "type1" : {
    "properties" : {
        "type1" : {
            "type" : "object",
            "properties" : {
                ......
            }
        },
        "type2" : {
            "type" : "object",
            "properties" : {
                ......
            }
        }
    }
  }
}
```

My two cents: I would prefer to have the `type1` subobject even if it's the only existing subobject in my documents rather than have my `type2` subobject ignored.
</comment><comment author="kimchy" created="2013-05-08T09:35:19Z" id="17595331">It all goes back to the early history of elasticsearch, a lot of users were confused about the double mapping when they were wrapping the document they were indexing with the type name (some auto domain_objects -&gt; json tools do that). I agree that we could fail the indexing request if there is something we don't expect.
</comment><comment author="javanna" created="2013-05-08T09:42:14Z" id="17595611">I see! Well, if you want to be more backwards compatible you can probably check if the top level object with the type name is the only one available in the document (thus probably not needed) in order to avoid ignoring other nested objects, but I don't like this option that much myself.
</comment><comment author="clintongormley" created="2013-05-08T09:49:31Z" id="17595906">Yeah, I'd prefer there to be just one definitive way to do it, so that we don't have to guess.
</comment><comment author="clintongormley" created="2013-11-29T10:05:07Z" id="29506526">Would be good to resolve this issue in 1.0, even though it requires breaking bwc.

The body of an indexing request should be just that: the body, not possibly the body wrapped in the type.  Type should be specified only in the URL.
</comment><comment author="timeu" created="2013-11-29T10:25:05Z" id="29507591">I think this issue is also related to has_parent and has_child queries/filters:

https://groups.google.com/d/msg/elasticsearch/8yTX17uoFiU/xGjYd4tfEzAJ
</comment><comment author="clintongormley" created="2013-12-17T15:12:44Z" id="30758508">Closed this issue in favour of:
- #4483 
- #4484
- #4081 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>key_script for term stats facets</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3004</link><project id="" key="" /><description>I cannot find a script for key fields in term stats. That can also be very helpful.

Thanks
</description><key id="14037452">3004</key><summary>key_script for term stats facets</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">kul</reporter><labels><label>feature</label></labels><created>2013-05-07T07:57:46Z</created><updated>2013-09-14T13:31:58Z</updated><resolved>2013-09-14T13:31:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-05-07T09:26:22Z" id="17532151">I think you are looking for #2109.
May be we can now merge it in master?
</comment><comment author="kul" created="2013-05-07T10:27:14Z" id="17534578">That sounds like a useful feature, But this issue asks for a key_script in place key_field attribute if the user needs it. And if i understand correctly your pr enhances the value_script field only.
</comment><comment author="dadoonet" created="2013-05-07T11:28:40Z" id="17536743">@Kul You're right. Sorry.
</comment><comment author="dadoonet" created="2013-09-14T13:31:58Z" id="24446059">Closing this one. Should be supported in #3300 Cc @uboness 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>empty sort parameter slows down query time</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3003</link><project id="" key="" /><description>Hi there,

I'm using elasticsearch 0.20.4.

When applying an empty sort, query time slows down:

```
# [Mon May  6 14:47:39 2013] Protocol: http, Server: 157.193.59.144:9200
curl -XGET 'http://127.0.0.1:9200/meercat/parent/_search?scroll=1m&amp;pretty=1&amp;search_type=query_then_fetch'  -d '
{
   "sort" : [],
   "query" : {
      "has_child" : {
         "query" : {
            "query_string" : {
               "fields" : [
                  "_all"
               ],
               "query" : "ugent",
               "default_operator" : "AND",
               "use_dis_max" : "true"
            }
         },
         "type" : "child"
      }
   },
   "from" : 0
}
'

# [Mon May  6 14:48:03 2013] Response:
# {
#    "hits" : {
#       "hits" : [
#         ..          
#       ],
#       "max_score" : 1,
#       "total" : 1402728
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMjE4Ok5KLXZkQkZkUnp1Wl
# &gt;    dObDJLQ2VNcFE7MTIyMDpOSi12ZEJGZFJ6dVpXTmwyS0NlTXBROzEyMTY6Tk
# &gt;    otdmRCRmRSenVaV05sMktDZU1wUTsxMjE3Ok5KLXZkQkZkUnp1WldObDJLQ2
# &gt;    VNcFE7MTIxOTpOSi12ZEJGZFJ6dVpXTmwyS0NlTXBROzA7",
#    "took" : 24675
# }
```

When the parameter is not set it runs a LOT faster:

```
# [Tue May  7 08:43:30 2013] Protocol: http, Server: 157.193.59.144:9200
curl -XGET 'http://127.0.0.1:9200/meercat/parent/_search?scroll=1m&amp;pretty=1&amp;search_type=scan'  -d '
{
   "query" : {
      "has_child" : {        
         "query" : {
            "query_string" : {
               "fields" : [
                  "_all"
               ],
               "query" : "ugent",
               "default_operator" : "AND",
               "use_dis_max" : "true"
            }
         },        
         "type" : "child"
      }
   },
   "from" : 0
}
# [Tue May  7 08:43:31 2013] Response:
# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : 0,
#       "total" : 1405509
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "_scroll_id" : "c2Nhbjs1Ozg6OGdNdGZwNzFTck9sSm1OSEZQY2dlZzs2Oj
# &gt;    hnTXRmcDcxU3JPbEptTkhGUGNnZWc7MTA6OGdNdGZwNzFTck9sSm1OSEZQY2
# &gt;    dlZzs5OjhnTXRmcDcxU3JPbEptTkhGUGNnZWc7Nzo4Z010ZnA3MVNyT2xKbU
# &gt;    5IRlBjZ2VnOzE7dG90YWxfaGl0czoxNDA1NTA5Ow==",
#    "took" : 357
# }

```

So that is 24675 for a empty sort, while it takes 357 milliseconds for not supplying the sort parameter at all.

Any idea what causes this behaviour?

Thanks in advance
</description><key id="14035651">3003</key><summary>empty sort parameter slows down query time</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nicolasfranck</reporter><labels /><created>2013-05-07T06:52:11Z</created><updated>2013-05-07T07:31:45Z</updated><resolved>2013-05-07T07:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-07T07:00:19Z" id="17526427">Hey,

could it be that your `search_type` makes a difference here:

`curl -XGET 'http://127.0.0.1:9200/meercat/parent/_search?scroll=1m&amp;pretty=1&amp;search_type=scan'` 
vs.
`curl -XGET 'http://127.0.0.1:9200/meercat/parent/_search?scroll=1m&amp;pretty=1&amp;search_type=query_then_fetch'`

the scan execution is by definition much faster since it stops collection once it has enough docs found while `query_then_fetch` scores all matching docs and takes the the top N.
</comment><comment author="nicolasfranck" created="2013-05-07T07:28:37Z" id="17527402">Thanks for your quick reply!

Yes, that sounds reasonable. But we cannot set the search_type to "scan" because of the sorting we would
like to apply. Probably the weight of the index is an issue here. Thanks anyway!
</comment><comment author="kimchy" created="2013-05-07T07:31:45Z" id="17527489">@nicolasfranck one thing to note is that hte first time you execute the search request on a shard with has_child / sort, it requires loading data into memory (the sort values, and the ids for the has_child query/filter, regardless of the query values itself). So, make sure execute it a few times and then see how long it takes. To make sure those are loaded in the background for a live index (that keeps on being indexed to), you can use the warmers feature.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix error getting array fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3002</link><project id="" key="" /><description>Fixes #3000
</description><key id="14031630">3002</key><summary>Fix error getting array fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-05-07T03:15:07Z</created><updated>2014-07-16T21:53:28Z</updated><resolved>2013-05-07T17:59:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Using scrolls in Java,but the search results not stable</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3001</link><project id="" key="" /><description>I use the same query  to search something  by scrolls for many times,each time the search result are big different.i check the ES log find somethig like this:
========================es log===================================
[2013-05-06 11:57:08,774][DEBUG][action.search.type       ] 
[node6] [1924] Failed to execute query phase
org.elasticsearch.transport.RemoteTransportException: [node3][inet[/10.0.0.3:9300]][search/phase/scan/scroll]
Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [1924]
        at org.elasticsearch.search.SearchService.findContext(SearchService.java:459)
        at org.elasticsearch.search.SearchService.executeScan(SearchService.java:208)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:697)
        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(SearchServiceTransportAction.java:686)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
# 

---
## javacode like this:

SearchResponse response =  client.prepareSearch(getIndexNames(query)).setTypes("post")
                .setSearchType(SearchType.SCAN)
                                .setScroll(new TimeValue(1000))
                                .setQuery(qb)setSize(5000).execute().actionGet();
......
response = client.prepareSearchScroll(response.getScrollId())
                                .setScroll(new TimeValue(1000000))
                                .execute()
                    .actionGet();
......
</description><key id="14031079">3001</key><summary>Using scrolls in Java,but the search results not stable</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">no126leon</reporter><labels /><created>2013-05-07T02:46:45Z</created><updated>2013-08-26T18:57:05Z</updated><resolved>2013-08-26T18:57:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-26T18:57:04Z" id="23286284">Sorry for the late reply, hopefully you solved the problem in the meantime. Given the error, I suspect your scroll timeout had expired when you sent one of your consecutive requests.
Anyways, I'd suggest to send this kind of questions/problems to our [google group](https://groups.google.com/forum/#!forum/elasticsearch) and make sure it's a bug before actually opening an issue.  Have a look [here](http://www.elasticsearch.org/help/) too.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get doc fails for some array fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/3000</link><project id="" key="" /><description>See the following two scripts to reproduce:
https://gist.github.com/dakrone/5528301
https://gist.github.com/dakrone/5528298

When run:

```
&#8756; ./broken-get.zsh 
{"ok":true,"acknowledged":true}{"ok":true,"acknowledged":true}
{"ok":true,"_index":"get-test","_type":"doc","_id":"1","_version":1}
{"ok":true,"_index":"get-test","_type":"doc","_id":"2","_version":1}
{
  "_index" : "get-test",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "exists" : true, "_source" : {"date":"2010-01-01"}
}
{
  "_index" : "get-test",
  "_type" : "doc",
  "_id" : "2",
  "_version" : 1,
  "exists" : true, "_source" : {"date":["2010-01-01","2011-01-01"]}
}
{
  "error" : "MapperParsingException[failed to parse date field [[2010-01-01, 2011-01-01]], tried both date format [yyyy-MM-dd], and timestamp number]; nested: IllegalArgumentException[Invalid format: \"[2010-01-01, 2011-01-01]\"]; ",
  "status" : 400
}
```

and:

```
&#8756; ./broken-get2.zsh 
{"ok":true,"acknowledged":true}{"ok":true,"acknowledged":true}
{"ok":true,"_index":"get-test","_type":"doc","_id":"1","_version":1}
{"ok":true,"_index":"get-test","_type":"doc","_id":"2","_version":1}
{
  "_index" : "get-test",
  "_type" : "doc",
  "_id" : "1",
  "_version" : 1,
  "exists" : true, "_source" : {"num":2}
}
{
  "_index" : "get-test",
  "_type" : "doc",
  "_id" : "2",
  "_version" : 1,
  "exists" : true, "_source" : {"num":[2,1]}
}
{
  "error" : "NumberFormatException[For input string: \"[2, 1]\"]",
  "status" : 500
}
```
</description><key id="14021909">3000</key><summary>Get doc fails for some array fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">dakrone</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-06T21:24:51Z</created><updated>2013-05-07T17:59:39Z</updated><resolved>2013-05-07T17:59:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-05-06T21:25:12Z" id="17509232">Forgot to mention, this is on vanilla ES 0.90.0.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>integrity of list in elasticsearch messed up while paging</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2999</link><project id="" key="" /><description>After I initially filled my index with documents, using sort, from and size works properly. But when I overwrite a document for updating it, its integrity across pages is messed up.
Some documents show up twice, triple or more and some don't show up. The _id is the same and everything looks normal, except this pagination problem. They are not lost! If I fetch all documents, they are still there and after downgrading to 0.20.6 while keeping everything else the same, pagination works like a charm.

``` json
{
  "thing": {
    "properties": {
      "name": {
        "type": "multi_field",
        "fields": {
          "name": {
            "type": "string",
            "index": "analyzed"
          },
          "sortable": {
            "type": "string",
            "index": "not_analyzed"
          }
        }
      }
    }
  }
}
```
</description><key id="14019653">2999</key><summary>integrity of list in elasticsearch messed up while paging</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sclausen</reporter><labels /><created>2013-05-06T20:34:33Z</created><updated>2013-05-31T06:37:11Z</updated><resolved>2013-05-31T06:37:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-08T16:25:38Z" id="17617093">Can you help me a bit to understand your problem in more detail?

Just to make sure, that we are talking about the same problem. The pagination feature in elasticsearch using an offset and a size is always per request. This means, that a document, when it is reindexed, can occur more than once, when you are paging through your data.

Example for an expected behaviour:
- Create a number of documents
- Query for 'foo' with an offset 0 and size 10, the first match is document a
- Reindex document a, that it is actually no more under the first ten matches, but at the 15th position (just an example)
- Query again for foo with an offset 10 and size 10 and see, that document a is included again in your page

However if you keep telling me, that the behaviour is the different with 0.90 (is this the version you are using actually) and 0.20, you might have hit a bug, which we will need to reproduce. Can you provide some sample documents you indexed, which are actually occuring more than once in your search results?

I'd be glad to help in order to find out, what is broken here. Thanks for your report so far!
</comment><comment author="sclausen" created="2013-05-08T23:27:18Z" id="17640506">Yes, I'm aware that pagination works per request in elasticsearch. Nevertheless, my scenario wasn't exactly as you described. After reindexing a single document, I started paging again at `"from":0` and while increasing `"from"` over the requests I didn't reindexed something.

Here is my query I'm talking about:

``` json
{
  "from": 0,
  "size": 10,
  "sort": {
    "name.sortable": {
      "order": "asc"
    }
  },
  "query": {
    "match_all": {}
  }
}
```

Unfortunately, I'm not authorized giving example data, but hopefully my additions helped.
</comment><comment author="kimchy" created="2013-05-08T23:46:52Z" id="17641250">When you do the pagination, do you have same sort values where you see the documents repeating? i.e. doc id 1,2,3 all have the same `name.sortable`? If so, it might happen due to the fact that one read request and another can go to different copies of the data (shards). One way to work around that is to use `preference=[hash]` which will make sure for that [hash], it will hit hte same copies of the data. That hash, in real web application, can be something like a session id.
</comment><comment author="sclausen" created="2013-05-10T19:36:55Z" id="17740257">My indices just have one shard, but after all I tried your approach, but setting `preference` (I used `setPreference("123456789")`) didn't fixed the problem. After Modifying a document in 0.90.0, paging is messed up. 0.20.6 still works perfect :-/
</comment><comment author="kimchy" created="2013-05-10T21:49:56Z" id="17746483">@Phosphoros we need an example then, can you provide a recreation of this? curl one would be best. Simple one where you create an index, index a few documents, execute the searches with paging, and then modify it to show that the paging doesn't work.
</comment><comment author="sclausen" created="2013-05-31T06:37:11Z" id="18725754">@kimchy I'm really sorry, I didn't responded, but unfortunately I have been very busy. Recently I switched to 0.90.1 and this strange thing seems to be gone.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Store Level Throttling (index level) is not dynamic</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2998</link><project id="" key="" /><description># Description

Modifying the max_bytes_per_sec value on as open index has no impact on the throttling level and it doesn't seems to be changed unless the index is close/reopen.

In the documentation it says that that this setting can be set using the index udpate settings API dynamically. So I'm expecting that changing the value apply immediately the change to the setting.

I am seeing a confirmation in the logs;

2013-05-06 13:11:35,938][INFO ][index.store.fs           ] [es97b] [public_XX] updating index.store.throttle.max_bytes_per_sec from [200kb] to [500kb], note, type is [all]

But the throttling is not applied until I close and reopen the index.
# Sample update command

```
curl -XPUT  localhost:9200/public_XX/_settings -d '{"index.store": {
&gt;               "throttle.max_bytes_per_sec": "500kb",
&gt;               "throttle.type": "all"
&gt; }}'
```
</description><key id="14017198">2998</key><summary>Store Level Throttling (index level) is not dynamic</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgagnon1</reporter><labels><label>:Logging</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2013-05-06T19:39:49Z</created><updated>2016-08-24T15:12:05Z</updated><resolved>2016-08-24T15:12:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-06T19:45:30Z" id="17503455">when the default throttle level is set, its on hte node level (indices), and then changing it to index level throttling will not work (and the log is wrong). But, typically, node level (indices) level throttling is the one that makes sense, which is set using the `indices.store.throttle.max_bytes_per_sec`, which you can change using the cluster update settings API (cause it changes the setting on teh cluster level).

If you open and close the index, then yes, the index level setting will then take affect. We should log a warning in that case when the index level setting is changed, and the index was created with throttling on the node (indices) level.
</comment><comment author="jgagnon1" created="2013-05-06T19:49:41Z" id="17503687">Let say I am bulk reindexing a index on a live cluster and I don't want the indexation to take all the IO... I would like to set throttling only on this index... so is the throttle type "node" on the index level is adequate ?
</comment><comment author="jpountz" created="2014-09-05T08:49:00Z" id="54599817">Moving to `adoptme`: the logs that elasticsearch writes need fixing.
</comment><comment author="jpountz" created="2016-08-24T15:12:05Z" id="242099585">Closing: throttling setings have been removed as throttling is now dynamic.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error in index configuration lead to API/cluster unresponsive and unrecoverable.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2997</link><project id="" key="" /><description># Description

First, I am using version 0.20.6 on a quite large cluster.

Pushed a non-dynamic settings with a invalid value for example;

"index.translog.flush_threshold_period": "30mb"

instead of 30m (ok this is kind of obvious)

The cluster API become unresponsive and unrecoverable due to the log overload (specially on the master). I had to stop the whole cluster, delete the index directory by hand, and restart the cluster to recover.

The cluster should be able to;
1 - first; catch the bad parameter and do not set the setting value;
2 - second; respond to the API, search, and every request, or at least close the index since it is not able to recover. 
# Steps to reproduce

1 - Create and index with valid parameters
2 - While being open; set invalid parameter as described
3 - Close the index
4 - Try to reopen the index with the invalid parameters set.
5 - Try to close/delete the index or update anythingt (shoudn't be able)
# Log samples

Master

```
[2013-05-06 08:41:53,423][WARN ][cluster.action.shard     ] [es84b] received shard failed for [public_20120601][28], node[FvESugqWT3OJphhj6nFIZA], [P], s[INITIALIZING], reason [Failed to create shard, message [IndexShardCreationException[[public_20120601][28] failed to create shard]; nested: ElasticSearchParseException[Failed to parse [60mb]]; nested: NumberFormatException[For input string: "60mb"]; ]]
[2013-05-06 08:41:53,423][WARN ][cluster.action.shard     ] [es84b] received shard failed for [public_20120601][22], node[SlES3cBSS8q7Fjqc4hOQeQ], [P], s[INITIALIZING], reason [Failed to create shard, message [IndexShardCreationException[[public_20120601][22] failed to create shard]; nested: ElasticSearchParseException[Failed to parse [60mb]]; nested: NumberFormatException[For input string: "60mb"]; ]]
```

Node

```
[2013-05-06 08:41:53,451][WARN ][indices.cluster          ] [es84b] [public_20120601][35] failed to create shard                                                                              [0/2]
org.elasticsearch.index.shard.IndexShardCreationException: [public_20120601][35] failed to create shard
        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:323)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:561)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:526)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:171)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:315)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.ElasticSearchParseException: Failed to parse [60mb]
        at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:253)
        at org.elasticsearch.common.settings.ImmutableSettings.getAsTime(ImmutableSettings.java:191)
        at org.elasticsearch.index.translog.TranslogService.&lt;init&gt;(TranslogService.java:79)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:129)
        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:66)
        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:321)
        ... 7 more
Caused by: java.lang.NumberFormatException: For input string: "60mb"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:441)
        at java.lang.Long.parseLong(Long.java:483)
        at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:249)
        ... 30 more

```
</description><key id="14015392">2997</key><summary>Error in index configuration lead to API/cluster unresponsive and unrecoverable.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgagnon1</reporter><labels><label>:Settings</label><label>enhancement</label><label>stalled</label></labels><created>2013-05-06T18:59:06Z</created><updated>2016-09-27T10:53:34Z</updated><resolved>2016-09-27T10:53:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-10T14:16:55Z" id="17722570">Commands needed to reproduce

```
curl -X DELETE localhost:9200/crash
curl -X PUT localhost:9200/crash
curl -X PUT localhost:9200/crash/_settings -d '{ "index.translog.flush_threshold_period": "30mb" }'
curl -X POST localhost:9200/crash/_close
curl -X POST localhost:9200/crash/_open
// check your logs filing rapidly with exceptions after the open
curl -X POST 'localhost:9200/crash/test/_search'
```

_Note_: Setting an invalid timevalue in settings isnt possible anymore with 0.90.0 and will correctly return an exception. Not sure if this fixes all of the problem (like trying to load the shards all the time), we have to check what could possibly trigger this as well.
</comment><comment author="clintongormley" created="2014-07-23T13:10:03Z" id="49870844">Still need confirmation that all bad settings are checked.
</comment><comment author="clintongormley" created="2014-11-29T15:11:10Z" id="64954709">Hopefully this could be fixed once we have proper validated settings from #6732
</comment><comment author="dakrone" created="2016-09-27T10:53:34Z" id="249831928">This is fixed now that we have validated settings, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.90.0 bool query different results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2996</link><project id="" key="" /><description>One of my unit tests in NEST started reporting different search results since upgrading to 0.90.0

See https://gist.github.com/Mpdreamz/5524841 

For raw posts (also scroll down to see the 0.90.0 search results).

As far as the request goes they are identical.

This unit test has been working the same way since atleast 0.18 as far as I can recall.

Can anyone shed some light on why it's now no longer filtering out `elasticflume` ? 
</description><key id="13999348">2996</key><summary>0.90.0 bool query different results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Mpdreamz</reporter><labels /><created>2013-05-06T12:42:17Z</created><updated>2013-05-08T06:47:13Z</updated><resolved>2013-05-06T19:55:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-06T19:20:10Z" id="17502036">@Mpdreamz I think this bool filter issue is related to #2979, which has been fixed last friday.
</comment><comment author="imotov" created="2013-05-06T19:24:49Z" id="17502314">@martijnvg I just verified it, your fix for #2979 fixes this issus as well.
</comment><comment author="s1monw" created="2013-05-06T19:55:00Z" id="17503993">thanks @martijnvg &amp; @imotov for verifying.
</comment><comment author="Mpdreamz" created="2013-05-08T06:47:13Z" id="17589441">Thanks guys, happy to hear the issue has already been resolved.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest Get Source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2995</link><project id="" key="" /><description>Allow to get the source directly using a specific REST endpoint without any additional content around it, the endpoint is `{index}/{type}/{id}/_source`.
Note, HEAD now also support the _source endpoint.
closes #2993
</description><key id="13998983">2995</key><summary>Rest Get Source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-06T12:30:08Z</created><updated>2014-06-12T12:07:11Z</updated><resolved>2013-05-06T12:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: span_near query not working</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2994</link><project id="" key="" /><description>Hi,

Making a simple span_near query just fails. Running
  query: span_near : { clauses : [ { span_term : { index.field : value } } ] }
I get
  QueryParsingException[[index] spanNear [clauses] must be of type span query]
I don't get any error if I put an unknown field.

NB I'm using version 0.90.0.

By looking at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java

``` java
      33. import static org.elasticsearch.index.query.support.QueryParsers.wrapSmartNameQuery;
     106. SpanTermQuery query = new SpanTermQuery(new Term(fieldName, valueBytes));
     108. return wrapSmartNameQuery(query, smartNameFieldMappers, parseContext);
```

I supposed that QueryParsers#wrapSmartNameQuery() does not return a span query (but a XFilteredQuery).

To reproduce (bash):

``` bash
HOSTPORT="127.0.0.1:9200"

function create_schema () {
    local SCHEMA='
{
    "mappings" : {
        "bugnearspanindex" : {
            "properties" : {
                "myField" : { "type" : "string" }
            }
        }
    }
}
'
    curl --noproxy '*' -XPUT "http://${HOSTPORT}/bugnearspanindex/" -d "${SCHEMA}"
}


function make_and_run_query () {
    local FIELD=$1
    local QUERY='
{
    "query" : {
        "span_near" : {
            "clauses" : [
                { "span_term" : { "bugnearspanindex.'${FIELD}'" : "foobar" } }
            ],
            "slop" : 0,
            "in_order" : false,
            "collect_payloads" : false
        }
    }
}
'
    curl --noproxy '*' -XGET "http://${HOSTPORT}/bugnearspanindex/_search?pretty=true&amp;format=yaml" -d "${QUERY}"
}

# create the schema
create_schema

# run the request on a non existing field
make_and_run_query nonExistingField
  # -&gt; hits: { total: 0 }

# run the request on an existing field
make_and_run_query myField
  # -&gt; error: QueryParsingException[[bugnearspanindex] spanNear [clauses] must be of type span query]
```
</description><key id="13997127">2994</key><summary>Query DSL: span_near query not working</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">iksnalybok</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-06T11:19:45Z</created><updated>2013-05-30T09:04:57Z</updated><resolved>2013-05-07T21:00:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-07T20:21:14Z" id="17567758">hey, the reason why this fails for you is because you explicitly specify the type in the field name. `index.field` will trigger a Filter that is wrapped around the SpanTermQuery for the type `index`. Yet, this is obviously not correct but it might help you to work around the issue for now.
</comment><comment author="s1monw" created="2013-05-07T20:35:20Z" id="17568644">I think the solution here is really not trying to wrap in a filter at all. This won't really work with spans anyways so that is not even a backwards break.
</comment><comment author="kimchy" created="2013-05-07T20:58:17Z" id="17570161">@s1monw  +1, I think this fix makes tons of sense.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rest Get Source</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2993</link><project id="" key="" /><description>Allow to get the source directly using a specific REST endpoint without any additional content around it, the endpoint is `{index}/{type}/{id}/_source`.
Note, HEAD now also support the _source endpoint.
</description><key id="13996669">2993</key><summary>Rest Get Source</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-06T11:00:17Z</created><updated>2013-05-06T12:37:53Z</updated><resolved>2013-05-06T12:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>when using a port range on unicast discovery, scanning stops after the first connection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2992</link><project id="" key="" /><description>Version: ElasticSearch 0.20.6

How to reproduce:
- NodeBuilder.settings.put("discovery.zen.ping.unicast.hosts", "localhost[9300-9400]")
- Have the server listening on 9310
- NodeClient only pings port 9300

Observations:
- tcpdump -nni any 'portrange 9300-9400' - I only see TCP connections to port 9300, not anything in range 9301-9400.

Expected?

Workaround:
- generate my own list of hosts: NodeBuilder.settings.put("discovery.zen.ping.unicast.hosts", "localhost:9300,localhost:9301,localhost:9302...,localhost:9400")
</description><key id="13972778">2992</key><summary>when using a port range on unicast discovery, scanning stops after the first connection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jordansissel</reporter><labels /><created>2013-05-05T01:34:38Z</created><updated>2013-05-13T17:35:34Z</updated><resolved>2013-05-13T17:14:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jordansissel" created="2013-05-05T01:37:20Z" id="17444377">The 'host[port1-port2]' stuff is documented here: http://www.elasticsearch.org/guide/reference/modules/discovery/zen/
</comment><comment author="kimchy" created="2013-05-08T09:44:32Z" id="17595693">we only try the first one because the default is 9300-9400, and we used to try all of them by default (way back), and it was very expensive trying to form the connection on all those ports (assuming nothing was there). So it was changed to try the first one with the assumption that if a range is specified, the first one should be up to answer it.
</comment><comment author="dadoonet" created="2013-05-13T10:23:16Z" id="17803599">@jordansissel As there is a workaround and that IMHO having more than one node on a single box is very uncommon (more a dev than a prod concern), could we close this issue?
</comment><comment author="jordansissel" created="2013-05-13T16:12:40Z" id="17822285">This instance was a production issue; where a logstash and elasticsearch nodes colocated on the same server, logstash starts up first occasionally and snags port 9300 and only talks to itself despite elasticsearch being on port 9301 and the port range logstash told via NodeBuilder to connect to as 9300-9400

Still, I've worked around this thusly:
https://github.com/jordansissel/jruby-elasticsearch/commit/4456e1832e074809a75a0fdbdf19e50bfba257d4

Basically, implemented the "try all ports" logic in my own code. Works well enough.
</comment><comment author="dadoonet" created="2013-05-13T16:55:39Z" id="17824870">@jordansissel So this is happenning when you start logstash with an embedded elasticsearch, right?
IMHO, it's safer to separate things. For example, configure logstash to use 9300-9349 ports and Elasticsearch nodes to use 9350-9399 ports.
My 2 cents.
</comment><comment author="jordansissel" created="2013-05-13T17:14:43Z" id="17825996">Agreed there. For backwards compatibility reasons I can't change the port. I think it's safe to close this given the reported behavior is the intended behavior :)
</comment><comment author="jordansissel" created="2013-05-13T17:27:09Z" id="17826836">re: logstash with embedded elasticsearch

No, this also occurs, in general, when logstash and elasticsearch are simply on the same server as separate procseses. The result is that the default configurations require startup ordering because both compete for port 9300, and if ES starts on port 9301, logstash searches for ES on ports 9300-9400, finds itself (logstash) on 9300 and keeps trying to ping itself which it ignores.
</comment><comment author="jordansissel" created="2013-05-13T17:31:24Z" id="17827095">Roughly, just to describe what I've seen - two separate cases:

## Case 1
- logstash starts up; activates a no-data elasticsearch node to connect to the cluster so logstash can index things. Uses port 9300 as default.
- elasticsearch starts up. 9300 is in use, so listens on 9301
- logstash's elasticsearch client-node goes into discovery on 9300-9400, finds port 9300 (itself) responds
- logstash pings itself endlessly on port 9300
- elasticsearch never gets contacted

## Case 2
- logstash starts up and is configured to listen on port 9900 (randomly chosen to avoid 9300)
- logstash is told elasticsearch can be found in port range 9300-9400
- elasticsearch starts up and is configured to listen on port 9305
- logstash's elasticsearch client-node tries to discover elasticsearch _only_ on port 9300
- elasticsearch never gets contacted because the client-node discovery only talks to 9300 when told to try 9300-9400

In both cases, my work around of setting the unicast discovery list of all port combinations (host:9300, host:9301, ...) works around this.

(Just documenting for completeness and posterity!)
</comment><comment author="dadoonet" created="2013-05-13T17:35:34Z" id="17827397">Thank you! I was trying to understand and now I think it's clear.
I think you have this issue because you are considering that elasticsearch embedded is a full data node but elasticsearch not embedded is a client node.

IMHO, you should try to use TransportClient when you don't need an embedded node and NodeClient if embedded is set to true. That way, logstash won't start any Node process and won't listen to any http/transport port.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BytesRefOrdValComparator ignores highest value in a segment during binarySearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2991</link><project id="" key="" /><description>The BytesRefOrdValComparator uses `Ordinals.Docs.getNumOrdinals() -1` as the upperbound for the binarysearch. The `-1` causes that we ignore the last value in the segment. 

This is kind of a very tricky bug since it only happens if we need to binary-search to align ords and the bottom of the sort queue is greater than the largest value in the segment but less than the second largest. This was causing an issue reported by a user on the mailing list: https://groups.google.com/d/msg/elasticsearch/W5s1KypYcYw/L1UgixO_gQ4J
</description><key id="13971495">2991</key><summary>BytesRefOrdValComparator ignores highest value in a segment during binarySearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-04T22:45:00Z</created><updated>2013-05-23T18:59:04Z</updated><resolved>2013-05-06T08:25:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.3.0</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2990</link><project id="" key="" /><description>Lucene 4.3.0 release vote has passed. Yet once the release is officially announce we can upgrade master and 0.90
</description><key id="13970327">2990</key><summary>Upgrade to Lucene 4.3.0</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-04T20:44:26Z</created><updated>2013-05-06T16:03:51Z</updated><resolved>2013-05-06T16:03:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Unable to run elasticsearch - Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2989</link><project id="" key="" /><description>I got an error using elasticsearch 0.90.0

I am unable to run it using elasticsearch sh script.

Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version
    at org.elasticsearch.bootstrap.Bootstrap.buildErrorMessage(Bootstrap.java:251)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:235)
    at org.elasticsearch.bootstrap.ElasticSearch.main(ElasticSearch.java:32)
</description><key id="13970305">2989</key><summary>Unable to run elasticsearch - Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">lucabelluccini</reporter><labels /><created>2013-05-04T20:42:52Z</created><updated>2014-04-01T10:19:38Z</updated><resolved>2013-05-14T08:07:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-04T20:49:48Z" id="17441266">Can you explain what you are doing exactly? I mean are you downloading the 0.90 release from here: http://www.elasticsearch.org/download/

and then unzip / untar and run `bin/elasticsearch -f` ?
</comment><comment author="lucabelluccini" created="2013-05-04T20:57:08Z" id="17441384">Exactly...
I have no root access (I am on my dev account on my company and I cannot ask for it for the moment).

22:56 lbelluccini@xxxxxx ~/tmp&gt; java -version                                                                                                                                                                                 10:56PM
java version "1.7.0_03"
Java(TM) SE Runtime Environment (build 1.7.0_03-b04)
Java HotSpot(TM) Server VM (build 22.1-b02, mixed mode)

I tried to printout the ES_CLASSPATH and ES_HOME and they are correctly set by the script itself...

Linux xxxxxx 2.6.32.23-0.3-default #1 SMP 2010-10-07 14:57:45 +0200 x86_64 x86_64 x86_64 GNU/Linux
</comment><comment author="KlausBrunner" created="2013-05-06T14:00:34Z" id="17482966">I got exactly the same error when using the official .deb (upgrade from 0.20.5 to 0.90.0), on Ubuntu 13.04 64bit and Oracle JDK 7u21.

I then apt-get purged the old installation, re-installed 0.90 from scratch, and everything worked fine. I didn't investigate any further, but I would suspect either a problem with the existing elasticsearch.yml, or with an existing native script that I referenced there (which of course was compiled against 0.20.x). HTH.
</comment><comment author="lucabelluccini" created="2013-05-06T16:15:56Z" id="17491400">Unfortunately I have no right to modify my system...
</comment><comment author="spinscale" created="2013-05-08T16:54:10Z" id="17618967">@KlausBrunner I guess you do not have a way to reproduce your problem? Did you use plugins, which maybe included some more jars (even an old es installation maybe)?

@lucabelluccini have you maybe unzipped your elasticsearch zip file into an existing elasticsearch installation and then tried to start? Can you set the log level to TRACE in your logging.yml and check if you are getting more data logged, so we can debug this issue?
</comment><comment author="lucabelluccini" created="2013-05-08T17:54:11Z" id="17622572"> @spinscale I downloaded and untarred both 0.20 and 0.90 in different paths.
I got 2 different errors.

19:51 lbelluccini@nxxxxxxxx ~/repos/pi_stats/elasticsearch-0.90.0&gt; Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version                                               7:51PM
    at org.elasticsearch.bootstrap.Bootstrap.buildErrorMessage(Bootstrap.java:251)
    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:235)
    at org.elasticsearch.bootstrap.ElasticSearch.main(ElasticSearch.java:32)

19:52 lbelluccini@nxxxxxxxx ~/repos/pi_stats/elasticsearch-0.20.6&gt; ./bin/elasticsearch -f                                                                                                                                        7:52PM
May 08, 2013 7:52:56 PM node
INFO: [Grasshopper I&amp;II] {0.20.6}[19688]: initializing ...
May 08, 2013 7:52:56 PM plugins
INFO: [Grasshopper I&amp;II] loaded [], sites []
May 08, 2013 7:52:56 PM bootstrap
SEVERE: {0.20.6}: Initialization Failed ...
- NoClassDefFoundError[org/apache/lucene/store/IndexInput]
  ClassNotFoundException[org.apache.lucene.store.IndexInput]
</comment><comment author="KlausBrunner" created="2013-05-08T17:58:05Z" id="17622823">@spinscale Yep, a plugin was also involved (phonetic). I can't easily reproduce this now, but it seems likely that either the plugin or the native script may have been the culprit, as both had been built for 0.20.x.
</comment><comment author="spinscale" created="2013-05-10T13:58:00Z" id="17721571">@lucabelluccini can you show me all the commands (including the wget to download the archive) you executed in order to get your elasticsearch versions and to start elasticsearch. It doesnt like you downloaded the zip or the tar.gz distribution, otherwise I cannot explain why you lack lucene jars when starting elasticsearch.
</comment><comment author="lucabelluccini" created="2013-05-10T15:29:57Z" id="17726988">wget http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.0.tar.gz
tar xvfz elasticsearch-0.90.0.tar.gz
cd elasticsearch-0.90.0
./bin/elasticsearch -f

I did the same on my unix vm and it worked...
I tried to print out es classpath etc... and it contains the jars...
</comment><comment author="spinscale" created="2013-05-10T17:33:53Z" id="17733664">Ok, this is getting even more strange. 

To what is your JAVA_HOME set?
What is the difference between your unix VM and the system you are trying to install on? It is a standard linux, right?
</comment><comment author="kimchy" created="2013-05-10T17:39:00Z" id="17733953">Also would be interseting to see an export of all env vars.
</comment><comment author="lucabelluccini" created="2013-05-13T10:10:29Z" id="17803088">Linux version 2.6.16.60-0.76.8-smp (geeko@buildhost) (gcc version 4.1.2 20070115 (SUSE Linux)) #1 SMP Tue Jan 11 11:23:59 UTC 2011

No JAVA_HOME set.

java version "1.7.0_03"
Java(TM) SE Runtime Environment (build 1.7.0_03-b04)
Java HotSpot(TM) Server VM (build 22.1-b02, mixed mode)

/nastools/java/bin/java

USER=lbelluccini
LOGNAME=lbelluccini
HOME=/remote/users3/lbellucc
PATH=/gctmp/ccache:/projects/intres/ccache:/nastools/oracle/products/10.2.0.3/bin:/projects/seidelde/ConfigTools/ConfigTools:.:/opt/gcc-4.3.2-sles10/bin:/opt/python-2.6-64/bin:/remote/projects1/intscs/delivery/rsync/latest/bin:/remote/projects1/intscs/delivery/valgrind/latest/bin:/remote/projects1/intscs/delivery/htop/latest/bin:/remote/projects1/intscs/delivery/ush/latest:/remote/projects1/intscs/delivery/dsm/latest:/remote/projects1/intscs/delivery/scripts/bin:/remote/users3/lbellucc/work/tools/:/nastools/oracle/products/10.2.0.3/bin:/nastools/oracle/products/11.2.0.2/bin:/nastools/cvs-1.11.12/bin:/nastools/java/bin:/nastools/gdb/bin:/opt/python-2.6-64/bin:/opt/gcc/bin:/opt/devsup/mercurial/bin:/opt/devsup/ctm:/opt/devsup/cmk/latest:/opt/devsup/bms/latest:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/usr/share/centrifydc/bin:/tools/fileutils/bin:/nastools/ccache/bin:/nastools/clewn/bin:/nastools/cscope/bin:/nastools/ctags/bin:/nastools/ddd/bin:/nastools/doxygen/bin:/nastools/fuse/bin:/nastools/gqlplus/bin:/nastools/graphviz/bin:/nastools/sles10/kcachegrind/bin:/nastools/lcov/bin:/nastools/ncftp/bin:/nastools/nfswatch/bin:/nastools/sles10/valgrind/bin:/nastools/valgui/bin:/nastools/valkyrie/bin:/nastools/vim/bin:.:/projects/csx/bin:/projects/csx/tools/edi:/projects/csx/tools:/projects/mwdeldev/EDITEST_INJECTOR_1-13/bin:/projects/mwdeldev/OBEetc:/projects/csx/bin:.:/remote/users3/lbellucc/bin
MAIL=/var/mail/lbelluccini
SHELL=/usr/bin/zsh

SSH_TTY=/dev/pts/2
TERM=xterm
CDC_JOINED_ZO

SHLVL=1
PWD=/remote/users3/lbellucc/repos/pi_stats/elasticsearch-0.90.0
OLDPWD=/remote/users3/lbellucc/repos/pi_stats
REPENV=/tools/profiles/zsh
MANPATH=/remote/projects1/intscs/delivery/python2.6/man:/opt/python-2.6-64/man:/remote/projects1/intscs/delivery/rsync/latest/share/man:/usr/share/man:/usr/local/man:/opt/gcc-3.4.2/man:/nastools/man:/opt/python-2.6-64/man:/opt/gcc/man::/usr/share/man:/usr/local/man:/tools/fileutils/man:/nastools/sles10/valgrind/man
LD_LIBRARY_PATH=/nastools/oracle/products/10.2.0.3/lib:/opt/gcc-4.3.2-sles10/lib64:/opt/gcc-3.4.2/lib64:/nastools/mysql/products/5-1-39/lib/mysql:/opt/python-2.6-64/lib:/nastools/oracle/products/10.2.0.3/lib:/nastools/oracle/products/11.2.0.2/lib:/opt/gcc/lib:/usr/local/lib::/opt/python-2.6-64/lib:/nastools/gqlplus/lib:/nastools/graphviz/lib/graphviz
USERNAME=lbelluccini

DISPLAY=ncelbelluccini:0.0
PS1=%{%}%T %{%}%n%{%}@%{%}%m %{%}%~%{%}&gt;%{%} 
RPS1=%t
DIRSTACKSIZE=50
HISTSIZE=1000
HISTFILE=/remote/users3/lbellucc/.zsh_history
SAVEHIST=500
TMPDIR=/gctmp/lbelluccini
FCEDIT=vi
EDITOR=vi
TZ=MET
TERMINFO=/usr/share/terminfo
MERCURIAL_VERSION=2.2.3
MERCURIAL_BASE=/opt/devsup/mercurial
MERCURIAL_PYTHONPATH=/opt/devsup/mercurial/mercurial-2.2.3/lib/python2.6/site-packages
PYTHONPATH=/remote/projects1/intscs/delivery/python2.6/lib/python2.6/site-packages:/projects/seidelde/ConfigTools::/opt/devsup/platinum/:/opt/devsup/mercurial/mercurial-1.6/lib/python
PLATINUM_CLIENT_HOME=/opt/devsup/platinum
POLONIUM_HOME=/opt/devsup/polonium
LD_RUN_PATH=/opt/gcc/lib::/nastools/fuse/lib:/nastools/sles10/valgrind/lib
KDEDIR=/nastools/sles10/kcachegrind
CSX_HOSTTYPE=Linux
CSX_HOSTREL=2.6.16.60-0.76.8-smp
TRACECONFIGPATH=.
CVS_RSH=remsh
CVS_SERVER=/usr/local/bin/cvs
CVSROOT=:pserver:ncedom+lbelluccini@ncecvsgco:/cvsgco/csxdev
stable=/remote/projects1/cplint/stable
staging=/remote/projects1/cplint/work
reference=/remote/projects1/cplint/reference
MERCURIAL_WORKING_DIR=/remote/users3/lbellucc/cpl
CONFMAKER_VERSION=latest
DISABLE_HUGETLBFS=1
TNS_ADMIN=/nastools/oracle/etc
TWO_TASK=DB10C11ls

PHASE=unit_dev
OBE_ROOT=/remote/users3/lbellucc/cpl
USER_CONF=/projects/csx/bin/userconfOtf
OBEAPP_ROOT=/remote/users3/lbellucc/cpl

CPX_RECEPTOR_PORT=17299
FE_PORT_SSR=17276
PORT_NUMBER=17200
CCACHE_DIR=/mwrep/ccache
CCACHE_NOLINK=1
CCACHE_UMASK=000
CCACHE_UNIFY=1
CSX_BMS=1
BMS_TARGET=sCxx64MtDbg
CSX_OTF=1
CSP_OTF=1
CPC_OTF=1
GREP_OPTIONS=--color=auto
USER_PACKAGE=CSX
WAS_COMPONENT=csx
CSX_USE_CH=1
OTF_VERSION=2-6-0-19
MW_PACK_VERSION=1-7a-1-23
CSX_USE_MWREPLICATION=1
CODECGEN_DIR=/remote/users3/lbellucc/work/tools/cog
CONFMAKER_CONFIG_ROOT=/remote/users3/lbellucc/cpl/confMaker
CONFMAKER_INSTALL_DIR=/gctmp/lbelluccini/cpl
DSM_GROUP=RDM_PNR_PSD_PAP
SCS_USE_CCACHE=1
DB_USER=lbe
NLS_LANG=AMERICAN_AMERICA.UTF8
WORKSPACE=cpl
BASE_ROOT_DIR=/remote/users3/lbellucc/cpl
SCS_PACKAGE_VERSION=1.28
OTF_PORT_RANGE=200
_=/usr/bin/env
</comment><comment author="spinscale" created="2013-05-13T14:24:05Z" id="17814678">Hey, judging from your setup I have one more assumption (and slowly but surely being more sure this is a administration setup preventing elasticsearch from starting).

Has your system administrator have any additional security tools like SELinux or AppArmor running?
Judging from your settings, is it possible that your home directory is a network share? Any additional possibilities, that this share is not allowed to execute or open certain files (again by security policy maybe?) or that executing programs may not open files on that share?

Wild guesses from the distance... maybe having one of sysadmins jump in may help to clear things up a bit with this setup.
</comment><comment author="lucabelluccini" created="2013-05-13T18:36:06Z" id="17831394">Hello... I am quite sure LinuxSe is active.
And yes, I am on a network mounted home.
I'll try later from the machine drive.

Confirmed!

Outside from the NAS works correctly.

[2013-05-14 09:11:15,600][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: initializing ...
[2013-05-14 09:11:15,605][INFO ][plugins                  ] [Aged Genghis] loaded [], sites []
[2013-05-14 09:11:17,314][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: initialized
[2013-05-14 09:11:17,314][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: starting ...
[2013-05-14 09:11:17,449][INFO ][transport                ] [Aged Genghis] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/172.16.136.244:9301]}
[2013-05-14 09:11:22,086][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: stopping ...
[2013-05-14 09:11:22,100][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: stopped
[2013-05-14 09:11:22,100][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: closing ...
[2013-05-14 09:11:22,108][INFO ][node                     ] [Aged Genghis] {0.90.0}[30251]: closed
</comment><comment author="spinscale" created="2013-05-14T08:07:30Z" id="17862235">Closing this issue then. Not an elasticsearch issue.

Still important to keep in mind, that running security hardening tools might be an issue in order to get elasticsearch up and running.

Thanks a lot for your patience and ongoing information delivery!
</comment><comment author="lucabelluccini" created="2013-05-14T08:26:27Z" id="17863027">Thank you for the support! Great job.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose underlying Lucene version in API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2988</link><project id="" key="" /><description>Sometimes it would be nice to be able to query the underlying Lucene version, fex. because the necessary query expression escaping rules depend on it.
</description><key id="13964334">2988</key><summary>Expose underlying Lucene version in API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">schnittchen</reporter><labels><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-04T13:30:12Z</created><updated>2013-05-06T08:25:55Z</updated><resolved>2013-05-06T08:25:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-04T14:34:06Z" id="17434115">we expose the version when you hit /, but we can add the Lucene version as well
</comment><comment author="s1monw" created="2013-05-04T19:38:09Z" id="17440234">+1 to add the lucene version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MoreLikeThis request + CustomNumericField + store=no</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2987</link><project id="" key="" /><description>Hello,

I get a ElasticSearchIllegalStateException("Field should have either a string, numeric or binary value") throwed from TransportMoreLikeThisAction.convertField(Field field).

Actually, i have a index field which is a CustomIntegerNumericField for which field.numericValue() returns null when the "store" option is not set to "yes".

Field.numericValue() is using the "fieldsData" which is obviously null when not storing.
When debugging, i can see the "fieldsData" is indeed null, but the "number" variable is not.

Shouldn't the method numericValue() be overriden in CustomIntegerNumericField to use the "number" variable instead ? Also in other classes extending CustomNumericField.

(I'm using version 0.90.0)

Thank you already,

Olivier
</description><key id="13940090">2987</key><summary>MoreLikeThis request + CustomNumericField + store=no</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">blop</reporter><labels /><created>2013-05-03T15:51:49Z</created><updated>2013-10-07T07:48:10Z</updated><resolved>2013-10-07T07:48:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="blop" created="2013-05-03T15:57:29Z" id="17402309">curl -XGET 'http://localhost:9200/indexname/typename/1/_mlt'
{"error":"MapperParsingException[failed to parse [my.field.name]]; nested: ElasticSearchIllegalStateException[Field should have either a string, numeric or binary value]; ","status":400}
</comment><comment author="blop" created="2013-05-03T16:01:51Z" id="17402563">Also, when specifying some fields to use (mlt_fields), i get this error on a field that was NOT selected.
</comment><comment author="spinscale" created="2013-07-23T10:08:57Z" id="21404591">More like this and numeric fields do not really work, so this feature has recently been disabled. See these issues

https://github.com/elasticsearch/elasticsearch/issues/3252
https://github.com/elasticsearch/elasticsearch/pull/3291
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>track_scores doesn't work with URI request queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2986</link><project id="" key="" /><description>Using [URI reqeust](http://www.elasticsearch.org/guide/reference/api/search/uri-request/) queries, the _track_scores_ parameter doesn't work.

Here's a curl recreation of the problem:
https://gist.github.com/radu-gheorghe/5476348

@clintongormley confirmed and says it works with query body types of queries. Thanks, Clint!
</description><key id="13934708">2986</key><summary>track_scores doesn't work with URI request queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels><label>bug</label><label>v0.90.7</label><label>v1.0.0.Beta1</label></labels><created>2013-05-03T13:48:33Z</created><updated>2013-11-12T11:21:13Z</updated><resolved>2013-05-08T10:54:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Einoras" created="2013-10-10T15:11:48Z" id="26062071">It's weird, but I'm still having the same problem.
Version 0.90.5, installed from .deb  Any idea why? 
</comment><comment author="s1monw" created="2013-11-12T11:08:01Z" id="28285092">I think this has never been ported to 0.90.x I will look into it!
</comment><comment author="s1monw" created="2013-11-12T11:21:13Z" id="28285801">pushed to 0.90 - this should be in `0.90.7`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Introduced a Opertaion enum that is passed to each call of WeightFunction#weight</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2985</link><project id="" key="" /><description>This allows dedicated weight calculations per operation. In certain
circumstance it is more efficient / required to ignore certain factors in the weight
calculation to prevent for instance relocations if they are solely triggered by tie-breakers.
In particular the primary balance property should not be taken into account if the delta for
early termination is calculated since otherwise a relocation could be triggered solely by the
fact that two nodes have different amount of primaries allocated to them.

Closes #2984
</description><key id="13932467">2985</key><summary>Introduced a Opertaion enum that is passed to each call of WeightFunction#weight</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-05-03T12:39:46Z</created><updated>2014-06-28T10:59:40Z</updated><resolved>2013-05-03T13:04:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PrimaryBalance in BalancedShardsAllocator can trigger unneeded relocation</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2984</link><project id="" key="" /><description>today if two nodes have very similar weights but only differ in the number of primaries a relocation can happen due to tie-breaking on the primaries per node. This might happen only if lots of relocations have happened before but still can trigger a unnecessary relocation.
</description><key id="13932234">2984</key><summary>PrimaryBalance in BalancedShardsAllocator can trigger unneeded relocation</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-03T12:31:51Z</created><updated>2013-05-03T13:04:40Z</updated><resolved>2013-05-03T13:04:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PR: Added update support to bulk api.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2983</link><project id="" key="" /><description>PR for #2982

Added support for the update operation in the bulk api.

Update requests can now be put in the bulk api. All update request options are supported.

Example usage:

```
curl -XPOST 'localhost:9200/_bulk' --date-binary @bulk.json
```

Contents of bulk.json that contains two update request items:

```
{ "update" : {"_id" : "1", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "doc" : {"field" : "value"} }
{ "update" : { "_id" : "0", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "script" : "counter += param1", "lang" : "js", "params" : {"param1" : 1}, "upsert" : {"counter" : 1}}
```

The `doc`, `upsert` and all script related options are part of the payload. The `retry_on_conflict` option is part of the header.
</description><key id="13931383">2983</key><summary>PR: Added update support to bulk api.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-05-03T11:55:52Z</created><updated>2015-05-18T23:34:33Z</updated><resolved>2013-05-10T14:20:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-05-10T14:33:29Z" id="17723531">Awesome, really looking forward to this; thanks @martijnvg!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk: Add support for update to bulk api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2982</link><project id="" key="" /><description>Add support for the update operation in the bulk api.

Update requests should be put in the bulk api. All update request options should be supported.

Example usage:

```
curl -XPOST 'localhost:9200/_bulk' --date-binary @bulk.json
```

Contents of bulk.json that contains two update request items:

```
{ "update" : {"_id" : "1", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "doc" : {"field" : "value"} }
{ "update" : { "_id" : "0", "_type" : "type1", "_index" : "index1", "_retry_on_conflict" : 3} }
{ "script" : "counter += param1", "lang" : "js", "params" : {"param1" : 1}, "upsert" : {"counter" : 1}}
```

The `doc`, `upsert` and all script related options are part of the payload. The `retry_on_conflict` option is part of the header.

Relates to #1985
</description><key id="13931125">2982</key><summary>Bulk: Add support for update to bulk api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-03T11:44:18Z</created><updated>2013-05-30T09:05:54Z</updated><resolved>2013-05-10T14:19:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PR for #2979</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2981</link><project id="" key="" /><description>PR for #2979
</description><key id="13928410">2981</key><summary>PR for #2979</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-05-03T09:58:39Z</created><updated>2014-06-18T07:46:08Z</updated><resolved>2013-05-03T15:40:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-03T15:40:50Z" id="17401340">Pushed to master and 0.90
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to parse query with a slash after a 0.90 upgrade</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2980</link><project id="" key="" /><description>Hi

I upgraded from 0.20 to 0.90 and a simple query I do failed when there is a slash in the query.
It's easy to test:
- create a simple element like in the documentation:

``` sh
curl -XPUT 'http://localhost:9200/twitter/tweet/1' -d '{
    "user" : "kimchy",
    "post_date" : "2009-11-15T14:12:12",
    "message" : "trying out Elastic Search"
}'
```
- Run a query with a slash in it

``` sh
curl -X GET 'http://localhost:9200/twitter/_search?from=0&amp;size=12&amp;pretty' -d '{
   "query":{
      "query_string":{
         "query":"user:kimchy/banon"
      }
   }
}'
```
- Check output 
  works fine with 0.20. With that example you have 0 results
  returns this error with 0.90

```
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[tJ5MGSY_RnOHfeAN2O8gnQ][twitter][2]: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n   \"query\":{\n      \"query_string\":{\n         \"query\":\"user:kimchy/banon\"\n      }\n   }\n}]]]; nested: QueryParsingException[[twitter] Failed to parse query [user:kimchy/banon]]; nested: ParseException[Cannot parse 'user:kimchy/banon': Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; nested: TokenMgrError[Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; }{[tJ5MGSY_RnOHfeAN2O8gnQ][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n   \"query\":{\n      \"query_string\":{\n         \"query\":\"user:kimchy/banon\"\n      }\n   }\n}]]]; nested: QueryParsingException[[twitter] Failed to parse query [user:kimchy/banon]]; nested: ParseException[Cannot parse 'user:kimchy/banon': Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; nested: TokenMgrError[Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; }{[tJ5MGSY_RnOHfeAN2O8gnQ][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n   \"query\":{\n      \"query_string\":{\n         \"query\":\"user:kimchy/banon\"\n      }\n   }\n}]]]; nested: QueryParsingException[[twitter] Failed to parse query [user:kimchy/banon]]; nested: ParseException[Cannot parse 'user:kimchy/banon': Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; nested: TokenMgrError[Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; }{[tJ5MGSY_RnOHfeAN2O8gnQ][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n   \"query\":{\n      \"query_string\":{\n         \"query\":\"user:kimchy/banon\"\n      }\n   }\n}]]]; nested: QueryParsingException[[twitter] Failed to parse query [user:kimchy/banon]]; nested: ParseException[Cannot parse 'user:kimchy/banon': Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; nested: TokenMgrError[Lexical error at line 1, column 18.  Encountered: &lt;EOF&gt; after : \"/banon\"]; }]",
```

Any idea?
Thanks
</description><key id="13927650">2980</key><summary>Failed to parse query with a slash after a 0.90 upgrade</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sgruhier</reporter><labels /><created>2013-05-03T09:33:42Z</created><updated>2014-07-17T18:02:27Z</updated><resolved>2013-05-03T12:47:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sgruhier" created="2013-05-03T12:43:44Z" id="17392148">I update the issue with a full example with more details on the errors.
</comment><comment author="sgruhier" created="2013-05-03T12:44:53Z" id="17392183">Ah, If I escape the slash like this:

``` sh
curl -X GET 'http://localhost:9200/twitter/_search?from=0&amp;size=12&amp;pretty' -d '{
   "query":{
      "query_string":{
         "query":"user:kimchy\\/banon"
      }
   }
}'
```

It works!
So not a bug but a good thing to know
</comment><comment author="dadoonet" created="2013-05-03T12:47:15Z" id="17392257">So should we close it?
</comment><comment author="sgruhier" created="2013-05-03T12:47:32Z" id="17392274">yes
</comment><comment author="s1monw" created="2013-05-03T13:18:13Z" id="17393472">FYI - this is  due to regular expression support in the query parser since lucene 4.0 the slash indicates a regexp
</comment><comment author="sgruhier" created="2013-05-03T13:21:34Z" id="17393610">Thanks for the explanation!
makes sense ! 

On May 3, 2013, at 3:18 PM, Simon Willnauer notifications@github.com wrote:

&gt; FYI - this is due to regular expression support in the query parser since lucene 4.0 the slash indicates a regexp
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="sgruhier" created="2013-05-03T13:22:26Z" id="17393642">BTW, how regexp performance is in lucene 4? Should I avoid them?
</comment><comment author="kimchy" created="2013-05-03T15:35:59Z" id="17401033">regex performance are considerably better in lucene 4, though obviously still slower compared to simple term queries or preparing the terms to search through during analysis on index time
</comment><comment author="s1monw" created="2013-05-03T21:29:18Z" id="17419262">as usual, it depends. As a rule of thumb if you have a reasonable constant prefix these kind of queries are pretty fast. You should measure how it performs on your index and especially what kind of regular expressions you wanna use. Bottomline is they are `useable` in production since 4.0 (0.90) compared to lucene 3.6 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Wrong result on bool filter with 'must' and 'should' clauses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2979</link><project id="" key="" /><description>ES version is 0.90.0 (from deb package). Count of products in test - 667 (333 in stock and 334 out of stock). 50 products was added less than a month ago (37 in stock and 13 out of stock). One product (in stock) have a "Hot Offer" label.

So, my first query (search_type = count):

``` JSON
{
    "query": {
        "match_all": {}
    },
    "filter": {
        "bool": {
            "should": [
                {"term": {"hotOffer": true}},
                {"range": {"created": {"from": "2013-04-02"}}}
            ]
        }
    }
}
```

The result is 51 hits. It's correct result.
Now add a &lt;code&gt;'must'&lt;/code&gt; clause to &lt;code&gt;'bool'&lt;/code&gt; filter:

``` JSON
{
    "query": {
        "match_all": {}
    },
    "filter": {
        "bool": {
            "must": {
                "term": {"inStock": true}
            },
            "should": [
                {"term": {"hotOffer": true}},
                {"range": {"created": {"from": "2013-04-02"}}}
            ]
        }
    }
}
```

The result is 332 hits. It's wrong result (should be 38).
Remove &lt;code&gt;'range'&lt;/code&gt; filter from &lt;code&gt;'should'&lt;/code&gt; clause:

``` JSON
{
    "query": {
        "match_all": {}
    },
    "filter": {
        "bool": {
            "must": {
                "term": {"inStock": true}
            },
            "should": [
                {"term": {"hotOffer": true}}
            ]
        }
    }
}
```

The result is 319 hits. It's wrong result, again (should be 1).
If a &lt;code&gt;'bool'&lt;/code&gt; filter replace by a combination of &lt;code&gt;'and'&lt;/code&gt; and &lt;code&gt;'or'&lt;/code&gt; filters then all is OK.

``` JSON
{
    "query": {
        "match_all": {}
    },
    "filter": {
        "and": [
            {"term": {"inStock": true}},
            {
                "or": [
                    {"term": {"hotOffer": true}},
                    {"range": {"created": {"from": "2013-04-02"}}}
                ]
            }
        ]
    }
}
```

The result is 38 hits.

It seems that if both clause (&lt;code&gt;must&lt;/code&gt; and &lt;code&gt;should&lt;/code&gt;) are in &lt;code&gt;bool&lt;/code&gt; filter then results are incorrect. In 0.20.6 all works fine. 
</description><key id="13925892">2979</key><summary>Query DSL: Wrong result on bool filter with 'must' and 'should' clauses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">ssenkevich</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-03T08:31:35Z</created><updated>2013-11-19T15:59:18Z</updated><resolved>2013-05-03T16:31:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-03T09:52:46Z" id="17386633">This is a bug. I will fix it soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for Collections to TermsQuery/InQuery </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2978</link><project id="" key="" /><description>Currently passing in a collection such as List&lt;Integer&gt; to search a field in will not work unless you first convert it to an array. 
</description><key id="13922034">2978</key><summary>Add support for Collections to TermsQuery/InQuery </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dmcneil</reporter><labels /><created>2013-05-03T05:20:14Z</created><updated>2014-07-16T21:53:32Z</updated><resolved>2013-05-06T19:45:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-03T07:22:47Z" id="17381530">this looks reasonable. Do you think you can add a simple test to those additions and squash the commits into one?
</comment><comment author="dmcneil" created="2013-05-03T08:54:11Z" id="17384391">Commits are squashed and tests are now in there.
</comment><comment author="s1monw" created="2013-05-06T19:45:53Z" id="17503485">pushed to master &amp; 0.90 thx!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Improve stability of SimpleDataNodesTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2977</link><project id="" key="" /><description>Make sure that we are waiting for the new state to be propagated to the node where we are executing the followup query that depends on this state.
</description><key id="13917791">2977</key><summary>Improve stability of SimpleDataNodesTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-05-03T00:47:46Z</created><updated>2014-07-16T21:53:32Z</updated><resolved>2013-05-03T13:57:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Distance calculation using geo_distance filter is off by at least 100 miles.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2976</link><project id="" key="" /><description>Distance calculation using geo_distance filter is at least 100 miles wrong, here are all the curl commands to reproduce the issue. I am using ES 0.20.6.

https://gist.github.com/brupm/5504613
</description><key id="13906476">2976</key><summary>Distance calculation using geo_distance filter is off by at least 100 miles.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">brupm</reporter><labels /><created>2013-05-02T19:22:43Z</created><updated>2013-05-02T20:11:23Z</updated><resolved>2013-05-02T19:38:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brupm" created="2013-05-02T19:28:29Z" id="17359215">@kimchy @clintongormley would you guys please take a look at this one, I'll buy you a beer in your next San Diego trip :)
</comment><comment author="kimchy" created="2013-05-02T19:33:52Z" id="17359525">@brupm you have the lat and lon reversed, once I switched it, it works fine

```
### Delete and Create index with proper mappings

curl -X DELETE http://localhost:9200/search

curl -X POST http://localhost:9200/search -d '{
  "mappings": {
    "document": {
      "properties": {
        "name": {
          "type": "string"
        },
        "lat_lon": {
          "type": "geo_point"
        }
      }
    }
  }
}
'

### Index document

curl -X POST "http://localhost:9200/search/document/" -d '{
  "name": "Kimberly",
  "lat_lon": [
    -95.36,
    29.76
  ]
}'

### Search with a geo_distance filter and a radius of 100 miles.

curl -X GET 'http://localhost:9200/search/_search?pretty' -d '{
  "filter": {
    "geo_distance": {
      "lat_lon": [
        -96.61,
        32.8
      ],
      "distance": "100mi"
    }
  }
}'

```
</comment><comment author="kimchy" created="2013-05-02T19:38:00Z" id="17359755">chatted to @brupm on IRC, that was the problem, closing...
</comment><comment author="brupm" created="2013-05-02T20:11:23Z" id="17361603">Ok, this is _SUPER_ confusing. 

The docs are misleading, the example of the array one is incorrect. It does not communicate that it needs to be both indexed and searched as [lon,lat] 

If you index as lan,lon and search as lat,lon it will work most of the time, the higher the radius the more incorrect it will be, so with smaller radiuses one may confuse it with diving vs. arc distance. 

I prose the following: 
1. Clarify in the docs that for array both indexing and searching needs to be [lon,lat]
2. Fix the example for array as it is incorrect.
3. Provide an index example for the hash indexing ie.: `location: { lat: 29.76, lon: -95.36 }`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolating an item of a type that has a default _ttl mapping configured throws an error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2975</link><project id="" key="" /><description>Using elasticsearch version 0.90 - windows binaries.

When I try to percolate an item for an index/type that has a default _ttl defined, I get the following error: 

```
{
  "error":
  "MapperParsingException[failed to parse [_ttl]]; nested: AlreadyExpiredException[already expired [test]/[type1]/[null] due to expire at [5184000000] and was processed at [1367509482308]]; ",
  "status":400
}
```

Here are the steps to recreate the error:

```
curl -XPUT localhost:9200/test

curl -XPUT localhost:9200/test/type1/_mapping -d '{ 
        "type1": { 
             "_ttl": { 
               "enabled": true,
               "default": "60d"
             },
             "_timestamp": { "enabled": true } 
        }
      }'

curl -XPUT localhost:9200/_percolator/test/kuku -d '{ 
          "query" : { 
              "term" : { 
                "field1" : "value1" 
              } 
            }
        }'

curl -XGET localhost:9200/test/type1/_percolate -d '{ "doc" : { "field1" : "value1" }}'
```
</description><key id="13898091">2975</key><summary>Percolating an item of a type that has a default _ttl mapping configured throws an error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">MrSnark</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-02T16:07:05Z</created><updated>2013-05-24T16:12:46Z</updated><resolved>2013-05-24T16:12:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>JSON parsing tolerance level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2974</link><project id="" key="" /><description>This is mostly a possible enhancement.

I have spent a significant amount of time searching for a problem that ended up being trivial. I build the query from PHP and use json_encode to serialize to JSON before sending it to ElasticSearch.

I could not get highlighting to work. It turns out the built query was:

```
{"query":{"match":{"contents":{"query":"description","boost":1}}},"highlight":{"fields":{"contents":[]}}}
```

The problem is that the serialization used []&#160;instead of {}.

Technically there is no problem with ElasticSearch, but it would have saved a tremendous amount of time if it had simply rejected the query, or accepted the empty array.

Would it be possible to have some sort of strict mode enabled to reject keys containing invalid values or some other mechanisms to help identify this type of issue?

Seems like I was not alone on the IRC channel that encountered this sort of problem.
</description><key id="13894979">2974</key><summary>JSON parsing tolerance level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">lphuberdeau</reporter><labels><label>discuss</label><label>low hanging fruit</label></labels><created>2013-05-02T15:01:48Z</created><updated>2014-10-22T16:25:24Z</updated><resolved>2014-10-22T16:25:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-09T13:05:04Z" id="48466691">Similar to #6133 and #5887.
</comment><comment author="cfontes" created="2014-09-02T07:13:30Z" id="54116622">From the very basic knowledge I have about this, it looks like this kind of validation is spread among several classes for each type of call (Mappings rest API, CRUD API, etc...)

So it will probably need to be fixed on several places. Just like #6133 fixed only one instance of this.

Or we could create/update a possible abstraction on the main REST API entry point validating empty arrays and rejecting them before they get any further into the logic. (too much overhead???)

Please correct me, as I am probably completely wrong about it.

Should this be worked as a single case or should I wait for a definition for a broader fix for this empty array problem?
</comment><comment author="clintongormley" created="2014-09-06T14:34:19Z" id="54713903">@jpountz what do you think about ^^ ?
</comment><comment author="clintongormley" created="2014-10-17T08:33:51Z" id="59483195">@polyfractal Do you have any idea whether PHP can distinguish between an empty associative array and an empty normal array? Apparently empty associative arrays are rendered in JSON as [] instead of as {}, which is causing these errors all over the place.

One other possibility would be to just delete all empty arrays...
</comment><comment author="polyfractal" created="2014-10-22T15:33:13Z" id="60104213">@clintongormley @lphuberdeau Yeah, this a known (and irritating) problem with PHP.  The ES-PHP client has workarounds documented here:  

http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_dealing_with_json_arrays_and_objects_in_php.html

The default behavior of `json_encode()` will convert empty arrays into json arrays, while arrays holding other values will become an object.

This behavior can be changed with the `JSON_FORCE_OBJECT` flag:  `json_encode($myArray, JSON_FORCE_OBJECT)`, which will always convert empty arrays into objects.  However, this will break other parts of the ES DSL since empty arrays are sometimes desired.

The best solution is to explicitly mark empty objects with a native PHP object:

``` php
$myArray = array(
    'query' =&gt; array(
        'match' =&gt; array(
            'content' =&gt; 'quick brown fox'
        )
    ),
    'highlight' =&gt; array(
        'fields' =&gt; array(
            'content' =&gt; new \stdClass()  // empty stdClass will convert into empty {}
        )
    )
);

$request = json_encode($myArray);
```

Which is equivalent to:

``` json
{
    "query" : {
        "match" : {
            "content" : "quick brown fox"
        }
    },
    "highlight" : {
        "fields" : {
            "content" : {} 
        }
    }
}
```
</comment><comment author="clintongormley" created="2014-10-22T16:25:24Z" id="60112886">@polyfractal thanks for the clarification.  Given that this can be handled client side, rather than having to add ambiguous checks all over the Elasticsearch parsing code, I'm going to close this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BalancedShardAllocator looses custom settings if un-related settings changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2973</link><project id="" key="" /><description>The `BalancedShardAllocator` uses the default settings as the settings that are used if the relevant setting is not present in the settings object. Yet, this can cause lost settings in the allocator if the custom settings are not always passed in. 
</description><key id="13893166">2973</key><summary>BalancedShardAllocator looses custom settings if un-related settings changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-02T14:20:59Z</created><updated>2013-05-02T16:01:09Z</updated><resolved>2013-05-02T16:01:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Added a first set of hamcrest matchers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2972</link><project id="" key="" /><description>A first implementation of adding matchers and helper methods to elasticsearch, open for comments and discussion.

The following ones are supported

```
assertHitCount(searchResponse, 2);

// helper methods to easily access the first hits
assertFirstHit(searchResponse, hasId("foo")):
assertSecondHit(searchResponse, hasType("foo")):
assertThirdHit(searchResponse, hasIndex("foo")):

// methods to access all other hits
assertSearchHit(searchResponse, 5, hasId("10"));
// same as above, but maybe more readable
assertSearchHit(searchResponse.getHits().getAt(4), hasIndex("foo"));
```

I changed GeoFilterTests to show how it works.

Furthermore I inlined `assertHighlight()` from HighlighterSearchTests.
The ElasticsearchAssertions class should be used now as an assertion class
in order have a centralized class for every developer to look at.
</description><key id="13890128">2972</key><summary>Added a first set of hamcrest matchers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-05-02T13:05:29Z</created><updated>2014-07-16T21:53:33Z</updated><resolved>2013-05-03T07:53:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Parent-Child: Properly cache parent child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2971</link><project id="" key="" /><description>Properly cache parent/child queries in the case they are wrapped in a compound filter.
</description><key id="13884484">2971</key><summary>Parent-Child: Properly cache parent child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-02T10:05:51Z</created><updated>2013-05-30T09:06:16Z</updated><resolved>2013-05-02T10:09:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Properly cache parent child queries and filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2970</link><project id="" key="" /><description>Properly cache parent/child queries and filters in the case they are wrapped in a compound filter.
</description><key id="13884210">2970</key><summary>Properly cache parent child queries and filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>v0.20.7</label></labels><created>2013-05-02T09:56:53Z</created><updated>2013-05-02T10:05:23Z</updated><resolved>2013-05-02T10:02:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Analysis: Have the hunspell filters do dedup by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2969</link><project id="" key="" /><description>For better out-of-the-box experience, the hunspell token filters should be set with `"dedup" : true` by default (it's always possible to set it to `false` in the filter configuration)
</description><key id="13869427">2969</key><summary>Analysis: Have the hunspell filters do dedup by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">uboness</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-01T22:24:45Z</created><updated>2013-05-30T09:06:25Z</updated><resolved>2013-05-01T22:24:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-05-01T22:24:56Z" id="17310374">closed by: f430953ca11ed6900fc00e1b723b07388fa37781
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>From and Size ignored when not in query string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2968</link><project id="" key="" /><description>The following didnt work until I've added from and size to query string like this:
http://localhost:9200/myindex/_search?from=0&amp;size=12

It's just stuck at defaults: 0/10

``` javascript
{
    "filtered": {
        "query": {
            "query_string": {
                "query": "auto"
            }
        },
        "filter": {
            "term": {
                "lang_id": "1"
            },
            "range": {
                "date_expires": {
                    "gt": "2013-05-01 22:16:07"
                }
            }
        }
    },
    "sort": {
        "deal_id": {
            "reverse": "true"
        }
    },
    "from": "0",
    "size": "12"
}
```
</description><key id="13868943">2968</key><summary>From and Size ignored when not in query string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tishma</reporter><labels /><created>2013-05-01T22:10:46Z</created><updated>2013-05-01T22:58:22Z</updated><resolved>2013-05-01T22:58:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-01T22:12:12Z" id="17309878">you need to wrap the `filtered` element in a `query` element, we should fail if this structure is provided..., can you provide with a full curl recreation, so we can see why we don't fail with this malformed search request, and we will fix it?
</comment><comment author="tishma" created="2013-05-01T22:17:08Z" id="17310071">Actually I use Elastica\Query\Builder, and it appears to wrap `filtered` in `query` twice for some reason, which is why I removed it.
</comment><comment author="tishma" created="2013-05-01T22:17:34Z" id="17310088">Thanks for the quick reply, btw :)
</comment><comment author="kimchy" created="2013-05-01T22:18:37Z" id="17310146">@tishma so its working fine? you need a top level `query` element always to wrap your actual query (in your case `filtered`).
</comment><comment author="tishma" created="2013-05-01T22:21:05Z" id="17310233">I'm still trying to get it to work... WIll let you know.
</comment><comment author="tishma" created="2013-05-01T22:33:11Z" id="17310739">OK. My bad. I was sending empty request body when I was getting 12 results, and query is totally wrong... Should I continue elsewhere if you are willing to help?
</comment><comment author="tishma" created="2013-05-01T22:58:21Z" id="17311675">@kimchy Thanks. It works like this. I still have to see why query (and even sort) are being wrapped twice, but it is hardly your concern.

``` javascript
{
    "query": {
        "filtered": {
            "query": {
                "query_string": {
                    "query": "auto"
                }
            },
            "filter": {
                "term": {
                    "lang_id": "1"
                }
            }
        }
    },
    "sort": {
            "deal_id": {
                "order": "desc"
        }
    },
    "from": "0",
    "size": "12"

}
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API doesn't support both script and doc</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2967</link><project id="" key="" /><description>In my application I want to update a document by incrementing a counter using `script` and by passing a partial document to be merged in using `doc`. However, it appears that only one of these is applied:

```
curl -XPOST http://localhost:9200/foo/bar/aoeu -d '{"counter": 0, "foo": "bar"}'
curl -XPOST http://localhost:9200/foo/bar/aoeu/_update -d '{"script": "ctx._source.counter += 1", "doc": {"foo": "barbaz"}}'
curl  http://localhost:9200/foo/bar/aoeu
# Returns: {"_index":"foo","_type":"bar","_id":"aoeu","_version":2,"exists":true, "_source" : {"counter":1,"foo":"bar"}}
```

The counter is incremented but the foo key's value isn't updated.

I expected that my document would be merged and that my script would be executed. But it seems that if `doc` is passed, then `script` (as well as `upsert`) is disregarded. The documentation doesn't mention this and the server doesn't provide a warning (as far as I can tell). 

This should either be described in the documentation or combinations of doc, script, and upsert should be accepted and applied. 
</description><key id="13860193">2967</key><summary>Update API doesn't support both script and doc</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">akahn</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-01T18:40:19Z</created><updated>2013-05-10T14:43:43Z</updated><resolved>2013-05-10T14:43:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-05-02T14:23:01Z" id="17340770">I'm not sure if this should be supported, it can lead to unexpected behaviour (like what executes first). It is not a common use case. I think the best way to handle this is to just add your partial document to the script itself.

If both `script` and `doc` is specified, then `doc` is ignored. I agree the documentation should  be clearer about this.
</comment><comment author="akahn" created="2013-05-02T14:27:41Z" id="17341027">Thanks. Adding my document to the script itself is the workaround I chose. 

What do you think of elasticsearch returning a client error when multiple update methods are provided?
</comment><comment author="martijnvg" created="2013-05-02T14:36:55Z" id="17341588">Returning a client error makes sense when both `script` and `doc` is specified. This should be added.

Btw: you can also add your partial document as a parameter to the script. This is a nice optimisation because it allows Elasticsearch to cache the compiled version of the script.
</comment><comment author="akahn" created="2013-05-02T15:13:46Z" id="17343852">Cool. I will take a look at that and try to submit a patch.

Can you explain what you mean by adding a partial document as a parameter to the script? Right now I'm just doing `ctx._source.count += 1; ctx._source.foo = 'barbaz';&#8230;`.
</comment><comment author="akahn" created="2013-05-09T20:59:01Z" id="17689432">@martijnvg The above patch makes elasticsearch consider an update request invalid if it contains both both doc and script.
</comment><comment author="martijnvg" created="2013-05-09T21:46:22Z" id="17692026">@akahn Looks good, I will pull this in soon.

I meant that `1` and `barbaz` are provided as argument to the script like this:

```
"script" : "ctx._source.counter += count; ctx._source.foo = foo;",
    "params" : {
        "count" : 4,
        "foo" : "barbaz"
    },
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add index based nested sort mode</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2966</link><project id="" key="" /><description>Add index based nested `sort_mode`, that allows the sort by fields from specified nested inner objects identified by an index. This can be a useful `sort_mode` if the order of the nested inner objects has a meaning.

The index based `sort_mode` should look like this:
- `index_1` - Instructs the nested sorting to only look at the second element.
- `index_9` - Instructs the nested sorting to only look at the tenth element.
- `index_first` - Instructs the nested sorting to only use the first nested inner object. 
- `index_last` - Instructs the nested sorting to only use the last nested inner object. 

Relates to #2662 
</description><key id="13846516">2966</key><summary>Add index based nested sort mode</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>discuss</label><label>feature</label></labels><created>2013-05-01T12:18:26Z</created><updated>2014-10-17T08:24:20Z</updated><resolved>2014-10-17T08:24:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-05-01T12:33:43Z" id="17279335">either:
- `index_1` - Instructs the nested sorting to only look at the **first** element.
- `index_9` - Instructs the nested sorting to only look at the ninth element.

or
- `index_1` - Instructs the nested sorting to only look at the second element.
- `index_9` - Instructs the nested sorting to only look at the **tenth** element.

:)
</comment><comment author="martijnvg" created="2013-05-01T12:34:22Z" id="17279354">Oops :)
</comment><comment author="sticky1" created="2013-07-09T18:55:50Z" id="20696900">Any chance for this feature to be included in immediate release.
</comment><comment author="ghost" created="2014-03-12T12:03:17Z" id="37400738">Is there some ETA for this?
</comment><comment author="PankaJJakhar" created="2014-06-14T06:04:34Z" id="46079610">Hi @martijnvg I want to work on it using `QuickSort algorithm's` extensive approach that will help me to find the Nth largest or smallest item in a list.

`index_1` `index_2`  `index_9` `index_first` `index_last`  
How can I do that? Code changes?
</comment><comment author="jpountz" created="2014-09-05T08:39:04Z" id="54598960">It seems to me that this can be achieved by indexing some properties on the nested documents (eg. their indexed in the array of nested docs) and then using a [nested_filter](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-sort.html#_sorting_within_nested_objects) to select which nested documents to use for sorting?
</comment><comment author="clintongormley" created="2014-09-06T17:36:14Z" id="54721810">Would anybody like to expand on the use case for this? I'm struggling to come up with anything.
</comment><comment author="martijnvg" created="2014-09-10T07:03:49Z" id="55078415">I can't come up with a real use case. I think we should close this issue.
</comment><comment author="sticky1" created="2014-09-10T08:22:28Z" id="55084807">This is useful if I want to sort the root documents based on the first matching nested document for the applied filter condition. In the same way, sort on the last matching nested document. For example, a patient record has multiple diagnoses as nested documents. Then, I want to sort patients based on the first diagnosis they have. This is quite important in scenario like this.
</comment><comment author="martijnvg" created="2014-09-10T09:23:32Z" id="55090758">@sticky1 So the actual nested sort field is different than the create date or other property that defines what diagnosis is first and what is last? Ok I see, even with the `nested_filter` you can't 'select' the first or last diagnosis, but with this proposed index based sort_mode you will need to rely on the order of the nested diagnostic objects in the _source.
</comment><comment author="sticky1" created="2014-09-10T10:21:44Z" id="55096752">Yes. We write the _source in order.  However, some are filtered out and some are filtered in by the nested_filter. So, we need the order based on the first or last diagnosis from among the filtered in nested documents.
</comment><comment author="clintongormley" created="2014-10-17T08:24:20Z" id="59482133">Closing in favour of #8127
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PR: Added support for sort_mode `avg` for sorting by geo_distance.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2965</link><project id="" key="" /><description>Relates to #2962
</description><key id="13844737">2965</key><summary>PR: Added support for sort_mode `avg` for sorting by geo_distance.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-05-01T10:54:31Z</created><updated>2015-05-18T23:41:59Z</updated><resolved>2013-05-01T11:22:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-01T10:57:10Z" id="17276675">Looks good, I think we can push this to both master and 0.90 branch.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PR: return proper result code for delete by query api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2964</link><project id="" key="" /><description>PR for #2963
</description><key id="13843582">2964</key><summary>PR: return proper result code for delete by query api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-05-01T09:55:58Z</created><updated>2015-05-18T23:42:02Z</updated><resolved>2013-05-01T10:05:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-05-01T09:57:58Z" id="17275183">Looks good, lets pull it in.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return proper status code in case of failure for delete by query api </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2963</link><project id="" key="" /><description /><key id="13843489">2963</key><summary>Return proper status code in case of failure for delete by query api </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v1.0.0.Beta1</label></labels><created>2013-05-01T09:52:19Z</created><updated>2013-05-01T10:05:44Z</updated><resolved>2013-05-01T10:05:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Sorting: Support sort_mode average for geo distance sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2962</link><project id="" key="" /><description>Implement `average` sort_mode for sorting by geo distance.
</description><key id="13842676">2962</key><summary>Sorting: Support sort_mode average for geo distance sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-05-01T09:14:51Z</created><updated>2013-05-30T09:06:36Z</updated><resolved>2013-05-01T11:22:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PropertyAccessException during Indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2961</link><project id="" key="" /><description>We get a small number of PropertyAccessException exceptions during indexing in a custom score script, if we stop indexing we have zero exceptions (approx 5% of requests). Is there a possibility to catch them in mvel or is this a bug?

Version: 0.90RC2 but had the same problem in older versions too.
</description><key id="13841651">2961</key><summary>PropertyAccessException during Indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">schaelle</reporter><labels /><created>2013-05-01T08:25:47Z</created><updated>2013-05-01T09:30:39Z</updated><resolved>2013-05-01T09:30:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Percolator not working with referenced parent/child queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2960</link><project id="" key="" /><description>I have a `parent` and a `child` collection. The child has the parent properly declared, and all the _search queries seem to be working correctly. But the percolator doesn't seem to work with the has_child query. Is this feature implemented?
</description><key id="13831293">2960</key><summary>Percolator not working with referenced parent/child queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">randunel</reporter><labels><label>:Parent/Child</label><label>adoptme</label><label>feature</label></labels><created>2013-04-30T22:55:53Z</created><updated>2016-08-31T07:54:37Z</updated><resolved>2016-08-31T05:50:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-04-30T22:59:32Z" id="17259680">Because the percolator API is processing one document at a time, it doesn&#8217;t support queries and filters that run against child and nested documents such as has_child, has_parent, top_children, and nested.
</comment><comment author="javanna" created="2014-03-04T11:52:27Z" id="36616807">Nested documents support was added to the percolator in #5082.

Parent/child is still not supported but we might be able to support it in the future, reopening this issue in the meantime.
</comment><comment author="rore" created="2014-03-04T11:57:36Z" id="36617180">+1, we have a strong need for this.
</comment><comment author="razafinr" created="2014-04-28T12:40:15Z" id="41553147">Any information about Parent/Child document support in future version ? Having an application based on this kind of relations and I am stuck while using percolator
</comment><comment author="mgreene" created="2014-09-13T15:18:30Z" id="55496933">We would have a need for this also. Currently we've had to refactor our child documents into nested fields inside the parent doc. The trade off here is that our parent docs are being reindexed faster than they other would. Doesn't dramatically impact our deployment but would prefer to leverage child documents for their strengths in conjunction with percolators. 
</comment><comment author="clintongormley" created="2014-11-29T15:09:31Z" id="64954656">@martijnvg do you think it is feasible that we could support parent/child in the percolator in the future? Or is it just a no go?
</comment><comment author="mgreene" created="2014-11-29T17:00:39Z" id="64957993">Actually after thinking about this for a while, support for parent/child in the percolator doesn't make a ton of sense, at least for our use case.

One of the main reasons to use child documents in the first place is that you anticipate a lot of writes and don't want to trigger a parent doc reindex every time. 

We have decently high cardinality (high hundreds in some cases) with our child documents so paging every single one of those into memory to throw against the percolator would prove to be prohibitive as the data set continues to grow. I suspect this was one of the reasons why it was not supported in the first place.
</comment><comment author="clintongormley" created="2014-12-01T09:45:10Z" id="65040689">@mgreene That's my thinking too, but I'll leave this here in case @mvg has some cleverer way of implementing this.
</comment><comment author="martijnvg" created="2014-12-01T11:30:13Z" id="65052242">@mgreene @clintongormley I think ES can support parent/child queries in the percolator, but it is odd to do this for the reasons @mgreene mentioned. (store the documents in memory and toss it away and perform the next percolator request).

What I think would be a nice addition to the percolator is to support percolating multiple documents in a single percolator call. So instead of defining a single `doc` element, the percolator should then support a `docs` element which would allow an array of documents. Regardless if parent/child should be supported this on its own can be a good addition. 
</comment><comment author="jpountz" created="2016-08-24T15:08:21Z" id="242098328">I suggest that we document that the percolator does not support parent/child queries and close this.
</comment><comment author="martijnvg" created="2016-08-25T09:59:14Z" id="242337079">The fact that these queries can't be used is already documented: https://github.com/elastic/elasticsearch/blob/master/docs/reference/mapping/types/percolator.asciidoc#limitations

In order to make it more clear that these queries are not supported, I also suggest that fail adding a percolator query that contains a `has_child` or `has_parent` query in the same as we fail when a percolator query contains `range` query with range based on the current time (now).
</comment><comment author="jpountz" created="2016-08-25T10:07:30Z" id="242338944">+1
</comment><comment author="jpountz" created="2016-08-31T07:54:37Z" id="243687055">Thanks @martijnvg !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alternate model for field AND logic within MultiMatch query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2959</link><project id="" key="" /><description>I wrote a patch to MultiMatch query that provides more natural and processing when considering multiple fields.

Consider document with fields:
title:  Something
description: featured on their 1969 album Abbey Road
author: Beatles
Now if I take user's input and run a query to match my documents, it would be natural to consider ether the dreaded _all field or a multi_match query like:
multi_match:{"query":"Something Beatles", "fields":["title", "description", "author"], "operator":"and"}

Which would get transformed into a boolean query such as:
(+title:something +title:beatles) (+description:something +description:beatles) (+author:something +author: beatles)

There is no match for our document! From human input perspective often the most natural way to AND multi-field search is to ensure each term is matched somewhere across all fields such as:
+(title:something description:something author:something) +(title:beatles description:beatles author:beatles)

My patch does exactly that and it also accounts for use of multiple analyzers which may remove tokens from some fields (ex: The Beatles). If a token is skipped by an analyzer it will be turned into a should requirement on remaining fields instead of a must.

I am using facilities of match query for minimum should match as well as fuzzy processing so a new match type felt natural. 
multi_match:{"query":"Something Beatles", "fields":["title", "description", "author"], "type":"across"}
</description><key id="13827989">2959</key><summary>Alternate model for field AND logic within MultiMatch query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">tarass</reporter><labels /><created>2013-04-30T21:25:33Z</created><updated>2014-06-17T16:18:21Z</updated><resolved>2014-03-12T20:38:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-02T20:09:52Z" id="17361535">this looks interesting... I hope I can get to it soon!
</comment><comment author="s1monw" created="2013-05-03T08:43:51Z" id="17384008">hey @tarass I have been playing with similar ideas in many projects where you have enough knowledge / structure about your data to match `across` fields. I think your assumption here is maybe the most generic and it makes perfect sense. Yet, I am not sure about how you handle different anaysis chains. I think we should somehow align this with the actual positions that are returned from the position increment attribute and take PositionLengthAttribute into account that tells us how many tokens a single token spans in the case of a multi term synonym or things like that (ie. word delimiter creates that one as well). I'd want to extract this maybe in a more general datastructure where you can align several analysis output in a graph or something like that.

Does this make sense?
</comment><comment author="tarass" created="2013-05-03T18:18:29Z" id="17409905">It does make sense in concept, but I'll have to do some testing on how PositionLengthAttribute works. The delimiter case is something I was concerned about but didn't know how to fix. I'll get out an update in a few days.
</comment><comment author="gibrown" created="2013-07-15T21:16:09Z" id="21004476">Hi @tarass any update on this patch? We've run into the same problem, and I'd rather have a solution that is built into ES than hack together an ugly query on the client side.

Let me know if I can help get this over the finish line.
</comment><comment author="tarass" created="2013-08-28T03:40:54Z" id="23387878">Have really not had the time to finish the switch to PositionLengthAttribute. Since someone else needs it, I'll try and find the time.
</comment><comment author="gibrown" created="2013-08-29T22:02:26Z" id="23527031">Thanks @tarass that would be really great. I'd hoped to get to working on it myself this month, but I'm bogged down with other things at the moment.
</comment><comment author="andrewmacheret" created="2013-09-04T00:23:18Z" id="23757792">+1
</comment><comment author="skade" created="2013-10-22T10:16:55Z" id="26791397">+1

Just ran into this problem at a client and I think the described assumption is very valid, especially as {multi_}match is propagated as an alternative to query_string, which can easily have this behaviour.
</comment><comment author="thorsten" created="2013-10-22T10:42:12Z" id="26792685">+1
</comment><comment author="s1monw" created="2014-01-30T16:30:05Z" id="33704636">for those of you that are interested I linked some WIP that I have ^^ and if anybody is up for some feedback that would be much appreciated
</comment><comment author="s1monw" created="2014-03-12T20:38:35Z" id="37461364">I am closing this since `cross_fields` is in for `1.1`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Accept loopback interfaces in the network.host setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2958</link><project id="" key="" /><description>Closes #2924. Adds support for loopback interfaces such as `_lo0_` in `network.host` and other network settings.
</description><key id="13816778">2958</key><summary>Accept loopback interfaces in the network.host setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-04-30T17:11:46Z</created><updated>2014-07-16T21:53:34Z</updated><resolved>2013-05-03T19:18:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Improve stability of SimpleRecoveryLocalGatewayTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2957</link><project id="" key="" /><description>Fixed testX and testSingleNodeNoFlush by specifying mapping on index creation instead of using dynamic mapping. Dynamic mapping is updated on the cluster level asynchronously and if mapping changes are not applied to the cluster state before node is closed, these changes are not be available after node restart. While data added in the test is preserved, due to absence of mapping, the test still fails. This is a known issue that we are not planning to fix at the moment.
</description><key id="13814116">2957</key><summary>Improve stability of SimpleRecoveryLocalGatewayTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels /><created>2013-04-30T16:14:15Z</created><updated>2014-07-16T21:53:35Z</updated><resolved>2013-04-30T19:03:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-30T18:12:57Z" id="17244522">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timing problem with aliases and index status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2956</link><project id="" key="" /><description>Hi everyone,

I tested elastic search 0.90 today and some of my test failed. I've isolated the minimum reproductible test case: https://gist.github.com/Lothiraldan/5488931.

I create an index without specific settings named test.1.1 I set two aliases test.read and test.write to test.1.1.

Then I get the index status of each alias (via localhost:9200/test.read/status) and extract the real index name behind alias (it's exactly how the python driver pyes does the job).

Then I create a second one named test.1.2 I move the two aliases (test.read and test.write) to the freshly created index and try to do the same requests (get index status and extract real index name).

On elastic search 0.20.6, no problem. On elastic search 0.90, when I do the request, I get this result:

{
    "_shards": {
        "failed": 0, 
        "successful": 0, 
        "total": 10
    }, 
    "indices": {}, 
    "ok": true
}

It looks like a timing issue, I tried to add a sleep in my python code but I must add a ~300ms minimum else my test will fail.

I don't know if it's a bug or a desired behaviour, if someone can confirm the difference of output between the two version.

Thanks
</description><key id="13809001">2956</key><summary>Timing problem with aliases and index status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Lothiraldan</reporter><labels /><created>2013-04-30T14:29:06Z</created><updated>2013-06-24T12:56:58Z</updated><resolved>2013-06-24T12:56:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-30T15:12:38Z" id="17233323">Hi @Lothiraldan 

How many nodes are you using for this test?
</comment><comment author="Lothiraldan" created="2013-05-02T08:09:41Z" id="17325437">Only one,
I forget to give my configuration, here is: https://novapaste.novapost.net/paste/0a71c83787a62ad1ceb439b279f933b704cc4faa#vFtrtEWk5oA9Yn0r7MLcTRYIrybPAHNZR4vSIB5/JnY=

During my tests, I erase all indexes before launching them (with curl -XDELETE http://localhost:9200/_all).
</comment><comment author="spinscale" created="2013-05-03T13:33:26Z" id="17394147">Hey,

the link you supplied in your last comment, is 404. I'll try to reproduce this behaviour, so any bit of information is helpful.

Thanks a lot for reporting! I might come back with a few more questions.
</comment><comment author="spinscale" created="2013-05-03T15:27:38Z" id="17400526">I cannot reproduce this issue with your minimum sample above, I always get back the test1.2 alias in the last two lines as expected.

Can you provide some more information (especially your configuration). Just for complete reference (and to get other people to reproduce it). What JVM version are you running 0.90 on, and on what operating system?
</comment><comment author="Lothiraldan" created="2013-05-06T08:12:47Z" id="17470575">Hi, sorry here is my configuration: https://novapaste.novapost.net/paste/d8f1d7974c287eff9c401d5d602c1c8818fde322#oJT1FtxNUuLYh2ke+hwF3BN5Ru5XtcJ3ovouRKnpy1k=

I'm running elastic search on debian wheezy using this java version:
    java -version
    java version "1.6.0_27"
    OpenJDK Runtime Environment (IcedTea6 1.12.4) (6b27-1.12.4-1)
    OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode)

I tried on a fresh installation on an amazon ec2 instance but didn't remember with which jvm with the same result. I can retry if needed.
</comment><comment author="spinscale" created="2013-05-10T11:27:52Z" id="17715572">Tried to reproduce it with your configuration settings, no luck so far. Neither on mac os X nor on a virtualized ubuntu (using also an icedtea openjdk distribution with the same java version).

Can you reproduce it in a stable manner on ec2 or your local system?

Running out of ideas how to reproduce at the moment...
</comment><comment author="Lothiraldan" created="2013-05-14T09:16:21Z" id="17865176">Ok will try to reproduce it on ec2 this week.
</comment><comment author="spinscale" created="2013-05-27T07:14:11Z" id="18485450">Hey,

did you have any chance to reproduce this? I really want to make sure we dont have an issue here as this is important functionality, so sorry for bugging you :-)

Thanks!
</comment><comment author="Lothiraldan" created="2013-06-04T13:13:48Z" id="18907847">Hi @spinscale, I just tried on a raw ec2 instance and I can reproduce. Steps to reproduce:
- Start an ec2 instance with ubuntu 12.04 LTS
- wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.1.deb
- sudo dpkg -i elasticsearch-0.90.1.deb
- sudo apt-get update
- sudo apt-get install -f
- touch test.sh
- nano test.sh
- bash test.sh

The java version installed was openjdk 7u21.

Oddly, my tests didn't fail anymore on my local environment with latest release. 
</comment><comment author="spinscale" created="2013-06-06T11:20:39Z" id="19038873">hey,

I was finally able to reproduce this on a virtualbox based VM locally as well. It fails about 20% of the time (sometimes twenty test runs it is working fine, pretty hard to debug). I will try to isolate this (JDK version, some OS setup) a bit more.

Thanks a lot for your work and help so far!
If you have any more indicating ideas, I am also really happy to hear them :-)

**Update**: This breaks on my ubuntu VM with openjdk7, oracle java 6 and oracle java 7 - so I'll exclude JVM as a problem for now.
</comment><comment author="Lothiraldan" created="2013-06-06T12:51:49Z" id="19042827">Cool, I just rerun my tests, I was on the bad branch. They continue to fail with 0.90.1. I hope you will found the problem, we would like to give a try to 0.90.X branch.
</comment><comment author="spinscale" created="2013-06-06T13:58:14Z" id="19046652">I think I found the issue. And it is luckily not a bug, but rather again I overlooked coping with the async nature of elasticsearch. So here is what happens (with your current sample).
- test.1.1 gets created, aliases added
- status read, everything fine
- test.1.2 gets created
- aliases get added, but here the adding of an alias does not work, because the shards are still in `INITIALIZING` state, instead of being in `STARTED` state. So adding the alias fails.

The solution is simple, just wait for the cluster state to jump to yellow after creating the index and before creating the aliases and you are safe

```
curl -XGET 'http://localhost:9200/_cluster/health?wait_for_status=yellow'
```

I ran the script above with the line above added after the indices are created and I didnt get any errors when running it 2000 times in the VM.

Now the answer why this happens merely on virtualized machines is also pretty easy. They're way slower (the reason I couldnt reproduce it on my workstation I guess).
</comment><comment author="Lothiraldan" created="2013-06-06T15:42:40Z" id="19054151">Ok, get it.

But why curl -i -XPOST 'http://localhost:9200/_aliases' -d '...' returns a 200 code if alias adding fails? Shouldn't we get a 503 status code with Retry-After header?

```
503 Service Unavailable

The server is currently unable to handle the request due to a temporary
overloading or maintenance of the server. The implication is that this
is a temporary condition which will be alleviated after some delay. If
known, the length of the delay MAY be indicated in a Retry-After header.
If no Retry-After is given, the client SHOULD handle the response as it
would for a 500 response.

Note: The existence of the 503 status code does not imply that a
server must use it when becoming overloaded. Some servers may wish
to simply refuse the connection.
```
</comment><comment author="Lothiraldan" created="2013-06-06T15:49:46Z" id="19054686">I just retried with the 0.20.6 release, I added the current line before creating the second couple of aliases:

```
curl -XGET 'http://localhost:9200/_cluster/health'
```

It show that cluster state is also red but so why alias adding works ? What changed between 0.20.6 and 0.90. Was the alias adding command waiting for cluster state changes to yellow?
</comment><comment author="spinscale" created="2013-06-06T16:49:12Z" id="19058399">We talked about this issue internally a bit and came to the conclusion that adding aliases should work, even if the shards of an index are not yet marked as `STARTED` (because everything for adding is existent). We will need to investigate this further in order to be sure what happened.

In the meantime I hope you have at least a solution to work with.
</comment><comment author="spinscale" created="2013-06-10T13:06:28Z" id="19197134">Hey, after spending some more time (and help) I think I found the issue - and the reason you are seing this on 0.90 only, even though it already existed in 0.20.

If you want to reproduce it more easily, just create an index with one hundred or more shards.

This is what happens
- You are creating an index, adding the aliases
- The aliases are added, regardless of the status of the index (can be red as well as in your case)
- You call the _status api - the status API return this

```
{"ok":true,"_shards":{"total":100,"successful":0,"failed":0},"indices":{}}
```
- As you can see, the shards are not yet allocated at all and therefore no indices are linked to it

Possible workaround in order to be sure that the aliases have been added without relying that the indices are in an operational state is to check the cluster state API:

```
curl -s 'localhost:9200/_cluster/state?pretty' | python -c "import json; import sys; x = json.load(sys.stdin); print x['metadata']['indices']['test.1.2']['aliases']"
```

The reason for this to happen merely in 0.90 is most likely compared to 0.20 is the introduction of throttling, which slows down index creation.

Hope this helps :-)
</comment><comment author="nonflet" created="2013-06-20T10:43:21Z" id="19743793">Hey everybody !
So, what's the fix for this bug now ? Using 0.20 ? Or 0.19 ? 
</comment><comment author="spinscale" created="2013-06-24T12:56:58Z" id="19904996">Hey,

sorry for not being clear enough in the first case. I try to explain better. First, this is not a bug and has always been handled the same, no matter in which elasticsearch version.

The reason why this occurs is the following:
- An index gets created, but is not yet ready to index data
- Because it is not yet ready, there is no `/_status` information for this index provided
- When an alias is added, the cluster state gets updated. This happens totally independent from the creation of the index and the state the index is in
- As soon as it is ready, the `/_status` API can be called and contains all information
- If an alias has been added, but the index is still not ready (because not all primary shards have been initialized yet), the `/_status` will still not show the alias (even the if is there)

Consider the `/_status` API for an index only valid if the index is ready to index data

In short: If you need to have information about aliases, regardless if they can be used, consider using the cluster state API, otherwise use the index status API.

I hope it is clear now, otherwise, feel free to ask further questions and I'll try to explain better.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContentMapValues.filter now works with nested arrays</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2955</link><project id="" key="" /><description>The filter method of XContentMapValues actually filtered out nested
arrays/lists completely due to a bug in the filter method, which threw
away all data inside of such an nested array.

Closes #2944
This bug was a follow up problem, because of the filtering of nested arrays
in case source exclusion was configured.
</description><key id="13808668">2955</key><summary>XContentMapValues.filter now works with nested arrays</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-30T14:22:15Z</created><updated>2014-06-15T07:36:31Z</updated><resolved>2013-04-30T16:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-30T16:05:09Z" id="17236620">Forgot to close by commit. Closed by a694e97ab97deccb6c533176737ecb055a95e54a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>please update install instructions - fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2954</link><project id="" key="" /><description>Incomplete information on to make this software.

Please update the instructions on how to complete this process manually

/usr/local/src # git clone git://github.com/elasticsearch/elasticsearch.git
/usr/local/src # cd elasticsearch/
/usr/local/src/elasticsearch # bin/elasticsearch -f
bin/elasticsearch.in.sh: line 3: $ES_CLASSPATH:$ES_HOME/lib/${project.build.finalName}.jar:$ES_HOME/lib/_:$ES_HOME/lib/sigar/_: bad substitution
You must set the ES_CLASSPATH var
</description><key id="13807079">2954</key><summary>please update install instructions - fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">john-clark</reporter><labels><label>enhancement</label><label>v0.90.8</label><label>v1.0.0.RC1</label></labels><created>2013-04-30T13:49:02Z</created><updated>2013-12-16T08:55:49Z</updated><resolved>2013-04-30T14:05:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-04-30T13:52:15Z" id="17228252">Where did you get theses instructions?
</comment><comment author="s1monw" created="2013-04-30T14:05:09Z" id="17228993">please download an official release or use the package that is produced from `mvn package` (in target/release) the instructions you got are wrong.
</comment><comment author="dadoonet" created="2013-04-30T14:06:21Z" id="17229062">Yes. Official documentation is here: http://www.elasticsearch.org/overview/#installation
</comment><comment author="john-clark" created="2013-04-30T14:11:15Z" id="17229325">nice support...rm -rf elastics\* 

if you ever decide to document your project I may look at it again. probably not.

tip: make instructions are typically put in the readme for us old timers
</comment><comment author="kimchy" created="2013-04-30T14:13:28Z" id="17229456">@john-clark unsure where you are coming from, read the README: https://github.com/elasticsearch/elasticsearch/blob/master/README.textile, it mentions downloading ES and then running it using `bin/elasticsearch`, or, if you head all the way to the bottom, talks about building it from source.
</comment><comment author="s1monw" created="2013-04-30T14:14:33Z" id="17229518">if you want to build from the source you should build a release with `mvn package` that is pretty standard and I am happy to help. Yet, this is an issue and not a support list so you should get pretty descent support there. If you want to get started with ES there is a pretty good link that is straight forward (http://www.elasticsearch.org/guide/reference/setup/) I am all sorry if that came across as a complaint or anything, I was just making sure that this is the wrong path and you might wanna start from a pre-build package. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TransportAnalyzeAction causes StringIndexOutOfBoundsException on first attempt to analyze a numeric field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2953</link><project id="" key="" /><description>I'm seeing the following in Elastic 0.90.0 during the first attempt attempt to initiate an AnalyzeRequest against a numeric field:

```
Caused by: java.util.concurrent.ExecutionException: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:272)
        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
        ... 38 more
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.&lt;init&gt;(String.java:207)
        at org.elasticsearch.index.analysis.NumericTokenizer.reset(NumericTokenizer.java:59)
        at org.elasticsearch.index.analysis.NumericTokenizer.reset(NumericTokenizer.java:54)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:202)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:57)
        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        ... 1 more
```

The cause of the problem appears to be that the `reset()` method of the `NumericTokenizer` is called twice the first time `analyzer.tokenStream()` is called for a the field in a thread:
- The first call happens as a result of Lucene calling `createComponents()` on the `NumericAnalyzer`.  This eventually results in the construction of a `NumericTokenizer` with a `char[]` buffer, which calls `reset()` during construction.  This first `reset()` call leaves the `FastStringReader` associated with the Tokenizer with a `next` value that is equal to the  length of that buffer because it reads all the chars out of the reader.
- The second call happens as a result of the explicit call to `reset()` in `TransportAnalyzeAction` immediately after the `TokenStream` has been retrieved from the analyzer.  Unfortunately calling the method a second time triggers the `if (next &gt;= length)` check in the `read()` method of the associated `FastStringReader` to return -1.  NumericTokenizer then tries to use -1 as the number of chars to use when constructing a `String`, which throws the `StringIndexOutOfBoundsException` above.
</description><key id="13795998">2953</key><summary>TransportAnalyzeAction causes StringIndexOutOfBoundsException on first attempt to analyze a numeric field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">recoil</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-04-30T08:02:04Z</created><updated>2013-04-30T14:58:10Z</updated><resolved>2013-04-30T14:58:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>TransportAnalyzeAction causes IllegalArgumentException: NumericTokenStream does not support CharTermAttribute</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2952</link><project id="" key="" /><description>I'm getting the following Exception in Elastic 0.90.0 when I attempt to initiate an AnalyzeRequest against a numeric field:

```
Caused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: NumericTokenStream does not support CharTermAttribute.
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)
        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:272)
        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)
        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)
        ... 38 more
Caused by: java.lang.IllegalArgumentException: NumericTokenStream does not support CharTermAttribute.
        at org.apache.lucene.analysis.NumericTokenStream$NumericAttributeFactory.createAttributeInstance(NumericTokenStream.java:136)
        at org.apache.lucene.util.AttributeSource.addAttribute(AttributeSource.java:271)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:203)
        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:57)
        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        ... 1 more
```

I believe that this is probably caused by the following change introduced in Lucene 4:

_NumericTokenStream now works directly on byte[] terms. If you plug a TokenFilter on top of this stream, you will likely get an IllegalArgumentException, because the NTS does not support TermAttribute/CharTermAttribute_

(From http://lucene.apache.org/core/4_2_1/changes/Changes.html#4.0.0-alpha.changes_in_backwards_compatibility_policy)

Line 203 of TransportAnalyzeAction is attempting to add a CharTermAttribute to a TokenStream instance.
</description><key id="13795407">2952</key><summary>TransportAnalyzeAction causes IllegalArgumentException: NumericTokenStream does not support CharTermAttribute</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">recoil</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-04-30T07:37:54Z</created><updated>2013-05-16T11:30:03Z</updated><resolved>2013-04-30T14:58:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-30T09:57:04Z" id="17218636">hey, so I am on the fence here to call this a bug. I mean I agree this is not nice and we should return either a clear exception that running this on a numeric field is not supported or return the same stuff as 0.20. Are you doing this intentionally and if so what is the usecase?
</comment><comment author="s1monw" created="2013-04-30T14:58:36Z" id="17232395">Closed via 773ea03
</comment><comment author="s1monw" created="2013-04-30T15:21:23Z" id="17233931">even if I closed this one I am still interested in what your usecase is. Please feel free to add it here! and thanks again for reporting this.
</comment><comment author="Schuk" created="2013-05-15T15:16:01Z" id="17945011">I ran into the same problem while testing the analyzer. I have a field called src_ip which is of type IP

```
curl -XGET 'http://localhost:9200/test/_analyze?field=src_ip&amp;pretty' -d "192.168.0.1"
{
  "error" : "IllegalArgumentException[NumericTokenStream does not support CharTermAttribute.]",
  "status" : 500
}
```
</comment><comment author="s1monw" created="2013-05-15T20:55:59Z" id="17965619">how would you want this field to be analyzed? I mean what do you expect as a return value here? I think what you get is correct, you can't analyze an IP here.
</comment><comment author="clintongormley" created="2013-05-16T09:08:55Z" id="17989798">In these cases, couldn't we just return the actual term that is stored in
the index?

On 15 May 2013 22:56, Simon Willnauer notifications@github.com wrote:

&gt; how would you want this field to be analyzed? I mean what do you expect as
&gt; a return value here? I think what you get is correct, you can't analyze an
&gt; IP here.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2952#issuecomment-17965619
&gt; .
</comment><comment author="Schuk" created="2013-05-16T09:55:25Z" id="17991827">Yes I was expecting the term which is stored in the index.
I misunderstood the "token" for "term" in the analyze results. As an IP is stored as integer I had expected to see this integer.
</comment><comment author="s1monw" created="2013-05-16T11:30:03Z" id="17995542">the terms that are indexed here are opaque binary terms. they don't make much sense at all. I mean we can return just the number but it might be misleading. not sure if we should do that
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support source include/exclude for realtime GET</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2951</link><project id="" key="" /><description>Currently realtime GET does not take source includes/excludes into account.
This patch adds support for the source field mapper includes/excludes
when getting an entry from the transaction log. Even though it introduces
a slight performance penalty, it now adheres to the defined configuration
instead of returning all source data when a realtime get is done.

Closes #2829
</description><key id="13795240">2951</key><summary>Support source include/exclude for realtime GET</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-30T07:30:59Z</created><updated>2014-06-19T10:37:39Z</updated><resolved>2013-04-30T16:06:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-30T16:06:38Z" id="17236711">Forgot to close by commit. Closed by a694e97ab97deccb6c533176737ecb055a95e54a
</comment><comment author="damienalexandre" created="2013-07-23T11:08:15Z" id="21406985">Great!

Do not know if this can be related to an issue I have right now.
I run a cluster with a 0.90.1 node and a 0.90.0 one. 

The _source `excludes` parameter is set for months, but today I noticed that when updating a document, without any changes to the config or mapping, I get the excluded node in the source every-time (via search or REST get), both from the 0.90.1 node and from 0.90.0.

Forcing the flush fixed that - but that's temporary and hacky. _I will request ES upgrades to my admin sys_.
</comment><comment author="spinscale" created="2013-07-23T11:29:10Z" id="21407846">this feature was not part of 0.90.0, so that could be the problem. I'd advice to make sure that your cluster is running the same versions if possible... finding/pinning down bugs might become quite an issue otherwise with this as a long-term setup.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.90.0 Java API Compilation Error: "org.apache.lucene.util.ByteRef cannot be resolved"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2950</link><project id="" key="" /><description>I originally posted about the issue here: https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/dljiLl-cDFw

I think ES 0.90.\* is not getting JAR'd correctly. The google thread has a self-contained test that I created to reproduce the error.  

I have a patch for it which I can submit if need be.
</description><key id="13786651">2950</key><summary>0.90.0 Java API Compilation Error: "org.apache.lucene.util.ByteRef cannot be resolved"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dexter87</reporter><labels /><created>2013-04-29T23:46:21Z</created><updated>2013-04-30T17:45:44Z</updated><resolved>2013-04-30T08:54:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2013-04-30T08:46:11Z" id="17215868">The class org.apache.lucene.util.ByteRef was patched by ES before 0.90 and that version was included in the ES jar but this is no longer the case in 0.90

Note, ES did never include the Lucene jars by intention. You have to add this dependency separately. I agree this is confusing developers (there are more confusing things when it comes to shading dependencies into the ES jar and to the optional jars). I assume this is just because ES deployments can be upgraded to a fix version of Lucene without the release of a new ES version.

Of course, you can build your own version of ES, adding Lucene into the shaded jar. 
</comment><comment author="s1monw" created="2013-04-30T08:54:29Z" id="17216217">how can you build something with ES and not have the lucene core jar?
</comment><comment author="ash211" created="2013-04-30T17:45:44Z" id="17242734">This is a little confusing because the elasticsearch 0.20 used to be all we needed to use ES in a Java application, but now in 0.90 there are other dependencies that must be included.  I guess we never realized that there were other dependencies, since the ES jar seemed to be intended as a standalone mega-jar.  A couple questions then:

1) Are there other dependencies that need to be included besides lucene-core?  The full list of dependencies in the [0.90 pom.xml](https://search.maven.org/remotecontent?filepath=org/elasticsearch/elasticsearch/0.90.0/elasticsearch-0.90.0.pom)?
2) The Java API seems to be less frequently used than the REST API in all the examples.  I don't think it's a problem to depend on the Java API since the REST API is built atop the Java API, but are there other gotchas we should know about?
3) Is there a good place to add documentation on building projects using the Java API?  [This page](http://www.elasticsearch.org/guide/reference/java-api/) is probably the best spot.  Maybe these are just things I would know if I was more immersed in the pom ecosystem.

Regardless, thanks for the help @jprante and @s1monw , good to learn from the experts!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow _meta (or any other annotation mechanism) for all mapping properties not just for the mapping itself</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2949</link><project id="" key="" /><description>Very helpful in storing additional info per property. for example a display name or some other application specific annotations for a property
</description><key id="13775047">2949</key><summary>Allow _meta (or any other annotation mechanism) for all mapping properties not just for the mapping itself</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-04-29T18:49:42Z</created><updated>2013-08-28T10:08:48Z</updated><resolved>2013-08-28T10:08:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-28T10:08:48Z" id="23403671">Closing as duplicate of #2857 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>support more options for _all field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2948</link><project id="" key="" /><description>_all supports busting based on contributing field boost 
Similar support for position_offset_gap between contributing would be very helpful for more meaningful phrase seraches on _all. Gap could be the same for entire _all and configured in _all's config or prehaps by using alternative form of include_in_all - object rather than boolean

Actually allowing for include_in_all to be either boolean or object would allow configuring how a field contributes to all without breaking backward compatibility
</description><key id="13774935">2948</key><summary>support more options for _all field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-04-29T18:47:08Z</created><updated>2014-08-08T11:43:04Z</updated><resolved>2014-08-08T11:43:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T11:43:04Z" id="51591132">This can be done on copy_to fields instead.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.90.0 - ClassNotFoundException: org.apache.lucene.analysis.ReusableAnalyzerBase on runtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2947</link><project id="" key="" /><description>Hello,

I'm trying to upgraded to 0.90.0.
I get a runtime ClassNotFoundException for org.apache.lucene.analysis.ReusableAnalyzerBase.

I see it's now using Lucene 4.2.1.
https://lucene.apache.org/core/4_2_1/MIGRATE.html states "During the refactoring some package names have changed, and ReusableAnalyzerBase was renamed to Analyzer".

Any idea ? Bug ?
Thank you,

Olivier.

Stacktrace:

Caused by: java.lang.NoClassDefFoundError: org/apache/lucene/analysis/ReusableAnalyzerBase
    at java.lang.ClassLoader.defineClass1(Native Method)
    at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
    at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
    at org.apache.catalina.loader.WebappClassLoader.findClassInternal(WebappClassLoader.java:2836)
    at org.apache.catalina.loader.WebappClassLoader.findClass(WebappClassLoader.java:1160)
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1668)
    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1546)
    at java.lang.Class.getDeclaredConstructors0(Native Method)
    at java.lang.Class.privateGetDeclaredConstructors(Class.java:2404)
    at java.lang.Class.getDeclaredConstructors(Class.java:1853)
    at org.elasticsearch.common.inject.spi.InjectionPoint.forConstructorOf(InjectionPoint.java:177)
    at org.elasticsearch.common.inject.ConstructorInjectorStore.createConstructor(ConstructorInjectorStore.java:59)
    at org.elasticsearch.common.inject.ConstructorInjectorStore.access$000(ConstructorInjectorStore.java:29)
    at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:37)
    at org.elasticsearch.common.inject.ConstructorInjectorStore$1.create(ConstructorInjectorStore.java:33)
    at org.elasticsearch.common.inject.internal.FailableCache$1.apply(FailableCache.java:38)
    at org.elasticsearch.common.collect.ComputingConcurrentHashMap$ComputingValueReference.compute(ComputingConcurrentHashMap.java:356)
    at org.elasticsearch.common.collect.ComputingConcurrentHashMap$ComputingSegment.compute(ComputingConcurrentHashMap.java:182)
    at org.elasticsearch.common.collect.ComputingConcurrentHashMap$ComputingSegment.getOrCompute(ComputingConcurrentHashMap.java:151)
    at org.elasticsearch.common.collect.ComputingConcurrentHashMap.getOrCompute(ComputingConcurrentHashMap.java:67)
    at org.elasticsearch.common.collect.MapMaker$ComputingMapAdapter.get(MapMaker.java:883)
    ... 67 more
</description><key id="13759973">2947</key><summary>0.90.0 - ClassNotFoundException: org.apache.lucene.analysis.ReusableAnalyzerBase on runtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">blop</reporter><labels /><created>2013-04-29T13:32:17Z</created><updated>2013-04-29T14:08:25Z</updated><resolved>2013-04-29T14:08:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-29T13:34:32Z" id="17166694">hey, are you using any plugins or home-grown analyzers?
</comment><comment author="blop" created="2013-04-29T13:36:30Z" id="17166816">No, nothing special ;-)
I have no reference to ReusableAnalyzerBase.
</comment><comment author="blop" created="2013-04-29T14:08:25Z" id="17168617">Ok I found what the problem was ;-)

I still had a lucene-spellchecker 3.6.2 dependency that was pulled.
I removed it, now it's working fine!

Ol.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Current external versioning semantics not suitable for all use cases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2946</link><project id="" key="" /><description>As discussed in issue #2938 and pull request #2939, the versioning semantics are not suiteable for all cases. For instance, deleting an externally versioned document with its current version fails ("version conflict, current [4], provided [4]]") which is allowed in internal versioning. For instance, when mirroring changes to a database, using the last version of a just deleted object should work as expected.
</description><key id="13759465">2946</key><summary>Current external versioning semantics not suitable for all use cases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">sfussenegger</reporter><labels><label>feature</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-04-29T13:19:57Z</created><updated>2014-03-21T10:06:25Z</updated><resolved>2014-03-10T20:11:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Use Lucene Version that was used to create the index in Analysis</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2945</link><project id="" key="" /><description>Lucene ships with a version constant that is mainly used to provide consistent behaviour across lucene release versions. Lucene's Analysis capabilities are commonly applied at index and search time such that the search-time behaviour should be identical to the index-time behaviour in most of the cases. Currently ElasticSearch always uses the latest version from Lucene which can break backwards compatibility with the index for users that rely on behaviour that changed in new Lucene version. 

Users should always use the version the index was created with unless it's explicitly configured. 
</description><key id="13754928">2945</key><summary>Use Lucene Version that was used to create the index in Analysis</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-29T11:04:23Z</created><updated>2013-05-08T16:25:46Z</updated><resolved>2013-04-29T11:18:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mkresse" created="2013-05-08T16:22:52Z" id="17616920">This change seems to add a dependency to the currently required lucene version, even if only the TransportClient will be used to connect to a remote server. Maybe, instead of lucene's enum-values, their names could be used?
</comment><comment author="kimchy" created="2013-05-08T16:25:46Z" id="17617100">we use Lucene classes in several places (for example, when asking for explanation), so its better to have the dependency already there when using the client not to get hit by other aspects.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>source exclusion mapping prevents geo shape coordinates to be returned in query result source field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2944</link><project id="" key="" /><description>ES Version: 0.90.0.RC2

Steps to reproduce:

ok case, without source exclusion:

``` bash
curl -XDELETE 'http://localhost:9200/geo_test'
curl -XPUT 'http://localhost:9200/geo_test'
curl -XPUT 'http://localhost:9200/geo_test/item/_mapping' -d '{
    "item" : {
        "properties" : {
            "location" : {
                "type" : "object",
                "properties": {
                     "point": {"type": "geo_point"},
                     "area": {"type": "geo_shape"}
                }
            }
        }
    }
}'
curl -XPUT 'http://localhost:9200/geo_test/item/1' -d '{
    "location": {"point": [45.0, 45.0]}
}'
curl -XPUT 'http://localhost:9200/geo_test/item/2' -d '{
    "location": {
        "area": {
            "type" : "envelope",
            "coordinates" : [[44.0, 46.0], [45.0, 45.0]]
        }
    }
}'
curl -XPOST 'http://localhost:9200/geo_test/item/_search?pretty' -d '{
    "query": {"match_all": {}}
}'
```

returns the coordinates for geo_point and geo_shape items:

``` JSON
{
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 1.0,
        "hits": [{
            "_index": "geo_test",
            "_type": "item",
            "_id": "1",
            "_score": 1.0,
            "_source": {
                "location": {
                    "point": [45.0, 45.0]
                }
            }
        }, {
            "_index": "geo_test",
            "_type": "item",
            "_id": "2",
            "_score": 1.0,
            "_source": {
                "location": {
                    "area": {
                        "type": "envelope",
                        "coordinates": [
                            [45.0, 45.0],
                            [44.0, 46.0]
                        ]
                    }
                }
            }
        }]
    }
}
```

however the same scenario but with a source exclusion mapping of an arbitrary field does not return the geo_shape coordinates any longer:

``` bash
curl -XDELETE 'http://localhost:9200/geo_test'
curl -XPUT 'http://localhost:9200/geo_test'
curl -XPUT 'http://localhost:9200/geo_test/item/_mapping' -d '{
    "item": {
        "_source": {
            "excludes": ["body"]
        },
        "properties" : {
            "location" : {
                "type" : "object",
                "properties": {
                     "point": {"type": "geo_point"},
                     "area": {"type": "geo_shape"}
                }
            }
        }
    }
}'
curl -XPUT 'http://localhost:9200/geo_test/item/1' -d '{
    "location": {"point": [45.0, 45.0]}
}'
curl -XPUT 'http://localhost:9200/geo_test/item/2' -d '{
    "location": {
        "area": {
            "type" : "envelope",
            "coordinates" : [[44.0, 46.0], [45.0, 45.0]]
        }
    }
}'
curl -XPOST 'http://localhost:9200/geo_test/item/_search?pretty' -d '{
    "query": {"match_all": {}}
}'
```

returns only the geo_point coordinates but not the geo_shape coordinates:

``` JSON
{
    "took": 1,
    "timed_out": false,
    "_shards": {
        "total": 1,
        "successful": 1,
        "failed": 0
    },
    "hits": {
        "total": 2,
        "max_score": 1.0,
        "hits": [{
            "_index": "geo_test",
            "_type": "item",
            "_id": "1",
            "_score": 1.0,
            "_source": {
                "location": {
                    "point": [45.0, 45.0]
                }
            }
        }, {
            "_index": "geo_test",
            "_type": "item",
            "_id": "2",
            "_score": 1.0,
            "_source": {
                "location": {
                    "area": {
                        "type": "envelope",
                        "coordinates": []
                    }
                }
            }
        }]
    }
}
```
</description><key id="13710970">2944</key><summary>source exclusion mapping prevents geo shape coordinates to be returned in query result source field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">fxh</reporter><labels><label>bug</label><label>v0.90.2</label><label>v1.0.0.Beta1</label></labels><created>2013-04-26T22:24:36Z</created><updated>2013-06-05T10:04:21Z</updated><resolved>2013-06-05T10:04:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-30T12:36:57Z" id="17224394">Hey,

this looks really strange.. I managed to reproduce it with a smaller example and will try to take a look at it

```
curl -XDELETE 'http://localhost:9200/geo_test'
curl -XPUT 'http://localhost:9200/geo_test'
curl -XPUT 'http://localhost:9200/geo_test/item/_mapping' -d '{
    "item": {
        "_source": { "excludes": ["body"] },
        "properties" : {
            "area": {"type": "geo_shape"}
        }
    }
}'

curl -XPUT 'http://localhost:9200/geo_test/item/1?refresh=true' -d '{
    "area": {
        "type" : "envelope",
        "coordinates" : [[-45.0, 45.0], [45.0, -45.0]]
    }
}'

curl -XPOST 'http://localhost:9200/geo_test/item/_search?pretty' -d '{
    "query": {"match_all": {}}
}'

curl http://localhost:9200/geo_test/item/1
```

As soon as the source exclude is omitted in the mapping, everything is working again. The GET on the id also works as expected.
</comment><comment author="fxh" created="2013-06-03T13:28:05Z" id="18840843">Hi, I was just trying this in ES 0.90.1 and I still get the same error as reported above. Could you please reopen this issues and doublecheck again? Thanks a lot.
</comment><comment author="spinscale" created="2013-06-03T15:52:09Z" id="18850848">@fxh hey, will reopen it and recheck as soon as possible
</comment><comment author="spinscale" created="2013-06-05T10:04:21Z" id="18966376">@fxh we did not include the fix in the 0.90 branch, even though I thought so. I have just pushed it into the 0.90 release branch so it will be included in the next release. Sorry for the inconvenience and thanks for bringing it up again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add median to statistical facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2943</link><project id="" key="" /><description>It would be really nice to have median statistical metric among metrics in statistical facet.
</description><key id="13697284">2943</key><summary>Add median to statistical facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pjancarik</reporter><labels /><created>2013-04-26T16:44:01Z</created><updated>2014-08-08T11:42:36Z</updated><resolved>2014-08-08T11:42:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-04-26T17:39:06Z" id="17088657">Doesn't median (and other percentiles) require total order of the data set across all [distributed] shards?

I might be wrong but I think the best you could get _easily_ is some kind of median estimate but I am not sure how [statistically] accurate it would be (given routing or default assumption about data distribution function as in reality not all data comes from process with normal distribution).
</comment><comment author="pjancarik" created="2013-04-27T16:57:09Z" id="17119371">I need to count median over several time based indexes(so several distributed shards). Now I can only fetch all values from ES and process it in application. In case of several thousands values it can take long time. I try to perform some benchmarks if this is usable approach.
</comment><comment author="lukas-vlcek" created="2013-04-27T17:43:40Z" id="17120148">Just out of curiosity, why you need median? May be if you can shed more light on your use case we can see if you can calculate median estimate (the chance is that statistical facet gives you all the values you need for estimate calculation).
</comment><comment author="pjancarik" created="2013-04-27T21:46:00Z" id="17123872">We store several user metrics in month indexes for last 3 months. One of the metrics for example is "user_response_time", which measure how long it takes user to reply on particular request. For each of these metrics we need to count mean, min, max and median for given time range and user. Except median we can count it  via statistical facet.
</comment><comment author="lukas-vlcek" created="2013-04-28T09:03:37Z" id="17130932">IIRC statistical facet gives you statistics that are based on sum and count. These are currently calculated across distributed shards. But unfortunately this is not the case for median.

I think you have basically two options:
- try to get the median out of elasticsearch using `sort` and `from/size`. You will need to `count` number of data items upfront, so for example if you have 10 data points then use sort and ask for 5th data item. But I am not sure if this would scale generally across large data sets.
- calculate median estimate on client size based on you assumptions about the data. Although it might sound scary I think it can work well in many cases.

As for the median estimate I would try the following:
- determine data distribution function
- use calculation for median estimate for that distribution

First you need to draw some assumptions about distribution of your data. In your case the data definitely does not follow normal distribution. You are more likely dealing with data that can be modelled by Gamma or Weibull distribution. You can use Goodness-of-fit to test if your data follow particular distribution. [1,2] Good news is you can use histogram facet for this test.

[1] http://en.wikipedia.org/wiki/Goodness_of_fit
[2] http://cs.wikipedia.org/wiki/Test_dobr&#233;_shody (in czech)

Then use median estimate calculation. For example if you find out that your data can be modelled by Weibull distribution then there is known way how to estimate median [3,4]

[3] http://en.wikipedia.org/wiki/Weibull_distribution
[4] http://www.itl.nist.gov/div898/handbook/eda/section3/eda3668.htm

The challenge can be to calculate all the parameters that are needed for median estimate (see example [5] or more general discussion on various approaches [6])

[5] http://www.itl.nist.gov/div898/handbook/apr/section4/apr413.htm
[6] http://interstat.statjournals.net/YEAR/2000/articles/0010001.pdf
</comment><comment author="lukas-vlcek" created="2013-04-28T09:05:23Z" id="17130946">Anyway, I would also recommend you to ask on ML. I might be simply wrong.
</comment><comment author="pjancarik" created="2013-04-28T16:57:13Z" id="17137364">Thanks Lukas for your thorough description and tips. Using sort and from/size will be suitable for this use case, because I don't expect the data items size in most cases exceed 1000. There are plans for others median use cases for future with much more data sets, so than I will try your suggestions.
</comment><comment author="jpountz" created="2014-03-03T17:55:43Z" id="36537592">FYI, we just added a new `percentiles` aggregation that allows to compute an approximate value of the median, see #5323.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add median to statistical facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2942</link><project id="" key="" /><description>It would be really nice to have median statistical metric among metrics in statistical facet.
</description><key id="13696610">2942</key><summary>Add median to statistical facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pjancarik</reporter><labels /><created>2013-04-26T16:27:07Z</created><updated>2013-04-26T16:44:56Z</updated><resolved>2013-04-26T16:44:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>elastic search accepts unparsable json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2941</link><project id="" key="" /><description>I have a csv file with some weird serialization issues. I discovered this because my search results are not parseable. Elastic search indexes the content successfully, retrieves it successfully and hands back the original content in search results even though it was invalid json. 

My csv contains two columns with an id and a json string. However, due to a bug in my csv generation, some lines got corrupted and actually contain four columns. In that case the last three columns get handed off to elastic search as the json object by my bulk indexing tool. The first of these is actually valid json and Elasticsearch seems to pick that up and ignores the rest of the line.

I would preferr Elasticsearch to fail on this at index time and be more strict. 
</description><key id="13688092">2941</key><summary>elastic search accepts unparsable json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jillesvangurp</reporter><labels /><created>2013-04-26T13:08:16Z</created><updated>2014-08-08T11:42:00Z</updated><resolved>2014-08-08T11:42:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-28T11:12:34Z" id="17132390">Hey Jilles,

can you provide an example, just to make sure we are talking about the same problem? When the JSON is not parseable, you should usually get a JsonParseException, so it seems your JSON is somewhat special to circumvent this.

thx a lot!
</comment><comment author="jillesvangurp" created="2013-04-29T07:37:43Z" id="17153386">Sure:

Pass this in to the _bulk API,
{"index":{"_index":"foo","_type":"bar"}}
{"name":"ok"}
{"index":{"_index":"foo","_type":"bar"}}
{"name":"not_ok"}thisbitshouldbreakthings

If you then do a search, it returns both entries including the non parseable '{"name":"not_ok"}thisbitshouldbreakthings'

It looks like the parser simply stops reading the input after the last parse event.
</comment><comment author="clintongormley" created="2013-05-16T09:55:56Z" id="17991852">Hi @jillesvangurp 

I've been caught with similar issues before.  The question is: just how strict should it be? I agree that trailing junk shouldn't be allowed (except for whitespace).

What about trailing commas or unquoted keys?  Personally I'd allow them - makes it easier to test things out from the command line.  That said, many JSON parsers will fail to parse those and throw errors.
</comment><comment author="jillesvangurp" created="2013-05-16T10:21:37Z" id="17992934">I'm biased towards being very strict. Anything elastic search returns should be parseable. In this case it actually passes through the junk and breaks that promise.
</comment><comment author="clintongormley" created="2013-05-16T13:13:31Z" id="17999963">Being very strict also makes it painful to try out from the command line,
which breaks the Elasticsearch philosophy of easy-to-start.

On 16 May 2013 12:21, Jilles van Gurp notifications@github.com wrote:

&gt; I'm biased towards being very strict. Anything elastic search returns
&gt; should be parseable. In this case it actually passes through the junk and
&gt; breaks that promise.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2941#issuecomment-17992934
&gt; .
</comment><comment author="kimchy" created="2013-05-16T13:15:34Z" id="18000078">I think that we can try and identify this cases where its evident that its not good, and reject it. The example shown, we can try and do better in that case...
</comment><comment author="jillesvangurp" created="2013-05-16T13:37:02Z" id="18001286">I think trailing whitespace is in any case allowed in json. So, I agree that that should be allowed. Any other characters making it through have the same issue that I described above: they break things when elastic search returns the trailing content with a search response. So, I don't think fixing this issue should make a lot of people unhappy on the command line.
</comment><comment author="spinscale" created="2013-05-21T13:18:03Z" id="18207830">FYI, this not a bulk issue but a general indexing issue. The `DocumentMapper.parse()` method does not throw an error, when indexing this 

```
curl -X PUT localhost:9200/foo/bar/1 -d '{"foo":"bar"} craaaaaaaaaaap'
```
</comment><comment author="clintongormley" created="2014-08-08T11:42:00Z" id="51591047">Closing in favour of #2315 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>make a separate jar of QueryBuilders and other class used by jest </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2940</link><project id="" key="" /><description>provide a separate jar to permit to use java rest client with jest project (https://github.com/searchbox-io/Jest) and java api without include all elesticsearch and lucene as dependency
</description><key id="13687286">2940</key><summary>make a separate jar of QueryBuilders and other class used by jest </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">filippor</reporter><labels /><created>2013-04-26T12:46:37Z</created><updated>2014-08-08T11:40:51Z</updated><resolved>2014-08-08T11:40:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ferhatsb" created="2013-05-02T08:27:27Z" id="17326078">To address lucene dependencies, you can exclude them while adding elasticsearch as dependency.
</comment><comment author="filippor" created="2013-05-14T13:37:06Z" id="17876225">i can't exclude lucene-core 
i get this error java.lang.NoClassDefFoundError: org/apache/lucene/util/BytesRef
</comment><comment author="s1monw" created="2013-05-14T13:38:55Z" id="17876339">Lucene has no dependencies, at least not the core. Yet, we might add even more dependencies to the API that are Lucene related in the future, I don't see a reason why you would want to exclude it. What are you issues with excluding it?
</comment><comment author="filippor" created="2013-05-14T18:02:15Z" id="17893739">My scope is use querybuilder api in a java application  using rest client. And mantain minimal dependency. 
</comment><comment author="kimchy" created="2013-05-14T18:04:59Z" id="17893919">we found ourself duplicating a lot of code from Lucene which is a shame, this is why we went ahead and use it...
</comment><comment author="aparo" created="2013-05-14T19:26:58Z" id="17899212">@filippor If you need to implement a Rest client without including ElasticSearch jar or lucene depends, you need write classes those replicate the ElasticSearch one. This was the only solution for python, but it was also very good for scala.
</comment><comment author="clintongormley" created="2014-08-08T11:40:51Z" id="51590982">Won't fix. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#2938 Bulk delete not working with versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2939</link><project id="" key="" /><description /><key id="13684007">2939</key><summary>#2938 Bulk delete not working with versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sfussenegger</reporter><labels /><created>2013-04-26T11:25:17Z</created><updated>2014-07-03T05:17:09Z</updated><resolved>2013-04-29T13:21:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sfussenegger" created="2013-04-26T12:07:04Z" id="17071050">[esi4j's BulkIndexHelperTest](https://github.com/molindo/esi4j/blob/3a854a0d5d0a484fcece4e52fc045b974cdec99f/src/test/java/at/molindo/esi4j/rebuild/util/BulkIndexHelperTest.java) demonstrates this problem (note the necessary workaround in [BulkIndexHelper](https://github.com/molindo/esi4j/blob/3a854a0d5d0a484fcece4e52fc045b974cdec99f/src/main/java/at/molindo/esi4j/rebuild/util/BulkIndexHelper.java#L317))
</comment><comment author="kimchy" created="2013-04-29T10:17:22Z" id="17159042">The version check for external type versioning is consistent for all operations, and does a "&lt;=". There are cases where just checking for "&lt;" is what people are after, but then it needs to be applies on all APIs and as an additional version type.
</comment><comment author="sfussenegger" created="2013-04-29T11:15:08Z" id="17161057">While I certainly think consistency is important I also think it's pretty confusing in this case. The context of index and delete/update operations is different. It surly makes sense to use &lt;= for index operations as the given version is the one of the new document. In case of a delete or update I don't have a new version (yet) but only the one of the already indexed document, so &lt; makes sense.

In other words, it feels unnatural to delete a document with version 5 if the latest document had version 4. In case of an update one should probably be able to distinguish between both versions, the one I'd like to update from and the one I'd like to update to.

At least, the documentation of delete and update API should be explicit about this.
</comment><comment author="kimchy" created="2013-04-29T11:20:34Z" id="17161242">I agree with you that in some use cases, it makes sense to differ between the two, though in other cases, specifically when delete/update are driven by an external versioning system as well, it doesn't. We need to support your mentioned case as well, but, we need to maintain the ability to support the current use case as well...
</comment><comment author="sfussenegger" created="2013-04-29T12:56:45Z" id="17164849">Actually I'm using an external versioning system (Hibernate/MySQL that is) and all I want is propagating a DB delete to the index. I don't see how incrementing the external version for a delete could feel natural in any use case. Getting a version conflict because I want to delete version 5 and version 5 is currently indexed seems pretty weird to me. I may be missing the big picture here, but I'm pretty confused with this behaviour. Realising that this is different for external version type (which is what I've been talking about all the way) just adds to the confusion. 

Anyway, I know it's difficult to change this behaviour but a major version change would probably be a good occasion. At least the guide should be explicit about this kind of versioning semantics.
</comment><comment author="kimchy" created="2013-04-29T12:58:34Z" id="17164921">There are cases where it makes sense, the current behavior that is, for example, to keep 2 clusters of ES at sync using external versioning systems, and systems that maintain versioning on deletes. We can support the mentioned behavior as well, I think using additional flag to the external version type system. Can you open an issue about it, so we can discuss it there, and make sure we add it properly?
</comment><comment author="sfussenegger" created="2013-04-29T13:21:27Z" id="17166053">I've just created the above ticket. I hope I described it the way you expected it to be.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk delete not working with versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2938</link><project id="" key="" /><description>Performing delete operations via bulk API does not work as it is causing wrong VersionConflictEngineExceptions:

```
VersionConflictEngineException: [default][2] [tweet][1]: version conflict, current [4], provided [4]
```

the error is in this code snippet of [RobinEngine](https://github.com/elasticsearch/elasticsearch/blob/v0.20.6/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java#L652):

```
} else if (currentVersion &gt;= delete.version()) {
  throw new VersionConflictEngineException(shardId, delete.type(), delete.id(), currentVersion, delete.version());
}
```

`currentVersion` is the currently stored version while `delete.version()` is the version given in the delete request. Obviously equal versions must not cause an exception. Therefore the condition must be changed to `currentVersion &gt; delete.version()`
</description><key id="13683753">2938</key><summary>Bulk delete not working with versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">sfussenegger</reporter><labels /><created>2013-04-26T11:19:00Z</created><updated>2014-08-08T11:39:40Z</updated><resolved>2014-08-08T11:39:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-26T21:22:08Z" id="17100696">I will look into this soon. thanks for reporting
</comment><comment author="clintongormley" created="2014-08-08T11:39:40Z" id="51590920">Solved by the addition of version_type `external_gte`: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timestamp defaults to parsing int even if given a format</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2937</link><project id="" key="" /><description>if given some document date field that is a basic date, such as
{
"doc_field" : "20130425"
}
and _timestamp uses it as its path, it defaults to parsing the integer as ms rather than as the format
_timestamp{
"format":"yyyyMMdd",
"enabled":true
}
</description><key id="13655830">2937</key><summary>Timestamp defaults to parsing int even if given a format</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">GustavMauler</reporter><labels><label>:Dates</label><label>adoptme</label><label>bug</label></labels><created>2013-04-25T18:58:46Z</created><updated>2015-08-13T13:50:28Z</updated><resolved>2015-06-23T18:21:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2015-06-23T18:21:16Z" id="114597413">Closing in favour of #10971
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set http headers on the response in a java plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2936</link><project id="" key="" /><description>Taking up the work started by @spinscale in https://github.com/elasticsearch/elasticsearch/pull/2723

"this patch allows to set custom headers in HTTP responses (like setting the WWW-Authenticate header for basic auth) by adding RestRequest.addHeader() method."

The extra hashmap is created lazily so this has no overhead for the core ES requests.
This patch only deals with http-header and has nothing related to redirection.

I hope this helps!
</description><key id="13648753">2936</key><summary>Set http headers on the response in a java plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hmalphettes</reporter><labels /><created>2013-04-25T16:48:44Z</created><updated>2014-06-13T01:00:15Z</updated><resolved>2013-05-10T16:03:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karussell" created="2013-04-26T06:16:09Z" id="17057233">This would be nice to have in the core. As the current workaround for me was to patch all the plugins to make some kind of authentication (via URL parameters) working. See e.g. elasticsearch-head:

https://github.com/mobz/elasticsearch-head/pull/57

But e.g. for bigdesk the workaround failed (or would cost too much changes): https://github.com/lukas-vlcek/bigdesk/issues/28

Also one cannot use remotely installed plugins with the current workaround.
</comment><comment author="spinscale" created="2013-04-26T09:41:06Z" id="17064142">Hey,

the reason I stopped this patch (temporarily) was mainly, because I could not come up with a working test quickly. I still plan to get #2723 in (and remove the redirect stuff). Adding a test and getting some feedback is the next step on my side.
</comment><comment author="hmalphettes" created="2013-04-27T00:00:01Z" id="17106474">@spinscale I did my best to do what you were suggesting in #2723:
- copied your changes
- removed the redirection things
- added a test

Here is a patched build of elasticsearch with the custom headers in action for basic authentication:
http://hugues-elasticsearch.cloudfoundry.com/_plugin/head/ (ping me if you want the login; there is no data in there)

I was hoping to save your time for this development ... and to take advantage of the feature right now as I need it.

Let me know if you want me to change something; add more tests; credit the real author?
Otherwise I am happy to wait for you to develop this in #2723 of course.
</comment><comment author="spinscale" created="2013-04-28T12:07:41Z" id="17133052">maybe I am blind or still overtired after a hard week, but where is that test? I dont see it in your commit above
</comment><comment author="hmalphettes" created="2013-04-28T12:33:42Z" id="17133357">Sorry about that @spinscale. I had forgotten to push or lost them after the rebase this morning.
Thanks for your attention!
</comment><comment author="hmalphettes" created="2013-04-30T03:34:28Z" id="17207950">I rebased the patch and squashed the changes into a single commit to keep up to date.
... and learnt to not forget to git add the test files this time.
</comment><comment author="spinscale" created="2013-05-03T09:15:43Z" id="17385252">Hey,

just to keep you informed: I changed your tests a little bit (created a simple rest action instead of http server modules). Before pushing this into master I need to find out, why my branch results in breaking the `SimpleNodesInfoTests` - this test loads my test plugin automatically if it runs after my `ResponseHeaderPluginTests` - I did not yet have time to investigate why this happens.

My commit is at https://github.com/spinscale/elasticsearch/commit/3e7231670a644ef59accb655277f7c3600b4a677
</comment><comment author="imotov" created="2013-05-03T11:22:05Z" id="17389577">@spinscale could you check if https://github.com/elasticsearch/elasticsearch/pull/2977 fixes your issue with `SimpleNodesInfoTests`.
</comment><comment author="hmalphettes" created="2013-05-08T01:21:10Z" id="17581214">@spinscale, @imotov I pulled @spinscale's http-headers branch and merged it with the current master https://github.com/hmalphettes/elasticsearch/commit/93aefd03cf315f3b56a818eed527f331d5560eae
`mvn test -Dtest=SimpleNodesInfoTests` works fine.
`mvn test -Dtest=ResponseHeaderPluginTests` works fine.

`mvn test -Dtest=SimpleNodesInfoTests,ResponseHeaderPluginTests` fails with:

```
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 24.108 sec &lt;&lt;&lt; FAILURE!
testNodeInfoPlugin(org.elasticsearch.test.integration.nodesinfo.SimpleNodesInfoTests)  Time elapsed: 12826 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError:
Expected: one of {"test-plugin", "test-no-version-plugin"}
     but: was "test-plugin-custom-header"
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
    at org.elasticsearch.test.integration.nodesinfo.SimpleNodesInfoTests.checkPlugin(SimpleNodesInfoTests.java:172)
    at org.elasticsearch.test.integration.nodesinfo.SimpleNodesInfoTests.testNodeInfoPlugin(SimpleNodesInfoTests.java:131)
```
</comment><comment author="hmalphettes" created="2013-05-08T02:55:01Z" id="17583827">Both SimpleNodesInfoTests,ResponseHeaderPluginTests are loading plugins in the same classloader. They interfere with each other.

Here is a suggested fix to sandbox the loading of the TestResponseHeaderPlugin just for its own test: 
On the top of a merge with the master branch:
https://github.com/hmalphettes/elasticsearch/commit/42b472ee0147ede9b7440b5c1e7fe76ca222613b

On the top of spinscale's http-headers branch:
https://github.com/hmalphettes/elasticsearch/commit/0e3f7d8406e482650e51f7c58c8ad3a957191f41

The tests are passing in both branches so in fact @imotov 's commit does not make a difference (on my machine at least).

I hope this helps.
</comment><comment author="spinscale" created="2013-05-08T12:36:11Z" id="17602539">Instead of playing around with setting class loaders I refactored the SimpleNodesInfoTests test a bit and got it working in all cases. My commit is at https://github.com/spinscale/elasticsearch/commit/2bbde08fd7644c64b6a9c8bc25f7a33dec57be45

I will try to get this into master soon
</comment><comment author="hmalphettes" created="2013-05-10T23:19:34Z" id="17749384">All right! Cheers @spinscale!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fixing possible NoClassDefFoundError bubbling from SettingsBuilder</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2935</link><project id="" key="" /><description>In order to handle exceptions correctly, when classes are not found, one
needs to handle ClassNotFoundException as well as NoClassDefFoundError
in order to be sure to have caught every possible case. We did not cater
for the latter in ImmutableSettings yet.

This fix is just executing the same logic for both exceptions instead of
simply bubbling up NoClassDefFoundError.

I still do not like the notion of loading classes via the SettingsBuilder, but that is a another issue :-)
</description><key id="13642311">2935</key><summary>Fixing possible NoClassDefFoundError bubbling from SettingsBuilder</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-25T14:51:14Z</created><updated>2014-07-16T21:53:37Z</updated><resolved>2013-04-26T10:48:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-26T10:48:03Z" id="17066691">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added stolen time to OsStats output</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2934</link><project id="" key="" /><description>Sigar supports stolen time, this adds support in OsStats for it.

Stolen time is important for virtualized environments, as it can tell possibly you if other systems are limiting performance.

We have to find out and document if this helps in virtualized blackbox environments like AWS (in order to be sure if this number is accurate).
</description><key id="13628612">2934</key><summary>Added stolen time to OsStats output</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-25T08:52:33Z</created><updated>2014-06-22T08:52:47Z</updated><resolved>2013-04-25T15:05:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add Snappy Dependency to pom.xml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2933</link><project id="" key="" /><description>As of 0.20.6 I get a NoClassDefFoundError when starting ElasticSearch within test classes. Looks like you just need to express the xerial snappy dependency in your pom file.

Here's the error:

18:28:59.930 [main] INFO  org.elasticsearch.plugins - [Jones, Hugh] loaded [], sites []
18:28:59.941 [main] DEBUG o.elasticsearch.common.compress.lzf - using [UnsafeChunkDecoder] decoder
18:28:59.949 [main] DEBUG org.elasticsearch.common.compress - failed to load xerial snappy-java
java.lang.NoClassDefFoundError: org/xerial/snappy/Snappy
    at org.elasticsearch.common.compress.snappy.xerial.XerialSnappy.&lt;clinit&gt;(XerialSnappy.java:42) ~[elasticsearch-0.20.6.jar:na]
    at org.elasticsearch.common.compress.CompressorFactory.&lt;clinit&gt;(CompressorFactory.java:58) ~[elasticsearch-0.20.6.jar:na]
    at org.elasticsearch.node.internal.InternalNode.&lt;init&gt;(InternalNode.java:134) [elasticsearch-0.20.6.jar:na]
    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159) [elasticsearch-0.20.6.jar:na]
    at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:166) [elasticsearch-0.20.6.jar:na]
</description><key id="13619922">2933</key><summary>Add Snappy Dependency to pom.xml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">xstevens</reporter><labels /><created>2013-04-25T01:43:34Z</created><updated>2013-04-25T15:48:49Z</updated><resolved>2013-04-25T08:19:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-25T07:39:34Z" id="16992647">Snappy is an optional dependency. If snappy cant be found, LZF is used. So it is not really a problem.
Also, with 0.90 this snappy support (and its detection) has been removed.

http://www.elasticsearch.org/guide/reference/mapping/source-field/
</comment><comment author="spinscale" created="2013-04-25T08:19:39Z" id="16994018">FYI: if you want snappy compression, you need to add the snappy jar into the lib directory, otherwise LZF is used.

Closing this one.
</comment><comment author="xstevens" created="2013-04-25T15:48:49Z" id="17015504">I noticed it was still working, but I was thinking it should handle this situation a bit more gracefully and not log a stack trace. Either way it sounds like this will go away in the next version when snappy is removed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when using DFS_QUERY_AND/THEN_FETCH using 0.90RC2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2932</link><project id="" key="" /><description>The following Exception is logged:

```
org.elasticsearch.transport.SendRequestTransportException: [es7][inet[/10.66.110.57:9300]][search/phase/query/id]
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:199)
    at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:171)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:181)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:148)
    at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchDfsQueryThenFetchAction.java:107)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:229)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:208)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:205)
    at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:122)
    at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:113)
    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:156)
    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:127)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)
    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)
    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.NullPointerException
```

When the following query is executed:

```
SearchResponse response = client.prepareSearch("heartbeat")
    .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)
    .setQuery(
        QueryBuilders.boolQuery()
            .must(QueryBuilders.termQuery("online", true))
            .must(QueryBuilders.boolQuery()
                .should(QueryBuilders.boolQuery()
                    .must(QueryBuilders.rangeQuery("ts").lt(now - (MAX_AGE_BS * 1000)))
                    .must(QueryBuilders.termQuery("_type", "bs"))
                    )
                .should(QueryBuilders.boolQuery()
                    .must(QueryBuilders.rangeQuery("ts").lt(now - (MAX_AGE_SENSOR * 1000)))
                    .must(QueryBuilders.termQuery("_type", "s"))
                )
            )
    )
    .setVersion(true)
    .setFrom(0).setSize(100).setExplain(true)
    .execute()
    .actionGet();
```

The query does return the expected results. So other then the log entry there seems to be nothing wrong with it.
Changing the SearchType fixes the issue.

The fields have the following properties:

```
"online": {
    "type": "boolean",
},
"ts": {
    "type": "date",
    "ignore_malformed": False,
    "format": "dateOptionalTime"
},
```

And the "s" type has a routing defined:

```
"_routing": {
    "required": True,
    "path": "bs"
},
```

the "bs" field is only in the "s" type defined:

```
"bs": {
    "type": "string",
    "index": "not_analyzed"
},
```
</description><key id="13579690">2932</key><summary>NullPointerException when using DFS_QUERY_AND/THEN_FETCH using 0.90RC2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mfussenegger</reporter><labels /><created>2013-04-24T10:19:09Z</created><updated>2013-05-14T07:10:02Z</updated><resolved>2013-05-14T07:10:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-25T12:21:59Z" id="17003411">hey, can you maybe provide a self-contained test that reproduces this. Do you know where the NPE is thrown ie. can you see something in the logs?
</comment><comment author="alambert" created="2013-04-30T15:38:05Z" id="17235010">cc @spinscale @imotov 

I think I've been able to repro this in our dev cluster running the 0.90.0 release. This is causing all shards' searches to fail with "SendRequestTransportException[....spindle-dev.net[inet/10.252.166.47:7200]search/phase/query/id]; nested: NullPointerException" It looks like writeTo() in AggregatedDfs is calling out.writeString(entry.getKey()) (line 106) but entry.getKey() is returning null. I'm still trying to find the query that's causing this, but it's definitely using DFS_QUERY_THEN_FETCH.

The thread stack is:

&lt;pre&gt;
Thread 18766: (state = BLOCKED)
 - org.elasticsearch.common.io.stream.HandlesStreamOutput.writeString(java.lang.String) @bci=1, line=55 (Interpreted frame)
 - org.elasticsearch.search.dfs.AggregatedDfs.writeTo(org.elasticsearch.common.io.stream.StreamOutput) @bci=174, line=106 (Interpreted frame)
 - org.elasticsearch.search.query.QuerySearchRequest.writeTo(org.elasticsearch.common.io.stream.StreamOutput) @bci=18, line=69 (Interpreted frame)
 - org.elasticsearch.transport.netty.NettyTransport.sendRequest(org.elasticsearch.cluster.node.DiscoveryNode, long, java.lang.String, org.elasticsearch.transport.TransportRequest, org.elasticsearch.transport.TransportRequestOptions) @bci=143, line=546 (Interpreted frame)
 - org.elasticsearch.transport.TransportService.sendRequest(org.elasticsearch.cluster.node.DiscoveryNode, java.lang.String, org.elasticsearch.transport.TransportRequest, org.elasticsearch.transport.TransportRequestOptions, org.elasticsearch.transport.TransportResponseHandler) @bci=86, line=184 (Interpreted frame)
 - org.elasticsearch.transport.TransportService.sendRequest(org.elasticsearch.cluster.node.DiscoveryNode, java.lang.String, org.elasticsearch.transport.TransportRequest, org.elasticsearch.transport.TransportResponseHandler) @bci=9, line=171 (Interpreted frame)
 - org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(org.elasticsearch.cluster.node.DiscoveryNode, org.elasticsearch.search.query.QuerySearchRequest, org.elasticsearch.search.action.SearchServiceListener) @bci=76, line=181 (Interpreted frame)
 - org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(org.elasticsearch.search.dfs.DfsSearchResult, java.util.concurrent.atomic.AtomicInteger, org.elasticsearch.search.query.QuerySearchRequest, org.elasticsearch.cluster.node.DiscoveryNode) @bci=21, line=148 (Interpreted frame)
 - org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.moveToSecondPhase() @bci=135, line=107 (Interpreted frame)
 - org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(org.elasticsearch.cluster.routing.ShardRouting, org.elasticsearch.search.SearchPhaseResult, org.elasticsearch.cluster.routing.ShardIterator) @bci=72, line=229 (Interpreted frame)
 - org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(org.elasticsearch.search.SearchPhaseResult) @bci=13, line=208 (Interpreted frame)
 - org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(java.lang.Object) @bci=5, line=205 (Interpreted frame)
 - org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(org.elasticsearch.search.dfs.DfsSearchResult) @bci=5, line=122 (Interpreted frame)
 - org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(org.elasticsearch.transport.TransportResponse) @bci=5, line=113 (Interpreted frame)
 - org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(org.elasticsearch.common.io.stream.StreamInput, org.elasticsearch.transport.TransportResponseHandler) @bci=75, line=156 (Interpreted frame)
 - org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(org.elasticsearch.common.netty.channel.ChannelHandlerContext, org.elasticsearch.common.netty.channel.MessageEvent) @bci=490, line=127 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(org.elasticsearch.common.netty.channel.ChannelHandlerContext, org.elasticsearch.common.netty.channel.ChannelEvent) @bci=13, line=70 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext, org.elasticsearch.common.netty.channel.ChannelEvent) @bci=9, line=564 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(org.elasticsearch.common.netty.channel.ChannelEvent) @bci=22, line=791 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(org.elasticsearch.common.netty.channel.ChannelHandlerContext, java.lang.Object, java.net.SocketAddress) @bci=16, line=296 (Interpreted frame)
 - org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(org.elasticsearch.common.netty.channel.ChannelHandlerContext, java.net.SocketAddress, java.lang.Object) @bci=123, line=462 (Interpreted frame)
 - org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(org.elasticsearch.common.netty.channel.ChannelHandlerContext, org.elasticsearch.common.netty.channel.Channel, org.elasticsearch.common.netty.buffer.ChannelBuffer, java.net.SocketAddress) @bci=97, line=443 (Interpreted frame)
 - org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(org.elasticsearch.common.netty.channel.ChannelHandlerContext, org.elasticsearch.common.netty.channel.MessageEvent) @bci=62, line=303 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(org.elasticsearch.common.netty.channel.ChannelHandlerContext, org.elasticsearch.common.netty.channel.ChannelEvent) @bci=13, line=70 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext, org.elasticsearch.common.netty.channel.ChannelEvent) @bci=9, line=564 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(org.elasticsearch.common.netty.channel.ChannelEvent) @bci=55, line=559 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(org.elasticsearch.common.netty.channel.Channel, java.lang.Object, java.net.SocketAddress) @bci=16, line=268 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(org.elasticsearch.common.netty.channel.Channel, java.lang.Object) @bci=3, line=255 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(java.nio.channels.SelectionKey) @bci=179, line=88 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(java.nio.channels.Selector) @bci=70, line=107 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run() @bci=368, line=312 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run() @bci=1, line=88 (Interpreted frame)
 - org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run() @bci=1, line=178 (Interpreted frame)
 - org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run() @bci=55, line=108 (Interpreted frame)
 - org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run() @bci=14, line=42 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Interpreted frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=722 (Interpreted frame)
&lt;/pre&gt;
</comment><comment author="waterdh" created="2013-05-08T01:02:20Z" id="17580664">I met the same problem, had to change the search_type
</comment><comment author="alambert" created="2013-05-08T14:21:19Z" id="17608946">I ran into this again; on both instances, it was after an 0.20.6-&gt;0.90.0 migration. Restarting ES globally didn't help.

I tried a global optimize down to one segment, though, and after that the query started working again:

curl -XPOST -v 'http://es-host:9200/_optimize?wait_for_merge=true&amp;max_num_segments=1'
</comment><comment author="s1monw" created="2013-05-08T16:13:30Z" id="17616383">cool, thanks for the update this give me more confidence that this (#3012) is the actual cause
</comment><comment author="mfussenegger" created="2013-05-13T21:23:22Z" id="17841687">After calling _optimize?wait_for_merge=true&amp;max_num_segments=1 the DFS query also works again for me.
</comment><comment author="s1monw" created="2013-05-14T07:10:01Z" id="17860193">@mfussenegger yeah that is expected with the fix I added, thanks for reporting back
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StringIndexOutOfBoundsException[String index out of range: -8] while Highlighting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2931</link><project id="" key="" /><description>This issue happens on 0.20.4 and 0.90RC2 and probably every other version(?) since it I guess its related to:

https://issues.apache.org/jira/browse/LUCENE-4899

Here is a test that manages to reproduce the error. First 2 queries should execute ok, but third should fail.

```
curl -XPOST 'http://127.0.0.1:9200/test?' -d '{ "mappings" : { "test" : { "properties" : { "name" : { "type": "string", "index_analyzer": "name_index_analyzer", "search_analyzer": "name_search_analyzer", "term_vector" : "with_positions_offsets" } } } }, "settings" : { "analysis" : { "filter" : { "my_ngram" : { "max_gram" : 20, "min_gram" : 1, "type" : "ngram" } }, "analyzer" : { "name_index_analyzer": { "tokenizer": "whitespace", "filter": [ "my_ngram" ] }, "name_search_analyzer": { "tokenizer": "whitespace" } } } }}'

curl -XPUT 'http://localhost:9200/test/test/1' -d '{"name": "logicacmg ehemals avinci - the know how company"}'

curl -XGET 'http://localhost:9200/test/test/_search' -d '{ "query": { "match": { "name": { "query": "logica" } } }, "highlight": { "fields": { "name": {} } }}'

curl -XGET 'http://localhost:9200/test/test/_search' -d '{ "query": { "match": { "name": { "query": "logica ma" } } }, "highlight": { "fields": { "name": {} } }}'

curl -XGET 'http://localhost:9200/test/test/_search' -d '{ "query": { "match": { "name": { "query": "logica m" } } }, "highlight": { "fields": { "name": {} } }}'
```

Maybe its possible a work around, or a Lucene upgrade to 4.3(since it seems to be fixed there)?
</description><key id="13576968">2931</key><summary>StringIndexOutOfBoundsException[String index out of range: -8] while Highlighting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.0</label></labels><created>2013-04-24T09:02:26Z</created><updated>2013-04-27T14:48:50Z</updated><resolved>2013-04-27T14:48:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-24T09:09:36Z" id="16915636">@lmenezes yes this is very likely caused by LUCENE-4899 - The release of 4.3 is already rolling and should be done by the end of the week. I will take your example here and put it into the 4.3 upgrade branch and see if it still fails to make sure this is actually fixed.
</comment><comment author="lmenezes" created="2013-04-24T09:12:23Z" id="16915790">@s1monw  Cool. Regarding ES updating to 4.3, is it realistic expecting that for 0.90? Or even just a patch for this particular issue? 
</comment><comment author="s1monw" created="2013-04-24T09:14:22Z" id="16915874">given that 0.90 is pretty close I can't promise anything but we are considering it. I don't think I can really patch this issue without copying a lot of code. The only thing you can do as a workaround is to not use the term vector highlighter until then.
</comment><comment author="lmenezes" created="2013-04-24T09:26:07Z" id="16916384">Hum... I tried using the regular highlighting, but that yields some pretty weird stuff:

"logicac&lt;em&gt;m&lt;/em&gt;&lt;em&gt;logica&lt;/em&gt;g ehe&lt;em&gt;m&lt;/em&gt;als avinci - the know how co&lt;em&gt;m&lt;/em&gt;pany"

Test:

```
curl -XPOST 'http://127.0.0.1:9200/test?' -d '{ "mappings" : { "test" : { "properties" : { "name" : { "type": "string", "index_analyzer": "name_index_analyzer", "search_analyzer": "name_search_analyzer"} } } }, "settings" : { "analysis" : { "filter" : { "my_ngram" : { "max_gram" : 20, "min_gram" : 1, "type" : "ngram" } }, "analyzer" : { "name_index_analyzer": { "tokenizer": "whitespace", "filter": [ "my_ngram" ] }, "name_search_analyzer": { "tokenizer": "whitespace" } } } }}'

curl -XPUT 'http://localhost:9200/test/test/1' -d '{"name": "logicacmg ehemals avinci - the know how company"}'

curl -XGET 'http://localhost:9200/test/test/_search' -d '{ "query": { "match": { "name": { "query": "logica m" } } }, "highlight": { "fields": { "name": {} } }}'
```

Doesn't really works for our case. 
</comment><comment author="lmenezes" created="2013-04-24T09:37:34Z" id="16916866">@s1monw the highlighted stuff wasn't formatted as it should, but I guess you get the idea.
</comment><comment author="s1monw" created="2013-04-24T10:01:03Z" id="16917816">@lmenezes I added the test and it passes, would be good if you can take a look if the result is as you expect it?
</comment><comment author="lmenezes" created="2013-04-24T11:17:16Z" id="16920505">@s1monw i think you just added tests for the 2 cases that already work on Lucene pre 4.3. Your are missing the 3rd query, the one that fails. I tried executing the third query and I got the same results on your branch(the weird HL and also the OutOfBounds).
- as a plus, this branch didn't work on my os x, only on ubuntu. The shards were constantly in "initializing", so i wasn't able to run that on osx. any idea?
</comment><comment author="s1monw" created="2013-04-24T14:45:05Z" id="16935338">hey @lmenezes so the issue why this doesn't work / fails is that the ngram filter you are using is basically broken and produces somehow wrong positions. I am working on a fix for this and I will update you accordingly. 

regarding mac osx, I am running on osx just fine... did you try mvn clean first?
</comment><comment author="lmenezes" created="2013-04-24T14:49:23Z" id="16935634">hey @s1monw cool!

about the mac osx, i'll give it a go later today and let you know. it was a fresh clone from github, so clean shouldnt be necessary i believe. anyway, i'll keep you posted just in case. 
</comment><comment author="s1monw" created="2013-04-24T16:49:53Z" id="16946939">I opened [LUCENE-4955](https://issues.apache.org/jira/browse/LUCENE-4955) for this since this is really caused by a bug / problem in the NGramFilter. This won't make it into lucene 4.3 but we can temporarily port that once it's committed in lucene.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Using script_field loses all indexed data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2930</link><project id="" key="" /><description>I posted this on [google groups](https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/Dw2kqpygwwg) so I apologize for creating an issue. I can close if it doesn't belong here. But I believe this is a bug.

It seems like using `script_fields` makes the rest of the document disappear. Here is what I am doing. I am trying to use elasticsearch with script_field to return distance in miles. The distance works but when I use script_field, all the data is lost that would have been returned if I didn't use script_field. Example:

``` json
{
  "script_fields": {
    "distance": {
      "script": "doc['lat_lon'].distanceInKm(0,50.3)"
    }
  },
 "query": {
    "match_all": {}
  }
}
```

This returns

``` json
{
    took: 1,
    timed_out: false,
    _shards: {
        total: 5,
        successful: 5,
        failed: 0
    },
    hits: {
        total: 1,
        max_score: 1,
        hits: [
            {
                _index: test_channels,
                _type: channel,
                _id: 1,
                _score: 1,
                fields: {
                    distance: 19629.025705718916
                }
            }
        ]
    }
}
```

As you can see there is distance but nothing else. Removing the script_field like this:

``` json
{
 "query": {
    "match_all": {}
  }
}
```

Produces the correct result. 

``` json
{
    took: 0,
    timed_out: false,
    _shards: {
        total: 5,
        successful: 5,
        failed: 0,
    }
    hits: {
        total: 1,
        max_score: 1,
        hits: [
            {
                _index: "test_channels",
                _type: "channel",
                _id: 1,
                _score: 1,
                _source: {
                    created_at: "2013-04-23T15:50:54Z",
                    description: "Odit nam vitae facere velit eum similique. Voluptatem sed voluptas necessitatibus architecto molestiae voluptates possimus. Quo necessitatibus doloribus aut. Omnis nam non ratione. Nihil harum esse eaque sunt error consequuntur quia quasi."
                    id: 1,
                    image_file_name: null,
                    latitude: 37.3320132,
                    longitude: -122.0289113,
                    name: "Repellat inventore architecto et similique quo in.",
                    lat_lon: {
                        lat: 37.3320132,
                        lon: -122.0289113
                    }
                }
            }
        ]
    }
}
```

Is this potentially a bug or works as designed? 
</description><key id="13552745">2930</key><summary>Using script_field loses all indexed data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">amir20</reporter><labels /><created>2013-04-23T19:33:20Z</created><updated>2013-06-05T19:52:52Z</updated><resolved>2013-04-24T14:40:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="pecke01" created="2013-04-23T19:57:58Z" id="16882386">script_field will limit the return to the field specifed just like using the normal fields param when searching. If you want more fields returned you can specify them in the same query.

So I would say that this is by design. Not my design so can't say 100% but for me this is normal behaviour.
</comment><comment author="amir20" created="2013-04-23T20:05:06Z" id="16882873">What do you mean exactly include them in the same query? Do you mean move the query to `script_field`? 

Edit: Found [this stackoverflow](http://stackoverflow.com/questions/11856155/no-more-source-if-script-fields-is-used-in-elasticsearch-query)

I would say this is confusing because I read all the documentation a lot to make sure I was't missing anything. 
</comment><comment author="pecke01" created="2013-04-23T20:09:15Z" id="16883120">Then I would do this:

``` javascript
{
  "fields" : ["description","name","created_at"],
  "script_fields": {
    "distance": {
      "script": "doc['lat_lon'].distanceInKm(0,50.3)"
    }
  },
 "query": {
    "match_all": {}
  }
}
```
</comment><comment author="amir20" created="2013-04-23T20:10:50Z" id="16883195">I see. Yea that's what I found too. There isn't a way to return all fields is there? 
</comment><comment author="amir20" created="2013-04-23T20:16:55Z" id="16883510">By the way thank you for responding. At least I am unblocked. This is weird behavior. 
</comment><comment author="pecke01" created="2013-04-23T20:25:16Z" id="16884001">Glad I could help. Good Luck
</comment><comment author="amir20" created="2013-04-24T14:40:01Z" id="16934975">I am closing this since it seems like it does what is supposed to. However, I do still find this behavior confusing. 
</comment><comment author="kimchy" created="2013-04-24T16:29:25Z" id="16945657">You can also return the actual source by asking for the `_source` in the fields element.
</comment><comment author="amir20" created="2013-04-24T16:42:59Z" id="16946484">@kimchy That seems to work when I use elasticsearch but when I use it via `tire` gem it seems that the `distance` field is no longer returned. I assume this is because elasticsearch returns all the fields under the `_source` key and not `fields`. 
</comment><comment author="karmi" created="2013-04-24T19:28:41Z" id="16959011">@amir20 You have to ask for the specific fields at the moment, see the original issue at karmi/tire#712.
</comment><comment author="amir20" created="2013-04-24T20:10:06Z" id="16961592">@karmi I did specify the fields explicitly and it worked. However, @kimchy suggested that I can use `_source`. Trying with elasticsearch, I can see the document being returned correctly in the `_source` tag instead of `fields`. However, tire gem does not return the `_source` data. Any suggestions to use `_source` and tire together?
</comment><comment author="amir20" created="2013-04-24T20:12:10Z" id="16961738">I just saw your other comment about https://github.com/karmi/tire/issues/687. So ignore my comment above. I will follow that issue. Thank you.  
</comment><comment author="karmi" created="2013-04-25T08:18:51Z" id="16993991">@amir20 Yeah, it's a bug in the way fields and _source is handled, when you use it together.
</comment><comment author="karmi" created="2013-06-05T19:35:03Z" id="19002919">@amir20 The bug is now closed on Tire master, you can try it out. See integration test: https://github.com/karmi/tire/blob/master/test/integration/results_test.rb#L52-L63 Thanks for the report! 
</comment><comment author="amir20" created="2013-06-05T19:52:52Z" id="19003937">Great. Will try it out soon. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Create a circuit breaker to prevent searches from bringing down a node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2929</link><project id="" key="" /><description>One of the fears that I have when using ElasticSearch is that expensive queries can bring down nodes in my cluster. 

It would be really nice if ElasticSearch could detect this type of node-killing event by adding logic that would trigger a circuit breaker and kill the offending query, leaving my node intact.  For example, if a search takes X% of the heap, the query would be killed by ElasticSearch.  It would be useful to expose the X% of heap_size as a configurable value since the level of concurrency of the system would vary by ES installation.

Another feature that would be helpful is when the circuit breaker is tripped, a response is generated from ElasticSearch saying that the query died from using excess memory.
</description><key id="13544401">2929</key><summary>Create a circuit breaker to prevent searches from bringing down a node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">maioriel</reporter><labels><label>enhancement</label><label>feature</label></labels><created>2013-04-23T16:47:14Z</created><updated>2014-11-25T16:34:17Z</updated><resolved>2014-01-28T21:18:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tmkujala" created="2013-04-25T19:43:44Z" id="17035913">This is exactly what I'm looking for as well! One of my requirements is to provide open API access to our ElasticSearch data for developers to run adhoc queries. There is a very real possibility that one of them may execute a bad query bringing down a single node or much worse multiple nodes in my cluster.

What would make this feature even better, is additional performance monitoring for what queries are running at any given time and what queries have been run as well as performance metrics for them.
</comment><comment author="tlieblfs" created="2013-04-25T21:14:49Z" id="17040805">+1
</comment><comment author="s1monw" created="2013-04-26T10:44:44Z" id="17066573">Hey folks, I want to jump in here and tell you that this is something that is pretty high on our wish-list as well. With the foundations 0.90 will bring we can approach things like this much easier and maybe more important more reliable. I might jump in here and have a first cut at this pretty soon.
</comment><comment author="rore" created="2013-05-08T14:24:01Z" id="17609124">+1
</comment><comment author="btiernay" created="2013-05-20T22:59:35Z" id="18179242">:+1: 
</comment><comment author="lmenezes" created="2013-06-05T10:09:51Z" id="18966616">+1
</comment><comment author="nik9000" created="2013-08-07T01:29:39Z" id="22224135">+1
Certainly it'd be cool to get a list of running queries and be able to kill them if they are running wild.  That'd be a wonderful first start to anything along these lines.
</comment><comment author="avleen" created="2013-11-20T18:18:55Z" id="28914233">@s1monw any update on this? We have some really large indices, and big searches over terrabytes of data can bring down the cluster right now because the searches just keep going forever :-(
</comment><comment author="dakrone" created="2013-11-20T19:54:04Z" id="28924127">@avleen we are actively developing this, so hopefully soon!
</comment><comment author="dakrone" created="2013-11-26T20:51:31Z" id="29332651">Related: https://github.com/elasticsearch/elasticsearch/pull/4261
</comment><comment author="lukas-vlcek" created="2013-11-26T21:54:58Z" id="29338053">Interesting!

BTW, is there any impact on bulk operations? Like bulk update? Meaning once the circuit breaks the bulk operation will still go on but all remaining updates targeting particular shard will not make it?
(Also might impact https://github.com/elasticsearch/elasticsearch/issues/2230 if implemented in the future?)
</comment><comment author="dakrone" created="2014-01-28T21:18:29Z" id="33526548">Closing this issue since #4261 landed.
</comment><comment author="roncemer" created="2014-10-29T21:15:35Z" id="61006381">I'd love to see ES automatically detect when a query is going to use more than a certain percentage of the heap, and automatically use temporary files to do its sorting, merging and so on.  That would give it the ability to run arbitrary queries (like MySQL) without bringing down the node.  The query would just take a long time to run.  And in many cases, that's absolutely fine -- especially when doing aggregations and similar analytic queries.
</comment><comment author="avleen" created="2014-10-29T22:18:34Z" id="61015042">I wouldn't say many cases. Maybe in some cases :-)
The problem with using disk for this is that you can increase the IO and
also hurt other queries, and again bring down s node. Elasticsearch is
quite sensitive to IO bandwidth. But it would certainly be nice to have the
option.

On Wed, Oct 29, 2014, 17:15 roncemer notifications@github.com wrote:

&gt; I'd love to see ES automatically detect when a query is going to use more
&gt; than a certain percentage of the heap, and automatically use temporary
&gt; files to do its sorting, merging and so on. That would give it the ability
&gt; to run arbitrary queries (like MySQL) without bringing down the node. The
&gt; query would just take a long time to run. And in many cases, that's
&gt; absolutely fine -- especially when doing aggregations and similar analytic
&gt; queries.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/2929#issuecomment-61006381
&gt; .
</comment><comment author="kimchy" created="2014-10-30T01:03:32Z" id="61031100">just putting note here, that though not "on demand", doc values as an option (using on disk storage for certain expensive, memory wise, fields that are used for aggs and/or sorting). A lot of progress has been made both in Lucene and ES to make them faster, 1.4 would be a huge step forward, and the following ES version that would work with Lucene 5 will be even better. We are heavily investing both in Lucene and ES to make this a performant and viable option.
</comment><comment author="avleen" created="2014-10-30T01:32:13Z" id="61033181">Shay, I think we'd noticed a significant I/O impact (probably caused by
more writes?) with doc values.
Do the recent changes improve that situation?

On Wed, Oct 29, 2014, 21:03 Shay Banon notifications@github.com wrote:

&gt; just putting note here, that though not "on demand", doc values as an
&gt; option (using on disk storage for certain expensive, memory wise, fields
&gt; that are used for aggs and/or sorting). A lot of progress has been made
&gt; both in Lucene and ES to make them faster, 1.4 would be a huge step
&gt; forward, and the following ES version that would work with Lucene 5 will be
&gt; even better. We are heavily investing both in Lucene and ES to make this a
&gt; performant and viable option.
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/2929#issuecomment-61031100
&gt; .
</comment><comment author="xelldran1" created="2014-11-25T16:34:17Z" id="64428637">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documentation: specify how to remove a setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2928</link><project id="" key="" /><description>Looking at:

http://www.elasticsearch.org/guide/reference/api/admin-indices-update-settings/

It is not clear to me how to set total_shards_per_node back to it's default once it has been modified. The ML provided the (as yet untested) answer of 0, but the documentation should probably specify the default value instead of "Unbounded".

Also, I looked for a way to remove settings altogether (undefine them) and found no way to do this. It would be nice if there was a documented way to REMOVE an arbitrary setting (or any object key for that matter) via the REST API.
</description><key id="13498980">2928</key><summary>Documentation: specify how to remove a setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">btimby</reporter><labels><label>docs</label></labels><created>2013-04-22T18:37:10Z</created><updated>2013-04-23T22:09:04Z</updated><resolved>2013-04-23T22:09:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-23T22:09:04Z" id="16889884">I updated the documentation: https://github.com/elasticsearch/elasticsearch.github.com/commit/6d77ecc0cfed15d87d2674f3fc63048d72ba5540 the default values i actually `-1` but any value &lt;= 0 will work here
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Log conflicting options / directives</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2927</link><project id="" key="" /><description>I ran into an issue today with ElasticSearch. The problem was my own bone-headedness. However, it took me longer to realize this than it should have.

My problem in a nutshell was that while troubleshooting some time ago, I had set total_shards_per_node to 4. For me, with 3 nodes and 12 shards (primaries) this number was too low. I had (incorrectly) assumed that shards referred to just primaries. I was recently trying to figure out where my replicas had all gone, and went to the mailing list for help.

https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/vCZN2LR7K0U

I have seen other systems (like Heartbeat) log a message when it has received conflicting instructions. On the one hand, I asked ES to make a replica for each shard. But on the other hand, I did not allocate enough shards per node to accommodate the replicas. The ES log file was completely silent, so I had nowhere except the ML to look for more information. I propose that in this situation, ES logs something like:

"WARNING: I need to store 24 shards, but total_shards_per_node \* node_count == 12."

P.S. I think that the terminology is a bit vague in regards to shards / replicas. Replicas are shards in some sense, but not in others. I would suggest separating the two, or introducing a term that applies to shards+replicas. For example, shard refers to primaries, replica refers to a copy of a shard, and **insert term here** refers to both (shards+replicas).
</description><key id="13498770">2927</key><summary>Log conflicting options / directives</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">btimby</reporter><labels /><created>2013-04-22T18:33:06Z</created><updated>2013-10-07T07:47:57Z</updated><resolved>2013-10-07T07:47:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-23T10:13:38Z" id="21404791">The nomenclature used by elasticsearch is to have primary shards, replica shards and shards to group them both.

Do you think the documentation should be improved in that regard? If so, please point us there or, even better, create a pull request for the documentation repo at https://github.com/elasticsearch/elasticsearch.github.com.
</comment><comment author="spinscale" created="2013-10-07T07:47:57Z" id="25790082">closing for now. If you have anything in regards to improve documentation, feel free to reopen anytime. I'd be glad to help and assist.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error executing must_not queries on sharded index</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2926</link><project id="" key="" /><description>Running this in 0.90.0 RC2(also latest build) fails with:

Query Failed [Failed to execute main query]]; nested: ElasticSearchIllegalArgumentException[Not distributed collection statistics for field: description]; "}]}

Works fine in 0.20.4. 

Also works if using only one shard, or if changing the must_not query for a must query. 

```
curl -XPOST http://localhost:9200/test -d '{"settings": { "index.number_of_replicas": 0, "index.number_of_shards": 2}}'

curl -XPOST http://localhost:9200/test/test/1 -d '{"id":1,"description":"foo other anything bar"}'
curl -XPOST http://localhost:9200/test/test/2 -d '{"id":2,"description":"foo other anything"}'
curl -XPOST http://localhost:9200/test/test/3 -d '{"id":3,"description":"foo other"}'

curl -XPOST http://localhost:9200/test/test/_search?search_type=dfs_query_then_fetch -d '{ "query": { "bool": { "must_not": [ { "match": { "description": { "query": "anything", "type": "boolean", "operator": "AND" } } } ] } }}'
```
</description><key id="13490064">2926</key><summary>Error executing must_not queries on sharded index</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>regression</label><label>v0.90.0</label></labels><created>2013-04-22T16:14:43Z</created><updated>2013-04-23T11:34:04Z</updated><resolved>2013-04-23T11:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-23T09:20:03Z" id="16847664">thanks for reporting this. This is actually caused by a change in Lucene that doesn't return terms for `must_not` clauses. I need to think a bit how to fix that but short term you can just use non-dfs search type?
</comment><comment author="lmenezes" created="2013-04-23T09:48:27Z" id="16848875">Well, We are not going live with 0.90 until the GA is out. So, if this is going to be fixed before 0.90 GA We will just wait. Otherwise, We would be willing to drop DFS queries until this is solved. Any plans for solving that before the GA?
</comment><comment author="s1monw" created="2013-04-23T09:49:03Z" id="16848908">@lmenezes I hope I will get a fix today! :)
</comment><comment author="lmenezes" created="2013-04-23T10:03:28Z" id="16849445">Oh, nice! I will just wait then. Changing to non DFS breaks a lot of our unit tests. Also, We are still waiting on another bug reported a few days ago(#2920). thanks for the fast reply
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>network.host (elasticsearch.yml): _eth1_ appears to bind to IPV6 only</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2925</link><project id="" key="" /><description>In order to get IPV4 access, I have to explicitly declare _eth1:ipv4_ - is that intended?

OS: CentOS release 6.3 (Final)
</description><key id="13489652">2925</key><summary>network.host (elasticsearch.yml): _eth1_ appears to bind to IPV6 only</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">astrostl</reporter><labels /><created>2013-04-22T16:06:23Z</created><updated>2013-04-30T18:12:36Z</updated><resolved>2013-04-30T18:12:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-04-24T06:24:28Z" id="16910008">Yes, IPV6 is preferred by default. You can change this behavior by setting [java.net.preferIPv4Stack](http://docs.oracle.com/javase/6/docs/technotes/guides/net/ipv6_guide/#ipv6-related) system property to `true`.
</comment><comment author="astrostl" created="2013-04-24T15:54:45Z" id="16942745">Consider this a vote for V6+V4 or V4 alone by default, then.  Thx for the info!  Clear to close as appropriate.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Config: elasticsearch.yml doesn't accept _lo_ or _lo0_ for network.host</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2924</link><project id="" key="" /><description>An explicit 127.0.0.1 works for me, while lo/lo0 don't.

Ran into this because I'm deploying ES using configuration management (Puppet), and would find it cleaner to go with all-names as opposed to a hybrid of numbers for localhost and names for non-localhost.
</description><key id="13489562">2924</key><summary>Config: elasticsearch.yml doesn't accept _lo_ or _lo0_ for network.host</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">astrostl</reporter><labels><label>enhancement</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-04-22T16:04:35Z</created><updated>2013-05-30T09:06:48Z</updated><resolved>2013-05-03T19:18:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Provide a SearchContext for DeleteByQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2923</link><project id="" key="" /><description>Fixes #2705
</description><key id="13489398">2923</key><summary>Provide a SearchContext for DeleteByQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2013-04-22T16:00:50Z</created><updated>2014-07-28T09:28:45Z</updated><resolved>2014-07-28T09:28:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-04-26T13:16:38Z" id="17073718">Hi @ofavre Great to see that you have come up with a pull request! The problem with this approach is that the searcher used inside the parent child like queries / filters doesn't have the same view of the data the index writer has. It can happen that child docs are not deleted because the parent docs have not yet made into the view of the searcher. This can lead the unexpected behaviour.
</comment><comment author="clintongormley" created="2014-07-28T09:28:45Z" id="50316727">Closing in favour of #7052 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UnavailableShardsException in elastic search cluster configuration !</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2922</link><project id="" key="" /><description>I have tried an elastic search cluster configuration with 3 nodes in Amazon EC2. But after making the call $type-&gt;addDocument($doc);, I am getting "UnavailableShardsException" .

Please help me to solve this issue ! Thanks in advance !
Asuthosh

The following are the configuration on each of the 3 nodes in the custer. 
# // Node1

cluster.name: MyCluster
node.name: node1
node.master: true
node.data: true
// # index.number_of_shards: 5 //Not confugured this, to use the default value 5
index.number_of_replicas: 2
transport.tcp.port: 9300
http.port: 9200
# // Node2

cluster.name: MyCluster
node.name: node2
node.master: false
node.data: true
// # index.number_of_shards: 5 //Not confugured this, to use the default value 5
index.number_of_replicas: 2
transport.tcp.port: 9301
http.port: 9201
# // Node3

cluster.name: MyCluster
node.name: node3
node.master: false
node.data: true
// # index.number_of_shards: 5 //Not confugured this, to use the default value 5
index.number_of_replicas: 2
transport.tcp.port: 9302
http.port: 9202
</description><key id="13472589">2922</key><summary>UnavailableShardsException in elastic search cluster configuration !</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Asuthosh</reporter><labels /><created>2013-04-22T09:58:59Z</created><updated>2013-10-30T10:42:33Z</updated><resolved>2013-10-30T10:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-22T13:44:49Z" id="16786453">have you waited until the cluster is ready? After creating an index it might take a short time in order to become fully operational.

See http://www.elasticsearch.org/guide/reference/api/admin-cluster-health/

If this does not apply, can you create a gist for full reproduction, so we can take closer took at the issue?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>It may lost exception  in response message  when submitting  StateUpdateTask in InternalClusterService</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2921</link><project id="" key="" /><description>If throw a exception when submitting StateUpdateTask in InternalClusterService,

System will log the exception ,
then return the mesage (acknowledged=false),
but i want to know the detailed information of exception from response message.

Maybe it is necessary to return exception information to the client.

the following is my idea.

1   add a method in ProcessedClusterStateUpdateTask.
     void failed(Throwable t);

2  implement this method in sub class
    public void failed(Throwable t) {
          listener.onFailure(t); 
    }
3  call this method in InternalClusterService
    ...
    try {
         ...
    } catch (Exception e) {
         updateTask.failed(e);
         ...
    }
</description><key id="13445828">2921</key><summary>It may lost exception  in response message  when submitting  StateUpdateTask in InternalClusterService</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">milesli</reporter><labels /><created>2013-04-21T07:46:39Z</created><updated>2013-11-02T09:25:53Z</updated><resolved>2013-11-02T09:25:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-10-30T12:29:04Z" id="27384712">Hey,

can you explain the use-case for what do you want to do? I guess you had this problem in production, but were unable to see further information/the information you needed? I'd like to have more context about what you are trying to improve here, if possible.

If you dont want to explain, a pull request would be awesome as well (including a test) :-)
</comment><comment author="javanna" created="2013-11-02T09:25:53Z" id="27618557">We can close this on. We used to have this problem, and to solve it we added the `onFailure` method to `ClusterStateUpdateTask` for the reason mentioned by @milesli . Here is the relevant commit: https://github.com/elasticsearch/elasticsearch/commit/4930b93c26506e8f063a3b817f0434bd17fca14c

Feel free to reopen if I misunderstood something though!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Error on Script Based Sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2920</link><project id="" key="" /><description>When used script based sorting on an index with mora than 1 shard, I get:

{"error":"ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[java.lang.String cannot be cast to org.apache.lucene.util.BytesRef]; ","status":500}

Eg:

```
curl -XPOST http://localhost:9200/script -d '{"settings": { "index.number_of_replicas": 0, "index.number_of_shards": 2}}'

curl -XPUT http://localhost:9200/script/test/_mapping -d '{"profile":{"dynamic":"strict","properties":{"id":{"type":"integer","index":"not_analyzed","store":"yes"},"groups_code":{"properties":{"id":{"type":"integer","index":"not_analyzed"},"date":{"type":"date","index":"not_analyzed","format":"date_time_no_millis"}}}}}}'

curl -XPUT http://localhost:9200/script/test/1 -d '{"groups_code":[{"id":47642,"date":"2010-08-12T07:54:55Z"}]}'

curl -XPUT http://localhost:9200/script/test/2 -d '{"groups_code":[{"id":47642,"date":"2010-05-04T12:10:54Z"}]}'


curl -XGET http://localhost:9200/script/test/_search -d '{"query":{"match_all":{}},"fields":"","sort":[{"_script":{"script":"if ( ! _source.groups_code.empty ) { result = ($.date in _source.groups_code if $.id == id); if ( ! result.empty ) { result[0] } else { '' } }","type":"string","reverse":true,"params":{"id":47642}}}]}'
```
</description><key id="13431914">2920</key><summary>Error on Script Based Sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>v0.90.0</label></labels><created>2013-04-20T09:21:10Z</created><updated>2013-04-23T11:34:45Z</updated><resolved>2013-04-23T11:34:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-04-20T09:22:46Z" id="16701223">sorry, forgot the version. This happens on 0.90.RC2 and also on the last build I had for RC3. Didn't use to happen on 0.20 afaik, but didn't check it.
</comment><comment author="spinscale" created="2013-04-20T11:21:25Z" id="16702481">This is not a shard issue, it also happens with one shard - and it also fails with 0.20.

To me it looks like your MVEL script is not correct and cannot be parsed. Check the last part of your error message, not the first name of the exception you are seeing.
</comment><comment author="lmenezes" created="2013-04-20T11:29:05Z" id="16702573">I did check the message and the line where it happens in the code. The exception is thrown at 
ShardFieldDocSortedHitQueue line 102(final BytesRef s1 = (BytesRef) docA.fields[i];). 
Didn't look much more into it, but as far as I know the script is correct, it's been running in production with 0.20.4. and also, just grabbed a 0.20.4 release, tested and it works fine. Doesn't on 0.90.0 though. Unless I'm doing something very wrong and not realizing, it still seems like a bug to me.
</comment><comment author="spinscale" created="2013-04-20T11:41:11Z" id="16702704">you sample above is not working, thats why I was confused. You put single ticks inside of the curl query and confused the shell with that as you also used them for the whole json data.
Slightly changing and running your query now does not give me an exception either. Something still has to be different I guess. Any hints to get it reproduced for me?

```
curl -XGET http://localhost:9200/script/test/_search -d '{"query":{"match_all":{}},"fields":"","sort":[{"_script":{"script":"if ( ! _source.groups_code.empty ) { result = ($.date in _source.groups_code if $.id == id); if ( ! result.empty ) { result[0] } }","type":"string","reverse":true,"params":{"id":47642}}}]}'

{"took":269,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":2,"max_score":null,"hits":[{"_index":"script","_type":"test","_id":"1","_score":null,"sort":["2010-08-12T07:54:55Z"]},{"_index":"script","_type":"test","_id":"2","_score":null,"sort":["2010-05-04T12:10:54Z"]}]}}
```
</comment><comment author="lmenezes" created="2013-04-20T11:47:53Z" id="16702772">From your response: {"took":269,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},
could you try it with 2 shards? For 1 shard it works fine for me too
</comment><comment author="lmenezes" created="2013-04-20T11:49:04Z" id="16702792">I mean, more than 1 shard on 0.90. On 0.20 works fine
</comment><comment author="spinscale" created="2013-04-20T12:41:27Z" id="16703398">Ok, I can reproduce it now, just do this:

```
curl -X DELETE localhost:9200/script

curl -XPOST http://localhost:9200/script -d '{"settings": { "index.number_of_replicas": 0, "index.number_of_shards": 2}}'

curl -XPUT http://localhost:9200/script/test/_mapping -d '{"profile":{"dynamic":"strict","properties":{"id":{"type":"integer","index":"not_analyzed","store":"yes"},"groups_code":{"properties":{"id":{"type":"integer","index":"not_analyzed"},"date":{"type":"date","index":"not_analyzed","format":"date_time_no_millis"}}}}}}'

curl -XPUT http://localhost:9200/script/test/1 -d '{"groups_code":[{"id":47642,"date":"2010-08-12T07:54:55Z"}]}'
curl -XPUT http://localhost:9200/script/test/2 -d '{"groups_code":[{"id":47642,"date":"2010-05-04T12:10:54Z"}]}'

curl -XGET http://localhost:9200/script/test/_search -d '{"query":{"match_all":{}},"fields":"","sort":[{"_script":{"script":"if ( ! _source.groups_code.empty ) { result = ($.date in _source.groups_code if $.id == id); if ( ! result.empty ) { result[0] } else { \u0027\u0027 } }","type":"string","reverse":true,"params":{"id":47642}}}]}'
```

And to make it even simpler, this leads to the exception as well:

```
curl -XGET http://localhost:9200/script/test/_search -d '{"query":{"match_all":{}},"fields":"","sort":[{"_script":{"script":"\u0027\u0027","type":"string" }}]}'
```

Looks like we forgot to change a BytesRef to a string (might explain that it happens since we switched to lucene 4 with ES 0.90). So yeah, bug.

Many many thanks for your time reporting and explaining!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Apply minimum_should_match to inner clauses of multi_match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2919</link><project id="" key="" /><description>When specifying minimum_should_match in a multi_match query it was being applied
to the outer bool query instead of to each of the inner field-specific bool queries.

Closes #2918
</description><key id="13405564">2919</key><summary>Apply minimum_should_match to inner clauses of multi_match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-04-19T15:16:07Z</created><updated>2014-07-09T18:47:45Z</updated><resolved>2013-04-19T19:59:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-19T19:59:55Z" id="16682470">pushed 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>minimum_should_match applied to wrong query in multi_match</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2918</link><project id="" key="" /><description>When using `minimum_should_match` with a `multi_match` query, it is being applied to the ``bool` query which wraps the per-field queries. It should be applied to each per-field query instead:

```
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "foo" : "one two three"
}
'
```

With a `match` query, the minimum of 70% doesn't find any results (correctly):

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "match" : {
         "foo" : {
            "minimum_should_match" : "70%",
            "query" : "three four five"
         }
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : null,
#       "total" : 0
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 10
# }
```

With `multi_match`, it finds results (incorrectly):

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "multi_match" : {
         "minimum_should_match" : "70%",
         "fields" : [
            "foo",
            "bar"
         ],
         "query" : "three four five",
         "use_dis_max": true
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : "one two three"
#             },
#             "_score" : 0.009060421,
#             "_index" : "test",
#             "_id" : "sa8shEUoR5SRtME0EA4Gyw",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 0.009060421,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 5
# } 
```
</description><key id="13402365">2918</key><summary>minimum_should_match applied to wrong query in multi_match</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>breaking</label><label>bug</label><label>v0.90.0</label></labels><created>2013-04-19T14:06:59Z</created><updated>2013-04-19T19:56:10Z</updated><resolved>2013-04-19T19:56:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-19T17:07:36Z" id="16665288">awesome! i will look at this soon I hope!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sorting based on parent/child relationship</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2917</link><project id="" key="" /><description>Currently there is no way to sort documents based on parent child relation. E.g.
Sorting a doc based on child doc field or the opposite.
</description><key id="13394115">2917</key><summary>Sorting based on parent/child relationship</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">kul</reporter><labels><label>:Parent/Child</label><label>adoptme</label><label>feature</label><label>high hanging fruit</label></labels><created>2013-04-19T09:43:06Z</created><updated>2017-07-07T18:17:46Z</updated><resolved>2016-08-24T15:05:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-04-19T14:58:23Z" id="16657902">@kul At the moment this isn't possible. This feature will be added in the near future.

For now you use @clintongormley's excellent workaround: http://stackoverflow.com/questions/14504180/elasticsearch-sorting-parents-through-child-values/14519947#14519947

This workaround allows you to sort on child values by using `custom_score` as child query. 
</comment><comment author="kul" created="2013-04-19T15:04:01Z" id="16658435">oh wow! if i can specify a query, it mean limitless possibilities for sorting using nested has_child/has_parent clause.

Thanks
</comment><comment author="GrantGochnauer" created="2013-05-14T17:50:11Z" id="17892952">Really looking forward to this feature - we use parent/child relationships extensively and right now have to copy children values on parent object to sort on them. Will give the work-around a try but hopefully we'll see this in the .90 series too ;) Thank you!
</comment><comment author="GrantGochnauer" created="2013-05-14T17:52:10Z" id="17893072">Is it true that the work-around requires you to leveraged nested mappings instead of a true/parent child relationship for the sorting to work? thanks!
</comment><comment author="P-Hill" created="2013-05-15T00:19:53Z" id="17913604">On 5/14/2013 10:52 AM, Grant Gochnauer wrote:

&gt; Is it true that the work-around requires you to leveraged nested 
&gt; mappings instead of a true/parent child relationship for the sorting 
&gt; to work? thanks!

http://www.elasticsearch.org/guide/reference/query-dsl/has-child-query/
"The |has_child| also has scoring support from version |0.20.2|. The 
supported score types are |max|, |sum|, |avg| or |none|"

Without having seen Clinton's post, but having discussed it a bit on the 
list, what I was looking for was the youngest child, so I used "max" to 
good effect to find parents with the youngest child ("newest parents").  
Originally I was playing with top_children, but has_children was what I 
really needed. The field that becomes my score is the date of a child file.

The problem that a score is a Float, so you can have round off problems 
when you convert a 64-bit Date long into a 32-bit Float. This round off 
can loose seconds, more often milliseconds.  Since my "children" are 
actually file instances.  I couldn't come up with any brilliant formula 
to stay away from the round off, because two files can have dates very 
close together that are not resolvable in the digits of a float.

If the score was a double I would be able use ~16 (base 10) digits of 
accuracy to better effect and rarely have round off of dates, so I hope 
someone changes how a score is stored in the entire Elastic Search and 
Lucene infrastructure from a Float to a Double :)  That is an easy 
change isn't it? :)

-Paul
</comment><comment author="GrantGochnauer" created="2013-05-15T13:23:44Z" id="17937906">Thanks for the reply P-Hill... We are developing an API that allows for an arbitrary sort on child fields which are different depending on who is leveraging our API. In other words, without built in support for sorting on child document fields, we aren't able to use the custom score very well.

Thanks
</comment><comment author="P-Hill" created="2013-05-16T17:07:25Z" id="18014714">+2
My example is just one case where a pretty simple thing like a datetime 
doesn't actually work to send through as a score.  I'm glad this is coming.
If somehow a result set of parents has a field from a matched child 
field, it would seem this could lead to other requested features like 
returning the one max/min/avg value or even a list of matching value 
(forget sorting them).  Since there seemed to be many requests for 
various things related to knowledge about the actual child matches of a 
parent, this should be a useful API.

On 5/15/2013 6:24 AM, Grant Gochnauer wrote:

&gt; We are developing an API that allows for an arbitrary sort on child 
&gt; fields which are different depending on who is leveraging our API.
</comment><comment author="vickenstein" created="2013-06-28T18:12:44Z" id="20204829">+1
Does any one know of any ways for fetching the _parent doc rather than just the _parent uid using the script field?
e.g. 
"script" : "_source._parent[\"somefield\"].value"
thanks! because if this is possible sorting using parent/child would be realized even if it is not optimized. 
</comment><comment author="serj-p" created="2014-03-18T09:55:20Z" id="37915094">Very important feature since it's computationally hard to update thousands of documents when all you need is update only one field in a big document and than make sorting by this field. For example contacts which have property like last contacted which changes very frequently but not whole contact. Update api doesn't solve my case since enabling _source will increase my index a lot.
</comment><comment author="amerov" created="2014-07-11T20:33:08Z" id="48779597">+1
</comment><comment author="travisbell" created="2014-09-04T14:31:44Z" id="54485472">+1

Hope to see this natively supported in ES!
</comment><comment author="machinelearner" created="2014-09-07T17:31:39Z" id="54753795">though the memory signature it leaves and the cost of compute is slightly high, this will be one of the most used feature if it comes out in ES. Eagerly looking forward to it!
</comment><comment author="parhammmm" created="2014-09-10T14:58:55Z" id="55128035">+10
</comment><comment author="stephane-bastian" created="2014-09-24T09:44:11Z" id="56647493">+1 IMHO, it's definitely one the top missing feature, along with:
- The ability to return matching inner objects: https://github.com/elasticsearch/elasticsearch/issues/3022
- The ability to 'join' parent and children https://github.com/elasticsearch/elasticsearch/issues/761
- and the ability to support paging in aggregations: https://github.com/elasticsearch/elasticsearch/issues/4915
</comment><comment author="pauleil" created="2015-04-15T21:15:44Z" id="93571684">Are there any plans to support this feature in the foreseeable future?
</comment><comment author="martijnvg" created="2015-04-16T11:53:21Z" id="93716207">Once the refactoring in #8134 is in, this is planned to be added. Like with the current refactoring the sorting by child or parent field should be added to the new Lucene query time join first. 
</comment><comment author="pauleil" created="2015-05-29T20:22:33Z" id="106925387">This is very exciting. Is there a plan of integrating this in an upcoming release, now that #8134 is solved?
</comment><comment author="vinusebastian" created="2015-06-16T10:54:41Z" id="112384567">@kul @martijnvg @clintongormley How do I do the workaround mentioned in http://stackoverflow.com/questions/14504180/elasticsearch-sorting-by-nested-documents-values/14519947#14519947 for a parent child relationship? In my script field what should replace "doc['locations.order'].value" to refer to the child document's field?

Thanks in advance
</comment><comment author="archie-sh" created="2015-08-31T10:52:35Z" id="136333075">would really like to see this implemented 
</comment><comment author="clintongormley" created="2015-09-19T17:33:18Z" id="141691805">@martijnvg With #8134 in, do you see a way forward for implementing this?
</comment><comment author="martijnvg" created="2015-09-20T19:21:29Z" id="141822794">@clintongormley Yes, I do see a way how this can be implemented. Similar to how the join is implemented, but instead of aggregating child scores per parent the sorting should aggregate sort values instead.
</comment><comment author="jaimemarijke" created="2015-10-20T17:52:02Z" id="149646444">@martijnvg - Do you have an idea on when this might be implemented? I am curious because this is a blocking issue for me for using parent/child relationships, which I would otherwise much prefer over nested documents. 
</comment><comment author="martijnvg" created="2015-10-26T05:46:29Z" id="151031311">@jaimemarijke No, there is no effort being done yet to get this feature in. 
</comment><comment author="christian-m" created="2015-11-11T16:07:33Z" id="155828934">&gt; **martijnvg** commented on 19 Apr 2013
&gt; @kul At the moment this isn't possible. This feature will be added _in the near future._

@martijnvg so, what means _in the near future_?
</comment><comment author="padusumilli" created="2015-12-22T19:38:25Z" id="166711756">+1
</comment><comment author="sundarv85" created="2016-01-12T22:41:21Z" id="171084646">+1
</comment><comment author="mrkamel" created="2016-01-14T20:19:09Z" id="171768157">+1
</comment><comment author="ecdeveloper" created="2016-01-16T09:25:17Z" id="172176563">+1
</comment><comment author="dohly" created="2016-01-28T08:50:33Z" id="176065663">+1
</comment><comment author="ecdeveloper" created="2016-01-28T09:00:56Z" id="176070838">[This link at SO](http://stackoverflow.com/questions/10415494/elasticsearch-sort-by-single-nested-document-key-in-array) actually helped me achieving what we need.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>terms_stats query - exclude terms from result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2916</link><project id="" key="" /><description>I currently have a terms facet  that excludes certain terms from its results, using the **exclude** option. I tried the same with a term_stats facet but the same results are not dropped.

I have looked at the Elasticsearch documentation (http://www.elasticsearch.org/guide/reference/api/search/facets/terms-stats-facet/) and see that the term_stats facet doesn't appear to have an exclude option. Since I don't always trust my full interpretation of the elasticsearch docs I was looking to see if anybody had found a workaround (beyond processing out the results client-side), so I tried this same question on Stackoverflow http://stackoverflow.com/questions/16046240/elasticsearch-terms-stats-query-exclude-terms-from-result

This facet doesn't work as expected (as I expect anyway):

```
"keywords_bad":{
  "terms_stats":{
    "size":100,
    "value_field":"retweet_count",
    "exclude":["http","consected"],
    "order":"total",
    "key_field":"text"
  }
}
```

whereas this facet works as expected:

```
"keywords_good":{    
  "terms":{
    "size":100,
    "exclude":["http","consected"],
    "order":"count",
    "field":"text"
  }
}
```

This seems a little inconsistent, and logically I can't think why exclude would be unreasonable in the terms_stats face. Adding the exclude option to the facet is what I would like to see.

So, I guess this issue is really a feature request. I took a look at the ES source for facets and I'll admit that I can read it, but I think my Java is a little rusty to contribute. So I'd like to define what I see the problem as, and if anybody thinks they can assist I'm open to help out with review, testing or whatever. 
</description><key id="13376251">2916</key><summary>terms_stats query - exclude terms from result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">philayres</reporter><labels /><created>2013-04-18T22:15:28Z</created><updated>2014-02-21T16:02:43Z</updated><resolved>2014-02-21T16:02:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2014-02-21T16:02:43Z" id="35743996">Closing this, as you can do this with the aggregations framework (using terms and stats aggregations)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add min_tf / max_tf option to match query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2915</link><project id="" key="" /><description>Add `min_tf` and `max_tf` options to `match` query, that controls what terms contribute to the matches of a particular document based on the term frequency (tf). Terms inside a document with tf lower than `min_tf` or higher than `max_tf` will not match.
</description><key id="13361111">2915</key><summary>Add min_tf / max_tf option to match query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>discuss</label><label>feature</label></labels><created>2013-04-18T17:41:34Z</created><updated>2014-09-05T08:27:46Z</updated><resolved>2014-09-05T08:27:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-21T12:36:19Z" id="16722489">@martijnvg I wonder if we should rather add a building block for stuff like this than specialising these things for certain queries. What I have in mind is something like the ability to filter an IndexReader that can be specified next to a query. BTW. we somehow can do what the min / max TF already with a custom similarity that returns a neg. score for certain terms :)
</comment><comment author="martijnvg" created="2013-05-01T09:19:02Z" id="17274236">Yes, it makes sense to have a wrapper for the `min_tf` / `max_tf` behaviour. I think something like this would be a nice building block in the query dsl:

```
"tf_filter" : {
   "min_tf" : 2,
   "max_tf" : 9,
   "query" : {
      ...
   }
}
```
</comment><comment author="clintongormley" created="2014-08-08T11:13:04Z" id="51589094">I don't get the use case here.  Is this worth supporting?
</comment><comment author="brwe" created="2014-09-05T08:27:46Z" id="54597958">Closing, there seems to be no use case for it. Feel free to open if you find one!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>MLT bug when source disabled?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2914</link><project id="" key="" /><description>I'm trying to use the more_like_this handler in almost the exact same way it's used in the documentation here:

http://www.elasticsearch.org/guide/reference/api/more-like-this/

curl -XGET "http://localhost:9200/foo/document/1008534/_mlt?mlt_fields=cs,ks,tpcs&amp;min_doc_freq=2"

{"error":"ElasticSearchException[No fields found to fetch the 'likeText' from]","status":500}

I'm guessing this bug stems from the fact that source is disabled, but I'm not really sure. If it is the case that source is required for MLT, you should document that fact.
</description><key id="13313189">2914</key><summary>MLT bug when source disabled?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/alexksikes/following{/other_user}', u'events_url': u'https://api.github.com/users/alexksikes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/alexksikes/orgs', u'url': u'https://api.github.com/users/alexksikes', u'gists_url': u'https://api.github.com/users/alexksikes/gists{/gist_id}', u'html_url': u'https://github.com/alexksikes', u'subscriptions_url': u'https://api.github.com/users/alexksikes/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/43475?v=4', u'repos_url': u'https://api.github.com/users/alexksikes/repos', u'received_events_url': u'https://api.github.com/users/alexksikes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/alexksikes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'alexksikes', u'type': u'User', u'id': 43475, u'followers_url': u'https://api.github.com/users/alexksikes/followers'}</assignee><reporter username="">rlvoyer</reporter><labels><label>:More Like This</label><label>bug</label></labels><created>2013-04-17T19:01:54Z</created><updated>2015-07-06T14:58:23Z</updated><resolved>2015-07-06T14:58:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-17T19:55:21Z" id="16531828">I think you either need the source or the field needs to be stored or you need to store term vectors for the field. But I agree we should document that!

thanks for raising this... what is your mapping for those fields?
</comment><comment author="rlvoyer" created="2013-04-17T20:37:46Z" id="16534152">``` json
{
    "document": {
        "_source" : {
            "enabled" : false
        },
        "term_vector": "yes",
        "dynamic": false,
        "properties": {
            "_id": {
                "type": "long", 
                "index": "not_analyzed"
            },
            "cs": {
                "type": "string", 
                "analyzer": "keyword",
                "store": "no"
            }, 
            "ks": {
                "type": "string", 
                "analyzer": "keyword", 
                "store": "no"
            },
            "tpcs": {
                "type": "string",
                "analyzer": "keyword", 
                "store": "no"
            }
        }
    }
}
```
</comment><comment author="s1monw" created="2013-04-18T08:40:42Z" id="16564373">ah I see you should put `term_vector` next to `store` for each filed you want to store term vectors. Can you try that?

like this:

```
{
  "type" : "string",
  "store" : "no",
  "term_vector" : "yes"
}
```

simon
</comment><comment author="s1monw" created="2013-04-18T08:44:25Z" id="16564528">I pushed a fix to the documentation: https://github.com/elasticsearch/elasticsearch.github.com/commit/25614ced9513e24dc3ad99b976b00e8c384ff9f2
</comment><comment author="rlvoyer" created="2013-04-18T16:21:57Z" id="16586903">Thanks -- I'll make that fix. What is the effect (if any) of enabling term_vector storage at the top-level as I have done here?
</comment><comment author="s1monw" created="2013-04-19T20:13:00Z" id="16683657">hmm it seems that this only works if it's stored or you enabled source. we should be able to support this if TV are stored for the fields as well... reopening
</comment><comment author="rlvoyer" created="2013-05-02T21:17:08Z" id="17365365">Hey @s1monw -- have you had an opportunity to look into this issue?
</comment><comment author="kimchy" created="2013-05-02T21:20:34Z" id="17365570">I am not a fan of supporting it for tern vector and no store, cause then we need to get that info(TV) from the document on the specific shard and then send it to all the shards to do the MLT based on it. Just store the source and MLT based on that. You can also, btw, always use the MLT query as part of a search request and provide the text there externally.
</comment><comment author="rlvoyer" created="2013-05-02T21:32:08Z" id="17366165">@kimchy can you explain how storing the source alleviates the problem of distributing the term vector to all the shards for the MLT computation?
</comment><comment author="kimchy" created="2013-05-02T21:33:47Z" id="17366254">cause with the source text to do MLT by, you don't need the term vectors.
</comment><comment author="s1monw" created="2013-05-02T21:34:10Z" id="17366272">I agree this seems odd... isn't the TV just a different representation of a field?
</comment><comment author="rlvoyer" created="2013-05-02T21:46:00Z" id="17366876">@kimchy @s1monw so why store the term vectors at all? (I was only storing them because of the following doc: http://www.elasticsearch.org/guide/reference/api/more-like-this/) If MLT doesn't need them when it has the source text, does it then recompute term vectors given the source text? 
</comment><comment author="s1monw" created="2013-05-02T21:46:59Z" id="17366930">I agree this should also work on TV though. yet at this point it doesn't so you might want to get rid of TV if you don't need them.
</comment><comment author="rlvoyer" created="2013-06-12T17:01:22Z" id="19339708">@kimchy @s1monw I'd like to try to write a plugin similar to more-like-this that does exactly what I want. Can you suggest any plugins that access term vectors that I might use as references? Any tips / documentation are much appreciated.
</comment><comment author="s1monw" created="2013-06-12T17:47:24Z" id="19342528">hey, we just added TermVector support lately. this issue is on our list to make use of the feature. Can you wait for it?
</comment><comment author="rlvoyer" created="2013-06-12T18:51:21Z" id="19346669">@s1monw Unfortunately, my company has a rapidly narrowing window for determining whether elasticsearch is right for the problem we're trying to solve. Given that the current built-in functionality doesn't seem to handle our use-case, a plugin seems like our only option in the short-term.
</comment><comment author="Signum" created="2013-11-19T20:12:28Z" id="28828232">Excuse me but I'm currently trying to use the MLT feature. I read http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-more-like-this.html#search-more-like-this and either my english is completely bad of I have not the remotest idea what it is supposed to mean:

"Note: In order to use the mlt feature a mlt_field needs to be either be stored, store term_vector or source needs to be enabled."

What is "stored"? Which "source"? I've been searching the internet for two hours now and can't any example of how to use MLT successfully. And to be honest this issue report doesn't help me either. Could anyone shed some light on it and fix the documentation please?
</comment><comment author="s1monw" created="2013-11-19T20:50:21Z" id="28833759">In Elasticsearch you can either store the entire document (the json you send to ES when you index) aka. the `source` or you can mark a field as `stored : true` then we only store the value of that particular field. By default the `source` is stored (or `enabled`) but you can also `disable` it via the mapping. The term_vectors don't work yet with `MLT` hence this issue. 

hope that helps
</comment><comment author="Signum" created="2013-11-19T21:01:48Z" id="28835035">@s1monw Thanks for the reply. So to rephrase: any field I'm using as "mlt_fields=..." needs to
- either part of the actual source/document
- or be explicitly marked as "stored:true"

Okay. In my case the documents contain two fields. Example:

&lt;pre&gt;
{
_index: "debshots",
_type: "jdbc",
_id: "396",
_version: 35,
exists: true,
_source: {
    description: "Alarm Clock for GTK Environments",
    name: "alarm-clock"
    }
}
&lt;/pre&gt;

But when I'm GETting http://localhost:9200/debshots/jdbc/396/_mlt Elasticsearch returns zero results:

&lt;pre&gt;
{
took: 3,
timed_out: false,
_shards: {
    total: 1,
    successful: 1,
    failed: 0
    },
hits: {
    total: 0,
    max_score: null,
    hits: [ ]
    }
}
&lt;/pre&gt;

There are many other documents with a description like "Alarm curl plugin for uWSGI" so I had assumed that at least the "Alarm" is a term that makes it "more-like-that"-style.

I'd welcome a hint what is going wrong here. Thanks.

And I would also welcome a rewrite of that quoted phrase in the documentation because it's wrong english and hard to understand. (I still didn't.)
</comment><comment author="s1monw" created="2013-11-19T21:03:02Z" id="28835161">Can you take this please to the mailing list this is only for development issues. 

thanks
</comment><comment author="Signum" created="2013-11-19T21:05:38Z" id="28835406">@s1monw  Will do. Please still consider rewriting this sentence in the documentation to make it understandable.
</comment><comment author="alexksikes" created="2015-07-06T14:58:22Z" id="118881135">This issue is now outdated, closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow boost fields to be indexed and stored</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2913</link><project id="" key="" /><description>Currently there is no way to change the default behavior of boost fields being not indexed and not stored. If a boosted field can be also indexed, then it can be sorted/faceted on if there is a use case that requires it. This change will make boost fields behave like the timestamp field, which can be indexed and stored.

I know that Shay is not a fan of document-level boosts and in fact, Lucene 4 has gotten rid of them. But the boost field feature is there, so it would be great to use it to its full potential.
</description><key id="13305553">2913</key><summary>Allow boost fields to be indexed and stored</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">brusic</reporter><labels /><created>2013-04-17T16:50:55Z</created><updated>2014-06-26T09:40:26Z</updated><resolved>2013-09-20T17:01:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-07-05T18:02:16Z" id="20531652">Clean repo: https://github.com/brusic/elasticsearch/commit/a506b5956293244e2fbe396ec4eecdf3f6fe8a4b
</comment><comment author="martijnvg" created="2013-09-19T12:40:38Z" id="24735594">Sorry @brusic we lost track of this PR. Can you isolate the 'Allow BoostFields to be indexed and stored' commit and rebase with master?
</comment><comment author="martijnvg" created="2013-09-20T17:01:26Z" id="24825037">@brusic I've pushed your 'Allow BoostFields to be indexed and stored' commit to master and 0.90. Thanks for opening this PR.
</comment><comment author="brusic" created="2013-09-20T17:03:34Z" id="24825161">Thanks for taking care of it. My plan was to squash the commit tonight (PDT), but you beat me to it. I'm assuming that 0.90.6 won't be released soon anyways.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>If a value/field is a Calendar, it will be converted to a Date using getTime()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2912</link><project id="" key="" /><description>Closes #2911
</description><key id="13303673">2912</key><summary>If a value/field is a Calendar, it will be converted to a Date using getTime()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lucaslward</reporter><labels /><created>2013-04-17T16:11:07Z</created><updated>2014-07-14T13:34:20Z</updated><resolved>2013-04-23T21:59:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lucaslward" created="2013-04-17T22:26:13Z" id="16539670">Not really Simon. I had a lot of parens due to type casting among other things. So, I suppose it's a code style thing. If the Elasticsearch codestyle is to prefer inline over additional methods, I will keep that in mind for future pull requests. By contrast, the Spring code style was much more 'wordy' in my experience, so it's probably just what I got used to from working in that code base.
</comment><comment author="s1monw" created="2013-04-18T08:59:26Z" id="16565177">fair enough... but couldn't we also move the cast into the method then an do something like assert obj instanceof Caldendar; 

I mean no big deal just curious... I will pull as it is in a bit.
</comment><comment author="s1monw" created="2013-04-23T21:59:23Z" id="16889408">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContentBuilder doesn't handle Java Calendar</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2911</link><project id="" key="" /><description>``` Java
Calendar calendar = new GregorianCalendar();
String expectedCalendar = XContentBuilder.defaultDatePrinter.print(calendar.getTimeInMillis());
XContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);
builder.startObject().field("calendar", calendar).endObject();
assertThat(builder.string(), equalTo("{\"calendar\":\"" + expectedCalendar + "\"}"));
```

The above will fail because the builder would have just called toString() on Calendar, even though it's trivial to say calendar.getTime() (which returns a normal Date)
</description><key id="13303161">2911</key><summary>XContentBuilder doesn't handle Java Calendar</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lucaslward</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-17T16:01:01Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-04-18T09:32:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Response for Cluster Settings Update API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2910</link><project id="" key="" /><description>If cluster settings are update the REST API returns the accepted values. For
example, updating the `cluster.routing.allocation.disable_allocation` via
cluster settings:

```
curl -XPUT http://localhost:9200/_cluster/settings -d '{
    "transient":{
        "cluster.routing.allocation.disable_allocation":"true"
    }
}'
```

will respond:

```
{
    "persistent":{},
    "transient":{
        "cluster.routing.allocation.disable_allocation":"true"
    }
}
```

Closes #2907
</description><key id="13302055">2910</key><summary>Response for Cluster Settings Update API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-04-17T15:38:12Z</created><updated>2014-07-16T21:53:39Z</updated><resolved>2013-07-15T15:37:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-18T09:40:05Z" id="16566989">cool this looks great! I will run tests and pull it in!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to write the query which gives exact result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2909</link><project id="" key="" /><description>Hi 

I am writing the query for _all like choudharynarendra@gmail.com inside what it is doing is it is splitting the signle email into chodharynarendra and gmail.com but i dont want to let it work that way ... how do i proceed to write the query so that my email choudharynarendra@gmail.com will treat as a single word and give me the search accordingly.
</description><key id="13290956">2909</key><summary>How to write the query which gives exact result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">narendrachoudhary</reporter><labels /><created>2013-04-17T11:06:55Z</created><updated>2013-04-17T11:32:01Z</updated><resolved>2013-04-17T11:32:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-17T11:32:00Z" id="16500403">Check out the uaxurlemail tokenizer at http://www.elasticsearch.org/guide/reference/index-modules/analysis/uaxurlemail-tokenizer/

Please do not use the issue tracker as a support forum. Use the google group instead. There are way more people to help you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Recovery should re-assign shards when a node re-joins the cluster</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2908</link><project id="" key="" /><description>When a node disconnects and then re-joins the cluster, it is stripped of all its shard assignments, even if it is the best recovery candidate. This significantly prolongs recovery, increases the burden on the cluster, and in observed cases can cause the cluster to go red (more below). 

Either of the following would help the situation:
1. Re-plan the recovery when a node joins. If the re-joining node has more segments than the current recoverer, stop that recovery and have the re-joining node take it over.
2. Do not un-assign the shards from the errant node. Let the shards become over-replicated, then rebalance (rather than letting them become under-replicated and then rebalancing).

---

We run nodes with hundreds of GB of data in EC2 instances. The combination of heavy shards and modest network bandwidth means recovery can take tens of minutes. This is much longer than a node would typically be off-line after encountering a fault -- even a full rebuild of a node takes only a few minutes, and comes back with all its data intact thanks to the magic of EBS. In the common case of a rolling restart each node rejoins within a few seconds. 

Suppose in my cluster Kitty has shards 1, 2; Xavier has shards 3, 4; Jane has 1, 3; Hank has 2, 3, 4; Scott has 1, 2, 4. Kitty will phase out (to later re-join); Xavier and Jane will start recover of shards 1 and 2 respectively.

In the current world, with `c.r.a.allow_rebalance` set to default, Kitty will re-join and do nothing until recovery is complete - she does not send or receive shards, and does not answer queries. Once the cluster is green, she is then assigned an arbitrary portion of shards and the cluster begins _rebalancing_ onto her. In general, only a few of the new shard assignments will overlap and so her first act is to delete most of the data on her disk. With allow rebalance set to `always`, she will at least begin rebalancing immediately on join, but again to an arbitrary set of shards: she'll rejoin with 1 and 2 complete or mostly-complete (due to intervening writes), but the odds are only (1/nshards^2) that a shard is re-assigned.

The downsides:
- Any transient disconnect results in a minimum recovery period of (MB per node) / (MB/s recovery throughput). 
- During that time the cluster has gone from 5 strong nodes to 4 nodes doing recovery and serving 125% of their normal data burden.
- The shards are under-replicated during this time. If Scott blips out during recovery as well, you all of a sudden have effectively no replication for shards 1 and 2 even though three machines have the data. If Hank additionally goes down, we've seen (at least in 19.8) a situation where the shards remained un-assigned until a full-cluster stop / full-cluster start could be effected.

Proposed Alternatives:
1. **Re-plan the recovery when a node joins.** When Kitty phases out, her shards are assigned to Xavier and Jane and recovery initiates as normal. When Kitty re-joins, the master node takes stock of how many segments she has for all under-replicated shards, and forms a new recovery plan. Suppose Kitty rejoins with 60% of the current segments for shard 1 and 95% of shard 2, while the other nodes are 70% through transfer. Xavier will complete the recovery of shard 1, and Kitty will delete hers; Jane will stop recovering shard 2, while Kitty will recover that last 5%. Once the cluster is green, some shard will be rebalanced onto Kitty.
2. **Do not un-assign the shards from the errant node.** When Kitty phases out, assign her shards as usual to Xavier and Jane -- but leave them also assigned to Kitty. If Kitty rejoins, she will complete whatever incremental recovery of her shards are necessary. The cluster will then choose how to discard the over-replicated shards to find optimal balance. The improvement here is that a) however quickly Kitty completes recovery is how quickly you're serving data from a full-strength cluster again; b) you're  spending as little time as possible in an under-replicated state.  It's safer to ski downhill.
</description><key id="13290260">2908</key><summary>Recovery should re-assign shards when a node re-joins the cluster</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/bleskes/following{/other_user}', u'events_url': u'https://api.github.com/users/bleskes/events{/privacy}', u'organizations_url': u'https://api.github.com/users/bleskes/orgs', u'url': u'https://api.github.com/users/bleskes', u'gists_url': u'https://api.github.com/users/bleskes/gists{/gist_id}', u'html_url': u'https://github.com/bleskes', u'subscriptions_url': u'https://api.github.com/users/bleskes/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/1006375?v=4', u'repos_url': u'https://api.github.com/users/bleskes/repos', u'received_events_url': u'https://api.github.com/users/bleskes/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/bleskes/starred{/owner}{/repo}', u'site_admin': False, u'login': u'bleskes', u'type': u'User', u'id': 1006375, u'followers_url': u'https://api.github.com/users/bleskes/followers'}</assignee><reporter username="">mrflip</reporter><labels><label>:Recovery</label></labels><created>2013-04-17T10:44:04Z</created><updated>2015-09-19T17:30:34Z</updated><resolved>2015-09-19T17:30:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ghoseb" created="2013-11-10T13:56:09Z" id="28151024">:thumbsup: 
</comment><comment author="clintongormley" created="2014-11-29T14:54:33Z" id="64954238">Related to #7288, #8190, #6069
</comment><comment author="clintongormley" created="2015-09-19T17:30:34Z" id="141691694">This is now closed by #11438, #12421, and #11417
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Empty response when updating cluster settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2907</link><project id="" key="" /><description>This happens with at least 0.20.4 and 0.90.0.RC2, so I guess it happens on every version.
Is that the desired behaviour? It's just a bit confusing not getting any response and not being sure if the new value was really applied or not(or having to do another request and check if it was applied)

Eg:
curl -XPUT http://localhost:9200/_cluster/settings -d '{"transient":{"cluster.routing.allocation.disable_allocation":"true"}}'
</description><key id="13283876">2907</key><summary>Empty response when updating cluster settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lmenezes</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-17T07:15:52Z</created><updated>2013-04-18T09:50:31Z</updated><resolved>2013-04-18T09:50:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="karmi" created="2013-04-17T07:33:08Z" id="16491088">//subscribe @karmi
</comment><comment author="s1monw" created="2013-04-17T10:22:47Z" id="16497954">this is the current behaviour but getting a response of the settings that we actually updated would be nice I agree. I think this should return the sub-set of settings that were accepted. Thoughts?
</comment><comment author="lmenezes" created="2013-04-17T11:29:21Z" id="16500327">sounds good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search Stats: Add current open searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2906</link><project id="" key="" /><description>Add the current number of open searches (context) that are maintained to support future (when scrolling), or in flight search requests (between query and fetch).
</description><key id="13276048">2906</key><summary>Search Stats: Add current open searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-17T01:07:15Z</created><updated>2013-04-17T01:09:01Z</updated><resolved>2013-04-17T01:09:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>"offset" for histogram facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2905</link><project id="" key="" /><description>The histogram facet supports an "interval", but being able to specify an "offset" as well would support the case where a field stores data in a unit that doesn't just differ from the desired unit in scale, but has an offset as well (e.g. Kelvin vs Celsius).
</description><key id="13274777">2905</key><summary>"offset" for histogram facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ejain</reporter><labels /><created>2013-04-17T00:11:56Z</created><updated>2014-08-08T11:08:27Z</updated><resolved>2014-08-08T11:08:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T11:08:27Z" id="51588764">Closed by #6980 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic_templates do not work when other fields are mapped</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2904</link><project id="" key="" /><description>In the case where the type of a field may change between documents, a solution is to nominate a dynamic_mapping rule for all fields.  This solution works if there are no other fields mapped, but fails when there are.

The following case works:

```
curl -XDELETE "http://localhost:9200/test/"

curl -XPUT "http://localhost:9200/test/"

curl -XPUT http://localhost:9200/test/test/_mapping -d '
{
  "test": {
    "dynamic_templates": [
      {
        "long_fields": {
          "match": "*",
          "match_mapping_type" : "long",
          "mapping": {
              "type": "string"
          }
        }    
      }   
    ]
  }
}'

curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "test": 123
}'

curl -XPUT http://localhost:9200/test/test/2 -d '
{
  "test": "abc"
}'    
```

The following case fails:

```
curl -XDELETE "http://localhost:9200/test/"

curl -XPUT "http://localhost:9200/test/"

curl -XPUT http://localhost:9200/test/test/_mapping -d '
{
  "test": {
    "dynamic_templates": [
      {
        "long_fields": {
          "match": "*",
          "match_mapping_type" : "long",
          "mapping": {
              "type": "string"
          }
        }    
      }   
    ]
  },  
  "properties": {
    "timestamp": {
      "store": "yes",
      "type": "date"
    }
  }
}'

curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "test": 123
}'

curl -XPUT http://localhost:9200/test/test/2 -d '
{
  "test": "abc"
}'    
```
</description><key id="13263370">2904</key><summary>dynamic_templates do not work when other fields are mapped</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonpolites</reporter><labels /><created>2013-04-16T19:04:50Z</created><updated>2014-07-23T13:24:51Z</updated><resolved>2014-07-23T13:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T13:24:51Z" id="49872485">This appears to have been fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>REST API: Failure to index docs that have their ids URL encoded and contain `/`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2903</link><project id="" key="" /><description>v0.20.5

assuming `%2F` is encoded `/`:

```
&#10095; curl -XPUT "$DB/a%2Az" -d '{}'
{"ok":true,"_index":"users","_type":"ids","_id":"a*z","_version":1}%
&#10095; curl -XPUT "$DB/a%2Bz" -d '{}'
{"ok":true,"_index":"users","_type":"ids","_id":"a+z","_version":1}%
&#10095; curl -XPUT "$DB/a%2Cz" -d '{}'
{"ok":true,"_index":"users","_type":"ids","_id":"a,z","_version":1}%
&#10095; curl -XPUT "$DB/a%2Dz" -d '{}'
{"ok":true,"_index":"users","_type":"ids","_id":"a-z","_version":2}%
&#10095; curl -XPUT "$DB/a%2Ez" -d '{}'
{"ok":true,"_index":"users","_type":"ids","_id":"a.z","_version":2}%
&#10095; curl -XPUT "$DB/a%2Fz" -d '{}'
403 Forbidden /users/ids/a%2Fz%
```

IDS that work: `_%2F`, `%2F_`

IDS that don&#8217;t: `_%2F_`, `%2F`, `%2F%2F`

regression for gh-681
</description><key id="13254101">2903</key><summary>REST API: Failure to index docs that have their ids URL encoded and contain `/`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">paulmillr</reporter><labels /><created>2013-04-16T16:01:25Z</created><updated>2014-08-08T11:07:18Z</updated><resolved>2014-08-08T11:07:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T11:07:18Z" id="51588690">Not sure when this was fixed, but it works in master.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Return HTTP 413 (Request Entity Too Large) when http.max_content_length exceeded</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2902</link><project id="" key="" /><description>Currently elasticsearch drops the connection if http.max_content_length is exceeded. While this is acceptable behavior based on RFC2616 (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.14), it's not particularly friendly to client libraries.

Depending on the library being used, it can be difficult to determine the exact size of the HTTP request prior to actually sending it. Additionally, when the connection is simply closed, it leaves the underlying cause of the problem somewhat ambiguous without also inspecting the elasticsearch logs.

**Proposed change:** add an option to return HTTP 413 when http.max_content_length is exceeded instead of just dropping the connection.

**Steps to repro the current behavior:**
1. Set http.max_content_length = 1kb in elasticsearch.yml
2. Create a test index:
   curl -XPUT 'http://localhost:9200/testindex/'
3. Index a case that exceeds http.max_content_length:
   curl -v -XPUT 'http://localhost:9200/testindex/testtype/1' -d '{
     "message": "large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message large message"
   }'

Expected: HTTP status code 413 (Request Entity Too Large)

Actual: Dropped connection client-side, and a TooLongFrameException in elasticsearch log

Here's the output from curl:

```
* About to connect() to localhost port 9200 (#0)
*   Trying 127.0.0.1... connected
* Connected to localhost (127.0.0.1) port 9200 (#0)
&gt; PUT /testindex/testtype/1 HTTP/1.1
&gt; User-Agent: curl/7.20.1 (i686-pc-cygwin) libcurl/7.20.1 OpenSSL/0.9.8o zlib/1.2.5 libidn/1.18 libssh2/1.2.5
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; Content-Length: 1026
&gt; Content-Type: application/x-www-form-urlencoded
&gt; Expect: 100-continue
&gt;
&lt; HTTP/1.1 100 Continue
* Empty reply from server
* Connection #0 to host localhost left intact
curl: (52) Empty reply from server
* Closing connection #0
```
</description><key id="13246296">2902</key><summary>Return HTTP 413 (Request Entity Too Large) when http.max_content_length exceeded</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dbertram</reporter><labels><label>:REST</label><label>enhancement</label></labels><created>2013-04-16T13:30:43Z</created><updated>2016-12-16T11:42:11Z</updated><resolved>2016-12-16T11:13:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="SpikeTheMaster" created="2013-09-03T13:26:53Z" id="23712046">+1 - This is causing me trouble at the moment (Have a specific list of indexes to search), specifically the Pyes python library raises NoServerAvailable exception when this happens, not very helpful!
</comment><comment author="spinscale" created="2013-09-13T07:30:20Z" id="24377825">Looks like a dupe of https://github.com/elasticsearch/elasticsearch/issues/2137 (or am I missing something) - You need to do some extra work to do this with netty3 IIRC, so maybe netty4 will help here.
</comment><comment author="seti123" created="2014-02-08T20:35:43Z" id="34555298">+1  Version 0.90.10

org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: HTTP content length exceeded 104857600 bytes.

Update: I increased size in /etc/elasticsearch/elasticsearch.yml and it works again

&lt;pre&gt;
    # Set a custom allowed content length:
    # 
    http.max_content_length: 500mb
&lt;/pre&gt;
</comment><comment author="denispeplin" created="2015-08-05T16:09:35Z" id="128052897">The exception with same name can be caused by large header, and it should be fixed differently: #5665
</comment><comment author="wflanagan" created="2016-04-28T15:25:52Z" id="215465527">Is there a workaround for this?  We are regularly getting this error on some larger documents.  
</comment><comment author="jasontedor" created="2016-04-28T15:45:22Z" id="215472498">&gt; Is there a workaround for this? We are regularly getting this error on some larger documents.

@wflanagan Did you increase `http.max_content_length`?

The issue _here_ is about how the connection is terminated and does not return HTTP status code 413 when the configured max content length is exceeded (by the way, this is an underlying issue with Netty).
</comment><comment author="karmi" created="2016-06-09T16:02:36Z" id="224942515">I can confirm that the problem still exists, and it's painful for the clients, since there's no good way how to properly handle the situation or propagate the error to the user (see the linked issue for the Ruby client). I've decreased the limit when launching Elasticsearch:

``` bash
$ ./tmp/builds/elasticsearch-2.4.0-SNAPSHOT/bin/elasticsearch -D es.http.max_content_length=1kb
```

When I try to index a document via Curl, I get back a `100 Continue` response, which is really weird:

``` bash
$ curl -v -X POST localhost:9200/test/test/1 -d @/Users/karmi/Contracts/Elasticsearch/Projects/BuildSystem/API/test/fixtures/builds_elasticsearch.json
* Hostname was NOT found in DNS cache
*   Trying ::1...
* Connected to localhost (::1) port 9200 (#0)
&gt; POST /test/test/1 HTTP/1.1
&gt; User-Agent: curl/7.37.1
&gt; Host: localhost:9200
&gt; Accept: */*
&gt; Content-Length: 75680
&gt; Content-Type: application/x-www-form-urlencoded
&gt; Expect: 100-continue
&gt; 
&lt; HTTP/1.1 100 Continue
* Empty reply from server
* Connection #0 to host localhost left intact
curl: (52) Empty reply from server
```

This is the log output from Elasticsearch:

```
[2016-06-09 17:23:58,635][WARN ][http.netty               ] [Armor] Caught exception while handling client http traffic, closing connection [id: 0xc370c548, /0:0:0:0:0:0:0:1:62732 =&gt; /0:0:0:0:0:0:0:1:9200]
org.jboss.netty.handler.codec.frame.TooLongFrameException: HTTP content length exceeded 1024 bytes.
  at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:169)
```

I don't know the options we have how to handle the situation when somebody sends a too big request, but I think we should try hard here to be correct and return the [`413 Payload Too Large`](https://tools.ietf.org/html/rfc7231#section-6.5.11) response?
</comment><comment author="clintongormley" created="2016-11-06T09:47:19Z" id="258667755">Netty has added the ability to respond with a 413. See https://github.com/netty/netty/pull/2211
</comment><comment author="jasontedor" created="2016-12-16T11:13:14Z" id="267572324">With the upgrade to Netty 4, this is now handled correctly:

```
06:10:30 [jason:~] $ ~/elasticsearch/elasticsearch-6.0.0-alpha1-SNAPSHOT/bin/elasticsearch -E http.max_content_length=64m -d
06:10:32 [jason:~] $ dd if=/dev/zero of=zero bs=1024k count=128
128+0 records in
128+0 records out
134217728 bytes transferred in 0.199715 secs (672045843 bytes/sec)
06:10:42 [jason:~] $ curl -v -H "Expect:" -XPOST localhost:9200/ --data-binary @zero
Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying ::1...
* TCP_NODELAY set
* Connected to localhost (::1) port 9200 (#0)
&gt; POST / HTTP/1.1
&gt; Host: localhost:9200
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt; Content-Length: 134217728
&gt; Content-Type: application/x-www-form-urlencoded
&gt;
&lt; HTTP/1.1 413 Request Entity Too Large
&lt; content-length: 0
* HTTP error before end of send, stop sending
&lt;
* Curl_http_done: called premature == 0
* Closing connection 0
```

Note that if you send an `Expect: 100-continue` header informing Elasticsearch that you would like to send a request with `Content-Length` greater than `http.max_content_length`, Elasticsearch responds with a `417 Expectation Failed` as the `100 Continue` can not be granted due to the excessive `Content-Length`:

```
06:10:49 [jason:~] $ curl -v -XPOST localhost:9200/ --data-binary @zero
Note: Unnecessary use of -X or --request, POST is already inferred.
*   Trying ::1...
* TCP_NODELAY set
* Connected to localhost (::1) port 9200 (#0)
&gt; POST / HTTP/1.1
&gt; Host: localhost:9200
&gt; User-Agent: curl/7.51.0
&gt; Accept: */*
&gt; Content-Length: 134217728
&gt; Content-Type: application/x-www-form-urlencoded
&gt; Expect: 100-continue
&gt;
&lt; HTTP/1.1 417 Expectation Failed
&lt; content-length: 0
* HTTP error before end of send, stop sending
&lt;
* Curl_http_done: called premature == 0
* Closing connection 0
```</comment><comment author="jasontedor" created="2016-12-16T11:14:08Z" id="267572469">Closed by #19526</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_msearch use source parameter if no body is given</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2901</link><project id="" key="" /><description>Permit `_msearch` to take the body content from the REST parameter `source` if no request body is given, just like `_search` does.
</description><key id="13244061">2901</key><summary>_msearch use source parameter if no body is given</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">ofavre</reporter><labels /><created>2013-04-16T12:43:38Z</created><updated>2014-07-02T17:38:46Z</updated><resolved>2014-01-28T15:09:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-04-16T17:31:31Z" id="16459130">Last time I checked _search would only use source parameter when using GET. When using source with POST it will fail with an exception https://github.com/elasticsearch/elasticsearch/issues/2618. I hope both _search and _msearch would fully support source parameter with POST as well as GET as sometimes queries get too big for GET URLs
</comment><comment author="javanna" created="2014-01-28T15:09:38Z" id="33486373">Addressed in #4901, added also support for the `source` parameter to all the APIs where it makes sense.
</comment><comment author="roytmana" created="2014-01-28T15:16:23Z" id="33487161">Hi @javanna, 

Is source parameter is now supported with POST (form-urlencoded parameter) ?

Alex
</comment><comment author="javanna" created="2014-01-28T15:17:42Z" id="33487360">Hi @roytmana, 
no, that's another issue (#2618), which is still open ;)
</comment><comment author="roytmana" created="2014-01-28T15:19:50Z" id="33487563">oh well, got to wait a bit more :-)

On Tue, Jan 28, 2014 at 10:18 AM, Luca Cavanna notifications@github.comwrote:

&gt; Hi @roytmana https://github.com/roytmana,
&gt; no, that's another issue (#2618https://github.com/elasticsearch/elasticsearch/issues/2618),
&gt; which is still open ;)
&gt; 
&gt; ## 
&gt; 
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/pull/2901#issuecomment-33487360
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make the has_child and has_parent filter support caching</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2900</link><project id="" key="" /><description>Add support for the `_cache` and `_cache_key` options to the `has_child` and `has_parent` filters.
</description><key id="13243976">2900</key><summary>Make the has_child and has_parent filter support caching</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-16T12:42:03Z</created><updated>2013-04-16T12:42:54Z</updated><resolved>2013-04-16T12:42:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>PolygonBuilder does not support holes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2899</link><project id="" key="" /><description>The `PolygonBuilder` does not support holes within a polygon. Since holes are supported by the GeoJson this function should be implemented in `PolygonBuilders` as well.
</description><key id="13236543">2899</key><summary>PolygonBuilder does not support holes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">chilling</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0</label></labels><created>2013-04-16T08:48:09Z</created><updated>2013-04-16T09:23:04Z</updated><resolved>2013-04-16T09:23:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index fails with MapperParsingException when dynamic_templates provided</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2898</link><project id="" key="" /><description>I'm not sure if the title of this is correct, because I'm not sure what the cause of this is, but here's what I'm seeing:

The following sequence of commands will fail:

```
curl -XDELETE "http://localhost:9200/test/"

curl -XPUT "http://localhost:9200/test/"

curl -XPUT http://localhost:9200/test/test/_mapping -d '
{
  "test": {
    "dynamic_templates": [
      {
        "all_fields": {
          "match": "*",
          "mapping": {
              "type": "string"
          }
        }
      }
    ],  
    "properties": {
      "meta": {
        "type": "object",
        "properties": {
          "timestamp": {
            "store": "yes",
            "type": "date"
          },
          "foo": {
            "type": "object",
            "properties": {
              "bar": {
                  "index": "not_analyzed",
                  "store": "yes",
                  "type": "string"
              }
            }
          }
        }
      }
    }
  }
}'


curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "meta": [{
    "timestamp": 1234567890,
    "foo": {
      "bar" : "Hello World"
    },
    "fruits" : {
      "canned": {
        "orchard" : ["Apples", "Oranges", "Peaches"]
      }
    }
  }]
}'
```

The failure is

```
{
    "error": "MapperParsingException[Failed to parse [meta.fruits]]; nested: ElasticSearchIllegalArgumentException[unknown property [canned]]; ",
    "status": 400
}
```

It's not clear from the documentation why this doesn't work, or how one would assign a default type to unexpected field values if this mapping approach is wrong.

The issue seems to have been introduced from this commit:

https://github.com/elasticsearch/elasticsearch/commit/6243f8e64d994ec1e0a45328cdc80f1ba1d87bb6
</description><key id="13225160">2898</key><summary>Index fails with MapperParsingException when dynamic_templates provided</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jasonpolites</reporter><labels /><created>2013-04-15T23:38:15Z</created><updated>2014-07-23T13:23:49Z</updated><resolved>2014-07-23T13:23:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jasonpolites" created="2013-04-16T16:58:37Z" id="16457176">FYI PUTting the document shown above with the "meta" element as a single object (not an array) works

```
curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "meta": {
    "timestamp": 1234567890,
    "foo": {
      "bar" : "Hello World"
    },
    "fruits" : {
      "canned": {
        "orchard" : ["Apples", "Oranges", "Peaches"]
      }
    }
  }
}'

{
    "ok": true,
    "_index": "test",
    "_type": "test",
    "_id": "1",
    "_version": 1
}
```
</comment><comment author="jasonpolites" created="2013-04-16T17:02:08Z" id="16457380">Strike that last comment.. the non-array variant works in 0.19.12 but still fails in 0.20.xx.

The original case fails in 0.19.12 also, but with a different error:

```
{
    "error": "MapperParsingException[object mapping for [test] with array for [meta] tried to parse as array, but got EOF, is there a mismatch in types for the same field?]",
    "status": 400
}
```
</comment><comment author="kimchy" created="2013-04-16T17:08:35Z" id="16457735">The reason this happens is that the `fruits` element ends up being forced to be a `string` type because of the dynamic template mapping that matches on `*`.... . 

Btw, no need to explicitly store fields, since the `_source` document is always stored.
</comment><comment author="jasonpolites" created="2013-04-16T17:18:50Z" id="16458357">OK thanks.  The "store" option is there because I copy/pasted from the "real" mapping which has source storage disabled.

I won't pollute the issue tracker with "forum" style questions, but perhaps you could advise as to how I create a mapping that defaults any unknown value to string.  I run into problems with auto-inferring types because my documents are not consistent (outside my control).

I have created a forum thread here:

https://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/Jason$20MapperParsingException/elasticsearch/kxBC3xZfeh0/-pirsB1eVjQJ

Thanks!
</comment><comment author="kimchy" created="2013-04-16T17:21:43Z" id="16458519">you can set in the root object mapping `date_detection` to `false`, and `numeric_detection` set to `false`, and it won't try and auto detect dates/numerics. That should work out I think.
</comment><comment author="jasonpolites" created="2013-04-16T17:35:36Z" id="16459405">oh awesome.. thanks
</comment><comment author="jasonpolites" created="2013-04-16T17:41:22Z" id="16459776">No dice :(  the following fails:

```
curl -XDELETE "http://localhost:9200/test/"

curl -XPUT "http://localhost:9200/test/"

curl -XPUT http://localhost:9200/test/test/_mapping -d '
{
  "test": {
    "date_detection" : false,
    "numeric_detection": false,
    "properties": {
      "meta": {
        "type": "object",
        "properties": {
          "timestamp": {
            "store": "yes",
            "type": "date"
          },
          "foo": {
            "type": "object",
            "properties": {
              "bar": {
                  "index": "not_analyzed",
                  "type": "string"
              }
            }
          }
        }
      }
    }
  }
}'

curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "meta": [{
    "timestamp": 1234567890,
    "test": 123,
    "foo": {
      "bar" : "Hello World"
    },
    "fruits" : {
      "canned": {
        "orchard" : ["Apples", "Oranges", "Peaches"]
      }
    }
  }]
}'

curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "meta": [{
    "timestamp": 1234567890,
    "test": "abc",
    "foo": {
      "bar" : "Hello World"
    },
    "fruits" : {
      "canned": {
        "orchard" : ["Apples", "Oranges", "Peaches"]
      }
    }
  }]
}'    
```
</comment><comment author="jasonpolites" created="2013-04-16T18:50:55Z" id="16464255">Also setting specific types on the dynamic_mapping doesn't seem to work for me:

```
curl -XDELETE "http://localhost:9200/test/"

curl -XPUT "http://localhost:9200/test/"

curl -XPUT http://localhost:9200/test/test/_mapping -d '
{
  "test": {
    "date_detection" : false,
    "numeric_detection": false,
    "dynamic_templates": [
      {
        "all_fields": {
          "match": "*",
          "match_mapping_type" : "integer",
          "mapping": {
              "type": "string"
          }
        }
      }
    ]
  }
}'

curl -XPUT http://localhost:9200/test/test/1 -d '
{
  "test": 123,
}'

curl -XPUT http://localhost:9200/test/test/2 -d '
{
  "test": "abc",
}'    
```

Results in:

```
java.lang.NumberFormatException: For input string: "abc"
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
    at java.lang.Long.parseLong(Long.java:410)
    at java.lang.Long.parseLong(Long.java:468)
    at org.elasticsearch.common.xcontent.support.AbstractXContentParser.longValue(AbstractXContentParser.java:72)
    at org.elasticsearch.index.mapper.core.LongFieldMapper.innerParseCreateField(LongFieldMapper.java:284)
    at org.elasticsearch.index.mapper.core.NumberFieldMapper.parseCreateField(NumberFieldMapper.java:182)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:307)
    ... 11 more
```
</comment><comment author="jasonpolites" created="2013-04-16T19:06:20Z" id="16465173">This last comment may be a separate issue, I have opened a new issue to isolate this case:

https://github.com/elasticsearch/elasticsearch/issues/2904

The original problem remains however.  That is, how to create a mapping that deals with variable types in documents.  More specifically, "default to string where not mapped".  This does not seem to be possible.
</comment><comment author="jasonpolites" created="2013-04-17T16:06:09Z" id="16515162">Any workarounds for this?  The only solution outside of  ES I can see is to manually traverse each document switching non-string values to strings.. which is possible but a little painful.
</comment><comment author="clintongormley" created="2014-07-23T13:23:49Z" id="49872378">@jasonpolites the original issue seems to have been fixed already.

the issue in your last example works if you use this instead:

```
 "match_mapping_type" : "integer",
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>XContentBuilder.rawField produces invalid JSON</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2897</link><project id="" key="" /><description>I often use the `XContentBuilder.rawField` method to append my own JSON objects or arrays to the builder. Nonetheless, I found something quite annoying: a comma is always prepend when adding a new raw field, whatever it is the first field or not. The JsonXContentGenerator.writeRawField methods [1] seems to confirm that.

This leads to malformed JSONs, for instance:

``` java
    @Test
    public void rawFieldBug() throws IOException {
        XContentBuilder factory = XContentFactory.jsonBuilder().startObject()
                .rawField("myfield", "{}".getBytes())
                .endObject();
        String json = factory.string();
        Assert.assertEquals("{\"myfield\":{}}", json);
        // returns &gt;{, "myfield" : {}}&lt; instead
    }
```

This bug was seen in the 0.90.RC2 elasticsearch version.

PS: In addition, a `rawField(String fieldName, String content)` (with potentially validation) might be very nice for such use cases :)

[1] Extracted form `org.elasticsearch.common.xcontent.json.JsonXContentGenerator`

``` java
    @Override
    public void writeRawField(String fieldName, byte[] content, OutputStream bos) throws IOException {
        generator.writeRaw(", \""); // &lt;= here?!
        generator.writeRaw(fieldName);
        generator.writeRaw("\" : ");
        flush();
        bos.write(content);
    }
```
</description><key id="13205939">2897</key><summary>XContentBuilder.rawField produces invalid JSON</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ncolomer</reporter><labels /><created>2013-04-15T15:45:15Z</created><updated>2014-03-24T20:29:46Z</updated><resolved>2014-03-24T17:12:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-04-15T17:05:31Z" id="16397923">we can't tell if its the first one or not, in the context of elasticsearch, we never use it for the first field, and adding knowledge on at what stage it is is quite problematic.
</comment><comment author="ncolomer" created="2013-04-15T19:23:35Z" id="16405973">Hi Shay,

Thanks for your answer. 
Anyway, we can deal with it using this (well, dirty but) simple workaround on the final JSON String:

``` java
jsonString.replaceAll("\\{,", "\\{");
```

And, if it's really annoying, just build these JSONs in another way, with Jackson for instance :)
</comment><comment author="kimchy" created="2013-04-15T19:40:16Z" id="16406810">we use jackson internally, and we hack it ourself on top of it, we do support "copyCurrentStrucutre" aspect though, but it does require parsing the json you append
</comment><comment author="msasa" created="2013-08-16T19:23:28Z" id="22788073">I can acknowledge this bug for ES v0.90.3. When rawField is first/only field in a XContentBuilder instance's object than it make problems. I have run on it when I was using elasticsearch-mapper-attachments and adding an attachment as stream to elasticsearch. 
I had to place one ordinary field(String,String) before rawField(String, InputStream) to make it work.
</comment><comment author="rore" created="2014-03-16T12:08:24Z" id="37755434">+1 on this bug.
</comment><comment author="s1monw" created="2014-03-24T17:12:12Z" id="38472040">fixed by #5514 
</comment><comment author="ncolomer" created="2014-03-24T20:23:59Z" id="38495910">@s1monw thanks for the fix ;)
</comment><comment author="kimchy" created="2014-03-24T20:29:46Z" id="38496546">FYI, new version of Jackson supports using the writeField now in the context of writing raw field, so the state management is done in Jackson (where it was already done before)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException in count and search with preference set to _primary</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2896</link><project id="" key="" /><description>version 0.90 RC2

steps to reproduce:

start 2 nodes
create an index with shards on both nodes
stop 1 node, so that the index state is red

issue a _count or a _search request like this:

```
curl -XGET $idx_url'/_count?pretty=1&amp;preference=_primary'

curl -XGET $idx_url'/_search?pretty=1&amp;preference=_primary' -d '{
"query":{
"match_all":{}
 }
 }
'
```

the result for both is 

```
{
  "error" : "NullPointerException[null]",
  "status" : 500
}
```
</description><key id="13203944">2896</key><summary>NullPointerException in count and search with preference set to _primary</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dobe</reporter><labels><label>bug</label><label>v0.20.7</label><label>v0.90.0</label></labels><created>2013-04-15T15:03:39Z</created><updated>2013-04-17T04:47:00Z</updated><resolved>2013-04-16T15:20:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-16T07:36:04Z" id="16430486">hey,

can you provide a full recreation of this failure. I tried to reproduce it in a testcase but never see the NPE 

simon
</comment><comment author="s1monw" created="2013-04-16T09:21:56Z" id="16434637">hmm this preproduces with the REST api but not with the internal API... I will dig
</comment><comment author="dobe" created="2013-04-16T09:38:16Z" id="16435306">hi,
i just figured out that the nullpointer happens in https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/support/broadcast/TransportBroadcastOperationAction.java

line 196::

```
 if (shard.currentNodeId().equals(nodes.localNodeId())) {
```

currentNodeId of shard is null in this case because the node is not there
i am not sure if the shard should not be in the iterator at this point or if it is ok to just check if currentnodeid is null
</comment><comment author="s1monw" created="2013-04-16T13:28:57Z" id="16444461">yeah that's right. I am trying to figure out if this is a bigger problem or not...
</comment><comment author="s1monw" created="2013-04-16T14:12:07Z" id="16446795">I just tested this with 0.20 branch and it has the same issue while only for `_search` since `_count` doesn't respect the preference setting here?
</comment><comment author="s1monw" created="2013-04-16T19:22:07Z" id="16466056">thanks @dobe for reporting this. I'd be curious how you ran into this, didn't this happen on 0.20 as well?
</comment><comment author="dobe" created="2013-04-17T04:47:00Z" id="16486641">thanks for fixing it. we've set "_primary" as the default in a new plugin we are writing (targeting 0.90). so i was hunting the bug in our plugin code for two days until i found out it was happening on _count and _search too :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add UNICODE_CHARACTER_CLASS flag to Regex flag parsing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2895</link><project id="" key="" /><description>Hi,

Java's Regex have issue with character classes when it comes to unicode content ( 
http://stackoverflow.com/questions/4304928/unicode-equivalents-for-w-and-b-in-java-regular-expressions
)

Apparently they solved things in Java7 by adding a special flag: UNICODE_CHARACTER_CLASS ( http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#UNICODE_CHARACTER_CLASS )

Sadly org.elasticsearch.common.regex doesn't "understand" it. As Java7 is the recommended runtime for ES I think it's OK to add it even if not supported by earlier JDK.

Cheers,
Boaz
</description><key id="13203121">2895</key><summary>Add UNICODE_CHARACTER_CLASS flag to Regex flag parsing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bleskes</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-15T14:47:38Z</created><updated>2013-04-16T08:20:53Z</updated><resolved>2013-04-16T08:20:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-16T08:12:15Z" id="16431744">yeah makes sense... I will add it in a bit
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update API list update does not work as expected</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2894</link><project id="" key="" /><description>Hi

there seems to be a glitch in the update API (using RC2) - or the MVEL engine

this works

```
curl -XPUT localhost:9200/test/type1/1 -d '{
    "counter" : 1,
    "tags" : ["red"]
}'
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "!ctx._source.tags.contains(tag) ? ctx._source.tags += tag : ctx.op = \"none\"",
    "params" : {
        "tag" : "blue"
    }
}'
curl localhost:9200/test/type1/1
{"_index":"test","_type":"type1","_id":"1","_version":2,"exists":true, "_source" : {"counter":1,"tags":["red","blue"]}}
```

When changing the checking order of the script it does not work (this is also written on the main documentation)

```
curl -XPUT localhost:9200/test/type1/1 -d '{
    "counter" : 1,
    "tags" : ["red"]
}'
curl -XPOST 'localhost:9200/test/type1/1/_update' -d '{
    "script" : "ctx._source.tags.contains(tag) ? ctx.op = \"none\" : ctx._source.tags += tag ",
    "params" : {
        "tag" : "blue"
    }
}'
curl localhost:9200/test/type1/1
{"_index":"test","_type":"type1","_id":"1","_version":2,"exists":true, "_source" : {"counter":1,"tags":["red"]}}
```
</description><key id="13197524">2894</key><summary>Update API list update does not work as expected</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-15T12:38:39Z</created><updated>2013-04-19T07:38:34Z</updated><resolved>2013-04-19T07:38:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-17T18:25:25Z" id="16523988">this boils down to an MVEL problem, here is a unit test for MVEL which fails reliably. Possibly this is expected behaviour in MVEL, so I need to check that out

```
    @Test
    public void mveltest() throws Exception {
        Map&lt;String, Object&gt; vars = Maps.newHashMap();
        vars.put("x", 1);

        Object compiled = MVEL.compileExpression("x == 11 ? x = 10 : x += 5");
        MVEL.executeExpression(compiled, vars);
        assertThat(vars.get("x").toString(), is("6"));
    }
```

If you use `x != 11 ? x += 5 : x = 10` as expression, everything is working... the `else` part seems to cut off somehow.
</comment><comment author="spinscale" created="2013-04-19T07:38:34Z" id="16639864">The problem is blatantly obvious, that I overlooked it. It is a simple precedence problem with the ternary operator. Solution is using more brackets:

```
( x == 11 ) ? (x = 10): (x += 5)
```

I will update the documentation today to reflect and explain this, so people dont get confused.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better error message when using bloom codec</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2893</link><project id="" key="" /><description>If the `postings_format` is specified as just `bloom`, then it throws a class-not-found exception. Instead, would be better to have a message like "The bloom codec cannot be used by itself. Use bloom_default or bloom_pulsing instead"
</description><key id="13170486">2893</key><summary>Better error message when using bloom codec</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-14T10:33:25Z</created><updated>2013-04-16T15:52:39Z</updated><resolved>2013-04-16T15:52:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-16T15:22:10Z" id="16450975">@martijnvg can we close this?
</comment><comment author="martijnvg" created="2013-04-16T15:52:39Z" id="16453018">Yes, we can close it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Inconsistent locations returned by _source field, doc['_source'] script field and doc['location'].lats and doc['location'].lons script fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2892</link><project id="" key="" /><description>Here's a gist that demonstrates the issue (tested in v0.19.10 and v0.20.6):

https://gist.github.com/richardpoole/5379348

It's almost like scripted fields are coming from different documents. Changing the size parameter of the search request seems to have an effect (e.g. problem disappears with size=1).
</description><key id="13160801">2892</key><summary>Inconsistent locations returned by _source field, doc['_source'] script field and doc['location'].lats and doc['location'].lons script fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">richardpoole</reporter><labels><label>regression</label><label>v0.90.0</label></labels><created>2013-04-13T18:35:07Z</created><updated>2013-04-16T08:22:52Z</updated><resolved>2013-04-16T08:22:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-04-14T01:26:29Z" id="16344162">Indeed there is inconsistency between fetching the source directly (i.e `"fields":["_source"]`) or by using a script (i.e `"script_fields" : { "source":{"script":"_source"} }`). This is due to internal optimizations we're doing which relate to script execution. We can fix it for future versions, but in any case the idea here is that if you need the full source, you shouldn't fetch it using a script.
</comment><comment author="richardpoole" created="2013-04-14T02:59:09Z" id="16345054">Hi Uri, thanks for looking into this. I think the problem runs deeper than getting the full source from a script though. It seems to affect the retrieval of individual fields too. For example, the second search result from the above gist looks like this:

`{
  _index: "test",
  _type: "test",
  _id: "B350EA03-8068-4B10-9A5C-53C54B68442B",
  _score: 1,
  _source: {
    location: [
      [-115.205377, 36.110736],
      [-115.076319, 36.037743]
    ]
  },
  fields: {
    source: {
      location: [
        [-122.030663, 37.332706]
      ]
    },
    lats: [32.715327, 41.595055],
    lons: [-117.15742499999999, -83.562434]
  }
}`

Query:

`{
  "fields": [
    "_source"
  ],
  "script_fields": {
    "lats": {
      "script": "doc['location'].lats"
    },
    "lons": {
      "script": "doc['location'].lons"
    },
    "source": {
      "script": "_source"
    }
  }
}`

The values returned by `"script": "doc['location'].lats"` and `"script": "doc['location'].lons"` are also inconsistent. Could that be caused by the same internal optimization? Supposing we can fix it, would the core team be interested in a PR?
</comment><comment author="peterwillis" created="2013-04-15T18:02:53Z" id="16401279">Access to lats and lons is fixed by eb21526552ad42988350245716b5cd6a6cdd97dd

Thank you uboness
</comment><comment author="s1monw" created="2013-04-16T08:22:32Z" id="16432165">closed via https://github.com/elasticsearch/elasticsearch/commit/eb21526552ad42988350245716b5cd6a6cdd97dd
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Implementation to support geo circles (closes #2713)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2891</link><project id="" key="" /><description>This one is based on spatial4j in order to prevent the creation of an own geo_circle type.

The GeoJSON standard does not support circles. A proposal for this has been rejected:
https://github.com/GeoJSONWG/geojson-spec/wiki/Proposal---Circles-and-Ellipses-Geoms

This implementation extends the geo_shape parsers and serializers to support circles.

It works like this:

```
curl -X DELETE localhost:9200/geotest
curl -X PUT localhost:9200/geotest
curl -X PUT localhost:9200/geotest/geotest/_mapping -d '{ "geotest": { "properties":  { "location" : { "type":"geo_shape" } } } }'

curl -X PUT localhost:9200/geotest/geotest/parkhotel?refresh=true -d '{ "name":"Park Hotel Amsterdam", "location" : { "type" : "point", "coordinates" : [4.883208, 52.362105] } }'
curl -X PUT localhost:9200/geotest/geotest/schiphol?refresh=true -d '{ "name":"Schiphol Airport", "location" : { "type" : "point", "coordinates" : [4.77253, 52.313096] } }'

echo "Querying with a radius of 10km (excluding the airport)"
curl -X POST localhost:9200/geotest/geotest/_search -d '{
  "from":"0",
  "size":"10",
  "query" : {
    "filtered" : {
      "query": { "match_all" : {} },
      "filter" : {
        "geo_shape" : {
          "location" : {
            "shape" : {
              "type" : "circle",
              "coordinates" : [ 4.895439, 52.37125 ],
              "radius": "10"
            }
          }
        }
      }
    }
  }
}'

echo "Querying with a radius of 15km (including the airport)"
curl -X POST localhost:9200/geotest/geotest/_search -d '{
  "from":"0",
  "size":"10",
  "query" : {
    "filtered" : {
      "query": { "match_all" : {} },
      "filter" : {
        "geo_shape" : {
          "location" : {
            "shape" : {
              "type" : "circle",
              "coordinates" : [ 4.895439, 52.37125 ],
              "radius": "15"
            }
          }
        }
      }
    }
  }
}'
```

TODO:
- Add real corner case tests
</description><key id="13151026">2891</key><summary>Implementation to support geo circles (closes #2713)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-13T08:08:40Z</created><updated>2014-06-12T13:34:55Z</updated><resolved>2013-07-03T12:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-03T12:06:23Z" id="20410997">Finally landed in elasticsearch with https://github.com/elasticsearch/elasticsearch/commit/0c2d12bda34fbbe0ffd3cef5140f5d5a3919b080
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clear Cache API: Streamline option names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2890</link><project id="" key="" /><description>Spin off #2888, just with the streamline option. Move `field_data` to `fielddata`, `filter` to `filter_cache`, and `id` to `id_cache`, while still supporting old options for backward comp.
</description><key id="13143104">2890</key><summary>Clear Cache API: Streamline option names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-12T22:57:18Z</created><updated>2013-04-12T22:58:29Z</updated><resolved>2013-04-12T22:58:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose field level field data statistics</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2889</link><project id="" key="" /><description>Both the node stats and indices stats APIs to allow for `fields` parameter and relevant endpoints to allow for selectively report on specific field stats. for example:

```
# Node Stats
curl localhost:9200/_nodes/stats/indices/fielddata/field1,field2?pretty
curl localhost:9200/_nodes/stats/indices/fielddata/field*?pretty

# Indices Stat
curl localhost:9200/_stats/fielddata/field1,field2?pretty
curl localhost:9200/_stats/fielddata/field*?pretty
```
</description><key id="13142918">2889</key><summary>Expose field level field data statistics</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.90.0</label></labels><created>2013-04-12T22:50:47Z</created><updated>2013-06-13T17:05:58Z</updated><resolved>2013-04-12T22:51:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>admin indices clearcache interface improvements</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2888</link><project id="" key="" /><description>Clearing fielddata should be done with "fielddata" and not "field_data" thus bringing it in line with issue 2727.

Also clear cache interface should report how much it clears.  
</description><key id="13140955">2888</key><summary>admin indices clearcache interface improvements</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">maphysics</reporter><labels /><created>2013-04-12T21:48:25Z</created><updated>2014-08-08T11:04:26Z</updated><resolved>2014-08-08T11:04:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Missing FilterBuilder.toString()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2887</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/reference/java-api/query-dsl-filters/ states

```
Note that you can easily print (aka debug) JSON generated queries
using toString() method on FilterBuilder object.
```

This is !true! Tried this for debugging and got Object.toString() output.
QueryBuilder.toString() instead works as advertised so the toString() from BaseQueryBuilder.java might just be pasted into BaseFilterBuilder.java.
</description><key id="13129742">2887</key><summary>Missing FilterBuilder.toString()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">gittaa</reporter><labels><label>enhancement</label><label>v0.90.0</label></labels><created>2013-04-12T17:05:22Z</created><updated>2013-04-12T20:28:40Z</updated><resolved>2013-04-12T20:28:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>The clear_cache API doesn't clear fielddata cache by default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2886</link><project id="" key="" /><description>In 0.90.RC3, a call to clear the cache without specifying any options doesn't clear the fielddata cache, but it does in v0.20.6:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "settings" : {
      "number_of_shards" : 1
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test/_bulk?pretty=1'  -d '
{"index" : {}}
{"num" : 1}
{"index" : {}}
{"num" : 2}
{"index" : {}}
{"num" : 3}
{"index" : {}}
{"num" : 4}
{"index" : {}}
{"num" : 5}
{"index" : {}}
{"num" : 6}
{"index" : {}}
{"num" : 7}
{"index" : {}}
{"num" : 8}
{"index" : {}}
{"num" : 9}
{"index" : {}}
{"num" : 10}
{"index" : {}}
{"num" : 11}
{"index" : {}}
{"num" : 12}
{"index" : {}}
{"num" : 13}
{"index" : {}}
{"num" : 14}
{"index" : {}}
{"num" : 15}
{"index" : {}}
{"num" : 16}
{"index" : {}}
{"num" : 17}
{"index" : {}}
{"num" : 18}
{"index" : {}}
{"num" : 19}
{"index" : {}}
{"num" : 20}
{"index" : {}}
{"num" : 21}
{"index" : {}}
{"num" : 22}
{"index" : {}}
{"num" : 23}
{"index" : {}}
{"num" : 24}
{"index" : {}}
{"num" : 25}
{"index" : {}}
{"num" : 26}
{"index" : {}}
{"num" : 27}
{"index" : {}}
{"num" : 28}
{"index" : {}}
{"num" : 29}
{"index" : {}}
{"num" : 30}
{"index" : {}}
{"num" : 31}
{"index" : {}}
{"num" : 32}
{"index" : {}}
{"num" : 33}
{"index" : {}}
{"num" : 34}
{"index" : {}}
{"num" : 35}
{"index" : {}}
{"num" : 36}
{"index" : {}}
{"num" : 37}
{"index" : {}}
{"num" : 38}
{"index" : {}}
{"num" : 39}
{"index" : {}}
{"num" : 40}
{"index" : {}}
{"num" : 41}
{"index" : {}}
{"num" : 42}
{"index" : {}}
{"num" : 43}
{"index" : {}}
{"num" : 44}
{"index" : {}}
{"num" : 45}
{"index" : {}}
{"num" : 46}
{"index" : {}}
{"num" : 47}
{"index" : {}}
{"num" : 48}
{"index" : {}}
{"num" : 49}
{"index" : {}}
{"num" : 50}
{"index" : {}}
{"num" : 51}
{"index" : {}}
{"num" : 52}
{"index" : {}}
{"num" : 53}
{"index" : {}}
{"num" : 54}
{"index" : {}}
{"num" : 55}
{"index" : {}}
{"num" : 56}
{"index" : {}}
{"num" : 57}
{"index" : {}}
{"num" : 58}
{"index" : {}}
{"num" : 59}
{"index" : {}}
{"num" : 60}
{"index" : {}}
{"num" : 61}
{"index" : {}}
{"num" : 62}
{"index" : {}}
{"num" : 63}
{"index" : {}}
{"num" : 64}
{"index" : {}}
{"num" : 65}
{"index" : {}}
{"num" : 66}
{"index" : {}}
{"num" : 67}
{"index" : {}}
{"num" : 68}
{"index" : {}}
{"num" : 69}
{"index" : {}}
{"num" : 70}
{"index" : {}}
{"num" : 71}
{"index" : {}}
{"num" : 72}
{"index" : {}}
{"num" : 73}
{"index" : {}}
{"num" : 74}
{"index" : {}}
{"num" : 75}
{"index" : {}}
{"num" : 76}
{"index" : {}}
{"num" : 77}
{"index" : {}}
{"num" : 78}
{"index" : {}}
{"num" : 79}
{"index" : {}}
{"num" : 80}
{"index" : {}}
{"num" : 81}
{"index" : {}}
{"num" : 82}
{"index" : {}}
{"num" : 83}
{"index" : {}}
{"num" : 84}
{"index" : {}}
{"num" : 85}
{"index" : {}}
{"num" : 86}
{"index" : {}}
{"num" : 87}
{"index" : {}}
{"num" : 88}
{"index" : {}}
{"num" : 89}
{"index" : {}}
{"num" : 90}
{"index" : {}}
{"num" : 91}
{"index" : {}}
{"num" : 92}
{"index" : {}}
{"num" : 93}
{"index" : {}}
{"num" : 94}
{"index" : {}}
{"num" : 95}
{"index" : {}}
{"num" : 96}
{"index" : {}}
{"num" : 97}
{"index" : {}}
{"num" : 98}
{"index" : {}}
{"num" : 99}
{"index" : {}}
{"num" : 100}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "facets" : {
      "foo" : {
         "terms" : {
            "field" : "num"
         }
      }
   },
   "size" : 0
}
'
```

In 0.90.RC3, a clear cache request without params:

```
curl -XPOST 'http://127.0.0.1:9200/_cache/clear?pretty=1' 
```

The fielddata is not cleared:

```
curl -XGET 'http://127.0.0.1:9200/_cluster/nodes/stats?pretty=1&amp;clear=true&amp;indices=true' 

#             "fielddata" : {
#                "evictions" : 0,
#                "memory_size_in_bytes" : 120,
#                "memory_size" : "120b"
#             },
```

It is cleared when specifying `field_data`:

```
curl -XPOST 'http://127.0.0.1:9200/_cache/clear?field_data=true&amp;pretty=1' 

curl -XGET 'http://127.0.0.1:9200/_cluster/nodes/stats?pretty=1&amp;clear=true&amp;indices=true' 

#             "fielddata" : {
#                "evictions" : 0,
#                "memory_size_in_bytes" : 0,
#                "memory_size" : "0b"
#             },
```

In 0.20.6, it is cleared even though `field_data` is not specified:

```
#             "cache" : {
#                "id_cache_size_in_bytes" : 0,
#                "field_evictions" : 0,
#                "field_size_in_bytes" : 0,
#                "id_cache_size" : "0b",
#                "field_size" : "0b",
#                "filter_count" : 0,
#                "filter_size" : "40b",
#                "bloom_size" : "0b",
#                "bloom_size_in_bytes" : 0,
#                "filter_size_in_bytes" : 40,
#                "filter_evictions" : 0
#             }
```

The docs http://www.elasticsearch.org/guide/reference/api/admin-indices-clearcache/ say  "The clear cache API allows to clear either all caches..."
</description><key id="13115343">2886</key><summary>The clear_cache API doesn't clear fielddata cache by default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>regression</label><label>v0.90.0</label></labels><created>2013-04-12T11:43:08Z</created><updated>2013-04-12T13:24:58Z</updated><resolved>2013-04-12T13:24:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>GeoFilterTests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2885</link><project id="" key="" /><description>- Added GeoFilterTests
- Added MultipolygonBuilder
- Added functionality to create polygons with hole
- Activated ShapeRelations (WithIn, Disjoint)
</description><key id="13070198">2885</key><summary>GeoFilterTests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-04-11T14:33:33Z</created><updated>2014-07-16T21:53:42Z</updated><resolved>2013-04-16T19:52:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-16T19:52:11Z" id="16467672">this has been merged with some small modifications
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>401 Unauthorized - issues in implementing elasticsearch-jetty plugin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2884</link><project id="" key="" /><description>Hi All, 

My Requirement: Secure authentication on Elasticsearch

Work Done: 

On Ubuntu 12.04.1 LTS 
following items installed and configured:
Jetty Plugin    0.20.1
Elasticsearch   0.20.2
Jetty           8.1.4.v20120524 
as per direction given at https://github.com/sonian/elasticsearch-jetty
[I HAVE TRIED LATEST AND EVEN OLDER Combinations of these 3 items as well]

All installation was successful and I was able to restrict XPUT/XDELETE operation with authentication, but EVEN WHEN I AM PROVIDING correct USERNAME:PASSWORD as below: 
curl -u admin:Passw0rd -X PUT 'http://localhost:9200/index'
it gives error:
401 Unauthorized .....

Please help if I miss something.

Thanks in advance,
sunilb

---

Following are the detail of steps done by me:
## 1st Step

Ubuntu 12.04 &#8211; Install Jetty 7

This article will show you how to install Jetty 7 on Ubuntu 12.04. Perform all steps as root.

Jetty requires Java. Install Java first, I prefer openjdk instead of oracle jdk, but it should work both.

apt-get install openjdk-7-jdk

Create a symlink for easier reference from jetty

mkdir /usr/java
ln -s /usr/lib/jvm/java-7-openjdk-amd64 /usr/java/default

Go to the opt directory

cd /opt

Download the latest Jetty distribution (7.x).

wget "http://eclipse.org/downloads/download.php?file=/jetty/stable-7/dist/jetty-distribution-8.1.4.v20120524.tar.gz&amp;r=1"
mv download.php\?file\=%2Fjetty%2Fstable-7%2Fdist%2Fjetty-distribution-8.1.4.v20120524.tar.gz&amp;r\=1  jetty-distribution-8.1.4.v20120524.tar.gz

Unpack the archive

tar -xvf jetty-distribution-8.1.4.v20120524.tar.gz
mv jetty-distribution-8.1.4.v20120524 jetty

Create jetty user and make it the owner of /opt/jetty

useradd jetty -U -s /bin/false
chown -R jetty:jetty /opt/jetty

Copy Jetty script to run as service

cp /opt/jetty/bin/jetty.sh /etc/init.d/jetty

Create the settings file /etc/default/jetty

nano /etc/default/jetty

and paste the following in it

JAVA_HOME=/usr/java/default # Path to Java
NO_START=0 # Start on boot
JETTY_HOST=0.0.0.0 # Listen to all hosts
JETTY_PORT=8085 # Run on this port
JETTY_USER=jetty # Run as this user

Start Jetty as service

service jetty start

It should now be running on port 8085! Visit in your browser http://example.com:8085 and you should see a Jetty screen.
Remove the unsecure test and async-rest application:

cd /opt/jetty/webapps
rm -rf test.d/ test.war test.xml async-rest.war
## 2nd STEP

INSTALL ELASTIC SEARCH
wget https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.20.6.deb
sudo dpkg -i elasticsearch-0.20.6.deb
sudo service elasticsearch start
## 3rd Step

Install curl on linux machine
apt-get install curl
## 4th step

Install https://github.com/sonian/elasticsearch-jetty as per given instructions
</description><key id="13063573">2884</key><summary>401 Unauthorized - issues in implementing elasticsearch-jetty plugin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sunilb128</reporter><labels /><created>2013-04-11T11:46:01Z</created><updated>2013-04-11T12:15:59Z</updated><resolved>2013-04-11T12:15:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-04-11T12:06:14Z" id="16230437">Could you move this issue to https://github.com/sonian/elasticsearch-jetty/issues and we will address it there?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Rename Suggester prefix_length to prefix_len for consistency with other params</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2883</link><project id="" key="" /><description>we already have `min_word_len` and other params like `min_doc_freq` that use short forms. We should be consistent here and use `prefix_len` rather than `prefix_length`
</description><key id="13025139">2883</key><summary>Rename Suggester prefix_length to prefix_len for consistency with other params</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>v0.90.0</label></labels><created>2013-04-10T15:38:53Z</created><updated>2013-04-10T15:43:58Z</updated><resolved>2013-04-10T15:43:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fielddata stats incorrect for multi-value fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2882</link><project id="" key="" /><description>I index 100,000 docs with two numeric fields, one value in each field:
- `unique` contains vals 1 to 100,000
- `repeated` contains vals 1 to 30

When sorting on `unique`, the fielddata size is reported as 800,020 bytes. When sorting on `repeated`, the fielddata size is reported as 100,020 bytes.

Then I index another document with `unique` set to the array [1..100] and `repeated` set to [1..30], and optimize down to one segment.

When sorting on `unique`, the fielddata size is reported as 400,032 bytes. When sorting on `repeated`, the fielddata size is reported as 59 bytes!

See this gist for a recreation: https://gist.github.com/clintongormley/5353777
</description><key id="13014026">2882</key><summary>Fielddata stats incorrect for multi-value fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.0</label></labels><created>2013-04-10T11:16:20Z</created><updated>2013-04-10T13:33:13Z</updated><resolved>2013-04-10T12:50:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Conflict when extending a field that uses index_name can cause document to silently fail to index.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2881</link><project id="" key="" /><description>Running 0.90.beta1

Here is the recreation:
https://gist.github.com/ppearcy/5349534

It looks like there can be some implicit conflicts around index_name and multi-fields. This is likely an edge case, but could cause some very funky behavior.

You will hit this if you have index_name set and then upgrade to a multi-field where that index_name conflicts with the name generated by the multi-field. 

I would expect the mapping update to fail and reject this as invalid. 

Let me know of any questions. 

Thanks!
</description><key id="12994421">2881</key><summary>Conflict when extending a field that uses index_name can cause document to silently fail to index.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppearcy</reporter><labels /><created>2013-04-09T21:57:13Z</created><updated>2014-08-08T10:55:52Z</updated><resolved>2014-08-08T10:55:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T10:55:52Z" id="51587865">`index_name` is deprecated. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>After deleting documents using _bulk, documents are still being returned in the search response. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2880</link><project id="" key="" /><description>After I delete documents using _bulk API, documents are still being returned in the search response, But if  I try to get the document using get API, it returns not found.

_bulk request =&gt; URL:  index/type/_bulk?refresh=true
                         Request: { "delete" : {"_id" : "2279"}}
                                       { "delete" : {"_id" : "2221"}}

response =&gt; 
{
"took": 7,
"items": [
{
"delete": {
"_index": "index",
"_type": "type",
"_id": "2279",
"_version": 1,
"ok": true
}
},
{
"delete": {
"_index": "index",
"_type": "type",
"_id": "2221",
"_version": 1,
"ok": true
}}]
}
</description><key id="12988180">2880</key><summary>After deleting documents using _bulk, documents are still being returned in the search response. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ytokas</reporter><labels /><created>2013-04-09T19:34:08Z</created><updated>2014-08-08T10:55:21Z</updated><resolved>2014-08-08T10:55:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-04-09T19:48:16Z" id="16135702">Can you create a curl recreation of this problem?
</comment><comment author="ytokas" created="2013-04-09T19:59:07Z" id="16136298">Below are the curl requests for the same, Thanks.

Bulk delete request:
$ curl -s -XPOST http://localhost:9200/index/type/_bulk?refresh=true -d { "delete" : {"_id" : "2279"}}
{ "delete" : {"_id" : "2221"}}

Search request 
$ curl -s -XPOST http://localhost:9200/index/type/_search -d {
  "track_scores": true,
  "fields": [ "_id"  ],
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "*",
          "default_operator": "OR",
          "analyze_wildcard": true
        }
     },
      "filter": {
        "and": [{
            "query": {
              "term": {
                "_id": "2279"
              }}}]
      }}
  },
  "size": 999,
  "from": 0
} 
</comment><comment author="spinscale" created="2013-04-10T08:26:43Z" id="16161431">Hey,

Is there really a newline in your deletes? Try writing the delete statement to a file to make sure.

```
{ "delete" : {"_id" : "1"} }
{ "delete" : {"_id" : "2"} }
```

Now run this curl call (make sure `--data-binary` is used)

```
curl -v -X POST 'localhost:9200/test/test/_bulk?refresh=true' --data-binary @/tmp/bulk-delete.json
```

This works for me, put perhaps we have a bug.

In case this still fails for you, can you tell us your elasticsearch version, so we can try to recreate your problem..
</comment><comment author="ytokas" created="2013-04-10T12:12:35Z" id="16170330">I forgot to mention earlier that I was using routing key to store the data and then was using the same routing key in the bulk delete call.

Issue with the call was that I was sending the routing key as query string parameter, where as if I sent it in each delete statement it works. 

earlier call that was not working:

```
    curl -s -X POST 'http://localhost:9200/test/test/_bulk?refresh=true&amp;routing=xyz' 
    -d {"delete" : {"_id" : "1"}}
```

Correct call:

```
    curl -s -X POST 'http://localhost:9200/test/test/_bulk?refresh=true' 
    -d {"delete" : {"_id" : "1", "_routing" : "xyz"}}
```

So this means that I have to send routing key with each command instead of just sending it once in the query string?

 And if so why is the get document throwing "404 Not Found", but returning the document in search calls. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Failed to start shard (data loss?) on 0.20.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2879</link><project id="" key="" /><description>Hello, we have daily indices with 3 shards \* 3 total copies each.  One of the indices can't be recovered - shard 1 gets allocated to one of the 3 nodes that holds it, and we see errors in the logs on that machine (shard should exist, but doesn't).  However, I do see a lot of data on 3 nodes for this shard - is there something I can do to get it back?  What other information can I use to help troubleshoot?

http://i.imgur.com/QXcbcvI.png

[2013-04-09 11:25:48,317][DEBUG][index.gateway            ] [search-01] [messages_20130405][1] starting recovery from local ...
[2013-04-09 11:25:48,318][WARN ][indices.cluster          ] [search-01] [messages_20130405][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [messages_20130405][1] shard allocated for local recovery (post api), should exists, but doesn't
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
</description><key id="12982390">2879</key><summary>Failed to start shard (data loss?) on 0.20.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">devoncrouse</reporter><labels /><created>2013-04-09T17:28:10Z</created><updated>2013-07-16T16:43:20Z</updated><resolved>2013-04-12T00:41:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="devoncrouse" created="2013-04-09T17:30:57Z" id="16127549">Also, on another node:

[2013-04-09 11:30:08,583][WARN ][cluster.action.shard     ] [search-02] sending failed shard for [messages_20130222][2], node[1EDq4kQjQqKkf2TPARuEdA], [P], s[STARTED], reason [master [search-m01][sHJuCwGzSqSFeDjyLo-Cmg][inet[/10.2.34.52:9300]]{data=false} marked shard as started, but shard have not been created, mark shard as failed]
[2013-04-09 11:30:08,583][WARN ][indices.cluster          ] [search-02] [messages_20130304][0] master [[search-m01][sHJuCwGzSqSFeDjyLo-Cmg][inet[/10.2.34.52:9300]]{data=false}] marked shard as started, but shard have not been created, mark shard as failed
</comment><comment author="imotov" created="2013-04-09T17:32:07Z" id="16127622">Did you upgrade or restart the cluster before it happend? 
</comment><comment author="devoncrouse" created="2013-04-09T17:33:26Z" id="16127717">Yes, this happened just after a cluster restart.  We've had the same issue during previous restarts as well (using the shutdown API, then starting back up with service wrapper).
</comment><comment author="imotov" created="2013-04-09T17:35:21Z" id="16127811">Could you post somewhere all log files since cluster restart from all nodes and your configuration? You can send me a link via IRC or email if you don't want to post them here.
</comment><comment author="imotov" created="2013-04-09T17:37:38Z" id="16127943">... and output of `curl localhost:9200/_nodes/hot_threads`.
</comment><comment author="devoncrouse" created="2013-04-09T17:41:05Z" id="16128143">Hot threads: http://sprunge.us/heHR
</comment><comment author="imotov" created="2013-04-09T17:44:05Z" id="16128319">Great! Could you run hot threads again, wait for a few minutes and run it again?
</comment><comment author="devoncrouse" created="2013-04-09T17:44:39Z" id="16128355">Master node config: http://sprunge.us/IfeP
Data node config: http://sprunge.us/Pida

Topology: 3 dedicated master nodes (VMs) running Zookeeper, 6 data nodes (physical)

We've seen the issue before implementing Zookeeper and Jetty plugins, too.
</comment><comment author="devoncrouse" created="2013-04-09T17:49:47Z" id="16128676">Hot threads t+0: http://sprunge.us/AihU
Hot threads t+3: http://sprunge.us/BaKK
</comment><comment author="devoncrouse" created="2013-04-09T17:52:20Z" id="16128818">Working on logs - we have a lot of them and this started a couple days ago :)
</comment><comment author="imotov" created="2013-04-09T17:53:31Z" id="16128891">Let's start with the logs from the day when it started. 
</comment><comment author="devoncrouse" created="2013-04-09T19:10:15Z" id="16133588">Sorry for the delay.  The failures start appearing at 07:35:10:

https://docs.google.com/file/d/0B21kqDCdcF2ONXNlcmxiVExiWTQ/edit?usp=sharing

Let me know if you want earlier/later logs.
</comment><comment author="devoncrouse" created="2013-04-09T19:54:58Z" id="16136071">Also, I do have ~250GB of data still in volumes across our data nodes, plus state files on our masters for this index.  Is there something I can do to analyze and validate the state files (i.e. stored location/hash of data files on each node, etc?)
</comment><comment author="imotov" created="2013-04-09T20:32:52Z" id="16138235">The most likely reason for this error is that your shard got corrupted. Unfortunately, files for this shard are not stored in a single directory, which complicates things. In cases like this, I typically suggest copying shard to another directory and running Lucene's CheckIndex on it and if it's successful, copying the shard back. In your case, you will have to collect all pieces and then bring them back, which might be quite cumbersome. There is build-in support for running CheckIndex in elasticsearch but I am not sure how well it can handle multi data directory setup - I never had a chance to try it on a multi data directory index. Moreover, it will run CheckIndex in place, which is risky. How difficult would it be to reindex data for this index?
</comment><comment author="devoncrouse" created="2013-04-09T20:41:46Z" id="16138758">It's kind of a pain to re-index.  My concern is that we had increased replicas to 2 after we ran into this previously, and they all appear to have been corrupted somehow?  Is there a way to confirm that they are all, indeed, corrupted?  And is it worth trying the CheckIndex feature described in https://github.com/elasticsearch/elasticsearch/pull/1890?  Worst case scenario, it doesn't work and we're still left with that one index unrecovered, correct?
</comment><comment author="imotov" created="2013-04-09T21:01:40Z" id="16139936">From the log file it looks like it tried to recover messages_20130405 only from search-00 and search-01. It never tried search-03 for some reason. So, in theory the shard on search-03 might be still OK. 

You mentioned that you had these types of issues before. Did you notice if they are related to having multiple data directories or not?
</comment><comment author="imotov" created="2013-04-09T22:11:39Z" id="16143643">Regarding checkIndex, I think it might make sense to do a dry run. Start a node with index.shard.check_on_startup set to "true" (don't set it to "fix" just yet). It will log the error message on startup. Please post these error message here and we will try to figure out if it's possible to restore the shard without data loss.
</comment><comment author="devoncrouse" created="2013-04-09T22:12:42Z" id="16143689">This is...weird:

The borked index was affecting one of our applications, so I decided to cut our losses and axe it.

curl -XDELETE 'http://localhost:9200/messages_20130405'
curl -XPUT 'http://localhost:9200/messages_20130405' # To create empty index so queries against the date wouldn't fail

I confirmed the new index was created, and empty in Head plugin.  Later, I restarted the cluster to make a heap adjustment (shutdown API, start with service wrapper).

The index is now back, with data.  I'm looking at the logs now to see if I can figure out how this worked.
</comment><comment author="devoncrouse" created="2013-04-09T22:25:56Z" id="16144273">It's also worth noting that the cluster had been restarted after the issue started, with no change in that index's recoverability.
</comment><comment author="devoncrouse" created="2013-04-09T22:32:46Z" id="16144610">The index's size is now lower, and I realize now that shard 1 and 2 are pretty much empty - only 0 has the data from before.  This is still better than nothing...
</comment><comment author="devoncrouse" created="2013-04-12T00:41:18Z" id="16270263">Not sure where to go from here, so closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms facets may return negative "other" count for script field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2878</link><project id="" key="" /><description>Repro: https://gist.github.com/imotov/983383fd17b215c44c71
</description><key id="12979756">2878</key><summary>Terms facets may return negative "other" count for script field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.0</label></labels><created>2013-04-09T16:24:32Z</created><updated>2013-04-09T16:43:27Z</updated><resolved>2013-04-09T16:43:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>sorry wrong project</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2877</link><project id="" key="" /><description /><key id="12975431">2877</key><summary>sorry wrong project</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bascii</reporter><labels /><created>2013-04-09T15:02:35Z</created><updated>2014-05-21T16:36:38Z</updated><resolved>2013-04-09T15:04:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Configure fielddata using a hash, not a string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2876</link><project id="" key="" /><description>With the changes in #2874 we are forced to specify a lot of config options in a long unreadable string.  The fielddata param should use a JSON object like all the other config in the mapping.

eg instead of: 

```
"fielddata": "format=paged_bytes;filter.frequency.min=0.001;filter.frequency.max=0.1;filter.frequency.min_segment_size=1000;filter.regex=^en_"
```

it should be:

```
"fielddata": {
    "format": "paged_bytes",
    "filter": {
        "min_freq": 0.001,
        "max_freq": 0.1,
        "min_segment_size":  1000,
        "regex": "^en_"
    }
}
```
</description><key id="12967380">2876</key><summary>Configure fielddata using a hash, not a string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>v0.90.0</label></labels><created>2013-04-09T11:41:09Z</created><updated>2013-04-09T13:56:35Z</updated><resolved>2013-04-09T13:56:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-09T11:42:28Z" id="16107745">just as a side note #2874 didn't introduce this. it just followed the convention that was there. but I agree it should be a hash
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>_source and indexing/storing type inconsistency</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2875</link><project id="" key="" /><description>I have strange situation. The mapping: 
... 
order_price: 
{
- type: "double", 
- include_in_all: false
  }, 
  ...

Response for some request:
- ... 
- { 
  ...
- order_price: 0, 
  ...
  }, 
- 
  { 
  ...
- order_price: "135" 
  ...
  } 
- ...

First entry is returned as number 
(order_price: 0) and the second one as string (order_price: "135").

The second one was inserted as string (in JSON indexing the 
entry there was probably "135" instead of 135). The mapping is double so I think it should be returned always as double.
</description><key id="12963319">2875</key><summary>_source and indexing/storing type inconsistency</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mdojwa</reporter><labels /><created>2013-04-09T09:32:50Z</created><updated>2013-04-10T09:18:03Z</updated><resolved>2013-04-10T09:18:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-09T16:14:58Z" id="16122976">can you reproduce this issue with an empty index, then create the mapping and index some documents? So we have a test case, we could use for debugging?

Also your elasticsearch version would be helpful as well as a check, that there is nothing in the logs.
</comment><comment author="mdojwa" created="2013-04-09T18:46:50Z" id="16132130">Sorry :) ES version 0.20.6. Below you can find the testcase (here is the gist too: https://gist.github.com/mdojwa/622488908880ff72d427) that creates an index, adds sample data and gets them. One number is returned as double (21.5) and one as string ("16.7").

``` bash
curl -XDELETE 'localhost:9200/test?pretty=1'

curl -XPOST 'localhost:9200/test?pretty=1' -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "mappings" : {
        "test1" : {
            "properties" : {
                "number" : { "type" : "double" }
            }
        }
    }
}'

curl -XPUT 'localhost:9200/test/test1/1?pretty=1' -d '{
    "number": 21.5
}'

curl -XPUT 'localhost:9200/test/test1/2?pretty=1' -d '{
    "number": "16.7"
}'

curl -XPOST 'localhost:9200/test/_refresh?pretty=1'

curl -XPOST 'localhost:9200/test/test1/_search?pretty=1'
```
</comment><comment author="spinscale" created="2013-04-10T07:48:17Z" id="16160044">Hey,

whenever you are indexing data, the _source field being returned in the data is always the same data you indexed. Elasticsearch does not change anything (unless configured to include/exclude fields). Hence the name of the field. :)

Internally however (for searches/calculations/faceting/...) the field is treated as a number (due to the mapping).
</comment><comment author="mdojwa" created="2013-04-10T07:54:00Z" id="16160223">Hi,

I understand this, but I guess this is confusing when we get "string" instead of "double" as it is defined in the mapping :) I assume that it can not be changed right ?

Thanks.
Best regards.
Marcin Dojwa
</comment><comment author="spinscale" created="2013-04-10T08:56:03Z" id="16162568">Hey

you can actually get the real value if you want. All you need to do, is store the field (done in the mapping), and then explicitely retrieve it in your query like this:

```
curl -X PUT localhost:9200/store
curl -X PUT localhost:9200/store/store/_mapping -d '{ "store" : { "properties" : { "field" : { "store" : "yes", "type":"double" } } } }'
curl -X PUT localhost:9200/store/store/1 -d '{"field":"1"}'
```

Now add the fields you want to receive to your query and you're done:

```
curl 'localhost:9200/store/store/_search?pretty' -d '{ "query": { "match_all": {} }, "fields" : ["field"] }'

{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "store",
      "_type" : "store",
      "_id" : "1",
      "_score" : 1.0,
      "fields" : {
        "field" : 1.0
      }
    } ]
  }
}
```

hope this helps. In case you dont feel this is a bug anymore, please close :)
</comment><comment author="mdojwa" created="2013-04-10T09:18:03Z" id="16163455">Thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow FieldData loading to be filtered</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2874</link><project id="" key="" /><description># FieldData Filter

FieldData is an in-memory representation of the term dictionary in an uninverted form. Under certain circumstances this FieldData representation can grow very large on high-cardinality fields like tokenized full-text. Depending on the use-case filtering the terms that are hold in the FieldData representation can heavily improve execution performance and application stability.

FieldData Filters can be applied on a per-segment basis. During FieldData loading the terms enumeration is passed through a filter predicate that  either accepts or rejects a term.

**Note: this feature is only supported on string fields**
## Frequency Filter

The Frequency Filter acts as a high / low pass filter based on the document frequencies of a certain term within the segment that is loaded into field data. It allows to reject terms that are very high or low frequent based on absolute frequencies or percentages relative to the number of documents in the segment or more precise the number of document that have at least one value in the field that is loaded in the current segment.

Here is an example mapping:

``` json
{
    "tweet" : {
        "properties" : {
            "locale" : {
                "type" : "string",
                "fielddata" : "format=paged_bytes;filter.frequency.min=0.001;filter.frequency.max=0.1",
                "index" : "analyzed",
            }
        }
    }
}
```
### Paramters
- `filter.frequency.min` - the minimum document frequency (inclusive) in order to be loaded in to memory. Either a percentage if &lt; `1.0` or an absolute value. `0` if omitted.
- `filter.frequency.max` - the maximum document frequency (inclusive) in order to be loaded in to memory. Either a percentage if &lt; `1.0` or an absolute value. `0` if omitted. 
- `filter.frequency.min_segment_size` - the minimum number of documents in a segment in order for the filter to be applied. Small segments might be omitted with this setting.
## Regular Expression Filter

The regular expression filter applies a regular expression to each term  during loading and only loads terms into memory that match the given regular expression. 

Here is an example mapping:

``` json
{
    "tweet" : {
        "properties" : {
            "locale" : {
                "type" : "string",
                "fielddata" : "format=paged_bytes;filter.regex=^en_.*",
                "index" : "analyzed",
            }
        }
    }
}
```
</description><key id="12962645">2874</key><summary>Allow FieldData loading to be filtered</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0</label></labels><created>2013-04-09T09:17:50Z</created><updated>2013-04-09T10:23:39Z</updated><resolved>2013-04-09T10:23:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.90 RC2 can no longer fetch template</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2873</link><project id="" key="" /><description>```
curl -XPUT http://localhost:9200/_template/test -d '{template: "session*", settings: { "foo.bar": 1 } }'

curl 'http://localhost:9200/_template/test?pretty'
```

returns

```
{
  "error" : "IndexMissingException[[_na] missing]",
  "status" : 404
}%
```
</description><key id="12941912">2873</key><summary>0.90 RC2 can no longer fetch template</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">awick</reporter><labels><label>bug</label><label>v0.90.0</label></labels><created>2013-04-08T20:13:57Z</created><updated>2013-04-08T20:49:48Z</updated><resolved>2013-04-08T20:49:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.90 RC2 Breaking Change - indices element moved in _stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2872</link><project id="" key="" /><description>My goal is to get a list of all indices, if there is a better way I'll change to that.  I had been using _stats.

```
curl http://localhost:9200/_stats?pretty&amp;clear=1

Previously indices was under _all
{
  "ok" : true,
  "_shards" : {
    "total" : 500,
    "successful" : 500,
    "failed" : 0
  },
  "_all" : {
    "primaries" : { },
    "total" : { },
    "indices" : {
      "sessions-130407" : {
        "primaries" : { },
        "total" : { }
      },
```

Now indices is at the same level as _all

```
{
  "ok" : true,
  "_shards" : {
    "total" : 500,
    "successful" : 500,
    "failed" : 0
  },
  "_all" : {
    "primaries" : { },
    "total" : { }
  },
  "indices" : {
    "sessions-130407" : {
      "primaries" : { },
      "total" : { }
    },
```
</description><key id="12932858">2872</key><summary>0.90 RC2 Breaking Change - indices element moved in _stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2013-04-08T16:50:44Z</created><updated>2013-04-16T19:53:46Z</updated><resolved>2013-04-16T19:53:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-04-08T18:03:06Z" id="16067323">Yea, stats is not the best way to get a list of indices (and there was a breaking change). Use the cluster state API to get the list if indices. Might also make sense for us to add a dedicated endpoint just to get the list of indices simply...
</comment><comment author="s1monw" created="2013-04-16T19:53:46Z" id="16467767">I am closing this... thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node Stats: Allow to explicitly get specific indices level node stats element</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2871</link><project id="" key="" /><description>for example, get things like `docs`, or `store`, using a URI element, for example: `/_nodes/stats/indices/docs`
</description><key id="12907090">2871</key><summary>Node Stats: Allow to explicitly get specific indices level node stats element</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-04-08T03:22:24Z</created><updated>2013-04-08T03:22:48Z</updated><resolved>2013-04-08T03:22:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>FieldData Stats: Add field data stats to indices stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2870</link><project id="" key="" /><description>Expose the field data stats through the indices stats API, enabled using the `fielddata` flag, or explicitly using `fielddata` URI endpoint (`/{indexName}/_stats/fielddata`.
</description><key id="12905492">2870</key><summary>FieldData Stats: Add field data stats to indices stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-04-08T01:29:56Z</created><updated>2013-04-08T01:30:22Z</updated><resolved>2013-04-08T01:30:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>index.auto_expand_replicas and shard allocation settings don't play well together</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2869</link><project id="" key="" /><description>With an index configured with auto_expand_replicas="0-all" the cluster will try to allocate one primary and ({number nodes} - 1) replicas for each shard, ie a copy of each shard on each node.

However, with "index.routing.allocation.include.zone"="zone1" the cluster is blocked from allocating a shard (either primary or replica) to any nodes that are not configured with a zone attribute set to "zone1", ie "node.zone=zone1" in the elasticsearch.yaml config file. So if a cluster has 3 nodes in "zone1" and 3 nodes in "zone2" it will allocate 3 shards (primary + 2 x replicas) to the "zone1" nodes and mark an additional 3 replicas as unassigned. I observed this using the elasticsearch head utility.

So the allocation behaviour is as desired, ie auto expand replicas to all nodes with a specific zone attribute, but the unassigned shards will make the cluster be in red state

See https://groups.google.com/forum/#!msg/elasticsearch/95hC-wGu7GE/BPPSWsfj8UkJ
</description><key id="12899202">2869</key><summary>index.auto_expand_replicas and shard allocation settings don't play well together</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">open</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels><label>:Allocation</label><label>discuss</label><label>enhancement</label><label>v5.4.4</label></labels><created>2013-04-07T18:10:01Z</created><updated>2017-06-27T10:28:29Z</updated><resolved /><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-04-07T18:10:18Z" id="16020046">using latest master I should add
</comment><comment author="clintongormley" created="2014-08-08T10:54:56Z" id="51587778">I'm wondering if we should deprecate auto-expand...  It just seems like the wrong solution
</comment><comment author="synhershko" created="2014-08-08T11:00:30Z" id="51588189">IMO no, auto-expand is a very useful feature and a way for clusters to automatically grow to accommodate peaks with read-heavy operations. It's just somewhat broken when shard-allocation rules aren't trivial.

I admit tho, auto-expand does require multi-cast to be enabled (or predefining IPs for the machines dynamically provisioned during peaks), so at this point it does seem more like of an exotic feature.
</comment><comment author="nik9000" created="2014-08-08T11:28:07Z" id="51590106">We used it in CirrusSearch so people with only a single node or two nodes
don't end up with a yellow cluster. We document the autocorrelating
behavior and the implications to redundancy. I still don't particularly
trust it BUT it is a nice way to make installation smooth even in small
environments. We use most of the CirrusSearch details both in development
and on the production cluster and it helps with that.
On Aug 8, 2014 12:00 PM, "Itamar Syn-Hershko" notifications@github.com
wrote:

&gt; IMO no, auto-expand is a very useful feature and a way for clusters to
&gt; automatically grow to accommodate peaks with read-heavy operations. It's
&gt; just somewhat broken when shard-allocation rules aren't trivial.
&gt; 
&gt; I admit tho, auto-expand does require multi-cast to be enabled (or
&gt; predefining IPs for the machines dynamically provisioned during peaks), so
&gt; at this point it does seem more like of an exotic feature.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/issues/2869#issuecomment-51588189
&gt; .
</comment><comment author="markharwood" created="2014-09-05T08:27:54Z" id="54597975">Attached an "adoptme" tag as this needs some further looking into to establish if this is an issue. 
The auto-expand feature is used internally to keep a copy of the .scripts index (stored scripts) on each node so this does need to work.
</comment><comment author="clintongormley" created="2015-09-19T17:29:45Z" id="141691663">Having reread this issue, it seems that if we were able to limit the max number of auto-expand replicas to the number of nodes that would allow allocation (eg awareness rules) then we'd avoid the yellow status.

Not sure how feasible this is.
</comment><comment author="colings86" created="2015-11-13T10:11:43Z" id="156385893">@dakrone Do you know whether @clintongormley suggestion above is feasible?
</comment><comment author="dakrone" created="2015-11-16T18:37:57Z" id="157130491">@colings86 @clintongormley I looked at where the setting is updated, it's
currently updated in `MetaDataUpdateSettingsService`, which submits a new
cluster state update to change the number of replicas if it detects that the
setting must be changed.

I think it would be possible to try and use the AllocationDeciders to adjust the
number of replicas to the number of nodes where it can be allocated, but I don't
think it's trivial.

Maybe it should be a separate setting? Instead of `0-all` it can be
`0-available` or something?
</comment><comment author="vb3" created="2016-09-20T18:28:28Z" id="248389704">I noticed that in ES1.7
`.scripts` index has the `0-all` setting automatically 
My cluster is yellow because it is unable to expand replicas to nodes that are full in data capacity bounded by 

```
        "cluster.routing.allocation.disk.watermark.low": "80%",
        "cluster.routing.allocation.disk.watermark.high": "85%"
```

is there a solution for this?
</comment><comment author="AlexP-Elastic" created="2016-10-17T14:37:37Z" id="254226142">We have run into this issue (or a heavily related one) twice in the last month on Cloud, both times on 2.x clusters (#1656, details [here](https://github.com/elastic/cloud/issues/1656#issuecomment-236293503) and [here](https://github.com/elastic/cloud/issues/1656#issuecomment-252672161) in the comments)

In particular it _appears_ (it's happening on production clusters so unfortunately there's a limit to how much debugging we can do before it is necessary to reset the cluster state) that an unallocated "auto expand" index is preventing other indexes from migrating given a `cluster.routing.allocation.exclude._name` directive - is that even remotely possible? ([here](https://github.com/elastic/cloud/issues/1656#issuecomment-252672161) is my notes on what I saw the most recent time) ... 

We are about to deploy a workaround that will temporarily disable "auto expand" for `.scripts` and `.security` while updating a cluster (and perhaps user indexes in the future), but `.security` settings cannot be changed in 2.x so this workaround is incomplete.

cc @alexbrasetvik 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get template does not seem to return warmers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2868</link><project id="" key="" /><description>Unless of course I am doing something wrong, it seems that registering a warmer using PUT template seems to work but if i then GET the template it does not return the warmers.

See: https://gist.github.com/Mpdreamz/5330654

For the raw HTTP calls.

I'm using elasticsearch 0.20.6
</description><key id="12895979">2868</key><summary>Get template does not seem to return warmers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">Mpdreamz</reporter><labels><label>bug</label><label>v0.20.7</label><label>v0.90.0</label></labels><created>2013-04-07T14:27:17Z</created><updated>2013-04-08T23:35:30Z</updated><resolved>2013-04-08T23:35:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Proximity BM25 ranking</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2867</link><project id="" key="" /><description>Since we have BM25 similarity in 0.90.0.Beta1, is there any way to implement Sphinx-like Proximity BM25 ranking?

```
SPH_RANK_PROXIMITY_BM25, the default ranking mode that uses and combines both phrase proximity and BM25 ranking.
```
</description><key id="12893752">2867</key><summary>Proximity BM25 ranking</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kshnurov</reporter><labels><label>:Search</label><label>feature</label></labels><created>2013-04-07T10:56:30Z</created><updated>2015-08-13T13:38:52Z</updated><resolved>2015-06-25T09:26:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-07T11:02:11Z" id="16013137">I don't know how sphinx score here in detail but I'd like to look into something like [this](http://stefan.buettcher.org/papers/buettcher_2006_term_proximity.pdf)
</comment><comment author="kshnurov" created="2013-04-07T11:29:48Z" id="16013493">Sphinx algorithm is described [here](http://sphinxsearch.com/blog/2010/08/17/how-sphinx-relevance-ranking-works/). Here it is:

```
1) query = one two three, field = one and two three
field_phrase_weight = 2 (because 2-keyword long "two three" subphrase matched)

2) query = one two three, field = one and two and three
field_phrase_weight = 1 (because single keywords matched but no subphrase did)

3) query = one two three, field = nothing matches at all
field_phrase_weight = 0

doc_phrase_weight = 0
foreach ( field in matching_fields )
{
   field_phrase_weight = max_common_subsequence_length ( query, field )
   doc_phrase_weight += user_weight ( field ) * field_phrase_weight
}

weight = doc_phrase_weight*1000 + integer(doc_bm25*999)
```

So, how can we implement this in ES?
</comment><comment author="vineelyalamarthy" created="2015-06-18T20:18:35Z" id="113278097">@clintongormley   is this some thing to be fixed?  How can I assign this bug to myself ? I m extremely interested in this.
</comment><comment author="kshnurov" created="2015-06-18T22:31:23Z" id="113305871">@kimchy, @s1monw, please add (Phrase proximity + BM25) ranking!
While ES is very good at search, it is very bad at text search **ranking**.

Let's say we have 2 documents:
1. `(2000 words) ... child elephant from America was ... (2000 words)`
2. `Egyptian child ... (2000 words) ... american coffee with elephant ... (2000 words) ... toy elephant`

We've heard something interesting about child elephant from America and now we want to find it: `"match": { "text": "child elephant america"}`. Guess what? 2nd document will be ranked higher(!) because it has 2 'elephants' vs 1 and this is how TF/IDF and current algorithm works.

Things get even worse when you're trying to find a restaurant called "China Street", since you have words "china" and "street" found in hundreds of restaurants. You can add a boost for name field, but then you won't be able to find "roosevelt street restaurants", since restaurants "Roosevelt" and "Lonely Street" will be ranked higher because of the boost. You'll add a boost for address and bang! - now you can't find a restaurant with words "good china restaurant, previously located at roosevelt street" in description. It will be ranked much, much lower than you expect (second page at least) because of the boosts and restaurants "China Street" and "Roosevelt", all china restaurants at Roosevelt st. and etc. This is where phrase proximity is needed to make ranking a lot better.
</comment><comment author="vineelyalamarthy" created="2015-06-18T22:35:58Z" id="113306471">@kimchy @kshnurov  can I work on this bug or some one already took it ? I would like to know where to get started.
</comment><comment author="vineelyalamarthy" created="2015-06-18T23:13:07Z" id="113314935">@kimchy @kshnurov @kshnurov @clintongormley @rmuir This bug sounds very interesting, I want to work on it,you can review my code before check-in , But could you please tell me, how to get started. 
</comment><comment author="brusic" created="2015-06-19T00:00:14Z" id="113322081">You still have phrase matching with slop at your disposal, so there is
nothing that is preventing from rolling your own. You can also disable term
frequencies, use bigrams, or all of the above. Many of us have tuned
relevancy with the current tool set. Your queries will get complicated and
the tuning process might take a bit.

Isn't BM25 about document length normalization? Need to look at the sphinx
algorithm to see why proximity is used only there.

As far as contributing to the code, got can find guides online on how to
contribute to a github project. Create a new branch, check in the code and
issue a pull request.  Beware that elasticsearch is notoroius for not
accepting external pull requests. Perhaps that has changed lately (I gave
up).
On Jun 19, 2015 6:32 AM, "Kirill Shnurov" notifications@github.com wrote:

&gt; @kimchy https://github.com/kimchy, @s1monw https://github.com/s1monw,
&gt; please add (Phrase proximity + BM25) ranking!
&gt; While ES is very good at search, it is very bad at text search _ranking_.
&gt; 
&gt; Let's say we have 2 documents:
&gt; 1. (2000 words) ... child elephant from America was ... (2000 words)
&gt; 2. Egyptian child ... (2000 words) ... american coffee with elephant ...
&gt; (2000 words) ... toy elephant
&gt; 
&gt; We're heard something interesting about child elephant from America and
&gt; now we want to find it: "match": { "text": "child elephant america"}.
&gt; Guess what? 2nd document will be ranked higher(!) because it has 2
&gt; 'elephants' vs 1 and this is how TF/IDF and current algorithm works.
&gt; 
&gt; Things get even worse when you're trying to find a restaurant called
&gt; "China Street", since you have words "china" and "street" found in hundreds
&gt; of restaurants. You can add a boost for name field, but then you won't be
&gt; able to find "roosevelt street restaurants", since restaurants "Roosevelt"
&gt; and "Lonely Street" will be ranked higher because of the boost. You'll add
&gt; a boost for address and bang! - now you can't find a restaurant with words
&gt; "good china restaurant, previously located at roosevelt street" in
&gt; description. It will be ranked much, much lower than you expect (second
&gt; page at least) because of the boosts and restaurants "China Street" and
&gt; "Roosevelt", all china restaurants at Roosevelt st. and etc. This is where
&gt; phrase proximity is needed to make ranking a lot better.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/2867#issuecomment-113305871
&gt; .
</comment><comment author="kshnurov" created="2015-06-19T01:40:06Z" id="113337207">@vineelyalamarthy this IS NOT a bug, it's just [how default ES ranking works](https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html). In most cases this is more than enough.
@brusic phrase matching is a query type, not a ranking method. Ranking will be the same for the example I've posted. For now ES has [only 2 ranking methods](https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html) - default one and BM25.
</comment><comment author="brusic" created="2015-06-19T02:08:54Z" id="113342004">All queries contribute to the score, so they implicitly form part of the
ranking method.

I use a complicated set of queries which includes phrase queries to provide
a boost to the score if there is a match. It would be nice to issue a
single query, but alas that is not possible if you want to fine tune
relevancy.

BM25 deals with field length normalization, so it still is TFIDF under the
hood. If your corpus has large variances in the length of a field and term
frequencies are still important, BM25 might be a good approach. It has
nothing to do with proximity.

Agree that this is not a bug, but a feature request. Since there is a
published algorithm, it would be great to incorporate into elasticsearch. I
would give it a try, but I do not have time.
On Jun 19, 2015 9:40 AM, "Kirill Shnurov" notifications@github.com wrote:

&gt; @vineelyalamarthy https://github.com/vineelyalamarthy this IS NOT a
&gt; bug, it's just how default ES ranking works
&gt; https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html.
&gt; In most cases this is more than enough.
&gt; @brusic https://github.com/brusic phrase matching is a query type, not
&gt; a ranking method. Ranking will be the same for the example I've posted. For
&gt; now ES have only 2 ranking methods
&gt; https://www.elastic.co/guide/en/elasticsearch/guide/current/pluggable-similarites.html
&gt; - default one and BM25.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/2867#issuecomment-113337207
&gt; .
</comment><comment author="vineelyalamarthy" created="2015-06-19T02:14:06Z" id="113342613">@brusic  This will be my first contribution to Elastic Search code base. I would like to work on this , if its approved as a feature request.I have the code set-up in Eclipse. Are there any other steps required to get started.If you know, how to start on this particular task, please let me know.
</comment><comment author="brusic" created="2015-06-19T03:22:00Z" id="113356413">Venkata, it is no different than any other git open source project. Clone
the repo, create a new branch, commit your changes (make sure you squash
your commits into a single one), and then issue a pull request. Add many
tests.

Befriend elastic devs on irc and the mailing list so that you can give some
visibility to your change.

This feature would be great to have. It feels like most do not use
elasticsearch for searching! Tuning is a skill.
 On Jun 19, 2015 10:15 AM, "Venkata Vineel" notifications@github.com
wrote:

&gt; @brusic https://github.com/brusic This will be my first contribution to
&gt; Elastic Search code base. I would like to work on this , if its approved as
&gt; a feature request.I have the code set-up in Eclipse. Are there any other
&gt; steps required to get started.If you know, how to start on this particular
&gt; task, please let me know.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/2867#issuecomment-113342613
&gt; .
</comment><comment author="brusic" created="2015-06-19T10:57:10Z" id="113472958">I should have added that Elasticsearch does not have different scoring
models itself, but simply exposes those found in the Lucene library.
Chances are that any changes you may want to add really belong at the
Lucene layer.

That said, there are many smart people that are committers in Lucene, many
of them are now employed by Elastic. If they have not already implemented
something similar already, there might be a reason. Perhaps someone like
@mikemccand can chime in.

There are other similarity models in Lucene/Elasticsearch, but none of them
will help with this issue.
On Jun 19, 2015 10:21 AM, "Ivan Brusic" ivan@brusic.com wrote:

&gt; Venkata, it is no different than any other git open source project. Clone
&gt; the repo, create a new branch, commit your changes (make sure you squash
&gt; your commits into a single one), and then issue a pull request. Add many
&gt; tests.
&gt; 
&gt; Befriend elastic devs on irc and the mailing list so that you can give
&gt; some visibility to your change.
&gt; 
&gt; This feature would be great to have. It feels like most do not use
&gt; elasticsearch for searching! Tuning is a skill.
&gt;  On Jun 19, 2015 10:15 AM, "Venkata Vineel" notifications@github.com
&gt; wrote:
&gt; 
&gt; &gt; @brusic https://github.com/brusic This will be my first contribution
&gt; &gt; to Elastic Search code base. I would like to work on this , if its approved
&gt; &gt; as a feature request.I have the code set-up in Eclipse. Are there any other
&gt; &gt; steps required to get started.If you know, how to start on this particular
&gt; &gt; task, please let me know.
&gt; &gt; 
&gt; &gt; &#8212;
&gt; &gt; Reply to this email directly or view it on GitHub
&gt; &gt; https://github.com/elastic/elasticsearch/issues/2867#issuecomment-113342613
&gt; &gt; .
</comment><comment author="brusic" created="2015-06-25T03:52:53Z" id="115093452">Finally read the Sphinx website and it seems like it does not do anything special. SPH_RANK_PROXIMITY_BM25 is just a scored phrase query while using the scaled TF values calculated by BM25. The two concepts are orthogonal.

Also judging by my quick read, Lucene's sloppy phrase freq scoring is more detailed and granular than Sphinx's longest common sub-sequence (LCS).
</comment><comment author="clintongormley" created="2015-06-25T09:26:17Z" id="115182134">thanks for investigating @brusic - I'm going to close this one
</comment><comment author="brusic" created="2015-06-25T11:39:59Z" id="115217399">Someone please correct me if I am wrong. I haven't even installed Sphinx, but that is my take on things.
</comment><comment author="rmuir" created="2015-06-25T12:30:59Z" id="115234762">Yes, in general here such hacks to BM25 are usually defined as only working on the highest portion of the top-N results[1][2].

So I think any work around this should be done as a rescorer, not a lucene similarity. Today you can already use a `query` rescorer with a sloppy phrase query to do this, see the examples in the documentation[3]. 

I do think it would be good to have rescorer implementations with maybe a better formula than sloppy phrase query, but that is also not obvious since there are still so many varying approaches out there, and all seem kinda ad-hoc. Personally, I don't see any winners, especially since BM25 has term independence assumptions baked in...
1. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.174.8359
2. http://research.microsoft.com/pubs/144542/ppm.pdf
3. https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-rescore.html
</comment><comment author="kshnurov" created="2015-06-26T00:05:11Z" id="115434664">@brusic you're wrong, Lucene's sloppy phrase freq scoring works _only_ for sloppy phrase and span queries. Scoring we're talking about would require putting `n! (n = words count)` or ever more different phrase queries into 1 request.

@clintongormley please reopen this issue. What we need is a rescorer, as @rmuir said, that will add LCS or any similar algorithm on top of the default ranking. It's very simple and probably could be done with scripting, but script won't be fast at all.
</comment><comment author="kshnurov" created="2015-06-26T00:07:30Z" id="115435076">Once again, here's the algorithm. We already have `doc_bm25` value in ES, all we need is to calculate and add `doc_phrase_weight`.

```
doc_phrase_weight = 0
foreach ( field in matching_fields )
{
   field_phrase_weight = max_common_subsequence_length ( query, field )
   doc_phrase_weight += user_weight ( field ) * field_phrase_weight
}

weight = doc_phrase_weight*1000 + integer(doc_bm25*999)
```
</comment><comment author="kshnurov" created="2015-06-26T00:10:36Z" id="115435393">And the LCS algorithm could be found here, it's also quite simple, you just need to add it into ES: https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Longest_common_subsequence#Java
</comment><comment author="rmuir" created="2015-06-26T12:54:24Z" id="115670292">&gt; Once again, here's the algorithm.

Hold your horses, where are IR tests for "your algorithm". By default, I'm telling you this algorithm sucks, unless you can prove otherwise. Don't be offended: this is how it works. You have to defend it with some results, otherwise we should not waste our time.
</comment><comment author="kshnurov" created="2015-06-26T15:03:09Z" id="115720272">@rmuir have you at all read this issue before coming and saying 'it sucks'? There's a link to a solid report and tests in [2nd comment](https://github.com/elastic/elasticsearch/issues/2867#issuecomment-16013137): http://stefan.buettcher.org/papers/buettcher_2006_term_proximity.pdf

You can find a lot more in 10 seconds, if all of this serious reports from well-known companies and universities (and the fact that it's been a default ranking method in Sphinx for years) isn't enough for you:
http://www.researchgate.net/publication/225174089_Term_Proximity_Scoring_for_Keyword-Based_Retrieval_Systems
http://research.microsoft.com/pubs/144542/ppm.pdf
http://sifaka.cs.uiuc.edu/czhai/pub/sigir07-prox.pdf
http://ir.dcs.gla.ac.uk/~ronanc/papers/cumminsSIGIR09.pdf
http://people.mpi-inf.mpg.de/~mtb/pub/proximity-spire07.pdf
</comment><comment author="rmuir" created="2015-06-26T15:36:47Z" id="115734278">None of that is good enough for me, its a bogus argument: you link to a bunch of completely different algorithms, different than what you propose.
</comment><comment author="kshnurov" created="2015-06-26T16:01:31Z" id="115741184">@rmuir saying "it sucks" and "is not good enough for me" without saying what is enough and doesn't sucks won't make it move. These links provide more than enough info about why term proximity is important and a lot of algorithms to choose from (with proof and tests, as you requested). 

Sounds like you just don't wanna do anything, since you didn't even spent enough time on reading this links. Saying about Sphinx being a 'bogus argument' is completely weird. @s1monw @clintongormley could you please join and make this conversation constructive, not a "I'm a pro, everything sucks" performance?
</comment><comment author="rmuir" created="2015-06-26T16:06:54Z" id="115742493">Again, the onus is not on me to prove that your proposed algorithm sucks. thats not how it works.

You have to prove that its better.

We don't need to try to copy what sphinx does. In general, its probably best to do the exact opposite. They do stupid things like removing length normalization completely.
</comment><comment author="brusic" created="2015-06-26T16:11:39Z" id="115743445">You keep avoiding the fact that Lucene already has proximity scoring. IMHO, a far better one.

Sphinx uses LCS, which is a variation of simple edit distance. Lucene uses an exponential function for the decay as the the slop between words increases. Which do you think will be more precise? Look at the second example in the Sphinx documentation. Do you think 1 is the best answer? 

That said, no edit distance method is perfect for every scenario. A query rescorer would be a good place for other methods. 

EDIT: do not want to link to the source, but I was able to find a recent blog post about proximity scoring in Lucene. 

http://sujitpal.blogspot.com/2015/04/scoring-token-distance-in-lucene-sloppy.html
</comment><comment author="kshnurov" created="2015-06-26T16:30:43Z" id="115747326">@rmuir what will make proof for you if not all of this reports saying and proving that term proximity scoring will dramatically improve text search ranking? Basically you're saying "all of this professors and experts are stupid, I know better - we shouldn't do anything".

@brusic yes, Lucene's proximity scoring is even better than LCS, but we can't use it for match queries, as I said above. If we can find a way to use it in conjunction with BM25 without making N! queries - this issue will be solved.
</comment><comment author="rmuir" created="2015-06-26T16:38:47Z" id="115748873">&gt; @rmuir what will make proof for you if not all of this reports saying and proving that term proximity scoring will dramatically improve text search ranking? Basically you're saying "all of this professors and experts are stupid, I know better - we shouldn't do anything"

I don't see dramatic improvements yet anywhere. And the underlying math is still broken and not yet figured out. So yeah, if that's calling people stupid, then i am calling people stupid. Trust me, I am ok with that.
</comment><comment author="kshnurov" created="2015-06-26T17:25:09Z" id="115788181">@rmuir check out this case: https://github.com/elastic/elasticsearch/issues/2867#issuecomment-113305871 Yes, sloppy phrase scoring would make a deal, but isn't it stupid to manually generate N! queries for that?

Can you tell **exactly** what is broken or wrong? You're just making unreasonable statements.
</comment><comment author="rmuir" created="2015-06-26T17:26:57Z" id="115790449">&gt; Can you tell exactly what is broken or wrong?

BM25 formula is based on the assumption that terms are independent. Its as simple as that. We are done here.
</comment><comment author="kshnurov" created="2015-06-26T17:31:56Z" id="115796710">@rmuir what the hell, have you read anything at all? This is **not about BM25**, it's about taking **term proximity** into account! If you don't like BM25, you can use TF/IDF + term proximity.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>BooleanFieldMapper should set `IndexOptions.DOCS_ONLY` by default.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2866</link><project id="" key="" /><description>today and it seems like 0.20 as well uses the default which is `IndexOptions.DOCS_AND_FREQS_AND_POSITIONS` - this stores extra term frequency and position information for each boolean which seems unnecessary here. I flagged this as breaking since its a new default that might case different behaviour under certain circumstances but I'd guess almost all users are not affected. Anybody doing phrase queries on boolean fields? 
</description><key id="12883197">2866</key><summary>BooleanFieldMapper should set `IndexOptions.DOCS_ONLY` by default.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-04-06T20:40:55Z</created><updated>2013-04-06T20:51:06Z</updated><resolved>2013-04-06T20:51:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow to set a global request timeout</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2865</link><project id="" key="" /><description>It is currently possible to set a timeout in each search request (is it possible for facets too ?), but it would be great to be able to set it globally per index or cluster, as a protection against dangerous queries.
</description><key id="12879318">2865</key><summary>Allow to set a global request timeout</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dav3860</reporter><labels /><created>2013-04-06T15:44:01Z</created><updated>2013-06-06T11:17:03Z</updated><resolved>2013-06-06T11:17:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-06-06T11:17:03Z" id="19038739">Closed in favour of #3129 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow an option to return only fields as the root</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2864</link><project id="" key="" /><description>Per [Elasticsearch - how to return only data, not meta information?](http://stackoverflow.com/questions/10860588/elasticsearch-how-to-return-only-data-not-meta-information) it can be a little bit of a hassle to de-serialize and re-serialize data to remove the metadata. If passing the metadata through is critical to the design of ES (such as aggregation), please mark this out as a WONTFIX issue.
</description><key id="12871428">2864</key><summary>Allow an option to return only fields as the root</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2013-04-06T03:10:00Z</created><updated>2013-04-06T08:18:43Z</updated><resolved>2013-04-06T06:20:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-04-06T06:20:18Z" id="15991609">See @imotov answer on SOF:

&gt; No, it's not possible at this moment. If performance and complexity of parsing are the main concerns, you might want to consider using different clients: [java client](http://www.elasticsearch.org/guide/reference/java-api/client.html) or [Thrift plugin](http://www.elasticsearch.org/guide/reference/modules/thrift.html), for example.
</comment><comment author="Downchuck" created="2013-04-06T08:18:43Z" id="15992757">There was ambiguity in the wording "it's not possible at this moment"; I'll take the re-affirmed close as a firm WONTFIX.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow non numeric numbers in JsonParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2863</link><project id="" key="" /><description>I need to index number with NaN values...
Could you allow non numeric number as you allowed comments (https://github.com/elasticsearch/elasticsearch/issues/1394).

`jsonFactory.configure(JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS, true);`

Could these settings be configurable in ElasticSearch?
</description><key id="12858414">2863</key><summary>Allow non numeric numbers in JsonParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">cthiebault</reporter><labels /><created>2013-04-05T18:39:00Z</created><updated>2014-11-24T13:25:59Z</updated><resolved>2014-08-08T10:52:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T10:52:51Z" id="51587615">You can map a numeric field to be `lenient`, which would work.
</comment><comment author="alecklandgraf" created="2014-11-20T23:44:10Z" id="63901278">Can you provide an example of how you can map a field to be `lenient`? I only see it in the docs for queries.

I'm getting the error with python infinity, 

`Caused by: org.elasticsearch.common.jackson.core.JsonParseException: Non-standard token 'Infinity': enable JsonParser.Feature.ALLOW_NON_NUMERIC_NUMBERS to allow
 at [Source: [B@5f26bc90; line: 1, column: 2880]`
</comment><comment author="clintongormley" created="2014-11-24T13:25:59Z" id="64193014">@alecklandgraf apologies, the correct parameter is `ignore_malformed`.  As you said, `lenient` is indeed for queries only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter / Id Cache Stats: Add to Indices Stats API, revise node stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2862</link><project id="" key="" /><description>Currently, the filter and id cache are only available on the node stats API (with the indices option), but not as part of the index stats API (because they are not kept on the shard level). Add them to the indices stats API (enabled using the `filter_cache` flag, and `id_cache` flag, or the respective URIs endpoints), and while at it, revise the node stats API to include the `filter_cache` and `id_cache` as top level elements in under `indices` response, and not under `cache`.
</description><key id="12856734">2862</key><summary>Filter / Id Cache Stats: Add to Indices Stats API, revise node stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-04-05T17:53:29Z</created><updated>2013-04-05T18:02:36Z</updated><resolved>2013-04-05T18:02:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make sure `all_terms` works consistently</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2861</link><project id="" key="" /><description>In some cases the `all_terms` option was ignored:
- Faceting on number based fields.
- The `execution_type` was set to `map`.
- In the case the `fields` option was used. 
</description><key id="12842810">2861</key><summary>Make sure `all_terms` works consistently</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label><label>regression</label><label>v0.90.0.RC2</label></labels><created>2013-04-05T12:09:41Z</created><updated>2013-04-05T12:45:33Z</updated><resolved>2013-04-05T12:28:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-05T12:45:33Z" id="15953602">thanks martijn!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Match for an ambiguous field silently searches only through first key occurence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2860</link><project id="" key="" /><description>I'm not really sure if it's a feature or bug. If i search for an ambiguous filed name without specifying unique path, match search _silently_ returns documents matched only on first found occurrence of the matched key. (checked in ES 0.90. and 0.19.)
### STEPS TO REPRODUCE:

1) Create a new index and add 2 items

```
curl -XPOST 'localhost:9200/test/item/' -d '{
    "name": "john", 
    "like":{"city": "Berlin"}, 
    "unlike": {"city": "Brussels"}
}'
```

```
curl -XPOST 'localhost:9200/test/item/' -d '{
    "name": "jane", 
    "like":{"city": "Paris"}, 
    "unlike": {"city": "Berlin"}
}'
```

2) Now execute match query search for "city" instead of "like.city" or "unlike.city"

```
curl -XPOST 'localhost:9200/test/_search?pretty=true' -d '{
    "query":{
        "match":{"city":"Berlin"}
    }
}'
```

```
curl -XPOST 'localhost:9200/test/_search?pretty=true' -d '{
    "query":{
        "match":{"city":"Brussels"}
    }
}'
```
### OBSERVED RESULTS:

only result from "like.city" silently is returned. 
**Search  for "Berlin" will return only 1 document where like.city="Berlin"**

``` javascript
{
  "hits" : {
    "total" : 1,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "test",
      "_type" : "item",
      "_id" : "Sh4vc0WZSDKIcfSENZstAA",
      "_score" : 1.0, "_source" : {
           "name": "john", 
           "like": {"city": "Berlin"}, 
           "unlike": {"city": "Brussels"}}
    } ]
  }
}
```

**Search for "Brussels" return 0 results.**
### EXPECTED RESULTS:

ambiguous error for field "city" or results from both "like.city" and "unlike.city"
</description><key id="12842488">2860</key><summary>Match for an ambiguous field silently searches only through first key occurence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gakhov</reporter><labels /><created>2013-04-05T11:57:41Z</created><updated>2014-07-08T19:38:08Z</updated><resolved>2014-07-08T19:38:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-04-05T12:00:21Z" id="15951885">Yea, this is the current behavior..., we will get the first mapping found and use that one...
</comment><comment author="gakhov" created="2013-04-05T12:06:26Z" id="15952088">Is there some reason not to raise an exception if no such key on top level?
</comment><comment author="kimchy" created="2013-04-05T12:10:32Z" id="15952230">The idea was to simplify finding fields, though I agree it can be confusing, we need to think how to properly solve it if we wish to, it will be a backward break, so we can revisit this post 0.90.
</comment><comment author="gakhov" created="2013-04-05T12:13:24Z" id="15952340">Cool! Actually, i found this behavior after noticed a bug in our system based on ElasticSearch :)
</comment><comment author="clintongormley" created="2013-04-05T13:10:57Z" id="15954621">Some thoughts about field resolution:

1) Make field resolution with types deterministic:

If you have two types: 

`user`:

```
{ name: "john"}
```

`post`:

```
{ title: "foo", user: { name: "john" }}
```

The `user` type recognises: `name` and `user.name`
The `post` type recognises `name`, `user.name` and `post.user.name`

The `user.name` conflicts.  It should always resolve to `type`.`field` before `field.field`, because `field.field` can always be referred to unequivocally as `type.field.field`

2) Warn/throw error on conflicts?

When referring to a new index/type/field combination, we should not stop at the first field, but check all other fields, then cache the result until a mapping changes. 

If we have two types, both with a `name` field.  Two possibilities. If the mapping is the same for both fields, then resolve to the first field.  If the mapping conflicts, throw an error about ambiguous field names. The user can then either:
- specify the field unambiguously
- limit the types that they are querying
- rewrite the clause as eg a bool or multi-match to run on both fields (or even `fields: ['*.name']`)

This requires a bit more work by the user but (a) it'll be correct and (b) won't produce hard to debug errors

3) Use any level of paths to refer to fields

If we have type `post` with 

```
{ 
  name: "my first post",
  author: { user: { name: "john"}}, 
  reviewer: { user: { name: "mary"}}
}
```

Would be nice to be able to use `name`, `user.name`, `author.user.name` and `post.author.user.name` 

Currently we wouldn't be able to use `user.name`.
</comment><comment author="benjismith" created="2014-02-13T17:04:13Z" id="35000262">Will this issue be fixed by https://github.com/elasticsearch/elasticsearch/issues/4081? Is this scheduled for 1.1.0?
</comment><comment author="clintongormley" created="2014-07-08T19:38:08Z" id="48389569">Closed in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How do you write a relative path in elasticsearch.yml?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2859</link><project id="" key="" /><description>I want to provide a value for `path.data` that is relative to the installation location of ElasticSearch.

Is it possible to do something like this?

```
path.data: {ES_DIR}/../../data
```
</description><key id="12824064">2859</key><summary>How do you write a relative path in elasticsearch.yml?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hekevintran</reporter><labels /><created>2013-04-04T22:32:01Z</created><updated>2013-04-04T23:30:03Z</updated><resolved>2013-04-04T23:29:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hekevintran" created="2013-04-04T23:29:54Z" id="15930616">`path.data: ${path.home}/../../data`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Thread Pool: Update default settings (move from default cached to fixed)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2858</link><project id="" key="" /><description>The default thread pool settings for most thread pools is problematic in it being `cached` (for example, for `index`, `search`) since its unbounded in its thread creations. Change the default thread pool settings to follows:
- `index`: type: fixed, size: num_proc
- `bulk`: type: fixed, size: num_proc 
- `get`: type: fixed, size: num_proc
- `search`: type: fixed, size: num_proc \* 2, queue_size: 1000
- `percolate`: type: fixed, size: num_proc
- `management`: type: scaling, size: 5
- `flush`: type: scaling, size: halfProcMax5
- `merge` : type: scaling, size: halfProcMax5, 
- `refresh`: type: scaling, size: halfProcMax10
- `warmer`: type: scaling, size: halfProcMax5
</description><key id="12820471">2858</key><summary>Thread Pool: Update default settings (move from default cached to fixed)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-04-04T21:09:57Z</created><updated>2013-04-24T17:36:24Z</updated><resolved>2013-04-04T21:24:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow _meta in mappings on a field level</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2857</link><project id="" key="" /><description>A mapping can have a _meta element - but it only keeps it if declared on the top level.
When using the _meta to annotate a mapping, or keeping custom configuration on it, or using it for some binding - a lot of times besides the top level you also need more custom data on a field level. So you need the mapping to keep also _meta nodes defined under fields.
</description><key id="12815372">2857</key><summary>Allow _meta in mappings on a field level</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">rore</reporter><labels><label>discuss</label></labels><created>2013-04-04T19:09:25Z</created><updated>2015-02-04T17:56:19Z</updated><resolved>2014-07-25T08:47:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-25T08:47:17Z" id="50123397">I'd rather avoid an explosion of meta fields - easy enough to store this in the type-level meta.  Closing
</comment><comment author="awick" created="2014-12-05T15:24:41Z" id="65804316">Unfortunately this means you have to keep 2 things in sync, every time you add a field you have to modify the type meta.  If you have multiple apps that can add fields you have consistency issues, type meta isn't versioned or "partial" updatable.  Example: To add a field you only have to put the mapping of the 1 field and it will be merged, and optionally ignored if already there.  Now to update meta you'll have to fetch, change, put.

So it isn't actually "easy" to use type meta.
</comment><comment author="mindbits" created="2015-02-04T17:56:19Z" id="72904039">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Option to specify pretty=true as default</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2856</link><project id="" key="" /><description>Hello,

I was wondering if one could use the pretty display as the default display without having to append `?pretty=true` to the URL each time when using `curl`?
</description><key id="12806911">2856</key><summary>Option to specify pretty=true as default</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">callmekatootie</reporter><labels /><created>2013-04-04T16:00:43Z</created><updated>2013-04-04T17:52:58Z</updated><resolved>2013-04-04T17:52:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T17:52:58Z" id="15912891">No, this is not supported. And it shouldn't be supported. `pretty` is a debugging aid, and has an impact on performance. You should choose when to use it and not blindly enable it by default.

Have a look at using the Sense Chrome plugin instead of curl: https://chrome.google.com/webstore/detail/sense/doinijnbnggojdlcjifpdckfokbbfpbo
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Settings / Config: Allow to explicitly specify external environment variable syntax, in which case its optional</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2855</link><project id="" key="" /><description>For example, using: `setting.name:${env.SETTING_VALUE}` will load `SETTING_VALUE` from an environment variable, but will not fail if it does not exists (and treat the setting as if it wasn't set).
</description><key id="12802562">2855</key><summary>Settings / Config: Allow to explicitly specify external environment variable syntax, in which case its optional</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.7</label><label>v0.90.0.RC2</label></labels><created>2013-04-04T14:29:06Z</created><updated>2013-04-04T14:30:31Z</updated><resolved>2013-04-04T14:30:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Nested JSON key name problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2854</link><project id="" key="" /><description>Hi,
I had a problem about indexing nested json.My json is
{
    'post_id': '51935006105_435208019905968',
    'message': 'something',
    'actor_id': '123456789',
    'links': [
      {
        'url': 'some url',
        'detail': 'some details'
      },
      {
        'url': '_url',
        'detail': 'some details2'
      }
    ],
    'updated_time': 1365063260
  }

When I post it into elasticsearch it throws that error:
org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [links]
Caused by: org.elasticsearch.ElasticSearchIllegalArgumentException: unknown property [url]

Then I changed "links" part as : 
'urlLinks': [
      {
        'contentKey': 'some details',
        'urlKey': 'some url''
      }
    ]
and it works.My question is "url" or "links/link" keys are special for elasticsearch?
PS: Im very sure that Im indexing the first data, this is not a object type problem.
</description><key id="12791477">2854</key><summary>Nested JSON key name problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zaferOzdogru</reporter><labels /><created>2013-04-04T09:03:47Z</created><updated>2013-04-04T17:57:20Z</updated><resolved>2013-04-04T17:57:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-04T17:57:19Z" id="15913114">No they aren't special.  And I can index the data you presented above just fine:

```
curl -XPOST localhost:9200/test/test/ -d '
{
   "updated_time" : 1365063260,
   "actor_id" : "123456789",
   "post_id" : "51935006105_435208019905968",
   "links" : [
      {
         "detail" : "some details",
         "url" : "some url"
      },
      {
         "detail" : "some details2",
         "url" : "_url"
      }
   ],
   "message" : "something"
}
'
{"ok":true,"_index":"test","_type":"test","_id":"bsgkuBuuQgu2mT_k6tpw6A","_version":1}
```

It is likely that your existing mapping is clashing with the above record.  If you want further help on this, please ask in the forum

ta
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Extend mapping options for geo_point type with custom key names for lat and lon</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2853</link><project id="" key="" /><description>Currently we have to do some preprocessing of JSON messages and replace strings like: latitude and longitude with lat and lon. It would be convenient if a mapping can be made with custom keys for a geo_point field. For example:

"geo": {
  "type": "geo_point",
  "lat_key" : "latitude",
  "lon_key" : "longitude"
}
</description><key id="12791400">2853</key><summary>Extend mapping options for geo_point type with custom key names for lat and lon</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jethrobakker</reporter><labels /><created>2013-04-04T09:01:45Z</created><updated>2014-08-08T10:51:18Z</updated><resolved>2014-08-08T10:51:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sourcec0de" created="2013-05-29T18:06:28Z" id="18635059">I second this motion. Has anyone been able to achieve this

I have a document that looks like this

``` javascript
{
        "name":"PaceOfInterest",
         "location": {
               "cc": "US",
               "country": "United States",
               "state": "OR",
               "city": "Salem",
               "postalCode": "97303",
               "lng": -122.97516453272692,
               "lat": 45.04876295210979,
               "crossStreet": "in Pilot Travel Center",
               "address": "4220 Brooklake Rd NE"
            }
}
```

I want to be able to map
location.lng -&gt; geo_point.lon
location.lat -&gt; geo_point.lat
</comment><comment author="clintongormley" created="2014-08-08T10:51:18Z" id="51587512">You can now do this yourself with the source transform script. 

http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-transform.html#mapping-transform
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>API Update Tags Problem</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2852</link><project id="" key="" /><description>elasticsearch-0.20.5

---

curl localhost:9200/logstash-2013.03.28/xxxxx_log-type/qWtE27BWQuCsjBWUxoCxdg

============================&gt;

{"_index":"logstash-2013.03.28","_type":"xxxxx_log-type","_id":"qWtE27BWQuCsjBWUxoCxdg","_version":2,"exists":true, "_source" : {"@source":"file://xxxxx.xxxx.xxxx/opt/xxxx/xxxx/logs/xxxx.log","@tags":["multiline","grepped"],"@fields":{"timestamp":["2013-03-28 04:40:00.034"],"thread":["xxxxxxx_xxxxxxx-9"],"logseverity":["ERROR"],"class_name":["xxxxx.xxxxx.xxxxx.xxxxxxx.xxxxxx.xxxxxxxxx"],"message":["Error xxxxxx xxx xxxxx xxxxxxxx 'xxxxxx.xxxxxxx.xxxx.xxxxx.xxxxxxx.xxxxx@xxxxxxx': xxxxx xxxxxxx xxxxxxxx xxxxx."]},"@timestamp":"2013-03-28T11:40:01.003Z","@source_host":"xxxx.xxxxx.xxxx","@source_path":"/opt/xxxxxxx/xxxx/xxxx/xxxxxxxx.log","@message":"2013-03-28 04:40:00.034 [xxxxxxxxxxx_xxxxxxxx-9] ERROR xxxx.xxxx.xxxx.xxxx.xxx - Error xxxxx xxx xxxxx xxxxx 'xxxxxx.xxxxxxx.xxxx.xxxxx.xxxxxxx.xxxxx@xxxxxxx': xxxxxx xxxxx xxxxx xxxxxxxx xxxxx.\nxxxx.xxxxx.xxxxxxxxx: xxxxxxx xxxxxxxxxx xxxxxxxx xxxxxx.\n\tat xxxxxx.xxxxxxx.xxxx.xxxxx.xxxxxxx.xxxxx.xxxxxx(xxxxxxxx.java:354) ~[xxxxxx-xxxxxx-xxxx-1.1.1.jar:na]\n\tat xxxxxx.xxxxxxx.xxxx.xxxxx.xxxxxxx.xxxxx.xxxxxxxx(xxxxxxxx.java:210) [xxxx.xxxx.xxxx.xxxxx-1.1.1.jar:na]\n\tat xxxxxx.xxxxxxx.xxxx.xxxxx.xxxxxxx.xxxxx.xxxxxxxx(xxxxxxxxxxxxxx.java:72) [xxxx.xxxxxxxxxx.xxxxxx.xxxxxx-1.1.1.jar:na]\n\tat xxxx.xxxxxx.xxxxxxx.xxxxxx.xxxxxxxxxxxxxxx.xxxxxxxxxxx(xxxxxxxxxxxxxx.java:66) [xxx.xxxxxxx.xxxx.xxxxxx-1.1.1.jar:na]\n\tat sun.xxxxxxx.xxxxxxxxx.xxxxx(Unknown Source) ~[na:na]\n\tat sun.xxxxx.xxxxxxxxxxx.xxxxx(xxxxxxxxxxxxxx.java:43) ~[na:1.1.1_17]\n\tat xxxx.xxxx.xxxxx.xxxx.xxxxx(xxxxx.java:601) ~[na:1.1.1_17]\n\tat xxxx.xxxxx.xxxx.xxxxx.xxxxx(xxxxxx.java:273) [xxxx.xxxxxxxx.xxxx-1.1.1.RELEASE.jar:1.1.1.RELEASE]\n\tat xxx.xxxxxxxxxx.xxxxxxxx.xxxxx.xxxxxxxxxxxxxxx$xxxxxxxxxxxxxx.xxxxxxxxx(xxxxxxxxxxxxxxxx.java:264) [xxx.xxxxxxxxxxxxxxxx.xxxxxxxx.xxxxx-1.1.1.RELEASE.jar:1.1.1.RELEASE]\n\tat xxxxxx.xxxxxxxxxxx.xxxxxxx.xxxxxxx.xxxxxxxxx.xxxxxx(xxxxxxxx.java:86) [xxxxxx.xxxxxx.xxxxxx.xxxx-1.1.1.RELEASE.jar:1.1.1.RELEASE]\n\tat xxx.xxxxxx.xxxx.xxxxxx.xxxx(xxxxxx.java:202) [xxxxxxx-1.1.1.jar:1.1.1]\n\tat xxxx.xxxxxx.xxxx.xxxxxxxxxxx$xxxxxxx.xxx(xxxxxx.java:529) [xxxx-1.1.1.jar:1.1.1]","@type":"xxxxx_log-type","reported":"yes"}}

---

No mappings defined

---

curl -XPOST 'localhost:9200/logstash-2013.03.28/xxxxx_log-type/qWtE27BWQuCsjBWUxoCxdg/_update' -d '{
     "script" : "ctx._source.tags += tag",
     "params" : {
         "tag" : "reported"
     }
 }'

=========================&gt;

Execption:
{"error":"ElasticSearchIllegalArgumentException[failed to execute script]; nested: PropertyAccessException[[Error: could not access: tags; in class: java.util.LinkedHashMap]\n[Near : {... ctx._source.tags+tag ....}]\n             ^\n[Line: 1, Column: 1]]; ","status":400}
</description><key id="12790911">2852</key><summary>API Update Tags Problem</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KafKaf</reporter><labels /><created>2013-04-04T08:48:11Z</created><updated>2013-04-05T12:42:50Z</updated><resolved>2013-04-05T12:42:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-04T12:43:12Z" id="15894868">The field you want to add data is actually '@tags' and not 'tags' - however there seems to be another exception, when doing using the '@tags' field... I will take a look at it.
</comment><comment author="spinscale" created="2013-04-04T14:32:44Z" id="15900663">Hey, can you try this

```
curl -XPOST 'localhost:9200/logstash-2013.03.28/xxxxx_log-type/qWtE27BWQuCsjBWUxoCxdg/_update' -d '{
"script" : "ctx._source['@tags'] += tag",
"params" : {
"tag" : "reported"
}
}'
```

The `@` sign is a special char in MVEL... 
</comment><comment author="KafKaf" created="2013-04-04T15:32:05Z" id="15904456">I did try @tags before, forgot to mention it.
Your query is stuck for me, I stopped it after 3 min.
</comment><comment author="imotov" created="2013-04-05T01:50:30Z" id="15935223">I think what @spinscale meant is:

```
curl -XPOST 'localhost:9200/logstash-2013.03.28/xxxxx_log-type/qWtE27BWQuCsjBWUxoCxdg/_update' -d '{
    "script" : "ctx._source[\"@tags\"] += tag",
    "params" : {
        "tag" : "reported"
    }
}'
```
</comment><comment author="KafKaf" created="2013-04-05T12:42:39Z" id="15953467">My bad, didn't see this line: "The @ sign is a special char in MVEL..."

Escaping works :)

Thanks alot, sorry for all the trouble.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can't retrieve a value from an empty GeoPointValues while doing Geo Distance sorting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2851</link><project id="" key="" /><description>```
curl -XPOST http://localhost:9200/geo_test
curl -XPUT http://localhost:9200/geo_test/points/_mapping -d '{"points":{"properties":{"location":{"type":"geo_point"},"title":{"type":"string"}}}}'
curl -XPOST http://localhost:9200/geo_test/points/1 -d '{"title":"tester"}'
curl -XPOST http://localhost:9200/geo_test/points/2 -d '{"title":"tester","location":[30.0,30.0]}'
curl -XGET http://localhost:9200/geo_test/points/_search -d '{ "query": { "match_all": {} }, "sort": [ { "_geo_distance": { "location": [ 20, 20], "order": "asc" } }]}'
```

This works for 0.20.4, but not for 0.90RC1.
</description><key id="12755490">2851</key><summary>Can't retrieve a value from an empty GeoPointValues while doing Geo Distance sorting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lmenezes</reporter><labels><label>bug</label><label>regression</label><label>v0.90.0.RC2</label></labels><created>2013-04-03T14:41:20Z</created><updated>2013-04-03T15:37:22Z</updated><resolved>2013-04-03T15:25:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-04-03T15:00:26Z" id="15841745">The complete document with the missing geo point is omitted. This should be fixed. We should just use Double.MAX_VALUE as value, like what happened in 0.20.x
</comment><comment author="martijnvg" created="2013-04-03T15:25:53Z" id="15843272">Thanks for reporting this issue!
</comment><comment author="lmenezes" created="2013-04-03T15:37:22Z" id="15843995">nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Not found CzechStemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2850</link><project id="" key="" /><description>elasticsearch v0.20.6

curl -XPUT 'localhost:9200/projects/' -d '{
   "settings": {
     "analysis" : {
            "analyzer" : {
                "mya" : {
                    "type" : "snowball",
                    "language" : "Czech"
                }
            }
        }
   }
}'

curl 'localhost:9200/projects/_analyze?text=kocka&amp;pretty=1&amp;analyzer=mya'
{
  "error" : "IllegalArgumentException[Invalid stemmer class specified: Czech]; nested: ClassNotFoundException[org.tartarus.snowball.ext.CzechStemmer]; ",
  "status" : 500
}
</description><key id="12753949">2850</key><summary>Not found CzechStemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">JosefJezek</reporter><labels /><created>2013-04-03T14:07:47Z</created><updated>2013-04-08T06:22:55Z</updated><resolved>2013-04-08T06:22:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-03T15:28:13Z" id="15843425">Hey,

from the documentation at http://www.elasticsearch.org/guide/reference/index-modules/analysis/snowball-analyzer/

```
The language parameter can have the same values as the snowball filter and defaults to English. 
```

However, on the token filter at http://www.elasticsearch.org/guide/reference/index-modules/analysis/snowball-tokenfilter/ there is no support for czech listed.

You might want to use the stemmer token filter at http://www.elasticsearch.org/guide/reference/index-modules/analysis/stemmer-tokenfilter/ - which has support for czech.

Hope this helps. If so, please close this, as it does not look like a bug to me.
</comment><comment author="lukas-vlcek" created="2013-04-03T16:45:58Z" id="15848444">Ahoj,

mo&#382;n&#225; pom&#367;&#382;e [tohle](https://gist.github.com/lukas-vlcek/4673027) kde jsou podle m&#283; asi v&#353;echny mo&#382;n&#233; kombinace konfigurace &#269;e&#353;tiny (v podstat&#283; se jedn&#225; po&#345;&#225;d o to sam&#233; dokola, akor&#225;d je v&#237;ce zp&#367;sob&#367;, co jak nastavit).

Co se &#269;e&#353;tiny t&#253;&#269;e, tak ur&#269;it&#233; mo&#382;nosti je&#353;t&#283; nab&#237;z&#237; i [hunspell plugin](https://github.com/jprante/elasticsearch-analysis-hunspell), kter&#225; pou&#382;&#237;v&#225; OS slovn&#237;ky, ale co si vzpom&#237;n&#225;m, tak zrovna &#269;e&#353;tina moc dob&#345;e udr&#382;ovan&#225; nen&#237;.
</comment><comment author="spinscale" created="2013-04-08T06:22:55Z" id="16034471">Closing this, doesnt seem like an issue to me. If you have questions, please use the google group (more eyes, better chance of getting a good response).
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Javascript plugin seems broken</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2849</link><project id="" key="" /><description>In 0.20.6, with the 1.3.0 JS plugin, I'm struggling to get anything to work. Perhaps I'm misunderstanding?

For instance, I'm trying to return a simple value as a script field. Any of the following requests throws a similar error (below):

```
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "script_fields" : {
      "foo" : {
         "script" : "return 5",
         "lang" : "js"
      }
   }
}
'
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "script_fields" : {
      "foo" : {
         "script" : "return 5.0",
         "lang" : "js"
      }
   }
}
'
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "script_fields" : {
      "foo" : {
         "script" : "return \u0027foo\u0027",
         "lang" : "js"
      }
   }
}
'
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "script_fields" : {
      "foo" : {
         "script" : "return \"foo\"",
         "lang" : "js"
      }
   }
}
'

'

[2013-04-03 12:22:06,596][DEBUG][action.search.type       ] [Ord] [test][1], node[SRNRWFu0QS2JTeRrD1-lrw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3b25ad0d]
org.elasticsearch.search.SearchParseException: [test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{
   "script_fields" : {
      "foo" : {
         "script" : "return 5",
         "lang" : "js"
      }
   }
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:566)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:481)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:466)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:236)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: org.mozilla.javascript.EvaluatorException: invalid return (Script12.js#1)
    at org.mozilla.javascript.DefaultErrorReporter.runtimeError(DefaultErrorReporter.java:77)
    at org.mozilla.javascript.DefaultErrorReporter.error(DefaultErrorReporter.java:64)
    at org.mozilla.javascript.Parser.addError(Parser.java:188)
    at org.mozilla.javascript.Parser.addError(Parser.java:166)
    at org.mozilla.javascript.Parser.reportError(Parser.java:223)
    at org.mozilla.javascript.Parser.reportError(Parser.java:210)
    at org.mozilla.javascript.Parser.reportError(Parser.java:203)
    at org.mozilla.javascript.Parser.returnOrYield(Parser.java:1623)
    at org.mozilla.javascript.Parser.statementHelper(Parser.java:999)
    at org.mozilla.javascript.Parser.statement(Parser.java:901)
    at org.mozilla.javascript.Parser.parse(Parser.java:540)
    at org.mozilla.javascript.Parser.parse(Parser.java:478)
    at org.mozilla.javascript.Context.compileImpl(Context.java:2348)
    at org.mozilla.javascript.Context.compileString(Context.java:1335)
    at org.mozilla.javascript.Context.compileString(Context.java:1324)
    at org.elasticsearch.script.javascript.JavaScriptScriptEngineService.compile(JavaScriptScriptEngineService.java:90)
    at org.elasticsearch.script.ScriptService.compile(ScriptService.java:175)
    at org.elasticsearch.script.ScriptService.search(ScriptService.java:193)
    at org.elasticsearch.search.fetch.script.ScriptFieldsParseElement.parse(ScriptFieldsParseElement.java:73)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:554)
    ... 11 more
```
</description><key id="12746292">2849</key><summary>Javascript plugin seems broken</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-04-03T10:26:11Z</created><updated>2013-04-04T17:59:04Z</updated><resolved>2013-04-04T17:59:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-03T10:28:54Z" id="15829354">Wrong version of JS plugin - closing
</comment><comment author="clintongormley" created="2013-04-03T10:32:27Z" id="15829496">Actually, in 0.20.6 with JS plugin 1.2.0, i'm still getting parse errors. What's going on here?
</comment><comment author="clintongormley" created="2013-04-03T10:34:18Z" id="15829557">Ah, it might be because i'm using `return` outside a function.  Is there any chance of being able to see the parse error from the JS engine, rather than the Java stacktrace which is meaningless here?
</comment><comment author="clintongormley" created="2013-04-04T17:59:04Z" id="15913234">It seems the error is there, just hidden in the flood of stack trace
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Uncaught exception in javascript</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2848</link><project id="" key="" /><description>When using the JS plugin in 0.20.6, the following request triggers an uncaught exception, which never returns:

```
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "script_fields" : {
      "num" : {
         "script" : "1/doc[\u0027num\u0027].value",
         "lang" : "js"
      }
   }
}
'

Exception in thread "elasticsearch[Stunner][search][T#4]" java.lang.AbstractMethodError
    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:70)
    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:250)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:438)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:345)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:149)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:136)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
```
</description><key id="12745892">2848</key><summary>Uncaught exception in javascript</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.20.7</label><label>v0.90.0.RC2</label></labels><created>2013-04-03T10:15:02Z</created><updated>2013-04-03T10:39:38Z</updated><resolved>2013-04-03T10:39:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-03T10:28:39Z" id="15829341">Wrong version of JS plugin - closing
</comment><comment author="s1monw" created="2013-04-03T10:28:40Z" id="15829344">doh... there are some catch(Exception) rather than catch(Throwable) I will fix
</comment><comment author="s1monw" created="2013-04-03T10:29:10Z" id="15829363">nah this should return and not just die...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow contributors add to documentation with comments ....</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2847</link><project id="" key="" /><description>or something similar, maybe the way apidock.com does it?
There are a bunch of gaps in the elasticsearch documentation (necessarily so, the writer can't think of every possible need of a reader), places where there isn't a lot of detail, or a corner case is overlooked.

It would be great if people could help others out if they've spent hours dealing with something. 

I'm happy to help out with this if need be.
</description><key id="12721417">2847</key><summary>Allow contributors add to documentation with comments ....</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">ike-bloomfire</reporter><labels /><created>2013-04-02T19:34:49Z</created><updated>2014-01-12T15:13:46Z</updated><resolved>2014-01-12T15:13:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-17T12:27:08Z" id="21109211">We are working on better documentation (especially for starters) - but of course there are still/always gaps. Are you unsatisfied with the possibiliy of just creating pull requests in the https://github.com/elasticsearch/elasticsearch.github.com repository?
</comment><comment author="clintongormley" created="2014-01-12T15:13:43Z" id="32124607">No more feedback. Closing this issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Location of lang attribute when using native scripts with custom_filter_query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2846</link><project id="" key="" /><description>Currently the `lang` attribute has to be on the same level as `query` and `filters`. (See http://www.elasticsearch.org/guide/reference/query-dsl/custom-filters-score-query/)

```
{
    "query": {
        "custom_filters_score": {
            "query": { &#8230; },
            "lang" : "native"
            "filters": [
                {
                    "filter": { &#8230; },
                    "script": "nativescript"
                }
            ]
        }
    }
}
```

This means that _all_ scripts for the custom_filter_query are in that language. Surely something like this would make more sense to allow the user to combine mvel and native scripts:

```
{
    "query": {
        "custom_filters_score": {
            "query": { &#8230; },
            "filters": [
                {
                    "filter": { &#8230; },
                    "lang" : "native",
                    "script": "nativescript"
                }
            ]
        }
    }
}
```

Alternatively one could nest the `lang` inside the `script` like so:

```
{
    "query": {
        "custom_filters_score": {
            "query": { &#8230; },
            "filters": [
                {
                    "filter": { &#8230; },
                    "script": {
                        "lang" : "native",
                        "script" : "nativescript"
                    }
                }
            ]
        }
    }
}
```
</description><key id="12704980">2846</key><summary>Location of lang attribute when using native scripts with custom_filter_query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">sqwk</reporter><labels /><created>2013-04-02T13:32:45Z</created><updated>2014-08-08T10:49:56Z</updated><resolved>2014-08-08T10:49:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T10:49:56Z" id="51587408">Custom filter score has been removed. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow to update index/timestamp/size field mapper while running</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2845</link><project id="" key="" /><description>This is the follow up pull request for issue #2136, which implements merging and enabling/disabling of field mappers at runtime (index, timestamp and size field mapper in particular).
</description><key id="12703107">2845</key><summary>Allow to update index/timestamp/size field mapper while running</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-04-02T12:41:22Z</created><updated>2014-07-16T21:53:42Z</updated><resolved>2013-04-04T13:20:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-04T13:20:18Z" id="15896483">Forgot to close. Landed in master with commit 230cbd3448df30158ff53fa9b4104cc47900fdeb
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix for #2837 by checking if all the indices were missing and failing that.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2844</link><project id="" key="" /><description>setting ignore_indices=missing will fail your query if all the indices are missing as opposed to making your query run against everything.. fixes #2837
</description><key id="12701025">2844</key><summary>Fix for #2837 by checking if all the indices were missing and failing that.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahdeto</reporter><labels /><created>2013-04-02T11:33:15Z</created><updated>2014-07-16T21:53:42Z</updated><resolved>2013-04-08T07:10:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mahdeto" created="2013-04-08T07:10:09Z" id="16035672">changes and tests already included with martijnvg's fix here https://github.com/elasticsearch/elasticsearch/commit/cf00acf5b048608e3d184cc5cc6876ef0af356fb for issue https://github.com/elasticsearch/elasticsearch/issues/2837
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field Data: Simplify field data cache settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2843</link><project id="" key="" /><description>The new 0.90 supports new field data cache settings, including a "node" level cache, that works across all shards allocated on that node. In order to use it, one need to opt into using it, and then configure it. We can simplify it.

By default, we can always use the "node" level cache, but simply have it default to "resident" or unbounded. Once actual bounds need to be set, all that is needed is to set `indices.fielddata.cache.size` and/or `indices.fielddata.cache.expire`.
</description><key id="12699361">2843</key><summary>Field Data: Simplify field data cache settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-04-02T10:36:05Z</created><updated>2013-05-10T15:32:24Z</updated><resolved>2013-04-02T10:44:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kovyrin" created="2013-05-10T15:18:50Z" id="17726299">is it "index.fielddata.cache.size" (from the docs) or "indices.fielddata.cache.size"?
</comment><comment author="clintongormley" created="2013-05-10T15:29:00Z" id="17726933">It looks like it is the `index.fielddata.cache.size` version: https://github.com/elasticsearch/elasticsearch/commit/31d1e6cfe77dc4e4883b293969b5e82253cbc4e1#L0R165
</comment><comment author="kimchy" created="2013-05-10T15:32:24Z" id="17727136">its `indices.fielddata.cache.size` actually.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add filter option to term suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2842</link><project id="" key="" /><description>Add optional `filter` option to term suggester, that only allows suggest options that match with the filter.
</description><key id="12695925">2842</key><summary>Add filter option to term suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels /><created>2013-04-02T08:46:37Z</created><updated>2014-08-08T10:49:17Z</updated><resolved>2014-08-08T10:49:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="julianhille" created="2013-10-23T09:31:48Z" id="26891473">What exactly is meant with that?
1. have some kind of regex, script etc filter which filters out suggestions after these are generated and reduce them
OR
2. pre filter the documents on which the suggestions are based or even only to a "_type" in the index?
</comment><comment author="nik9000" created="2013-10-25T12:32:04Z" id="27086863">Similar but not a duplicate of #3482.
</comment><comment author="martijnvg" created="2013-10-28T17:43:47Z" id="27236162">@julianhille This means that a term will only be suggested, if the documents that have this term also match with the provided filter. At least one of the term's document need to be in the filter. 

This can best be seen as a kind of post filtering of terms. A `_type` in the index could be turn into a filter.
</comment><comment author="julianhille" created="2013-10-28T23:26:37Z" id="27266676">@martijnvg  then this is exactly what i need. Also for the autocomplete. I was unsure about the type of filter (pre, post) because both is named filter or known in the filter context. Sorry if that was clear to anyone else.
</comment><comment author="bigerock" created="2014-07-30T12:53:47Z" id="50609812">i need this as well. i think the suggestions should be responsive to your overall query including filters, etc. i'm not sure where it would make sense to suggest terms outside of these parameters. for example, if i have a movies index and i am asking for children's movies (rated G), and i search on 'frozen', i don't want the suggester to suggest some rated R movie that also happens to contain the word 'frozen'.
</comment><comment author="clintongormley" created="2014-07-30T12:58:27Z" id="50610320">@bigerock Why not use the phrase suggester instead? It does everything that the term suggester does, and more. And also supports filtering - see #3482 
</comment><comment author="bigerock" created="2014-07-30T13:03:20Z" id="50610862">apologies, pius sent me to this link. i am actually already using a phrase suggester but it's not adhering to my filters. perhaps there's something wrong with the structure of my query.
</comment><comment author="ppf2" created="2014-07-30T16:18:33Z" id="50638531">@bigerock I think it is not working in your case because NEST does not yet support this (https://github.com/elasticsearch/elasticsearch-net/issues/824).  Will chat more in our existing ZD ticket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cannot reset index.cache.field.expire with -1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2841</link><project id="" key="" /><description>According to the docs http://www.elasticsearch.org/guide/reference/index-modules/cache/ you should be able to reset `index.cache.field.expire` using `-1`.

However, setting -1 just causes an error to be thrown:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "settings" : {
      "cache.field.expire" : -1
   }
}
'

IndexCreationException[[test] failed to create index]; nested: IllegalArgumentException[duration cannot be negative: -1000000 NANOSECONDS]; 
```

A value of -1 should be caught here: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/cache/query/parser/resident/ResidentQueryParserCache.java#L57
</description><key id="12694909">2841</key><summary>Cannot reset index.cache.field.expire with -1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-04-02T08:12:37Z</created><updated>2013-05-13T08:26:45Z</updated><resolved>2013-05-13T08:26:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-02T09:36:47Z" id="15765582">Also, should this index setting be dynamic? It works dynamically in 0.20.\* but not in 0.90.*
</comment><comment author="jgagnon1" created="2013-05-10T14:08:13Z" id="17722102">+1 we cannot reset it to default ?
</comment><comment author="clintongormley" created="2013-05-13T08:26:45Z" id="17798344">As of 0.90, this setting no longer exists. It has been replaced by a node-level setting which is only applied at node startup.  See: http://www.elasticsearch.org/guide/reference/index-modules/fielddata/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to build with JDK 1.6</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2840</link><project id="" key="" /><description>0.2.4 worked fine, trying to build 0.2.6 and unable to use JDK 1.6. Switching to building with 1.7 works. Was this an expected change?

Error:

```
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project elasticsearch: Compilation failure: Compilation failure:
[ERROR] /data/users/macvicar/elasticsearch/src/main/java/jsr166e/ConcurrentHashMapV8.java:[4103,28] &lt;K,V,U&gt;reduce(jsr166e.ConcurrentHashMapV8&lt;K,V&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;? super K,? super V,? extends U&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;? super U,? super U,? extends U&gt;) in jsr166e.ConcurrentHashMapV8.ForkJoinTasks cannot be applied to (jsr166e.ConcurrentHashMapV8&lt;K,V&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;capture#247 of ? super K,capture#88 of ? super V,capture#232 of ? extends U&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;capture#708 of ? super U,capture#916 of ? super U,capture#484 of ? extends U&gt;)
[ERROR] /data/users/macvicar/elasticsearch/src/main/java/jsr166e/ConcurrentHashMapV8.java:[4240,28] cannot find symbol
[ERROR] symbol  : method reduceKeys(jsr166e.ConcurrentHashMapV8&lt;K,V&gt;,jsr166e.ConcurrentHashMapV8.Fun&lt;capture#627 of ? super K,capture#678 of ? extends U&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;capture#937 of ? super U,capture#637 of ? super U,capture#395 of ? extends U&gt;)
[ERROR] location: class jsr166e.ConcurrentHashMapV8.ForkJoinTasks
[ERROR] /data/users/macvicar/elasticsearch/src/main/java/jsr166e/ConcurrentHashMapV8.java:[4375,28] cannot find symbol
[ERROR] symbol  : method reduceValues(jsr166e.ConcurrentHashMapV8&lt;K,V&gt;,jsr166e.ConcurrentHashMapV8.Fun&lt;capture#978 of ? super V,capture#790 of ? extends U&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;capture#169 of ? super U,capture#183 of ? super U,capture#236 of ? extends U&gt;)
[ERROR] location: class jsr166e.ConcurrentHashMapV8.ForkJoinTasks
[ERROR] /data/users/macvicar/elasticsearch/src/main/java/jsr166e/ConcurrentHashMapV8.java:[4511,28] cannot find symbol
[ERROR] symbol  : method reduceEntries(jsr166e.ConcurrentHashMapV8&lt;K,V&gt;,jsr166e.ConcurrentHashMapV8.Fun&lt;java.util.Map.Entry&lt;K,V&gt;,capture#994 of ? extends U&gt;,jsr166e.ConcurrentHashMapV8.BiFun&lt;capture#749 of ? super U,capture#219 of ? super U,capture#930 of ? extends U&gt;)
[ERROR] location: class jsr166e.ConcurrentHashMapV8.ForkJoinTasks
[ERROR] -&gt; [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project elasticsearch: Compilation failure
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:213)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:84)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:59)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.singleThreadedBuild(LifecycleStarter.java:183)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:161)
    at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:320)
    at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:156)
    at org.apache.maven.cli.MavenCli.execute(MavenCli.java:537)
    at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:196)
    at org.apache.maven.cli.MavenCli.main(MavenCli.java:141)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:290)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:230)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:409)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:352)
Caused by: org.apache.maven.plugin.CompilationFailureException: Compilation failure
    at org.apache.maven.plugin.AbstractCompilerMojo.execute(AbstractCompilerMojo.java:656)
    at org.apache.maven.plugin.CompilerMojo.execute(CompilerMojo.java:128)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:101)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:209)
```
</description><key id="12693845">2840</key><summary>Unable to build with JDK 1.6</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scottmac</reporter><labels /><created>2013-04-02T07:30:17Z</created><updated>2013-04-02T15:16:45Z</updated><resolved>2013-04-02T15:13:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-04-02T12:21:01Z" id="15771767">Building it with 1.6.0 update 43 works well, thats what we used to build it.
</comment><comment author="scottmac" created="2013-04-02T15:11:29Z" id="15781078">Our default version on servers is "1.6.0_07", pulled in 1.6.0_29 and it worked fine.

Not sure if its worth setting a minimum java version in the build file?
</comment><comment author="kimchy" created="2013-04-02T15:12:43Z" id="15781142">@scottmac I am not familiar with an option to set the minimum update version in a build file (maven). In any case, its highly recommended to use newer java (1.6) to both build and run ES. update 7 is really old...
</comment><comment author="scottmac" created="2013-04-02T15:16:45Z" id="15781390">Oh I'm aware it's old, I think it was the default on centos. We run it with something much newer, just took me a little while to work out what changes.

http://maven.apache.org/enforcer/maven-enforcer-plugin/ for version enforcing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Weird highlighting when using plain highlighting with min_ngram set to 1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2839</link><project id="" key="" /><description>This problem has been in every version of elasticsearch I've tried
17.6, 19.2 and I just tried it again in 20.2 (its the highest I can install with brew on OS X)

Fully outlined here http://stackoverflow.com/questions/13750330/elasticsearch-highlighting-on-ngram-filter-is-wierd-if-min-gram-is-set-to-1/15005321#15005321

Wasn't sure it was a bug at first, but its looking more and more like it is. Just wanted to file it just in case.
</description><key id="12682042">2839</key><summary>Weird highlighting when using plain highlighting with min_ngram set to 1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ike-bloomfire</reporter><labels><label>:Highlighting</label><label>bug</label></labels><created>2013-04-01T23:29:52Z</created><updated>2016-11-24T18:15:40Z</updated><resolved>2016-11-24T18:15:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-04-06T06:03:34Z" id="15991471">thanks for reporting this so detailed. I will hopefully look into it soon.
</comment><comment author="clintongormley" created="2016-11-06T09:42:49Z" id="258667572">This is another case where the plain highlighter works better than FVH.  Recreation for 5.0:

```
PUT /myindex
{
  "settings":{
    "index":{
      "analysis":{
        "analyzer":{
          "default":{
            "type":"custom",
            "tokenizer":"my_ngram"
          }
        },
        "tokenizer":{
          "my_ngram":{
            "type":"nGram",
            "min_gram":2,
            "max_gram":20
          }
        }
      }
    }
  },
  "mappings": {
    "doc": {
      "properties": {
        "text": {
          "type": "text",
          "term_vector": "with_positions_offsets"
        }
      }
    }
  }
}

POST myindex/doc
{
  "text": "xyz post"
}

```

The FVH highlighter returns `&lt;em&gt;xy&lt;/em&gt;z post`

```
GET myindex/_search
{
  "query": {
    "query_string": {
      "query": "text:xyz",
      "default_operator": "AND"
    }
  },
  "highlight": {
    "fields": {
      "text": {}
    }
  }
}

```

The plain highlighter returns `&lt;em&gt;xyz&lt;/em&gt; post`:

```
GET myindex/_search
{
  "query": {
    "query_string": {
      "query": "text:xyz",
      "default_operator": "AND"
    }
  },
  "highlight": {
    "fields": {
      "text": {
        "type": "plain"
      }
    }
  }
}

```
</comment><comment author="clintongormley" created="2016-11-24T18:15:40Z" id="262827921">Closing in favour of #21621</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.90 facet memory issue?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2838</link><project id="" key="" /><description>Was hoping that 0.90 would fix the facet memory issue.  30 node x 20G cluster with 7 billion documents

I have "index.cache.field.type" : "soft"

The first time the cluster approaches the max memory I see a huge free and the nodes go from 20G to 12G.  As it approaches the max memory in the future it frees less and less and GC takes longer and longer.  Seems like there is something still holding on.  Should I be using weak instead of soft?

[2013-04-01 18:31:02,536][INFO ][monitor.jvm              ] [moloches-m01b] [gc][ConcurrentMarkSweep][352206][7681] duration [6s], collections [1]/[6.6s], total [6s]/[43.3m], memory [19.8gb]-&gt;[19.6gb]/[19.8gb], all_pools {[Code Cache] [10.6mb]-&gt;[10.6mb]/[48mb]}{[Par Eden Space] [865.3mb]-&gt;[704.4mb]/[865.3mb]}{[Par Survivor Space] [59.1mb]-&gt;[0b]/[108.1mb]}{[CMS Old Gen] [18.9gb]-&gt;[18.9gb]/[18.9gb]}{[CMS Perm Gen] [36.1mb]-&gt;[36.1mb]/[82mb]}

[2013-04-01 18:31:20,602][INFO ][monitor.jvm              ] [moloches-m01b] [gc][ConcurrentMarkSweep][352213][7688] duration [6s], collections [1]/[6s], total [6s]/[43.5m], memory [19.8gb]-&gt;[19.6gb]/[19.8gb], all_pools {[Code Cache] [10.6mb]-&gt;[10.6mb]/[48mb]}{[Par Eden Space] [865.3mb]-&gt;[697.8mb]/[865.3mb]}{[Par Survivor Space] [104.8mb]-&gt;[0b]/[108.1mb]}{[CMS Old Gen] [18.9gb]-&gt;[18.9gb]/[18.9gb]}{[CMS Perm Gen] [36.1mb]-&gt;[36.1mb]/[82mb]

Here is the top 60 jmap -histo

https://gist.github.com/awick/5288313
</description><key id="12680046">2838</key><summary>0.90 facet memory issue?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2013-04-01T22:35:08Z</created><updated>2014-03-12T16:01:42Z</updated><resolved>2014-03-12T16:01:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-04-01T22:40:54Z" id="15741816">The mentioned setting `index.cache.field.type` is no longer applicable in 0.90. The field data is loaded to memory is now controlled using different settings, called `index.fielddata.cache` which defaults to `resident`. You can set it to `soft` to retain the old usage behavior, or you can set it to:

```
index.fielddata.cache: node
indices.fielddata.cache.size: 30%
```

(the `30%` can also be an explicit size, like `12gb`)

In which case it will use 30% of the heap for the field data. Note, reloading the field data that does not fit memory will be expensive, though the above will be better compared to soft.
</comment><comment author="devoncrouse" created="2013-04-01T22:43:59Z" id="15741942">Hi Andy,

We ran into the same behavior with 6 nodes x 72GB heap (128GB physical).  The G1 collector with -XX:MaxGCPauseMillis=50 seems to have solved our GC pauses, though Shay has recommended against it in a post I found earlier this year due to reported instability.  Your mileage may vary, I suppose...
</comment><comment author="awick" created="2013-04-02T00:49:15Z" id="15746740">soft didn't work very well, but I switched to 50% and working much better!

2 possible issues, when I fetch the _settings
a) the old index.cache.field.type is still there.  Might be nice to not have it show up in 0.90 since it isn't used
b) even though I set with "indices.fielddata.cache.size" when you retrieve _settings it shows up with a leading "index.", not sure if that is on purpose or not.  "index.indices.fielddata.cache.size" : "50%"
</comment><comment author="awick" created="2013-04-02T11:10:16Z" id="15769099">Maybe I'm just confused, should these two settings be per index or in the yml file?  Right now I have them per index.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ignore_indices doesn't fail if all the indices requested are not present</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2837</link><project id="" key="" /><description>ignore_indices=missing doesn't fail if all the indices requested are not present, it defaults to searching everything which is far from desirable, specially when doing a heavy facet (e.g. termstats) in a daily index case (e.g. logs_2013-04-01) and there is no data indexed for the day

To reproduce start a node and index any document to index "A" then curl this:

curl 'http://localhost:9200/B,C/_search?ignore_indices=missing&amp;pretty'

Tried on 0.20.6 this returns the document just indexed to index "A".

If that is the expected behavior for the 'missing' value then I suggest adding another value to ignore_indices, for example "ignore_indices=missing_some" that will fail if none of the queried indices are there
</description><key id="12656025">2837</key><summary>ignore_indices doesn't fail if all the indices requested are not present</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahdeto</reporter><labels><label>breaking</label><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-04-01T09:05:13Z</created><updated>2013-10-02T23:53:42Z</updated><resolved>2013-04-02T17:06:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-04-02T12:19:29Z" id="15771709">This isn't expected behaviour. I think the best thing if none of the specified indices exists, is to return an error.
</comment><comment author="martijnvg" created="2013-04-02T17:08:07Z" id="15788731">@mahdeto Thanks for reporting this issue! The fix (based on your fix) will be included in the next release.
</comment><comment author="mahdeto" created="2013-04-02T17:52:22Z" id="15791232">Sweet :) glad I could help :) this was my first contrib to ES and hopefully
won't be the last.

On Tue, Apr 2, 2013 at 7:08 PM, Martijn van Groningen &lt;
notifications@github.com&gt; wrote:

&gt; @mahdeto https://github.com/mahdeto Thanks for reporting this issue!
&gt; The fix (based on your fix) will be included in the next release.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2837#issuecomment-15788731
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ignore_indices doesn't fail if all the indices found are not there</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2836</link><project id="" key="" /><description>ignore_indices=missing doesn't fail if all the indices found are not there, it defaults to searching everything which is far from desirable, specially when doing a heavy facet (e.g. termstats) in a daily index case (e.g. logs_2013-04-01) and there is no data indexed for the day (i.e. the day doesn't exist)

To reproduce go to any node that has any documents indexed and browse to this:
</description><key id="12655949">2836</key><summary>ignore_indices doesn't fail if all the indices found are not there</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mahdeto</reporter><labels /><created>2013-04-01T09:00:46Z</created><updated>2013-04-01T09:06:03Z</updated><resolved>2013-04-01T09:06:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>{sort: "field"} throws misleading errors</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2835</link><project id="" key="" /><description>eg:

```
curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "sort" : "_score",
   "query" : {
      "match_all" : {}
   }
}
'
```

throws the error: 

```
ElasticSearchIllegalArgumentException[sort option [match_all] not supported]
```

Either we should support `{ sort: "string"}` or we should complain that an array hasn't been passed to `sort`
</description><key id="12629271">2835</key><summary>{sort: "field"} throws misleading errors</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-03-30T12:11:57Z</created><updated>2013-03-30T12:46:55Z</updated><resolved>2013-03-30T12:46:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-30T12:38:35Z" id="15673962">We can easily support this format, and also give better failure if we get the wrong format....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Scripts not casting integers to doubles</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2834</link><project id="" key="" /><description>When running a calculation on an integer which results in a double, v0.20.6 returns the double correctly, while v0.90.0.RC1 leaves it as an integer, and returns the incorrect value:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "num" : {
               "type" : "integer"
            }
         }
      }
   }
}
'
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "num" : 8
}
'

curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "script_fields" : {
      "num" : {
         "script" : "1 / doc[\u0027num\u0027].value "
      }
   }
}
'
```

In version 0.20.6:

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "fields" : {
#                "num" : 0.125
#             },
#             "_index" : "test",
#             "_id" : "CyF_z9ZJT1-NyYRAxjNJxw",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```

In version 0.90.0.RC1:

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "fields" : {
#                "num" : 0
#             },
#             "_index" : "test",
#             "_id" : "csZwbG85TqyqOKfg8A5j2g",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 150
# }
```
</description><key id="12629256">2834</key><summary>Scripts not casting integers to doubles</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0.RC1</label><label>v0.90.0.RC2</label></labels><created>2013-03-30T12:09:29Z</created><updated>2013-04-03T14:08:19Z</updated><resolved>2013-04-03T14:08:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-02T15:50:47Z" id="15783743">Hey Clint,

not sure if this is actually a bug or expected behaviour. You are dividing two integers, which results in an integer. If you divided 1.0 / doc.num the result is as expected and returns a float. This is how java caclulations works, but might not be what is expected by the user...
</comment><comment author="clintongormley" created="2013-04-02T16:20:39Z" id="15785719">Heya @spinscale 

Two things: one, we're talking about scripting here, where you expect such stuff to be handled for you, and two, it used to work correctly, pre 0.90.

It may be that working correctly previously was just by accident - maybe i didn't check it correctly?

But certainly, it returning an integer was a surprise to me - not what i expected

c
</comment><comment author="spinscale" created="2013-04-03T08:36:11Z" id="15824756">FYI: The commit f5331c953518f747bee45dd952df05aea929a230 introduced the faulty behaviour. Maybe Simon knows more...
</comment><comment author="s1monw" created="2013-04-03T09:10:19Z" id="15826096">what is happening here is that we moved to long vs int in the interface. if you use a long in the mapping this should not work in 0.20.6 either. IMO the behaviour here is now correct while it was buggy in 0.20.6. 
</comment><comment author="s1monw" created="2013-04-03T14:08:06Z" id="15838596">I flagged this as feature / enhancement ;) This is really an inconsistency in the mvel language trying to be smart. I can  buy into this if it is consistent but if it depends on the type we have in the mapping we should rather break bw compat than trying to fix this. I will close...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>index.blocks.read_only prevents _status</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2833</link><project id="" key="" /><description>In trying to protect completed daily indices from future changes, we tried setting index.blocks.read_only : true, but found that we were then unable to query _status, and head  plugin could no longer display shard information.  I think this best illustrates the behavior we're seeing, but not expecting:

curl -XGET 'http://localhost:9200/messages_20130301/_status'
{"error":"ClusterBlockException[blocked by: [FORBIDDEN/5/index read-only (api)];]","status":403}

Also, as an aside: what's the difference between index.blocks.read_only : true and index.blocks.write : true (which lets me query status and prevents delete?)
</description><key id="12612187">2833</key><summary>index.blocks.read_only prevents _status</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/tlrx/following{/other_user}', u'events_url': u'https://api.github.com/users/tlrx/events{/privacy}', u'organizations_url': u'https://api.github.com/users/tlrx/orgs', u'url': u'https://api.github.com/users/tlrx', u'gists_url': u'https://api.github.com/users/tlrx/gists{/gist_id}', u'html_url': u'https://github.com/tlrx', u'subscriptions_url': u'https://api.github.com/users/tlrx/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/642733?v=4', u'repos_url': u'https://api.github.com/users/tlrx/repos', u'received_events_url': u'https://api.github.com/users/tlrx/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/tlrx/starred{/owner}{/repo}', u'site_admin': False, u'login': u'tlrx', u'type': u'User', u'id': 642733, u'followers_url': u'https://api.github.com/users/tlrx/followers'}</assignee><reporter username="">devoncrouse</reporter><labels><label>:Cluster</label><label>bug</label><label>v1.6.0</label><label>v2.0.0-beta1</label></labels><created>2013-03-29T17:31:52Z</created><updated>2015-04-23T13:23:45Z</updated><resolved>2015-04-23T13:23:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2013-08-15T00:09:39Z" id="22677078">+1 on that. 
any plans on fixing this soon? even though i'm not really using this setting, i'm developing an admin tool that allows changing index settings and this just breaks everything.
</comment><comment author="lmenezes" created="2013-08-15T00:16:44Z" id="22677356">just as a side note, blocks.metadata also prevents _status from working correctly.
</comment><comment author="eon01" created="2014-07-27T14:45:47Z" id="50266186">No fix for this till now ?
</comment><comment author="clintongormley" created="2014-11-29T14:20:08Z" id="64953264">Related #8102, #5855, #5876
</comment><comment author="tlrx" created="2015-04-23T13:23:43Z" id="95581444">Closed in af79a2ae49073cd18d415d3c7bc4a798c6ffd9f1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Optimize aliases processing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2832</link><project id="" key="" /><description>A large number of aliases may result in noticeable delays during index recovery. 
</description><key id="12606726">2832</key><summary>Optimize aliases processing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>enhancement</label><label>v0.20.7</label><label>v0.90.0.RC2</label></labels><created>2013-03-29T14:46:39Z</created><updated>2013-04-02T09:16:32Z</updated><resolved>2013-04-02T09:16:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-04-02T09:16:32Z" id="15764756">Closed by https://github.com/elasticsearch/elasticsearch/commit/b657bdfa1a9467848cc1844b5c732087e5eae1ca
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added /_plugins/ endpoint to list the names of installed plugins.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2831</link><project id="" key="" /><description>Previous pullrequest was a bit messy...
</description><key id="12605526">2831</key><summary>Added /_plugins/ endpoint to list the names of installed plugins.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q42jaap</reporter><labels /><created>2013-03-29T14:08:12Z</created><updated>2014-06-12T08:18:59Z</updated><resolved>2013-03-29T14:14:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-03-29T14:14:16Z" id="15642461">Here is the _work in progress_ API related to this: #2668
</comment><comment author="q42jaap" created="2013-03-29T14:25:25Z" id="15642872">Sorry, I didn't know that some work has already been done for this... I had fun anyway!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugins endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2830</link><project id="" key="" /><description>I saw a TODO in HttpServer, /_plugin/ now works, it lists the names of the plugins. I couldn't find how to list the installed sites though.
</description><key id="12604966">2830</key><summary>Plugins endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">q42jaap</reporter><labels /><created>2013-03-29T13:49:20Z</created><updated>2014-07-16T21:53:43Z</updated><resolved>2013-03-29T13:58:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>_source includes/excludes has no effect when getting documents by ID</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2829</link><project id="" key="" /><description>## Initial problem report:

As far as I've tested, the "includes" and "excludes" options for the _source field have no effect - I always get the whole source. Here's a gist for recreating:
https://gist.github.com/radu-gheorghe/5265495

I'm on 64-bit Ubuntu and I've used the .deb packages of 0.19.7 and 0.90.0RC1 with the same results.
## @clintongormley's comments (thanks!):

Hmm - it doesn't work for get-by-id. It works for search though.
Flushing the index helped, so it looks like its a translog thing
</description><key id="12598389">2829</key><summary>_source includes/excludes has no effect when getting documents by ID</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">radu-gheorghe</reporter><labels><label>bug</label><label>v0.90.1</label><label>v1.0.0.Beta1</label></labels><created>2013-03-29T08:40:18Z</created><updated>2013-04-30T16:09:12Z</updated><resolved>2013-04-30T16:09:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="thomasst" created="2013-04-08T22:44:36Z" id="16083829">Same problem here. Also, when I set the `_source` field to `"enabled": False` and create a custom stored property called `original` then I can usually retrieve it via GET `?fields=original`, but never via search using `"fields": ["original"]`. In some cases (which I don't know how yet how to reproduce) the `original` field would completely disappear, even in GET requests.
</comment><comment author="thomasst" created="2013-04-08T23:08:14Z" id="16084761">Also, saying `"_source": { "includes": ["original" ] }` stores all fields (not just `original`), but if I flush the index then all fields (including `original`) are gone.
</comment><comment author="spinscale" created="2013-04-09T13:29:56Z" id="16112384">Hey,

I can confirm, that this is a problem with translog as clint already mentioned. Currently entries from the translog are always returned as is by the `ShardGetService`. So, if the data is read from the translog and not from the indexreader (before a refresh or a flush), the problem occurs.

I have created a first preliminary and incomplete try to fix this, which I dont like very much and hope for input of other developers. See https://github.com/spinscale/elasticsearch/commit/4fccc68ea7db75758e31d752e52a1e1f21a23006
</comment><comment author="spinscale" created="2013-04-30T16:09:12Z" id="17236858">Forgot to close by commit. Closed by a694e97ab97deccb6c533176737ecb055a95e54a
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make highlight function pluggable so that customized highlight function ...</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2828</link><project id="" key="" /><description /><key id="12598175">2828</key><summary>Make highlight function pluggable so that customized highlight function ...</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">dongaihua</reporter><labels /><created>2013-03-29T08:27:51Z</created><updated>2014-07-16T21:53:43Z</updated><resolved>2013-05-17T07:07:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dongaihua" created="2013-03-29T08:35:45Z" id="15632655">I want to implement a new cusomized highlight function. To avoid modify es core codes, I want es can allow to add plugin to replace the default highlight function.
</comment><comment author="dongaihua" created="2013-04-12T01:37:21Z" id="16271683">@s1monw , can you help have a look this pull reqeust, if it is ok, can you help merge it? Thank you.
</comment><comment author="s1monw" created="2013-04-12T06:59:28Z" id="16278551">ah thanks for pinging... i will look into this soon hopefully!
</comment><comment author="dongaihua" created="2013-04-24T01:59:04Z" id="16904082">@s1monw, I make a custom highligh module, and do some tests, it works fine with my pull request codes. I'm looking forward to my pull request is merged into elasticsearch. If you find some improvements for my codes, please help modificate it directly. Thank you very much! :)
</comment><comment author="dongaihua" created="2013-05-06T01:26:57Z" id="17463209">@s1monw, sorry for disturbing you again. I know you are very busy, but I'm looking forward this function very much. I'd like to do some tests for the latest 0.90.0 version on about 20 nodes with this function and the custom highlight pluign. Can you help merge this pull request into elasticsearch? Thank you very much!
</comment><comment author="dongaihua" created="2013-05-16T08:10:04Z" id="17987293">@s1monw , I saw some other people in the community also want to add the custom highlight function, so I think this feature is useful for many users. Do you have any schedule to provide this feature? Thank you very much!
</comment><comment author="kimchy" created="2013-05-16T08:12:32Z" id="17987403">@dongaihua we are looking into this, to figure out the best way to expose this
</comment><comment author="dongaihua" created="2013-05-16T09:02:32Z" id="17989526">@kimchy, thank you very much for your response. This is really a good news. Is it possible to have such feture in next elasticsearch release? Thank you very much!
</comment><comment author="dongaihua" created="2013-05-17T06:24:24Z" id="18045729">@spinscale , I checked your code, it can meet my requirement. Thank you very much for your effort. Will you merge your changes into the next elasticsearch version? Thank you.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field similarity on int field type es.0.90.0.RC1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2827</link><project id="" key="" /><description>Hi, i hava some issue, with calculate scoring on int type field (name = someId). On this field i have own similarity provider (plugin). 

So when I query specific value let say 2, scoring doesnt use PerFieldSimilarity, it use ConstantScore.

{
  "from": 0,
  "size": 13,
  "query": {
    "bool": {
      "must": [
        {
          "term": {
            "someId": {
              "value": 2,
              "boost": 0.3
            }
          }
        }]
    }
  }
}

On score explanation i saw that this query is rewrite to Range Query. Is any positility to set rewrite option like in multi term query (prefix). Or is only way to change field type to string
</description><key id="12592099">2827</key><summary>Field similarity on int field type es.0.90.0.RC1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukapor</reporter><labels /><created>2013-03-29T01:17:02Z</created><updated>2013-03-29T22:14:53Z</updated><resolved>2013-03-29T22:14:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-29T07:29:14Z" id="15631400">what exactly are you trying to do? why do you need custom similarity for numeric field? if its for custom scoring based on its value, use custom_score, similarity doesn't really make sense in that case.
</comment><comment author="lukapor" created="2013-03-29T08:30:38Z" id="15632537">Hello,

I have documents and on documents I have document field documentTypeId, so I do query scoring like this
"should": [
  "term": { "documentTypeId" : { "value" : 1, "boost" : 5 }},
  "term": { "documentTypeId" : { "value" : 2, "boost" : 3 }} ]

Problem of ConstantScore is that queryNorm on calculation is not 1, so i dont get boost score.

The same problem I have when I do query scoring od document modified date (range query - boosting one week age document, one month age document) 

On es 0.20.x i was using index.similarity.search.type, is that removed in 0.90.0, because it doesnt work any more, so I now i using field similarity.

Only one question on lucene you can specified similarity on IndexSearcher, why on search api i can not specified similarity.
</comment><comment author="lukapor" created="2013-03-29T22:14:53Z" id="15663240">Now I check the code of SimilarityService.

In elasticsearch version 0.90.0 is new setting to replace default similarity index.similarity.base.type : custom_similarity
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 0.90 Multiple Numeric Range filters within boolean.should return incorrect results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2826</link><project id="" key="" /><description>I have a build of 0.9 from source (3-4 days ago) and it does not calculate numeric filters correctly when combined in bool should 
The number of hits in the following query should be 35 (16 from the 1st range and 19 from the 2nd). Works correctly in 0.20.5 but in 0.9 it returns  wrong number of hits which is also different depending on which range filter is listed first in the bool filter (the numbers are 32 and 28 depending on which range is first)

when removing one of the ranges from bool.should the calculations are correct. The calculations are also correct when using the same ranges in Range facet

``` javascript
{
  "query": {
    "match_all": {}
  },
  "from": 0,
  "filter": {
    "bool": {
      "should": [
        {
          "numeric_range": {
            "money.totals.obligationTotal": {
              "from": 20000000,
              "to": 50000000,
              "include_upper": false
            }
          }
        },
        {
          "numeric_range": {
            "money.totals.obligationTotal": {
              "from": 50000000,
              "to": 100000000,
              "include_upper": false
            }
          }
        }
      ]
    }
  }
}
```
</description><key id="12582198">2826</key><summary>ES 0.90 Multiple Numeric Range filters within boolean.should return incorrect results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-28T20:14:47Z</created><updated>2013-03-29T17:59:44Z</updated><resolved>2013-03-29T15:50:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-03-29T10:22:41Z" id="15635527">@roytmana Thanks for reporting this issue. This is a bug that manifests itself under certain circumstances (specific types of filters inside the bool filter that yield certain optimisations during filter execution). I will fix this asap. 
</comment><comment author="martijnvg" created="2013-03-29T15:50:43Z" id="15646913">@roytmana The bug is fixed by the following commit:
https://github.com/elasticsearch/elasticsearch/commit/a89dde8bac344f676f5deadd4a49e12ff614d3ca

Also in the above case it makes more sense to use the `range` filter instead of the `numeric_range` filter. The `range` filter should be faster in your case and is cached automatically.
</comment><comment author="roytmana" created="2013-03-29T16:26:47Z" id="15648558">@martijnvg thanks a lot. Docs say numeric filter is more memory intensive but faster. My use case is
1. facet on entire range of obligationTotal values using range facet with about dozen of ranges from 0 to infinity
2. user selects one or more facet ranges and I apply filter based on it.

Do you think range or numeric range is faster here since all values were probably already enumerated for faceting
and does regular range filter recognize and handle numeric fields properly (I do not want lexicographical ranges but numerical)

Thanks,
alex
</comment><comment author="martijnvg" created="2013-03-29T16:34:56Z" id="15648913">@roytmana `range` filter does also work for number based fields. Since you're faceting on these fields it maybe makes sense to use `numeric_range` filter, just make sure you execute it in combination with other filters (like term filter).
</comment><comment author="roytmana" created="2013-03-29T17:44:35Z" id="15651912">user can very well select just one of my range facet values and I will then filter on this range alone. do you think it may be an issue?
</comment><comment author="martijnvg" created="2013-03-29T17:58:20Z" id="15652554">I expect the `range` filter to execute a bit faster in the case it is the only filter driving the search request. I don't expect using the `numeric_range` filter to be a problem.
</comment><comment author="roytmana" created="2013-03-29T17:59:44Z" id="15652611">many thanks!

On Fri, Mar 29, 2013 at 1:58 PM, Martijn van Groningen &lt;
notifications@github.com&gt; wrote:

&gt; I expect the range filter to execute a bit faster in the case it is the
&gt; only filter driving the search request. I don't expect using the
&gt; numeric_range filter to be a problem.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2826#issuecomment-15652554
&gt; .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Shard replica differs from origin</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2825</link><project id="" key="" /><description>Shards in cluster became inconsistent and they return different results, as they contain different data. How can i fix it? What is the reason for that?

Version 0.20.4
</description><key id="12574851">2825</key><summary>Shard replica differs from origin</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">Rzulf</reporter><labels /><created>2013-03-28T17:22:29Z</created><updated>2013-07-17T15:24:25Z</updated><resolved>2013-07-17T15:24:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Rzulf" created="2013-04-04T14:26:43Z" id="15900274">I managed to find a reason for shard inconsistency. For a few days flush was disabled and everything was stored in translog. In those few days ES cluster has undergone some heavy conditions (a lot of saves -&gt; running out of memory -&gt; node restart). I managed to fixed this issue, by enabling saving to disk and physically deleting replicas in order to fully restore them.

Shouldn't the cluster be aware of the fact that there are inconsistencies in replicas? The differences were quite huge, up to few millions per shard and cluster health was "green".
</comment><comment author="spinscale" created="2013-07-17T12:31:05Z" id="21109370">If you disabled flush/refresh by configuration there's not too much one can do in order to detect this kind of problems.

If you think this is a bug, can you provide a gist to reproduce your problem in order to make sure we are talking about the same kind of problem? Thanks a lot!
</comment><comment author="Rzulf" created="2013-07-17T14:09:17Z" id="21115207">Unfortunately I cannot reproduce this problem. As I written earlier cluster had some stability problems. 
</comment><comment author="spinscale" created="2013-07-17T15:24:25Z" id="21120781">thanks for the fast answer. I'll close it, but I'd like to get more information if you stumble over this again in order to reproduce!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Vector highlighter matches shortest string, not longest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2824</link><project id="" key="" /><description>In 0.90.0.RC1, when using edge_ngrams on a field, the vector highlighter is highlighting the shortest term, rather than the longest.  The standard highlighter works correctly:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "name" : {
               "fields" : {
                  "autocomplete" : {
                     "type" : "string",
                     "analyzer" : "autocomplete"
                  },
                  "name" : {
                     "type" : "string"
                  },
                  "vector_autocomplete" : {
                     "type" : "string",
                     "term_vector" : "with_positions_offsets",
                     "analyzer" : "autocomplete"
                  }
               },
               "type" : "multi_field"
            }
         }
      }
   },
   "settings" : {
      "analysis" : {
         "filter" : {
            "autocomplete" : {
               "max_gram" : 20,
               "min_gram" : 1,
               "type" : "edge_ngram"
            }
         },
         "analyzer" : {
            "autocomplete" : {
               "filter" : [
                  "standard",
                  "lowercase",
                  "autocomplete"
               ],
               "tokenizer" : "standard"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "name" : "Deutsche Telekomm"
}
'
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "match" : {
         "name.autocomplete" : "deutsche telek"
      }
   },
   "highlight" : {
      "fields" : {
         "name.autocomplete" : {},
         "name.vector_autocomplete" : {}
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "name" : "Deutsche Telekomm"
#             },
#             "_score" : 0.2765934,
#             "_index" : "test",
#             "_id" : "zN2-e844SraXAwkhQkAc-w",
#             "_type" : "test",
#             "highlight" : {
#                "name.autocomplete" : [
#                   "&lt;em&gt;Deutsche&lt;/em&gt; &lt;em&gt;Telek&lt;/em&gt;omm"
#                ],
#                "name.vector_autocomplete" : [
#                   "&lt;em&gt;D&lt;/em&gt;eutsche &lt;em&gt;T&lt;/em&gt;elekomm"
#                ]
#             }
#          }
#       ],
#       "max_score" : 0.2765934,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 17
# }
```
</description><key id="12567590">2824</key><summary>Vector highlighter matches shortest string, not longest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-03-28T14:50:28Z</created><updated>2013-11-04T18:32:33Z</updated><resolved>2013-11-04T18:32:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-23T13:40:29Z" id="18343687">After taking a look at the source I think we are running into [LUCENE-4118](https://issues.apache.org/jira/browse/LUCENE-4118) and/or [LUCENE-4734](https://issues.apache.org/jira/browse/LUCENE-4734) here.
</comment><comment author="jpountz" created="2013-05-27T14:03:42Z" id="18500114">This behavior is due to the way FastVectorHighlighter discards matches that have overlapping offsets: it first finds "d" as a matching term and then discards "de", "deu", "deut", ... since their offsets overlap. I'm not sure this is the right way to proceed or not but anyway, EdgeNGramTokenFilter offset handling changed in Lucene recently ([LUCENE-3907](https://issues.apache.org/jira/browse/LUCENE-3907)), so the next release (4.4) will highlight "Deutsche Telekomm" differently again:

```
&lt;em&gt;Deutsche&lt;/em&gt; &lt;em&gt;Telekomm&lt;/em&gt;"
```

even if the query was just "Deutsche Telek". The good news is that this behavior will be consistent between Highlighter and FastVectorHighlighter, the bad news is that it prevents from performing partial highlighting anymore.

This might look like a regression since there is a loss in flexibility here but unfortunately the previous behavior that updated offsets could lead to severe highlighting bugs when applied in conjunction with filters that generate/modify graphs such as SynonymFilter or ShingleFilter.
</comment><comment author="javanna" created="2013-11-04T18:04:50Z" id="27707626">Just tested this against 0.90.6 and I confirm that both plain and fastvector highlight `&lt;em&gt;Deutsche&lt;/em&gt; &lt;em&gt;Telekomm&lt;/em&gt;` . Same happens with the postings highlighter too.
</comment><comment author="javanna" created="2013-11-04T18:32:16Z" id="27709941">To get this working with any of the three highlighters we need to make ngrams only at index time (index_analyzer instead of analyzer) and use the [edge ngram tokenizer](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-edgengram-tokenizer.html) with `token_chars: ["letter","digit"]` instead of the edge ngram token filter.

Closing this one as we found a solution for it somehow.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Segments API: Add version &amp; compound for each segment</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2823</link><project id="" key="" /><description>Add a version (which Lucene version generate it) and compound (if the segment is in compound form or not) to each segment returned as part of the segments API.
</description><key id="12566845">2823</key><summary>Segments API: Add version &amp; compound for each segment</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-03-28T14:34:08Z</created><updated>2013-03-28T14:34:41Z</updated><resolved>2013-03-28T14:34:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Boolean Filter must(FilterBuilder... filterBuilders) methods generate invalid JSON in the Java API </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2822</link><project id="" key="" /><description>When using the must() method from BooleanFilterBuilder the output is 2 "must" key within the same "bool" attribute which is invalid JSON according to the RFC;
"A single comma separates a value from a following name.  The names within an object SHOULD be unique." http://www.ietf.org/rfc/rfc4627.txt

The correct output (which already works) should be:

```
"bool" : { "must" : [ { cond1 }, { cond2} ] } 
```

instead of;

```
"bool" : {
  "must" : { cond1 },
  "must" : { cond2 }
}
```

Because converting this json to javascript, python, ruby, php or perl will drop one of those keys.
</description><key id="12527391">2822</key><summary>Boolean Filter must(FilterBuilder... filterBuilders) methods generate invalid JSON in the Java API </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jgagnon1</reporter><labels /><created>2013-03-27T18:36:29Z</created><updated>2013-03-27T18:45:43Z</updated><resolved>2013-03-27T18:45:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-27T18:45:40Z" id="15544893">this was fixed in master... / 0.90.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cluster doesn't accept new client nodes or update settings API calls while initializing or rebalancing shards.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2821</link><project id="" key="" /><description>When an ElasticSearch cluster is initializing shards or relocating them, it will not let new client nodes join the cluster or any update settings API calls take effect until all initializing and rebalancing is complete. When this process starts to take hours due to a large amount of indices/shards to initialize on startup, this problem creates an unnecessarily long amount of downtime because even once the cluster reaches yellow or even green status, no client nodes can connect.
</description><key id="12523736">2821</key><summary>Cluster doesn't accept new client nodes or update settings API calls while initializing or rebalancing shards.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Awnedion</reporter><labels /><created>2013-03-27T17:37:30Z</created><updated>2014-08-08T10:47:30Z</updated><resolved>2014-08-08T10:47:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-03-27T17:39:58Z" id="15539256">Which version of Elasticsearch are you using? Did you change anything in elasticsearch configuration?
</comment><comment author="Awnedion" created="2013-03-27T20:02:00Z" id="15549578">I'm using version 0.20.5.

cluster_concurrent_rebalance: 20
node_concurrent_recoveries: 150
node_initial_primaries_recoveries:150

indices.recovery.concurrent_streams: 10

Unicast discovery:
ping_interval: 5s
ping_timeout: 60s
ping_retries: 5

blocking thread pool
auto_import_dangled: no
</comment><comment author="folke" created="2013-04-10T03:46:59Z" id="16153850">Just wanted to say we're having the same issue. This is actually a big issue for us since we have a lot of shards (20k in total). We just added 10 new servers and it takes forever to rebalance them. Bulk operations and other thing simply timeout after a while. We're on 0.20.6
</comment><comment author="clintongormley" created="2014-08-08T10:47:29Z" id="51587231">Trying to do too much I/O at once is always going to hurt responsiveness. I/O needs to be throttled to suit your hardware.  This will also be helped by the addition of sequence numbers to speed up recovery.  Closing in favour of #6069 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Least used store distributor allocates all data to the last directory on the list</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2820</link><project id="" key="" /><description>See https://groups.google.com/d/msg/elasticsearch/UaaCsk7xwFs/vHpB9TOl5n8J
</description><key id="12520640">2820</key><summary>Least used store distributor allocates all data to the last directory on the list</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-27T16:36:11Z</created><updated>2013-03-28T15:47:52Z</updated><resolved>2013-03-28T15:47:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Date Math does not work in filtered query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2819</link><project id="" key="" /><description>newbie to ES.
This does not return any docs:

``` json
  "query": {
    "filtered": {
      "query": {
        "match_all": {}
      },
      "filter": {
        "range": {
          "screened": {
            "from": "now-3d",
            "to": "now+3d"
          }
        }
      }
    }
  }
```

This does:

``` json
      "query": {
        "match_all": {}
      },
      "filter": {
        "range": {
          "screened": {
            "from": "now-3d",
            "to": "now+3d"
          }
        }
      }
```
</description><key id="12516721">2819</key><summary>Date Math does not work in filtered query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">garcimouche</reporter><labels /><created>2013-03-27T15:21:40Z</created><updated>2014-02-14T18:30:40Z</updated><resolved>2014-02-14T18:30:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-03-27T15:36:59Z" id="15530746">That might be a duplicate of #2808
</comment><comment author="spinscale" created="2013-04-04T15:21:29Z" id="15903793">I could not reproduce this immediately on master or 0.90 or 0.20.6.

@garcimouche can you give us some more information, what you did, which data was inserted, how your mapping looks like?
</comment><comment author="garcimouche" created="2013-04-04T17:22:05Z" id="15911151">sure
ES v 0.20.6

data:

``` json
[
{ 
    "branch": "50",
    "port": 123,
    "user": "MY_USER",
    "shipper_id": 1234567,
    "client_id": 90020690,
    "shipper_name": "MY SHIPPER NAME",
    "client_name": "MY CLIENT NAME",
    "tags":["tag1","tag2"],
    "screened": 20130325110225,
    "shipment-value":503.22
},
{ 
    "branch": "99",
    "port": 351,
    "user": "ANOTHER USER",
    "shipper_id": 1234568,
    "client_id": 90020691,
    "shipper_name": "WALMART",
    "client_name": "TARGET",
    "tags":["tag2","tag3"],
    "screened": 20130325110225,
    "billed": 20130325130527,
    "shipment-value":1503.22,
    "billing-revenue-fees":27.00
}
]
```

mapping

``` json
{
    "settings" : {
        "index.query.default_field" : "client-name",
    },
    "mappings": {
        "entry": {
            "_ttl" : { "enabled" : "true", "default" : "93d" },
            "_all" : {"enabled" : "false"},
            "properties": {
                "branch": {"type":"string", "index" : "not_analyzed"},
                "port": {"type":"integer"},
                "user": {"type":"string", "index" : "not_analyzed"},
                "shipper-id": {"type":"integer"},
                "client-id": {"type":"integer"},
                "shipper-name": {"type":"string", "analyzer": "standard"},
                "client-name": {"type":"string", "analyzer": "standard"},
                "tags": {"type" : "string", "index" : "not_analyzed"},
                "screened": {"type" : "date", "format" : "yyyyMMddhhmmss"},
                "billed": {"type" : "date", "format" : "yyyyMMddhhmmss"},
                "shipment-value": {"type" : "float"},
                "billing-revenue-fees": {"type" : "float"}
            }
        }
    }
}
```

queries snapshot mentioned in 1st post.

Thank you!
</comment><comment author="javanna" created="2013-10-18T12:29:17Z" id="26591720">@garcimouche I had a look at your example data, and actually none of the range queries would actually work with those documents. 

The reason is that you did map the `screened` field as a date, specifying a format or it as well, but that format is used when you submit the document as a `string`. If you provide it as a number, it will get indexed as is, since date fields are internally represented as `long`. So, the number doesn't get converted to a string and then parsed as a date...but it gets indexed directly as a time from epoch. In fact in your case the numeric value 20130325110225 was indexed as something like  Fri, 27 Nov 2607 20:58:30 GMT I believe.

To solve this you need to submit the field as a string with the specified format (just add double quotes around it). Otherwise if you want to submit it as number, the long must be a time from epoch.

Adding the double quotes both queries work with 0.90.5, also after #2808 was fixed.
</comment><comment author="javanna" created="2014-02-14T18:30:40Z" id="35110934">Closing, seems like there was an issue with the submitted documents as said above.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>elasticsearch.yaml comment unclear</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2818</link><project id="" key="" /><description> Set this option to a higher value (2-4) for large clusters:
 discovery.zen.minimum_master_nodes: 3

"2-4" should probably be "enough for a quorum", as per clintongormley on IRC.
</description><key id="12471461">2818</key><summary>elasticsearch.yaml comment unclear</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">redchuck</reporter><labels /><created>2013-03-26T21:12:16Z</created><updated>2014-08-08T10:44:41Z</updated><resolved>2014-08-08T10:44:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Phrase suggester throws ArrayIndexOutOfBoundsException when stopwords are the only text values</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2817</link><project id="" key="" /><description>-Version 0.90.0.RC1
When searching for phrase suggestions and all suggestions are stop words, then I would expect empty results instead of an ArrayIndexOutOfBoundsException.

``` bash
curl -XPUT http://localhost:9200/test -d '{ "number_of_shards":1, "number_of_replicas":0 }'
curl -XPUT http://localhost:9200/test/test/1 -d '{ "subject": "a test subject" }'
curl -XPOST http://localhost:9200/test/_suggest?pretty=true -d '{
  "text": "a an the",
  "sug2": {
    "phrase": {
      "field": "subject",
      "size": 1,
      "real_word_error_likelihood": 0.95,
      "max_errors": 0.5,
      "gram_size": 2,
      "direct_generator": [
        {
          "field": "subject",
          "suggest_mode": "always",
          "min_word_len": 1
        }
      ]
    }
  }
}'

# Response
{
  "_shards" : {
    "total" : 1,
    "successful" : 0,
    "failed" : 1,
    "failures" : [ {
      "index" : "test",
      "shard" : 0,
      "reason" : "BroadcastShardOperationFailedException[[test][0] ]; nested: ElasticSearchException[failed to execute suggest]; nested: ArrayIndexOutOfBoundsException[0]; "
    } ]
  }
}
```
</description><key id="12456576">2817</key><summary>Phrase suggester throws ArrayIndexOutOfBoundsException when stopwords are the only text values</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">tteats</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-26T16:14:10Z</created><updated>2013-03-26T16:47:49Z</updated><resolved>2013-03-26T16:47:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-26T16:40:11Z" id="15469917">argh! thanks for opening this issue. I will fix soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias filter is ignored if a sort field is specified</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2816</link><project id="" key="" /><description>Repro: https://gist.github.com/imotov/5242738
</description><key id="12432494">2816</key><summary>Alias filter is ignored if a sort field is specified</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-26T02:57:06Z</created><updated>2013-03-26T11:25:59Z</updated><resolved>2013-03-26T11:25:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Warmers: Have an explicit warmer thread pool</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2815</link><project id="" key="" /><description>Have an explicit threadpool `warmer` that is dedicated to execute warmers. Currently, it uses the `search` threadpool, which does not work well since the number of concurrent searches should be separate from the number of concurrent warmers allows, also the characteristics of the search pool (for example, bounded queue_size) might not fit well with how warmers should be executed (they should not be "rejected").
</description><key id="12405974">2815</key><summary>Warmers: Have an explicit warmer thread pool</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.7</label><label>v0.90.0.RC2</label></labels><created>2013-03-25T15:44:46Z</created><updated>2013-03-25T15:46:45Z</updated><resolved>2013-03-25T15:46:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>0.90.0.RC1 Percolator, Not Matching Properly on Custom Analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2814</link><project id="" key="" /><description>I found this bug in 0.90.0.RC1. (Works properly in 0.20.6 and earlier). It seems to have something to do with the custom analyzer/tokenizer being specified.

1) PUT http://localhost:9200/foo
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "index": {
      "analysis": {
        "analyzer": {
          "lwhitespacecomma": {
            "tokenizer": "whitespacecomma",
            "filter": [
              "lowercase"
            ]
          }
        },
        "tokenizer": {
          "whitespacecomma": {
            "pattern": "(,|\s+)",
            "type": "pattern"
          }
        }
      }
    }
  },
  "mappings": {
    "doc": {
      "properties": {
        "filingcategory": {
            "type": "string",
            "analyzer": "lwhitespacecomma"
        }
      }
    }
  }
}

2) POST http://localhost:9200/_percolator/foo/1
{
  "source": "productizer",
  "query": {
    "constant_score": {
      "query": {
        "query_string": {
          "query": "filingcategory:s"
        }
      }
    }
  }
}

3) POST http://localhost:9200/foo/doc/_percolate
{
  "query": {
    "term": {
      "source": "productizer"
    }
  },
  "doc": {
    "filingcategory": "s"
  }
}

Returns (in 0.90.0.RC1): {"ok":true,"matches":[]}

Returns (in 0.20.6 or below): {"ok":true,"matches":["1"]}

Thanks!
</description><key id="12399487">2814</key><summary>0.90.0.RC1 Percolator, Not Matching Properly on Custom Analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-25T13:19:18Z</created><updated>2013-04-08T20:44:40Z</updated><resolved>2013-03-25T22:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-25T21:44:46Z" id="15427631">tricky, this bug only manifests if the tokenizer used consumes the input / field values on TokenStream#reset(). I will push a fix soon. Thanks for the reproduction code.
</comment><comment author="bly2k" created="2013-04-08T20:21:36Z" id="16075848">Simon, thanks. I have verified that your fix is working. I am currently testing performance now. Thanks!
</comment><comment author="s1monw" created="2013-04-08T20:44:39Z" id="16077185">thanks for reporting back!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Java6 u31 compilation issues with GeoDistanceTests (was working the other day)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2813</link><project id="" key="" /><description>Attempting to compile with Java 6 u31 results in the following error:
type parameters of &lt;V&gt;V cannot be determined; no unique maximal instance exists for type variable V with upper bounds double,java.lang.Object

in GeoDistanceTests#distanceScriptTests on lines 275, 279, 283, 287, 291, and 294

Changing double to Double fixes this.
</description><key id="12345076">2813</key><summary>Java6 u31 compilation issues with GeoDistanceTests (was working the other day)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">AngusDavis</reporter><labels /><created>2013-03-22T23:56:57Z</created><updated>2014-07-16T21:53:44Z</updated><resolved>2013-03-23T07:57:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>TooManyOpenFiles might cause data-loss in ElasticSearch (Lucene)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2812</link><project id="" key="" /><description>Under certain circumstances a TooManyOpenFiles exception (in Java thrown as FileNotFoundException) might cause data loss where entire shards (lucene indices) are deleted. This is mainly caused by [Lucene-4870](https://issues.apache.org/jira/browse/LUCENE-4870) - currently all Elasticsearch releases are affected by this.
</description><key id="12326073">2812</key><summary>TooManyOpenFiles might cause data-loss in ElasticSearch (Lucene)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC2</label></labels><created>2013-03-22T16:17:31Z</created><updated>2013-06-18T13:34:27Z</updated><resolved>2013-03-22T16:19:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow to update ttl field mapping after initial creation. Fixes #2136</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2811</link><project id="" key="" /><description>Several field mappers cannot be changed via the Mapping API after initial creation. However I do not see something speaking against this, see bug report #2136...

This PR allows the TTL field mapper to be enabled or disabled at runtime.
</description><key id="12319832">2811</key><summary>Allow to update ttl field mapping after initial creation. Fixes #2136</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-03-22T13:58:31Z</created><updated>2014-07-16T21:53:44Z</updated><resolved>2013-04-02T06:37:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Geoutils</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2810</link><project id="" key="" /><description>Add tests for GeoDistances in scripts.
</description><key id="12317093">2810</key><summary>Geoutils</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-03-22T12:32:40Z</created><updated>2014-07-16T21:53:44Z</updated><resolved>2013-03-22T12:49:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-22T12:40:24Z" id="15294547">the test looks good I will cherry-pick it. I think you worked on an old branch...
</comment><comment author="s1monw" created="2013-03-22T12:49:45Z" id="15294873">pushed - thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Validate query ignores date maths</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2809</link><project id="" key="" /><description>Date maths is not properly handled in the `validate` query API.

If you run a query string query with date maths, then validate the same query, it uses the cached value for `now()` (also see #2808):

```
curl -XDELETE 'http://127.0.0.1:9200/test/?pretty=1' 
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "date" : {
               "type" : "date"
            }
         }
      }
   }
}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "query_string" : {
         "query" : "date:[now TO *]"
      }
   },
   "explain" : 1
}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "query_string" : {
      "query" : "date:[now TO *]"
   }
}
'

# {
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "explanations" : [
#       {
#          "index" : "test",
#          "explanation" : "date:[1363953888884 TO *]",
#          "valid" : true
#       }
#    ],
#    "valid" : true
# }
```

If you run the `validate` query first, then it ignores the date calculation. If you run the query_string query next, it uses the value generated by validate, which sets `now()` to zero:

```
curl -XDELETE 'http://127.0.0.1:9200/test/?pretty=1' 
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "date" : {
               "type" : "date"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "date" : "2020-01-01"
}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "query_string" : {
      "query" : "date:[now TO *]"
   }
}
'

# {
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "explanations" : [
#       {
#          "index" : "test",
#          "explanation" : "date:[0 TO *]",
#          "valid" : true
#       }
#    ],
#    "valid" : true
# }


curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "query_string" : {
         "query" : "date:[now TO *]"
      }
   },
   "explain" : 1
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "date" : "2020-01-01"
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_shard" : 1,
#             "_id" : "IjNYSDcJTnG41RVl322oRA",
#             "_node" : "gx3hk4y7S0Khhx8IQ4uYQQ",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScore(date:[0 TO *]), product of:"
#             }
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 5
# }
```

Validating a `range` query also incorrectly uses zero for `now()`:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "range" : {
      "date" : {
         "gt" : "now"
      }
   }
}
'
```

But searching on a range query generates a new value for now():

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "range" : {
         "date" : {
            "gt" : "now"
         }
      }
   },
   "explain" : 1
}
'

# [Fri Mar 22 13:09:58 2013] Response:
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "date" : "2020-01-01"
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_shard" : 1,
#             "_id" : "IjNYSDcJTnG41RVl322oRA",
#             "_node" : "gx3hk4y7S0Khhx8IQ4uYQQ",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScore(date:{1363954198120
# &gt;                 TO *]), product of:"
#             }
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```
</description><key id="12316515">2809</key><summary>Validate query ignores date maths</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-03-22T12:11:10Z</created><updated>2013-10-18T12:09:31Z</updated><resolved>2013-10-18T12:09:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-10-18T12:09:31Z" id="26590707">After #2808 was solved, the validate query remained without any now set, which was then zero all the time. That was fixed in #3629 .
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Date math in query_string caches now()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2808</link><project id="" key="" /><description>When using `"now"` in date math in the query string, the value for `now` is cached:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "date" : {
               "type" : "date"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "date" : "2020-01-01"
}
'
```

Run this query multiple times:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "query_string" : {
         "query" : "date:[now TO *]"
      }
   },
   "explain" : 1
}
'
```

You will see that the description line `"ConstantScore(date:[1363953493782 TO *])"` reuses the same start date

```
# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "date" : "2020-01-01"
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_shard" : 4,
#             "_id" : "iOPrtwzdQFGixdfwaAAlcQ",
#             "_node" : "gx3hk4y7S0Khhx8IQ4uYQQ",
#             "_type" : "test",
#             "_explanation" : {
#                "value" : 1,
#                "details" : [
#                   {
#                      "value" : 1,
#                      "description" : "boost"
#                   },
#                   {
#                      "value" : 1,
#                      "description" : "queryNorm"
#                   }
#                ],
#                "description" : "ConstantScore(date:[1363953493782 TO *]), product of:"
#             }
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 5
# }
```

A `range` query on the other hand, updates the start date on each run:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "range" : {
         "date" : {
            "gt" : "now"
         }
      }
   },
   "explain" : 1
}
'
```
</description><key id="12316247">2808</key><summary>Date math in query_string caches now()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.20.7</label><label>v0.90.0.RC2</label></labels><created>2013-03-22T12:01:46Z</created><updated>2013-05-31T09:38:21Z</updated><resolved>2013-03-27T19:32:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-03-22T12:11:54Z" id="15293563">Also see #2809 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When faceting on integer field using TERMS , 0.9 fails to count records where the facet field is missing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2807</link><project id="" key="" /><description>reproduceable example:
https://gist.github.com/roytmana/5215441

sorry my curl commands could be somewhat wrong - I am on windows and not using it much
</description><key id="12290862">2807</key><summary>When faceting on integer field using TERMS , 0.9 fails to count records where the facet field is missing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">roytmana</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-21T19:26:45Z</created><updated>2013-03-22T13:43:09Z</updated><resolved>2013-03-21T21:46:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-21T20:37:05Z" id="15263806">thanks for reporting this. we will look into it soon
</comment><comment author="roytmana" created="2013-03-21T20:42:42Z" id="15264097">thanks, note that not only missing field is not right but total field (within facets) is also less than it should be (100 vs 200 hits) 

You probably do not need all 200 docs just 5 will do the job. I was just struggling to reproduce it since it appeared very erratic from my app (some facets working and some not) till I realized it is the ones on numeric fields which are broken
</comment><comment author="clintongormley" created="2013-03-21T21:22:10Z" id="15266334">@roytmana yep, the missing count is wrong. Note, the size=0 thing is not supported.  See #1776
</comment><comment author="roytmana" created="2013-03-21T21:38:32Z" id="15267226">Hi Clint,

So size=0 is a regression relative to 0.20.5 because 0.20.5 does support it? Any chance it'll be supported in 0.9GA?

Also may I ask if there is any hope to have terms_stats support total/count (if not the rest of stats) for "missing" and "other" like terms does? It would be very helpful. Right now I decompose my terms_stats to 3 facets, terms_stats, stats with missing filter and stats for that field to calculate "complete" totals (per term, other, missing) and then recompose results back. I also have few useful and simple ideas for new facets ("logical" facets I currently use by decomposing them onto multiple ES facets and recomposing data together) just not sure it is a good time to lobby for them :-)
</comment><comment author="s1monw" created="2013-03-21T21:43:40Z" id="15267490">@roytmana can you open a sep. issue for the `"size" : 0` issue please?
</comment><comment author="roytmana" created="2013-03-21T21:46:32Z" id="15267638">There is a similar issue https://github.com/elasticsearch/elasticsearch/issues/1776

Would you like me to open another one specifically for 0.9RC1?
</comment><comment author="s1monw" created="2013-03-21T21:47:26Z" id="15267673">@roytmana well I wonder why this one is still open if that is in 0.20.5 so lets sort that out first, can you comment?
</comment><comment author="roytmana" created="2013-03-21T21:49:48Z" id="15267776">I was not aware of that old issue. Clint pointed it out (see above) I as happily using it since 0.20.3 or 4
</comment><comment author="clintongormley" created="2013-03-22T09:48:23Z" id="15288407">Hiya @roytmana.

I've checked on 0.20.3-0.20.5 and specifying `size:0` on a facet has never returned all terms.  It has always returned zero terms.  I think you must be thinking of something else?

cilnt
</comment><comment author="roytmana" created="2013-03-22T13:43:09Z" id="15297092">sorry you are correct I mixed it up. I was having problem with it in 0.20.3 and then moved to terms_stats facet (since I needed some $ totals anyways) which supports it correctly and now mixed it up. It is not working in 0.20.5

It would be very nice to make terms and terms_stats consistent in two ways
1) terms to support size=0
2) terms_stats to support totals/counts for "missing" and "other" 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.90.0.RC1 Percolator Query, NPE</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2806</link><project id="" key="" /><description>I think this is a bug in the latest build but I'm not sure how to open a ticket. This is using 0.90.0.RC1. We've been using ES from 0.19.x through 0.20.5 and this worked fine up to this point. Looks like it is broken in the newest build.

1) POST http://localhost:9200/_percolator/foo/1
{
  "source": "foo",
  "query": {
    "term": {
      "foo": "bar"
    }
  }
}

2) PUT http://localhost:9200/foo

3) POST http://localhost:9200/foo/doc/_percolate
{
  "query": {
    "term": {
      "source": "foo"
    }
  },
  "doc": {
    "foo": "bar"
  }
}

ES 0.90.0.RC1 returns:

{
error: NullPointerException[null]
status: 500
}

Thanks!
</description><key id="12282401">2806</key><summary>0.90.0.RC1 Percolator Query, NPE</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">bly2k</reporter><labels><label>bug</label><label>v0.90.0.RC2</label></labels><created>2013-03-21T16:20:58Z</created><updated>2013-03-25T13:24:58Z</updated><resolved>2013-03-21T18:39:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-21T18:08:53Z" id="15255752">this is a bug in UID, thanks for reporting this. I will have a fix soon.
</comment><comment author="bly2k" created="2013-03-21T18:31:39Z" id="15257072">Simon thanks! I'm seeing that there is now some pooling implemented with the in-memory percolator index. We use the percolator extensively so I would be interested to test that after your fix to check also if the newly added pooling  code improves (or degrades) performance while not breaking anything else. I hope. Good stuff! :)
</comment><comment author="s1monw" created="2013-03-21T18:39:51Z" id="15257524">this should take you further! :)
</comment><comment author="bly2k" created="2013-03-25T13:24:58Z" id="15392766">Simon, got the patch and this seems to be fixed. I did more testing and found another bug. I just opened another bug titled "0.90.0.RC1 Percolator, Not Matching Properly on Custom Analyzer". Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>On query DSL errors display the expected type for nested objects.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2805</link><project id="" key="" /><description>It seems like a common problem I am seeing developers new to ES struggling with is how to fit the pieces of the query DSL together. Returning more structured feedback about what the query DSL is expecting feels like it would help.

Some examples from helping someone recently:

``` javascript
'filter' : {
  'not' : {
    'post_id' : [ 1, 2 ]
  }
},
```

Problem here is the NOT filter should have a filter inside of it rather than a field name

If the error response could return the type of expected data at each level that would aid debugging. For example this could return:
"filter": { "not" : "error: expects a filter" } }

Another case:

``` javascript
'filter' : {
  'and' : {
    'terms' : {
      'post_type' : ['post']
    },
    'not' : {
      'terms' : {
        'post_id' : [1, 2]
      }
    }
  }
},
```

Error:
"filter": { "and": "error: expects an array of filter objects" } }

With more complex nesting:

``` javascript
'filter' : {
  'and' : [
    {
      'terms' : {
       'post_type' : ['post']
      }
    },
    {
      'not' : {
        'post_id' : [1,2]
      }
    }
  ]
}
```

Error:
"filter": { "and": [{"terms" : "ok"}, {"not" : "error: expects a filter"} } }

Not entirely sure how feasible this is, or if there is better syntax or a more standard method for reporting such structured errors.
</description><key id="12254557">2805</key><summary>On query DSL errors display the expected type for nested objects.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gibrown</reporter><labels><label>adoptme</label><label>high hanging fruit</label></labels><created>2013-03-20T23:57:40Z</created><updated>2014-11-29T14:17:31Z</updated><resolved>2014-11-29T14:17:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-23T13:17:09Z" id="49871605">I don't think this is really feasible.  Some queries accept a large number of options (eg `query_string`).  These days we don't just accept bad query syntax like we did before, so at least the user gets an error about which part of the query is bad.

That said I think it could still be improved,  eg in a deeply nested query, it may not be obvious which clause is being referred to. Perhaps we could have the equivalent of a query "stack trace" here.
</comment><comment author="gibrown" created="2014-07-23T14:52:17Z" id="49884541">Thanks for taking a look at this. 

A query stack trace would help a lot I think.

There are a few other things that make the output very tough to grok.

For instance, here is a bad query I just ran across our 8 indices:

```
{
   "query": {
      "filtered": {
            "match_all": {}
      }
   }
}
```

Even for this simple mistake, the error output is a bad user experience: https://cloudup.com/cuHH-WJ9Uaf

This output:
- Repeats the same error multiple times which means that the user doesn't read any of it.
- The query is not pretty printed so it is hard to parse.
- Just tells the user what is illegal rather than saying what was expected. (Not clear to me if the expected output was the part of my original suggestion that was not feasible vs the suggestion of returning a JSON structure).

If the fix is to reformat the output, then it would be nice to address these points as well.
</comment><comment author="clintongormley" created="2014-11-29T14:17:31Z" id="64953191">Closing as duplicate of #3303
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoShape Precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2804</link><project id="" key="" /><description>The `geo_shape` precision could be only set via `tree_levels` so far. A new option `precision` now allows to set the levels of the underlying tree structures to be set by distances like `50m`. The default unit for this option is `meters`. In case of both options, `tree_levels` and `precision`, are set the greater depth of these values will be taken into account.
## Example

``` json
curl -XPUT 'http://127.0.0.1:9200/myindex/' -d '{
    "mappings" : {
        "type1": {
            "dynamic": "false",
            "properties": {
                "location" : {
                    "type" : "geo_shape",
                    "geohash" : "true",
                    "store" : "yes",
                    "precision":"50m"
                }
            }
        }
    }
}'
```
## Changes
- GeoUtils defines the [WGS84](http://en.wikipedia.org/wiki/WGS84) reference ellipsoid of earth
- DistanceUnits refer to a more precise definition of earth circumference
- DistanceUnits for inch, yard and meter have been defined
- Set default levels in GeoShapeFieldMapper to 50m precision

Closes #2803
</description><key id="12219895">2804</key><summary>GeoShape Precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-03-20T10:11:19Z</created><updated>2014-06-25T18:12:19Z</updated><resolved>2013-03-20T13:54:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-20T13:54:55Z" id="15176811">pushed!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoShape precision</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2803</link><project id="" key="" /><description># GeoShape precision

The `geo_shape` precision could only be set via `tree_levels` option. Since setting this option requires extensive knowledge of the underlying data structures, a new option should to be defined to set the levels of these structures by precision. Such a precision in turn should be defined as a distance like `50m`.
</description><key id="12219426">2803</key><summary>GeoShape precision</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-20T09:58:12Z</created><updated>2014-07-08T18:21:53Z</updated><resolved>2013-03-20T13:54:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-20T10:26:43Z" id="15167505">cool stuff - this would also resolve this issue right? #2756 
</comment><comment author="chilling" created="2013-03-20T10:39:37Z" id="15167969">@s1monw yes - by setting the default precision to 50m, quadtrees will have a depth 21 and geohashtree a depth of 9 levels. This will reduce the index size notably.
</comment><comment author="jillesvangurp" created="2013-03-20T11:17:59Z" id="15169451">For geo hashes, it is possible to calculate a sensible geohash length given a latitude/longitude. There's a bit of variation depending on the latitude though since the distance covered by a degree of longitude varies with the latitude and gets smaller the closer you get to the poles. 

I think quad tree has the same issue. The rest of this post is specific for geohashes but should mostly apply to the quad tree implementation as well.

I have a little library that includes function to calculate the appropriate geohash length given a distance in meters and a coordinate. https://github.com/jillesvangurp/geogeometry I think spatial4j has similar functionality.

I used it to generate an overview of how the accuracy of a given geohash length changes with the latitude:

latitude 0 accuracy 1 requires length 10
latitude 0 accuracy 5 requires length 10
latitude 0 accuracy 39 requires length 9
latitude 0 accuracy 153 requires length 8
latitude 0 accuracy 1222 requires length 7
latitude 0 accuracy 4887 requires length 6

## latitude 0 accuracy 39092 requires length 5

latitude 10 accuracy 1 requires length 10
latitude 10 accuracy 5 requires length 10
latitude 10 accuracy 38 requires length 9
latitude 10 accuracy 151 requires length 8
latitude 10 accuracy 1204 requires length 7
latitude 10 accuracy 4813 requires length 6

## latitude 10 accuracy 38517 requires length 5

latitude 20 accuracy 1 requires length 10
latitude 20 accuracy 5 requires length 10
latitude 20 accuracy 36 requires length 9
latitude 20 accuracy 144 requires length 8
latitude 20 accuracy 1148 requires length 7
latitude 20 accuracy 4592 requires length 6

## latitude 20 accuracy 36767 requires length 5

latitude 30 accuracy 1 requires length 10
latitude 30 accuracy 5 requires length 10
latitude 30 accuracy 34 requires length 9
latitude 30 accuracy 133 requires length 8
latitude 30 accuracy 1058 requires length 7
latitude 30 accuracy 4234 requires length 6

## latitude 30 accuracy 33895 requires length 5

latitude 40 accuracy 1 requires length 10
latitude 40 accuracy 5 requires length 10
latitude 40 accuracy 30 requires length 9
latitude 40 accuracy 117 requires length 8
latitude 40 accuracy 936 requires length 7
latitude 40 accuracy 3744 requires length 6

## latitude 40 accuracy 29989 requires length 5

latitude 50 accuracy 1 requires length 10
latitude 50 accuracy 5 requires length 10
latitude 50 accuracy 25 requires length 9
latitude 50 accuracy 99 requires length 8
latitude 50 accuracy 786 requires length 7
latitude 50 accuracy 3144 requires length 6

## latitude 50 accuracy 25169 requires length 5

latitude 60 accuracy 1 requires length 10
latitude 60 accuracy 5 requires length 10
latitude 60 accuracy 20 requires length 9
latitude 60 accuracy 77 requires length 8
latitude 60 accuracy 611 requires length 7
latitude 60 accuracy 2445 requires length 6
latitude 60 accuracy 19581 requires length 5

## latitude 60 accuracy 80388 requires length 4

latitude 70 accuracy 1 requires length 10
latitude 70 accuracy 5 requires length 10
latitude 70 accuracy 14 requires length 9
latitude 70 accuracy 53 requires length 8
latitude 70 accuracy 418 requires length 7
latitude 70 accuracy 1675 requires length 6
latitude 70 accuracy 13396 requires length 5

## latitude 70 accuracy 56275 requires length 4

latitude 80 accuracy 1 requires length 10
latitude 80 accuracy 5 requires length 10
latitude 80 accuracy 7 requires length 9
latitude 80 accuracy 27 requires length 8
latitude 80 accuracy 213 requires length 7
latitude 80 accuracy 851 requires length 6
latitude 80 accuracy 6802 requires length 5
latitude 80 accuracy 30506 requires length 4

One of the problems with the current implementation is that it doesn't do any kind of distance filtering on the results. This means that the tree_levels is the only way of controlling the accuracy and it is fixed as part of the mapping. Typically, increasing the levels to improve accuracy is far more expensive than simply choosing a low enough level to get decent accuracy and then doing some post processing.

I have another project, https://github.com/jillesvangurp/geokv, that I use for batch processing geospatial data. It implements geospatial search using my geohash library. Typically, I break things down into blocks of a few hundred meter (i.e. length 7) and use simple distance based filtering to throw out things I'm not interested in.  So, load everything within the  hashes of length 7 that cover the area of interest (circle or polygon) and throw out anything that falls outside the shape.

So you control result set size with the hash length and accuracy with the filtering. If you want to do less work filtering, use a larger hash length. If you want to do less work moving around data, use a smaller hash length.

For large data sets that span the globe with tens to hundreds of thousands of objects in cities like Berlin, specifying an accuracy of 1m would translate into a hashlenth of 10 (I think this is the default in es currently). Even very modest data sets result in extremely large indices with this. I've done some experimentation with open street map data: https://www.dropbox.com/sh/ulr6s08km87o17o/dCNsRgiyTT. Indexing this 35 MB file with the default settings results in nearly a GB of index data.

So, having just an accuracy setting might be kind of deceptive. What is really needed is a way to control the amount of terms generated at indexing and querying time and enabling/disabling any post processing. For most users a length of 7 or 8 with post processing would probably be vastly superior to indexing everything at length 10 (or worse).
</comment><comment author="s1monw" created="2013-03-20T12:51:18Z" id="15173786">I agree @jillesvangurp there is a lot to improve here. I think we will open followup issues to address your concerns. For now I think having a precision in meters at equator level is a net/net win for lots of users. We will go off and explore the ability to dynamically adjust the levels for the prefix tree based on the latitude of the incoming object / search request / shape etc. and go even further and allow to set precision based on `(lat,distance_precision)` pairs so you can adjust this as you go north/south. Does this make sense?
</comment><comment author="jillesvangurp" created="2013-03-20T12:53:16Z" id="15173878">That would work. 
</comment><comment author="s1monw" created="2013-03-20T12:54:43Z" id="15173940">cool stuff...
</comment><comment author="chilling" created="2013-03-20T13:18:07Z" id="15174996">Thanks a lot for your comments. You are absolutely right that the definition of precision might be deceptive since shorter `geohashes`/`tree_levels` provide the same accuracy at latitudes away from the equator. My implementation so far guarantees that the cell size will not exceed the distance defined by `precision`. But to respect this fact to reduce the index size, we need support in Lucene. I'm going to work on this.
</comment><comment author="s1monw" created="2013-03-20T13:55:24Z" id="15176835">I marked this as breaking since we changed the defaults to be 50m on equator level for both quadtree and geohash
</comment><comment author="dsmiley" created="2014-02-19T19:02:59Z" id="35534982">FWIW on the Solr side, you specify precision as a distance, and then under the hood, Lucene-spatial's SpatialPrefixTree (geohash or quad impl) figures out how many levels are needed to satisfy that for any location on the globe, be it the equator or pole.  Sure, it might be more levels than needed for stuff at the pole but I think it's not worth bothering trying to optimize that.

@jillesvangurp  had some interesting comments on spatial information-retrieval algorithms using a combination of a grid and then at a certain detail level one does a simple iteration/scan to check the distance.  But guess what?  Lucene-spatial's RecursivePrefixTreeStrategy's Intersects &amp; IsWithin already does this!  It's been there in various forms since 2011 -- a long time.  What you can configure is the "prefixGridScanLevel", which is the level at which the algorithm switches from recursive grid decomposition to scanning.  Does ES expose it?  It should.  By default it's 4 up from the bottom, which was about right for geohashes and the benchmark I ran long ago.  I have plans to optimize this to use the docFreq as a better guide rather than a fixed #.  And, what I'd like to do is make a better tree encoding that won't generate intermediate cells a the bottom few levels, which usually won't be seek()'ed to anyway due to the scanning algorithm kicking in above that.

My closing remark is that people should please submit issues to Lucene's JIRA flagged with the spatial module for improving Lucene-spatial. 
</comment><comment author="jillesvangurp" created="2014-02-19T19:31:17Z" id="35538100">+1 for @dsmiley 

Right now you have to choose between accuracy or performance. We should be able to get both (within reason).
</comment><comment author="dsmiley" created="2014-02-19T20:20:06Z" id="35543599">I feel that for indexed point-data, accuracy &amp; performance has been very good for a long time.  Performance can be improved, sure, but IMO it's darned good today.  The only knock on accuracy is LUCENE-4978 but there are work-arounds: either index a micro-circle/box instead of a point, or slightly buffer the query shape.

For indexing non-point data (e.g. polygons), it's another story -- there's a big trade-off on accuracy &amp; precision.  I plan to address that in ES on issue #2361 with Lucene 4.7's new [SerializedDVStrategy](https://issues.apache.org/jira/browse/LUCENE-5408).  Subscribe to that issue to stay informed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support for partial document updates in the Bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2802</link><project id="" key="" /><description>I added support for partial document updates in the bulk API. We are using this functionality for one month now in our development environment. Script functionality is not supported via this change and i'm not planning to make it either.
</description><key id="12217275">2802</key><summary>Support for partial document updates in the Bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">msimons</reporter><labels /><created>2013-03-20T08:40:24Z</created><updated>2014-07-16T21:53:46Z</updated><resolved>2013-05-28T08:46:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-28T07:17:40Z" id="18534714">Can we close this PR since we committed 9ddd675a02431a72daf48da991ce7eda726f1c79 ?

@msimons does your implementation have more features than the one already included? Even if this is the case, it would be great if you could rebase your current PR against master, as this one is quite hard to merge/take a look at. Thanks a lot!
</comment><comment author="msimons" created="2013-05-28T08:40:44Z" id="18537820">@spinscale my implementation only supports partial document updates without script support. My commit doesn't have more features then the commit 9ddd675. I will try that implementation soon. You can close this PR for now. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>missing not working in filters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2801</link><project id="" key="" /><description>I am working on a query to do some searching between multiple indexes with different fields and I am having an issue with the location filter. 

The error message that I am getting is "QueryParsingException[[&lt;index&gt;] failed to find geo_point field [location.lat_lon]". 

The bool statements with the exists clause in the query section fixes this issue but I have not been able to find a way to work around it in the filters section.  I have tried every combination I can think of with and, or and bool to get this to work.

I am running my query against an alias and what I am looking for is test1 that matches "Dallas" in title or description and test2 that matches "Dallas" in description or location.lat_lon within 50 miles of [-96.795402, 32.778149].  The boolean in the query section works so that I do not get an error when test2 does not have the field title but is does not work within the filters section when test1 does not have the field location.lat_lon.  If I run the query against just test2 I do not get any errors.

I talked with Clint via the google group(https://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/missing/elasticsearch/msN_XS3Ad2M/8Xw2CI7yMqYJ) and his findings were:
It does indeed fail if the field is not in the mapping of whichever index it looks at first (which varies per shard and per run)

The setup that I have is:

index test1

``` ruby
curl -X POST http://localhost:9200/test1 -d '{
  "mappings": {
    "a": {
      "properties": {
        "id": {
          "type": "string",
          "index": "not_analyzed",
          "include_in_all": true
        },
        "title": {
          "type": "string",
          "analyzer": "snowball",
          "boost": 2.0
        },
        "description": {
          "type": "string",
          "analyzer": "snowball"
        }
      }
    }
  }
}'
```

index test2

``` ruby
curl -X POST http://localhost:9200/test2 -d '{
  "mappings": {
    "b": {
      "properties": {
        "id": {
          "type": "string",
          "index": "not_analyzed",
          "include_in_all": true
        },
        "description": {
          "type": "string",
          "analyzer": "snowball",
          "boost": 2.0
        },
        "location": {
          "properties": {
            "lat_lon": {
              "type": "geo_point",
              "lat_lon": true
            }
          }
        }
      }
    }
  }
}'
```

my alias is set up as

``` ruby
curl -X POST "http://localhost:9200/_aliases" -d '{
  "actions":[
    {"add":{
      "index":"test1",
      "alias":"u1"
      }
    },
    {"add":{
      "index":"test2",
      "alias":"u1"
      }
    }
  ]
}'
```

my query is

``` ruby
'{
  "query": {
    "custom_filters_score": {
      "query": {
        "bool": {
          "should": [
            {
              "bool": {
                "must": [
                  {
                    "query_string": {
                      "query": "_exists_:title"
                    }
                  },
                  {
                    "query_string": {
                      "query": "title:Dallas",
                      "boost": 2.0
                    }
                  }
                ]
              }
            },
            {
              "bool": {
                "must": [
                  {
                    "query_string": {
                      "query": "_exists_:description"
                    }
                  },
                  {
                    "query_string": {
                      "query": "description:Dallas",
                      "boost": 2.0
                    }
                  }
                ]
              }
            }
          ],
          "minimum_number_should_match": 1
        }
      },
      "filters": [
        {
          "filter": {
            "or": [
              {
                "missing": {
                  "field": "location.lat_lon"
                }
              },
              {
                "geo_distance": {
                  "distance": "50mi",
                  "location.lat_lon": [
                    -96.795402,
                    32.778149
                  ]
                }
              }
            ]
          },
          "boost": 2.0
        }
      ]
    }
  },
  "size": 20,
  "from": 0
}'
```
</description><key id="12201997">2801</key><summary>missing not working in filters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">swarzech</reporter><labels><label>:Geo</label><label>adoptme</label><label>bug</label></labels><created>2013-03-19T21:11:46Z</created><updated>2015-09-19T17:26:25Z</updated><resolved>2015-09-19T17:26:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-21T06:53:17Z" id="15222056">seems like a bug. I will try looking into this.
</comment><comment author="clintongormley" created="2013-03-21T11:38:43Z" id="15231713">@s1monw to be clear, the issue is that the geopoint only exists in one index, so searching on two indices causes the geopoint not found error (depending on which index is checked first)
</comment><comment author="markharwood" created="2014-07-10T16:24:09Z" id="48628288">The challenge here is that the required logic for the search is that if the "missing" part of the "OR" filter is satisfied then the normal "geo_distance" filter parsing logic should ignore any of the usual (and sensible) error reporting it has around requiring valid field names.

There are possibly 2 fixes we could make to ES to make this work:

1) Add an "ignore_missing" property to the geo_distance filter in which case it will not complain noisily about the absent field and match nothing
2) Introduce e a generic wrapper filter e.g. an extension to the "exists" filter which only attempts to parse a nested "filter" clause if the required field name exists on the index.

What we can't do is make the geo_distance filter parser know about the sibling "missing" clause in this example as this violates our design principle of composable, extensible filters not needing to have intimate knowledge about the implementations of parents/children/siblings etc in the query tree
</comment><comment author="owainb" created="2014-08-14T16:06:44Z" id="52204073">I'm having a similar problem with a geo bounding box filter aggregation as described and demonstrated at https://groups.google.com/d/topic/elasticsearch/6S2JFW0HJG4/discussion. It would be great if a solution could be found for this.
</comment><comment author="clintongormley" created="2014-11-29T13:45:45Z" id="64952297">I think the geo-filters (and geo-sort) need some parameter which indicates how to deal with missing geo values and geo-points.

#7039 deprecated the `ignore_unmapped` parameter in favour of the `unmapped_type` parameter.  Maybe it is just a matter of adding that to geo, or should we allow a default value to be used instead?
</comment><comment author="clintongormley" created="2015-09-19T17:26:25Z" id="141691538">Closing in favour of https://github.com/elastic/elasticsearch/issues/12016
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Stemmer override with large rule set causing java.lang.OutOfMemoryError: Java heap space</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2800</link><project id="" key="" /><description>Hello,

I was trying to create czech lemmatisation analyzer using [stemmer override filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/stemmer-override-tokenfilter.html) and czech dictionary from [aspell](ftp://ftp.gnu.org/gnu/aspell/dict/cs).

This dictionary contains around `300 000` words in base form and some suffix/prefix rules. After expansion format file looks like this

```
Aakjaer Aakjaerech Aakjaery Aakjaer&#367;m Aakjaer&#367; Aakjaerem Aakjaere Aakjaerovi Aakjaeru Aakjaera Aakjaerov&#233;
Aakjaerov&#225; Aakjaerov&#253;mi Aakjaerov&#253;m Aakjaerov&#253;ch Aakjaerovou Aakjaerov&#233;
Aakjaer&#367;v Aakjaerov&#253;ma Aakjaerov&#253;mi Aakjaerov&#253;ch Aakjaerovou Aakjaerovo Aakjaerovy Aakjaerovi Aakjaerov&#253;m 
```

each line is one word with its forms. 

Because of rules format `form =&gt; lemma` the final rule set is expanded from `300 000` to `4 364 674` lines. 

When I was trying on my local machine to index czech wikipedia pages (around `400 000` documents) `java.lang.OutOfMemoryError: Java heap space` error occured after approx 10 minutes of indexing (log file [here](https://gist.github.com/vhyza/9f88b921a1d0f650ccd8#file-elasticsearch-log))

I'm using snapshot build of elasticsearch ([54e7e309a5d407b2fb1123a79e6af9d62e41ea1e](https://github.com/elasticsearch/elasticsearch/commit/54e7e309a5d407b2fb1123a79e6af9d62e41ea1e)), `JAVA_OPTS -Xss200000 -Xms2g -Xmx2g` with no other indices.

Index settings/mapping and river settings are in separate [gist](https://gist.github.com/vhyza/9f88b921a1d0f650ccd8#file-stemmer_override_test-sh)

I was trying to achieve this functionality using [synonym token filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/synonym-tokenfilter.html), because of better format of synonym rules - `form1, form2, form3, form4 =&gt; lemma` (so number of rules are only about `300 000`).

But it's not the same. In the case of using `stemmer override filter`, when token was not found in rule set, stemmer was used. I probably can do the same by adding [keyword marker](http://www.elasticsearch.org/guide/reference/index-modules/analysis/keyword-marker-tokenfilter.html) and [stemmer](http://www.elasticsearch.org/guide/reference/index-modules/analysis/stemmer-tokenfilter.html) in the filter chain, but I don't think it is the right way to do that.

Please, is there some better 'compressed' format of `stemmer override filter` rules? Any thoughts how to avoid `java.lang.OutOfMemoryError: Java heap space` error?
</description><key id="12195371">2800</key><summary>Stemmer override with large rule set causing java.lang.OutOfMemoryError: Java heap space</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">vhyza</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-03-19T18:42:18Z</created><updated>2013-03-21T06:59:14Z</updated><resolved>2013-03-21T06:59:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-20T14:17:42Z" id="15178026">hey, I opened a [Lucene issue](https://issues.apache.org/jira/browse/LUCENE-4857) for this and I will port the quick fix to master. I think the main reason is that this map gets copied each time the filter is created which seems to kill your application in the first place.
</comment><comment author="vhyza" created="2013-03-20T14:36:30Z" id="15179148">Great, thanks a lot!
</comment><comment author="vhyza" created="2013-03-20T14:46:53Z" id="15179815">Just out of curiosity, I tried "workaround" with `synonym token filter`, I mentioned before, with combination with `keyword marker` and everything seems ok. During indexing about `400 000` documents HEAP grows up to 1.6gb /1.9gb and then it drops to around 500mb /1.9gb. I'd like to ask if this combination of `synonym token filter` and `keyword marker` will have memory/performance issues, because of dictionary duplication (synonym and keyword marker dictionaries have each `300 000` lines)
</comment><comment author="s1monw" created="2013-03-20T17:22:45Z" id="15190253">I added a [patch to lucene](https://issues.apache.org/jira/browse/LUCENE-4863) with a more efficient solution similar to SynonymFilter I also patched ES with this impl.
</comment><comment author="vhyza" created="2013-03-20T23:20:56Z" id="15210283">Thanks a lot. I tried it and it is working fine. When indexing, memory consumption is similar to `synonym token filter` solution - HEAP grows up to 1.6gb /1.9gb and then it drops
</comment><comment author="s1monw" created="2013-03-21T06:54:57Z" id="15222090">@vhyza thanks for verifying. I will pull this in soon.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Specialise the default codec to reuse Lucene41 files in the default case </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2799</link><project id="" key="" /><description>today we use a BloomFilter format for out uid field which almosts doubles the # of files per segment since it's a dedicated PostingsFormat. Yet, since this is the default for I'd say almost all people (I don't expect many folks to no use the BloomFilter on the uid field) we should optimise for the common case
</description><key id="12183495">2799</key><summary>Specialise the default codec to reuse Lucene41 files in the default case </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-19T14:50:39Z</created><updated>2013-03-20T11:44:01Z</updated><resolved>2013-03-20T11:44:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Filter strategy comments and code inconsistent</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2798</link><project id="" key="" /><description>The comments in XFilteredQuery claim that the strategy will fall back to LEAP_FROG_FILTER FIRST https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L186 
but the code refers to:
-  LEAP_FROG_QUERY_FIRST https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L208 and 
- QUERY_FIRST https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L216

is this logic correct?
</description><key id="12150158">2798</key><summary>Filter strategy comments and code inconsistent</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/jpountz/following{/other_user}', u'events_url': u'https://api.github.com/users/jpountz/events{/privacy}', u'organizations_url': u'https://api.github.com/users/jpountz/orgs', u'url': u'https://api.github.com/users/jpountz', u'gists_url': u'https://api.github.com/users/jpountz/gists{/gist_id}', u'html_url': u'https://github.com/jpountz', u'subscriptions_url': u'https://api.github.com/users/jpountz/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/299848?v=4', u'repos_url': u'https://api.github.com/users/jpountz/repos', u'received_events_url': u'https://api.github.com/users/jpountz/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/jpountz/starred{/owner}{/repo}', u'site_admin': False, u'login': u'jpountz', u'type': u'User', u'id': 299848, u'followers_url': u'https://api.github.com/users/jpountz/followers'}</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-03-18T19:37:31Z</created><updated>2014-07-03T20:24:36Z</updated><resolved>2014-07-03T20:24:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-21T22:04:50Z" id="15268774">will look into this soonish!
</comment><comment author="clintongormley" created="2014-07-03T19:38:24Z" id="47975143">@jpountz could you check this - just to make sure there aren't any more surprises in this code?
</comment><comment author="jpountz" created="2014-07-03T20:24:36Z" id="47980422">@clintongormley I think the confusion comes from the fact that `LEAP_FROG_FILTER FIRST` is returned through the parent class: `RandomAccessFilterStrategy`. I doubled-checked the logic of this class and it looks good to me.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>TermsFacet across index returns wrong results (0.90)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2797</link><project id="" key="" /><description>When faceting on more than one index, each with one shard, no replicas, and containing the same data structure, TermsFacet will return the wrong counts. This can be easily verified by running a simple query with the facet value. Additionally, changing the size parameter will cause facet results to change (while we only expected to get a shorter/longer list , we actually see values change and terms disappear).

Our faceting query:

``` javascript
{
  "from" : 0,
  "size" : 0,
  "query" : {
    "filtered" : {
      "query" : {
        "query_string" : {
          "query" : "test",
          "fields" : [ "title", "topic" ]
        }
      },
      "filter" : {
        "numeric_range" : {
          "topic_date" : {
            "from" : "2012-12-17T00:00:00.000+02:00",
            "to" : "2013-03-17T23:59:59.999+02:00",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      }
    }
  },
  "facets" : {
    "termFacet" : {
      "terms" : {
        "fields" : [ "author" ],
        "size" : 10
      },
      "facet_filter" : {
        "numeric_range" : {
          "topic_date" : {
            "from" : "2012-12-17T00:00:00.000+02:00",
            "to" : "2013-03-17T23:59:59.999+02:00",
            "include_lower" : true,
            "include_upper" : true
          }
        }
      }
    }
  }
}
```
</description><key id="12139347">2797</key><summary>TermsFacet across index returns wrong results (0.90)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-03-18T15:45:20Z</created><updated>2014-02-08T09:23:24Z</updated><resolved>2013-03-18T18:39:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-03-18T17:06:08Z" id="15067361">IMHO, that's somehow the same issue as #1305, isn't it?
</comment><comment author="synhershko" created="2013-03-18T17:10:33Z" id="15067614">I'm familiar with that issue, but people are reporting there single shards of multiple indexes works for them, and use that approach as a workaround. If this indeed is the same issue, this becomes a regression issue.

Either way, we would really like to have this fixed...
</comment><comment author="dadoonet" created="2013-03-18T17:21:31Z" id="15068315">What people are reporting as far as I understand is that when your narrow your search on a single shard, you will will have the exact values. But I can be wrong here.
</comment><comment author="clintongormley" created="2013-03-18T18:39:15Z" id="15073184">This is indeed a duplicate of #1305 and there is no easy fix, just workarounds:
1. use a single shard, or
2. request more terms than you actually need

It is the nature of distributed search...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoJSONShapParser can parse MultiLineString and GeometryCollection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2796</link><project id="" key="" /><description>Here is an update of the GeoJSON Shape Parser so it can parse MultiLineString and GeometryCollection.

As far as I know, this was the only reason why elasticsearch geo_shape type was not supporting MultiLineString and GeometryCollection.
</description><key id="12135331">2796</key><summary>GeoJSONShapParser can parse MultiLineString and GeometryCollection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/colings86/following{/other_user}', u'events_url': u'https://api.github.com/users/colings86/events{/privacy}', u'organizations_url': u'https://api.github.com/users/colings86/orgs', u'url': u'https://api.github.com/users/colings86', u'gists_url': u'https://api.github.com/users/colings86/gists{/gist_id}', u'html_url': u'https://github.com/colings86', u'subscriptions_url': u'https://api.github.com/users/colings86/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/236731?v=4', u'repos_url': u'https://api.github.com/users/colings86/repos', u'received_events_url': u'https://api.github.com/users/colings86/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/colings86/starred{/owner}{/repo}', u'site_admin': False, u'login': u'colings86', u'type': u'User', u'id': 236731, u'followers_url': u'https://api.github.com/users/colings86/followers'}</assignee><reporter username="">Volune</reporter><labels /><created>2013-03-18T14:21:09Z</created><updated>2014-08-01T13:08:19Z</updated><resolved>2014-08-01T13:08:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-07-30T14:52:16Z" id="21796308">just fyi: multilinestring is supported in the meantime. Need to investigate why GeometryCollection isnt. Out of curiousity: Do you have a particular use-case for that one?
</comment><comment author="colings86" created="2014-08-01T13:07:51Z" id="50881578">The geo_shape parsing code has changed quite a bit since this PR was created.  I have re-implemented the functionality on the up to date codebase in #7123. Closing this PR in favour of the new version
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field Data: optimize long type to use narrowest possible type automatically</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2795</link><project id="" key="" /><description>We load the values to memory, while we load, we can detect what is the narrowest type we need to in order to represent it in memory. This is the main reason why typically an explicit numeric type would be used in mapping, which is now no longer needed really...
</description><key id="12129645">2795</key><summary>Field Data: optimize long type to use narrowest possible type automatically</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-18T11:36:52Z</created><updated>2013-03-18T11:37:23Z</updated><resolved>2013-03-18T11:37:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Downloads have incomplete notices and license texts</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2794</link><project id="" key="" /><description>The downloads are missing the LGPL 2.1 license text and some kind of notice for both JNA and JTS. Getting the sources would be good too.
The Snappy notice is missing too (even though it is deprecated, it is still distributed)
As well as the Sigar notice and may be a few other.

@kimchy Tell me if you want a pull request for this
It could make sense to have a doc folder where the notices lay, as well as keep in Git the sources for the LGPL-licensed bits to honor source code redistribution.
</description><key id="12128772">2794</key><summary>Downloads have incomplete notices and license texts</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/clintongormley/following{/other_user}', u'events_url': u'https://api.github.com/users/clintongormley/events{/privacy}', u'organizations_url': u'https://api.github.com/users/clintongormley/orgs', u'url': u'https://api.github.com/users/clintongormley', u'gists_url': u'https://api.github.com/users/clintongormley/gists{/gist_id}', u'html_url': u'https://github.com/clintongormley', u'subscriptions_url': u'https://api.github.com/users/clintongormley/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/56599?v=4', u'repos_url': u'https://api.github.com/users/clintongormley/repos', u'received_events_url': u'https://api.github.com/users/clintongormley/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/clintongormley/starred{/owner}{/repo}', u'site_admin': False, u'login': u'clintongormley', u'type': u'User', u'id': 56599, u'followers_url': u'https://api.github.com/users/clintongormley/followers'}</assignee><reporter username="">pombredanne</reporter><labels><label>:Packaging</label><label>enhancement</label><label>low hanging fruit</label><label>v2.0.0-beta1</label></labels><created>2013-03-18T11:10:33Z</created><updated>2015-06-17T16:06:56Z</updated><resolved>2015-06-17T16:06:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-12-10T16:00:23Z" id="66473935">Hi @pombredanne 

Apologies it has taken so long to reply.  Trove and Snappy have been removed a while ago. Sigar, JNA and JTS are imported, and distributed as part of the Elasticsearch package, but (as I understand it) are not linked.  From my reading, we need to include a notice explaining the licenses of these works (and linking to the source), but are not required to provide the source itself.

Is my understanding here correct?  If so, and you are still up for sending a PR, that'd be awesome.
</comment><comment author="paulbakker" created="2014-12-11T11:31:05Z" id="66606530">I'm currently looking into this as well because I'm trying to embed ElasticSearch in Amdatu, which is also Apache licensed. As I understand it, it would only be allowed to use a LGPLv2 libraries when they are completely optional. This is currently not the case, because there is a compile time dependency on JTS classes.

Sources: http://copyleft.org/guide/comprehensive-gpl-guidech11.html#x14-9800010.4 and http://www.apache.org/legal/resolved.html.
</comment><comment author="pombredanne" created="2014-12-13T09:55:29Z" id="66871314">@paulbakker @clintongormley  
I am not a lawyer and this is not a legal advice ....

That said, the policies of the Apache Foundation wrt. LGPL are for own-released code, not for ES or anybody else. The interpretation of copyleft.org linked by @paulbakker  above is rather clear, and MO is that the general accepted approach is that it is OK for Apache-licensed Java code to depend on unmodified LGPL-licensed third-party Jars without having these Jars have an impact of the Apache licensing. This would considered as a work "using the library". The requirements of the LGPL still apply of course to the LGPL-licensed library. 

@clintongormley linking as understood when interpreting the LGPL does not really matter in Java as I would understand this to be of dynamic nature.

My point here is that for ES to comply with these requirements, there are a few essential things to do such as ensuring the proper license texts and credit notices are produced where needed and that the corresponding source code is available for redistribution. And yes, the source code need to provided (either on request or simplest side-by-side or in the download itself) and this is not a requirement that can be delegated elsewhere IMHO.

Note also that beyond LGPL-licensed components, the notices included in ES are rather minimalist and possibly incomplete and I would expect more from an open source project.

All in all not a lot of work though @clintongormley I think it would be best for a project member to work on that than me ...

## 

Cordially
</comment><comment author="kimchy" created="2014-12-13T10:42:07Z" id="66872364">Yea, we need to add JTS, JNA to the notice page, mention they use LGPLv2 license with relevant links to the project pages in the notice, we should also mention that they are optional (the system can run without them). Same for Sigar, except its under Apache2 version (and optional as well). This is similar to what projects like spring-framework do, where they integrate with Hibernate (LGPL) on the ORM layer, but its not required to run spring-framework.

We should go over and add any missing libraries we depend on and make sure we list all of them under the notice file (like Guava, HPPC).

@clintongormley I agree that a project member should do it.
</comment><comment author="pombredanne" created="2014-12-13T10:43:55Z" id="66872401">FWIW, the latest JNA 4.x is dual licensed LGPL or Apache, though it does not mean that the proper credit notice should not be produced IMHO 
</comment><comment author="kimchy" created="2014-12-13T10:46:23Z" id="66872452">@pombredanne thanks, yea, we should see if we can upgrade to it soonish. Agreed on notice, it is just a miss on our end, but we should put in the notice file all the libraries we use, the license they are under, the URL to the project page, and if its optional or not.
</comment><comment author="pombredanne" created="2014-12-13T10:51:40Z" id="66872570">@kimchy you are already using JNA 4.1.0 ... and I assume you would be selecting the Apache license ... ;)
</comment><comment author="kimchy" created="2014-12-13T11:10:34Z" id="66873056">@pombredanne argh, I read the sentence wrongly, of course, we choose Apache, same way we do with Jackson :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tar.gz and Zip downloads are different: the tarball should have the same content as the zip</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2793</link><project id="" key="" /><description>@kimchy There is no reason IMHO for these to be different, which is quite surprising.

The .zip has extra files for Windows-only:

``` bin/elasticsearch.bat
bin/plugin.bat
lib/sigar/sigar-amd64-winnt.dll
lib/sigar/sigar-x86-winnt.dll
```

This means that the tar.gz distro CANNOT be used on Windows in practice.

The pull request fixes the file sets, though there is may be a simpler way to do this
</description><key id="12126909">2793</key><summary>Tar.gz and Zip downloads are different: the tarball should have the same content as the zip</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pombredanne</reporter><labels><label>adoptme</label></labels><created>2013-03-18T10:06:33Z</created><updated>2014-10-21T21:41:06Z</updated><resolved>2014-09-08T17:47:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-18T10:13:17Z" id="15047114">its intentional..., the assumption is that `tar.gz` will be used on unix, or cygwin if pushing it... . Is this an actual problem?
</comment><comment author="pombredanne" created="2013-03-18T12:56:35Z" id="15053063">Yes, I think this is a problem at least this is a problem to me.
I cannot use one distro on multiple OS:
- if I take the zip, I will have to fix the perms of shell scripts on posix
- if I take the tarball, I cannot run on Windows

My initial assumption was that the two distros would have at least the same content (baring differences between zip and tar, such as permissions).
I cannot see a good reason why the tarball would not be working on windows too.
</comment><comment author="kimchy" created="2013-03-18T12:58:20Z" id="15053148">I just explained the reasons for that above, typically, you don't use tar.gz on windows platform, and I don't want to "pollute" the tar.gz with bat files.
</comment><comment author="pombredanne" created="2013-03-18T13:14:11Z" id="15053813">I get your point, but this make it harder and surprising on users for no good reason imho. There is nothing that says that zip are windows only and not Linux or posix OSes and tar.gz is not for windows.
If anything this is a rather uncommon way to distribute things as in general when there is both a zip and a tarball, both have the same content, just a different archive format (which is the general case for instance at Apache).

About "polluting", note that you still bundle: `lib/sigar/sigar-x86-winnt.lib` which is for Windows only as well as JNA DLLs (which are part of the standard JNA) and end up both distributed in the tarball. And pushing it further, and me just being silly, you would not distribute MacOSX and Sun sigar binaries for Linux then ;) and remove all the various posix binaries and shell scripts from the zip, which would not make sense and MO would be even weirder.

With your current packaging, no distribution works out of the box on all supported OSes.
</comment><comment author="kimchy" created="2013-03-18T21:45:39Z" id="15084027">The winnt one is a bug, we shouldn't include it in the `tar.gz`, we exclude the dlls, but by mistake we didn't exclude that one, I will fix it. We don't control jna, it comes with whatever it comes with. And when I said unix, I meant OS that supports `bin/sh`.
</comment><comment author="kimchy" created="2013-03-18T21:47:08Z" id="15084118">Don't mind keeping this open, if other people will come across this and think that its valuable, let them vote here, and lets see how it goes. If enough people feel strongly about it, then we can reconsider.
</comment><comment author="pombredanne" created="2013-03-18T22:20:31Z" id="15085804">If that is the route you want take, be kind enough to not remove the posix scripts from the zip, so there still is at least one pre-built download that works across all OSes
</comment><comment author="kimchy" created="2013-03-18T22:25:23Z" id="15086050">we don't remove the posix scripts from the zip...
</comment><comment author="CptnKirk" created="2013-03-19T04:11:21Z" id="15096588">If you'd like to have separate OS specific distributions of your product, be my guest.  However, I don't feel that packaging format is a good indicator of OS support.  Having an elasticsearch-version-win.zip and elasticsearch-version-unix.tgz as the only officially available packages for these distributions would also seem reasonable.  

But if I just see a downloads page that lists the same package in two different formats, I assume that the same product was simply repackaged for user convenience.  And like others, if I'm in mixed OS company, I'll often just download the .zip package because it's the easiest thing to grok for the largest number of my users.

That's my vote.  Either explicitly called out OS support, or pile everything into both package formats. 
</comment><comment author="kimchy" created="2013-03-19T09:14:33Z" id="15104055">@CptnKirk "I'll often just download the .zip package because it's the easiest thing to grok for the largest number of my users..." &lt;&lt; thats my reasoning as well, thats why the zip package includes everything, both *nix and windows.
</comment><comment author="iksnalybok" created="2013-05-21T20:01:36Z" id="18235455">I share the same idea. I really do not expect a zip and a tar.gz to have different content (and especially when it's java software), except if it is clearly stated, and the differences are explained. You may want to keep several packagings, but the target should made be obvious in the archive name, or at least in the download page. For my part, I often download the tar.gz (or tar.bz2, tar.xz) because it's generally smaller than the zip.
</comment><comment author="pombredanne" created="2014-08-03T15:51:46Z" id="50993948">This is still an issue, none can rightfully expect a zip and tar using the same base name to be different in content and I got bitten by this again today. Sad that this was not pulled when still fresh.  @kimchy I guess you may still feel strongly about this based on your comments on #3702 ? 
FWIW, yours is the _only project_ I know of that has this weird approach. I consider this an aberration personally no offense meant ;)  FWIW, the venerable Tomcat, Lucene and all Apache projects package zips and tars identically. When they want to make the package OS-specific they mark it as such clearly in the package name which is the sane way to do things. 
</comment><comment author="iksnalybok" created="2014-09-08T19:08:12Z" id="54871028">That's good news, thanks.
</comment><comment author="pombredanne" created="2014-09-08T20:00:43Z" id="54878070">Thx mucho ! :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move all generated files during tests to maven target dir</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2792</link><project id="" key="" /><description>Reopening issue #2310:

By now when launching tests, some dir are created under `/data` or `/work` dirs.
IMHO, it's best to use `/target` dir to hold test dirs.

So, here is the change proposal:
- all tests creates data files under `/target/es/data` and work files under `/target/es/work`
- `src/test/resources/es-test.properties` is created and contain `path.data` and `path.work` settings
- `pom.xml` is modified to parse test resources and modify `${project.build.directory}` to maven target dir
- some TestNG tests are modified to use theses settings
- some _main()_ tests (stress tests) are modified to use theses settings
</description><key id="12125280">2792</key><summary>Move all generated files during tests to maven target dir</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-03-18T09:09:02Z</created><updated>2013-09-05T13:18:45Z</updated><resolved>2013-09-05T13:18:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nik9000" created="2013-08-15T14:50:46Z" id="22707747">data and work are no longer created by running regular maven tests.  I'm not sure about the main() tests but maven seems fine now.
</comment><comment author="dadoonet" created="2013-09-05T13:18:30Z" id="23866662">@nik9000 right! 

Thanks to https://github.com/elasticsearch/elasticsearch/commit/ed473e272dfa369de771358a3c6a9b1075dd3d43, it works fine when running `mvn test` from command line. 

That said, when running tests from IntelliJ (I don't know for other IDE), it's still writing in `./` dir.

Closing this issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose zero_terms_query to the mutl_match API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2791</link><project id="" key="" /><description>The MatchQuery already exposes the zero_terms_query property via the match API, this exposes the same to the multi_match API by adding parsing to the MultiMatchQueryParser and building to the MultiMatchQueryBuilder.
</description><key id="12105498">2791</key><summary>Expose zero_terms_query to the mutl_match API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">AngusDavis</reporter><labels /><created>2013-03-17T06:33:10Z</created><updated>2014-07-16T21:53:46Z</updated><resolved>2013-03-23T08:19:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-17T08:12:24Z" id="15019078">looks cool, is there a chance we can get a small testcase for this as well? This would be awesome!
</comment><comment author="AngusDavis" created="2013-03-17T08:33:27Z" id="15019258">I wasn't certain where to put one. SimpleQueryTests was the best I saw. Think that's the best place for it?
</comment><comment author="s1monw" created="2013-03-17T10:52:31Z" id="15020690">yeah that would be a good place. we try to keep the # of test classes low for those queries to make sure we don't start too many nodes (those are integration tests and take quite a while compared to a simple unit test.
</comment><comment author="AngusDavis" created="2013-03-23T00:01:15Z" id="15327287">Let me know if you'd like to see more tests added - I can probably beef it up a bit (was mostly starting with match query's ZeroTermsQuery tests)
</comment><comment author="s1monw" created="2013-03-23T08:19:02Z" id="15333614">looks good. I made one commit out of the two and pushed! Thanks Angus!!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>SearchPhaseExecutionException No active shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2790</link><project id="" key="" /><description>After you reboot the server, we have broken elasticsearch

/tovar/product/_search?pretty=true
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[_na_][tovar][0]: No active shards}{[_na_][tovar][1]: No active shards}{[_na_][tovar][2]: No active shards}{[_na_][tovar][3]: No active shards}{[_na_][tovar][4]: No active shards}]",
  "status" : 500
}
</description><key id="12091465">2790</key><summary>SearchPhaseExecutionException No active shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nekulin</reporter><labels /><created>2013-03-16T06:53:21Z</created><updated>2013-03-16T06:57:43Z</updated><resolved>2013-03-16T06:57:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-03-16T06:57:43Z" id="15000614">I think you have to wait for a yellow cluster status.
Please ask questions on the mailing list. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: dynamic flag is explicitly returned even when not set</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2789</link><project id="" key="" /><description>When we rebuild the mappings based on dynamic additions of fields / wrappers, or explicitly, we set in the serialized form the dynamic field, even when its in its "default" set. We don't do that with other mappings, we should not do it with this one.
</description><key id="12088232">2789</key><summary>Mapping: dynamic flag is explicitly returned even when not set</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-16T00:29:08Z</created><updated>2013-03-16T06:44:29Z</updated><resolved>2013-03-16T00:29:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-16T06:44:29Z" id="15000481">;) thanks shay!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>moved test resources to test resources directory</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2788</link><project id="" key="" /><description>Netbeans miserably complained about that. And it is against the spec. :-)
</description><key id="12085836">2788</key><summary>moved test resources to test resources directory</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jjYBdx4IL</reporter><labels /><created>2013-03-15T23:18:17Z</created><updated>2014-07-16T21:53:47Z</updated><resolved>2013-03-18T03:41:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-15T23:19:38Z" id="14991565">I was never a fan of having the test resources in a separate folder, always personally liked it to be next to the classes being tested.
</comment><comment author="jjYBdx4IL" created="2013-03-16T04:37:35Z" id="14999402">A "personal" standpoint on this is ok, but there is a standard and Netbeans gets red all over the place from it (as should every properly implemented IDE).
</comment><comment author="kimchy" created="2013-03-17T21:01:38Z" id="15030778">This really isn't a standard, and when you import the maven project (which is configured to properly work with it). So, for example, IntelliJ IDEA works well by importing the maven project.
</comment><comment author="jjYBdx4IL" created="2013-03-18T03:41:13Z" id="15037870">ok, you have configured the resource directories in pom.xml. What Netbeans complains about is not that the json files cannot be compiled or that they are in the wrong location, but that they are not valid json files! The files themselves violate the JSON standard. ;-)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix bug in RateLimiter.SimpleRateLimiter causing numeric overflow in StoreStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2787</link><project id="" key="" /><description>Closes #2785
</description><key id="12084470">2787</key><summary>Fix bug in RateLimiter.SimpleRateLimiter causing numeric overflow in StoreStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-03-15T22:33:34Z</created><updated>2014-07-16T21:53:47Z</updated><resolved>2013-03-15T22:38:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-15T22:38:37Z" id="14988744">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RecoveryWhileUnderLoadTests fail</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2786</link><project id="" key="" /><description>in particular:
# recoverWhileUnderLoadAllocateBackupsTest in latest master and v0.20.5 and v0.90.0 beta1.

Would be great to have a continuous integration server so everybody could check if test failures are "ok". ;-) How about builds.apache.org?

Output:

---
##  T E S T S

Running org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests
Configuring TestNG with: org.apache.maven.surefire.testng.conf.TestNG652Configurator@4d86d315
[2013-03-15 21:39:18,323][INFO ][test                     ] ==&gt; Test Starting [integration.recovery.RecoveryWhileUnderLoadTests#recoverWhileUnderLoadAllocateBackupsTest]
[2013-03-15 21:39:18,330][INFO ][test.integration.recovery] --&gt; starting [node1] ...
[2013-03-15 21:39:18,438][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: initializing ...
[2013-03-15 21:39:18,443][INFO ][plugins                  ] [node1] loaded [], sites []
[2013-03-15 21:39:19,749][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: initialized
[2013-03-15 21:39:19,749][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: starting ...
[2013-03-15 21:39:19,826][INFO ][transport                ] [node1] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.0.7:9300]}
[2013-03-15 21:39:22,877][INFO ][cluster.service          ] [node1] new_master [node1][SqutnFSwQ9ORDlQGFSr3cQ][inet[/192.168.0.7:9300]], reason: zen-disco-join (elected_as_master)
[2013-03-15 21:39:22,902][INFO ][discovery                ] [node1] test-cluster-i5/SqutnFSwQ9ORDlQGFSr3cQ
[2013-03-15 21:39:22,910][INFO ][gateway                  ] [node1] recovered [0] indices into cluster_state
[2013-03-15 21:39:22,911][INFO ][http                     ] [node1] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.0.7:9200]}
[2013-03-15 21:39:22,911][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: started
[2013-03-15 21:39:22,911][INFO ][test.integration.recovery] --&gt; creating test index ...
[2013-03-15 21:39:23,126][INFO ][cluster.metadata         ] [node1] [test] creating index, cause [api], shards [5]/[1], mappings []
[2013-03-15 21:39:23,226][INFO ][index.gateway.none       ] [node1] [test][0] deleting shard content
[2013-03-15 21:39:23,332][INFO ][index.gateway.none       ] [node1] [test][1] deleting shard content
[2013-03-15 21:39:23,341][INFO ][test.integration.recovery] --&gt; starting 5 indexing threads
[2013-03-15 21:39:23,343][INFO ][test.integration.recovery] ***\* starting indexing thread 0
[2013-03-15 21:39:23,343][INFO ][test.integration.recovery] ***\* starting indexing thread 1
[2013-03-15 21:39:23,344][INFO ][test.integration.recovery] ***\* starting indexing thread 2
[2013-03-15 21:39:23,345][INFO ][test.integration.recovery] ***\* starting indexing thread 3
[2013-03-15 21:39:23,350][INFO ][test.integration.recovery] --&gt; waiting for 2000 docs to be indexed ...
[2013-03-15 21:39:23,351][INFO ][test.integration.recovery] ***\* starting indexing thread 4
[2013-03-15 21:39:23,396][INFO ][index.gateway.none       ] [node1] [test][2] deleting shard content
[2013-03-15 21:39:23,440][INFO ][index.gateway.none       ] [node1] [test][3] deleting shard content
[2013-03-15 21:39:23,482][INFO ][index.gateway.none       ] [node1] [test][4] deleting shard content
[2013-03-15 21:39:23,494][INFO ][cluster.metadata         ] [node1] [test] update_mapping [type1](dynamic)
[2013-03-15 21:39:24,816][INFO ][test.integration.recovery] --&gt; 2000 docs indexed
[2013-03-15 21:39:24,816][INFO ][test.integration.recovery] --&gt; flushing the index ....
[2013-03-15 21:39:26,094][INFO ][test.integration.recovery] --&gt; waiting for 4000 docs to be indexed ...
[2013-03-15 21:39:26,097][INFO ][test.integration.recovery] --&gt; 4000 docs indexed
[2013-03-15 21:39:26,098][INFO ][test.integration.recovery] --&gt; starting [node2] ...
[2013-03-15 21:39:26,101][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: initializing ...
[2013-03-15 21:39:26,101][INFO ][plugins                  ] [node2] loaded [], sites []
[2013-03-15 21:39:26,430][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: initialized
[2013-03-15 21:39:26,433][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: starting ...
[2013-03-15 21:39:26,590][INFO ][transport                ] [node2] bound_address {inet[/0:0:0:0:0:0:0:0:9301]}, publish_address {inet[/192.168.0.7:9301]}
[2013-03-15 21:39:29,608][INFO ][cluster.service          ] [node2] new_master [node2][Z3y2EP4OQhSXLMSPAkjDPQ][inet[/192.168.0.7:9301]], reason: zen-disco-join (elected_as_master)
[2013-03-15 21:39:29,626][INFO ][discovery                ] [node2] test-cluster-i5/Z3y2EP4OQhSXLMSPAkjDPQ
[2013-03-15 21:39:29,627][INFO ][gateway                  ] [node2] recovered [0] indices into cluster_state
[2013-03-15 21:39:29,641][INFO ][http                     ] [node2] bound_address {inet[/0:0:0:0:0:0:0:0:9201]}, publish_address {inet[/192.168.0.7:9201]}
[2013-03-15 21:39:29,641][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: started
[2013-03-15 21:39:29,641][INFO ][test.integration.recovery] --&gt; waiting for GREEN health status ...
[2013-03-15 21:40:29,670][ERROR][test                     ] ==&gt; Test Failure [integration.recovery.RecoveryWhileUnderLoadTests#recoverWhileUnderLoadAllocateBackupsTest]
[2013-03-15 21:40:29,672][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: stopping ...
[2013-03-15 21:40:29,674][WARN ][test.integration.recovery] ***\* failed indexing thread 2
java.lang.NullPointerException
    at org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests$1.run(RecoveryWhileUnderLoadTests.java:77)
[2013-03-15 21:40:29,750][WARN ][test.integration.recovery] ***\* failed indexing thread 1
java.lang.NullPointerException
    at org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests$1.run(RecoveryWhileUnderLoadTests.java:77)
[2013-03-15 21:40:29,797][WARN ][test.integration.recovery] ***\* failed indexing thread 0
java.lang.NullPointerException
    at org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests$1.run(RecoveryWhileUnderLoadTests.java:77)
[2013-03-15 21:40:29,860][WARN ][test.integration.recovery] ***\* failed indexing thread 3
java.lang.NullPointerException
    at org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests$1.run(RecoveryWhileUnderLoadTests.java:77)
[2013-03-15 21:40:29,982][WARN ][test.integration.recovery] ***\* failed indexing thread 4
java.lang.NullPointerException
    at org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests$1.run(RecoveryWhileUnderLoadTests.java:77)
[2013-03-15 21:40:30,003][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: stopped
[2013-03-15 21:40:30,003][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: closing ...
[2013-03-15 21:40:30,012][INFO ][node                     ] [node1] {0.90.0.Beta2-SNAPSHOT}[7651]: closed
[2013-03-15 21:40:30,012][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: stopping ...
[2013-03-15 21:40:30,019][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: stopped
[2013-03-15 21:40:30,019][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: closing ...
[2013-03-15 21:40:30,021][INFO ][node                     ] [node2] {0.90.0.Beta2-SNAPSHOT}[7651]: closed
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 72.152 sec &lt;&lt;&lt; FAILURE!
recoverWhileUnderLoadAllocateBackupsTest(org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests)  Time elapsed: 71348 sec  &lt;&lt;&lt; FAILURE!
java.lang.AssertionError: 
Expected: &lt;false&gt;
     but: was &lt;true&gt;
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
    at org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests.recoverWhileUnderLoadAllocateBackupsTest(RecoveryWhileUnderLoadTests.java:117)

Results :

Failed tests:   recoverWhileUnderLoadAllocateBackupsTest(org.elasticsearch.test.integration.recovery.RecoveryWhileUnderLoadTests): (..)

Tests run: 1, Failures: 1, Errors: 0, Skipped: 0

---
## BUILD FAILURE

Total time: 1:16.088s
Finished at: Fri Mar 15 21:40:30 UTC 2013
## Final Memory: 17M/337M
</description><key id="12082907">2786</key><summary>RecoveryWhileUnderLoadTests fail</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jjYBdx4IL</reporter><labels /><created>2013-03-15T21:43:25Z</created><updated>2013-06-06T14:46:43Z</updated><resolved>2013-06-06T14:46:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-06T14:46:42Z" id="19050071">closing this. works for 0.90 branch and master.

Also, there is CI in place now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>StoreStats's throttleTimeInNanos overflows causing serialization issues</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2785</link><project id="" key="" /><description>It appears that the new SimpleRateLimiter that was added to Lucene 4.x returns the targetNS (meaning the time when the method should exit) instead of the duration that was actually paused.  This then leads to StoreStats overflowing the throttleTimeInNanos which then causes serialization issues with writeVLong (arithmetic exceptions, etc)
</description><key id="12080523">2785</key><summary>StoreStats's throttleTimeInNanos overflows causing serialization issues</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">AngusDavis</reporter><labels><label>bug</label><label>v0.90.0.Beta1</label></labels><created>2013-03-15T20:38:42Z</created><updated>2013-03-17T04:44:06Z</updated><resolved>2013-03-15T22:38:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-15T21:08:51Z" id="14984989">good catch!! I will look into this asap!
</comment><comment author="s1monw" created="2013-03-15T22:32:20Z" id="14988523">here is the lucene issue including a patch to push this fix upstream: https://issues.apache.org/jira/browse/LUCENE-4836
</comment><comment author="s1monw" created="2013-03-15T22:39:11Z" id="14988764">thank you very much for figuring this out and reporting this! Very much appreciated!
</comment><comment author="kimchy" created="2013-03-15T23:18:31Z" id="14991533">indeed, and I must add to that, very impressive in nailing down why it happens.
</comment><comment author="AngusDavis" created="2013-03-17T04:44:06Z" id="15017390">And thank you for getting this fixed so quickly.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add version to plugins</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2784</link><project id="" key="" /><description>Relative to #2668 

Plugin developpers can now add a version number to their es-plugin.properties file:

``` properties
plugin=org.elasticsearch.test.integration.nodesinfo.TestPlugin
version=0.0.7-SNAPSHOT
```

Also, for site plugins, it's recommended to add a `es-plugin.properties` file in root site directory with `description` and `version` properties:

``` properties
description=This is a description for a dummy test site plugin.
version=0.0.7-BOND-SITE
```

When running Nodes Info API, you will get information on versions:

``` sh
$ curl 'http://localhost:9200/_nodes?plugin=true&amp;pretty'
```

``` javascript
{
  "ok" : true,
  "cluster_name" : "test-cluster-MacBook-Air-de-David.local",
  "nodes" : {
    "RHMsToxiRcCXwHiS6mEaFw" : {
      "name" : "node2",
      "transport_address" : "inet[/192.168.0.15:9301]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9201]",
      "plugins" : [ {
        "name" : "dummy",
        "version" : "0.0.7-BOND-SITE",
        "description" : "This is a description for a dummy test site plugin.",
        "url" : "/_plugin/dummy/",
        "site" : true,
        "jvm" : false
      } ]
    },
    "IKiUOo-LSCq1Km1GUhBwPg" : {
      "name" : "node3",
      "transport_address" : "inet[/192.168.0.15:9302]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9202]",
      "plugins" : [ {
        "name" : "test-plugin",
        "version" : "0.0.7-SNAPSHOT",
        "description" : "test-plugin description",
        "site" : false,
        "jvm" : true
      } ]
    },
    "H64dcSF2R_GNWh6XRCYZJA" : {
      "name" : "node1",
      "transport_address" : "inet[/192.168.0.15:9300]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9200]",
      "plugins" : [ ]
    },
    "mGEZcYl8Tye0Rm5AACBhPA" : {
      "name" : "node4",
      "transport_address" : "inet[/192.168.0.15:9303]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/192.168.0.15:9203]",
      "plugins" : [ {
        "name" : "test-plugin",
        "version" : "0.0.7-SNAPSHOT",
        "description" : "test-plugin description",
        "site" : false,
        "jvm" : true
      }, {
        "name" : "test-no-version-plugin",
        "version" : "NA",
        "description" : "test-no-version-plugin description",
        "site" : false,
        "jvm" : true
      }, {
        "name" : "dummy",
        "version" : "NA",
        "description" : "No description found for dummy.",
        "url" : "/_plugin/dummy/",
        "site" : true,
        "jvm" : false
      } ]
    }
  }
}
```
</description><key id="12075490">2784</key><summary>Add version to plugins</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v1.0.0.RC2</label><label>v1.1.0</label><label>v2.0.0-beta1</label></labels><created>2013-03-15T18:31:26Z</created><updated>2014-05-05T06:31:24Z</updated><resolved>2014-01-30T13:17:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>fixing SocketException handling in MulticastZenPing receiver</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2783</link><project id="" key="" /><description>With this patch, Maven surefire test runs successful on Mac OS X 10.8 (1164 tests successful). 
Without patch, it hangs in infinite loop, logging a socket exception "socket is closed" over and over again.
</description><key id="12069985">2783</key><summary>fixing SocketException handling in MulticastZenPing receiver</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-03-15T16:15:09Z</created><updated>2014-06-13T01:44:09Z</updated><resolved>2013-03-19T21:15:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-17T22:35:16Z" id="15032593">What exception do you actually get? SocketException doesn't necessarily means that its no longer running, it might be transient. 
</comment><comment author="jprante" created="2013-03-17T22:52:51Z" id="15032929">The exception is

java.net.SocketException: Socket closed
        at java.net.PlainDatagramSocketImpl.receive0(Native Method)
        at java.net.AbstractPlainDatagramSocketImpl.receive(AbstractPlainDatagramSocketImpl.java:145)
        at java.net.DatagramSocket.receive(DatagramSocket.java:786)
        at org.elasticsearch.discovery.zen.ping.multicast.MulticastZenPing$Receiver.run(MulticastZenPing.java:374)
        at java.lang.Thread.run(Thread.java:722)

See https://gist.github.com/jprante/5183980

The test is integration.recovery.FullRollingRestartTests#testFullRollingRestart
</comment><comment author="kimchy" created="2013-03-17T22:55:31Z" id="15032968">thats strange, because we only close the socket after we set the running flag to true, so this should not be logged (its only logged if its running). Can you maybe check why its in that state (running on OSX 10.8 and it seems fine).
</comment><comment author="jprante" created="2013-03-17T23:27:55Z" id="15033542">I see this in the JavaDoc of java.net.DatagramSocket:

"Any thread currently blocked in receive(java.net.DatagramPacket) upon this socket will throw a SocketException."

http://docs.oracle.com/javase/7/docs/api/java/net/DatagramSocket.html
</comment><comment author="kimchy" created="2013-03-17T23:37:54Z" id="15033693">Sure, but we only actually close the socket _after_ we set running to false (check the `doStop` method), so I wonder how does the socket ends up being closed, without the `running` variable being set to false... . Since I can't recreate it here, I need your help :) to check the flow at which this failure happens...
</comment><comment author="jprante" created="2013-03-18T00:54:15Z" id="15034899">I'm checking against a series of JDK versions now. Seems odd that a volatile variable can not be written to.
</comment><comment author="kimchy" created="2013-03-18T01:09:25Z" id="15035121">What do you mean volatile variable can't be written to? which version does it fail on? 
</comment><comment author="jprante" created="2013-03-18T01:29:48Z" id="15035446">I tried to replace the volatile `running` variable with a check `Thread.currentThread().isInterrupted()` but it did not change much. With all installed JDK 6/7/8, I get the SocketException "Socket closed". Only some runs with JDK 6 do not throw SocketException, still I don't know the reason why. Have to investigate further.
</comment><comment author="jprante" created="2013-03-18T01:53:21Z" id="15035861">Reworked patch, this one checks for "Socket closed" SocketException and thread interrupt state. Although I would like to prefer to always abort the Receiver thread if there is a SocketException, to avoid a loop.
</comment><comment author="spinscale" created="2013-03-18T11:29:01Z" id="15050014">Hey Joerg,

I am running into this issue as well, but not when running elasticsearch master with standalone tests, but when running the tests of my FST suggester plugin using the current elasticsearch version or master.

However it seems, this issue occurs in my plugin due to a resource leak - are you using elasticsearch and testing some plugin with it or just running the tests on master?
</comment><comment author="jprante" created="2013-03-18T11:35:09Z" id="15050222">Just running on master, no plugins. 

Can you explain what was the resource leak?
</comment><comment author="kimchy" created="2013-03-18T11:45:33Z" id="15050554">@jprante I don't like fixing things without understanding why they happen, and as I mentioned, a SocketException can happen not just because the socket is closed. I still would like to understand why the current logic fails before we try and fix something...
</comment><comment author="spinscale" created="2013-03-18T15:46:22Z" id="15062285">This is really weird.. I can reproduce this on master... but only in about 20% of my test runs...

Used this for a bit more automation:

```
for i in 1 2 3 4 5 6 7 8 9 10 ; do
  mvn -Dtest=FullRollingRestartTests test 2&gt;&amp;1 | tee /tmp/es-log-$i.log | grep 'Tests run' | grep elapsed
done
```

I tried making the MulticastSocket volatile, but that does not change anything. Before going on, I'll test on a different operating system, as everyone involved uses a pretty up-to-date mac I guess.
</comment><comment author="jprante" created="2013-03-18T15:58:17Z" id="15063106">@kimchy Agreed. I have to investigate with extra test code and variations of JVM parameters. Maybe Mac OS X JVM specific.
</comment><comment author="jprante" created="2013-03-18T16:27:28Z" id="15064896">@spinscale I will also test on Solaris, Linux, and Windows.
</comment><comment author="spinscale" created="2013-03-18T18:25:10Z" id="15072268">Ubuntu with the .17 jdk looks stable.
</comment><comment author="jprante" created="2013-03-18T22:48:48Z" id="15087132">From my tests I am quite sure now that executing the test exceeds the default Mac OS X 10.8 process limit of open files. It always breaks at a sudden when the 5th ES node is started up in FullRollingRestartTests. Increasing the process file limit with `ulimit -S -n 2048` (via a root bash shell) does no longer break the test, as far as I can judge from my runs.

Interestingly, there is no "too many open files" error thrown. It seems to get hidden somewhere in the internals of Mac OS X JVM. Suddenly the multicast socket is marked as closed, and it seems this new close state is immediately exposed to the Java application as a SocketException.

I think I have to dig deeper to verify this (I'm more a Linux kernel guy). If my assumption holds I will try to create a patch for the JVM folks because it looks like a Mac OS X JVM bug. A "too many open files" error should be exposed if this is the underlying reason.

On the other hand, the ES multicast socket receiver thread should get protected against such weird exceptions, and close down more gracefully instead of entering an infinite loop.
</comment><comment author="kimchy" created="2013-03-19T10:21:58Z" id="15106712">It is strange.., on my mac, I don't run out of file descriptors when running the tests...

I have pushed two changes, made multicastSocket volatile (it should have been), if the socket is closed, try and restart the multicast discovery, and also throttle on unknown socket failures so it won't spin....
</comment><comment author="jprante" created="2013-03-19T21:15:19Z" id="15143623">Test is passing now, thanks! Output is at https://gist.github.com/jprante/5200157

I think we can close the issue.

Just for info, it's not "too many open files", my machine tells me there are a lot of open files available for the process (in fact the test passes 1024 but UnixOperatingSystemMXBean tell me there are 10240 maximum). In the JVM code for multicast socket receive I find code that throws a SocketException with message "Socket closed" if a file descriptor is NULL or EBADF. This looks very much like a low-level issue in the OS and I'm still investigating. I have updated to Mac OS X 10.8.3 but problem persists.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove alternative keys in suggest request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2782</link><project id="" key="" /><description>similar to #2781 we have alternative names in the suggest requests where we support `shard_size` and `shardSize`. This API is pretty new and I don't think we should support any alternative names here. This was introduced in 0.90Beta1 so I think we should be free to remove those.
</description><key id="12062718">2782</key><summary>Remove alternative keys in suggest request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>non-issue</label></labels><created>2013-03-15T13:21:37Z</created><updated>2013-03-15T14:06:22Z</updated><resolved>2013-03-15T14:06:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-15T14:06:22Z" id="14962278">ok forget about this.... this me being totally ignorant. I didn't know we support camel case and I thought it's a bug in the code but wait, no its a feature.... closing and fixing the missing places....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove unnecessary sort_order, sort_mode keys from sort object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2781</link><project id="" key="" /><description>this came up due to #2767 since we had documentation issues that used `sort_mode` instead of `mode` in the sort object. I tried to make this consistent and added `sort_order` next to `order` since I though this was introduced before 0.90Beta1 but it wasn't so I'd rather drop those entirely. Any objections?
</description><key id="12061861">2781</key><summary>Remove unnecessary sort_order, sort_mode keys from sort object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>breaking</label><label>v0.90.0.Beta1</label></labels><created>2013-03-15T12:55:20Z</created><updated>2013-03-15T13:13:15Z</updated><resolved>2013-03-15T13:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-03-15T12:58:56Z" id="14958998">Agreed - should be `order` and `mode` only. 
</comment><comment author="kimchy" created="2013-03-15T12:59:08Z" id="14959010">+1
</comment><comment author="s1monw" created="2013-03-15T13:13:15Z" id="14959615">pushed, thanks guys!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make StupidBackoff the default model for phrase suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2780</link><project id="" key="" /><description>given the feedback on #2709 I think using stupid backoff by default make most sense.
</description><key id="12041146">2780</key><summary>Make StupidBackoff the default model for phrase suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>v0.90.0.RC1</label></labels><created>2013-03-14T21:59:58Z</created><updated>2013-03-14T22:32:54Z</updated><resolved>2013-03-14T22:32:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rename artifact when building with CI</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2779</link><project id="" key="" /><description>When building with CI tool, we want to define a specific name for our atifacts.

``` sh
$ mvn install -PCI
```

Will build `elasticsearch-0.90.0.Beta2-SNAPSHOT-20130314-1021.jar` artifact.

When using default build, artifact naming is preserved:

``` sh
$ mvn install
```

Will build `elasticsearch-0.90.0.Beta2-SNAPSHOT.jar` artifact.
</description><key id="12010789">2779</key><summary>Rename artifact when building with CI</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>non-issue</label></labels><created>2013-03-14T09:26:33Z</created><updated>2014-02-03T07:44:09Z</updated><resolved>2014-02-03T07:44:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sreeshas" created="2013-09-19T04:17:56Z" id="24716196">I don't think this issue is relevant anymore. 
'grep -R "profile" . ' returned 0 results.  Maven profiles are not being used anywhere. This issue is not valid anymore.
</comment><comment author="dadoonet" created="2013-09-19T04:30:56Z" id="24716498">Of course, you can't see it. The PR is in my private repo! :-)

That said, you could be right. I can probably close it. Need to check with the team though.
</comment><comment author="kimchy" created="2013-09-19T12:44:10Z" id="24735757">Yea, I think we can. We now have build information that is included in the distro, so when you run ES, even when building yourself, you will know which build hash it was built from.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Doing Geo Bounding Box filtering using a single geohash value</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2778</link><project id="" key="" /><description>I'm writing an application which handles geohashes explicitly, as a way to define a grid in the space (a cell in the grid is defined by a geohash).

In this use case I would find handy to be able to do a geo bounding box filter using a single geohash. The bounding box to filter upon would be the geohash itself. My idea for the syntax is something like below:

``` json
{
    "filtered" : {
        "query" : {
            "match_all" : {}
        },
        "filter" : {
            "geo_bounding_box" : {
                "pin.location" : {
                    "cell" : "drm3btev3"
                }
            }
        }
    }
}
```

Could this use case be of common interest?
</description><key id="12008971">2778</key><summary>Doing Geo Bounding Box filtering using a single geohash value</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ginaluca</reporter><labels /><created>2013-03-14T08:08:46Z</created><updated>2015-03-10T18:37:10Z</updated><resolved>2013-06-19T12:37:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="chilling" created="2013-06-12T10:56:45Z" id="19318564">Hi @ginaluca, I think this is a very useful and interesting idea. IMO this should be also implemented as query.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug. Installing plugins with -url local file does not work.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2777</link><project id="" key="" /><description>When I try to install any plugin previously downloaded as zip file, all looks ok:

```
 ./plugin -url file:/home/czy/elasticsearch-head-master.zip -install head

 -&gt; Installing head...
 Trying file:/home/czy/elasticsearch-head-master.zip...
 Downloading ..DONE
 Identified as a _site plugin, moving to _site structure ...
 Installed head
```

but if I try to use it (in the navigator, with URL : http://localhost:9200/_plugin/head/), nothing happen. 

This is because when the script plugin unzip the plugin, create a directory excess wich Elastic Search does not recognize.

```
  &#9472;&#9472; head
      &#9492;&#9472;&#9472; _site
             &#9492;&#9472;&#9472; elasticsearch-head-master
                   &#9500;&#9472;&#9472; elasticsearch-head.sublime-project
                   &#9500;&#9472;&#9472; index.html
                   &#9500;&#9472;&#9472; lib
                   &#9500;&#9472;&#9472; LICENCE
                   &#9492;&#9472;&#9472; README.textile
```

This is easily solved, simply removing that directory, and its contents up one level.

```
    &#9472;&#9472; head
          &#9492;&#9472;&#9472; _site
                &#9500;&#9472;&#9472; elasticsearch-head.sublime-project
                &#9500;&#9472;&#9472; index.html
                &#9500;&#9472;&#9472; lib
                &#9500;&#9472;&#9472; LICENCE
                &#9492;&#9472;&#9472; README.textile
```
</description><key id="11983242">2777</key><summary>Bug. Installing plugins with -url local file does not work.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CecilioCanoCalonge</reporter><labels /><created>2013-03-13T17:05:40Z</created><updated>2013-03-13T20:24:21Z</updated><resolved>2013-03-13T20:24:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-03-13T18:37:43Z" id="14860390">The zip file you downloaded is not an ElasticSearch plugin zip file, but a clone of the git repository in a zip file. 

Github no longer allows uploads. You must clone/download the repo and build the plugin zip file yourself. However, head is a site plugin, so you can install it directly from Github.
</comment><comment author="CecilioCanoCalonge" created="2013-03-13T20:24:21Z" id="14866240">Thanks for your note. I thought downloading the plugin from Github as zip file, was the right way to get a Elastic Search plugin. And yes, I know I can install directly from Github, but unfortunately my Elastic Search cluster must not have internet connection.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>avoiding NPE in SigarFsProbe</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2776</link><project id="" key="" /><description>Some users report NPEs in 0.20.5 SigarFsProbe when starting ES on OpenVZ or USB drives. Checking sigar return objects for null might prevent this.
</description><key id="11983183">2776</key><summary>avoiding NPE in SigarFsProbe</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-03-13T17:04:19Z</created><updated>2014-06-17T13:00:53Z</updated><resolved>2013-03-13T17:07:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-13T17:07:26Z" id="14854631">pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title># REST Suggester API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2775</link><project id="" key="" /><description>The REST Suggester API binds the 'Suggest API' to the REST Layer directly. Hence there is no need to touch the query layer for requesting suggestions.
This API extracts the Phrase Suggester API and makes 'suggestion request' top-level objects in suggestion requests. The complete API can be found in the
underlying ["Suggest Feature API"](http://www.elasticsearch.org/guide/reference/api/search/suggest.html).
# API Example

The following examples show how Suggest Actions work on the REST layer. According to this a simple request and its response will be shown.
## Suggestion Request

``` json
curl -s -XPOST 'localhost:9200/_suggest -d {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
        "phrase" : {
            "analyzer" : "bigram",
            "field" : "bigram",
            "size" : 1,
            "real_word_error_likelihood" : 0.95,
            "max_errors" : 0.5,
            "gram_size" : 2
        }
    }
}
```

This example shows how to query a suggestion for the global text 'Xor the Got-Jewel'. A 'simple phrase' suggestion is requested and
a 'direct generator' is configured to generate the candidates.
## Suggestion Response

On success the request above will reply with a response like the following:

``` json
{
    "simple_phrase" : [ {
        "text" : "Xor the Got-Jewel",
        "offset" : 0,
        "length" : 17,
        "options" : [ {
            "text" : "xorr the the got got jewel",
            "score" : 3.5283546E-4
        } ]
    } ]
}
```

The 'suggest'-response contains a single 'simple phrase' which contains an 'option' in turn. This option represents a suggestion of the
queried text. It contains the corrected text and a score indicating the probability of this option to be meant.

Closes #2774
</description><key id="11973982">2775</key><summary># REST Suggester API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-03-13T13:55:21Z</created><updated>2014-06-26T22:23:34Z</updated><resolved>2013-03-13T18:37:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-13T18:37:58Z" id="14860403">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Suggest Feature should have a dedicated REST endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2774</link><project id="" key="" /><description># REST Suggester API

The REST Suggester API binds the 'Suggest API' to the REST Layer directly. Hence there is no need to touch the query layer for requesting suggestions.
This API extracts the Phrase Suggester API and makes 'suggestion request' top-level objects in suggestion requests. The complete API can be found in the
underlying ["Suggest Feature API"](http://www.elasticsearch.org/guide/reference/api/search/suggest.html).
# API Example

The following examples show how Suggest Actions work on the REST layer. According to this a simple request and its response will be shown.
## Suggestion Request

``` json
curl -s -XPOST 'localhost:9200/_suggest -d {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
        "phrase" : {
            "analyzer" : "bigram",
            "field" : "bigram",
            "size" : 1,
            "real_word_error_likelihood" : 0.95,
            "max_errors" : 0.5,
            "gram_size" : 2
        }
    }
}
```

This example shows how to query a suggestion for the global text 'Xor the Got-Jewel'. A 'simple phrase' suggestion is requested and
a 'direct generator' is configured to generate the candidates.
## Suggestion Response

On success the request above will reply with a response like the following:

``` json
{
    "simple_phrase" : [ {
        "text" : "Xor the Got-Jewel",
        "offset" : 0,
        "length" : 17,
        "options" : [ {
            "text" : "xorr the the got got jewel",
            "score" : 3.5283546E-4
        } ]
    } ]
}
```

The 'suggest'-response contains a single 'simple phrase' which contains an 'option' in turn. This option represents a suggestion of the
queried text. It contains the corrected text and a score indicating the probability of this option to be meant.
</description><key id="11973766">2774</key><summary>Suggest Feature should have a dedicated REST endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">chilling</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0.RC1</label></labels><created>2013-03-13T13:49:52Z</created><updated>2013-03-13T18:37:14Z</updated><resolved>2013-03-13T18:37:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-13T14:08:52Z" id="14842976">cool I will look into this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort Fails with AIOOB exception if field has rarely a value.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2773</link><project id="" key="" /><description>I ran into this playing with kibana-dashboard this morning and I am very suprised that our tests don't catch that. It seems we using numDocs as an upperBound rather than numOrds in the sorting code....

```
org.elasticsearch.search.query.QueryPhaseExecutionException: [twitter][1]: query[filtered(place.country:united _all:states)-&gt;cache(created-at:[1363171743730 TO 1363175343730])],from[0],size[100],sort[&lt;custom:"place.country": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@38aa2a95&gt;!]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:139)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:242)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2538
    at org.apache.lucene.util.packed.Direct8.get(Direct8.java:52)
    at org.elasticsearch.index.fielddata.plain.PagedBytesAtomicFieldData$BytesValues.getValueByOrd(PagedBytesAtomicFieldData.java:143)
    at org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.binarySearch(BytesRefOrdValComparator.java:459)
    at org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.binarySearch(BytesRefOrdValComparator.java:452)
    at org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.setBottom(BytesRefOrdValComparator.java:431)
    at org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator$PerSegmentComparator.setBottom(BytesRefOrdValComparator.java:165)
    at org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.setNextReader(BytesRefOrdValComparator.java:410)
    at org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator$PerSegmentComparator.setNextReader(BytesRefOrdValComparator.java:155)
    at org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:602)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:192)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:572)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:501)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:128)
    ... 9 more
```
</description><key id="11973301">2773</key><summary>Sort Fails with AIOOB exception if field has rarely a value.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-03-13T13:37:21Z</created><updated>2013-03-13T14:15:05Z</updated><resolved>2013-03-13T14:15:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>tieBreaker in multiMatch should be a float, not an integer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2772</link><project id="" key="" /><description>In multiMatchQueryBuilder, tieBreaker is an integer, but it should be a float
</description><key id="11971543">2772</key><summary>tieBreaker in multiMatch should be a float, not an integer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-03-13T12:44:32Z</created><updated>2013-03-13T12:45:11Z</updated><resolved>2013-03-13T12:45:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Documentation Error: Geo Distance Filter does not have distance_unit, use "unit" instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2771</link><project id="" key="" /><description>Tested on 0.20.2 and 0.20.5 

The documentation on  http://www.elasticsearch.org/guide/reference/query-dsl/geo-distance-filter.html
states that one can use a "distance_unit" parameter to set the distance unit when the distance parameter is a numeric value. but the correct name of the parameter is "unit".
example:

 "filter": {
            "geo_distance": {
              "distance": "2",
              "unit": "km",
              "distance_type": "plane",
              "optimize_bbox": "indexed",
              "related.location": {
                "lat": 51,
                "lon": 9
              }
            }
          }
</description><key id="11965484">2771</key><summary>Documentation Error: Geo Distance Filter does not have distance_unit, use "unit" instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">nahap</reporter><labels><label>docs</label></labels><created>2013-03-13T09:31:09Z</created><updated>2013-10-28T18:20:25Z</updated><resolved>2013-04-16T08:27:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="amnesia7" created="2013-04-15T18:38:31Z" id="16403380">I agree. I've just found the same thing out using 0.20.6
</comment><comment author="s1monw" created="2013-04-16T08:27:06Z" id="16432332">thanks for raising this. I fixed this in the documentation repo https://github.com/elasticsearch/elasticsearch.github.com/commit/2764dc4f460262c9e2d2fce6516f988817dbf122
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoJSONShapeParser parses JSON correctly if "crs" field is included</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2770</link><project id="" key="" /><description>Fixes #2763

The [http://www.geojson.org/geojson-spec.html](GeoJSON spec) defines additional fields like `bbox` and `crs`. The latter one can include its own `type` field, which might overwrite the shape type in the current parser implementation.

This fix simply skips the `bbox` and `crs` fields during parsing if found, so that no overwriting happens. In addition the skipping prevents the parser from exiting too early, because of finding the end of an object.
</description><key id="11964078">2770</key><summary>GeoJSONShapeParser parses JSON correctly if "crs" field is included</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-03-13T08:40:49Z</created><updated>2014-07-16T21:53:49Z</updated><resolved>2013-03-13T14:21:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-13T14:21:17Z" id="14843663">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index Mapping with Specific Field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2769</link><project id="" key="" /><description>Hello All,

I have created template for mapping index. I want to map specific filed to index. I dont want all filed. Please find below command.

curl -XPUT http://localhost:9200/fact/Activity/_mapping -d " {\"Activity\" : { 

\"_all\" : { \"enabled\" : false}, \"_source\": { \"compress\": false }, 

\"properties\" : {\"ActiveUser\" : {\"type\" : \"string\", \"store\" : \"yes\"},

\"ActiveOrganization\" : {\"type\" : \"string\", \"store\" : \"yes\"}}}}

In this i have added two field like ActiveUser and Activeorganization into fact Index. But when I push data form coucbase then it mapped all fileds with same index.I have set _all filed to false but its not working. I have also attched image please check it.

Before push data from coucbase it looks as below. I want only two fields like as below

![ElasticSearch-part1](https://f.cloud.github.com/assets/3797406/252534/9e28df90-8ba4-11e2-92bb-ced7b2dfe74e.png)

JSON format for mapping which is as below.

![ElasticSearch-part2](https://f.cloud.github.com/assets/3797406/252536/d974a2a0-8ba4-11e2-80cb-fd47d3aa3b02.png)

After push data from couchbase it mapped all fields with index which is as below. I dont want this? How to disable all fileds? 

![ElasticSearch-part3](https://f.cloud.github.com/assets/3797406/252538/090f0e92-8ba5-11e2-83d3-8fd3de704afb.png)

Please help me for same.
Quick response will be appreciable

Thanks 
Suraj Bhansali
</description><key id="11961332">2769</key><summary>Index Mapping with Specific Field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">bhansalisuraj</reporter><labels><label>non-issue</label></labels><created>2013-03-13T06:16:40Z</created><updated>2013-03-13T07:38:55Z</updated><resolved>2013-03-13T07:00:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-03-13T07:00:28Z" id="14827357">Please use the mailing list.
</comment><comment author="bhansalisuraj" created="2013-03-13T07:19:28Z" id="14827762">hello Dadoonet

Can you please let me know process for mailing list. 
</comment><comment author="dadoonet" created="2013-03-13T07:38:55Z" id="14828206">Instructions here http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES doesn't start up on OpenVZ</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2768</link><project id="" key="" /><description>After much investigation, I think this may be a bug. I'll let you guys be the judge.

[2013-03-12 19:56:02,696][INFO ][node                     ] [Holocaust] {0.20.5}[63428]: initializing ...
[2013-03-12 19:56:02,696][DEBUG][node                     ] [Holocaust] using home [/opt/logging/elasticsearch-0.20.5], config [/opt/logging/elasticsearch-0.20.5/config], data [[/opt/logging/elasticsearch-0.20.5/data]], logs [/opt/logging/elasticsearch-0.20.5/logs], work [/opt/logging/elasticsearch-0.20.5/work], plugins [/opt/logging/elasticsearch-0.20.5/plugins]
[2013-03-12 19:56:02,700][INFO ][plugins                  ] [Holocaust] loaded [], sites []
[2013-03-12 19:56:02,707][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder
[2013-03-12 19:56:02,857][DEBUG][env                      ] [Holocaust] using node location [[/opt/logging/elasticsearch-0.20.5/data/elasticsearch/nodes/0]], local_node_id [0]
[2013-03-12 19:56:03,464][DEBUG][threadpool               ] [Holocaust] creating thread_pool [generic], type [cached], keep_alive [30s]
[2013-03-12 19:56:03,468][DEBUG][threadpool               ] [Holocaust] creating thread_pool [index], type [cached], keep_alive [5m]
[2013-03-12 19:56:03,468][DEBUG][threadpool               ] [Holocaust] creating thread_pool [bulk], type [cached], keep_alive [5m]
[2013-03-12 19:56:03,468][DEBUG][threadpool               ] [Holocaust] creating thread_pool [get], type [cached], keep_alive [5m]
[2013-03-12 19:56:03,469][DEBUG][threadpool               ] [Holocaust] creating thread_pool [search], type [cached], keep_alive [5m]
[2013-03-12 19:56:03,469][DEBUG][threadpool               ] [Holocaust] creating thread_pool [percolate], type [cached], keep_alive [5m]
[2013-03-12 19:56:03,469][DEBUG][threadpool               ] [Holocaust] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]
[2013-03-12 19:56:03,471][DEBUG][threadpool               ] [Holocaust] creating thread_pool [flush], type [scaling], min [1], size [10], keep_alive [5m]
[2013-03-12 19:56:03,471][DEBUG][threadpool               ] [Holocaust] creating thread_pool [merge], type [scaling], min [1], size [20], keep_alive [5m]
[2013-03-12 19:56:03,471][DEBUG][threadpool               ] [Holocaust] creating thread_pool [refresh], type [scaling], min [1], size [10], keep_alive [5m]
[2013-03-12 19:56:03,471][DEBUG][threadpool               ] [Holocaust] creating thread_pool [cache], type [scaling], min [1], size [4], keep_alive [5m]
[2013-03-12 19:56:03,471][DEBUG][threadpool               ] [Holocaust] creating thread_pool [snapshot], type [scaling], min [1], size [5], keep_alive [5m]
[2013-03-12 19:56:03,483][DEBUG][transport.netty          ] [Holocaust] using worker_count[48], port[9300], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1], receive_predictor[512kb-&gt;512kb]
[2013-03-12 19:56:03,489][DEBUG][discovery.zen.ping.unicast] [Holocaust] using initial hosts [], with concurrent_connects [10]
[2013-03-12 19:56:03,490][DEBUG][discovery.zen            ] [Holocaust] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]
[2013-03-12 19:56:03,490][DEBUG][discovery.zen.elect      ] [Holocaust] using minimum_master_nodes [-1]
[2013-03-12 19:56:03,491][DEBUG][discovery.zen.fd         ] [Holocaust] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-03-12 19:56:03,493][DEBUG][discovery.zen.fd         ] [Holocaust] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]
[2013-03-12 19:56:03,508][DEBUG][monitor.jvm              ] [Holocaust] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name='ParNew', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name='ConcurrentMarkSweep', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]
[2013-03-12 19:56:04,028][DEBUG][monitor.os               ] [Holocaust] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@549ad840] with refresh_interval [1s]
[2013-03-12 19:56:04,030][DEBUG][monitor.process          ] [Holocaust] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@dec3c6d] with refresh_interval [1s]
[2013-03-12 19:56:04,031][DEBUG][monitor.jvm              ] [Holocaust] Using refresh_interval [1s]
[2013-03-12 19:56:04,032][DEBUG][monitor.network          ] [Holocaust] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@6d7ffbf] with refresh_interval [5s]
[2013-03-12 19:56:04,035][DEBUG][monitor.network          ] [Holocaust] net_info
host 
venet0  display_name [venet0]
        address [/10.137.22.179] [/127.0.0.1]
        mtu [1500] multicast [false] ptp [true] loopback [false] up [true] virtual [false]
            sub interfaces:
            venet0:0    display_name [venet0:0]
                    address [/10.137.22.179]
                    mtu [1500] multicast [false] ptp [true] loopback [false] up [true] virtual [true]
lo  display_name [lo]
        address [/0:0:0:0:0:0:0:1%1] [/127.0.0.1]
        mtu [16436] multicast [false] ptp [false] loopback [true] up [true] virtual [false]

[2013-03-12 19:56:04,159][DEBUG][indices.store            ] [Holocaust] using indices.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]
[2013-03-12 19:56:04,163][DEBUG][cache.memory             ] [Holocaust] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]
[2013-03-12 19:56:04,168][DEBUG][script                   ] [Holocaust] using script cache with max_size [500], expire [null]
[2013-03-12 19:56:04,185][DEBUG][cluster.routing.allocation.decider] [Holocaust] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]
[2013-03-12 19:56:04,186][DEBUG][cluster.routing.allocation.decider] [Holocaust] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]
[2013-03-12 19:56:04,186][DEBUG][cluster.routing.allocation.decider] [Holocaust] using [cluster_concurrent_rebalance] with [2]
[2013-03-12 19:56:04,188][DEBUG][gateway.local            ] [Holocaust] using initial_shards [quorum], list_timeout [30s]
[2013-03-12 19:56:04,304][DEBUG][http.netty               ] [Holocaust] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb-&gt;512kb]
[2013-03-12 19:56:04,313][DEBUG][indices.recovery         ] [Holocaust] using max_size_per_sec[0b], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]
[2013-03-12 19:56:04,318][DEBUG][indices.memory           ] [Holocaust] using index_buffer_size [101.1mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]
[2013-03-12 19:56:04,319][DEBUG][indices.cache.filter     ] [Holocaust] using [node] weighted filter cache with size [20%], actual_size [202.2mb], expire [null], clean_interval [1m]
[2013-03-12 19:56:04,342][DEBUG][gateway.local.state.meta ] [Holocaust] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]
[2013-03-12 19:56:04,342][DEBUG][gateway.local.state.meta ] [Holocaust] took 0s to load state
[2013-03-12 19:56:04,342][DEBUG][gateway.local.state.shards] [Holocaust] took 0s to load started shards state
[2013-03-12 19:56:04,354][ERROR][bootstrap                ] {0.20.5}: Initialization Failed ...
1) NullPointerException[null]

This happens on multiple OpenVZ nodes with interface venet0:0

I have tried setting the host.network IP with no luck.
</description><key id="11957426">2768</key><summary>ES doesn't start up on OpenVZ</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jaxxstorm</reporter><labels /><created>2013-03-13T02:58:53Z</created><updated>2013-05-16T13:45:57Z</updated><resolved>2013-03-25T07:22:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-13T14:17:59Z" id="14843481">Can you set loglevel in logging.yml to DEBUG and check the logfile (not the console output) for a stack trace in order to get more information?
</comment><comment author="jaxxstorm" created="2013-03-13T14:23:04Z" id="14843766">That's what is attached above?

Here it is in Gist format:

https://gist.github.com/jaxxstorm/c97e0e80f0b3d77bff4a
</comment><comment author="spinscale" created="2013-03-13T14:41:36Z" id="14844942">The above is missing the important piece (something has to be written out after the last line), which would help us debugging your problem. Your loglevel is already set correctly, but it seems you pasted this from your console and not from the elasticsearch log file - which is by default in logs/ directory of your elasticsearch installation.
</comment><comment author="jaxxstorm" created="2013-03-13T15:05:50Z" id="14846422">Ah, apologies. Here you go:

https://gist.github.com/jaxxstorm/e19be9dcc222e7c1cf6e
</comment><comment author="spinscale" created="2013-03-13T15:38:08Z" id="14848574">What operating system is this running on? There seem to be problems with the (operating system specific) sigar library. There was a similar problem on the mailinglist today, which was solved by removing sigar (however this is only a workaround, as you are losing many of the monitoring capabilities of elasticsearch).

See https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/8-qlr-7esas 
</comment><comment author="jaxxstorm" created="2013-03-13T15:44:22Z" id="14848996">This is running on OpenVZ on CentOS 6.3. What monitoring capabilities will I lose?
</comment><comment author="spinscale" created="2013-03-13T15:51:28Z" id="14849509">Mainly these http://www.elasticsearch.org/guide/reference/api/admin-cluster-nodes-stats.html

Can you read #1975 and https://github.com/hyperic/sigar/issues/15 and check if the same applies to you as well?

There seem to be general problems/needed workarounds with OpenVZ - I am tempted to close this bug :)
</comment><comment author="jaxxstorm" created="2013-03-13T18:11:46Z" id="14858845">#1975 looks similar, however I can't see where to specify the user ES runs as.
</comment><comment author="jaxxstorm" created="2013-03-13T18:13:44Z" id="14858958">Also, confirmed that removing the sigar libs gets it running, although is possible I'd like to keep them. What perms issues can I investigate?
</comment><comment author="spinscale" created="2013-03-14T08:23:01Z" id="14891125">If you are not using the debian init script, just start as root with `bin/elasticsearch -f` and see it if works.
In case you are doing this already I can't help you any further, as I don't have any experience with OpenVZ.

If it works as root, please close this issue - I think it is rather some complex sigar/openvz issue than an elasticsearch problem.
</comment><comment author="kimchy" created="2013-03-17T22:40:50Z" id="15032698">Btw, we did push this: https://github.com/elasticsearch/elasticsearch/pull/2776, maybe this will help to still run it with sigar?
</comment><comment author="jaxxstorm" created="2013-03-18T08:19:59Z" id="15043310">I shall give it a try, thankyou.

On Sun, Mar 17, 2013 at 10:41 PM, Shay Banon notifications@github.comwrote:

&gt; Btw, we did push this: #2776https://github.com/elasticsearch/elasticsearch/issues/2776,
&gt; maybe this will help to still run it with sigar?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2768#issuecomment-15032698
&gt; .
</comment><comment author="jaxxstorm" created="2013-03-25T02:03:14Z" id="15374734">Just wanted to say that push #2776 fixed this completely, runs fine and also I get all my stats back! Thanks!
</comment><comment author="spinscale" created="2013-03-25T07:22:16Z" id="15380537">Not all your stats, but most of them (no filesystem information). Closing this now.
Thanks for providing all the information!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Multi-valued field sorting: "sort_order" is actually "order" ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2767</link><project id="" key="" /><description>The syntax used in the new [multi-valued fields](https://github.com/elasticsearch/elasticsearch/issues/2634) (and on the [Sort documentation](http://www.elasticsearch.org/guide/reference/api/search/sort.html)) specifies "sort_order", but it appears that the property is actually "order".

Just looking for a clarification if "order" or "sort_order" is the intended parameter name. 

Recreation, on 0.90.0.Beta1:

``` bash
#add some test data
$ curl -X POST "http://localhost:9200/index/document/2" -d '{"id":2,"sortingfield":["g","h","i"]}'
$ curl -X POST "http://localhost:9200/index/document/3" -d '{"id":3,"sortingfield":["e","f"]}'
$ curl -X POST "http://localhost:9200/index/document/1" -d '{"id":1,"sortingfield":["a", "b", "c"]}'
```

``` bash
# "sort_order" examples
$ curl -XGET localhost:9200/index/document/_search?pretty -d '{
   "query": {
     "match_all": {}
   },
   "sort": [
     {"sortingfield" : {"sort_order" : "asc", "sort_mode" : "avg"}}
   ]
 }' | grep "_source"

      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},
      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},
      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},


$ curl -XGET localhost:9200/index/document/_search?pretty -d '{
   "query": {
     "match_all": {}
   },
   "sort": [
     {"sortingfield" : {"sort_order" : "desc", "sort_mode" : "avg"}}
   ]
 }' | grep "_source"

      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},
      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},
      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},
```

And now using just "order", which shows correct sorting:

``` bash
# "order" examples
$ curl -XGET localhost:9200/index/document/_search?pretty -d '{
   "query": {
     "match_all": {}
   },
   "sort": [
     {"sortingfield" : {"order" : "asc", "sort_mode" : "avg"}}
   ]
 }' | grep "_source"

      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},
      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},
      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},

$ curl -XGET localhost:9200/index/document/_search?pretty -d '{
   "query": {
     "match_all": {}
   },
   "sort": [
     {"sortingfield" : {"order" : "desc", "sort_mode" : "avg"}}
   ]
 }' | grep "_source"

      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},
      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},
      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},
```
</description><key id="11947286">2767</key><summary>Multi-valued field sorting: "sort_order" is actually "order" ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">polyfractal</reporter><labels><label>v0.90.0.RC1</label></labels><created>2013-03-12T21:19:34Z</created><updated>2013-03-15T13:15:18Z</updated><resolved>2013-03-15T08:04:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-15T07:46:47Z" id="14947186">hey, I don't see `sort_order` used in any of the referenced documentations. Yet, I think we should support it for consistency reason same as we support `mode` instead of `sort_mode`
</comment><comment author="s1monw" created="2013-03-15T13:15:18Z" id="14959707">FYI - I removed all the `sort_` prefixed keys now, this was a 0.90Beta1 issue so we decided we should break bw compatibility here before we release this in GA mode.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Repeated ConnectExceptions in logs until node is restarted</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2766</link><project id="" key="" /><description>We were reviewing our logs today and found this particular log message repeated for the same channel for multiple days:

```
[2013-03-08 17:56:32,922][TRACE][transport.netty          ] [xxx] (ignoring) exception caught on transport layer [[id: 0xe611b9cc]]
java.net.ConnectException: connection timed out
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:136)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:82)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2013-03-08 17:56:32,923][TRACE][transport.netty          ] [xxx] (ignoring) exception caught on transport layer [[id: 0xe611b9cc]]
java.net.ConnectException: connection timed out
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:136)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:82)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
[2013-03-08 17:56:33,425][TRACE][transport.netty          ] [xxx] (ignoring) exception caught on transport layer [[id: 0xe611b9cc]]
java.net.ConnectException: connection timed out
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:136)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:82)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
```

The message went away when the node was restarted. I don't understand the Netty code well enough to attempt a fix on my own, but here's what I've found: given that the channel ID was constant for the 48+ hours the message was appearing, it looks like ES was continuing to use a dead channel instead of initiating a fresh connection to the remote node. I think the fix is to explicitly close the channel in all cases in NettyTransport#exceptionCaught (as the isCloseConnectionException and fall-through cases do) but given that the log message specifically says the exception is ignored, I'm not sure if this is current behavior is intentional or not.

Thanks,
Alex
</description><key id="11944654">2766</key><summary>Repeated ConnectExceptions in logs until node is restarted</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">alambert</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-03-12T20:24:20Z</created><updated>2013-03-12T21:49:13Z</updated><resolved>2013-03-12T21:49:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-12T20:27:51Z" id="14800871">The expectation is that netty would close this connection, but it doesn't see like this happens. We have improved in master nad 0.20 branch (post 0.20.5 release) the close logic to double check the close operation (not just rely on closing the channel). I will add the relevant code to actually make sure we disconnect in this case as well just to be on the safe side.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make `discovery.zen.minimum_master_nodes` a cluster-wide setting</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2765</link><project id="" key="" /><description>Shouldn't `discovery.zen.minimum_master_nodes` be a cluster-wide setting?
That way, when making a cluster grow we could easily reconfigure the threshold/quorum to prevent split brain on network split.

Does ElasticSearch function properly when some node cannot reach another one, during shard rebalancing for example?
If not, then it's a good indication that this setting should be cluster-wide.
</description><key id="11934308">2765</key><summary>Make `discovery.zen.minimum_master_nodes` a cluster-wide setting</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2013-03-12T17:07:04Z</created><updated>2013-03-13T10:20:15Z</updated><resolved>2013-03-13T10:20:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="ofavre" created="2013-03-12T17:39:20Z" id="14790630">I've read the code and `ZenDiscovery` registers a `NodeSettingsService.Listener`, which makes me think of a node-level setting.
But reading the docs, [Cluster Update Settings API](http://www.elasticsearch.org/guide/reference/api/admin-cluster-update-settings.html) says, along with an example, that it is a cluster-wide setting, either persisted or transient.
`NodeSettingsService` looks at `settings`, which is populated using persistent settings, overridden with transient settings. 

Are Node Settings a code-wise illusions and only Cluster and Index Settings do exist?

Does that mean that if one used the Cluster Update Settings API once to update some setting using the `persistent` style, the value read from the configuration file is not used at node restart, or full cluster restart?
</comment><comment author="ofavre" created="2013-03-13T10:19:56Z" id="14833699">That setting already is cluster-wide. Sorry for the noise.

But I'm now thinking of a case where you're stuck with a `discovery.zen.minimum_master_nodes` higher than your actual node count, if the cluster shrinks too rapidly (or if we didn't use `N &lt;= 2 ? 1 : floor(N/2)+1` rule and set some tighter bound).
It's easy to test by starting one node, and giving a value of 2 to the setting. The node stops working and you can't reach it through HTTP to recover easily.
The only solution I see is to start multiple master-only nodes (can be on the same machine) to satisfy `discovery.zen.minimum_master_nodes`, then use the Cluster Update Settings API to lower the setting, wait for propagation, and shoot the temporary masters.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>List of plugins on /_nodes/plugins/ end point, like #2664</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2764</link><project id="" key="" /><description>As #2664 got reverted and closed, here is a new pull request with an implementation, integrated in Nodes Info API.

Added parameter `plugins`.
Added end point `_nodes/plugins`.
Output information follows the struture below:

``` json
{
  "plugins" : {
    "mandatory": [],
    "sites": ["head"],
    "nonsites": ["foo"]
  }
}
```
</description><key id="11928984">2764</key><summary>List of plugins on /_nodes/plugins/ end point, like #2664</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2013-03-12T15:20:21Z</created><updated>2014-06-12T22:20:32Z</updated><resolved>2013-03-12T16:21:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-03-12T15:53:40Z" id="14783642">Thanks Olivier.
But we have already a fix on the way: https://github.com/dadoonet/elasticsearch/commit/7c7cce1e46d30a6b7a1ab326897e2ddd1e980232

It's related to #2668.

That said, looking at your commit, I should probably add also the list of mandatory plugins. I will speak with @kimchy about that.

I think you should close your PR.
</comment><comment author="ofavre" created="2013-03-12T16:21:54Z" id="14785548">Argh, should've updated my master more often...
Sure, I'll close it.

Anyways, good catch with the descriptions, and adding versions is indeed a very, very good idea!
</comment><comment author="dadoonet" created="2013-03-12T16:24:39Z" id="14785722">@ofavre No regret Olivier. You would not have found it in master as it's not yet merged in master! ;-)
</comment><comment author="ofavre" created="2013-03-12T16:48:04Z" id="14787202">Aha! After all it's logic, or I would have conflicted with your code.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>geojson parsing error with optional field crs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2763</link><project id="" key="" /><description>Tested with ElasticSearch 0.20.2

I try to index a document with a field of type geo_shape. The value of this field is : 

```
{
  "type": "point",
  "crs": {
    "type": "name",
    "properties": {
      "name": "EPSG:4326"
    }
  },
  "coordinates": [
    -4.107070529535928,
    47.99207200300211
  ]
}
```

This value contains the optional field `crs` _before_ the field `coordinates`. (I think it could work with reordered fields.)

The parsing fails with this error : 

```
org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [json_way]
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:320)
        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:507)
        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:449)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:486)
        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:430)
        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:318)
        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:157)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:533)
        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:431)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.ElasticSearchParseException: Coordinates not included
        at org.elasticsearch.common.geo.GeoJSONShapeParser.parse(GeoJSONShapeParser.java:98)
        at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parseCreateField(GeoShapeFieldMapper.java:133)
        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:307)
        ... 11 more
```
</description><key id="11924677">2763</key><summary>geojson parsing error with optional field crs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Volune</reporter><labels /><created>2013-03-12T13:46:37Z</created><updated>2013-03-13T14:20:21Z</updated><resolved>2013-03-13T14:20:21Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-13T07:59:32Z" id="14828740">Hey,

this is definately a bug in GeoJSONShapeParser parser (it doesn't work as well if you change the order of "type" and "crs" values, as the "crs.type" field is used for the geo_shape type then, which also fails of course).

The parser has needs to be fixed.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Alias filters cached/type inferenced incorrectly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2762</link><project id="" key="" /><description>A while ago we started playing with alias filters, but initially ran into a confusing bug.

Our environment creates our index along with its type mappings and aliases all at once, then proceeds to index documents into it.

The problem lies with using a numeric filter in an alias on a field that does not have an explicit type via its mapping. After indexing documents that match the alias' filter, no results are returned. This appears to be due to the alias being cached.

The following is a minimal reproduction of the issue:

```
curl -XPUT 'http://localhost:9200/testindex/'

echo create alias that filters by numericField:1
curl -XPOST 'http://localhost:9200/_aliases' -d '{
  "actions": [
    {
      "add": {
        "index": "testindex",
        "alias": "testalias",
        "filter": { "term": { "numericField": 1 } }
      }
    }
  ]
}'

echo index a case that should match our filter
curl -XPUT 'http://localhost:9200/testindex/testtype/1' -d '{
  "name": "bob",
  "numericField": 1,
  "message": "testing numeric alias filters"
}'

echo confirm we can find the document via term filter
curl -XPOST 'http://localhost:9200/testindex/_search' -d '{
  "filter": {
    "term": { "numericField": 1 }
  }
}'

echo document not matched when filtering via the alias
curl -XPOST 'http://localhost:9200/testalias/_search' -d '{
  "query": {
    "match_all": {}
  }
}'
```

Inspecting the alias via the _aliases endpoint doesn't reveal any information that would explain the lack of matching documents.

If you explicitly type the filtered field in the mapping before creating the alias, the alias filter works as expected.

```
curl -XPOST 'http://localhost:9200/testindex/' -d '{
  "mappings": {
    "testtype": {
      "properties": {
        "numericField": { "type": "long" }
      }
    }
  }
}'
```

Given that this was relatively tricky to figure out (nothing is mentioned about type inferencing in relation to alias filters in the docs), this struck us as being a bug in how alias filters are being stored/cached.
</description><key id="11918629">2762</key><summary>Alias filters cached/type inferenced incorrectly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dbertram</reporter><labels><label>bug</label></labels><created>2013-03-12T10:47:37Z</created><updated>2014-08-08T10:37:18Z</updated><resolved>2014-08-08T10:37:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-12T11:14:21Z" id="14769592">hey,

the reason here is not caching I am afraid. This would be simpler to fix :) The problem here is that we parse the filter from the alias and then pre-build it with the "current" knowledge about the field. Since there is no mapping we can't tell that we need to build a numeric filter which is encoded differently in the index. From the top of my head this is not easy to fix really but I will think about it. I guess the easiest fix for you are dynamic templates that add a mapping for those fields on index creation you can maybe prefix them with numierc_ or something like that?
</comment><comment author="dbertram" created="2013-03-12T11:24:05Z" id="14769943">I had a feeling it might not be as straightforward as a caching issue.

For us, this is easy to work around as the field we want to filter on is known at index creation time (user account number), so we can just specify it explicitly in our type mapping. I just wanted to report it as it was something that tripped us up and I'm sure other people might run into it.

Couple of ideas/questions (not knowing the guts of ES):
1. Why aren't the alias filters type inferenced the same way the document fields are when being indexed? The example is specifying a numeric term filter (i.e., "numericField": 1, not "numericField": "1"). Shouldn't that be sufficient for ES to know to build a numeric filter?
2. Could the type mismatch between the alias filter and the underlying field in the index be detected at search time (and repaired/reported via an error)?
3. When a document is indexed that updates the type mapping, could affected alias filters be invalidated/rebuilt as appropriate?
</comment><comment author="tmkujala" created="2014-01-29T15:27:31Z" id="33594253">I'm also seeing this issue on 0.90.3. While adding an explicit type mapping for the filtered field works, it appears that a default mapping does not. I'm leaning toward adding a specific type mapping for the field in addition to the default type mapping for dynamic types.

Here's a brief recreation:

```
# create a new dated index
curl -XPUT http://localhost:9200/test_index_20140129

# add a default mapping
curl -XPUT http://localhost:9200/test_index_20140129/_default_/_mapping -d '{
  "_default_" : {
    "properties" : {
      "@timestamp" : {
        "type" : "date",
        "format" : "dateOptionalTime"
      }
    }
  }
}'

# create an alias to the index with a time range filter
curl -XPUT http://localhost:9200/test_index_20140129/_alias/alias_test_index_20140129 -d '{
  "filter" : {
    "range" : {
      "@timestamp" : {
        "lt" : "2014-01-30T00:00:00-0500",
        "gte" : "2014-01-29T00:00:00-0500"
      }
    }
  }
}'

# index a document
curl -XPOST http://localhost:23392/alias_test_index_20140129/test -d '{
  "@timestamp" : "2014-01-29T12:00:00-0500",
  "@message" : "this is a test"
}'

# try to search for the document via the alias
curl -XGET http://localhost:9200/alias_test_index_20140129/_search?pretty
```
</comment><comment author="clintongormley" created="2014-08-08T10:37:18Z" id="51586501">Closing in favour of #6664
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 0.20.5 stuck in RED after node loss</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2761</link><project id="" key="" /><description>0.19.x series seemed to have fixed random file deletion issues for me but now 0.20.x series seems to have reverted.  

Recently upgraded from 0.19.12 to 0.20.5.  A node went down over the weekend in my 30 node cluster.  I use ES as a DB so there were constant writes while the node was down.  Restarted the node and went RED.

I get the following exception over and over.

[2013-03-10 15:28:22,209][WARN ][indices.cluster          ] [moloches-m13b] [stats][0] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [stats][0] shard allocated for local recovery (post api), should exists, but doesn't
        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)
        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

What is the correct way to recover?

I have full replication on for this index, and it is a tiny index (8 documents for total store size of 10k)

{
  "stats" : {
    "settings" : {
      "index.number_of_replicas" : "29",
      "index.auto_expand_replicas" : "0-all",
      "index.number_of_shards" : "1",
      "index.version.created" : "200599"
    }
  }
}

My desire is that ES should just take care of this, I have 29 other copies why do I need to do anything? :-)  In a dream world ES would delete the broken index and copy from somewhere else.  What am I missing?

I don't have any other logs any more, so was hoping the exception will be enough.
</description><key id="11894607">2761</key><summary>ES 0.20.5 stuck in RED after node loss</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">awick</reporter><labels /><created>2013-03-11T19:52:41Z</created><updated>2013-06-13T14:11:05Z</updated><resolved>2013-06-13T14:11:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-03-11T22:00:06Z" id="14745649">Do you remember what health request was returning when cluster was RED? With one index with 29 replicas I could understand cluster going into YELLOW, but RED doesn't make much sense. 
</comment><comment author="awick" created="2013-03-11T23:02:40Z" id="14748715">It was definitely RED, my app now shows ES health so I saw it.  localhost:9200/_status would hang.  elasticsearch head would hang, probably because status would hang :)  curl -XDELETE http://localhost:9200/stats would hang.

I did find this, which is where I hit node m01a with a _status request and I guess it failed going to the m11b node.

[2013-03-10 15:18:21,938][DEBUG][action.admin.indices.status] [moloches-m01a] [stats][0], node[06xpbgJZQ2iiVRzrocXvLg], [P], s[INITIALIZING]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@6954dec7]
org.elasticsearch.transport.RemoteTransportException: [moloches-m11b][inet[/0.0.0.0:9301]][indices/status/s]
Caused by: org.elasticsearch.index.IndexShardMissingException: [stats][0] missing
        at org.elasticsearch.index.service.InternalIndexService.shardSafe(InternalIndexService.java:179)
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:153)
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:59)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)
        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Check if hit has a parent ID in collector</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2760</link><project id="" key="" /><description>Closes #2744
</description><key id="11881593">2760</key><summary>Check if hit has a parent ID in collector</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-03-11T15:24:59Z</created><updated>2014-06-29T01:10:17Z</updated><resolved>2013-03-11T20:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-11T20:12:27Z" id="14739639">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Crash on search request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2759</link><project id="" key="" /><description>Found this issue, while testing my project for XSS vulnerabilities.

Environment:
- Ubuntu 12.04 x64
- ElasticSearch 0.20.5
- Ruby on Rails 3.2.12
- Tire 0.5.4
- PostgreSQL 9.2.2

When I search for `'';!--"&lt;XSS&gt;=&amp;{()}` - my app crashes,

Logs here: https://gist.github.com/agladkyi/5133702
</description><key id="11873661">2759</key><summary>Crash on search request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">arg</reporter><labels><label>non-issue</label></labels><created>2013-03-11T11:54:47Z</created><updated>2013-03-11T13:06:28Z</updated><resolved>2013-03-11T13:06:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-11T13:04:18Z" id="14712003">the log file is indicating that your query could not be parsed and therefore is not executed... however running your query locally I can not reproduce a crash of the elasticsearch instance or a halt of it.

What exactly do you mean by "my app crashes"?
</comment><comment author="s1monw" created="2013-03-11T13:06:28Z" id="14712107">this is an exception thrown by the query parser that your query can not be parsed. You either need to use matchQuery or you need to handle those exeptions (I recommend doing both) the query you passed in is illegal syntax for the lucene query parser....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for creating a RPM package with maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2758</link><project id="" key="" /><description>Note: This has been disabled by default and is therefore not included in a standard build. The main reason for this is, that you need to have a RPM binary and the rpm development packages installed, which is not the case on many systems (especially mac os x).

I was able to install the RPM on opensuse and fedora and centos (using systemd to start elasticsearch on opensuse and the init script on centos). I would be glad if other people could test this as well.

You can build your own RPM package simply by running `maven rpm:rpm` in the source directory.
</description><key id="11871800">2758</key><summary>Add support for creating a RPM package with maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-03-11T10:51:21Z</created><updated>2014-07-09T03:14:58Z</updated><resolved>2013-04-02T14:25:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="deverton" created="2013-03-13T22:51:08Z" id="14873742">Have you looked at this ES RPM project https://github.com/tavisto/elasticsearch-rpms/ ? We've been using that in production for a while now and the init scripts and layout are all pretty good. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Use repository and signatures for .deb packages</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2757</link><project id="" key="" /><description>Currently I&#180;m checking once in a while if there are new packages available for Debian systems, but this is cumbersome. Ideally I could add your repository and import public keys into my apt keyring and get updates whenever they're ready. It doesn't really scale on an admin level to verify all packages manually ;-)
</description><key id="11856625">2757</key><summary>Use repository and signatures for .deb packages</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">KwadroNaut</reporter><labels /><created>2013-03-10T19:45:07Z</created><updated>2014-04-27T22:21:38Z</updated><resolved>2014-04-25T19:43:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sammcj" created="2013-08-30T04:37:23Z" id="23540242">An official apt repository would be really, very nice.

At present we're hesitant to install any software that we can't automate updates through the package manager.

Currently going to the elasticsearch website and downloading the latest deb from a web page just isn't suitable.
</comment><comment author="kimchy" created="2013-09-02T15:29:31Z" id="23666469">Understood on the repo part, but why would you download it manually when you can `wget` it from our download service?
</comment><comment author="KwadroNaut" created="2013-09-03T10:00:51Z" id="23701443">wget is what I would mean with manual, besides I don't see where to find a link to the .deb besides on the website. Next to scaling, automatization, ease of use and upgrades there's also the problem that we can't verify the packages, no signing, sha1-sum only available over http.

For reference, there's also a package request at Debian: http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=660826
</comment><comment author="spinscale" created="2014-04-25T19:43:55Z" id="41432006">closing this, as official repositories are in place now, see http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html#_apt
</comment><comment author="sammcj" created="2014-04-27T22:21:38Z" id="41511319">It is indeed in place, but see #5536 for the issues with it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>change geo_shape default configuration</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2756</link><project id="" key="" /><description>Currently geo_shape defaults to the geohash implementation and a rather high tree_levels setting. I've been testing with some open street map data consisting of about 120K points, polygons, and linestrings in the Brandenburg area on Simon Willnauer's lucene 4.2 branch which is about to be merged in the next few days. The pre lucene 4.2 geo_shape seems to have severe accuracy problems currently.

When I index this data with the default settings (geohash, tree_levels=24), I end up with nearly a GB of index data (the raw data is 35MB). Indexing takes nearly ten minutes using the bulk api, batches of 500, and with six threads Reducing the tree_levels to 9 reduces the index size substantially. However with this setting, the accuracy drops significantly and such indiceses are useless for searching unless you find margins of errors measured in kilometers acceptable.

I've also tested with the quadtree implementation. The default setting for this offers very poor accuracy and returns tens of thousands of results for queries that should return only a few dozen results. However, increasing the tree_levels to 20 results in pretty good accuracy (a few tens of meters margin of error) and an index size of only 72MB, which is very reasonable and it indexes in seconds. Increasing the tree_levels to 25 increases the index size to half a GB and the accuracy improves to being comparable with geohash at its default settings.

Based on this, I would like the defaults for geo_shape to be changed as follows:
1) make quadtree the default implementation. I think that overall it offers better index size, indexing speed than geo_hash. A smaller index size also means it probably performs better on queries and uses less memory.
2) increase default tree_levels for quadtree to 20. This seems to give a reasonable tradeoff between index size, indexing speed, and accuracy. The user can increase it for better accuracy. The current default does not provide anywhere near acceptable levels of accuracy.
</description><key id="11850694">2756</key><summary>change geo_shape default configuration</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jillesvangurp</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-10T10:29:43Z</created><updated>2013-03-20T14:10:31Z</updated><resolved>2013-03-20T14:02:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-11T07:31:45Z" id="14700475">Jiles, thanks for opening this issue. This provides valuable input and I agree we should totally fix our defaults here. I wonder if we should adjust the default levels for GeoHash as well here since it seems to bloat the index dramatically we might change that too to prevent pitfalls?

I wonder if we should also allow the users to configure this via enums rather than levels where you an say, "HIGH_ACCURACY" rather than quad_tree with level 24? I guess that would mean much more to a lot of users?

Regarding Lucene 4.2 I am just waiting for the maven mirrors to pick up the release so that should be in later today. 
</comment><comment author="IamJeffG" created="2013-03-11T18:57:38Z" id="14733322">Related: Given that we require extremely fine resolution in the index (i.e. the lowest possible tree levels) to avoid exorbitant false positives, I have trouble imagining a scenario where anyone would want distance_error_pct to be &gt; 0.0.  (All distance_error_pct does is allow higher levels of the tree to be used -- resulting in still more errant results).   If we are going to be serious about improving accuracy in geo_shape queries, it worth defaulting distance_error_pct to 0.0?

In either case, defaulting tree_levels greater than about 14 will result in unacceptable latency for anybody who tries to index shapes on the order of hundreds of kilometers, or query over similar extents.  The ES mapping must generate all the tiles over your shape, which ends up being an algorithm exponential in the number of levels.  For instance, if I am indexing shapes on the scale of countries or timezones, and I set quadtree tree_levels to 20, an index operation generates hundreds of thousands of cells for the Lucene index, driving up latency to &gt; 10 seconds _per document._

To effectively use the geo_shape mapping really requires an understanding by the end-user of its implementation and trial and error around ideal tree_levels settings for his/her documents.  Better ES documentation would do wonders here.  Frankly, I feel that merely changing the defaults is a mere optimization for a presumed use case (we have no idea what shapes the typical user is indexing), and skirts two bigger problems:
1. **no** spatial index is immune from errant matches without actually verifying candidates against the query (see Issue #2361)
2. The quadtree and geohash implementations do not exploit the tree structure of the index at query time.  Rather they pick a single tree level to create a naive grid over the earth.
</comment><comment author="jillesvangurp" created="2013-03-11T21:09:40Z" id="14742874">I partially agree, though I think the majority of users is probably interested in smaller features like pois, building polygons, etc. The current defaults with quad tree are kind of useless for that. With geohash they are fine but you do end up with a very large index 

For extremely large polygons, there is indeed an issue at higher tree_levels. Maybe the behavior should vary depending on the size of the geojson feature being indexed/queried instead of being fixed for all documents. I'm not sure how feasible that is with the current implementation.

In any case, I've uploaded the osm data I have been testing with here. It's derived from open street map and I've also included a public domain file with geojson for the country borders: https://dl.dropbox.com/u/18756426/osm-geojson.zip
</comment><comment author="jillesvangurp" created="2013-03-11T21:17:55Z" id="14743320">@s1monw I think changing the parameters to be less technical would be a good idea. I think people think about accuracy in terms of meters. It should be fairly straightforward to translate that into the appropriate tree_levels for each implementation. 
</comment><comment author="s1monw" created="2013-03-20T13:56:03Z" id="15176864">@jillesvangurp can we close this with the push or #2803 
</comment><comment author="jillesvangurp" created="2013-03-20T14:02:28Z" id="15177233">Yes, just looked at the commit and new defaults seem fine. I'll close it.
</comment><comment author="s1monw" created="2013-03-20T14:10:30Z" id="15177671">thanks man!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Node stats - field data stats is mixed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2755</link><project id="" key="" /><description>0.90 Beta1 Node stats API is reporting the following in:
field_data: {
memory_size: 1880317692,
memory_size_in_bytes: "1.7gb"
},

I believe memory_size_in_bytes should report the integer value and memory_size the string.
</description><key id="11849701">2755</key><summary>Node stats - field data stats is mixed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">fasher</reporter><labels /><created>2013-03-10T08:05:26Z</created><updated>2013-03-10T08:10:26Z</updated><resolved>2013-03-10T08:10:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="fasher" created="2013-03-10T08:10:26Z" id="14678005">Sorry for the dup. Already fixed in master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>dynamic install or remove plugins without restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2754</link><project id="" key="" /><description>as the num of plugins grown up,we need to restart the cluster the install new plugins or replace new version of the plugins,it is not cool,if we can do this on the fly,that will be great.
may be we can install plugin like this:

curl  -XPOST http://localhost:9200/_plugin/install/medcl/analysis-string2int/1.0

so,after that,it is suggested that the plugin will be installed  all over the cluster-wide,no need to setup one by one,and without shard relocation.
</description><key id="11847008">2754</key><summary>dynamic install or remove plugins without restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">medcl</reporter><labels /><created>2013-03-10T01:48:42Z</created><updated>2016-07-28T16:00:53Z</updated><resolved>2014-09-05T08:22:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="zhaoming-mike" created="2013-03-11T07:53:13Z" id="14700926">great!
</comment><comment author="mishu-" created="2013-12-18T11:27:16Z" id="30834424">+1
</comment><comment author="jpountz" created="2014-09-05T08:22:51Z" id="54597532">Although we understand how this could make things easier, we are a bit concerned that it would add a lot of complexity (dynamic classloading, how to deal with nodes that are down or have different version, etc.). We would rather recommend on using orchestration tools like chef or puppet to ensure consistent installation of plugins across all nodes of the cluster.
</comment><comment author="vchekan" created="2015-11-12T23:17:38Z" id="156267965">Perhaps `plugin list` should distinguish between installed and loaded plugins and those which were not loaded yet.
</comment><comment author="AlmightyOatmeal" created="2016-02-22T14:56:46Z" id="187218236">+1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add KeywordRepeatFilter from Lucene to Beta2</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2753</link><project id="" key="" /><description>I just added a new TokenFilter in [https://issues.apache.org/jira/browse/LUCENE-4817](LUCENE-4817) that allows to index stemmed and unstemmed versions of a token into the same field. I find that pretty useful and some users on the list asked for it as well. I think we should add it to ES as a copy until Lucene 4.3 is released.
</description><key id="11844884">2753</key><summary>Add KeywordRepeatFilter from Lucene to Beta2</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-09T22:08:18Z</created><updated>2014-07-16T10:59:10Z</updated><resolved>2013-03-09T22:11:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-04-09T19:20:22Z" id="16134149">@s1monw The keyword repeat filter needs doc'ing
</comment><comment author="s1monw" created="2013-04-12T09:10:40Z" id="16283106">@clintongormley done via https://github.com/elasticsearch/elasticsearch.github.com/commit/751a69f171790c908d41581d4d04727cdf4f6e7b
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>PhraseSuggest CandidateGenerator doesn't respect `size` parameter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2752</link><project id="" key="" /><description>the size parameter on CandidateGenerator is ignored and always set to 5.
</description><key id="11837952">2752</key><summary>PhraseSuggest CandidateGenerator doesn't respect `size` parameter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-03-09T12:16:46Z</created><updated>2013-03-11T17:32:53Z</updated><resolved>2013-03-09T13:12:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jtreher" created="2013-03-11T17:32:53Z" id="14727680">:100: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>How to optimize   data structure with nested type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2751</link><project id="" key="" /><description>My ES index and type mapping is as follows:

curl -XPUT 'http://192.168.1.1:9200/linkdb/' -d '{
    "settings" : {
        "number_of_shards" :20,
        "number_of_replicas":1,
        "index.translog.flush_threshold_ops": "100000" 
     }
}'

curl -XPUT http://192.168.1.1:9200/linkdb/FriendLink/_mapping  -d '{
     "FriendLink": {
         "_source" : { "compress": "true"},
           "_all" : {"enabled" : "false"},
         "properties": {
  "_id" : {"type" : "string", "store" : "no"},
   "Tl" : {"index" : "no","type" : "string", "store" : "no"},
   "DM" : {"index" : "not_analyzed","type" : "string", "store" : "no"},
   "SF" : {"index" : "not_analyzed","type" : "string", "store" : "no"},
  "Num" : {"index" : "not_analyzed","type" : "integer", "store" : "no"},
  "RK" : {"index" : "not_analyzed","type" : "integer", "store" : "no"},
              "LK" : {
               "type" : "nested",
               "properties" : {
                  "D" : {"index" : "not_analyzed","type" : "string", "store" : "no"},
                  "H" : {  "index" : "not_analyzed","type" : "string" , "store" : "no"},
                  "P" : {  "index" : "not_analyzed","type" : "string" , "store" : "no"},
                  "T" : {
                          "type" : "multi_field",
          "fields" : {
               "T":{"type" : "string","indexAnalyzer": "standard", "searchAnalyzer": "standard", "store" : "no"},
                         "UT":{"index" : "not_analyzed", "type" :  "string", "store" : "no"}
                    }
               },
                  "I" : {    "index" : "not_analyzed", "type" : "boolean", "store" : "no" }
               }
            }
    }
}
}'

My query is as follows:

curl -XGET 'http://192.168.1.1:9200/linkdb/FriendLink/_search?pretty=true'  -d '{
from: 0, size: 10,
fields:["_id","Tl","Num","RK","LK"],
"sort":[
         { "Num" : {"order" : "desc"} }
],

```
    "filter" : {
        "nested" : {
            "path" : "LK",
            "query" : {
                "bool" : {
                    "must" : [
                       { "term" : { "LK.D" : "baidu.com" } } , 
                       { "prefix" : {  "LK.P" : "/news" } },
                       { "text" : { "LK.T" : "&#30334;&#24230;" }
                    ]
                }
            },
            "_cache" : true
        }
    }
```

}'

I use nested object type. There are about fifty million documents now,each document an average of 30 nested objects.
  It takes an average of a few hundred milliseconds to query once&#12290;
With the growth of the data, the query speed is getting slower and slower.
If the data is more than 100 million documents  , I worry about more than one second average query.
For this nested type  , I do not know how to optimize my data structure.
Can you help me? Thanks very much.
</description><key id="11833434">2751</key><summary>How to optimize   data structure with nested type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hguchen</reporter><labels /><created>2013-03-09T02:32:37Z</created><updated>2013-03-09T07:12:55Z</updated><resolved>2013-03-09T07:12:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-03-09T07:12:55Z" id="14659341">Please ask questions in the mailing list.
See http://www.elasticsearch.org/help/
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child query AVG score mode does not always work correctly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2750</link><project id="" key="" /><description /><key id="11812216">2750</key><summary>has_child query AVG score mode does not always work correctly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-03-08T16:48:14Z</created><updated>2013-03-08T16:50:18Z</updated><resolved>2013-03-08T16:50:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Invalid string for refresh_interval is successfully accepted(but internally fails), log spews heavily after restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2749</link><project id="" key="" /><description>curl -X PUT localhost:9200/mytestindex
curl -X PUT 'localhost:9200/mytestindex/_settings?pretty' -d ' { "index.refresh_interval" : "" } '
{
  "ok" : true
}

However in the logs - 
    [2013-03-08 10:13:44,021][WARN ][index.settings           ] [Lyja] [mytestindex] failed to refresh settings for [org.elasticsearch.index.shard.service.InternalIndexShard$ApplyRefreshSettings@78fc8ee1]
        org.elasticsearch.ElasticSearchParseException: Failed to parse []
            at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:253)
            at org.elasticsearch.common.settings.ImmutableSettings.getAsTime(ImmutableSettings.java:191)
            at org.elasticsearch.index.shard.service.InternalIndexShard$ApplyRefreshSettings.onRefreshSettings(InternalIndexShard.java:700)
            at org.elasticsearch.index.settings.IndexSettingsService.refreshSettings(IndexSettingsService.java:54)
            at org.elasticsearch.indices.cluster.IndicesClusterStateService.applySettings(IndicesClusterStateService.java:323)
            at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:175)
            at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:315)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:619)
    Caused by: java.lang.NumberFormatException: For input string: ""
            at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
            at java.lang.Long.parseLong(Long.java:431)
            at java.lang.Long.parseLong(Long.java:468)
            at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:249)
            ... 9 more

If the issue is missed, and ES restarted, then there is heavy spew in the logs with the above trace repeated continuously. I can pass it garbage like "asfad" and it still accepts successfuly but prints error in the log. I am using version 0.20.1.
</description><key id="11808515">2749</key><summary>Invalid string for refresh_interval is successfully accepted(but internally fails), log spews heavily after restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">ajhalani</reporter><labels /><created>2013-03-08T15:18:51Z</created><updated>2013-03-12T19:25:16Z</updated><resolved>2013-03-12T19:25:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support for creating a fedora RPM package with maven</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2748</link><project id="" key="" /><description>Note: This has been disabled by default and is therefore not included in a
standard build. The main reason for this is, that you need to have a RPM
binary and the rpm development packages installed, which is not the case
on many systems.

Also the created RPM only works for Fedora. There does not seem to be a
possibility to create RPMs for different platforms out of the box (for
example the location of the init-scripts differ with OpenSUSE and Fedora).

You can build your own RPM package simply by running 'maven rpm:rpm'
</description><key id="11803207">2748</key><summary>Support for creating a fedora RPM package with maven</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-03-08T12:57:33Z</created><updated>2014-07-16T21:53:50Z</updated><resolved>2013-03-11T07:42:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-09T16:44:58Z" id="14666136">damn. I branched away from the wrong branch for this PR... will create a new clean one
</comment><comment author="spinscale" created="2013-03-11T07:42:44Z" id="14700698">closing this pull request. Will create a clean one with additional support for fedora and opensuse.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Added advance override for AvgParentScorer.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2747</link><project id="" key="" /><description>For scoring using advance the AvgParentScorer used the advance override for ParentScorer resulting in returning the sum. This fix adds the advancce override to the AvgParentScorer calculating the avg correctly.

This fix mainly fixes issues with the 0.20.x releasse where advance is used when calculating scores for child queries returning the wrong score when using the 'avg' score_type.
</description><key id="11795804">2747</key><summary>Added advance override for AvgParentScorer.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lindstromhenrik</reporter><labels /><created>2013-03-08T08:27:29Z</created><updated>2014-07-16T21:53:51Z</updated><resolved>2013-03-08T17:05:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-08T17:05:46Z" id="14630971">Pushed, thanks!.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Term/Terms filters on numeric fields gives wrong result</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2746</link><project id="" key="" /><description>We use the wrong shift when doing numeric based exact term/terms filter. This only applies to 0.90.0.Beta1 release.
</description><key id="11793016">2746</key><summary>Term/Terms filters on numeric fields gives wrong result</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-03-08T06:11:54Z</created><updated>2013-03-08T06:12:27Z</updated><resolved>2013-03-08T06:12:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Binary Fields don't retain 'store' setting - assumes "store" : "true"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2745</link><project id="" key="" /><description>Symptom - if you have a binary field 'blob' with "store":"false", then you MUST qualify the field name as "_source:blob" in order to retrieve it.  Details below:

I figured out the bug in 0.19.4.

Basically, we have mappings like this (in part):

```
"mappings" : {
  "_default_" : {
   "properties" : {
      "blob" :      {"type" : "binary", "store" : "false", "index" : "not_analyzed", "include_in_all" : "false" }
   }
  }
}
```

When starting 'from scratch' (i.e. indexing data for the first time), internally ES has BinaryFieldMapper element for "blob", where "store" is "NO", which is correct.

However - when shutting down and re-starting ES, the BinaryFieldMapper entry that gets created has "store" set to "YES".  So, it tries in FetchPhase execute() to place the "blob" info into the fieldSelectorMapper, rather than into the extractFieldNames.  And, since there is no "blob" field other than in _source, it fails to retrieve the "blob" value.

So - the but appears to be that the "store" "false" is not being preserved properly when you just shut down ES and then start it up again.
</description><key id="11774706">2745</key><summary>Binary Fields don't retain 'store' setting - assumes "store" : "true"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bsandiford</reporter><labels><label>bug</label></labels><created>2013-03-07T19:23:19Z</created><updated>2013-03-08T01:53:05Z</updated><resolved>2013-03-08T01:53:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-07T19:24:26Z" id="14580652">Does this happens in 0.20?
</comment><comment author="bsandiford" created="2013-03-07T19:26:22Z" id="14580773">I'll see if I can set it up and test it.  Just 0.20 or also 0.90?
</comment><comment author="bsandiford" created="2013-03-07T21:28:59Z" id="14587312">Looks like it is OK in 20.5.  The handling is different there.  The BinaryFieldMapper still has "YES" for "store", but the  method loadDocument() in FetchPhase, when the binary "blob" field is in the FieldSelector, is indeed returning the binary data, which wasn't happening in 0.19.4.  So - binary field handling looks to have been changed somewhere in the versions post-0.19.4.  Thanks!
</comment><comment author="bsandiford" created="2013-03-08T01:53:05Z" id="14598507">Looks OK in 0.90.0 also.  Closing log.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>has_child returns parent and child</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2744</link><project id="" key="" /><description>When I run a `has_child` query on 0.20.5, it is returning both the child and the parent:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "_parent" : {
            "type" : "foo"
         }
      }
   }
}
'

curl -XPUT 'http://127.0.0.1:9200/test/foo/1?pretty=1'  -d '
{
   "foo" : 1
}
'

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "foo",
#    "_version" : 1
# }

curl -XPOST 'http://127.0.0.1:9200/test/test?parent=1&amp;pretty=1'  -d '
{
   "foo" : 1
}
'

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "GBEoJRcGQnCDN93_JiUAKQ",
#    "_type" : "test",
#    "_version" : 1
# }


curl -XGET 'http://127.0.0.1:9200/test/_search?pretty=1'  -d '
{
   "query" : {
      "has_child" : {
         "query" : {
            "match" : {
               "foo" : 1
            }
         },
         "type" : "test"
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : 1
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "1",
#             "_type" : "foo"
#          },
#          {
#             "_source" : {
#                "foo" : 1
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "GBEoJRcGQnCDN93_JiUAKQ",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 2
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```
</description><key id="11771922">2744</key><summary>has_child returns parent and child</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-03-07T18:12:07Z</created><updated>2013-03-11T20:11:54Z</updated><resolved>2013-03-11T20:11:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-09T13:46:13Z" id="14663212">this should only return the parent? Excuse my naive question ;) 
</comment><comment author="clintongormley" created="2013-03-11T14:01:03Z" id="14714481">Yes
</comment><comment author="s1monw" created="2013-03-11T14:53:07Z" id="14717387">ok ;) do you know if this happens on master too?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Can version numbers go out of sync across shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2743</link><project id="" key="" /><description>When indexing a document shortly after having deleted it, the version number is incremented.  

```
curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1' 

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "test",
#    "_version" : 1
# }


curl -XDELETE 'http://127.0.0.1:9200/test/test/1?pretty=1' 

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "test",
#    "found" : true,
#    "_version" : 2
# }


curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1' 

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "test",
#    "_version" : 3
# }
```

However, indexing a previously deleted document after deletes are expunged through merging, would result in a version number of 1 for the new document.

Given that merges can happen on different shards at different times, is it possible that version numbers might be out of sync across shards?
</description><key id="11762181">2743</key><summary>Can version numbers go out of sync across shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-03-07T14:47:43Z</created><updated>2014-08-08T10:34:17Z</updated><resolved>2014-08-08T10:34:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Repeated deletes increment version numbers</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2742</link><project id="" key="" /><description>When deleting an already deleted document (before deletes have been expunged), ES returns a 404 NOT FOUND code, but increments the version number.

Is this correct behaviour?

```
curl -XPUT 'http://127.0.0.1:9200/test/test/1?pretty=1' 

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "test",
#    "_version" : 1
# }



curl -XDELETE 'http://127.0.0.1:9200/test/test/1?pretty=1' 

# {
#    "ok" : true,
#    "_index" : "test",
#    "_id" : "1",
#    "_type" : "test",
#    "found" : true,
#    "_version" : 2
# }


curl -XDELETE 'http://127.0.0.1:9200/test/test/1?pretty=1'
# {
#   "ok" : true,
#   "found" : false,
#   "_index" : "test",
#   "_type" : "test",
#   "_id" : "1",
#   "_version" : 3
# }
```
</description><key id="11761936">2742</key><summary>Repeated deletes increment version numbers</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-03-07T14:42:05Z</created><updated>2013-03-08T18:42:39Z</updated><resolved>2013-03-08T18:42:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-03-08T18:42:39Z" id="14636391">The version number is also incremented when a document which didn't exist is deleted.  This behaviour makes sense because we can't be sure of the order in which requests reach the shard.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RabbitMQ River : does not reconnect after RabbitMQ restart</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2741</link><project id="" key="" /><description>When RabbitMQ restart, the river does not reconnect.
Reason is consumer.nextDelivery(bulkTimeout.millis()) throws a ShutdownSignalException.
This RuntimeException is not catched by the catch block which only catch InterruptedException.
So the thread exit silently.
</description><key id="11751141">2741</key><summary>RabbitMQ River : does not reconnect after RabbitMQ restart</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">vpernin</reporter><labels /><created>2013-03-07T09:07:07Z</created><updated>2013-03-07T09:18:46Z</updated><resolved>2013-03-07T09:18:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="tlrx" created="2013-03-07T09:13:08Z" id="14549639">You finally got it :)
</comment><comment author="dadoonet" created="2013-03-07T09:16:06Z" id="14549744">@vpernin Could you open it in https://github.com/elasticsearch/elasticsearch-river-rabbitmq repository? And close this one?
Thanks
</comment><comment author="vpernin" created="2013-03-07T09:18:45Z" id="14549837">Not enough coffee.
Done : https://github.com/elasticsearch/elasticsearch-river-rabbitmq/issues/16
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Percolating based on More Like This queries returns "incorrect" results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2740</link><project id="" key="" /><description>When a more like this query is added to the percolator for a given index the results of percolating a document through the query are inconsistent with what you would get if you re-ran the MLT query against the index. The document being percolated may match the MLT query in the percolator, but may not be contained in the results of the MLT being ran against the index.

The reason for this is that the MLT query uses the document frequency of the terms, and number of documents in the index, to weight the term frequency in the MLT like text in order to generate the score for the given term, i.e.:
      float idf = similarity.idf(docFreq, numDocs);
      float score = tf \* idf;

Since the index that is being used to generate the document frequency and number of documents contains just the singular document being percolated this causes the score to be vastly different (i.e. common terms in the index are not discounted).

Therefore when the rewrite is performed on the MoreLikeThisQuery from within the PercolatorExecutor (via search / createNormalizedWeight / rewrite(query) - in IndexSearcher) it would be more correct to use the IndexReader defined for the index associated with this percolated query to calculate the document frequency and number of documents.

Not sure if using the "correct" index to rewrite other queries would make similar improvements with other query types.

(Note: this is based on my understanding of this code which might be slightly off and is based on 0.19.10)
</description><key id="11742227">2740</key><summary>Percolating based on More Like This queries returns "incorrect" results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">brycenz</reporter><labels /><created>2013-03-07T01:32:09Z</created><updated>2014-08-08T10:33:56Z</updated><resolved>2014-08-08T10:33:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-08-27T19:42:35Z" id="23364677">That is just how it works with this design. The document frequency is always equal to 1 in the case when using the percolate api, so mlt or any that does something with df will always yield different results compared to if the search api is used.

What are you trying to achieve with using the mlt query in the percolate api? 
</comment><comment author="clintongormley" created="2014-08-08T10:33:56Z" id="51586216">Can't fix - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Index template aliases</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2739</link><project id="" key="" /><description>Added support for aliases in index templates.
</description><key id="11741102">2739</key><summary>Index template aliases</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/javanna/following{/other_user}', u'events_url': u'https://api.github.com/users/javanna/events{/privacy}', u'organizations_url': u'https://api.github.com/users/javanna/orgs', u'url': u'https://api.github.com/users/javanna', u'gists_url': u'https://api.github.com/users/javanna/gists{/gist_id}', u'html_url': u'https://github.com/javanna', u'subscriptions_url': u'https://api.github.com/users/javanna/subscriptions', u'avatar_url': u'https://avatars1.githubusercontent.com/u/832460?v=4', u'repos_url': u'https://api.github.com/users/javanna/repos', u'received_events_url': u'https://api.github.com/users/javanna/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/javanna/starred{/owner}{/repo}', u'site_admin': False, u'login': u'javanna', u'type': u'User', u'id': 832460, u'followers_url': u'https://api.github.com/users/javanna/followers'}</assignee><reporter username="">jbrook</reporter><labels /><created>2013-03-07T00:51:06Z</created><updated>2014-06-13T05:43:22Z</updated><resolved>2014-02-19T16:09:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dav3860" created="2013-04-03T20:25:15Z" id="15862055">That would be great to automatically create specific views/authorizations on index creation.
</comment><comment author="jbrook" created="2013-04-03T21:19:56Z" id="15866031">It's really handy. I hope someone will merge it one day soon.
</comment><comment author="martijnvg" created="2013-04-04T11:04:53Z" id="15891272">@jbrook Looks nice! I'll try to get it in soon!
</comment><comment author="dav3860" created="2013-04-04T11:50:02Z" id="15892873">jbrook, how do you write the template if you want to add an alias with a term filter ?
</comment><comment author="jbrook" created="2013-04-04T14:23:26Z" id="15900081">@dav3860 we use something like this:

```
"aliases": {
      "my_alias_name": {
        "filter" : { "term" : { "@myAttrName" : "someTerm" } }
      },
      "my_alias_name_2": {
        "filter" : { "term" : { "@myAttrName" : "anotherTerm" } }
      }
    }
```
</comment><comment author="dav3860" created="2013-04-04T20:31:07Z" id="15921825">@jbrook thank you. I hope it gets merged.
</comment><comment author="jbrook" created="2013-05-28T10:04:26Z" id="18541427">@martijnvg , I tested this pull request today with 'master' and '0.90' in my local environment and it still merges without conflicts and tests OK. I would love to have your feedback or see it merged. I think it is very useful functionality to have when using logstash. I am sure it may also be useful to @rashidkpc in his work on Kibana 3 - custom dashboards could be set up to work only with a specific index alias or aliases instead of always having to specify the date pattern. Anyway, enough of the hard sell - I know you are all really busy with getting to 1.0! 
</comment><comment author="dav3860" created="2013-05-28T20:02:37Z" id="18576078">@jbrook You mention Kibana 3. One of its killer features is that you can set a time stamped indice pattern. Then all the time-based queries are done sequentially to optimize the user experience.
If you automate the creation of the alias, it should create time stamped aliases too, eg : alias1-05.05.2013 -&gt; logstash-05.05.2013, so as to keep the benefit of the sequential queries. We should avoid hiding all the indices behind the same alias. Does your pull request allow to use some kind of regexp to name the alias in the template ?
</comment><comment author="jbrook" created="2013-06-04T17:33:04Z" id="18925560">@dav3860 - Is that really a killer feature? Elasticsearch handles concurrent queries very efficiently. I guess the sequential querying is an optimisation in the client so that the histogram, etc build up gradually. Couldn't the same effect be achieved with date filters?

I think it's a good feature suggestion but I would like to have the feeling that the original pull request will be pulled before doing more work on it.
</comment><comment author="kimchy" created="2013-09-04T16:34:37Z" id="23804028">I think having the actual index name be allowed to be used inside the aliases make sense, something like ${index} where it will be replaced with the actual index name.
</comment><comment author="jbrook" created="2013-09-05T17:18:14Z" id="23885273">I implemented the suggestion from @kimchy. I played with allowing the ${index} for the creation of any index but I don't think it makes sense because when using the alias APIs directly, the client doesn't need this feature and I think it makes sense to keep this functionality close to where the other index template code is. The result is that this only works for aliases created by index templates. I hope that makes sense.
</comment><comment author="kimchy" created="2013-09-05T17:31:02Z" id="23886301">looks good, can we squash the changes into a single commit? would be simpler to review it then. Quick note, I don't see the aliases being properly serialized (readFrom/writeTo in `IndexTemplateMetaData`), also just double checking its on both from and to xcontent methods.

The serialization part, it would be nice to have it versioned, so we backport it to 0.90 potentially (see `CommonStats` with how it works with completion service).
</comment><comment author="jbrook" created="2013-09-06T16:26:08Z" id="23952331">I fixed the various serialization bits, added some tests for that and squashed the commits. I haven't added the versioning conditions. Which version constant should I use with the onOrAfter check?
</comment><comment author="dav3860" created="2013-09-09T20:04:12Z" id="24110041">Cool. It will help adding authorization to ES, and Kibana for example. We could let ES generate specific views each time it creates an index.
</comment><comment author="jbrook" created="2013-09-10T09:56:51Z" id="24148442">@dav3860 - yes. I see how useful your suggestion was now. I had been looking for a way to do auth like this with a proxy we have written. Unfortunately there is an alias bug that is causing problems for me - #2762 - Alias filters cached/type inferenced incorrectly.
</comment><comment author="jbrook" created="2013-09-12T18:23:48Z" id="24343977">Issue #3677 is also problematic. Aliases have to be specifically listed as a comma separated list rather than using a wild-card if using the multi-index syntax, otherwise the filters are not applied.
</comment><comment author="kimchy" created="2013-09-16T21:10:54Z" id="24544958">@jbrook if we use version based serialization, then we can backport it to 0.90, in which case, use the latest 0.90 release available. At this time its 0.90.4, but 0.90.5 will probably come out soon due to the site plugin installation problem. Need another review cycle on my end, @martijnvg can you look at it as well?
</comment><comment author="jbrook" created="2013-09-27T12:36:06Z" id="25241880">Updated and rebased again to support randomized testing etc and bring the request up-to-date with development on master.
</comment><comment author="dav3860" created="2013-10-11T11:22:18Z" id="26130109">Do you plan to merge it into the next release ?
</comment><comment author="javanna" created="2013-10-23T09:09:25Z" id="26890262">I'm going to start reviewing it, so that it hopefully makes it to the next release. Will leave inline comments on the code if needed.
</comment><comment author="javanna" created="2013-10-23T18:11:13Z" id="26929173">Thanks a lot for your work on this @jbrook !!! My review is completed, can you make the suggested changes?
We should include this in the documentation too, which is now included in this same repository. Can you look into that?
Feel free to ping me if you have any question!
</comment><comment author="jbrook" created="2013-10-30T11:12:09Z" id="27380737">Thanks a lot for the detailed review @javanna My time is rather limited at the moment. I will do my best to look at this soon. I deliberately left the version numbers out of the serialization/de-serialization for the moment because I didn't know which version to use. Perhaps you can take care of that part once I have made the other changes?
</comment><comment author="javanna" created="2013-10-30T11:16:13Z" id="27380934">Hey @jbrook take your time and let me know if you need help! Don't worry too much about the serialization for now, we'll fix that when the rest is ready.
</comment><comment author="javanna" created="2013-11-14T11:35:15Z" id="28477221">Hi @jbrook did you have the chance to work on this? If your time is still limited I can have a look and apply some changes myself, if you don't mind.
</comment><comment author="jbrook" created="2013-11-14T12:24:26Z" id="28479785">I've done most of it. Still need to dig in to duplicate aliases, invalid aliases and tests. Hope to find time soon. Could commit what I have - new baby just joined the family :)

Sent from my iPhone

&gt; On 14 Nov 2013, at 12:35, Luca Cavanna notifications@github.com wrote:
&gt; 
&gt; Hi @jbrook did you have the chance to work on this? If your time is still limited I can have a look and apply some changes myself, if you don't mind.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub.
</comment><comment author="javanna" created="2013-11-14T21:46:24Z" id="28525976">Congrats! Thanks for your work, if you don't mind pushing what you have I can take it from there and get this merged soon.
</comment><comment author="dav3860" created="2014-01-11T16:55:35Z" id="32100864">Thank you @jbrook for the work on this feature. @javanna, do you think it can be merged soon ? We've been waiting for this feature to configure authorization profiles for Kibana 3 !
</comment><comment author="javanna" created="2014-01-28T09:38:10Z" id="33463294">We decided to split this task into two sub-tasks:
- add support for aliases in create index api
- add support for aliases in index templates, which will make use of the infrastructure added in step 1

We've been working on step 1, have a look at #4920. After that we'll look into merging this PR.
</comment><comment author="kakbit" created="2014-02-08T06:55:22Z" id="34537243">Still waiting...
</comment><comment author="javanna" created="2014-02-19T16:09:34Z" id="35515127">Thanks again @jbrook for your work, it took a while but we almost made it ;) 

The support for aliases in create index (#4920) got pushed to master and 1.x branch. 

I just created a new PR where I pulled your commit, updated it and finalized your work so that we can get this merged. Have a look at #5180 if you are interested. Closing this one as it got replaced by #5180.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add endpoint for dumping all effective settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2738</link><project id="" key="" /><description>We should provide an endpoint that dumps a map of all settings and their values.

```
    % curl localhost:9200/_config
    {
        "cluster": {
            "name": "elasticsearch"
        }, 
        "index": {
            "number_of_shards": 1
        }, 
        "node": {
            "data": true, 
            "master": true
        },
        "etc": "..."
    }
```
</description><key id="11739095">2738</key><summary>Add endpoint for dumping all effective settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels /><created>2013-03-06T23:47:19Z</created><updated>2014-07-04T12:37:47Z</updated><resolved>2014-07-04T12:37:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lmenezes" created="2014-04-03T20:36:51Z" id="39501669">is this getting any attention at all? 
if not, I will be glad to work on it :)
</comment><comment author="clintongormley" created="2014-07-04T12:37:47Z" id="48039266">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>RemoteTransportException on request to _mlt</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2737</link><project id="" key="" /><description>When routing a request to _mlt for a non existing document, like so

``` shell
$ curl http://localhost:9210/products/product/5076dad2e1d491063c34e2d/_mlt
```

I get a RemoteTransportException

``` javascript
{
  "error": "RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: EOFException; ",
  "status": 500
}
```

instead of the expected ElasticSearchException when calling the data node

``` shell
$ curl http://localhost:9200/products/product/5076dad2e1d491063c34e2d/_mlt
```

``` javascript
{
  "error": "ElasticSearchException[document missing]",
  "status": 500
}
```

In the example setup I have the master node with

``` yml
node.master: true
node.data: true 
transport.tcp.port: 9300
http.port: 9200
```

The routing node has 

``` yml
node.master: false
node.data: false 
transport.tcp.port: 9310
http.port: 9210
```
</description><key id="11739094">2737</key><summary>RemoteTransportException on request to _mlt</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nordbergm</reporter><labels /><created>2013-03-06T23:47:15Z</created><updated>2013-03-19T12:37:40Z</updated><resolved>2013-03-19T12:37:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-14T13:47:25Z" id="14902799">Hey, 

which version of elasticsearch are you using? I just tried with 0.20.5 using your configuration and got this, which looks ok.

```
curl http://localhost:9210/products/product/5076dad2e1d491063c34e2d/_mlt
{"error":"RemoteTransportException[[Bloodlust][inet[/192.168.178.28:9300]][mlt]]; nested: ElasticSearchException[document missing]; ","status":500}
```
</comment><comment author="nordbergm" created="2013-03-19T12:37:40Z" id="15111702">I'm using 0.20.5 as well, but the weird thing is that when I tried it again today it worked as expected. I can't really explain it so I suppose I'll close the ticket.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Update README.textile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2736</link><project id="" key="" /><description>Update license year
</description><key id="11730541">2736</key><summary>Update README.textile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">juanpastas</reporter><labels /><created>2013-03-06T20:14:19Z</created><updated>2014-07-16T21:53:51Z</updated><resolved>2013-03-07T10:58:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-07T10:58:14Z" id="14553758">pushed thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Move Smoothing Model into its own sub-object in the Phrase suggest request</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2735</link><project id="" key="" /><description>rather than having

``` json
curl -s -XPOST 'localhost:9200/_search' -d '{
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
      ...
        "stupid_backoff" : {
          "discount" : 0.4
        }
      }
    }
  }
}
```

 we should have: 

``` json
curl -s -XPOST 'localhost:9200/_search' -d '{
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
      ...
        "smoothing" : {
          "stupid_backoff" : {
            "discount" : 0.4
          }
        }
      }
    }
  }
}
```
</description><key id="11712561">2735</key><summary>Move Smoothing Model into its own sub-object in the Phrase suggest request</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-06T13:30:22Z</created><updated>2013-03-06T13:32:35Z</updated><resolved>2013-03-06T13:32:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add support for ignore_indices to delete by query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2734</link><project id="" key="" /><description>As the doc says all multi indices API support the ignore_indices option, and the delete by query API is a multi indices API I think it makes sense to support it.
</description><key id="11690815">2734</key><summary>Add support for ignore_indices to delete by query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-03-05T23:06:00Z</created><updated>2014-07-16T21:53:52Z</updated><resolved>2013-03-07T09:32:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Network: A closed channel might not always fire up a close event</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2733</link><project id="" key="" /><description>For some reason, it seems like netty will not always fire up a proper close event on a channel, thus causing us to propagate it up the stack to treat a node as disconnected. It does throw a failure up the networking handlers in that case, which we catch. We should try and disconnect on those events as well...
</description><key id="11683009">2733</key><summary>Network: A closed channel might not always fire up a close event</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>enhancement</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-03-05T19:48:48Z</created><updated>2013-03-05T19:49:11Z</updated><resolved>2013-03-05T19:49:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>possible code update of internal Guice code to align with JDK 8 compile, see #2721</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2732</link><project id="" key="" /><description /><key id="11661410">2732</key><summary>possible code update of internal Guice code to align with JDK 8 compile, see #2721</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-03-05T10:32:11Z</created><updated>2014-06-19T09:28:52Z</updated><resolved>2013-03-18T00:33:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jprante" created="2013-03-05T10:36:00Z" id="14433459">See also #2721
</comment><comment author="kimchy" created="2013-03-18T00:33:57Z" id="15034591">have pushed a simpler fix for this to compile with java 8, it seems to compile now
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Problems with range searches for time with lte</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2731</link><project id="" key="" /><description>Hello,

I cannot find this issue as reported, but I'm having problems with range queries over fields of date type with 'lte' enabled. Full gist reproducing the problem, as well as some analysis by clintongormley can be found here: https://gist.github.com/anonymous/4482696
For now I can live with 'lt' query which seems to work fine, but this is either an evident bug or I'm misusing the date field.

NOTE: My ElasticSearch version is 0.20.5.

Przemek
</description><key id="11658876">2731</key><summary>Problems with range searches for time with lte</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">CGenie</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-03-05T09:14:19Z</created><updated>2013-12-15T21:15:17Z</updated><resolved>2013-03-06T02:10:35Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-06T01:45:28Z" id="14477535">As clinton noted, our parsing when doing range filters can be better implemented to support "just" hours. Easily fixed...
</comment><comment author="CGenie" created="2013-03-06T07:48:48Z" id="14486771">Thanks! That was quick ;)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NoShardAvailableActionException caused by missing value in elasticsearch.yml</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2730</link><project id="" key="" /><description>We created an index and tried to "curl -PUT" a document, failed after a timeout with:

```
{
    "error": "NoShardAvailableActionException[[test][0] No shard available for [[test][test_AS24Elasticsearch][1]: routing [null]]]",
    "status": 500
}
```

The reason was this line in the elasticsearch.yml:

```
index.search.slowlog.threshold.query.warn: s
```

When reproducing this i found that the elasticsearch.yml has to be broken at startup of the elasticsearch service. 

It produced about 1'000'000 lines in the logfile.

I post this hoping for a more precise error message, an earlier parsing of the elasticsearch.yml or that people running into the same problem will find this post and identify their problem faster.

The wrong stanza in elasticsearch.yml:

```
index.search.slowlog.level: TRACE
index.search.slowlog.threshold.query.warn: s
index.search.slowlog.threshold.query.info: s
index.search.slowlog.threshold.query.debug: s
index.search.slowlog.threshold.query.trace: ms
```

(This kind of mistake is, as you will guess, the result of a beginner meddling with Puppet)

In the logfile - only the first 82 lines:

```
[2013-03-05 09:35:10,805][WARN ][indices.cluster          ] [dexxxv001] [sunytest][0] failed to create shard
org.elasticsearch.index.shard.IndexShardCreationException: [sunytest][0] failed to create shard
        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:323)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:561)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:526)
        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:171)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:315)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: org.elasticsearch.ElasticSearchParseException: Failed to parse [s]
        at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:253)
        at org.elasticsearch.common.settings.ImmutableSettings.getAsTime(ImmutableSettings.java:191)
        at org.elasticsearch.index.search.slowlog.ShardSlowLogSearchService.&lt;init&gt;(ShardSlowLogSearchService.java:132)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)
        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)
        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)
        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)
        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)
        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)
        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:56)
        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)
        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)
        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)
        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)
        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)
        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:129)
        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:66)
        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:321)
        ... 7 more
Caused by: java.lang.NumberFormatException: empty String
        at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1011)
        at java.lang.Double.parseDouble(Double.java:540)
```
</description><key id="11658370">2730</key><summary>NoShardAvailableActionException caused by missing value in elasticsearch.yml</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ynux</reporter><labels><label>:Settings</label><label>bug</label><label>stalled</label></labels><created>2013-03-05T08:56:00Z</created><updated>2015-09-19T17:36:36Z</updated><resolved>2015-09-19T17:36:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-05T19:47:06Z" id="14461030">We don't validate it since its used during actual shard creation. When will happen is that an index will be created, but when the shard is actually allocated, it will fail to be allocated (so the index is in red state).
</comment><comment author="spinscale" created="2013-11-19T15:17:23Z" id="28797821">reverted, as we do not want to create an exception to the rule for parsing time based settings. need to come up with something more consistent here
</comment><comment author="clintongormley" created="2014-11-29T14:12:15Z" id="64953037">Possibly this can be handled by the proper settings framework planned in #6732?
</comment><comment author="clintongormley" created="2015-09-19T17:36:36Z" id="141692428">Closing as a duplicate of #2997
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Phrase Suggester "size" should override "shard_size" when searching on index with 1 shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2729</link><project id="" key="" /><description>When using the phrase suggester against and index with 1 shard, setting the "size" parameter &gt; 5 will not return the correct number of suggestions unless you also set "shard_size" to the same number.   The minimum "shard_size" should be set to something like ceil(size/num_shards) to ensure the correct number of suggestions are returned.
</description><key id="11651043">2729</key><summary>Phrase Suggester "size" should override "shard_size" when searching on index with 1 shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">mattweber</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-03-05T03:19:31Z</created><updated>2013-03-05T08:23:47Z</updated><resolved>2013-03-05T08:22:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-05T08:23:47Z" id="14428475">hey @mattweber I pushed a fix for this to simply use the size as the shard size if it isn't explicitly specified. I mean there could be situations where you want this behaviour so we should support it but be smarter by default. Thanks for reporting
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove JMX connector creation flags, and JMX attributes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2728</link><project id="" key="" /><description>Currently, we allow to have a flag and auto create JMX connection, thats tricky to get right, so remove it, and users can simply provide the JMX flags if needed as part of the JVM flags when started.

Also, remove the (very minimum) data we expose through JMX, since they are first very minimal today, and not really used (as they don't expose enough data). Second, we much prefer users to use our RESTful API for it, as they are much more optimized.
</description><key id="11645881">2728</key><summary>Remove JMX connector creation flags, and JMX attributes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.90.0.RC1</label></labels><created>2013-03-04T23:57:07Z</created><updated>2013-05-24T21:41:24Z</updated><resolved>2013-03-05T00:12:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mrflip" created="2013-04-05T20:00:34Z" id="15977414">For anyone looking to preserve overall JMX, add the following flags:

```
  -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9400 -Djava.rmi.server.hostname=1.2.3.4 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false
```

Those respectively: enable JMX; set the port; have it bind to the named interface; and disable ssl and authentication.

You can keep JMX'ing all the GC stats you love to hate, there's just no elasticsearch bean to consult.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change field data stats header from `field_data` to `fielddata`.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2727</link><project id="" key="" /><description>Breaking from `0.90.0.Beta1`.
</description><key id="11643338">2727</key><summary>Change field data stats header from `field_data` to `fielddata`.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>v0.90.0.RC1</label></labels><created>2013-03-04T22:49:57Z</created><updated>2013-03-04T22:50:41Z</updated><resolved>2013-03-04T22:50:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Implement search shards API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2726</link><project id="" key="" /><description>An "admin.cluster" based API, which for a given a search request URI returns back the shards and nodes that the search is going to execute on.

The search shards API is using the same index as search API, except it uses the `_search_shards` end point instead of `_search`. For example, the following command will return a list of nodes and shards that would be used to execute a search request on index `twitter` with routing `march`:

```
$ curl -XGET 'http://localhost:9200/twitter/tweet/_search_shards?routing=march'
```

Shards that this command returns are grouped by `index` and `shard_id`. 
</description><key id="11624832">2726</key><summary>Implement search shards API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">imotov</reporter><labels><label>feature</label><label>v0.90.0.RC1</label></labels><created>2013-03-04T16:00:04Z</created><updated>2013-03-12T13:29:35Z</updated><resolved>2013-03-05T15:50:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>fixed interchanged values in field_data stats </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2725</link><project id="" key="" /><description>fixes issue #2724
</description><key id="11612882">2725</key><summary>fixed interchanged values in field_data stats </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gakhov</reporter><labels /><created>2013-03-04T10:23:13Z</created><updated>2014-07-07T09:17:24Z</updated><resolved>2013-03-04T10:42:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="gakhov" created="2013-03-04T10:29:21Z" id="14373981">ups ... it seams automatically duplicate for #2724
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Interchanged values in field_data stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2724</link><project id="" key="" /><description>In version `0.9.0.Beta1` there is an interchange between `field_data.memory_size` and `field_data.memory_size_in_bytes`.

To reproduce:
1. curl `http://localhost:9200/_nodes/_local/stats?pretty=true`

Actual result:

```
        "field_data" : {
          "memory_size" : 257199478,
          "memory_size_in_bytes" : "245.2mb"
        },
```

Expected result:

```
        "field_data" : {
          "memory_size" : "245.2mb",
          "memory_size_in_bytes" : 257199478
        },
```

Here is a pull request: https://github.com/elasticsearch/elasticsearch/pull/2725
</description><key id="11612271">2724</key><summary>Interchanged values in field_data stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">gakhov</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-03-04T10:05:23Z</created><updated>2013-03-06T01:16:33Z</updated><resolved>2013-03-04T10:42:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-04T10:24:46Z" id="14373760">makes perfect sense, thanks!
</comment><comment author="s1monw" created="2013-03-04T10:43:13Z" id="14374487">thanks @gakhov 
</comment><comment author="kimchy" created="2013-03-04T22:49:11Z" id="14411826">I am going to change the `field_data` part to `fielddata`, though less readable it matches our configuration options that match it...
</comment><comment author="gakhov" created="2013-03-05T09:11:13Z" id="14430142">good to know ... right now we use that information for monitoring our clusters ... this change will require us to rewrite our monitoring plugins (for `munin`, for instance) to support 2 versions ... not big deal, but still ...
</comment><comment author="kimchy" created="2013-03-06T01:16:33Z" id="14476645">@gakhov thats why we are still not GA with it, I think its a good change if we want to be consistent across the board with how we name references to the `fielddata` component.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Set custom headers in HTTP responses</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2723</link><project id="" key="" /><description>Hi,

this patch allows to set custom headers in HTTP responses (like setting the WWW-Authenticate header for basic auth) by adding RestRequest.addHeader() method. There are are few caveats in my opinion, which should be considered before inclusion:

Every HTTP request now instantiates a hashmap (with a predefined size of 2). I am not sure if this is a good thing, as most request (especially all the elasticsearch core ones) do not set any headers. We could alternatively use some sort of init() method and do not create the hashmap on request instantiation.

This should close #2540
</description><key id="11608473">2723</key><summary>Set custom headers in HTTP responses</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">spinscale</reporter><labels /><created>2013-03-04T08:02:26Z</created><updated>2014-07-13T05:07:42Z</updated><resolved>2013-03-11T07:44:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-11T07:44:59Z" id="14700739">I will close this pull request and create a clean one, which only features setting the HTTP headers, but ignores the Location: redirect for plugins (this is hard to get right and the other solution works for now).
</comment><comment author="hmalphettes" created="2013-04-25T16:51:06Z" id="17020177">@spinscale I made a clean pull request that focuses only on setting HTTP headers and ignores the redirection issues: https://github.com/elasticsearch/elasticsearch/pull/2936

It is based on copy pasting your code in this patch and adding an integration test.
I hope this helps.
</comment><comment author="s1monw" created="2013-05-11T19:37:06Z" id="17766101">Alex, can you port this to 0.90 too it seems it's only been pushed to master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Field Data: Add `node` level cache type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2722</link><project id="" key="" /><description>Allow to configure field data to use node level cache, which can then be bounded based on size and/or expiration.

In order to configure field data to use node level cache across all indices, simply put `index.fielddata.cache` to `node` in the configuration.

Node level configuration allows to set:
- `indices.fielddata.cache.size`: the size in bytes or percentage of heap. Defaults to `40%`.
- `indices.fielddata.cache.expire`: expiration, defaults to not set.

Note, even though size based eviction sounds attractive, the effort of loading field data is very expensive, so its better to have enough nodes in the cluster to work with the default `resident` field data cache type.

Last, the default is still `resident`.
</description><key id="11595523">2722</key><summary>Field Data: Add `node` level cache type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-03T18:54:44Z</created><updated>2013-03-03T18:55:11Z</updated><resolved>2013-03-03T18:55:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>building under JDK 8</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2721</link><project id="" key="" /><description>Hi Elasticsearch team,

For development, I'd like to prepare the transition to JDK 8. So aligning the Elasticsearch maven build for JDK 8 would be nice. I receive only 10 minor build errors related to annotations and JMX ResourceDMBean, and I wonder if patches are welcome to fix this or if JDK 8 transition is delayed.

Elasticsearch 0.90.0.Beta2-SNAPSHOT

```
mvn --version
Apache Maven 3.0.4 (r1232337; 2012-01-17 09:44:56+0100)
Maven home: /usr/share/maven
Java version: 1.8.0-ea, vendor: Oracle Corporation
Java home: /Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home/jre
Default locale: de_DE, platform encoding: utf-8
OS name: "mac os x", version: "10.8.2", arch: "x86_64", family: "mac"
```

Maven log excerpt

```
[INFO] -------------------------------------------------------------
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/Key.java:[509,29] error: cannot find symbol
[ERROR]  variable annotationType of type Class&lt;? extends Annotation&gt;
/Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/jmx/ResourceDMBean.java:[242,26] error: cannot find symbol
[ERROR]  variable method of type Method
/Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/jmx/ResourceDMBean.java:[362,29] error: cannot find symbol
[ERROR] /Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java:[59,43] error: cannot find symbol
[ERROR] 
    CAP#1 extends Annotation from capture of ? extends Annotation
/Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java:[72,29] error: cannot find symbol
[ERROR]  variable annotationType of type Class&lt;? extends Annotation&gt;
/Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/internal/Annotations.java:[110,43] error: cannot find symbol
[ERROR] /Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethodsModule.java:[79,26] error: cannot find symbol
[ERROR]  variable method of type Method
/Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/internal/ProviderMethod.java:[56,29] error: cannot find symbol
[ERROR]  variable method of type Method
/Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/common/inject/spi/DefaultBindingTargetVisitor.java:[74,25] error: incompatible types: Object cannot be converted to V
[ERROR] /Users/joerg/Projects/elasticsearch/jprante/elasticsearch/src/main/java/org/elasticsearch/jmx/JmxModule.java:[64,20] error: cannot find symbol
[INFO] 10 errors
```
</description><key id="11591179">2721</key><summary>building under JDK 8</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jprante</reporter><labels /><created>2013-03-03T13:26:38Z</created><updated>2013-03-18T00:34:44Z</updated><resolved>2013-03-18T00:34:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-18T00:34:44Z" id="15034602">fixed in latest push to master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Change the geo_shape support to only rely on the lucene geo_shape support</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2720</link><project id="" key="" /><description>Currently we use our own `XTermQueryPrefixTreeStrategy` for all the geo_shape indexing/searching (which our own version of lucene's `TermQueryPrefixTreeStrategy`). This strategy is not very ideal when it comes to accuracy. We should remove it, and set the `RecursivePrefixTreeStrategy` instead by default, while enable users to define the strategy in the mappings. 

Since both strategies use the same indexing method, we should also enable the user change the strategy at query time. This will enable the user to easily play around with the different strategies and try out different filter/query approaches without needing to reindex the documents.

This change will effectively remove support for shape "relations". More precisely, only the "intersects" relation will be supported ("within" and "disjoint" will be removed). One could emulate "disjoint" relation with a bool filter/query:

``` json
{
   "bool" : {
      "must_not" : [
          { "geo_shape" : { ... } }
      ]
   }
}
```
</description><key id="11576686">2720</key><summary>Change the geo_shape support to only rely on the lucene geo_shape support</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">uboness</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-02T14:24:26Z</created><updated>2013-03-19T18:02:17Z</updated><resolved>2013-03-02T16:14:36Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sarmiena" created="2013-03-14T00:16:08Z" id="14877003">How would we emulate a "within" or "contains" using strategy? @uboness 
</comment><comment author="kliffy" created="2013-03-14T17:34:57Z" id="14916935">I too am having an issue with this. is there still a way to use within?
</comment><comment author="kliffy" created="2013-03-18T22:22:06Z" id="15085882">@uboness 
</comment><comment author="uboness" created="2013-03-18T22:24:52Z" id="15086022">If you're looking for a point within a polygon it will still work with intersects. As for polygon within polygon, we removed this experimental support for now, until we figure out a way to do it properly (the previous implementation was experimental and hight erroneous
</comment><comment author="kliffy" created="2013-03-18T23:05:04Z" id="15087839">@uboness 
Was it experimental on the lucene side or for elasticsearch? Also, does this have to do with just 'within' or does that also include 'contains'?

Im trying to do a search using a single point (or a small polygon) finding all polygons that contain that point. During my experimentations i have found that intersects does not work like what you said and only finds when the edges intersect. We were editing the elasticsearch codebase to use lucenes 'contains' method and that seemed to do the trick. however, this is not the solution we want to go with unless there is no better way to do so.. and if that method too is highly erroneous
</comment><comment author="sarmiena" created="2013-03-19T05:31:31Z" id="15098269">Arghh yes... I need to use lucene's "contains" as well. What was wrong with the implementation?
</comment><comment author="sarmiena" created="2013-03-19T18:02:17Z" id="15132227">@uboness 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Make BoolFilterBuilder output proper json</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2719</link><project id="" key="" /><description>@kimchy
</description><key id="11567649">2719</key><summary>Make BoolFilterBuilder output proper json</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">tristanbuckner</reporter><labels /><created>2013-03-01T23:52:44Z</created><updated>2014-07-16T21:53:53Z</updated><resolved>2013-03-02T00:10:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-02T00:10:12Z" id="14318692">pushed to master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Filtered query to make query optional (defaults to mach_all)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2718</link><project id="" key="" /><description /><key id="11563256">2718</key><summary>Query DSL: Filtered query to make query optional (defaults to mach_all)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-03-01T21:39:35Z</created><updated>2013-03-01T21:40:27Z</updated><resolved>2013-03-01T21:40:27Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Beta: Min_prefix Option does not work</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2717</link><project id="" key="" /><description>According to the Suggest API, "min_prefix" is available as an option to increase performance or try spellcheck from the start of word.

https://github.com/elasticsearch/elasticsearch/issues/2585

Adding the option results in this exception:
ElasticSearchIllegalArgumentException[suggester[fuzzy] doesn't support [min_prefix]]; 
</description><key id="11560021">2717</key><summary>Beta: Min_prefix Option does not work</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">jtreher</reporter><labels /><created>2013-03-01T20:14:36Z</created><updated>2013-03-01T20:34:49Z</updated><resolved>2013-03-01T20:34:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T20:29:27Z" id="14309677">can you post a gist for this?
</comment><comment author="s1monw" created="2013-03-01T20:31:50Z" id="14309788">ah man the option is called `prefix_length` not `min_prefix` can you check if that works for you? I am afraid this is very much work in progress still and since 2 days ago this is not even called `fuzzy` but `term` and we have a new suggester that might be worth exporing #2709 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Mapping: Posting a mapping with default analyzer fails</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2716</link><project id="" key="" /><description>Seems like reposting a mapping explicitly mentioning the default analyzer throws an error when it should not
with the following mapping.json

``` json
{"foobar":{"dynamic":false,"properties":{"description": {
  "index":"analyzed",
  "analyzer":"default",
  "type":"string"}}}}
```

putting the mapping the 1st time everything goes well

`curl -XPUT -d @mapping.json http://localhost:9200/twitter/foobar/_mapping`

``` json
{"ok":true,"acknowledged":true}
```

but the second time
`curl -XPUT -d @mapping.json http://localhost:9200/twitter/foobar/_mapping`

``` json
{"error":"MergeMappingException[Merge failed with failures {[mapper [description] has different index_analyzer, mapper [description] has different search_analyzer]}]","status":400}
```

with this mapping instead there's no problem posting it over and over

``` json
{"foobar":{"dynamic":false,"properties":{"description": {
  "index":"analyzed",
  "analyzer":"english",
  "type":"string"}}}}
```

so seems like for some reason the default analyzer is not considered to be equal to itself
</description><key id="11545783">2716</key><summary>Mapping: Posting a mapping with default analyzer fails</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hungryblank</reporter><labels><label>bug</label><label>low hanging fruit</label><label>v1.5.0</label><label>v2.0.0-beta1</label></labels><created>2013-03-01T14:09:36Z</created><updated>2014-11-14T10:09:56Z</updated><resolved>2014-11-14T09:59:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="nwarz" created="2014-09-26T19:53:53Z" id="57011981">Just submitted PR #7902. Looks like there's only one point where null index analyzers and "default"-named index analyzers were being treated as nonequivalent.
</comment><comment author="colings86" created="2014-11-14T10:09:42Z" id="63036600">closed using https://github.com/elasticsearch/elasticsearch/pull/7902
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail in metadata parsing if the id path is not a value but rather an array or an object.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2715</link><project id="" key="" /><description>Closes #2275
</description><key id="11542012">2715</key><summary>Fail in metadata parsing if the id path is not a value but rather an array or an object.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-03-01T12:04:40Z</created><updated>2014-07-01T22:27:55Z</updated><resolved>2013-03-01T12:13:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>ES have bugs in 0.20.4 and 0.20.5 which cause creating some indices failure.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2714</link><project id="" key="" /><description>These days I found ES maybe have bugs in 0.20.4 and 0.20.5 which cause creating some indices failure.
  The test steps are following:
  1) I setup 20 nodes with 0.20.4, and bring a fresh cluster up. 3 nodes are master nodes, 2 nodes are load balancer, 15 nodes are data nodes 
  2) After the cluster is up, I tried to create some empty indices for example index-2013-02-25, index-2013-02-26, index-2013-02-27, index-2013-03-01, etc
  But some shards stuck in initializing status for long time.
{
  "cluster_name" : "es-test",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 105,
  "active_shards" : 201,
  "relocating_shards" : 0,
  "initializing_shards" : 9,
  "unassigned_shards" : 0
}
  Moreover when I created index-2013-03-02, the cluster became to red.
{
  "cluster_name" : "es-test",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 119,
  "active_shards" : 228,
  "relocating_shards" : 0,
  "initializing_shards" : 11,
  "unassigned_shards" : 1
}

  I set the log level to trace, and checked the logs, no error logs are shown. but from the logs, I can know some shards are initializing and unassigned. I will attach the logs later.
  And I also tried 0.20.5, the same problem happened.  
  But for 0.19.11, the problem disappeared. All the empty indices can be created successfully instantly even for some strange index names.
  So I guess ES have some bugs in 0.20.4 and 0.20.5.
  By the way, I also tested 0.20.4 on single node and two nodes with multiple ES instances, but such problem doesn't happen.
</description><key id="11528492">2714</key><summary>ES have bugs in 0.20.4 and 0.20.5 which cause creating some indices failure.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dongaihua</reporter><labels /><created>2013-03-01T01:44:02Z</created><updated>2014-01-28T21:15:29Z</updated><resolved>2014-01-28T21:15:29Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-03-01T11:54:26Z" id="14285702">Can you gist the logs from the server, plus a complete curl recreation of the problem?
</comment><comment author="dongaihua" created="2013-03-04T06:33:57Z" id="14365923">Hi, Clint:
  I don't know how to upload .rar file into gist, so I updated my post in google mail list, and attached the logs of 20 nodes in my post. You can download it from https://groups.google.com/forum/#!topic/elasticsearch/mYW-ruxCJ8A
  The logs are for 0.90.0.Beta1.  
  Today I tested the latest the ES version 0.90.0.Beta1, it has the same problem.
  My test configure is as following:
  20 nodes. 10.96.250.211,212,213 are master nodes. 211 is the leader; 214 and 215 are load balancer; other 15 nodes are data nodes.
  My test step is as following:
  1) after the cluster is up, I created an empty index: test1
curl -XPUT 10.96.250.214:10200/test1
{"ok":true,"acknowledged":true}
curl -XGET 10.96.250.211:10200/_cluster/health?pretty=true
{
  "cluster_name" : "es-test-0-90-0-beta1",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 15,
  "active_shards" : 27,
  "relocating_shards" : 0,
  "initializing_shards" : 3,
  "unassigned_shards" : 0
}
  The cluster stayed in this state for long time.
 2) Then I created another empty index: abcd1234
curl -XPUT 10.96.250.214:10200/abcd1234
{"ok":true,"acknowledged":false}
curl -XGET 10.96.250.211:10200/_cluster/health?pretty=true
{
  "cluster_name" : "es-test-0-90-0-beta1",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 29,
  "active_shards" : 53,
  "relocating_shards" : 0,
  "initializing_shards" : 6,
  "unassigned_shards" : 1
  The cluster stayed in this state for long time
 3) Then I created one more empty index: 
curl -XPUT 10.96.250.214:10200/1234abcd
{"ok":true,"acknowledged":false}
curl -XGET 10.96.250.211:10200/_cluster/health?pretty=true
{
  "cluster_name" : "es-test-0-90-0-beta1",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 43,
  "active_shards" : 78,
  "relocating_shards" : 0,
  "initializing_shards" : 6,
  "unassigned_shards" : 6
}
  The cluster stayed in this state for long time.
  Thank you.

-Regards-
-Dong Aihua-
</comment><comment author="dongaihua" created="2013-03-12T06:45:22Z" id="14760930">Hi, 
  I tested 0.19.12, it works fine.
  Then I tested 0.20.0, it has the same problem.
   The test steps are as before. And the following are some logs:
{
  "cluster_name" : "test-0.20.0",
  "status" : "red",
  "timed_out" : false,
  "number_of_nodes" : 20,
  "number_of_data_nodes" : 15,
  "active_primary_shards" : 224,
  "active_shards" : 445,
  "relocating_shards" : 0,
  "initializing_shards" : 4,
  "unassigned_shards" : 1
}
{ cluster_name: 'test-0.20.0',
  master_node: 
   { name: 'xseed021.kdev',
     transport_address: 'inet[/10.96.250.211:9400]',
     attributes: { data: 'false', master: 'true' } },
  initShards: 
   [ { node: '{"name":"xseed038.kdev","ip":"10.96.250.228:9400"}',
       primary: false,
       shard: 5,
       index: 'testx-xxx-2012-03-11' },
     { node: '{"name":"xseed034.kdev","ip":"10.96.250.224:9400"}',
       primary: false,
       shard: 11,
       index: 'test-2013-03-11' },
     { node: '{"name":"xseed034.kdev","ip":"10.96.250.224:9400"}',
       primary: true,
       shard: 7,
       index: 'testx-xxx-2013-zzz' },
     { node: '{"name":"xseed038.kdev","ip":"10.96.250.228:9400"}',
       primary: false,
       shard: 5,
       index: 'testx-xxx-2013-yyy' } ],
  unassigned_shards_total: 1,
  unassigned_indices: [ 'testx-xxx-2013-zzz' ] }
  The detail logs are located at https://github.com/dongaihua/shares/test-0.20.0_LOGS.tar.gz
  Thank you. 

-Regards-
-Dongaihua-
</comment><comment author="s1monw" created="2014-01-28T21:15:29Z" id="33526245">I will move out here - not sure if that is still relevant given that 1.0 is so close...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add circle shape to geo_shape mapping</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2713</link><project id="" key="" /><description>It's difficult to represent a very accurate circle as a polygon without using an excessive amount of points. It would be very useful if a circle shape could be added into the geo_shape mapping type. 

It seems that Spatial4J already has support for circles. Don't know if JTS's lack of built in support makes this problematic, however.
</description><key id="11524486">2713</key><summary>Add circle shape to geo_shape mapping</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">willtrking</reporter><labels /><created>2013-02-28T23:18:10Z</created><updated>2014-03-31T17:35:07Z</updated><resolved>2013-07-03T12:07:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="willtrking" created="2013-02-28T23:24:58Z" id="14264282">For my specific case this could also be solved by allowing the "geo_distance" filter's "distance" parameter to reference a field in a document. This way we could use a geo_point mapping to store coordinates, and a string mapping to store something like "12km" which the geo_distance could then reference on a per document basis, effectively allowing you to search within a circle which each document could define independently. 
</comment><comment author="jillesvangurp" created="2013-03-08T15:18:01Z" id="14625244">You can easily convert a circle to a polygon (taken from https://github.com/jillesvangurp/geotools). Use more segments for increased accuracy.

```
private static final double EARTH_RADIUS_METERS = 6371000.0;

public static double[][] circle2polygon(int segments, double latitude, double longitude, double radius) {
    if (segments &lt; 5) {
        throw new IllegalArgumentException("you need a minimum of 5 segments");
    }
    double[][] points = new double[segments+1][0];

    double relativeLatitude = radius / EARTH_RADIUS_METERS * 180 / PI;

    // things get funny near the north and south pole, so doing a modulo 90
    // to ensure that the relative amount of degrees doesn't get too crazy.
    double relativeLongitude = relativeLatitude / cos(Math.toRadians(latitude)) % 90;

    for (int i = 0; i &lt; segments; i++) {
        // radians go from 0 to 2*PI; we want to divide the circle in nice
        // segments
        double theta = 2 * PI * i / segments;
        // trying to avoid theta being exact factors of pi because that results in some funny behavior around the
        // north-pole
        theta = theta += 0.1;
        if (theta &gt;= 2 * PI) {
            theta = theta - 2 * PI;
        }

        // on the unit circle, any point of the circle has the coordinate
        // cos(t),sin(t) where t is the radian. So, all we need to do that
        // is multiply that with the relative latitude and longitude
        // note, latitude takes the role of y, not x. By convention we
        // always note latitude, longitude instead of the other way around
        double latOnCircle = latitude + relativeLatitude * Math.sin(theta);
        double lonOnCircle = longitude + relativeLongitude * Math.cos(theta);
        if (lonOnCircle &gt; 180) {
            lonOnCircle = -180 + (lonOnCircle - 180);
        } else if (lonOnCircle &lt; -180) {
            lonOnCircle = 180 - (lonOnCircle + 180);
        }

        if (latOnCircle &gt; 90) {
            latOnCircle = 90 - (latOnCircle - 90);
        } else if (latOnCircle &lt; -90) {
            latOnCircle = -90 - (latOnCircle + 90);
        }

        points[i] = new double[] { latOnCircle, lonOnCircle };
    }
    // should end with same point as the origin
    points[points.length-1] = new double[] {points[0][0],points[0][1]};
    return points;
}
```
</comment><comment author="jillesvangurp" created="2013-03-08T15:19:17Z" id="14625311">In any case, the geo_shape search is pretty inaccurate currently so I don't think the circle accuracy matters much.
</comment><comment author="schovi" created="2013-04-06T09:47:42Z" id="15993590">Looking for this feature too! :+1:

@spinscale is this in any available version?
</comment><comment author="peterwillis" created="2013-04-09T12:41:47Z" id="16110061">I am looking for this feature also, will it ever be pulled in?
</comment><comment author="spinscale" created="2013-04-09T14:20:55Z" id="16115127">Hey,

sorry for getting back late yo you, didnt see that thread. My commit is only a tiny fraction of the actual work needed to be done. It only allows you to store a geo circle field. The next step is to use the field for further calculations. I should probably be able dive into that next.
</comment><comment author="spinscale" created="2013-07-03T12:07:12Z" id="20411026">Added to master by https://github.com/elasticsearch/elasticsearch/commit/0c2d12bda34fbbe0ffd3cef5140f5d5a3919b080
</comment><comment author="viktornordling" created="2014-03-27T06:46:36Z" id="38773475">I was looking for a way to add circles to elasticsearch and ended up here. Great job for adding this!

I tried to do a search for whether a point falls in a circle, but I can't get it working. My search looks something like:

```
POST polygons/_search
 {
  "size" : 1000000,
  "query" : {
    "terms" : {
      "_id" : [ "GKN_32UnRiylOsYwyltX4g" ]
    }
  },
  "post_filter" : {
    "geo_shape" : {
      "vertices" : {
        "shape" : {
          "type" : "point",
          "coordinates" : [ 150, -33 ]
        },
        "relation" : "intersects"
      }
    }
  }
}
```

What am I missing?
</comment><comment author="glennji" created="2014-03-27T23:58:10Z" id="38875713">Hey Viktor, does the geo_distance filter work?

```
        "geo_distance" : {
            "distance" : "200km",
            "pin.location" : {
                "lat" : 40,
                "lon" : -70
            }
        }
```

To me, it looks like the above is defining a single point geo_shape, which means you're only searching for SHAPES which intersect the point (not points within a shape).
</comment><comment author="viktornordling" created="2014-03-29T13:39:34Z" id="38995789">I think I tried that but I think I had some issues with making the query syntactically correct. What should pin refer to here?

My reasoning was that searching for a geo_shape should work since both geo_polygon and geo_circle are geo_shapes..
</comment><comment author="spinscale" created="2014-03-30T14:49:00Z" id="39027216">`pin` is just a field of the document... please ask questions like this on the mailinglist instead of appending it to a closed ticket. There will be much more eyes notified and happily help you. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Correct order of routing and parent params for Get Requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2712</link><project id="" key="" /><description>The order in which routing and parent parameters are set is important.  The
routing parameter must be set first or it will overwrite the parent routing
value.
</description><key id="11523739">2712</key><summary>Correct order of routing and parent params for Get Requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels /><created>2013-02-28T22:57:13Z</created><updated>2014-07-16T21:53:54Z</updated><resolved>2013-03-01T21:25:00Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-03-01T21:20:14Z" id="14312043">FYI, get requests are broken in 0.90 (and probably earlier versions) if specifying the parent parameter.  This pull request fixes that.
</comment><comment author="kimchy" created="2013-03-01T21:25:00Z" id="14312260">Pushed to master and 0.20.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException when applying a sort and using ignoreMapped(true) and the field does not exist (0.20.5)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2711</link><project id="" key="" /><description>code:
SortBuilder sortBuilder = SortBuilders.fieldSort(query.getSortField()).ignoreUnmapped(true);
sortBuilder.order(query.getSortOrder());

Result:
org.elasticsearch.search.builder.SearchSourceBuilderException: Failed to build search source
    at org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:559)
    at org.elasticsearch.action.search.SearchRequest.source(SearchRequest.java:251)
    at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:814)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:62)
    at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:57)
    at 
...........
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.jackson.dataformat.smile.SmileGenerator._writeFieldName(SmileGenerator.java:580)
    at org.elasticsearch.common.jackson.dataformat.smile.SmileGenerator.writeFieldName(SmileGenerator.java:453)
    at org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeFieldName(JsonXContentGenerator.java:74)
    at org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:267)
    at org.elasticsearch.common.xcontent.XContentBuilder.startObject(XContentBuilder.java:136)
    at org.elasticsearch.search.sort.FieldSortBuilder.toXContent(FieldSortBuilder.java:78)
    at org.elasticsearch.search.builder.SearchSourceBuilder.toXContent(SearchSourceBuilder.java:673)
    at org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:556)
    ... 100 more
</description><key id="11518817">2711</key><summary>NullPointerException when applying a sort and using ignoreMapped(true) and the field does not exist (0.20.5)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">GrantGochnauer</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-02-28T20:54:14Z</created><updated>2013-03-03T18:27:33Z</updated><resolved>2013-03-01T11:26:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T11:12:49Z" id="14283795">hey, it seems you application is passing a null field in the FieldSortBuilder constructor, I agree we should throw an exception earlier and for sure no NPE but it seems like a bug in your code?
</comment><comment author="GrantGochnauer" created="2013-03-01T11:21:51Z" id="14284545">Hi Simon, thanks for the reply. I'll double check that no nulls are being passed in. 
</comment><comment author="s1monw" created="2013-03-03T18:27:33Z" id="14351811">i pushed this to 0.20 as well
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain API does not support POST</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2710</link><project id="" key="" /><description>It looks like the Explain API does not support POST requests, which means that certain clients will be unable to utilize it since they don't support GET with request bodies (notably javascript clients).

Any chance of adding POST support to Explain like we have for the other GET methods?

Demonstration, on 0.20.5:

``` bash
$ curl -XGET localhost:9200/test/benchmark/Yju6VrbaTUauZVMXhU5IBw/_explain?pretty -d '{
  "query":{
    "match":{
      "field1":"pattern"
    }
  }
}'

{
  "ok" : true,
  "_index" : "test",
  "_type" : "benchmark",
  "_id" : "Yju6VrbaTUauZVMXhU5IBw",
  "matched" : true,
  "explanation" : {
    "value" : 0.33165708,
    "description" : "weight(field1:pattern in 26), product of:",
    "details" : [ {
      "value" : 0.99999994,
      "description" : "queryWeight(field1:pattern), product of:",
      "details" : [ {
        "value" : 3.5376759,
        "description" : "idf(docFreq=263631, maxDocs=3335007)"
      }, {
        "value" : 0.28267145,
        "description" : "queryNorm"
      } ]
    }, {
      "value" : 0.3316571,
      "description" : "fieldWeight(field1:pattern in 26), product of:",
      "details" : [ {
        "value" : 1.0,
        "description" : "tf(termFreq(field1:pattern)=1)"
      }, {
        "value" : 3.5376759,
        "description" : "idf(docFreq=263631, maxDocs=3335007)"
      }, {
        "value" : 0.09375,
        "description" : "fieldNorm(field=field1, doc=26)"
      } ]
    } ]
  }



$ curl -XPOST localhost:9200/test/benchmark/Yju6VrbaTUauZVMXhU5IBw/_explain?pretty -d '{
  "query":{
    "match":{
      "field1":"pattern"
    }
  }
}'

No handler found for uri [/test/benchmark/Yju6VrbaTUauZVMXhU5IBw/_explain?pretty] and method [POST]
```
</description><key id="11509495">2710</key><summary>Explain API does not support POST</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-02-28T17:07:51Z</created><updated>2013-02-28T17:29:47Z</updated><resolved>2013-02-28T17:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-28T17:09:05Z" id="14244326">Yes, it makes sense to add this.
</comment><comment author="polyfractal" created="2013-02-28T17:29:47Z" id="14245413">Great, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add Phrase Suggester</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2709</link><project id="" key="" /><description># Phrase Suggester

The `term` suggester provides a very convenient API to access word alternatives on token
basis within a certain string distance. The API allows accessing each token in the stream
individually while suggest-selection is left to the API consumer. Yet, often already ranked
/ selected suggestions are required in order to present to the end-user.
Inside ElasticSearch we have the ability to access way more statistics and information quickly
to make better decision which token alternative to pick or if to pick an alternative at all.

This `phrase` suggester adds some logic on top of the `term` suggester to select entire
corrected phrases instead of individual tokens weighted based on a _ngram-langugage models_. In practice it
will be able to make better decision about which tokens to pick based on co-occurence and frequencies.
The current implementation is kept quite general and leaves room for future improvements.
# API Example

The `phrase` request is defined along side the query part in the json request:

``` json
curl -s -XPOST 'localhost:9200/_search' -d {
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
        "analyzer" : "body",
        "field" : "bigram",
        "size" : 1,
        "real_word_error_likelihood" : 0.95,
        "max_errors" : 0.5,
        "gram_size" : 2,
        "direct_generator" : [ {
          "field" : "body",
          "suggest_mode" : "always",
          "min_word_len" : 1
        } ]
      }
    }
  }
}
```

The response contains suggested sored by the most likely spell correction first. In this case we got the expected correction
`xorr the god jewel` first while the second correction is less conservative where only one of the errors is corrected. Note, the request
is executed with `max_errors` set to `0.5` so 50% of the terms can contain misspellings (See parameter descriptions below).

``` json
  {
  "took" : 37,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 2938,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "suggest" : {
    "simple_phrase" : [ {
      "text" : "Xor the Got-Jewel",
      "offset" : 0,
      "length" : 17,
      "options" : [ {
        "text" : "xorr the god jewel",
        "score" : 0.17877324
      }, {
        "text" : "xor the god jewel",
        "score" : 0.14231323
      } ]
    } ]
  }
}
```
# Phrase suggest API
## Basic parameters
- `field` - the name of the field used to do n-gram lookups for the language model, the suggester will use this field to gain statistics to score corrections.
- `gram_size` - sets max size of the n-grams (shingles) in the `field`. If the field doesn't contain n-grams (shingles) this should be omitted or set to `1`.
- `real_word_error_likelihood` - the likelihood of a term being a misspelled even if the term exists in the dictionary. The default it `0.95` corresponding to 5% or the real words are misspelled.
- `confidence` - The confidence level defines a factor applied to the input phrases score which is used as a threshold for other suggest candidates. Only candidates that score higher than the threshold will be included in the result. For instance a confidence level of `1.0` will only return suggestions that score higher than the input phrase. If set to `0.0` the top N candidates are returned. The default is `1.0`.
- `max_errors` - the maximum percentage of the terms that at most considered to be misspellings in order to form a correction. This method accepts a float value in the range `[0..1)` as a fraction of the actual query terms a number `&gt;=1` as an absolut number of query terms. The default is set to `1.0` which corresponds to that only corrections with at most 1 misspelled term are returned.
- `separator` - the separator that is used to separate terms in the bigram field. If not set the whitespce character is used as a separator.
- `size` - the number of candidates that are generated for each individual query term Low numbers like `3` or `5` typically produce good results. Raising this can bring up terms with higher edit distances. The default is `5`.
- `analyzer` -  Sets the analyzer to analyse to suggest text with. Defaults to the search analyzer of the suggest field passed via `field`.
- `shard_size` - Sets the maximum number of suggested term to be retrieved from each individual shard. During the reduce phase the only the top N suggestions are returned based on the `size` option. Defaults to `5`.
- `text` - Sets the text / query to provide suggestions for.
## Smoothing Models

The `phrase` suggester supports multiple smoothing models to balance weight between infrequent grams (grams (shingles) are not existing in the index) and frequent grams (appear at least once in the index).
- `laplace` - the default model that uses an additive smoothing model where a constant (typically `1.0` or smaller) is added to all counts to balance weights, The default `alpha` is `0.5`.
- `stupid_backoff` - a simple backoff model that backs off to lower order n-gram models if the higher order count is `0` and discounts the lower order n-gram model by a constant factor. The default `discount` is `0.4`.
- `linear_interpolation` - a smoothing model that takes the weighted mean of the unigrams, bigrams and trigrams based on user supplied weights (lambdas). Linear Interpolation doesn't have any default values. All parameters (`trigram_lambda`, `bigram_lambda`, `unigram_lambda`) must be supplied.
## Candidate Generators

The `phrase` suggester uses candidate generators to produce a list of possible terms per term in the given text. A single candidate generator is similar to a `term` suggester called for each individual term in the text. The output of the generators is subsequently scored in in combination with the candidates from the other terms to for suggestion candidates.
Currently only one type of candidate generator is supported, the `direct_generator`. The Phrase suggest API accepts a list of generators under the key `direct_generator` each of the generators in the list are called per term in the original text.
## Direct Generators

The direct generators support the following parameters:
- `field` - The field to fetch the candidate suggestions from. This is an required option that either needs to be set globally or per suggestion.
- `analyzer` - The analyzer to analyse the suggest text with. Defaults to the search analyzer of the suggest field.
- `size` - The maximum corrections to be returned per suggest text token.
- `suggest_mode` - The suggest mode controls what suggestions are included or controls for what suggest text terms, suggestions should be suggested. Three possible values can be specified:
  - `missing` - Only suggest terms in the suggest text that aren't in the index. This is the default.
  - `popular` - Only suggest suggestions that occur in more docs then the original suggest text term.
  - `always` - Suggest any matching suggestions based on terms in the suggest text.
- `max_edits` - The maximum edit distance candidate suggestions can have in order to be considered as a suggestion. Can only be a value between 1 and 2. Any other value result in an bad request error being thrown. Defaults to 2.
- `min_prefix` - The number of minimal prefix characters that must match in order be a candidate suggestions. Defaults to 1. Increasing this number improves spellcheck performance. Usually misspellings don't occur in the beginning of terms.
- `min_query_length` -  The minimum length a suggest text term must have in order to be included. Defaults to 4.
- `max_inspections` - A factor that is used to multiply with the `shards_size` in order to inspect more candidate spell corrections on the shard level. Can improve accuracy at the cost of performance. Defaults to 5.
- `threshold_frequency` - The minimal threshold in number of documents a suggestion should appear in. This can be specified as an absolute number or as a relative percentage of number of documents. This can improve quality by only suggesting high frequency terms. Defaults to 0f and is not enabled. If a value higher than 1 is specified then the number cannot be fractional. The shard level document frequencies are used for this option.
- `max_query_frequency` - The maximum threshold in number of documents a sugges text token can exist in order to be included. Can be a relative percentage number (e.g 0.4) or an absolute number to represent document frequencies. If an value higher than 1 is specified then fractional can not be specified. Defaults to 0.01f. This can be used to exclude high frequency terms from being spellchecked. High frequency terms are usually spelled correctly on top of this this also improves the spellcheck performance.  The shard level document frequencies are used for this option.
- pre_filter -  a filter (analyzer) that is applied to each of the tokens passed to this candidate generator. This filter is applied to the original token before candidates are generated. (optional)
- post_filter - a filter (analyzer) that is applied to each of the generated tokens before they are passed to the actual phrase scorer. (optional)

The following example shows a `phrase` suggest call with two generators, the first one is using a field containing ordinary indexed terms and the second one uses a field that uses
terms indexed with a `reverse` filter (tokens are index in reverse order). This is used to overcome the limitation of the direct generators to require a constant prefix to provide high-performance suggestions. The `pre_filter` and `post_filter` options accept ordinary analyzer names. 

``` json
curl -s -XPOST 'localhost:9200/_search' -d {
 "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
        "analyzer" : "body",
        "field" : "bigram",
        "size" : 4,
        "real_word_error_likelihood" : 0.95,
        "confidence" : 2.0,
        "gram_size" : 2,
        "direct_generator" : [ {
          "field" : "body",
          "suggest_mode" : "always",
          "min_word_len" : 1
        }, {
          "field" : "reverse",
          "suggest_mode" : "always",
          "min_word_len" : 1,
          "pre_filter" : "reverse",
          "post_filter" : "reverse"
        } ]
      }
    }
  }
}
```

`pre_filter` and `post_filter` can also be used to inject synonyms after candidates are generated. For instance for the query `captain usq` we might generate a candidate `usa` for term `usq` which is a synonym for `america` which allows to present `captain america` to the user if this phrase scores high enough.
</description><key id="11503300">2709</key><summary>Add Phrase Suggester</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0.RC1</label></labels><created>2013-02-28T14:49:40Z</created><updated>2014-03-30T10:11:38Z</updated><resolved>2013-02-28T15:18:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2013-02-28T21:39:03Z" id="14259217">naive is a more commonly used term (vs "stupid") for the StupidBackoff param.
</comment><comment author="s1monw" created="2013-03-01T07:07:17Z" id="14275762">@Downchuck I haven't heard of "Naive Backoff"  - do you have any pointers where folks refer to this language model as "Naive Backoff" vs. ["Stupid Backoff"](https://www.google.com/search?q=stupid%20backoff%20language%20model)
</comment><comment author="Downchuck" created="2013-03-01T07:23:31Z" id="14276133">Oops, turns out I'm completely wrong (thinking of something else); I should pay more attention when I'm at work.
</comment><comment author="s1monw" created="2013-03-01T09:04:26Z" id="14278749">@Downchuck no worries - it's friday though :)
</comment><comment author="jtreher" created="2013-03-01T20:38:36Z" id="14310098">Well, this looks pretty sexy.
</comment><comment author="mattweber" created="2013-03-02T00:32:16Z" id="14319241">So what is the best way to use this?  Setup a multi-field say "suggestion" that does regular analysis, then "suggestion.bigram" that adds a n-gram token filter?  Then set the phrase field to "suggestion.bigram" and the direct generator field to "suggestion"?
</comment><comment author="s1monw" created="2013-03-02T06:39:52Z" id="14324079">@mattweber yeah that is pretty much what I intended! just make sure you use shingle filter not ngram. NGram is a character n-gram this guy needs word n-grams
</comment><comment author="mattweber" created="2013-03-05T01:13:36Z" id="14417818">Would it make sense to move the smoothing options under a "smoothing" object?  Something like:

``` javascript
curl -s -XPOST 'localhost:9200/_search' -d '{
  "suggest" : {
    "text" : "Xor the Got-Jewel",
    "simple_phrase" : {
      "phrase" : {
        "analyzer" : "body",
        "field" : "bigram",
        "size" : 1,
        "real_word_error_likelihood" : 0.95,
        "max_errors" : 0.5,
        "gram_size" : 2,
        "smoothing" : {
          "stupid_backoff" : {
            "discount" : 0.4
          }
        },
        "direct_generator" : [ {
          "field" : "body",
          "suggest_mode" : "always",
          "min_word_len" : 1
        } ]
      }
    }
  }
}'
```
</comment><comment author="jtreher" created="2013-03-05T17:38:34Z" id="14453570">I'm trying to get corrections for missing spaces, i.e. Recipe for ItalienFood. 
Did you mean: Recipe for italian food?

I'm guessing for this you would almost need to match grams stripped of whitespace? 

Regarding the phrase correct in general, I'm only getting corrected results for the last word in my phrase, so I must have my field setup improperly. I'm using an out of the box shingle filter.
</comment><comment author="s1monw" created="2013-03-05T20:28:01Z" id="14463175">@jtreher I am afraid we don't have the infrastructure in place to support this at this point but I am working on generators that support this kind of checking. 
</comment><comment author="s1monw" created="2013-03-05T20:29:49Z" id="14463277">@mattweber I mean we can do that, what would be the benefit? would it be more natural to use?
</comment><comment author="mattweber" created="2013-03-05T20:35:14Z" id="14463589">Yes, I think it is more natural and fits with the api better.  Throughout the api there really isn't any other places where  you just have variably named object like that.  Doesn't really matter though.
</comment><comment author="jtreher" created="2013-03-05T20:35:56Z" id="14463627">Well, I did get phrase correction working quite well save for the spaces. I'm still messing with the configs and am excited to see what you settle on for parameter names. Looking at the source works for now. :)

I was struggling initially because I did not double check my candidate field. It was using a stupid kstem filter! I was all over the place messing with the configs. I was getting good results once in a while for certain phrases. Oh well, it was a good learning experience as I think I've toyed with every setting. Flip the switch to a field using standard and bam!
</comment><comment author="s1monw" created="2013-03-05T20:37:57Z" id="14463739">@jtreher did you try using stupid backoff?
</comment><comment author="jtreher" created="2013-03-05T20:52:25Z" id="14464495">@s1monw I changed discount all around, but didn't really see much difference. 
</comment><comment author="mattweber" created="2013-03-05T20:54:39Z" id="14464608">Switching from laplace to stupid_backoff with the default discount made a huge difference in my tests.
</comment><comment author="jtreher" created="2013-03-05T20:58:37Z" id="14464803">Hah, I don't think I actually switched smoothing models. No wonder discount didn't do anything. I've only been working with it for a few hours.
</comment><comment author="mattweber" created="2013-03-05T21:09:09Z" id="14465332">If it helps, this is my current settings that are returning pretty good results.

```
"analyzer": "standard"
"field" : "Title.Shingles"    // shingles max size of 2
"real_word_error_likelihood" : 0.95
"confidence" : 2.0
"max_errors": 0.75
"gram_size" : 2
"stupid_backoff": {}
```
</comment><comment author="jtreher" created="2013-03-05T21:15:35Z" id="14465659">Yeah, I just got it after scouring the source again. I figured that since "discount" wasn't throwing an error, it must be working. I wasn't putting it in the stupid backoff object. Thanks for the support guys.
</comment><comment author="s1monw" created="2013-03-06T12:16:01Z" id="14496378">@mattweber @jtreher I will go ahead and open an issue to move the smoothing into it's own object and make sure we fail if there is a parameter that is not known (I actually thought this works already....) - thanks for getting all this infos back to me!
</comment><comment author="s1monw" created="2013-03-06T13:32:48Z" id="14499256">see #2735 
</comment><comment author="jtreher" created="2013-03-06T18:47:17Z" id="14518187">@s1monw  Thanks a ton again. I see you changed the term API to match the phrase. Nicely done. 

Now, if I can only get tawl to turn into towel without using a phonetic filter! Phonetic bigrams are providing some interesting results with this as well, but it's a lot of traffic. I'm also experimenting with the character gram filtered with a regex that strips whitespace. I've been able to make sense out of a lot of mistyped phrases. It's quite satisfying.
</comment><comment author="jtreher" created="2013-03-08T12:50:02Z" id="14617667">@s1monw Is there something that would be constraining suggest size to 5, even with the size field override? At first I thought it was just an edit distance issue, but I've set my string_distance to be straight-up Levenshtein,  and I know that the edit distance is equal for tawl =&gt; towel and tawl =&gt; tail (2). It seems that I only ever get 5 results. I can set size to less than 5 which works out well, but I can't seem to get more. I find the same with Term and Phrase in Beta2. I use max_edits:2.
</comment><comment author="s1monw" created="2013-03-08T13:55:45Z" id="14620398">@jtreher there are 2 size params one on the phrase suggester itself and one on the candidate generator. you should be able to override the one on the candidate generator to get what you want.

I actually thought about the phonetic stuff. So in theory you can actually make this work with phonetics as well. If you have a field that creates tokens in a certain way like "soundex|actualword" in your example "T400|towel" you can build a direct generator that uses a prefix_len of 5 and produces tokens like "T400|tawl" with a pre-filter on the generator and removes the 5 leading chars with a post filter. Maybe you give it a try but that way you only getting LD matches that also have the same soundex code 
</comment><comment author="jtreher" created="2013-03-08T14:19:19Z" id="14622362">@s1monw Interesting idea, I also found some other interesting uses of the phonetic filter of the last few days. I'm finding doublemetaphone to be the most useful. I think I might have found a bug with phonetic highlighting as well. Usually it works, but once in a while it will highlight the whole string. I'll test that more and log it if so.

Adding size:10 to both areas results in the same effect, I'm afraid. 
</comment><comment author="s1monw" created="2013-03-09T12:20:18Z" id="14662158">@jtreher you are right, see #2752

note if you raise the # of candidates you should also raise (lower) accuracy to get "far away" candidates
</comment><comment author="jtreher" created="2013-03-11T16:11:10Z" id="14722422">Edited. Thanks! RE #2752. I looked at the revision and it seems that most files are hitting the phrase. I found that the term is also not respecting the size. Does this fix that?
</comment><comment author="s1monw" created="2013-03-12T09:54:59Z" id="14766798">@jtreher `size` is a parameter on the candidate generator. if you set size on `term` you should also set `shard_size` if you have only one shard. `shard_size` is used for the number of terms per shard. Hope that helps
</comment><comment author="jtreher" created="2013-04-10T14:52:31Z" id="16179114">What happened to the threshold_frequency  for the direct generator? Some other arguments seem to be throwing illegal argument exceptions as well. 

Also, of note, this behavior might cause some new to information retrieval users, like myself, to be quite confused. I think it just needs more attention in the documentation, so I will put a small note on this page. Notice my query below to _suggest REST API. If I leave the size as default for the candidate generator, it never finds "towel" for "paper tawl" even though it is the second most common phrase with the word paper. If I override size to 10 in the candidate generator, it finds it and puts it at position 2. So, obviously users need to understand that the direct generator doesn't care about the whole phrase, it is merely providing candidates for the phrase suggest to use in it's shingle calculations. I'm not sure how the candidate generator sorts, but I can see that tawl scores very low by default on a text suggest, but is ordered much higher when it comes to frequency sorting.

Of course it is definitely a balancing act, but just wanted to throw this out there for any google ninjas.

{
  "text": "paper tawl",
  "did_you_mean": {
    "phrase": {
      "field": "description_spellcheck_biword_shingle",
      "size": 5,
      "direct_generator": [
        {
          "field": "description_plain",
          "max_edits": 2,
          "size": 10
        }
      ]
    }
  }
}
//with size 10, we get the right results
    did_you_mean: [
        {
            text: paper tawl
            offset: 0
            length: 10
            options: [
                {
                    text: paper table
                    score: 0.005753702
                }
                {
                    text: paper towel
                    score: 0.00501787
                }
                {
                    text: paper take
                    score: 0.002958646
                }
                {
                    text: paper tall
                    score: 0.000378143
                }
                {
                    text: paper teal
                    score: 0.00027579395
                }
            ]
        }
    ]
//if we don't give it enough candidates, we don't get the right results
did_you_mean: [

```
{
    text: paper tawl
    offset: 0
    length: 10
    options: [
        {
            text: paper table
            score: 0.005753702
        }
        {
            text: paper take
            score: 0.002958646
        }
        {
            text: paper tall
            score: 0.000378143
        }
        {
            text: paper tank
            score: 0.00022716245
        }
        {
            text: paper tail
            score: 0.000048908416
        }
    ]
}
```

]
</comment><comment author="s1monw" created="2013-04-10T15:04:56Z" id="16180010">hey, yeah I am sorry it's not perfect so you still need to know something about how the software works. Regarding the threshold_frequency it's been renamed to `min_doc_freq` I will update documentation. Did you encounter any other not working params?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unable to index null value for geo_shape</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2708</link><project id="" key="" /><description>I'm not able to index null values for geo_shapes.
The use case is to index several cities and and add the various polygons later.

I insert the cities through a postgres database, so the workaround for now will to remove polygons with null values inside the jdbc river.

It would be nice, if ElasticSearch could handle such data properly.

I posted some sample data in the following gist.
https://gist.github.com/mgruel/5055147

Best regards,
Meykel
</description><key id="11491632">2708</key><summary>Unable to index null value for geo_shape</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mgruel</reporter><labels /><created>2013-02-28T08:31:25Z</created><updated>2013-07-15T14:50:58Z</updated><resolved>2013-07-05T14:11:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Custom SimilarityProvider does not work 0.90.0-Beta1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2707</link><project id="" key="" /><description>On index creation i define own custom similarity provider (index.similarity.index.type andindex.similarity.search.type).

Search scoring is calculated from default similarity and not from my similarity provider
</description><key id="11483290">2707</key><summary>Custom SimilarityProvider does not work 0.90.0-Beta1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lukapor</reporter><labels><label>non-issue</label></labels><created>2013-02-28T01:02:05Z</created><updated>2013-03-27T20:24:28Z</updated><resolved>2013-03-07T14:52:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-28T07:54:40Z" id="14221366">hey, can you provide a gist with json / curl statements that reproduce your problem?
</comment><comment author="lukapor" created="2013-03-02T01:23:18Z" id="14320305">I provide small test  https://github.com/lukapor/customsimilarity
</comment><comment author="s1monw" created="2013-03-02T06:35:24Z" id="14324036">thanks @lukapor I will look into this and try to figure out what the problem is
</comment><comment author="s1monw" created="2013-03-02T09:49:41Z" id="14325749">hey,

I didn't try it but from looking at it I think you are missing to add the similarityprovider in you plugin.

register it in BcsocialSimilarityPlugin.java 

``` java
public void onModule(SimilarityModule module) {
  module.addSimilarity("bcsocial", BcsocialSimilarityProvider.class);
}
```

then when you create the index just do: 

``` json
{
   "settings":{
      "index":{
         "number_of_shards":1,
         "number_of_replicas":1
      },
      "similarity":{
         "index":{
            "type":"bcsocial"
         },
         "search":{
            "type":"bcsocial"
         }
      }
   }
}
```

this should pick up your similarity then.

Please report back if that works
</comment><comment author="s1monw" created="2013-03-07T12:00:08Z" id="14555988">hey, did you try my recommendations? any updates on this?
</comment><comment author="lukapor" created="2013-03-07T14:10:21Z" id="14562231">Simon hello.

Yes i did but it didnt work. This weekend we planned to replace our core lucene search imlementation with elastic search(0.20.5) - with our own similarity provider on our product (http://www.bcsocial.io). 

So next week i have a little bit more time - and i get deep into it.
</comment><comment author="s1monw" created="2013-03-07T14:52:41Z" id="14564525">alright I looked into it and found the problems:
1. you need to register the provider

``` java
public void onModule(SimilarityModule module) {
  module.addSimilarity("bcsocial", BcsocialSimilarityProvider.class);
}
```
1. add a es-plugin.property

add src/main/resources/es-plugin.properties

```
plugin=org.elasticsearch.bcsocial.similarity.BcsocialSimilarityPlugin
```
1. make sure you add a mapping
   when you create the index you need to set the similarity type in your mapping not in the settings

``` json
{
   "settings":{
      "index":{
         "number_of_shards":1,
         "number_of_replicas":1
      }
   },
   "mappings":{
      "entry":{
         "properties":{
            "message":{
               "type":"string",
               "similarity":"bcsocial"
            }
         }
      }
   }
}
```

then you can search 

``` json
$ ./Search.bat | python -mjson.tool
{
    "_shards": {
        "failed": 0, 
        "successful": 1, 
        "total": 1
    }, 
    "hits": {
        "hits": [
            {
                "_id": "1", 
                "_index": "test", 
                "_score": 1.4142135, 
                "_source": {
                    "message": "test custum similarity", 
                    "user": "luka"
                }, 
                "_type": "entry"
            }
        ], 
        "max_score": 1.4142135, 
        "total": 1
    }, 
    "timed_out": false, 
    "took": 1
}
```

and the normal script:

``` json
$ ./SearchNormal.bat | python -mjson.tool
{
    "_shards": {
        "failed": 0, 
        "successful": 1, 
        "total": 1
    }, 
    "hits": {
        "hits": [
            {
                "_id": "1", 
                "_index": "test_normal", 
                "_score": 0.2169777, 
                "_source": {
                    "message": "test default similarity", 
                    "user": "luka"
                }, 
                "_type": "entry"
            }
        ], 
        "max_score": 0.2169777, 
        "total": 1
    }, 
    "timed_out": false, 
    "took": 2
}
```

I guess this is what you wanted... I will close the issue its a topic for the mailing list.
</comment><comment author="lukapor" created="2013-03-27T20:24:28Z" id="15550768">Finnaly I have time to test new version of elasticsearch. It is fantastic. Great work.

I manage to create plugin with own similarity and create index with field similarity. And everything si ok. Term queries scoring is correct.

When i use prefix query with (constant_score_boolean) i think that default similarity is taken (below i see that queryNorm is not 1). If i choose scoring_boolean option it work ok (because it takes PerFieldSimilarity) , but I need constant_score_boolean.
Is any setting to set default search similarity

thank in advance

query:
{
  "from": 0,
  "size": 10,
  "explain" : true,
  "query": {
    "bool": {
      "must": [
        {
          "prefix": {
            "message": {
              "value": "te",
               "boost": 2,
           "rewrite" :"constant_score_boolean",
             }
          }
        },
        {
          "term": {
            "message": {
               "value": "similarity",
               "boost": 2
             }
          }
        }
      ]
    }
  }
}

result:

```
  "_explanation" : {
    "value" : 2.8944273,
    "description" : "sum of:",
    "details" : [ {
      "value" : 0.8944272,
      "description" : "ConstantScore(message:test^2.0 message:testiram^2.0)^
```

2.0, product of:",
          "details" : [ {
            "value" : 2.0,
            "description" : "boost"
          }, {
            "value" : 0.4472136,
            "description" : "queryNorm"
          } ]
        }, {
          "value" : 2.0,
          "description" : "weight(message:similarity^2.0 in 0) [PerFieldSimilari
ty], result of:",
          "details" : [ {
            "value" : 2.0,
            "description" : "score(doc=0,freq=1.0 = termFreq=1.0\n), product of:
",
            "details" : [ {
              "value" : 2.0,
              "description" : "queryWeight, product of:",
              "details" : [ {
                "value" : 2.0,
                "description" : "boost"
              }, {
                "value" : 1.0,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 1.0,
                "description" : "queryNorm"
              } ]
            }, {
              "value" : 1.0,
              "description" : "fieldWeight in 0, product of:",
              "details" : [ {
                "value" : 1.0,
                "description" : "tf(freq=1.0), with freq of:",
                "details" : [ {
                  "value" : 1.0,
                  "description" : "termFreq=1.0"
                } ]
              }, {
                "value" : 1.0,
                "description" : "idf(docFreq=1, maxDocs=1)"
              }, {
                "value" : 1.0,
                "description" : "fieldNorm(doc=0)"
              } ]
            } ]
          } ]
        } ]
      }
    } ]
  }
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support all_terms option in terms facet using new field data</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2706</link><project id="" key="" /><description>all_terms support in terms facet has been commented out since the new field data implementation.
</description><key id="11481753">2706</key><summary>Support all_terms option in terms facet using new field data</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>regression</label></labels><created>2013-02-28T00:09:57Z</created><updated>2014-08-08T10:27:05Z</updated><resolved>2014-08-08T10:27:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-01T13:06:41Z" id="14288340">Note, all terms is supported for the default execution type (ordinal, non map), we need to add it to map, agreed.
</comment><comment author="clintongormley" created="2014-08-08T10:27:05Z" id="51585693">Given that facets are deprecated, I'm going to close this.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Parent/child queries don't work with via the delete by query api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2705</link><project id="" key="" /><description>Reported via ML:
https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/Wsv3ziKdeTk
</description><key id="11480318">2705</key><summary>Parent/child queries don't work with via the delete by query api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>bug</label></labels><created>2013-02-27T23:25:41Z</created><updated>2013-10-09T08:48:15Z</updated><resolved>2013-10-09T08:48:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="hari-tw" created="2013-03-01T09:39:09Z" id="14280000">we have the same issue.. any updates on this?
</comment><comment author="snmaynard" created="2013-04-02T21:24:19Z" id="15803566">Any updates on when this might be fixed? I just need to know whether I should work round the issue or if we should just wait for an official fix?
</comment><comment author="martijnvg" created="2013-04-16T11:04:50Z" id="16438427">I might take a while, before we fully support parent / child queries via the delete by query api.
</comment><comment author="ofavre" created="2013-04-22T13:40:55Z" id="16786218">Here is a recreation gist https://gist.github.com/ofavre/5434929 , it fails under 0.90.0.RC2.
Is it safe to use `{"term":{"_parent":"parenttype#parentid"}}` as a workaround?
</comment><comment author="martijnvg" created="2013-04-22T15:42:10Z" id="16795717">@ofavre This is safe to use. I'll make sure you don't get a NPE, when you use parent/child like queries with delete by query, but a descriptive error message for now.
</comment><comment author="martijnvg" created="2013-10-09T08:48:15Z" id="25955402">Implemented via #3822
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Non-data nodes do not support node stats endpoint</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2704</link><project id="" key="" /><description>It can be super helpful to piggyback on ElasticSearch's node stats endpoint to report various stats on all nodes on the cluster, however non-data nodes (data=false or client=true)  do not seem to have this enabled, and node stats return mostly empty for them.
</description><key id="11462544">2704</key><summary>Non-data nodes do not support node stats endpoint</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-02-27T16:25:50Z</created><updated>2013-02-28T08:56:26Z</updated><resolved>2013-02-27T17:19:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-02-27T17:09:16Z" id="14185830">I can't reproduce this.  `curl -s localhost:9202/_nodes/stats\?all=1` shows me all the nodes' stats.  This is with a freshly started cluster of two data nodes and one client node (`9202` points to the client node).  Can you be more specific about how to see what you're seeing?
</comment><comment author="synhershko" created="2013-02-27T17:18:08Z" id="14186345">I'm using the Java API, not the REST endpoint. I'm issuing this:

NodesStatsResponse nodesStatsResponse = client.admin().cluster().prepareNodesStats().all().execute().actionGet();
for (NodeStats nodeStats : nodesStatsResponse.nodes()) {
// nodeStats.getOs() etc are empty for non-data nodes
}
</comment><comment author="kimchy" created="2013-02-27T17:19:41Z" id="14186447">@synhershko os and some other stats require the sigar lib in order to work. You probably don't have it in your client apps.
</comment><comment author="drewr" created="2013-02-27T17:19:42Z" id="14186449">Sounds like this is likely a duplicate of #2703 then?
</comment><comment author="synhershko" created="2013-02-28T08:56:26Z" id="14223260">@kimchy getNode().getOs().getMem() is still null for non-data nodes also when I explicitly add dependency on sigar 1.6.4 . Same goes for Swap and FS data, and probably other stats as well.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NodesStatsResponse isn't fully populated if search wasn't made</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2703</link><project id="" key="" /><description>Using 0.90

If a data node hasn't been searched on, the getOs().getMem() in the NodesStatsResponse for that node will be null. It appears as if an actual search request has to be issued against a node for that to be populated.
</description><key id="11462378">2703</key><summary>NodesStatsResponse isn't fully populated if search wasn't made</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-02-27T16:22:24Z</created><updated>2014-06-24T00:18:34Z</updated><resolved>2014-06-24T00:18:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-02-27T20:18:32Z" id="14196499">With `sigar 1.6.4` on the classpath, `.getNodes()[0].getOs().getMem()` returns `null` for me from a client node.  Making a search request through the same client node (either Java API or REST) doesn't change anything.
</comment><comment author="drewr" created="2013-02-27T20:46:43Z" id="14198287">Tracing it down to `java.lang.UnsatisfiedLinkError: org.hyperic.sigar.Sigar.getPid()` in `SigarService`. The os-dependent lib doesn't get loaded for `node.client=true` for some reason....
</comment><comment author="synhershko" created="2013-02-28T08:57:57Z" id="14223308">As far as I can tell this is also the case for node.data=false
</comment><comment author="spinscale" created="2013-07-17T07:23:07Z" id="21096182">Hey Itamar,

what operating system does this happen on for you? Just tested on macos and everything works as expected with 0.90.2

My setup
- Fired up a normal node
- Fired up `node.data:false` node
- Checked the stats API, where everything was populated

Anything different on your side?
</comment><comment author="synhershko" created="2013-07-17T07:25:12Z" id="21096269">Ubuntu something, and initialized a node from code using the Java lib
</comment><comment author="synhershko" created="2014-06-24T00:18:34Z" id="46918445">This is a stale issue, and I'm pretty sure it was related to sigar not being referenced properly. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ES 0.20.5/0.90.0.Beta1 only ever names node "Aardwolf"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2702</link><project id="" key="" /><description>On Linux, with Oracle's java:

```
&#8756; java -version
java version "1.7.0_15"
Java(TM) SE Runtime Environment (build 1.7.0_15-b03)
Java HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)
```

Every time I start a foreground node, "Aardwolf" is chosen for the name.
</description><key id="11460888">2702</key><summary>ES 0.20.5/0.90.0.Beta1 only ever names node "Aardwolf"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-02-27T15:50:55Z</created><updated>2013-02-27T17:02:56Z</updated><resolved>2013-02-27T17:02:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-02-27T16:02:48Z" id="14181701">I just tested it and got:
- Arizona Annie
- Abdol, Ahmet
- Tempest

I can not reproduce it. Did you define any settings in `elasticsearch.yml`?
</comment><comment author="dakrone" created="2013-02-27T16:19:15Z" id="14182669">Nope, no settings in `elasticsearch.yml`, vanilla ES 0.90.0.Beta1, no plugins installed.
</comment><comment author="dakrone" created="2013-02-27T16:22:41Z" id="14182855">I tested this on a different node and the name changed, however, it's still stuck at "Aardwork" for my local machine.
</comment><comment author="dakrone" created="2013-02-27T16:25:32Z" id="14183037">Also, now that I go back and test it, I see this same issue in 0.20.5 for my machine.
</comment><comment author="drewr" created="2013-02-27T16:33:19Z" id="14183549">@imotov and I have seen this on early versions of OpenJDK.  I just tested Oracle 7u10 and 7u15 on Linux and neither has the issue.  So, is it possible your Java isn't installed right?  After starting a node, check and make sure `curl -s localhost:9200/_nodes\?all=1 | python -mjson.tool | fgrep vm_name` is `Oracle Corporation`.
</comment><comment author="dakrone" created="2013-02-27T17:02:24Z" id="14185424">Alright, looks like it was a case of having JAVA_HOME exported to a different directory than what was being run. Doing an unset JAVA_HOME fixes the issue.

Interesting though, this is a strange manifestation of this issue.
</comment><comment author="dakrone" created="2013-02-27T17:02:56Z" id="14185464">Thanks for the debugging help, @drewr and @dadoonet 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested sort not calculating avg/sum</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2701</link><project id="" key="" /><description>In 0.90.0.Beta1, while `score_mode`s of `min` and `max` return the correct values in `sort[]`, `avg` and `sum` don't:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "foo" : {
               "type" : "nested"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "foo" : [
      {
         "val" : 10,
         "tag" : "foo"
      },
      {
         "val" : 20,
         "tag" : "bar"
      }
   ]
}
'

curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "sort" : [
      {
         "foo.val" : {
            "sort_order" : "asc",
            "sort_mode" : "sum"
         }
      }
   ]
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : [
#                   {
#                      "tag" : "foo",
#                      "val" : 10
#                   },
#                   {
#                      "tag" : "bar",
#                      "val" : 20
#                   }
#                ]
#             },
#             "sort" : [
#                10
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "kHkGLjyASUycZveKsg9Low",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : null,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 4
# }


curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "sort" : [
      {
         "foo.val" : {
            "sort_order" : "asc",
            "sort_mode" : "avg"
         }
      }
   ]
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : [
#                   {
#                      "tag" : "foo",
#                      "val" : 10
#                   },
#                   {
#                      "tag" : "bar",
#                      "val" : 20
#                   }
#                ]
#             },
#             "sort" : [
#                10
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "kHkGLjyASUycZveKsg9Low",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : null,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 2
# }
```
</description><key id="11451171">2701</key><summary>Nested sort not calculating avg/sum</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-02-27T11:15:44Z</created><updated>2013-03-01T18:53:28Z</updated><resolved>2013-03-01T18:53:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T12:16:44Z" id="14286596">can we close this with @martijnvg commit?
</comment><comment author="clintongormley" created="2013-03-01T12:50:51Z" id="14287787">No - this issue was opened after that commit.
</comment><comment author="s1monw" created="2013-03-01T12:51:28Z" id="14287806">ok just checking in... thanks @clintongormley 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Exception during startup</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2700</link><project id="" key="" /><description>Elastic Search Version - 0.20.5
Environment - Windows 8
JDK - Oracle JDK 1.7.0_07

Logs and Exception text - 
C:\MyPrograms\elasticsearch-0.20.5\bin&gt;elasticsearch
[2013-02-26 23:40:51,309][INFO ][node                     ] [Gertrude Yorkes] {0.20.5}[8220]: initializing ...
[2013-02-26 23:40:51,314][INFO ][plugins                  ] [Gertrude Yorkes] loaded [], sites []
[2013-02-26 23:40:53,229][INFO ][node                     ] [Gertrude Yorkes] {0.20.5}[8220]: initialized
[2013-02-26 23:40:53,229][INFO ][node                     ] [Gertrude Yorkes] {0.20.5}[8220]: starting ...
[2013-02-26 23:40:53,588][INFO ][transport                ] [Gertrude Yorkes] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.
168.11.175:9300]}
[2013-02-26 23:40:56,987][INFO ][cluster.service          ] [Gertrude Yorkes] new_master [Gertrude Yorkes][BcbZnVxvRyWfGsjEXeueFQ][inet[/192.168.11.17
5:9300]], reason: zen-disco-join (elected_as_master)
[2013-02-26 23:41:18,037][WARN ][cluster.service          ] [Gertrude Yorkes] failed to connect to node [[Gertrude Yorkes][BcbZnVxvRyWfGsjEXeueFQ][ine
t[/192.168.11.175:9300]]]
org.elasticsearch.transport.ConnectTransportException: [Gertrude Yorkes][inet[/192.168.11.175:9300]] connect_timeout[30s]
        at org.elasticsearch.transport.netty.NettyTransport.connectToChannels(NettyTransport.java:665)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:604)
        at org.elasticsearch.transport.netty.NettyTransport.connectToNode(NettyTransport.java:574)
        at org.elasticsearch.transport.TransportService.connectToNode(TransportService.java:127)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.ConnectException: Connection timed out: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:692)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:148)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:104)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:78)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
        at org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:41)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        ... 3 more
[2013-02-26 23:41:18,049][INFO ][discovery                ] [Gertrude Yorkes] elasticsearch/BcbZnVxvRyWfGsjEXeueFQ
[2013-02-26 23:41:18,179][INFO ][gateway                  ] [Gertrude Yorkes] recovered [0] indices into cluster_state
[2013-02-26 23:41:18,287][INFO ][http                     ] [Gertrude Yorkes] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.
168.11.175:9200]}
[2013-02-26 23:41:18,287][INFO ][node                     ] [Gertrude Yorkes] {0.20.5}[8220]: started
</description><key id="11441961">2700</key><summary>Exception during startup</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">punchouty</reporter><labels /><created>2013-02-27T04:45:57Z</created><updated>2013-03-01T12:49:30Z</updated><resolved>2013-03-01T12:49:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T12:49:30Z" id="14287749">please ask those kind of question on the mailling list. 

thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>cluster can Dynamically expansion?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2699</link><project id="" key="" /><description>cluster can Dynamically expansion?
 after cluster installed ,  index.number_of_shards  not allow change. So there is a problem: 
cluster can Dynamically expansion?
such as  5 shards, 6 or more data node (not include replicas)  or chang index.number_of_shards
</description><key id="11440443">2699</key><summary>cluster can Dynamically expansion?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">zhanghaidi326</reporter><labels /><created>2013-02-27T03:26:13Z</created><updated>2013-02-27T06:44:20Z</updated><resolved>2013-02-27T06:08:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-02-27T06:08:07Z" id="14158733">Please use the mailing list to ask your questions. See: http://www.elasticsearch.org/community/
</comment><comment author="zhanghaidi326" created="2013-02-27T06:44:20Z" id="14159584">ok, thanks.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>`_count` should take the `preference` parameter for targeting specific shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2698</link><project id="" key="" /><description>`_search` takes a preference for specifying shards.  `_count` should too.
</description><key id="11431968">2698</key><summary>`_count` should take the `preference` parameter for targeting specific shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">drewr</reporter><labels><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-02-26T22:22:12Z</created><updated>2013-02-26T22:30:58Z</updated><resolved>2013-02-26T22:30:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Field Data in Node Stats now in its own element within indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2697</link><project id="" key="" /><description>Field data refactoring to reduce the size required in memory to represent field data for features such as facets / sorts.

This change also includes a breaking change which moves the field data stats on node stats indices element to their own element and not under the cache element. This is done to reduce the confusion in treating them as caches.
</description><key id="11429256">2697</key><summary>Field Data in Node Stats now in its own element within indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-02-26T21:23:28Z</created><updated>2013-02-26T21:23:38Z</updated><resolved>2013-02-26T21:23:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow index: "no" for _type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2696</link><project id="" key="" /><description>0.90 Beta 1 breaking change from prior versions.  Looks like index: no with _type no longer works.

curl -XPUT http://localhost:9200/sequence -d '
{
  settings: {
    number_of_shards: 1,  
    number_of_replicas: 0,  
    auto_expand_replicas: "0-all" 
  }
}'

curl -XPUT http://localhost:9200/sequence/sequence/_mapping -d '
{
  sequence: {
    _source : { enabled : 0 },
    _all    : { enabled : 0 },
    _type   : { index : "no" },
    enabled : 0
  }
}'

{"error":"MapperParsingException[Wrong value for index [false] for field [_type]]","status":400}

No clue where the false is coming from unless the code is converting no to false somewhere.
</description><key id="11427403">2696</key><summary>Allow index: "no" for _type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-02-26T20:38:18Z</created><updated>2013-02-26T21:06:58Z</updated><resolved>2013-02-26T21:06:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="P-Hill" created="2013-02-26T20:56:49Z" id="14139151">It is the case that the published (somewhere) behavior is that "0", 0, 
"no", "false" are all the same value and mean "false" as you conjectured.

This doesn't change the fact that the docs
http://www.elasticsearch.org/guide/reference/mapping/type-field.html
list the exact fragment you used

"_type"  :  {"index"  :  "no"}

-Paul

On 2/26/2013 12:38 PM, Andy Wick wrote:

&gt; 0.90 Beta 1 breaking change from prior versions. Looks like index: no 
&gt; with _type no longer works.
&gt; 
&gt; [..]
&gt; 
&gt; {"error":"MapperParsingException[Wrong value for index [false] for 
&gt; field [_type]]","status":400}
&gt; 
&gt; No clue where the false is coming from unless the code is converting 
&gt; no to false somewhere.
</comment><comment author="kimchy" created="2013-02-26T21:06:12Z" id="14139658">@P-Hill its a bug, will push a fix in a sec...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Ids filter without a type throws IndexOutOfBounds</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2695</link><project id="" key="" /><description>In 0.90.0.Beta1, an `ids` filter without a `type` throws an IndexOutOfBoundsException. Used to work correctly in 0.20

```
Failure [Failed to parse source [
{
   "query" : {
      "constant_score" : {
         "filter" : {
            "ids" : {
               "values" : [
                  1
               ]
            }
         }
      }
   }
}
]]
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:566)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:481)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:466)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:236)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
    at java.util.ArrayList.rangeCheck(ArrayList.java:604)
    at java.util.ArrayList.get(ArrayList.java:382)
    at org.elasticsearch.index.mapper.Uid.createTypeUids(Uid.java:149)
    at org.elasticsearch.index.query.IdsFilterParser.parse(IdsFilterParser.java:111)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:230)
    at org.elasticsearch.index.query.ConstantScoreQueryParser.parse(ConstantScoreQueryParser.java:68)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:193)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:272)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:250)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:554)
    ... 11 more
```
</description><key id="11419799">2695</key><summary>Ids filter without a type throws IndexOutOfBounds</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-02-26T17:46:39Z</created><updated>2013-03-08T03:21:05Z</updated><resolved>2013-02-27T17:59:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-27T17:45:14Z" id="14187923">I will push a fix and a test for this soon. 
</comment><comment author="dadoonet" created="2013-03-08T03:21:05Z" id="14600758">@s1monw as seen in the mailing list, I tag this one with 0.90.0.Beta2 instead of Beta1
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>"-4" parsed as a date</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2694</link><project id="" key="" /><description>```
$ curl -s -XPOST localhost:9200/test/1 \
       -d '{"foo":[{"bar": "-4"},{"bar": "::1"}]}' | \
       jq .
{
  "status": 400,
  "error": "MapperParsingException[Failed to parse [foo.bar]]; nested: MapperParsingException[failed to parse date field [::1], tried both date format [dateOptionalTime], and timestamp number]; nested: IllegalArgumentException[Invalid format: \"::1\"]; "
}
```
</description><key id="11412429">2694</key><summary>"-4" parsed as a date</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">pcarrier</reporter><labels><label>bug</label><label>enhancement</label><label>v0.90.0.RC1</label></labels><created>2013-02-26T15:06:12Z</created><updated>2013-03-01T21:15:34Z</updated><resolved>2013-03-01T21:15:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T20:39:03Z" id="14310119">this is above my head is '-4' a valid timestamp or not. It could be relative to 01.01.1970 in millis?
</comment><comment author="s1monw" created="2013-03-01T20:44:00Z" id="14310331">hmm maybe I missunderstand are you complaining that -4 shoudl throw an exception as well or that -4 is interpreted as a date and you get a subsequent exception?
</comment><comment author="pcarrier" created="2013-03-01T20:48:32Z" id="14310542">Hu no, I believe no human would ever say that this document looks like it contains any dates, so I'd expect not to run into any exceptions.
</comment><comment author="kimchy" created="2013-03-01T20:49:56Z" id="14310609">Agreed on no humans, sadly, we are using computers :). Will work on fixing it, the _string_ `-4` should not be detected as a date, but as a string.
</comment><comment author="pcarrier" created="2013-03-01T20:54:12Z" id="14310793">:computer: doesn't have to overthrow :family:, but if it cannot find a year, month and day, I'd say it should not be guessing a timestamp.
</comment><comment author="pcarrier" created="2013-03-01T20:56:21Z" id="14310884">If anybody arrives here by searching for the error, here's a workaround (disables date detection altogether on `foo`!):

```
% curl -XPUT localhost:9200/foo/_default_/_mapping -d '{"_default_":{"date_detection":0}}'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>malformed elasticsearch.yml causes unresponsive hang</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2693</link><project id="" key="" /><description>The sample application below will be stuck after "it runs...". ThreadDump/Debugging shows, it is never returning from XContentSettingsloader#serializeValue(...) [118:125].

This is especially annoying, as there is no Exception thrown.

jdk: jdk1.7.0_15
elasticsearch: 0.20.5
### Sample application

src/main/java/Runner.java:

```
    import org.elasticsearch.client.Client;
    import org.elasticsearch.node.Node;
    import static org.elasticsearch.node.NodeBuilder.nodeBuilder;
    public class Runner {    
        public static void main(String[] args) {
            System.out.println("it runs...");
            //http://www.elasticsearch.org/guide/reference/java-api/client.html
            Node node = nodeBuilder().node();
            System.out.println("got node");
            Client client = node.client();
            System.out.println("got client");
            node.close();
        }
    }
```

src/main/resources/elasticsearch.yml:

```
    cluster.name=malformed_yaml__XContentSettingsLoader_will_hang
```
</description><key id="11412047">2693</key><summary>malformed elasticsearch.yml causes unresponsive hang</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">konradkonrad</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-02-26T14:56:46Z</created><updated>2013-02-27T17:58:15Z</updated><resolved>2013-02-27T17:58:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Using Java node client and deleting all indexes cause system hungs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2692</link><project id="" key="" /><description>Using elasticsearch 0.20.5 in Java 1.6 with Ubuntu 12.04 (but also happens with Windows), the system is frozen when you try to delete all indexes when there are no indexes yet into the elasticsearch. This error is a bit strange but I can reproduce it without any problem.

First of all create a new project in Eclipse (in my case but of course in command line or Intellij should do the same). Then add elasticsearch 0.20.5 as dependency and write one simple test:

``` java
Node node = nodeBuilder().local(true).node();
Client client = node.client();

DeleteByQueryRequestBuilder deleteByQueryRequestBuilder = new DeleteByQueryRequestBuilder(client);

deleteByQueryRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
deleteByQueryRequestBuilder.execute().actionGet();

node.close();
```

Then I run the test and the output is:

```
26-feb-2013 10:17:31 org.elasticsearch.node
INFO: [Assassin] {0.20.5}[4352]: initializing ...
26-feb-2013 10:17:31 org.elasticsearch.plugins
INFO: [Assassin] loaded [], sites []
26-feb-2013 10:17:33 org.elasticsearch.node
INFO: [Assassin] {0.20.5}[4352]: initialized
26-feb-2013 10:17:33 org.elasticsearch.node
INFO: [Assassin] {0.20.5}[4352]: starting ...
26-feb-2013 10:17:33 org.elasticsearch.transport
INFO: [Assassin] bound_address {local[1]}, publish_address {local[1]}
26-feb-2013 10:17:33 org.elasticsearch.cluster.service
INFO: [Assassin] new_master [Assassin][1][local[1]]{local=true}, reason: local-disco-initial_connect(master)
26-feb-2013 10:17:33 org.elasticsearch.discovery
INFO: [Assassin] elasticsearch/1
26-feb-2013 10:17:33 org.elasticsearch.http
INFO: [Assassin] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/&lt;ipaddress&gt;:9200]}
26-feb-2013 10:17:33 org.elasticsearch.node
INFO: [Assassin] {0.20.5}[4352]: started
26-feb-2013 10:17:33 org.elasticsearch.gateway
INFO: [Assassin] recovered [0] indices into cluster_state
```

But the test never ends, the actionGet() call does not return any result it is hung there.

Then I abort the test and I do the next modification in test:

``` java
Node node = nodeBuilder().local(true).node();
Client client = node.client();

String json = "{" +
            "\"user\":\"kimchy\"," +
            "\"postDate\":\"2013-01-30\"," +
            "\"message\":\"trying out Elastic Search\"" +
        "}";

IndexResponse response = client.prepareIndex("twitter", "tweet")
                .setSource(json)
                .execute()
                .actionGet();

DeleteByQueryRequestBuilder deleteByQueryRequestBuilder = new DeleteByQueryRequestBuilder(client);

deleteByQueryRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
deleteByQueryRequestBuilder.execute().actionGet();

node.close();
```

And then it works as expected, but now comes the "paranoic" part, I rerun the first test again without preparing an index before deleting:

``` java
Node node = nodeBuilder().local(true).node();
Client client = node.client();

DeleteByQueryRequestBuilder deleteByQueryRequestBuilder = new DeleteByQueryRequestBuilder(client);

deleteByQueryRequestBuilder.setQuery(QueryBuilders.matchAllQuery());
deleteByQueryRequestBuilder.execute().actionGet();

node.close();
```

But now also it works as expected, mostly because there is one index. (data directory is there already created so can work perfectly).

```
INFO: [Cold War] recovered [1] indices into cluster_state
```

So it seems that delete all command does not work correctly when there is no indexes to remove (first time you start the node). Maybe I am doing wrong about how to remove all indexes, but anyway there is still an error on operation which makes all application (test in my case) hungs.

Alex.
</description><key id="11399841">2692</key><summary>Using Java node client and deleting all indexes cause system hungs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lordofthejars</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-02-26T09:26:42Z</created><updated>2013-03-03T18:28:06Z</updated><resolved>2013-03-01T14:17:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T14:03:24Z" id="14290504">hey, thanks for reporting this. This is indeed a bug. It manifests if you do the delete by query on a cluster without any indices. Yet, the fact that this works on the second try is that with delete by query you are only deleting all documents but not the index. So the index exists and the bug is not triggered. If you want to delete the index try `client.admin().indices("index_name").prepareDelete().execute().actionGet();` &lt;-- you can omit the name to del all indices but be careful!
</comment><comment author="s1monw" created="2013-03-03T18:28:06Z" id="14351824">i also applied this to 0.20
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow analyzed fields to be returned in fields array</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2691</link><project id="" key="" /><description>It seems we currently need to [use facets to get the results of an analyzer](http://codeslashslashcomment.com/2012/09/01/search-query-suggestions-using-elasticsearch-via-shingle-filter-and-facets/) on a non-stored index.

For example, on a single record, this will return the results of the analyzer:

curl localhost:9200/test/idx/_search?pretty -d '{"fields":["title"], "facets":{"terms":{"terms": {"field":"terms"}}}, "query": {"match_all":{}}}'

Via:
     {
    "_source": { "enabled": false },
        "properties" : {
            "terms" : { "type" : "string", "analyzer": "camel", store: "no" },
            "folder" : { "type" : "string", store: "yes" },
            "title" : { "type" : "string", store: "yes" }
        }
   }

Analyzer/tokenizers are lossy, that's the point, but it'd be nice to be able to pull the results from records more naturally, with something such as:

curl localhost:9200/test/idx/_search?pretty -d '{"fields":["title","_analyzer.terms"], "query": {"match_all":{}}}'
</description><key id="11393783">2691</key><summary>Allow analyzed fields to be returned in fields array</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels><label>feature</label></labels><created>2013-02-26T04:43:51Z</created><updated>2013-09-30T19:34:39Z</updated><resolved>2013-09-30T19:20:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-03T20:20:56Z" id="14354328">I think this boils down to a term vector API which we should add I agree.
</comment><comment author="Downchuck" created="2013-09-30T19:20:48Z" id="25394915">Seems like this issue can be closed.
</comment><comment author="s1monw" created="2013-09-30T19:34:39Z" id="25396642">thanks @Downchuck does the TermVector stuff fulfil your requirements?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE with Fuzzy Like This on non existing field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2690</link><project id="" key="" /><description>Consider the following curl recreation:

``` sh
curl -XDELETE http://localhost:9200/npefuzzy

curl -XPUT http://localhost:9200/npefuzzy?pretty -d '{
  "settings" : { "index" : { "number_of_shards" : 1, "number_of_replicas" : 0 }}
}'

curl -XPUT http://localhost:9200/npefuzzy/beer/1?pretty -d '{
   "brand":"Grimbergen",
   "colour":"PALE",
   "size":1.3476184461445255,
   "price":7.318647685097387,
   "date":1320090719440
}'

curl -XPOST "localhost:9200/npefuzzy/_refresh?pretty"

# This one is OK
curl -XPOST http://localhost:9200/npefuzzy/beer/_search?pretty -d '
{"query":{
  "flt" : {
    "fields" : [ "brand", "colour" ],
    "like_text" : "heineken is a pale beer",
    "max_query_terms" : 12
  }
}}'

# This one was ok (no error) in 0.20 (Lucene 3) but fails with 0.21 (Lucene 4)
curl -XPOST http://localhost:9200/npefuzzy/beer/_search?pretty -d '
{"query":{
  "flt" : {
    "fields" : [ "brand", "nonexistingfield" ],
    "like_text" : "heineken is a pale beer",
    "max_query_terms" : 12
  }
}}'
```

In 0.20.x (Lucene 3), we get no result:

``` javascript
{
 "took": 5,
 "timed_out": false,
 "_shards": {
  "total": 1,
  "successful": 1,
  "failed": 0
 },
 "hits": {
  "total": 0,
  "max_score": null,
  "hits": []
 }
}
```

With 0.21 (Lucene 4):

``` javascript
{
  "error" : "SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[5d_VkN-ARky1L_LUVoKJEQ][npefuzzy][0]: QueryPhaseExecutionException[[npefuzzy][0]: query[filtered(null)-&gt;cache(_type:beer)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",
  "status" : 500
}
```

Logs are:

```
[2013-02-25 21:10:21,141][DEBUG][action.search.type       ] [Umar] [npefuzzy][0], node[5d_VkN-ARky1L_LUVoKJEQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@313e36eb]
org.elasticsearch.search.query.QueryPhaseExecutionException: [npefuzzy][0]: query[filtered(null)-&gt;cache(_type:beer)],from[0],size[10]: Query Failed [Failed to execute main query]
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:139)
    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:316)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:243)
    at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NullPointerException
    at org.apache.lucene.sandbox.queries.SlowFuzzyTermsEnum$LinearFuzzyTermsEnum.&lt;init&gt;(SlowFuzzyTermsEnum.java:89)
    at org.apache.lucene.sandbox.queries.SlowFuzzyTermsEnum.maxEditDistanceChanged(SlowFuzzyTermsEnum.java:58)
    at org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:211)
    at org.apache.lucene.search.FuzzyTermsEnum.&lt;init&gt;(FuzzyTermsEnum.java:144)
    at org.apache.lucene.sandbox.queries.SlowFuzzyTermsEnum.&lt;init&gt;(SlowFuzzyTermsEnum.java:48)
    at org.apache.lucene.sandbox.queries.FuzzyLikeThisQuery.addTerms(FuzzyLikeThisQuery.java:209)
    at org.apache.lucene.sandbox.queries.FuzzyLikeThisQuery.rewrite(FuzzyLikeThisQuery.java:262)
    at org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:93)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:616)
    at org.elasticsearch.search.internal.ContextIndexSearcher.rewrite(ContextIndexSearcher.java:112)
    at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:663)
    at org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:126)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)
    at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:155)
    at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)
    at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:135)
    ... 9 more
```

Full curl recreation is: https://gist.github.com/dadoonet/5031671
</description><key id="11378653">2690</key><summary>NPE with Fuzzy Like This on non existing field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-02-25T20:20:20Z</created><updated>2013-03-11T09:31:49Z</updated><resolved>2013-03-11T09:31:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T12:44:37Z" id="14287572">I created [LUCENE-4809](https://issues.apache.org/jira/browse/LUCENE-4809) for this. Its a lucene bug essentially and it should be fixed with the 4.2 upgrade
</comment><comment author="s1monw" created="2013-03-01T12:56:02Z" id="14287946">I just committed the fixes so this should be fixed once 4.2 is out and integrated
</comment><comment author="s1monw" created="2013-03-11T09:31:49Z" id="14703617">fixed by commit 11bf7a8b1a7ec88e4d38ee69c8b5c577001fb68d
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Points not intersecting with polygons</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2689</link><project id="" key="" /><description>I'm trying to answer a question on stackoverflow about indexing multi-polygons then querying to see if a point falls within a polygon, and I'm not having much success.  May just be my ignorance about how geo-shapes work, but would appreciate somebody taking a look.

Map a geo_shape field:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "poly" : {
               "type" : "geo_shape"
            }
         }
      }
   }
}
'
```

Add a multi-polygong:

```
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "poly" : {
      "coordinates" : [
         [
            [
               [
                  102,
                  2
               ],
               [
                  103,
                  2
               ],
               [
                  103,
                  3
               ],
               [
                  102,
                  3
               ],
               [
                  102,
                  2
               ]
            ]
         ]
      ],
      "type" : "multipolygon"
   }
}
'
```

Search for a point within the polygon: (note, `within` and `intersects` both fail):

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "filtered" : {
         "filter" : {
            "geo_shape" : {
               "poly" : {
                  "relation" : "intersects",
                  "shape" : {
                     "coordinates" : [
                        102.5,
                        3.5
                     ],
                     "type" : "point"
                  }
               }
            }
         },
         "query" : {
            "match_all" : {}
         }
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [],
#       "max_score" : null,
#       "total" : 0
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```

Using an `envelope` works:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "query" : {
      "filtered" : {
         "filter" : {
            "geo_shape" : {
               "poly" : {
                  "relation" : "intersects",
                  "shape" : {
                     "coordinates" : [
                        [
                           102,
                           3.5
                        ],
                        [
                           102.5,
                           3
                        ]
                     ],
                     "type" : "envelope"
                  }
               }
            }
         },
         "query" : {
            "match_all" : {}
         }
      }
   }
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "poly" : {
#                   "coordinates" : [
#                      [
#                         [
#                            [
#                               102,
#                               2
#                            ],
#                            [
#                               103,
#                               2
#                            ],
#                            [
#                               103,
#                               3
#                            ],
#                            [
#                               102,
#                               3
#                            ],
#                            [
#                               102,
#                               2
#                            ]
#                         ]
#                      ]
#                   ],
#                   "type" : "multipolygon"
#                }
#             },
#             "_score" : 1,
#             "_index" : "test",
#             "_id" : "yjhjLshzRzymtMs6vacA6Q",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 8
# }
```
</description><key id="11376762">2689</key><summary>Points not intersecting with polygons</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-02-25T19:43:33Z</created><updated>2013-03-14T15:14:08Z</updated><resolved>2013-03-14T15:14:08Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-03-14T15:14:08Z" id="14907823">Talked to Clint, not an issue due to query being outside of the rectangle (102.5,3.5 instead of 102.5,2.5)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>The API isn't a REST API or even a Restful API and that limits its usage</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2688</link><project id="" key="" /><description>http://www.elasticsearch.org/guide/reference/api/ calls out "This section describes the REST APIs elasticsearch provides (mainly) using JSON"

This might be like throwing a stone at a bee-hive but here goes:

One central theme in the REST/Restful on HTTP is the correct use of verbs: PUT and POSTS are for server side effects and GET is for retrieval/reads. Amongst other(perhaps) things: Using POSTS for getting search results breaks this. And most of the (high-end) features are sadly only available under this premise.

One side effect of ES not respecting this separation is that it becomes very difficult to put a proxy that exposes the search part of the ES but protects the ES cluster from writes/updates. 

Here is a comparison (even if limited): No one would ever dream of an UPDATE statement in SQL to suddenly do what essentially a SELECT does. 

I am aware of comments like "ES is like a DB .. you would never expose a DB so why expose ES to the outside world". Instead of poking holes into this kind of faulty reasoning let me present a counter example. If some one would write a DB that provides an HTTP interface that respects the HTTP VERBS then I would not have any qualms in exposing it to the outside world. Wait a second there are data stores that actually do that today. 

I would encourage the ES team to seriously consider supporting a better HTTP API that allows reuse of things that make the HTTP ecosystem so great like proxies, http-caches etc. 

And yes you can do GETs with entity bodies in case query-string based API isn't expressive enough. 
</description><key id="11364541">2688</key><summary>The API isn't a REST API or even a Restful API and that limits its usage</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ppurang</reporter><labels /><created>2013-02-25T15:48:53Z</created><updated>2013-02-25T17:27:20Z</updated><resolved>2013-02-25T16:09:03Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-02-25T16:09:03Z" id="14050729">GET is the default method for /_search.  POST is only supported because some clients don't allow GET with body, eg Javascript ajax calls.

An alternative is to use GET /_search but to pass the JSON "body" as the _source parameter in the URL -- another workaround introduced specifically to handle this limitation in some HTTP libraries for many popular programming languages.
</comment><comment author="ppurang" created="2013-02-25T16:30:40Z" id="14052207">That sounds excellent in that case Why not also support GET with entity bodies for _search?
</comment><comment author="spinscale" created="2013-02-25T17:27:20Z" id="14056996">See http://www.elasticsearch.org/guide/reference/api/search/request-body.html at the last paragraph:

&lt;i&gt;Both HTTP GET and HTTP POST can be used to execute search with body. Since not all clients support GET with body, POST is allowed as well.&lt;/i&gt;

Or did you mean something different?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Terms/Ids filter: Support empty list of values, resulting in no match for it</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2687</link><project id="" key="" /><description /><key id="11355214">2687</key><summary>Terms/Ids filter: Support empty list of values, resulting in no match for it</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-02-25T11:26:21Z</created><updated>2013-02-25T11:26:53Z</updated><resolved>2013-02-25T11:26:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>IdsQueryBuilder with empty ids list causes error</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2686</link><project id="" key="" /><description>Maybe this could create a query that match zero documents?

Exception :
Caused by: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [query], total failure; shardFailures {[1][telenet][0]: SearchParseException[[telenet][0]: from[0],size[84]: Parse Failure [Failed to parse source [{"from":0,"size":84,"query":{"f
iltered":{"query":{"bool":{"must":{"ids":{"type":"media","values":[]}}}},"filter":{"fquery":{"query":{"match_all":{}},"_cache":true}}}},"sort":[{"org.taktik.mediafield.creationdate":{"order":"desc"}}]}]]]; nested: QueryParsingException[[telenet] [ids] query, no ids values prov
ided]; }{[1][telenet][1]: SearchParseException[[telenet][1]: from[0],size[84]: Parse Failure [Failed to parse source [{"from":0,"size":84,"query":{"filtered":{"query":{"bool":{"must":{"ids":{"type":"media","values":[]}}}},"filter":{"fquery":{"query":{"match_all":{}},"_cache":t
rue}}}},"sort":[{"org.taktik.mediafield.creationdate":{"order":"desc"}}]}]]]; nested: QueryParsingException[[telenet] [ids] query, no ids values provided]; }{[1][telenet][3]: SearchParseException[[telenet][3]: from[0],size[84]: Parse Failure [Failed to parse source [{"from":0,
"size":84,"query":{"filtered":{"query":{"bool":{"must":{"ids":{"type":"media","values":[]}}}},"filter":{"fquery":{"query":{"match_all":{}},"_cache":true}}}},"sort":[{"org.taktik.mediafield.creationdate":{"order":"desc"}}]}]]]; nested: QueryParsingException[[telenet] [ids] quer
y, no ids values provided]; }{[1][telenet][4]: SearchParseException[[telenet][4]: from[0],size[84]: Parse Failure [Failed to parse source [{"from":0,"size":84,"query":{"filtered":{"query":{"bool":{"must":{"ids":{"type":"media","values":[]}}}},"filter":{"fquery":{"query":{"matc
h_all":{}},"_cache":true}}}},"sort":[{"org.taktik.mediafield.creationdate":{"order":"desc"}}]}]]]; nested: QueryParsingException[[telenet] [ids] query, no ids values provided]; }
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:260)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:213)
        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:144)
        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
        ... 3 more
</description><key id="11354323">2686</key><summary>IdsQueryBuilder with empty ids list causes error</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">abaudoux</reporter><labels /><created>2013-02-25T10:52:55Z</created><updated>2013-02-25T14:30:47Z</updated><resolved>2013-02-25T11:26:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="abaudoux" created="2013-02-25T11:35:26Z" id="14036940">Thanks for the quick fix, I have a small question, is there the opposite of QueryBuilders.matchAllQuery() clientside? like a QueryBuilders.matchNoneQuery() ?
</comment><comment author="kimchy" created="2013-02-25T13:34:01Z" id="14041175">@abaudoux there isn't, trying to think when would you need one?
</comment><comment author="abaudoux" created="2013-02-25T14:30:47Z" id="14044374">Well we build a Query as a filter for permissions, and in some case the user has acces to nothing, so for now we are forced to code something ugly like id=-1 or ids = []  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Native client requests with types passed as "_all" yield no results</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2685</link><project id="" key="" /><description>I've noticed that when search request has types set to `"_all"`, even Match All query
fails.

To make sure, I've added a test to `org.elasticsearch.test.integration.search.simple.SimpleSearchTests`, which is a replicated passing test where
mapping type is added as "_all":

``` java
  @Test
  public void simpleIdTestsAcrossAllMappingTypes() {
    client.admin().indices().prepareDelete().execute().actionGet();
    client.admin().indices().prepareCreate("test").setSettings(ImmutableSettings.settingsBuilder().put("index.number_of_shards", 1)).execute().actionGet();

    client.prepareIndex("test", "type", "XXX1").setSource("field", "value").setRefresh(true).execute().actionGet();
    // id is not indexed, but lets see that we automatically convert to
    SearchResponse searchResponse = client.prepareSearch().setTypes("_all").setQuery(QueryBuilders.termQuery("_id", "XXX1")).execute().actionGet();
    assertThat(searchResponse.getHits().totalHits(), equalTo(1l));

    searchResponse = client.prepareSearch().setTypes("_all").
        setQuery(QueryBuilders.queryString("_id:XXX1")).execute().actionGet();
    assertThat(searchResponse.getHits().totalHits(), equalTo(1l));

    // id is not index, but we can automatically support prefix as well
    searchResponse = client.prepareSearch().setTypes("_all").
        setQuery(QueryBuilders.prefixQuery("_id", "XXX")).execute().actionGet();
    assertThat(searchResponse.getHits().totalHits(), equalTo(1l));

    searchResponse = client.prepareSearch().setTypes("_all").
        setQuery(QueryBuilders.queryString("_id:XXX*").lowercaseExpandedTerms(false)).execute().actionGet();
    assertThat(searchResponse.getHits().totalHits(), equalTo(1l));
  }
```

and it fails (totalHits returns `0`). When `SearchRequest#setTypes` is not used, it passes
just fine.

It may be my misunderstanding of how to query against all types and indices via the native
client, but documentation and mailing list archives yield nothing on the subject.

Should `"_all"` be recognized in this case or should types be not explicitly set at all
on the search request?
</description><key id="11352743">2685</key><summary>Native client requests with types passed as "_all" yield no results</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">michaelklishin</reporter><labels /><created>2013-02-25T10:00:31Z</created><updated>2014-08-08T13:14:49Z</updated><resolved>2014-08-08T10:26:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaelklishin" created="2013-02-25T10:10:35Z" id="14033737">An update: JavaDocs for search request setters and some experimentation suggest that not specifying types or specifying indices as an empty array should be used instead of `"_all"`. I think it may be a good idea to
support `"_all"` for the sake of consistency. And the guides could use an update that clarifies this behavior.

I will leave the issue open if you don't mind.
</comment><comment author="clintongormley" created="2014-08-08T10:26:24Z" id="51585634">Hi @michaelklishin 

Working through old issues I found this.  So, `_all` is only used as a placeholder when we require a positional parameter, eg `/index/type/_search` - you can't specify a type without having a value for index. But you can specify an index without a type.  

Given that nobody else has commented on this issue in a year and a half, I'm going to wager that it is not a big problem and close it.
</comment><comment author="michaelklishin" created="2014-08-08T13:14:49Z" id="51598265">@clintongormley yes, it's not a big deal.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updated the copyright in NOTICE.txt and README.textile</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2684</link><project id="" key="" /><description /><key id="11343327">2684</key><summary>Updated the copyright in NOTICE.txt and README.textile</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">coreymason</reporter><labels /><created>2013-02-25T00:13:27Z</created><updated>2014-07-16T21:53:55Z</updated><resolved>2013-03-08T20:00:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix bug when searching concrete and routing aliased indices fix #2682</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2683</link><project id="" key="" /><description /><key id="11342054">2683</key><summary>Fix bug when searching concrete and routing aliased indices fix #2682</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">Paikan</reporter><labels /><created>2013-02-24T23:26:45Z</created><updated>2014-06-24T13:25:36Z</updated><resolved>2013-03-03T13:46:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-03-03T14:33:16Z" id="14347692">@s1monw as it is a bug fix shouldn't is also be pushed to 0.20 branch?
</comment><comment author="s1monw" created="2013-03-03T18:22:25Z" id="14351739">@Paikan pushed to 0.20
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bug when searching concrete and routing aliased indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2682</link><project id="" key="" /><description>To reproduce:
- let's create 2 concrete indices `foo` and `foo_2`

``` bash
$ curl -XPOST 'http://localhost:9200/foo'
$ curl -XPOST 'http://localhost:9200/foo_2'
```
- let's create a routing alias `foo_1` for index `foo` and routing value 1

``` bash
$ curl -XPOST 'http://localhost:9200/_aliases' -d '
{
    "actions" : [
        {
            "add" : {
                 "index" : "foo",
                 "alias" : "foo_1",
                 "routing" : "1"
            }
        }
    ]
}'
```
- let's index 2 docs one in `foo_1` and the other in `foo_2`

``` bash
$ curl -XPOST 'http://localhost:9200/foo_1/type/1' -d '{"foo1":"bar1"}'
$ curl -XPOST 'http://localhost:9200/foo_2/type/2' -d '{"foo2":"bar2"}'
```
- Now this search gives 1 result instead of the 2 I expected

``` bash
$ curl -XGET 'http://localhost:9200/foo_*/_search' 
```
</description><key id="11342041">2682</key><summary>Bug when searching concrete and routing aliased indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">Paikan</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-02-24T23:25:16Z</created><updated>2013-08-18T07:19:21Z</updated><resolved>2013-03-03T14:18:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Paikan" created="2013-02-24T23:29:24Z" id="14018833">I have submitted PR #2683 that should fix it
</comment><comment author="Paikan" created="2013-03-03T14:18:04Z" id="14347484">Ok closing this one, thanks @s1monw 
</comment><comment author="s1monw" created="2013-03-03T16:33:02Z" id="14349593">ah cool thanks for closing this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fix exception typo</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2681</link><project id="" key="" /><description /><key id="11330967">2681</key><summary>Fix exception typo</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-02-24T06:49:32Z</created><updated>2014-07-16T21:53:56Z</updated><resolved>2013-02-24T08:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Can't index with geo location?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2680</link><project id="" key="" /><description>```
   { 
      title: 'abc',
      body: 'hello'
      location: { lat: -33.8756, lon: 151.204 },
     }
```

The above is the JSON that I want to index.  However, when I index it, the error is:

```
{"error":"MapperParsingException[Failed to parse [location]]; nested: ElasticSearchIllegalArgumentException[unknown property [lat]]; ","status":400}
```

If I remove the "location" field, everything works.
How do I index geo? I read the tutorial and I'm still confused how it works.  It should work like this, right...?
</description><key id="11328776">2680</key><summary>Can't index with geo location?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">felixchan</reporter><labels /><created>2013-02-24T02:32:13Z</created><updated>2013-02-25T10:07:37Z</updated><resolved>2013-02-25T10:07:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-02-25T10:07:37Z" id="14033619">Hi Felix

The issues list is not the place to ask questions.  Rather ask in the forum.

Also, please provide a complete replication of what you are doing, rather than just a snippet.

ta

clint
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide server side REINDEX command</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2679</link><project id="" key="" /><description>it would have performance advantage over client side reindex command and it will increase availability of reindexing because not every client supports it.
</description><key id="11323895">2679</key><summary>Provide server side REINDEX command</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hsn10</reporter><labels /><created>2013-02-23T19:09:26Z</created><updated>2013-08-28T10:14:20Z</updated><resolved>2013-08-28T10:14:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2013-08-28T10:14:20Z" id="23403936">Closing as duplicate of #514 , feel free to reopen if you meant something different.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NPE when indexing a polygon geo-shape</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2678</link><project id="" key="" /><description>```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "polygon" : {
               "tree" : "quadtree",
               "type" : "geo_shape"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "polygon" : {
      "coordinates" : [
         [
            -122.83,
            48.57
         ],
         [
            -122.77,
            48.56
         ],
         [
            -122.79,
            48.53
         ],
         [
            -122.83,
            48.57
         ]
      ],
      "type" : "polygon"
   }
}
'


org.elasticsearch.index.mapper.MapperParsingException: Failed to parse [polygon]
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:320)
    at org.elasticsearch.index.mapper.object.ObjectMapper.serializeObject(ObjectMapper.java:507)
    at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:449)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:486)
    at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:430)
    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:297)
    at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:211)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)
    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.common.geo.GeoJSONShapeParser.toCoordinates(GeoJSONShapeParser.java:192)
    at org.elasticsearch.common.geo.GeoJSONShapeParser.buildPolygon(GeoJSONShapeParser.java:173)
    at org.elasticsearch.common.geo.GeoJSONShapeParser.buildShape(GeoJSONShapeParser.java:146)
    at org.elasticsearch.common.geo.GeoJSONShapeParser.parse(GeoJSONShapeParser.java:101)
    at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parseCreateField(GeoShapeFieldMapper.java:133)
    at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:307)
    ... 11 more
```
</description><key id="11318959">2678</key><summary>NPE when indexing a polygon geo-shape</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-02-23T11:44:40Z</created><updated>2013-02-25T19:02:07Z</updated><resolved>2013-02-25T19:02:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="uboness" created="2013-02-25T17:10:58Z" id="14055252">The format for the polygon fields is malformed. It should be:

```
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "polygon" : {
      "coordinates" : [[
         [
            -122.83,
            48.57
         ],
         [
            -122.77,
            48.56
         ],
         [
            -122.79,
            48.53
         ],
         [
            -122.83,
            48.57
         ]
      ]],
      "type" : "polygon"
   }
}
'
```

note, that `polygon` accepts and array of arrays of points. The first array is the closed polygon shape, where the arrays that follow (if any) represent "holes" in the polygon. This is not documented at all currently, I'll add an example to the guide.

see (GeoJson spec): http://www.geojson.org/geojson-spec.html#id4 
</comment><comment author="clintongormley" created="2013-02-25T19:02:07Z" id="14066236">Excellent thanks @uboness I wasn't sure where to find the specs for these shapes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Maven Surefire Issue</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2677</link><project id="" key="" /><description>My integration test starts within the setUp() method a new elasticsearch "master" + "data" node and in the test method using client node i want to perform some operations on that master node. Everything runs without any problem if i run the test case within the Eclipse but as soon as i try to package my application and the tests are being run, i get following exception. The problem is that our CI server can not run these tests, too.  Here is how i configure the server node:
        map.put("cluster.name", clusterName);
        map.put("node.name", "search-engine-test");
        map.put("node.master", true);
        map.put("node.data", true);
        map.put("node.client", false);
        map.put("node.gateway.type", "none");
        map.put("node.http.enabled", true);
        map.put("path.data", "target/elasticsearch");
        map.put("index.number_of_shards", 1);
        map.put("index.number_of_replicas", 0);
        map.put("index.store.type", "memory");

and here is how the client node configured:
        map.put("cluster.name", clusterName);
        map.put("node.name", DEFAULT_NODE_NAME);
        map.put("node.master", false);
        map.put("node.data", false);
        map.put("node.client", true);
        map.put("discovery.zen.ping.multicast.enabled", false);
        map.put("discovery.zen.ping.unicast.hosts", "127.0.0.1");

i appreciate your help !

Tests run: 49, Failures: 0, Errors: 24, Skipped: 0, Time elapsed: 6.648 sec &lt;&lt;&lt; FAILURE!
all(de.guj.searchengine.domain.search.ElasticSearchQueryIntegrationTest)  Time elapsed: 0.126 sec  &lt;&lt;&lt; ERROR!
org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];[SERVICE_UNAVAILABLE/2/no master];
        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:138)
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.checkGlobalBlock(TransportIndicesStatusAction.java:103)
        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.checkGlobalBlock(TransportIndicesStatusAction.java:59)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.&lt;init&gt;(TransportBroadcastOperationAction.java:136)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:73)
        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:44)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:47)
        at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:64)
        at org.elasticsearch.client.support.AbstractIndicesAdminClient.status(AbstractIndicesAdminClient.java:333)
</description><key id="11308226">2677</key><summary>Maven Surefire Issue</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">bagdemir</reporter><labels /><created>2013-02-22T22:13:20Z</created><updated>2013-06-28T11:32:33Z</updated><resolved>2013-06-28T11:32:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-03-08T18:06:55Z" id="14634252">What happens if you add `map.put("network.host", "127.0.0.1");` to the server configuration block?
</comment><comment author="spinscale" created="2013-05-24T12:46:10Z" id="18402616">hey,

do you still have this issue? Can you gist the whole test somewhere so we can take a closer look at it?

Also please close the issue, if it is no longer valid. Thanks!
</comment><comment author="spinscale" created="2013-06-28T11:32:33Z" id="20183326">Closing. Happy to reopen with more information provided.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Get settings on empty node fails with ArrayIndexOutOfBoundsException[0]</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2676</link><project id="" key="" /><description>Running curl http://localhost:9200/_settings returns with {"error":"ArrayIndexOutOfBoundsException[0]","status":500} when there are no indices. 
</description><key id="11307659">2676</key><summary>Get settings on empty node fails with ArrayIndexOutOfBoundsException[0]</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mikecx</reporter><labels /><created>2013-02-22T21:59:00Z</created><updated>2013-02-22T22:08:46Z</updated><resolved>2013-02-22T22:08:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index dynamic settings might not be allowed to be applied on master with no data node</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2675</link><project id="" key="" /><description>Each component internally in ES registers which settings are allowed to be changed. The problem is that shard level components registering dynamic settings will not be registered in case of master with no data nodes. Note, index level components will work well, because hte index is created on the master first to check if it can be created, so it only applies to shard level settings.
</description><key id="11298480">2675</key><summary>Index dynamic settings might not be allowed to be applied on master with no data node</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.RC1</label></labels><created>2013-02-22T18:37:05Z</created><updated>2013-02-26T15:23:45Z</updated><resolved>2013-02-26T15:23:45Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Query DSL: Terms filter to allow for terms lookup from another document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2674</link><project id="" key="" /><description>The terms filter requires providing all the terms as part of the filter itself. Allow to automatically extract them from an external document.

Here is an example:

```
# index the information for user with id 2, specifically, its friends
curl -XPUT localhost:9200/users/user/2 -d '{
   "friends" : ["1", "3"]
}'

# index a tweet, from user with id 2
curl -XPUT localhost:9200/tweets/tweet/1 -d '{
   "user" : "2"
}'

# search on all the tweets that match the friends of user 2
curl -XGET localhost:9200/tweets/_search -d '{
  "query" : {
    "filtered" : {
        "filter" : {
            "terms" : {
                "user" : {
                    "index" : "users",
                    "type" : "user",
                    "id" : "2",
                    "path" : "friends"
                },
                "_cache_key" : "user_2_friends"
            }
        }
    }
  }
}'
```

The above is higly optimized, both in a sense that the list of friends will not be fetched if the filter is already cached in the filter cache, and with internal LRU cache for fetching external values for the terms filter. Also, the entry in teh filter cache will not hold _all_ the terms reducing the memory required for it.

`_cache_key` is recommedned to be set, so its simple to clear the cache associated with it using the clear cache API. For example:

```
curl -XPOST 'localhost:9200/tweets/_cache/clear?filter_keys=user_2_friends'
```

The structure of the external terms document can also include array of inner objects, for example:

```
curl -XPUT localhost:9200/users/user/2 -d '{
   "friends" : [
     {
       "id" : "1"
     },
     {
       "id" : "2"
     }
   ]
}'
```

In which case, the lookup path will be `friends.id`.

There is an additional cache involved, which caches the lookup of the lookup document to the actual terms. It is by default set to `10mb` LRU size, but can be explicitly set using `indices.cache.filter.terms.size`.

Also, consider using an index with a single shard and fully replicated across all nodes if the "reference" terms data is not large. The lookup terms filter will prefer to execute the get request on a local node if possible, reducing the need for networking.
</description><key id="11284603">2674</key><summary>Query DSL: Terms filter to allow for terms lookup from another document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-02-22T12:55:26Z</created><updated>2016-10-05T06:24:42Z</updated><resolved>2013-02-22T13:04:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2013-02-22T17:16:38Z" id="13956372">This nearly finishes/fixes the feature issue #2671
</comment><comment author="telvis07" created="2013-04-26T18:46:02Z" id="17093168">In this example, shouldn't tweet/1 have user "1" or "3"? This example doesn't return hits for me but it does when I change it to 1 or 3. I have a gist here: https://gist.github.com/telvis07/5469479
</comment><comment author="loris" created="2013-05-17T19:50:58Z" id="18082512">@kimchy Quick question about using this feature vs using the IDs filter
I have a use case where I would need to fetch IDs from an external datastore (mysql and redis) and make some get (with multi get) or search (with the IDs filter) in ElasticSearch against the list of documents matching the IDs.
The amount of IDs per search can vary from some dozens to a few thousands.
That said, will this perform poorly? Should I use the lookup term feature instead (would also mean that I would need to index the IDs and maintain sync with the primary datastores) ?

I will probably implement both for benchmark purpose but would love to hear from your feedback!
</comment><comment author="clintongormley" created="2013-05-18T11:34:05Z" id="18099349">If you index the terms into ES, and use the "external terms filter", you
will get significantly better performance, because:
1. you greatly reduce the amount of network traffic
2. you greatly reduce the amount of query parsing
3. your filter will be cached after the first use, and thus very fast on
   subsequent uses

clint

On 17 May 2013 21:51, Loris Guignard notifications@github.com wrote:

&gt; @kimchy https://github.com/kimchy Quick question about using this
&gt; feature vs using the IDs filter
&gt; I have a use case where I would need to fetch IDs from an external
&gt; datastore (mysql and redis) and make some get (with multi get) or search
&gt; (with the IDs filter) in ElasticSearch against the list of documents
&gt; matching the IDs.
&gt; The amount of IDs per search can vary from some dozens to a few thousands.
&gt; That said, will this perform poorly? Should I use the lookup term feature
&gt; instead (would also mean that I would need to index the IDs and maintain
&gt; sync with the primary datastores) ?
&gt; 
&gt; I will probably implement both for benchmark purpose but would love to
&gt; hear from your feedback!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2674#issuecomment-18082512
&gt; .
</comment><comment author="junjun-zhang" created="2014-01-12T23:07:31Z" id="32137439">This is a very useful feature. Just curious whether it's possible to generalize this to support JOIN. In the case of join, the list of lookup terms is not fetched from another document, but rather it's the result of a query from a related document. Replicating this related document in all nodes can also eliminate networking.

I have a use case where I need to embed a particular document under another related document as nested doc. As it is a many-to-many relationship, this embedding introduced a huge number of redundant docs. If JOIN is supported, I will not need to embed the actual doc, include a field keeping the related doc IDs will be sufficient.

It seems Solr supports join in a similar fashion: http://wiki.apache.org/solr/Join. It is somewhat limited, but if used properly, it can be very helpful.
</comment><comment author="mattweber" created="2014-01-13T16:17:31Z" id="32183844">@junjun-zhang see #3278.  Hopefully @martijnvg and @kimchy will get a chance to have a look at this soon.
</comment><comment author="brupm" created="2015-04-17T22:09:46Z" id="94086216">In this example: 

``` son
curl -XGET localhost:9200/tweets/_search -d '{
  "query" : {
    "filtered" : {
        "filter" : {
            "terms" : {
                "user" : {
                    "index" : "users",
                    "type" : "user",
                    "id" : "2",
                    "path" : "friends"
                }
            }
        }
    }
  }
}
```

Say I wanted to pass an array of ids instead of a single id as it's shown `"id" : "2"` 

Reason is I have several documents I want to combine.
</comment><comment author="clintongormley" created="2015-04-25T14:41:31Z" id="96216654">@brupm then just use several terms lookup filters, wrapped in a `bool.should` filter.  Doing this lookup is not cheap, so I would prefer not to add syntax that makes it look cheap to the naive user.
</comment><comment author="brupm" created="2015-04-25T18:09:16Z" id="96250712">Is there an upper limit on who many terms filters I can have wrapped in a bool.should?  @clintongormley - thank you!
</comment><comment author="clintongormley" created="2015-04-26T17:52:05Z" id="96413672">Probably 1024, which should be more than enough...
</comment><comment author="banupriya20" created="2016-10-05T06:24:42Z" id="251592208">please provide a suggestion on this Index 1 and index 2 had common entity (Ex. Empl no.)
how to Create join query to search on index1 and get the document from index2 based on the common entity(emp no)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Aliases API - listing all Indexes for an alias given.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2673</link><project id="" key="" /><description>Is there a way to find out all indexes for a given alias name using Java API ? 
</description><key id="11282448">2673</key><summary>Aliases API - listing all Indexes for an alias given.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bagdemir</reporter><labels /><created>2013-02-22T11:34:09Z</created><updated>2016-06-21T08:58:27Z</updated><resolved>2013-02-23T04:09:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="brusic" created="2013-02-23T00:27:49Z" id="13981265">String aliasName = ...

client.admin().cluster().state(new ClusterStateRequest()).actionGet().getState().getMetaData().aliases().get(aliasName);

Questions such as this one are better suited for the mailing list.
</comment><comment author="kingaj" created="2015-09-15T10:34:58Z" id="140349025">i can't get close indexes how to get ?
</comment><comment author="chaitu0292" created="2016-06-21T08:57:59Z" id="227380961">(The method aliases() is undefined for the type MetaData) am getting this error..wt to do
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>List all aliases for a specific index ?</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2672</link><project id="" key="" /><description>Is there way to retrieve all aliases of an index using Java API ? 
</description><key id="11279606">2672</key><summary>List all aliases for a specific index ?</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bagdemir</reporter><labels /><created>2013-02-22T09:51:26Z</created><updated>2013-02-22T11:32:46Z</updated><resolved>2013-02-22T11:32:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Support autocomplete indexes on select fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2671</link><project id="" key="" /><description>The [LinkedIn Cleo](http://sna-projects.com/cleo/design.php) project provides type-ahead indexes on set fields, a very handy feature for certain circumstances. This has come up for me in a project where one of the fields is essentially a dynamic enumeration of about five thousand entries; names of people and companies and the like. The Cleo project is available under Apache 2.0, should ES authors see fit to borrow any of its components to fulfill this feature request. ES already has its own Bloom implementation.

An auto-complete search request would allow me to say field X must equal "Z" and field "Y" is the field which I'm running the auto-complete request on.

Example record: { assigned_rep: "Bob", company: "Tractor mowing", company_tokens: ["tractor","mowing","lawn"] }.

My search index would be on assigned_rep and company_tokens. I would pass assigned_rep = Bob and autocomplete_search on company_tokens for "Law", which would match on "lawn" and return the full record (and any other matches).
</description><key id="11261069">2671</key><summary>Support autocomplete indexes on select fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Downchuck</reporter><labels /><created>2013-02-21T20:48:42Z</created><updated>2013-03-08T00:16:39Z</updated><resolved>2013-03-08T00:16:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Downchuck" created="2013-02-22T17:32:03Z" id="13957171">This has been brought up before on mailing lists. The best post I found was [Cleo or ElasticSearch with NGram](http://grokbase.com/t/gg/elasticsearch/12bggk2fxg/autocomplete-cleo-or-elasticsearch-with-ngram). With the new patch from issue #2674, this feature is very close to supportable. It's likely that a few borrowings from the Cleo codebase would finish the feature.
</comment><comment author="Downchuck" created="2013-02-25T18:10:18Z" id="14060673">Also see, [LinkedIn Blog post about Cleo typeahead search](http://engineering.linkedin.com/open-source/cleo-open-source-technology-behind-linkedins-typeahead-search)
</comment><comment author="Downchuck" created="2013-02-28T21:42:15Z" id="14259389">Some of the API for issue #2709 may be relevant.
</comment><comment author="Downchuck" created="2013-03-06T02:10:36Z" id="14478226">It seems like performance issues partly addressed by issue #988 improvements in Lucene. Simply, bool.must.wildcard on a lowercase filtered + camel case analyzed field works quite well over a medium sized set of documents.
</comment><comment author="kimchy" created="2013-03-06T02:21:13Z" id="14478564">@Downchuck another option is to simply use edge ngrams analysis on the relevant field you wish to do autocomplete on.
</comment><comment author="Downchuck" created="2013-03-08T00:16:39Z" id="14595309">ES has this thoroughly handled, closing the bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding java-6-openjdk-i386 check into init.d</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2670</link><project id="" key="" /><description>There is check for /usr/lib/jvm/java-6-openjdk-amd64, but no for 32-bit systems (/usr/lib/jvm/java-6-openjdk-i386).
</description><key id="11241820">2670</key><summary>Adding java-6-openjdk-i386 check into init.d</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nilya</reporter><labels /><created>2013-02-21T14:15:27Z</created><updated>2014-07-16T21:53:56Z</updated><resolved>2013-02-21T14:27:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Shard Reroute API temporarily breaks Cluster/nodes/stats API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2669</link><project id="" key="" /><description>If you manually move a shard using the Reroute API, it appears that the Cluster node stats API breaks for the duration of the shard reassignment (500 Internal Server error, message at bottom of the gist).  It begins working again once the shard has been fully relocated and intialized.

Here is a recreation.  The index in question is "test".  The cluster has three data nodes (S1, S2, S3) and one non-data client node (C1)

Edit:  this is with ES Version 0.20.2

``` bash
curl -XPOST 'localhost:9200/_cluster/reroute' -d '{
  "commands":[
    {
      "move":{
        "index":"test",
        "shard":2,
        "from_node":"kH152vsLTL-y20mcLFs9GQ",
        "to_node":"J47gdOIwQMq2GTmzzmzJBA"
      }
    }
  ]
}' 

{
  "ok":true,
  "state":{
    "master_node":"J47gdOIwQMq2GTmzzmzJBA",
    "blocks":{

    },
    "nodes":{
      "ez-rzcnfSESrVYP6zTWksA":{
        "name":"S1",
        "transport_address":"inet[/144.76.3.102:9300]",
        "attributes":{

        }
      },
      "-VNWEiiaSG2IRlwM6kzcxg":{
        "name":"C1",
        "transport_address":"inet[/144.76.8.228:9300]",
        "attributes":{
          "data":"false"
        }
      },
      "kH152vsLTL-y20mcLFs9GQ":{
        "name":"S3",
        "transport_address":"inet[/144.76.2.205:9300]",
        "attributes":{

        }
      },
      "J47gdOIwQMq2GTmzzmzJBA":{
        "name":"S2",
        "transport_address":"inet[/144.76.3.103:9300]",
        "attributes":{

        }
      }
    },
    "routing_table":{
      "indices":{
        "test":{
          "shards":{
            "0":[
              {
                "state":"STARTED",
                "primary":true,
                "node":"kH152vsLTL-y20mcLFs9GQ",
                "relocating_node":null,
                "shard":0,
                "index":"test"
              },
              {
                "state":"STARTED",
                "primary":false,
                "node":"J47gdOIwQMq2GTmzzmzJBA",
                "relocating_node":null,
                "shard":0,
                "index":"test"
              }
            ],
            "1":[
              {
                "state":"STARTED",
                "primary":true,
                "node":"ez-rzcnfSESrVYP6zTWksA",
                "relocating_node":null,
                "shard":1,
                "index":"test"
              },
              {
                "state":"STARTED",
                "primary":false,
                "node":"J47gdOIwQMq2GTmzzmzJBA",
                "relocating_node":null,
                "shard":1,
                "index":"test"
              }
            ],
            "2":[
              {
                "state":"STARTED",
                "primary":false,
                "node":"ez-rzcnfSESrVYP6zTWksA",
                "relocating_node":null,
                "shard":2,
                "index":"test"
              },
              {
                "state":"RELOCATING",
                "primary":true,
                "node":"kH152vsLTL-y20mcLFs9GQ",
                "relocating_node":"J47gdOIwQMq2GTmzzmzJBA",
                "shard":2,
                "index":"test"
              }
            ]
          }
        },
        "test123":{
          "shards":{
            "0":[
              {
                "state":"STARTED",
                "primary":false,
                "node":"ez-rzcnfSESrVYP6zTWksA",
                "relocating_node":null,
                "shard":0,
                "index":"test123"
              },
              {
                "state":"STARTED",
                "primary":true,
                "node":"kH152vsLTL-y20mcLFs9GQ",
                "relocating_node":null,
                "shard":0,
                "index":"test123"
              }
            ]
          }
        }
      }
    },
    "routing_nodes":{
      "unassigned":[

      ],
      "nodes":{
        "ez-rzcnfSESrVYP6zTWksA":[
          {
            "state":"STARTED",
            "primary":true,
            "node":"ez-rzcnfSESrVYP6zTWksA",
            "relocating_node":null,
            "shard":1,
            "index":"test"
          },
          {
            "state":"STARTED",
            "primary":false,
            "node":"ez-rzcnfSESrVYP6zTWksA",
            "relocating_node":null,
            "shard":2,
            "index":"test"
          },
          {
            "state":"STARTED",
            "primary":false,
            "node":"ez-rzcnfSESrVYP6zTWksA",
            "relocating_node":null,
            "shard":0,
            "index":"test123"
          }
        ],
        "kH152vsLTL-y20mcLFs9GQ":[
          {
            "state":"STARTED",
            "primary":true,
            "node":"kH152vsLTL-y20mcLFs9GQ",
            "relocating_node":null,
            "shard":0,
            "index":"test"
          },
          {
            "state":"RELOCATING",
            "primary":true,
            "node":"kH152vsLTL-y20mcLFs9GQ",
            "relocating_node":"J47gdOIwQMq2GTmzzmzJBA",
            "shard":2,
            "index":"test"
          },
          {
            "state":"STARTED",
            "primary":true,
            "node":"kH152vsLTL-y20mcLFs9GQ",
            "relocating_node":null,
            "shard":0,
            "index":"test123"
          }
        ],
        "J47gdOIwQMq2GTmzzmzJBA":[
          {
            "state":"STARTED",
            "primary":false,
            "node":"J47gdOIwQMq2GTmzzmzJBA",
            "relocating_node":null,
            "shard":0,
            "index":"test"
          },
          {
            "state":"STARTED",
            "primary":false,
            "node":"J47gdOIwQMq2GTmzzmzJBA",
            "relocating_node":null,
            "shard":1,
            "index":"test"
          },
          {
            "state":"INITIALIZING",
            "primary":true,
            "node":"J47gdOIwQMq2GTmzzmzJBA",
            "relocating_node":"kH152vsLTL-y20mcLFs9GQ",
            "shard":2,
            "index":"test"
          }
        ]
      }
    },
    "allocations":[

    ]
  }
}

##For the duration of the move, this API returns a 500 error
$ curl -XGET localhost:9200/_cluster/nodes/stats?all=true
{
    "error": "ArithmeticException[Value cannot fit in an int: -2562047788015]",
    "status": 500
}
```
</description><key id="11204245">2669</key><summary>Shard Reroute API temporarily breaks Cluster/nodes/stats API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels /><created>2013-02-20T16:31:04Z</created><updated>2013-02-20T21:05:33Z</updated><resolved>2013-02-20T17:23:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-02-20T16:35:04Z" id="13841463">If you can reproduce it again, could you run this: `curl -s -XGET 'http://localhost:9200/_nodes/_local/stats?all=true'` on every node in your cluster and post results somewhere?
</comment><comment author="polyfractal" created="2013-02-20T16:56:00Z" id="13842825">Here you go:  https://gist.github.com/polyfractal/4997040

Just realized that the C1 client node was not configured in Ansible, so I didn't collect `_nodes/_local/stats` for that node...do you want me to redo it and collect stats from that node as well?
</comment><comment author="imotov" created="2013-02-20T17:00:27Z" id="13843098">No need to redo stats, I think I already found what I was looking for. This looks like a duplicate of #2609. 
</comment><comment author="drewr" created="2013-02-20T17:04:15Z" id="13843349">Also #2589.  (I realize it's linked in 2609 too.)
</comment><comment author="polyfractal" created="2013-02-20T17:07:35Z" id="13843550">Ah, cool, thanks for the quick reply.  

And this is why I should upgrade to the latest version of ES before adding an issue =)
</comment><comment author="imotov" created="2013-02-20T17:23:31Z" id="13844397">Sounds like we can close this one then. 
</comment><comment author="polyfractal" created="2013-02-20T21:05:33Z" id="13856428">Just to confirm, this issue is indeed fixed after I upgraded to 0.20.5.  Thanks again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>List of existing plugins with Node Info API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2668</link><project id="" key="" /><description>We want to display information about loaded plugins in Node Info API using plugin option:

``` sh
curl http://localhost:9200/_nodes?plugin=true
```

For example, on a 4 nodes cluster, it could provide the following output:

``` javascript
{
  "ok" : true,
  "cluster_name" : "test-cluster-MacBook-Air-de-David.local",
  "nodes" : {
    "lodYfbFTRnmwE6rjWGGyQQ" : {
      "name" : "node1",
      "transport_address" : "inet[/172.18.58.139:9300]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/172.18.58.139:9200]",
      "plugins" : [ ]
    },
    "hJLXmY_NTrCytiIMbX4_1g" : {
      "name" : "node4",
      "transport_address" : "inet[/172.18.58.139:9303]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/172.18.58.139:9203]",
      "plugins" : [ {
        "name" : "test-plugin",
        "description" : "test-plugin description",
        "site" : true,
        "jvm" : false
      }, {
        "name" : "test-no-version-plugin",
        "description" : "test-no-version-plugin description",
        "site" : true,
        "jvm" : false
      }, {
        "name" : "dummy",
        "description" : "No description found for dummy.",
        "url" : "/_plugin/dummy/",
        "site" : false,
        "jvm" : true
      } ]
    },
    "bnoySsBfTrSzbDRZ0BFHvg" : {
      "name" : "node2",
      "transport_address" : "inet[/172.18.58.139:9301]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/172.18.58.139:9201]",
      "plugins" : [ {
        "name" : "dummy",
        "description" : "This is a description for a dummy test site plugin.",
        "url" : "/_plugin/dummy/",
        "site" : false,
        "jvm" : true
      } ]
    },
    "0Vwil01LSfK9YgRrMce3Ug" : {
      "name" : "node3",
      "transport_address" : "inet[/172.18.58.139:9302]",
      "hostname" : "MacBook-Air-de-David.local",
      "version" : "0.90.0.Beta2-SNAPSHOT",
      "http_address" : "inet[/172.18.58.139:9202]",
      "plugins" : [ {
        "name" : "test-plugin",
        "description" : "test-plugin description",
        "site" : true,
        "jvm" : false
      } ]
    }
  }
}
```

Information are cached for 10 seconds by default. Modify `plugins.info_refresh_interval` property if needed.
Setting `plugins.info_refresh_interval` to `-1` will cause infinite caching.
Setting `plugins.info_refresh_interval` to `0` will disable caching.
</description><key id="11201343">2668</key><summary>List of existing plugins with Node Info API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.0.RC2</label></labels><created>2013-02-20T15:25:25Z</created><updated>2013-12-08T13:26:22Z</updated><resolved>2013-04-05T09:37:37Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T20:48:30Z" id="14310539">hey @dadoonet what is the status of this?
</comment><comment author="dadoonet" created="2013-03-01T20:53:33Z" id="14310763">@s1monw : @kimchy would like to review it before I can push it to master.
</comment><comment author="uboness" created="2013-03-03T20:20:39Z" id="14354321">didn't we say that next the the node info api we'll have a `/_plugin` endpoint where an html is returned for with a list of links to all the plugins? 
</comment><comment author="lukas-vlcek" created="2013-03-03T20:42:53Z" id="14354786">Would it be possible to get also JSON output?
</comment><comment author="kimchy" created="2013-03-03T20:50:37Z" id="14354983">Yes, the idea is that plugins will return as part of nodes info, not using this endpoint at the end.
</comment><comment author="jprante" created="2013-03-05T10:54:00Z" id="14434167">@dadoonet is it possible to rename the plugin type 'java" to something like 'jvm' or 'classpath', since plugins can be written not only in Java but in any JVM language?
</comment><comment author="dadoonet" created="2013-03-08T22:46:01Z" id="14650638">@jprante Agree. Done.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Display list of all available site plugins on /_plugins/ end point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2667</link><project id="" key="" /><description>See issue #2664

When you need to know all available site plugins, you can send:

``` sh
$ curl localhost:9200/_plugin/
```

And get the following result:

``` javascript
{
  "sites" : [ {
    "name" : "anotherplugin",
    "url" : "/_plugin/anotherplugin/"
  }, {
    "name" : "dummy",
    "url" : "/_plugin/dummy/"
  } ]
}
```
</description><key id="11199347">2667</key><summary>Display list of all available site plugins on /_plugins/ end point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-02-20T14:39:04Z</created><updated>2014-06-18T08:44:51Z</updated><resolved>2013-02-24T22:34:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index templates in conf directory are not parsed properly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2666</link><project id="" key="" /><description>I am trying to get index templates working properly but I want to include them in my configuration. I have added lots of logging to the code and I see the following:
1. When I index something that should match a template I see ES go looking for matching templates. However none ever match.
2. All template objects return null when calling the template method.

The cause of the problem seems to be the parsing in the "fromXContent()" method in "IndexTemplateMetadata.Builder". The field it starts with is the template name but it proceeds to the next token without storing it in the template object that is being built.

Duplicating the "template" field makes it work:

```
{
 &#160; &#160;"template" : "my_template",
 &#160; &#160;"order" : 2,
 &#160; &#160;"template" : "my_template",
 &#160; &#160;"settings" : {
    ...
```
</description><key id="11157620">2666</key><summary>Index templates in conf directory are not parsed properly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jbrook</reporter><labels /><created>2013-02-19T15:30:35Z</created><updated>2013-02-25T19:24:39Z</updated><resolved>2013-02-25T19:24:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="jbrook" created="2013-02-25T19:24:39Z" id="14068396">Not a bug.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HTTP Pipelining causes responses to mixed up.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2665</link><project id="" key="" /><description>ElasticSearch seems to advertise HTTP pipelining support by using HTTP 1/1 and not supplying a Connection-header in the response, but fails to deliver on the promises of responding to the requests in the same order they are sent, which means that clients might get the response from an unexpected request.

Example reproduction:

It sometimes works es expected

```
$ printf "GET /_nodes HTTP/1.1\r\n\r\nGET / HTTP/1.1\r\n\r\n" | nc -i 1 127.0.0.1 9200
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 222

{"ok":true,"cluster_name":"elasticsearch","nodes":{"MVf7UrJJRyaOJj35MAdODg":{"name":"Caiera","transport_address":"inet[/10.0.0.6:9300]    ","hostname":"machine.local","version":"0.20.4","http_address":"inet[/10.0.0.6:9200]"}}}HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 169

{
  "ok" : true,
  "status" : 200,
  "name" : "Caiera",
  "version" : {
    "number" : "0.20.4",
    "snapshot_build" : false
  },
  "tagline" : "You Know, for Search"
}
```

But sometimes, given the exact same request, changes the order of the responses:

```
$ printf "GET /_nodes HTTP/1.1\r\n\r\nGET / HTTP/1.1\r\n\r\n" | nc -i 1 127.0.0.1 9200
HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 169

{
  "ok" : true,
  "status" : 200,
  "name" : "Caiera",
  "version" : {
    "number" : "0.20.4",
    "snapshot_build" : false
  },
  "tagline" : "You Know, for Search"
}HTTP/1.1 200 OK
Content-Type: application/json; charset=UTF-8
Content-Length: 222

{"ok":true,"cluster_name":"elasticsearch","nodes":{"MVf7UrJJRyaOJj35MAdODg":{"name":"Caiera","transport_address":"inet[/10.0.0.6:9300]    ","hostname":"machine.local","version":"0.20.4","http_address":"inet[/10.0.0.6:9200]"}}}    
```
</description><key id="11151970">2665</key><summary>HTTP Pipelining causes responses to mixed up.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">nkvoll</reporter><labels><label>bug</label></labels><created>2013-02-19T12:49:27Z</created><updated>2014-10-31T15:31:24Z</updated><resolved>2014-10-31T15:31:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-22T22:55:43Z" id="13977925">HTTP 1.1 and HTTP pipeline are two different things, don't confuse them :). Yea, there will be problems today with using HTTP pipeline feature. 
</comment><comment author="spinscale" created="2013-04-12T07:34:13Z" id="16279550">maybe worth a look in this regard: https://github.com/typesafehub/netty-http-pipelining
</comment><comment author="spinscale" created="2013-04-26T08:05:14Z" id="17060293">@nkvoll Are you trying to make use of the HTTP pipelining feature in production? I actually was not really able to find a Java library supporting this from the client side in order to conduct some tests. Just trying to understand, what you are trying to archive.

What you can do as a different solution at the moment (which is also returning the responses when they are finished instead of creating the responses in a queue and ensuring their order on server side), is to use the "X-Opaque-Id" header of elasticsearch in order to map back the request/response pairs on the client instead of the server side (which might not be what you want depending on your use-case). You will obviously have a hard time with this inside of a browser (not all of them is supporting pipelining anyway)

```
printf "GET /_nodes HTTP/1.1\r\nX-Opaque-Id: 2\r\n\r\nGET / HTTP/1.1\r\nX-Opaque-Id: 1\r\n\r\n" | nc -i 1 127.0.0.1 9200
```
</comment><comment author="spinscale" created="2013-06-06T14:53:15Z" id="19050554">@nkvoll could you solve your issue by using the `X-Opaque-Id` header or do you think there needs more to be done in elasticsearch in order to support your use-case? 

I am not sure if we can 'not advertise' pipelining support (as you wrote in the initial ticket), but still have this functionality. Would like to hear your opinions on that!
</comment><comment author="tkurki" created="2014-02-18T16:20:04Z" id="35401212">Not that I need pipelining with ES for anything, but http 1.1 spec at http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html#sec8.1.2.2 says "A server MUST send its responses to those requests in the same order that the requests were received"
</comment><comment author="kevin-montrose" created="2014-07-25T20:12:33Z" id="50198506">We just got bit **really badly** by this issue over at Stack Overflow.

We've got an app over here that does an awful lot of (highly parallel) reads and writes by id over a small number of types.  Because the _types_ matched most of the time, we'd normally be able to deserialize and proceed normally; oftentimes even updating the wrong document at the end.

This is exacerbated by [.NET's default behavior being to pipeline all web requests](http://msdn.microsoft.com/en-us/library/system.net.httpwebrequest.pipelined%28v=vs.110%29.aspx).

It took the better part of a week to isolate and debug this issue.  It's particularly insidious since capturing requests at a proxy can easily strip out pipelining (as Fiddler does, for example).

We have a workaround (just setting `myHttpWebRequest.Pipelined = false`), pipelined really should fail if they're not going to be honored to spec.

There's a non-trivial performance penalty to disabling pipeling, at least in .NET.  I threw together a [quick gist in LinqPad](https://gist.github.com/kevin-montrose/cc614baa67e066696352) against our production cluster.  In my testing it's about 3 times faster to pipeline requests on our infrastructure.
</comment><comment author="jprante" created="2014-07-25T22:01:03Z" id="50209796">Maybe a priority queue like in https://github.com/typesafehub/netty-http-pipelining could be integrated into ES netty http transport, and enabled by an option, e.g. `http.netty.pipelining: true|false`.
</comment><comment author="gmarz" created="2014-07-25T22:12:39Z" id="50210669">@kevin-montrose , we've never run into that issue, but it sounds a bit scary.  We're considering making a change to the .NET client to disable pipelining by default (exposed as a connection setting) until this is addressed with elasticsearch.  However, I'm a bit hesitant after hearing your claim about performance.

For what it's worth though, I ran your gist on my machine to see if there would be a notable difference, and there wasn't.  In fact, pipelined was a bit slower more times than non.  Maybe totally environment related.
</comment><comment author="kimchy" created="2014-07-25T22:35:06Z" id="50212205">@jprante looking that the code, I don't think its enough what is done there in the context of ES. The implementation supports returning the results in the same order, while in ES, we need to make sure that we execute it in the same order. For example, if an index request and then a get request for the same id are in the same inflight requests, yet serialized on the client side, they need to be serialized on the request side to be executed one after the other, not just return the _results_ of them in order, since in that case, they might still execute out of order.

I still need to think about this more, but effectively, in order to implement ordered pipeline support, it means it needs to be ordered when handling the upstream events, and progressing with one at all before the the previous one is sent downstream as a response.
</comment><comment author="kimchy" created="2014-07-25T22:43:39Z" id="50212744">btw, to make things more interesting..., assume you only execute search requests, then HTTP pipelining with just ordering the responses is great, since it allow for concurrent execution of them on the server side. Tricky.... . 
</comment><comment author="jprante" created="2014-07-26T09:07:10Z" id="50228976">@kimchy maybe it is possible just to timestamp the incoming HTTP request in NettyHttpServerTransport, with a channels handler especially for HTTP pipelining, so the responses can be ordered downstream in a priority queue by timestamp? Queueing up a certain number of responses in a queue in this special HTTP pipeline channels handler would be the downside.

I think executions can be still unordered, even with HTTP pipelining, because these are separate things. If e.g. set/get on the same id fails, there might be other obscure reasons. But it will just become reliably visible, by eliminating Netty's weakness in HTTP pipelining.
</comment><comment author="NickCraver" created="2014-07-27T14:04:30Z" id="50265156">@gmarz Pipelining's main gains are in any environment with non-0 latency: the higher the ratio of network latency vs. time spent inside elastic, the higher the impact. It's definitely going to vary by environment, but there are many cases where this is a huge impact...in others like link-local or on-box it may have little difference. It's important to test the performance impact of pipelining over _not_ on your local machine as well.
</comment><comment author="gmarz" created="2014-07-27T15:36:46Z" id="50270898">@NickCraver thanks a lot for the info, I figured as such.  Definitely plan on doing more testing before making any changes to the .NET client.  I just opened #830 for this, suggestions/input very much welcomed.
</comment><comment author="hamiltop" created="2014-07-29T08:08:15Z" id="50447087">We're getting bit by it as well. Using erlang ibrowse client, which uses pipelining by default.
</comment><comment author="NickCraver" created="2014-10-15T20:51:57Z" id="59273987">Has there been any update on this? It was a deciding factor in us dropping ElasticSearch as a viable option for projects at Stack Overflow, but we'd love to see this fixed for other potential uses.

This is a serious bug, and it doesn't seem (at least to our developers) to be treated as serious as the impact it has.
</comment><comment author="kimchy" created="2014-10-15T21:16:34Z" id="59277785">@NickCraver thanks for giving context into this, I have personally been thinking about how to solve this, I think that potentially we can start with baby steps and make sure that at the very least the response are returned in order, and iterate from there. Will keep you posted.
</comment><comment author="asnare" created="2014-10-29T14:48:01Z" id="60936550">@kimchy I'm not sure that execution order is a problem for pipelined requests, mainly because pipelining is a transport optimisation. Pipelined requests are semantically equivalent to the same requests being issued on separate connections. If client applications care about ordering, they should not issue requests that depend on requests whose response status is still pending.
</comment><comment author="NickCraver" created="2014-10-30T11:15:35Z" id="61076572">To better illustrate for people the impact here, let's compare a real world example. Let's say for example I'm querying our elasticsearch cluster in Oregon from New York. I have 10 requests to make. The trip from NY to OR takes about 40ms, and elasticsearch only takes 10ms to fulfill each request. Let's compare performance with and without pipelining:

Pipelining:

```
10 requests issued in order immediately
40ms travel time
10 requests processed (any order), responded to (in order) (10+ms, elasticsearch processing)
40ms travel time
10 responses received in order
```

We're talking about 90+ ms (the + depends on how well elasticsearch handles the concurrent queries). Let's say it's not even concurrent (worst case), then we're talking 100ms for elastic, so 190ms total.

Now let's turn pipelining off:

```
1 request issued
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Single request received, next request sent
40ms travel time
10ms 1 request processed, 1 response sent
40ms travel time
Last request received
```

That took 90ms **per request**, so at best it's 900ms. Hopefully that better illustrates the performance problem that not having HTTP pipelining working introduces. The same happens at &lt; 1ms latency which adds up for millions of requests per day. We hit some systems like this (redis for example) with 4 billion+ hits a day...without pipelining Stack Overflow would be hosed.

Currently, if one for 10 indexing operations fails (especially on the indexing side) we can get an "okay, indexed!" when in fact it failed...and we don't even know _which one_ failed. So not only is this a major problem (which is why I feel this makes easily the resiliency list), it's a problem a user is incapable of solving. You currently just have to turn off pipelining and take a _huge_ performance hit for high traffic applications.

As for execution order, yes it _does_ matter. If a Stack Overflow post changes, we may issue an index request as soon as that happens - edits can happen rapidly and 2 indexing requests for the same document (with a new post body, title, tags, last activity date, etc. in our case) can easily be in the pipeline. If the last index command doesn't execute last, then an invalid document has been indexed which is out of date.
</comment><comment author="nkvoll" created="2014-10-30T11:22:26Z" id="61077257">@NickCraver regarding the execution order -- wouldn't using the `external` version type for example with the source timestamp work for you? (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-index_.html#_version_types) I'm assuming that you have an explicit document identifier and explicitly versioning documents makes a lot of sense when working with a distributed system. This way, it wouldn't even matter which server you send the request to and the actual operation speed of that particular server -- much less the exact execution order for requests coming from a given HTTP pipeline -- the latest version would be the one that sticks in any case.
</comment><comment author="NickCraver" created="2014-10-30T12:18:27Z" id="61082695">@nkvoll for that exact case, we could version the documents yes (though we have no other reason to was we only care about the current one). But we'd be explicitly doing so just to work around this bug.

Let's take another example we hit that causes Elasticsearch to be dropped from the project:
1. Index a document.
2. Do a GET for that document, by Id.

Since 2 can execute and return before 1, _on the same node_, even a getting a document you just stored isn't guaranteed, and bit us several times.

Not related to (regardless of) the execution order, let me address transport impact:

The transport switch (even if executed in order) is still a compounding issue. We were doing batch index operations for documents, GETting documents as well (all at high volume/speed) and checking cluster status along every so often as well to check index queue limits and such.  We'd get cluster status responses to the document GET by Id requests. I don't think I have to impress just how bad that behavior is - and it's exactly what we hit many times that led us to this Github issue.

For all the documents that where the correct type and didn't throw deserialization errors as a result, we then had no confidence they were the right documents returned to the right requests. We know at least a decent portion of them weren't. As a result we had to throw away all of the data and start over. That's when the decision was made to redesign the data layout and abandon Elasticsearch for the project.
</comment><comment author="kimchy" created="2014-10-30T12:28:31Z" id="61083672">@NickCraver I read pipelining wrong and actually part of the spec is not to allow to pipeline non-idempotent methods, and its the responsibility of the client not to do so (http://www.w3.org/Protocols/rfc2616/rfc2616-sec8.html). This heavily simplifies the implementation on our side, so I hope it happens quicker (@spinscale is on it, we are aiming for 1.4). So the question around order is not relevant.

Regarding the example you gave, of course pipelining will help on a single connection, I don't think there was any denying it.
</comment><comment author="nkvoll" created="2014-10-30T12:28:58Z" id="61083719">@NickCraver true, the document will not be available before the Lucene segment is written, which is after the default `refresh_interval` of 1 second (configurable, of course). You could do a an explicit call to `_refresh` (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-refresh.html#indices-refresh), but I'm not sure I'd recommend trying to work _around_ the Near Real Time-properties of Elasticsearch/Lucene as much as work _with_ it. I'm unaware of any planned changes to this part, as it's pretty central to achieving good indexing performance and coordinating shards across nodes, but then again, I'm not an Elasticsearch employee.

Regarding the transport level mix-up, I certainly feel you (which is why I created this issue in the first place). I think the worst part is that pipelining is advertised according to the HTTP 1.1 specification, but the actual results are wrong -- and that this might not be evident when doing small-scale development/testing runs. It can certainly catch unwary developers out quickly.

While maybe not optimal for your case, it's possible to work around this as well running a small proxy on the same server as your Elasticsearch instance that supports pipelining proper on the server side, but is possible to configure to _not_ pipeline when forwarding requests to Elasticsearch. Whether this is a feasible approach depends on how you do operations and what systems you're comfortable with using. It's not at all optimal, but will get you closer to the goal until it's supported properly.
</comment><comment author="Mpdreamz" created="2014-10-31T14:40:22Z" id="61269770">@NickCraver  out of interest what are you using to issue a:

```
POST [index doc]
GET [doc]
```

On a pipelined http connection ?

To the best of my understanding `HttpWebRequest` (which the new `HttpClient` class still uses under cover by default)  adheres to the RFC and won't pipeline any request that has a body:

http://referencesource.microsoft.com/#System/net/System/Net/_Connection.cs#800

and force it to wait on a new/free connection of it sees a request with a body on a pipelined connection:
http://referencesource.microsoft.com/#System/net/System/Net/_Connection.cs#614

As @kimchy pointed out The RFC disallows this:

`Clients SHOULD NOT pipeline requests using non-idempotent methods or non-idempotent sequences of methods (see section 9.1.2). Otherwise, a premature termination of the transport connection could lead to indeterminate results. A client wishing to send a non-idempotent request SHOULD wait to send that request until it has received the response status for the previous request.`

Of course the ordering issue still exists when doing 10 GETS sequentially on a pipelined connection and if they return out of order your application might end up updating the wrong documents (which should be fixed with the new pipelining support)

Please nudge me If my understanding of the `HttpWebRequest` in this regard is flawed!

@nkvoll elasticsearch should be realtime when indexing a single doc and then doing a single doc get provided the client waits for an ack on the client for the index and they are executed in the right order.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Display list of all available site plugins on /_plugin/ end point</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2664</link><project id="" key="" /><description>When you need to know all available site plugins, you can send:

``` sh
$ curl localhost:9200/_plugin/
```

And get the following result:

``` javascript
{
  "sites" : [ {
    "name" : "anotherplugin",
    "url" : "/_plugin/anotherplugin/"
  }, {
    "name" : "dummy",
    "url" : "/_plugin/dummy/"
  } ]
}
```
</description><key id="11151613">2664</key><summary>Display list of all available site plugins on /_plugin/ end point</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-02-19T12:35:12Z</created><updated>2013-03-03T20:51:52Z</updated><resolved>2013-02-23T08:38:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-03-03T20:51:52Z" id="14355014">Note, this has been reverted, a better place for it would be under the nodes info API.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Routing and Parent/Child Can lead to documents with same id in the same index and type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2663</link><project id="" key="" /><description>Changing the routing value or parent id of a document will lead to duplicate documents with the same index, type, and id.  I _think_ that internally a documents id is constructed from the index, type, id, and routing value so internally the id's are different, yet they look the same to the end user. 

Is there something we can do to address this?  Maybe add an "old_route" and "old_parent" flag that when found can be used to do issue a delete on the old shard?  Is there a better approach? 
</description><key id="11139904">2663</key><summary>Routing and Parent/Child Can lead to documents with same id in the same index and type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">mattweber</reporter><labels><label>:Parent/Child</label><label>adoptme</label><label>low hanging fruit</label></labels><created>2013-02-19T03:14:11Z</created><updated>2016-07-27T15:45:17Z</updated><resolved>2014-12-24T15:51:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-19T10:02:39Z" id="13765018">Yes, changing routing / parent value is trappy. We could implement this nicely via the update api with a script.

```
curl -XPOST 'localhost:9200/index/type/1/_update' -d '{
   "script" : "ctx._routing = newRouting",
   "params" : {
        "newRouting" : "my_new_routing"
    }
}'
```

At the moment only updating `_ttl` and `_timestamp` is supported via this way.
I guess when indexing existing docs via index api, something like `old_route` / `old_parent` is the way to go.
</comment><comment author="kimchy" created="2013-02-22T22:53:37Z" id="13977716">Note that its very delicate to update routing using the update API, effectively, you need to delete the doc from one shard, and update it in another shard..., we don't do those now...
</comment><comment author="clintongormley" created="2014-07-04T10:07:38Z" id="48027688">We should disallow changing the routing value during an update request.
</comment><comment author="clintongormley" created="2014-12-24T15:51:15Z" id="68059875">It looks like you can't change id, routing or parent values during an update request, so this appears to work already. Closing
</comment><comment author="zanona" created="2016-07-23T16:09:37Z" id="234726116">Just wondering if transferring parent ownership is still something not possible?

Let's say a company has many members as children. If I, as the admin/owner of the company wants to transfer the ownership of the company to another user, then using parent/child relationship wouldn't be possible since I would need to update the parent value and so all the members of the company point to the new shard? How would you guys work with this kind of situation?

Would the solution still be deleting the document and all its children and re-create them pointing to the new parent?
</comment><comment author="clintongormley" created="2016-07-27T15:45:17Z" id="235627976">&gt; Would the solution still be deleting the document and all its children and re-create them pointing to the new parent?

yes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for sorting by fields inside one or more nested objects</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2662</link><project id="" key="" /><description>The sorting by nested field support has the following parameters on top of the already existing sort options:
- `nested_path` - Defines the on what nested object to sort. The actual sort field must be a direct field inside this nested object. The default is to use the most immediate inherited  nested object from the sort field. 
- `nested_filter` - A filter the inner objects inside the nested path should match with in order for its field values to be taken into account by sorting. Common case is to repeat the query / filter inside the nested filter or query. By default no `nested_filter` is active.

Either the highest (max) or lowest (min) inner object is picked for during sorting depending on the `sort_mode` being used. The `sort_mode` options `avg` and `sum` can still be used for number based fields inside nested objects. All the values for the sort field are taken into account for each nested object.
</description><key id="11127103">2662</key><summary>Add support for sorting by fields inside one or more nested objects</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-02-18T19:01:50Z</created><updated>2016-10-10T12:44:33Z</updated><resolved>2013-02-18T21:28:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="sticky1" created="2013-02-27T10:15:54Z" id="14166233">Any chance if I wanted to sort on sum of value of sort field from all the documents. Say for example if price is nested object. with following value price:[{"date":"2010-10-10","value":100},{"date":"2010-11-12","value":200}]. So, can I choose sort_mode as "sum" with the expected sort value of 100+200=300 for the root document.
</comment><comment author="martijnvg" created="2013-02-27T15:29:22Z" id="14179568">@sticky1 At the moment the `sort_mode` options avg and sum only work on the actual sort field level. If the `sort_mode` is `avg` or `sum` then on the nested level it either picks the most ascending or most descending nested object depending on the sort order. I'm planning to fix this. This relates to #2701 

Sorting with a nested object and using the `sort_mode` options `min` and `max` work as expected.
</comment><comment author="sticky1" created="2013-02-28T05:09:21Z" id="14217166">@martijnvg Thanks for the update.
</comment><comment author="davrock" created="2013-03-28T09:58:02Z" id="15577356">Does this work in version 20.6?
I have a nested array of objects which I am trying to sort on using this new filter feature.
This is my query and sort:
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "relevancies.relevance": {
        "mode": "avg",
        "order": "asc",
        "nested_filter": {
          "term": {
            "relevancies.category.id": 14
          }
        }
      }
    }
  ]
}
It doesn't seem to be sorting at all and I tried including the "nested_path": "relevancies" field but it doesn't like it.
</comment><comment author="kimchy" created="2013-03-28T10:49:25Z" id="15580443">@davrock no, its a 0.90 feature only
</comment><comment author="davrock" created="2013-03-28T12:12:17Z" id="15585308">@kimchy Well that would explain why it didn't work! Any plans to include it in a stable release?
</comment><comment author="kimchy" created="2013-03-28T13:46:50Z" id="15589041">@davrock once 0.90 becomes GA, yea :)
</comment><comment author="tommymonk" created="2013-04-29T20:12:19Z" id="17191019">Thank you, I am loving this new feature.

In addition to min, max, avg and sum, would it be possible to add support for n-th element sort, such that;

gist: https://gist.github.com/tommymonk/5484380

It would helpful when there is a meaningful order to the JSON array like the example above.

This is currently not possible with Scripted Sort as doc['field-name'] and _fields['field-name'] only operate on root level untokenised primitives
</comment><comment author="martijnvg" created="2013-05-01T11:42:52Z" id="17277924">@tommymonk Makes sense to add this as `sort_mode`. 
</comment><comment author="sticky1" created="2013-11-15T05:26:17Z" id="28548270">Its really going good that we can now sort root documents on the sum (aggregation) on the values of nested object. Is there a way that we can filter the root document based on the sum of values in nested document?
</comment><comment author="emperorxiaomai" created="2016-10-10T12:44:32Z" id="252608056">"Either the highest (max) or lowest (min) inner object is picked for during sorting depending on the sort_mode being used. The sort_mode options avg and sum can still be used for number based fields inside nested objects. All the values for the sort field are taken into account for each nested object."
You mean the min or max mode base on all the values for the sort field? is it possible to base on some filtered values for the sort field?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Short response for Bulk API</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2661</link><project id="" key="" /><description>When using the bulk API with _a lot_ of documents (say 100K), it's kind of pointless to get a response for each of those documents because the response is huge! Cannot we have a way to call this API and only receive a response that is either:
- Successful insertion of all documents (count).
- Some insertion failed (count of the ones that failed).
- All insertion failed (count).

I know that I could probably use Bulk UDP (and I'll probably give it a go soon), but UDP doesn't receive any response whatsoever, so it's like having black&amp;white, but not any shade of grey.

Thanks!
</description><key id="11121178">2661</key><summary>Short response for Bulk API</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/nik9000/following{/other_user}', u'events_url': u'https://api.github.com/users/nik9000/events{/privacy}', u'organizations_url': u'https://api.github.com/users/nik9000/orgs', u'url': u'https://api.github.com/users/nik9000', u'gists_url': u'https://api.github.com/users/nik9000/gists{/gist_id}', u'html_url': u'https://github.com/nik9000', u'subscriptions_url': u'https://api.github.com/users/nik9000/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/215970?v=4', u'repos_url': u'https://api.github.com/users/nik9000/repos', u'received_events_url': u'https://api.github.com/users/nik9000/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/nik9000/starred{/owner}{/repo}', u'site_admin': False, u'login': u'nik9000', u'type': u'User', u'id': 215970, u'followers_url': u'https://api.github.com/users/nik9000/followers'}</assignee><reporter username="">knocte</reporter><labels><label>:Bulk</label><label>enhancement</label><label>low hanging fruit</label></labels><created>2013-02-18T16:22:54Z</created><updated>2017-01-17T19:34:34Z</updated><resolved>2016-12-09T10:53:22Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="colings86" created="2014-09-05T08:17:08Z" id="54597061">@knocte Would your use-case be solved if the response of the Bulk API contained a field which showed the number of failed insertions? This way you will not need to parse the whole response if all are successful, you can just check the value of this field is 0. If the value is not 0 then you can parse the rest of the response to find the cause.
</comment><comment author="clintongormley" created="2014-09-06T17:33:30Z" id="54721733">@colings86 we already have an error flag in the bulk response, but there is something to be said for returning a minimal body if all requests succeeded.

I wouldn't be in favour of not reporting failures though, and given that requests are ordered, that implies the need to return all responses.
</comment><comment author="knocte" created="2014-09-06T23:50:45Z" id="54732387">Sorry, I no longer use ElasticSearch
</comment><comment author="colings86" created="2014-09-08T08:53:10Z" id="54791526">So maybe we should have an option in the request to send back a minimal response if there are no errors and a full response if there is even 1 error.
</comment><comment author="astefan" created="2014-10-03T07:04:51Z" id="57762795">I would be in favor of such a feature.
For example, if I use the search API and I execute a query and I'm only interested in the number of hits, not the actual hits as well, I add ?search_type=count and I don't get back the possibly long list of hits which don't actually interest me.

Maybe add an attribute for _bulk - something like "response_type". With three values: "full" - to get back what we get today for _bulk, "partial_with_failures" - minimal response if there are no errors and a full response if there is at least one error, "minimal" - minimal response no matter if there were errors or not.
</comment><comment author="clintongormley" created="2014-10-17T08:17:20Z" id="59480919">I don't like the `minimal` response in case of errors, but I do like an optional short response if there were no errors.
</comment><comment author="mbonaci" created="2015-10-28T13:06:15Z" id="151838791">Very nice, this is really overdue.
Have you thought about returning only failed docs in the response? Or even just ordinal numbers of failed docs. I assume people often want to further treat unsuccessful docs in some way, e.g. retry after modifications, store unstructured, ...

Also, instead of returning `200` or `201`, there are perhaps more suitable response codes in this situation, which would make the flag check inside the response obsolete.
Perhaps `207` or `422`, depending on whether you want to hint success or failure, respectively.

&gt; The 207 (Multi-Status) status code provides status for multiple independent operations (see Section 13 for more information). A Multi-Status response conveys information about multiple resources in situations where multiple status codes might be appropriate. The default Multi-Status response body is a text/xml or application/xml HTTP entity with a 'multistatus' root element. Further elements contain 200, 300, 400, and 500 series status codes generated during the method invocation. 100 series status codes SHOULD NOT be recorded in a 'response' XML element. Although '207' is used as the overall response status code, the recipient needs to consult the contents of the multistatus response body for further information about the success or failure of the method execution. The response MAY be used in success, partial success and also in failure situations. The 'multistatus' root element holds zero or more 'response' elements in any order, each with information about an individual resource. Each 'response' element MUST have an 'href' element to identify the resource.
&gt; MULTI_STATUS(207)
&gt; 
&gt; The 422 (Unprocessable Entity) status code means the server understands the content type of the request entity (hence a 415(Unsupported Media Type) status code is inappropriate), and the syntax of the request entity is correct (thus a 400 (Bad Request) status code is inappropriate) but was unable to process the contained instructions. For example, this error condition may occur if an XML request body contains well-formed (i.e., syntactically correct), but semantically erroneous, XML instructions.
</comment><comment author="rpedela" created="2016-03-08T12:47:18Z" id="193771333">+1
</comment><comment author="avoxm" created="2016-04-15T17:48:49Z" id="210562790">It has been over 3 years guys, any hope to have this implemented. Bulk API is quite memory hungry because of this issue.
</comment><comment author="nik9000" created="2016-04-15T17:58:13Z" id="210565510">The `adoptme` and `low hanging fruit` mean the repository owners would love to review and merge a pull request that implements the feature but it isn't anyone's priority. If anyone wants to write a PR for this I'll certainly review it and, once we get to the other side of the review process, merge it.
</comment><comment author="nik9000" created="2016-04-22T13:50:22Z" id="213437014">I'll just go do it.

I'll make an option that will just count successful operations rather than returning the whole result for it. Any failures will still come back because you can act on them.
</comment><comment author="rpedela" created="2016-04-22T16:08:53Z" id="213493952">Thanks Nik!
</comment><comment author="kimchy" created="2016-05-02T21:36:10Z" id="216371748">sorry for chiming in late, especially in the context of #17932. I am not a fan of this change, a short response format breaks the structure of how we return responses for bulk requests, and no longer relies on position within an array to match response to request. It means special code handling, different one, depending on the parameter of the request, which is not user friendly.

I also challenge the fact that parsing the response of a bulk requests is "heavy". Compared to setting up the requests to be indexed (which is the "other" client side work), I am very surprised that parsing the response takes so long as a whole compared to the whole bulk execution. This is definitely not the case in Java.

I would be ok with a flag that will simply not return anything except for top level summary fields if everything is successful. This is probably the most common case, and if something failed, just use the current way to correlate request/response.
</comment><comment author="nik9000" created="2016-05-03T20:03:51Z" id="216648404">For what it is worth I don't have a strong opinion on how this should comes out or even if we merge #17932. I just implemented my first instinct and my first instinct is rarely right. My only strong opinion is "if Elasticsearch doesn't want any sort of short format on the bulk response then we need to close this issue".
</comment><comment author="clintongormley" created="2016-11-06T09:25:00Z" id="258666904">&gt; I would be ok with a flag that will simply not return anything except for top level summary fields if everything is successful. This is probably the most common case, and if something failed, just use the current way to correlate request/response.

+1
</comment><comment author="rpedela" created="2016-11-06T17:52:49Z" id="258697517">&gt; I would be ok with a flag that will simply not return anything except for top level summary fields if everything is successful. This is probably the most common case, and if something failed, just use the current way to correlate request/response.

I think the summary should be the default, and the current response should be behind a verbose flag. Otherwise +1.
</comment><comment author="clintongormley" created="2016-11-07T13:03:30Z" id="258829945">&gt; I think the summary should be the default, and the current response should be behind a verbose flag. Otherwise +1.

This would be a breaking change, so best to keep the default as it is today.
</comment><comment author="rpedela" created="2016-11-07T17:24:55Z" id="258901695">The default should always be the most common case. In addition, the current, default behavior requires more CPU, memory, and network resources which seems like a bad thing. Besides aren't breaking API changes the norm for this project? It seems like every major version, sometimes even minor versions, have major changes to the API.
</comment><comment author="clintongormley" created="2016-11-07T17:50:14Z" id="258908522">@rpedela We think long and hard about every breaking change that we make, and try to find ways to smooth the migration.  This is a good example of something that we shouldn't break.
</comment><comment author="clintongormley" created="2016-12-09T10:53:22Z" id="265986814">Rediscussed this in FixItFriday. We're loathe to change the API (which pushes complexity towards the clients and runs the risk of bugs) without any benchmarks demonstrating how much performance gain there is from making this change.  The bulk response is very compressible so we could well find that leaving out the bulk items make a negligible difference.

I'm going to close this for now, but feel free to reopen if you can show a significant difference in performance when compression is enabled.</comment><comment author="arakelian" created="2017-01-17T18:59:40Z" id="273264492">@kimchy @nik9000 I came across this thread while researching bulk api requests format. It's worth mentioning here that [bulk api documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html) doesn't mention that request/response are correlated by array order.  Would you mind adding that as an API guarantee?

I use in ES a production environment which indexes hundreds of billions of documents, and we retry individual documents that fail via the bulk API. Since the documentation doesn't guarantee ordering of the response, we had been matching request/response items via index/type/id, but there is a subtle problem with that approach.  We use index aliases, and when you do that, the request will have the alias, but the response has the physical index name.. a mismatch occurs.

</comment><comment author="jasontedor" created="2017-01-17T19:05:05Z" id="273265984">@garakelian I believe that's covered [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/_batch_processing.html):

&gt; When the bulk API returns, it will provide a status for each action (in the same order it was sent in) so that you can check if a specific action failed or not.</comment><comment author="arakelian" created="2017-01-17T19:14:59Z" id="273268758">@jasontedor Thanks for quick reply. I missed that page, thank you. Do you know if a PR request would be welcomed to update the documentation on the other page I referenced, (to point to the page you linked), and/or add the note about the index name being different on the reply when aliases are being used?</comment><comment author="jasontedor" created="2017-01-17T19:26:58Z" id="273272273">An update to the bulk API documentation that you referenced would be welcomed (I would prefer explicitly mentioning the API guarantee that we provide, rather than linking elsewhere in our docs).

As for the response when indexing using an alias, I think that is covered in the alias docs (if it's not, it should be here), but I don't think that the bulk API docs are the right place (should be more general).

Does that make sense?</comment><comment author="arakelian" created="2017-01-17T19:32:26Z" id="273273948">@jasontedor Sure. I'll submit a PR based on your suggestion... Thanks.</comment><comment author="jasontedor" created="2017-01-17T19:34:34Z" id="273274603">Thank you @garakelian.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose CJKBigram and Width TokenFilters in ElasticSearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2660</link><project id="" key="" /><description>CJKBigram especially can be used together with ICU / StandartTokenizer to for Bigrams for CJK lanugages only and leave other scripts untouched. We should expose those filters directly.
</description><key id="11094586">2660</key><summary>Expose CJKBigram and Width TokenFilters in ElasticSearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-02-17T20:58:47Z</created><updated>2013-02-17T22:59:12Z</updated><resolved>2013-02-17T22:59:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>API feature for ES update</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2659</link><project id="" key="" /><description>As we know, to update elasticsearch we need to follow some steps aka flush index, shutdown node etc.

This works fine if we have only a few indices. But imagine we have a weekly-based (or even daily-based) indices ... this process requires to create some custom scripts to iterate through indices and prepare them (e.g. flush) ... sounds not reasonable and dangerous

It would be nice to have some API from elasticsearch which on call can prepare whole my cluster for update procedure. Besides that, in the future to this API (as a step of preparation) could be added an option to backup that cluster before (as soon as Backup API would be you released).

What do you think?
</description><key id="11092638">2659</key><summary>API feature for ES update</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gakhov</reporter><labels /><created>2013-02-17T19:43:34Z</created><updated>2013-02-19T10:15:31Z</updated><resolved>2013-02-19T09:50:55Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-18T22:36:24Z" id="13747013">hey @gakhov I wonder what else than _flush should be called here. You can call flush without specifying an index and flush all indices in the cluster (http://www.elasticsearch.org/guide/reference/api/admin-indices-flush.html) using

`$ curl -XPOST 'http://localhost:9200/_flush'`

are you thinking of anything else?
</comment><comment author="gakhov" created="2013-02-19T00:01:36Z" id="13749961">hi!  @s1monw I don't really sure ... just wanted to have upgrade process more simple, since i'm a bit lazy .. lol

Currently, to upgrade ES i need to 
- prevent writing
- flush indices
- iterate through all nodes and do backup/stop/update/start (some downtime is acceptable for us, so we don't do the trick with renaming a cluster to keep frontend application running). 

Of course, ES can't help me with the actual upgrading procedure, but I thought it can manage the preparation process. 

Right now we update our ES nodes with deployment script (fabric and puppet), so we need just to be sure that ES cluster (and indices) is ready for that from the data state point of view.

In our case, we can't stop the indexation process, but if it receives write exception, the failed items will be rescheduled. So, we need just to do `index.blocks.read_only` before starting the actual upgrade. We don't use automatical shard allocation, so no worries about ES starting reallocate shards when nodes will go off for upgrade.

Actually, i also thinking about the backup procedure (e.g. with rsync), which requires data copy. All these two require some common steps: at least flush and read_only. And  thought it would be very useful  to have some standard way to do that, e.g. shortcuts from ES API.

So, it might be `read_only` + `flush` as _backup_ preparation procedure and _backup_+(optionally)shutdown as upgrade prepration procedure. In some cases, `optimize` also (e.g. to 1 segment)

 Does it make sense for you?
</comment><comment author="s1monw" created="2013-02-19T09:20:50Z" id="13763491">hey @gakhov 

well laziness is good otherwise there wouldn't be any automation I guess, I'm not really sure if I get this right so lets just clarify here if we are missing something API wise. At the current stage if you are doing a code upgrade on ElasticSearch you don't need to necessarily prevent writing to ElasticSearch while you upgrade. You can do this entire upgrade procedure without downtime etc. There are a couple of things you might want to apply before doing this:
- Flushing all your indices might be a good idea
- Prevent shards from being relocated or allocated on other nodes while nodes are going down during the upgrade
  - `cluster.routing.allocation.disable_new_allocation = true`
  - `cluster.routing.allocation.disable_allocation = true`
  - also set `cluster.routing.allocation.cluster_concurrent_rebalance`  and ``cluster.routing.allocation.node_concurrent_recoveries` to a low number to prevent your cluster to go nuts when you allow allocation again. (see http://www.elasticsearch.org/guide/reference/modules/cluster.html for detail)
  - along the same lines it might make sense to throttle recoveries and merges for stability and preventing your segments from diverging too much (http://www.elasticsearch.org/guide/reference/index-modules/store.html
- now you can start upgrading nodes. It might make sense at this point to move all allocated shards away from the machine you upgrade using 

```
curl -XPUT localhost:9200/_cluster/settings -d '{
    "transient" : {
        "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
    }
}' 
```
- now you can simply shutting down that node and move over to a new version &amp; bring it up again
- continue doing this for the rest of the cluster. 

you might also want to look into this gist from clinton https://gist.github.com/clintongormley/3888120 that shows a technique that can be used to upgrade.

In general I recommend to practice this on a staging system with more than 2 machines etc. Another thing I would recommend is to take your time doing this, don't rush, wait after your cluster stabilizes after upgrading a node. 

I can see the problem with backing up stuff but then you can simply run 2 commands:

```
# make all indices read only
curl -XPUT 'localhost:9200/_settings' -d '
{
  "index.blocks.write" : true
}

# flush data to disk and clean transaction logs
curl -XPOST 'http://localhost:9200/_flush'

# optimize to a single segment -- yet I would not recommend this!!
curl -XPOST 'http://localhost:9200/_optimize'
```

I think this would still work find in a client scrip so we don't need a dedicated API, right? I we have more sophisticated backup / restore apis this might change.
</comment><comment author="gakhov" created="2013-02-19T09:46:43Z" id="13764420">Thank you, @s1monw 
I agree all that could be done with existing API. I just notices that the algorithm contained some steps which need to be done every time, so could be wrapped in a single call.

Concerning `optimize`, i remember that "Simon says: optimize is bad for you", but still at some point could be useful for our time-sliced indices.

I think, we can close this ticket for now. Probably, this should be skipped until some more use cases come (e.g. after ES will have nice backup API).

Anyway, thank you for the help and patience :)
</comment><comment author="s1monw" created="2013-02-19T09:50:55Z" id="13764595">@gakhov I agree if you have time sliced indices which don't change anymore at some point it might totally make sense to call optimize during a quite period. Yeah I think there are always use-cases but in general if you have a incrementally changing index optimize is not needed IMO.

I will close this for now! Hope that helped you to get more clarity along the lines.

simon
</comment><comment author="synhershko" created="2013-02-19T10:10:18Z" id="13765289">@s1monw this is a good procedure for standard-case upgrades, but this still doesn't account for upgrading to an incompatible version (0.20 to 0.21 for example), where nodes will fail to communicate with one another, and clients compiled against a previous version of ES.jar won't be able to access the cluster.

Does the only option we have when upgrading to a new major version is creating a new cluster? If so, could it become an easier upgrade path in the future?
</comment><comment author="s1monw" created="2013-02-19T10:15:31Z" id="13765463">&gt; Does the only option we have when upgrading to a new major version is creating a new cluster? If so, could it become an easier upgrade path in the future?

yeah I think at this point you have to gracefully bring your cluster over to a new clustername and once you have the majority of the nodes migrated you bring you clients over and continue with the rest of the machines. I don't see how we can get around this with major API incompatibilities at this point but there is lots of room for improvement here.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master node operations might be executed twice</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2658</link><project id="" key="" /><description>If a master node operation, such as create index, is executed on a cluster with a global master block, it might be executed twice once block is removed.
</description><key id="11079257">2658</key><summary>Master node operations might be executed twice</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.20.6</label><label>v0.90.0.Beta1</label></labels><created>2013-02-16T21:42:44Z</created><updated>2013-02-17T01:30:39Z</updated><resolved>2013-02-17T01:30:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Refactoring accessors using only getters and setters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2657</link><project id="" key="" /><description>We want to clean up the code and use only _standard_ accessors (gettters and setters).
So we remove old non standard accessors.

Current request builders to build request with elasticsearch stay the same, and its the recommended way to execute APIs with elasticsearch. The pure "request" APIs have changed, and now have setters methods instead of the non setters options, code will need to change if pure Request objects/api are used.

Last, all response levels objects now only expose the getter API variant. We have had both variants (getter and non getter) for some time, and its time to clean it up as it creates both confusion and overhead on our end to maintain it.
</description><key id="11070857">2657</key><summary>Refactoring accessors using only getters and setters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels><label>breaking</label><label>v0.90.0.Beta1</label></labels><created>2013-02-16T16:43:57Z</created><updated>2013-10-06T19:21:53Z</updated><resolved>2013-02-18T16:25:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="michaelklishin" created="2013-02-22T10:57:56Z" id="13938049">Please reconsider this change. It breaks a lot of examples in the docs, on the mailing list and all over the Internet. Leave alone a lot of apps that use the transport client.
</comment><comment author="kimchy" created="2013-02-22T11:09:51Z" id="13938498">@michaelklishin the assumption is that people have moved to use the RequestBuilders on the client side, and nothing breaks there on the request side. Regarding the response, maintaining the non getter options is just an overhead on our end, we have the getter option for a long time now, hopefully people have migrated to it so there won't be a problem.
</comment><comment author="kimchy" created="2013-02-22T12:42:43Z" id="13941380">@michaelklishin thinking about ti a bit more, I think we took teh change a bit too far with changing the XXXRequest methods. We will revert changing those, but still keep the removal of Response xxx methods and just stay with getXXX methods.
</comment><comment author="jprante" created="2013-02-22T12:59:09Z" id="13941928">This breaks a lot of method API calls I have use in plugins, just because I assumed the other way round is preferred (getters/setters are from the old JavaBeans model, in contrast to fluent interface style). I would have loved to see @Deprecated markers before. Please, reconsider this change in favor to announce such large changes in the API carefully, with deprecated method markers, so we have time to rewrite the code.
</comment><comment author="michaelklishin" created="2013-02-22T12:59:18Z" id="13941931">There's plenty of information on the Web that constructs requests directly, and this is what I've been doing with
the native client in [Elastisch](http://clojureelasticsearch.info). I will switch to request builder but it still sounds
like a good idea to deprecate + keep the current API methods for a release or two (say, remove them after 0.22).

Thank you for taking the time!
</comment><comment author="kimchy" created="2013-02-22T13:00:46Z" id="13941977">Actually, the main goal of the change was remove the duplication on the getXXX nad xxx methods on the Response side. We probably took it a step too far with changing the request methods as well, there are benefits (also on teh groovy client) to use non settter methods.
</comment><comment author="derryos" created="2013-03-20T15:14:01Z" id="15181599">Hi there,

I have logged an issue on this in the forum:
https://groups.google.com/d/msg/elasticsearch/O3uzXQtyrKE/5kysBggsft8J

For many of these accessors that changed, the old version still exists (for deprecation, i guess?) e.g.:
GetResponse
exists()    -&gt;    isExists() 
CountResponse
count()    -&gt;    getCount() 
SearchResponse
hits()    -&gt;    getHits() 
ClusterHealthResponse
status()    -&gt;    getStatus() 

This means that i _could_ easily go back to v0.20.4 from 0.90.0.Beta1 once i update my code to use the alternative accessor.

One, however, that i don't see  is:
BuldResponse
items() vs getItems()

This means that i can't easily switch between versions without code changes.

Is there are reason why some of the accessors have changed and not others? 
</comment><comment author="dadoonet" created="2013-03-20T17:33:43Z" id="15190967">I added the following comment in the forum:

I can't see old versions in repo:
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/get/GetResponse.java#L56
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/count/CountResponse.java#L51
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/search/SearchResponse.java#L91
https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/cluster/health/ClusterHealthResponse.java#L119

https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkResponse.java#L92

Where do you see old getters?
</comment><comment author="derryos" created="2013-03-20T17:53:12Z" id="15192198">After conversations with david above, it seems that this was down to me not having a clean maven project. The issue occured because i seemed to have co-existing es libraries (ie was switching my pom between versions, forcing a project clean and only issue i had was in the bulkResponse items() vs getItems call). Restarting eclipse and forcing a clean maven update means that i now only see the correct accessors.
</comment><comment author="randunel" created="2013-04-30T10:07:53Z" id="17219033">So we should use ES 0.20 until all the plugins/rivers have been updated, right? Could someone please post an official download link for the latest version until this change? Is this the one? http://www.elasticsearch.org/downloads/0-20-6/
</comment><comment author="derryos" created="2013-10-04T11:12:53Z" id="25691659">Just a comment on this - the java api doc (specifically using scrolls) on the elasticsearch site mixes hits() and getHits().
http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/search.html

for (SearchHit hit : scrollResp.getHits()) {
        //Handle the hit...
    }
    //Break condition: No hits are returned
    if (scrollResp.***_hits()**_.hits().length == 0) { //ES 0.90+ complains!
        break;
    }
</comment><comment author="spinscale" created="2013-10-06T19:21:53Z" id="25774631">Hey, thanks a lot for bringing this up! I just changed it in the documentation. Dont hesitate to create a new issue for outdated or wrong documentation!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>UncategorizedExecutionException in Terms Query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2656</link><project id="" key="" /><description>I got that weird UncategorizedExecutionException when i execute a terms - query through the API: 

QueryBuilder builder = QueryBuilders.boolQuery().must(QueryBuilders.termsQuery("foo", docIds.toArray())).must(QueryBuilders.rangeQuery(FIELD_INDEXED_AT).lt(dateTime));
SearchResponse searchResponse = getClient().prepareSearch(indexName).setQuery(builder).execute().actionGet(10*1000);

Here is what query builder generates: 

{
  "bool" : {
    "must" : [ {
      "terms" : {
        "foo" : [ "33425", "33862", "42689" ]
      }
    }, {
      "range" : {
        "indexedAt" : {
          "from" : null,
          "to" : "2013-02-15T22:03:10.556+01:00",
          "include_lower" : true,
          "include_upper" : true
        }
      }
    } ]
  }
}

If i send that terms query using HTTP Requestor, then everything seems ok. Query is valid and i get some hits from ES. We are using ES 0.19.8. 
</description><key id="11053437">2656</key><summary>UncategorizedExecutionException in Terms Query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bagdemir</reporter><labels /><created>2013-02-15T21:32:34Z</created><updated>2013-03-01T20:27:23Z</updated><resolved>2013-03-01T20:27:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-15T21:41:37Z" id="13629631">Can you gist the full stack trace that you get? Also, can you check the logs of the server and see if there are failures there?
</comment><comment author="bagdemir" created="2013-02-15T21:47:55Z" id="13629921">org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution
    at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:90)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:77)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:60)
    at com.foo.searchengine.service.ElasticSearchEngine.getGlobalIdsOfDocumentsToBeDeleted(ElasticSearchEngine.java:454)
    at com.foo.searchengine.service.ElasticSearchEngine.indexDocuments(ElasticSearchEngine.java:435)
    at com.foo.searchengine.service.ElasticSearchEngine.indexDocuments(ElasticSearchEngine.java:488)
    at com.foo.searchengine.service.SearchEngineServiceImpl.index(SearchEngineServiceImpl.java:371)
    at com.foo.searchengine.domain.jobs.imports.changeset.ChangeSetImport.callIndexService(ChangeSetImport.java:16)
    at com.foo.searchengine.domain.jobs.imports.changeset.AbstractChangeSetImport.synchronize(AbstractChangeSetImport.java:81)
    at com.foo.searchengine.domain.jobs.imports.changeset.AbstractChangeSetImport.processImport(AbstractChangeSetImport.java:57)
    at com.foo.searchengine.domain.jobs.imports.changeset.AbstractChangeSetImport.execute(AbstractChangeSetImport.java:35)
    at com.foo.searchengine.service.ElasticsearchIndexDocumentsIntegrationTest.executeImport(ElasticsearchIndexDocumentsIntegrationTest.java:194)
    at com.foo.searchengine.service.ElasticsearchIndexDocumentsIntegrationTest.importChangeSet(ElasticsearchIndexDocumentsIntegrationTest.java:188)
    at com.foo.searchengine.service.ElasticsearchIndexDocumentsIntegrationTest.testImportChangeSet(ElasticsearchIndexDocumentsIntegrationTest.java:123)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:74)
    at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:83)
    at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:72)
    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:231)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
    at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:174)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:258)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:92)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:71)
    ... 41 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.cluster.metadata.MetaData.convertFromWildcards(MetaData.java:551)
    at org.elasticsearch.cluster.metadata.MetaData.concreteIndices(MetaData.java:469)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.&lt;init&gt;(TransportSearchTypeAction.java:110)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:70)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:61)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:58)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:48)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:108)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)
    at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:206)
    at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:743)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
    ... 40 more
org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution
    at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:90)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:77)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:60)
    at com.foo.searchengine.service.ElasticSearchEngine.getGlobalIdsOfDocumentsToBeDeleted(ElasticSearchEngine.java:454)
    at com.foo.searchengine.service.ElasticSearchEngine.indexDocuments(ElasticSearchEngine.java:435)
    at com.foo.searchengine.service.ElasticSearchEngine.indexDocuments(ElasticSearchEngine.java:488)
    at com.foo.searchengine.service.SearchEngineServiceImpl.index(SearchEngineServiceImpl.java:371)
    at com.foo.searchengine.domain.jobs.imports.changeset.ChangeSetImport.callIndexService(ChangeSetImport.java:16)
    at com.foo.searchengine.domain.jobs.imports.changeset.AbstractChangeSetImport.synchronize(AbstractChangeSetImport.java:81)
    at com.foo.searchengine.domain.jobs.imports.changeset.AbstractChangeSetImport.processImport(AbstractChangeSetImport.java:57)
    at com.foo.searchengine.domain.jobs.imports.changeset.AbstractChangeSetImport.execute(AbstractChangeSetImport.java:35)
    at com.foo.searchengine.service.ElasticsearchIndexDocumentsIntegrationTest.executeImport(ElasticsearchIndexDocumentsIntegrationTest.java:194)
    at com.foo.searchengine.service.ElasticsearchIndexDocumentsIntegrationTest.importChangeSet(ElasticsearchIndexDocumentsIntegrationTest.java:188)
    at com.foo.searchengine.service.ElasticsearchIndexDocumentsIntegrationTest.testImportChangeSet(ElasticsearchIndexDocumentsIntegrationTest.java:123)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:74)
    at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:83)
    at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:72)
    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:231)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
    at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:71)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:174)
    at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)
    at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
    at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:258)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:92)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:71)
    ... 41 more
Caused by: java.lang.NullPointerException
    at org.elasticsearch.cluster.metadata.MetaData.convertFromWildcards(MetaData.java:551)
    at org.elasticsearch.cluster.metadata.MetaData.concreteIndices(MetaData.java:469)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.&lt;init&gt;(TransportSearchTypeAction.java:110)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:70)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.&lt;init&gt;(TransportSearchQueryThenFetchAction.java:61)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:58)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction.doExecute(TransportSearchQueryThenFetchAction.java:48)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:108)
    at org.elasticsearch.action.search.TransportSearchAction.doExecute(TransportSearchAction.java:43)
    at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)
    at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)
    at org.elasticsearch.client.support.AbstractClient.search(AbstractClient.java:206)
    at org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:743)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)
    at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)
    ... 40 more
</comment><comment author="bagdemir" created="2013-02-15T22:21:28Z" id="13631314">the first element of the aliasesOrIndices array passed as parameter to convertFromWildcards is null. server's log is clean. 
</comment><comment author="kimchy" created="2013-02-22T22:46:13Z" id="13976931">Can you maybe create a full repro curl of this failure? also, which version are you running?
</comment><comment author="s1monw" created="2013-03-01T20:27:23Z" id="14309597">this is caused by passing a null value to indices in the java API. I can reproduce in a test and pushed a fix.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Simplified range syntax when using a query string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2655</link><project id="" key="" /><description>Support the following simplified syntax for ranges: `age:&gt;10`, `age:&gt;=10`, `age:&lt;20`, `age:&lt;=20`.
</description><key id="11020297">2655</key><summary>Simplified range syntax when using a query string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.6</label><label>v0.90.0.Beta1</label></labels><created>2013-02-15T00:30:34Z</created><updated>2013-02-15T12:47:13Z</updated><resolved>2013-02-15T00:31:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-02-15T01:43:37Z" id="13589333">That is awesome!  :+1: :+1: 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support trailing slashes on plugin _site URLs</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2654</link><project id="" key="" /><description>For common plugins like head and paramedic, I often attempt to visit the plugin with this URL:

http://localhost:9200/_plugin/paramedic

but get a blank screen.  The correct URL has a trailing slash like so:

http://localhost:9200/_plugin/paramedic/

I'm assuming this is an elasticsearch issue rather than an individual plugin, but can we redirect to the URL with trailing slash to prevent the blank screen "why is $PLUGIN not working?" questions?

Thanks!
Andrew
</description><key id="11019034">2654</key><summary>Support trailing slashes on plugin _site URLs</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">ash211</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-02-14T23:41:56Z</created><updated>2013-05-06T12:55:22Z</updated><resolved>2013-02-19T08:50:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mahdeto" created="2013-02-17T10:21:43Z" id="13683766">+1
</comment><comment author="ash211" created="2013-02-20T07:19:34Z" id="13819127">Wow, quick fix dadoonet!  Thanks much for the fix, and love the test to make sure it doesn't regress.  Awesome experience, you guys are truly great and everyone appreciates the hard work you're putting into ES.  Rock on!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Clear cache: allow to invalidate specific filter cache keys</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2653</link><project id="" key="" /><description>If explicit `_cache_key` is associated with filters, allow to explicitly invalidate them. For example:

```
curl -XPOST localhost:9200/test/_cache/clear?filter_keys=key1,key2
```
</description><key id="11011696">2653</key><summary>Clear cache: allow to invalidate specific filter cache keys</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.6</label><label>v0.90.0.Beta1</label></labels><created>2013-02-14T20:12:40Z</created><updated>2013-02-14T20:13:24Z</updated><resolved>2013-02-14T20:13:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Searching for * and ? in the text </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2652</link><project id="" key="" /><description>I am having documents with text containing hotel\* , hotel*\* ,hotel**\* which represents hotels status. And I would like for exact search like 'hotel*****'.  I tried by escaping the \* but its not working. Is there is any way to implement this?
</description><key id="11002815">2652</key><summary>Searching for * and ? in the text </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">priyamadan</reporter><labels /><created>2013-02-14T16:20:32Z</created><updated>2013-02-14T21:39:25Z</updated><resolved>2013-02-14T21:39:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-02-14T16:30:22Z" id="13562206">Set the field to not_analyzed, then use a match query.  You should ask this type of question on the mailing list.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cached script filter is not used in consecutive searches</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2651</link><project id="" key="" /><description>To reproduce, start Elasticsearch in foreground mode, run this script [file-script_filter_cache.sh](https://gist.github.com/imotov/4948908#file-script_filter_cache-sh) and check the standard output of the Elasticsearch server.

Expected result: only first request should cause script to be executed

Actual result: script is executed for both requests
</description><key id="10991506">2651</key><summary>Cached script filter is not used in consecutive searches</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-02-14T11:11:53Z</created><updated>2013-02-14T11:30:33Z</updated><resolved>2013-02-14T11:30:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a way to randomly select which data directory to store on when using multiple data directories.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2650</link><project id="" key="" /><description>The store distributor settings `index.store.distributor` allows specifying how index files should be distributed among multiple store directories. Two types of distributors are supported. The `least_used` distributor always selects the directory with the most available space. The `random` distributor selects directories at random. The probability of selecting a particular directory is proportional to amount of available space in this directory.
## Original Request

When using multiple data directories all on different hard disks, if a new disk is added later on with 0% capacity used, then all ElasticSearch insertions will using this new disk until it is even with the others. This causes a large I/O load on the single disk and leaves the others untouched. Even worse, other processes that also share that new disk, such as Hadoop processes, end up being I/O blocked whenever they need to access it.

If a way to configure ElasticSearch to randomly select a data directory to write new data to was created, then the I/O load could still be distributed across all disks when new disks are added in.

Hadoop does random disk selection until a disk becomes almost full in which case it is excluded from the random selection. I think this is a good strategy but even just random all the time would prevent the new disk I/O problem.
</description><key id="10960007">2650</key><summary>Add a way to randomly select which data directory to store on when using multiple data directories.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">Awnedion</reporter><labels><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-02-13T16:18:03Z</created><updated>2013-02-21T19:04:10Z</updated><resolved>2013-02-20T03:10:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-02-20T03:15:47Z" id="13813687">Nice!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Warmers do not load field data cache for sorting on new segments</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2649</link><project id="" key="" /><description>Hi,

One the use cases for warmers is to pre-load the fields data cache with fields used for sorting. 

The documentation doesn't specify how to load a field for sorting, but the following loads the id field into the cache if you put a new warmer _after_ data has already been indexed:

PUT to /tests/_warmer/warmer1

``` json
{
    "query": {
        "match_all": {}
    },
    "sort": [
        { "id": { "order" : "desc" }}
    ]

}
```

However, new segments will not have the id field loaded. 

To reproduce follow this sequence (on a fresh node). I tried it on 0.20.4:
Create an empty tests index with the folldwing mapping:

``` json
{
   "test" : {
       "properties" : {
           "id" : { "type": "string" }
       }
   }
}
```
- Load the warmer:  PUT to /tests/_warmer/warmer1 

``` json
{
    "query": {
        "match_all": {}
    },
    "sort": [
        { "id": { "order" : "desc" }}
    ]

}
```
- Load two documents with the following structure (different ids): PUT to /tests/test/2

``` json
{
    "id": "2"
}
```
- Call refresh: GET to _refresh
- Get the nodes stats. Cache is empty: GET to /_cluster/nodes/stats

``` json
            "cache": {
               "field_evictions": 0,
               "field_size": "0b",
               "field_size_in_bytes": 0,
               "filter_count": 0,
               "filter_evictions": 0,
               "filter_size": "0b",
               "filter_size_in_bytes": 0,
               "bloom_size": "0b",
               "bloom_size_in_bytes": 0,
               "id_cache_size": "0b",
               "id_cache_size_in_bytes": 0
            },
```
- Put the warmer in again: PUT to /tests/_warmer/warmer1 

``` json
{
    "query": {
        "match_all": {}
    },
    "sort": [
        { "id": { "order" : "desc" }}
    ]

}
```
- Now the cache will be loaded: GET to /_cluster/nodes/stats :

``` json
"cache": {
               "field_evictions": 0,
               "field_size": "132b",
               "field_size_in_bytes": 132,
               "filter_count": 0,
               "filter_evictions": 0,
               "filter_size": "0b",
               "filter_size_in_bytes": 0,
               "bloom_size": "0b",
               "bloom_size_in_bytes": 0,
               "id_cache_size": "0b",
               "id_cache_size_in_bytes": 0
            },
```

Cheers,
Boaz

NOTE: This may just be a documentation issue as I'm guessing how you should "warm" a sort field. If this is the wrong way, I'd be happy know.
</description><key id="10959082">2649</key><summary>Warmers do not load field data cache for sorting on new segments</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bleskes</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-02-13T15:57:55Z</created><updated>2013-04-16T17:59:34Z</updated><resolved>2013-02-13T16:51:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-13T16:50:58Z" id="13504099">Yea, we use `COUNT` search type, which does not kick in sorting, will change it to regular search. Btw, its much simpler to just have regular curl recreation, so we can more easily run it.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added Filter Factory supporting loading tokenizer using reflection</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2648</link><project id="" key="" /><description>Added support loading Tokenizers using reflection. E.G. loading NorwegianMinimalStemFilter : 
{
    "settings": {
        "analysis": {
            "filter": {
                "my_stemmer": {
                    "type": "class_token_filter",
                    "class": "org.apache.lucene.analysis.no.NorwegianMinimalStemFilter"
                }
            },
            "analyzer": {
                "default": {
                    "filter": ["standard", "lowercase", "my_stemmer"],
                    "tokenizer": "standard"
                }
            }
        }
    }
}
</description><key id="10953600">2648</key><summary>added Filter Factory supporting loading tokenizer using reflection</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msegelvik</reporter><labels /><created>2013-02-13T13:39:49Z</created><updated>2014-08-08T08:15:05Z</updated><resolved>2014-08-08T08:15:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T08:15:05Z" id="51574742">Thanks for the PR - We've talked about it and we'd certainly like to simplify how analyzers etc are incorporated into Elasticsearch, but not using reflection. I'm going to close this
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Fail when updating non dynamic index setting for an open index. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2647</link><project id="" key="" /><description>Currently when updating a non dynamic index setting for an open index only logs a warning and other index settings they may have been specified that are valid will be processed.

This can be confusing, since it is hard to find out what index settings have actually been updated. This issue makes sure that that the whole update request fails and specifies what non dynamic index settings are invalid.
</description><key id="10949492">2647</key><summary>Fail when updating non dynamic index setting for an open index. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-02-13T11:21:41Z</created><updated>2013-02-13T12:11:01Z</updated><resolved>2013-02-13T12:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Add a flag to the Segments API indicating a merge is in progress</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2646</link><project id="" key="" /><description>It would be great if the Segments API exposed which segments are currently being merged by Lucene.  

It would be even better if merging segments could be identified with a merge_id or similar.  This would allow visualization tools to determine which sets of segments are merging together, as opposed to simply merging.
</description><key id="10934087">2646</key><summary>Add a flag to the Segments API indicating a merge is in progress</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels /><created>2013-02-12T23:33:13Z</created><updated>2013-10-30T11:51:28Z</updated><resolved>2013-10-30T11:51:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="polyfractal" created="2013-10-30T11:51:28Z" id="27382709">Added in #3904 :)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk DeleteByQueryRequest</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2645</link><project id="" key="" /><description>Is it possible to send a few DeleteByQueryRequests using BulkRequestBuilder ? 
</description><key id="10927924">2645</key><summary>Bulk DeleteByQueryRequest</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">bagdemir</reporter><labels /><created>2013-02-12T21:39:06Z</created><updated>2014-08-08T10:14:11Z</updated><resolved>2014-08-08T10:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-12T21:39:59Z" id="13460616">no, not today. Might be in the future, but its not high on the priority list.
</comment><comment author="clintongormley" created="2014-08-08T10:14:11Z" id="51584677">No plans to support this. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Once TTL has been set it cannot be disabled.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2644</link><project id="" key="" /><description>curl -v -XPUT 'http://localhost:9200/machineindex/logs/_mapping' -d '
{
   "logs": {
       "_ttl": {
           "enabled": false
       }
   }
}
'

Response is: {"ok":true,"acknowledged":true}

When I check back with curl -XGET 'http://localhost:9200/machineindex/logs/_mapping?pretty=true' it is still shown as "enabled":true.
</description><key id="10925785">2644</key><summary>Once TTL has been set it cannot be disabled.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rzulf</reporter><labels /><created>2013-02-12T21:03:45Z</created><updated>2013-04-04T13:20:34Z</updated><resolved>2013-04-04T13:20:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-04-04T13:20:34Z" id="15896503">Fixed in master (PR #2845)
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>possibly incorrect use of Lucene OneMerge.totalBytesSize</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2643</link><project id="" key="" /><description>...  in TrackingConcurrentMergeScheduler.

These stack traces in the logs always precede extremely slow index times and the need to forcibly (with SIGKILL) restart Elasticsearch:

[19:43:57,851][WARN ][index.merge.scheduler ] [Droom, Doctor Anthony] [analytics][11] failed to merge
java.io.FileNotFoundException: _guwj_1.del
at org.elasticsearch.index.store.Store$StoreDirectory.fileLength(Store.java:448)
at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:303)
at org.apache.lucene.index.MergePolicy$OneMerge.totalBytesSize(MergePolicy.java:174)
at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:81)
at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:456)

Here are some who've been bitten by this bug:

https://groups.google.com/d/topic/elasticsearch/NCLWvGEz6dk/discussion
https://groups.google.com/d/topic/elasticsearch/7a0FKmqtbnM/discussion
http://elasticsearch-users.115913.n3.nabble.com/quot-Failed-to-merge-quot-java-io-FileNotFoundException-td3654491.html
http://elasticsearch-users.115913.n3.nabble.com/failed-to-mege-exception-td4021139.html

It looks like Lucene is being used inappropriately by Elasticsearch. See the response here:

https://issues.apache.org/jira/browse/LUCENE-3051?focusedCommentId=13576972&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13576972

We'll have to make this work with an older version of Elasticsearch, but I thought I'd raise a flag in case Lucene's suggested workaround (calling OneMerge.estimatedMergeBytes) is helpful.

Best,
Josh
</description><key id="10925574">2643</key><summary>possibly incorrect use of Lucene OneMerge.totalBytesSize</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">joshbronson</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-02-12T20:58:39Z</created><updated>2013-02-12T21:09:58Z</updated><resolved>2013-02-12T21:09:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-12T20:59:36Z" id="13457798">I just saw the issue in Lucene itself, working on a fix for 0.20 and master.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk execution while a shard is replication might send erroneous version conflict failures for certain items</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2642</link><project id="" key="" /><description /><key id="10914091">2642</key><summary>Bulk execution while a shard is replication might send erroneous version conflict failures for certain items</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-02-12T16:37:22Z</created><updated>2013-02-12T16:38:15Z</updated><resolved>2013-02-12T16:38:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index with no replicas might loose on going documents while relocating a shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2641</link><project id="" key="" /><description /><key id="10912424">2641</key><summary>Index with no replicas might loose on going documents while relocating a shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-02-12T16:01:58Z</created><updated>2013-02-12T16:25:42Z</updated><resolved>2013-02-12T16:25:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-12T16:25:42Z" id="13440843">Fixed by c4c0df80f3d6b9a8d95bcd1ae438b8ba73061b94.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to re-score Top-K query results with a secondary query,</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2640</link><project id="" key="" /><description># Rescore Feature

The rescore feature allows te rescore a document returned by a query based
on a secondary algorithm. Rescoring is commonly used if a scoring algorithm
is too costly to be executed across the entire document set but efficient enough
to be executed on the Top-K documents scored by a faster retrieval method. Rescoring
can help to improve precision by reordering a larger Top-K window than actually
returned to the user. Typically is it executed on a window between 100 and 500 documents
while the actual result window requested by the user remains the same. 
# Query Rescorer

The `query` rescorer executes a secondary query only on the Top-K results of the actual
user query and rescores the documents based on a linear combination of the user query's score
and the score of the `rescore_query`. This allows to execute any exposed query as a
`rescore_query` and supports a `query_weight` as well as a `rescore_query_weight` to weight the
factors of the linear combination. 
# Rescore API

The `rescore` request is defined along side the query part in the json request:

``` json
curl -s -XPOST 'localhost:9200/_search' -d {
  "query" : {
    "match" : {
      "field1" : {
        "query" : "the quick brown",
        "type" : "boolean",
        "operator" : "OR"
      }
    }
  },
  "rescore" : {
    "window_size" : 50,
    "query" : {
      "rescore_query" : {
        "match" : {
          "field1" : {
            "query" : "the quick brown",
            "type" : "phrase",
            "slop" : 2
          }
        }
      },
      "query_weight" : 0.7,
      "rescore_query_weight" : 1.2
    }
  }
}
```

Each `rescore` request is executed on a per-shard basis within the same roundtrip. Currently the rescore API
has only one implementation (the `query` rescorer) which modifies the result set in-place. Future developments
could include dedicated rescore results if needed by the implemenation ie. a pair-wise reranker.
_Note:_ Only regualr queries are rescored, if the search type is set to `scan` or `count` rescorers are not executed.
</description><key id="10912305">2640</key><summary>Add ability to re-score Top-K query results with a secondary query,</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-02-12T15:59:05Z</created><updated>2017-07-01T06:57:00Z</updated><resolved>2013-02-12T16:23:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-02-28T01:00:30Z" id="14210543">Any special reason for the rescore_query object?  Seems like something along the lines of the following would be more consistent with the api.

``` javascript
curl -s -XPOST 'localhost:9200/_search' -d '{
  "query" : {
    "match" : {
      "field1" : {
        "query" : "the quick brown",
        "type" : "boolean",
        "operator" : "OR"
      }
    }
  },
  "rescore" : {
    "window_size" : 50,
    "query_weight" : 0.7,
    "rescore_query_weight" : 1.2,
    "query" : {
      "match" : {
        "field1" : {
          "query" : "the quick brown",
          "type" : "phrase",
          "slop" : 2
        }
      }
    }
  }
}'
```
</comment><comment author="s1monw" created="2013-02-28T07:52:57Z" id="14221317">Hey Matt,

I agree this would be more consistent. The reason behind this is that there might be additional implementations for rescoring in the future that are not necessarily use a query. So the "query" attribute is a context marker here so we can add additional rescorers in the future or open up this api for extension more easily. Does this make sense?
</comment><comment author="mattweber" created="2013-02-28T16:29:29Z" id="14242133">It does when I look how the parsing is implemented.  Maybe just switch query with rescore_query?  Doesn't really matter, it just seemed out of place to me when I was looking at it.

``` javascript
curl -s -XPOST 'localhost:9200/_search' -d '{
  "query" : {
    "match" : {
      "field1" : {
        "query" : "the quick brown",
        "type" : "boolean",
        "operator" : "OR"
      }
    }
  },
  "rescore" : {
    "window_size" : 50,
    "rescore_query" : {
      "query" : {
        "match" : {
          "field1" : {
            "query" : "the quick brown",
            "type" : "phrase",
            "slop" : 2
          }
        }
      },
      "query_weight" : 0.7,
      "rescore_query_weight" : 1.2
    }
  }
}'
```
</comment><comment author="Kaidanov" created="2014-07-28T10:29:54Z" id="50322806">how can you rescore and sort together ? 
</comment><comment author="xritchie" created="2014-08-13T11:28:00Z" id="52036667">Hi Guys,

I'm not entirely sure if I found a Bug or if the functionality is intended to work like so.

Basically I have a rescore query together with multiple sort fields if I remove the sort functionality the query works as intended and sorting is done on the rescore query, If I use any kind of Sort, the rescore query is totally ignored and sorting is done on the original query score.

To put stuff into perspective the following query is sorted on the original query score and the re score query is totally ignored.

```
{
    "from": 0,
    "size": 10,
    "explain": false,
    "sort": ["_score", {
        "networks": {
            "order": "desc",
            "mode": "sum"
        }
    }, {
        "rich": {
            "order": "desc",
            "mode": "sum"
        }
    }, {
        "picture": {
            "order": "desc",
            "mode": "sum"
        }
    }],
    "query": {
        "filtered": {
            "query": {
                "bool": {
                    "should": [{
                        "constant_score": {
                            "query": {
                                "match": {
                                    "_all": {
                                        "query": "Daryl"
                                    }
                                }
                            },
                            "boost": 1.0
                        }
                    }, {
                        "constant_score": {
                            "query": {
                                "match": {
                                    "_all": {
                                        "query": "Davies"
                                    }
                                }
                            },
                            "boost": 1.0
                        }
                    }, {
                        "constant_score": {
                            "query": {
                                "match": {
                                    "_all": {
                                        "query": "php"
                                    }
                                }
                            },
                            "boost": 1.0
                        }
                    }, {
                        "constant_score": {
                            "query": {
                                "match": {
                                    "_all": {
                                        "query": "developer"
                                    }
                                }
                            },
                            "boost": 1.0
                        }
                    }],
                    "disable_coord": 1
                }
            },
            "filter": [{
                "or": [{
                    "query": {
                        "match": {
                            "_all": {
                                "query": "Daryl"
                            }
                        }
                    }
                }, {
                    "query": {
                        "match": {
                            "_all": {
                                "query": "Davies"
                            }
                        }
                    }
                }, {
                    "query": {
                        "match": {
                            "_all": {
                                "query": "php"
                            }
                        }
                    }
                }, {
                    "query": {
                        "match": {
                            "_all": {
                                "query": "developer"
                            }
                        }
                    }
                }]
            }]
        }
    },
    "rescore": [{
        "query": {
            "query_weight": 0.0,
            "rescore_query_weight": 1.0,
            "score_mode": "total",
            "rescore_query": {
                "constant_score": {
                    "query": {
                        "match_all": {}
                    },
                    "boost": 20.0
                }
            }
        },
        "window_size": 50
    }]
}
```
</comment><comment author="clintongormley" created="2014-08-13T11:38:18Z" id="52037436">@xritchie see https://github.com/elasticsearch/elasticsearch/issues/6788
</comment><comment author="deepkg" created="2016-11-18T05:45:48Z" id="261453951">Can we provide complex queries for re-scoring?
For example : a function for linear sum of attributes with some weights for each attributes.
Also, how can be plug the response to be sorted after getting a final score(x_initial pass score + y_re-ranked score)?
</comment><comment author="lanpay-lulu" created="2017-07-01T06:55:21Z" id="312415277">Will rescore query drop some results that should be returned? In the definition above, I don't thik it should, but actually it can. So I was totally confused.
For example, without rescore query, the result number is 100; But with rescore query, the result number drops to 93.</comment></comments><attachments /><subtasks /><customfields /></item><item><title>added norwegian minimal stemmer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2639</link><project id="" key="" /><description>Exposing lucene norwegian minimal stemmer
</description><key id="10909633">2639</key><summary>added norwegian minimal stemmer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">msegelvik</reporter><labels /><created>2013-02-12T14:52:28Z</created><updated>2014-07-09T14:22:38Z</updated><resolved>2013-02-12T15:37:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-12T15:00:04Z" id="13436047">Thanks for bringing this up. I think we should call it `norwegian_minimal` instead of `norwegian_minimal_stemmer`. This is more in line with other stemfilters, like `minimal_german`.
</comment><comment author="msegelvik" created="2013-02-12T15:10:08Z" id="13436563">assumed you wanted minimal_norwegian(like minimal_german) 
</comment><comment author="martijnvg" created="2013-02-12T15:19:57Z" id="13437087">Oops :) Yes, I meant that. Looks good, can you squash the two commits into one commit?
</comment><comment author="msegelvik" created="2013-02-12T15:26:22Z" id="13437446">done :)
</comment><comment author="msegelvik" created="2013-02-12T15:32:17Z" id="13437782">Is there a good reason why we don't load TokenStream using reflection as a fallback? Seems a bit cumbersom to do this mapping for every stemmer in lucene.
</comment><comment author="martijnvg" created="2013-02-12T15:37:34Z" id="13438076">Thanks! pushed to master
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>In bulk API, Generated _id failed if there is _id field</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2638</link><project id="" key="" /><description>I tried to PUT this data using the bulk API

``` json
{
  "index":
    {
      "_index":"someIndex",
      "_type":"someType"
    }
},
{
  "name":"Some Field",
  "_id":"505572696f6de74d11000001"
}
```

But it resulted in this error:

```
 org.elasticsearch.index.mapper.MapperParsingException: Provided id [nCqxvjyUQgWwURj7hSj26Q] does not match the content one [505572696f6de74d11000001]
```
</description><key id="10896321">2638</key><summary>In bulk API, Generated _id failed if there is _id field</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">wlzch</reporter><labels /><created>2013-02-12T05:58:13Z</created><updated>2015-05-29T14:17:32Z</updated><resolved>2014-08-08T10:13:50Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-02-12T09:10:42Z" id="13423688">I'm afraid you can't do that. To have fast bulk operations, bulk does not read the payload but only send it to the right shard using it's ID. So, ID can not be read from the payload. If you don't pass it to the bulk request, one ID is generated.

What you should try is to use the same ID as _id when creating your IndexRequest:

``` javascript
{
  "index":
    {
      "_index":"someIndex",
      "_type":"someType",
      "_id":"505572696f6de74d11000001"
    }
}
{
  "name":"Some Field",
  "_id":"505572696f6de74d11000001"
}
```

This should work.
</comment><comment author="wlzch" created="2013-02-13T06:08:40Z" id="13475923">Is it okay if I use my own ID and not the generated ID? Is there any performance impact with the index?
</comment><comment author="aliakhtar" created="2015-05-29T09:58:37Z" id="106762313">I encountered the same issue. In the java driver api, an id can be set explicitly for bulk requests:

``` java
BulkRequestBuilder req = client.prepareBulk();
IndexRequestBuilder indexReq = client.prepareIndex(index, type);
indexReq.setId(id).setSource(json);
req.add(indexReq);
```
</comment><comment author="clintongormley" created="2015-05-29T13:41:46Z" id="106805049">In v2, we're removing the ability to set the `_id` in the document body.  See https://github.com/elastic/elasticsearch/pull/11074
</comment><comment author="aliakhtar" created="2015-05-29T13:56:56Z" id="106811449">Whats the alternative? Setting I'd this way is very useful for upserts,
where you want the record to be updated if it exists or inserted if it
doesn't, and don't care whichever the case is.
On May 29, 2015 6:42 PM, "Clinton Gormley" notifications@github.com wrote:

&gt; In v2, we're removing the ability to set the _id in the document body.
&gt; See #11074 https://github.com/elastic/elasticsearch/pull/11074
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/2638#issuecomment-106805049
&gt; .
</comment><comment author="clintongormley" created="2015-05-29T14:09:26Z" id="106817934">Set the _id in the metadata line. That works in the same way for upserts.
</comment><comment author="aliakhtar" created="2015-05-29T14:14:58Z" id="106820463">Will there be  method in the java driver for setting that per document?
When doing bulks?

Thanks.
On May 29, 2015 7:10 PM, "Clinton Gormley" notifications@github.com wrote:

&gt; Set the _id in the metadata line. That works in the same way for upserts.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elastic/elasticsearch/issues/2638#issuecomment-106817934
&gt; .
</comment><comment author="clintongormley" created="2015-05-29T14:17:31Z" id="106821859">yes
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Search's result is not sorted if _cache option is false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2637</link><project id="" key="" /><description>When I have searched with "_cache": "false" option, I got unsorted result
- search query : 
  {
  "from": 0,
  "size": 1000,
  "filter": {
    "and": [
      {
        "term": {
          "content": "signature31",
          "_cache": "false"
        }
      },
      {
        "term": {
          "no": "31",
          "_cache": "false"
        }
      },
      {
        "range": {
          "evt_time": {
            "from": "2013-02-07 00:02:00",
            "to": "2013-02-07 23:20:59"
          },
          "_cache": "false"
        }
      }
    ],
    "_cache": "false"
  },
  "sort": [
    {
      "evt_time": {
        "order": "desc"
      }
    }
  ]
  }

![search_result](https://f.cloud.github.com/assets/3538511/147718/dcfa7c22-74cc-11e2-8242-35becfd4e1cb.png)
</description><key id="10894997">2637</key><summary>Search's result is not sorted if _cache option is false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">KangYongKyun</reporter><labels /><created>2013-02-12T04:25:06Z</created><updated>2014-08-08T10:13:04Z</updated><resolved>2014-08-08T10:13:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-02-12T12:43:19Z" id="13430856">It looks like a bug in parser. The filter element parser doesn't expect to see the "_cache" element, gets confused and ignores the rest of the query. It's not really related to cache. You can actually replace the last `"_cache": "false"` with `"foo":"bar"` and you will get the same result.

To make this query work, you can simply remove the last  `"_cache": "false"`. The `and` filter is not cached by default. You can enable caching for this filter by adding `_cache` field, but it requires slightly different syntax. See the last example on the [And Filter](http://www.elasticsearch.org/guide/reference/query-dsl/and-filter.html) page for more details. By the way, you might get better performance by replacing the top level filter with a filtered query.
</comment><comment author="KangYongKyun" created="2013-02-13T01:28:34Z" id="13469974">I removed "_cache" option of and filter, but cache size is increased. It looks like "_cache" : "true" of and filter by default
search query : 
{
  "from": 0,
  "size": 1000,
  "filter": {
    "and": [
      {
        "term": {
          "content": "signature31",
          "_cache": "false"
        }
      },
      {
        "term": {
          "no": "31",
          "_cache": "false"
        }
      },
      {
        "range": {
          "evt_time": {
            "from": "2013-02-12 00:02:00",
            "to": "2013-02-12 23:20:59"
          },
          "_cache": "false"
        }
      }
    ]
  },
  "sort": [
    {
      "evt_time": {
        "order": "desc"
      }
    }
  ]
}

![search_result1](https://f.cloud.github.com/assets/3538511/151476/a1526c1a-757c-11e2-8f0d-f0b62521f158.png)
</comment><comment author="imotov" created="2013-02-13T02:11:48Z" id="13471187">Could it be because of the filter on the type? Run the following query first, check the size of the filter cache, then run your query and see if the cache size increases or not.

```
{
    "query" : {
        "match_all" : {
         }
    }
}
```
</comment><comment author="KangYongKyun" created="2013-02-13T02:34:56Z" id="13471716">If I search with your query, Filter and field cache is not increased. But I search with my query, filter cache is not increased. But filed cache is increased. 
If I search with my query and "_cache" : "false" of and filter, filter and field cache is not increased. But result is not sorted.
</comment><comment author="imotov" created="2013-02-13T02:38:30Z" id="13471821">Oh, I didn't realize that you were asking about field cache. The field cache is increased because of sort, not because of the filter. In order to perform sorting the field that you are sorting on has to be loaded into field cache, there is really no way around it.
</comment><comment author="KangYongKyun" created="2013-02-13T04:22:13Z" id="13473964">Thanks imotov. 
I realize!!
and filter is "_cache" : "false" by default. but if I use explicitly "_cache" : "false" to and filter with sort, result is not sorted. 
</comment><comment author="clintongormley" created="2014-08-08T10:13:04Z" id="51584577">This original issue was caused by invalid query syntax, which now throws an exception.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Constantly falls elasticsearch</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2636</link><project id="" key="" /><description>As soon as we give the load on the elasticsearch we have it falls down and stops responding. We do not put anything except the elasticsearch and java, it can be used in some sort of bundle for better performance?

[2013-02-09 06:02:20,645][INFO ][monitor.jvm              ] [Maddicks, Artie] [gc][ConcurrentMarkSweep][87951][553] duration [5s], collections [1]/[5s], total [5s]/[6.9m], memory [988mb]-&gt;[988.1mb]/[989.8mb], all_pools {[Code Cache] [6.6mb]-&gt;[6.6mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[273mb]/[273mb]}{[Par Survivor Space] [32.5mb]-&gt;[32.6mb]/[34.1mb]}{[CMS Old Gen] [682.4mb]-&gt;[682.4mb]/[682.6mb]}{[CMS Perm Gen] [29.9mb]-&gt;[29.9mb]/[166mb]}
[2013-02-09 06:06:50,083][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2013-02-09 06:08:16,812][WARN ][http.netty               ] [Maddicks, Artie] Caught exception while handling client http traffic, closing connection [id: 0xfa4b4fa0, /127.0.0.1:59288 =&gt; /127.0.0.1:9200]
java.lang.OutOfMemoryError: Java heap space
[2013-02-09 06:07:27,220][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2013-02-09 06:07:16,069][WARN ][http.netty               ] [Maddicks, Artie] Caught exception while handling client http traffic, closing connection [id: 0x7b6365ce, /127.0.0.1:59043 =&gt; /127.0.0.1:9200]
java.lang.OutOfMemoryError: Java heap space
[2013-02-09 06:08:50,491][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: initializing ...
[2013-02-09 06:08:50,495][INFO ][plugins                  ] [Hyperstorm] loaded [], sites []
[2013-02-09 06:08:52,982][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: initialized
[2013-02-09 06:08:52,982][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: starting ...
[2013-02-09 06:08:53,147][INFO ][transport                ] [Hyperstorm] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/144.76.16.174:9300]}
[2013-02-09 06:08:56,180][INFO ][cluster.service          ] [Hyperstorm] new_master [Hyperstorm][L-ivSLvZTdShkyDefd3naw][inet[/144.76.16.174:9300]], reason: zen-disco-join (elected_as_master)
[2013-02-09 06:08:56,202][INFO ][discovery                ] [Hyperstorm] elasticsearch/L-ivSLvZTdShkyDefd3naw
[2013-02-09 06:08:56,214][INFO ][http                     ] [Hyperstorm] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/144.76.16.174:9200]}
[2013-02-09 06:08:56,215][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: started
[2013-02-09 06:08:57,517][INFO ][gateway                  ] [Hyperstorm] recovered [1] indices into cluster_state
[2013-02-09 06:57:34,329][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: stopping ...
[2013-02-09 06:57:37,383][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: stopped
[2013-02-09 06:57:37,383][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: closing ...
[2013-02-09 06:57:39,274][INFO ][node                     ] [Hyperstorm] {0.20.4}[14686]: closed
[2013-02-09 07:00:03,057][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: initializing ...
[2013-02-09 07:00:03,060][INFO ][plugins                  ] [Mammomax] loaded [], sites []
[2013-02-09 07:00:04,664][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: initialized
[2013-02-09 07:00:04,665][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: starting ...
[2013-02-09 07:00:04,742][INFO ][transport                ] [Mammomax] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/144.76.16.174:9300]}
[2013-02-09 07:00:07,758][INFO ][cluster.service          ] [Mammomax] new_master [Mammomax][xHW0IXt-Seit3VZfrvXUrA][inet[/144.76.16.174:9300]], reason: zen-disco-join (elected_as_master)
[2013-02-09 07:00:07,790][INFO ][discovery                ] [Mammomax] elasticsearch/xHW0IXt-Seit3VZfrvXUrA
[2013-02-09 07:00:07,815][INFO ][http                     ] [Mammomax] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/144.76.16.174:9200]}
[2013-02-09 07:00:07,815][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: started
[2013-02-09 07:00:08,831][INFO ][gateway                  ] [Mammomax] recovered [1] indices into cluster_state
[2013-02-09 10:05:38,203][WARN ][monitor.jvm              ] [Mammomax] [gc][ParNew][11115][1147] duration [1.5s], collections [1]/[4.7s], total [1.5s]/[13.9s], memory [950.5mb]-&gt;[804.5mb]/[989.8mb], all_pools {[Code Cache] [5.4mb]-&gt;[5.4mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[124.8mb]/[273mb]}{[Par Survivor Space] [30.6mb]-&gt;[0b]/[34.1mb]}{[CMS Old Gen] [646.7mb]-&gt;[679.6mb]/[682.6mb]}{[CMS Perm Gen] [29.8mb]-&gt;[29.8mb]/[166mb]}
[2013-02-09 10:22:30,313][INFO ][monitor.jvm              ] [Mammomax] [gc][ConcurrentMarkSweep][11717][432] duration [5.2s], collections [1]/[5.2s], total [5.2s]/[9.8m], memory [978mb]-&gt;[980.8mb]/[989.8mb], all_pools {[Code Cache] [5.5mb]-&gt;[5.5mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[273mb]/[273mb]}{[Par Survivor Space] [23.4mb]-&gt;[25.4mb]/[34.1mb]}{[CMS Old Gen] [681.5mb]-&gt;[682.3mb]/[682.6mb]}{[CMS Perm Gen] [29.8mb]-&gt;[29.8mb]/[166mb]}
[2013-02-09 11:02:34,499][WARN ][monitor.jvm              ] [Mammomax] [gc][ParNew][13091][1298] duration [2.4s], collections [1]/[5.6s], total [2.4s]/[17.2s], memory [971.6mb]-&gt;[805.2mb]/[989.8mb], all_pools {[Code Cache] [5.5mb]-&gt;[5.5mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[122.7mb]/[273mb]}{[Par Survivor Space] [32.2mb]-&gt;[0b]/[34.1mb]}{[CMS Old Gen] [666.2mb]-&gt;[682.6mb]/[682.6mb]}{[CMS Perm Gen] [29.8mb]-&gt;[29.8mb]/[166mb]}
[2013-02-09 14:28:56,659][INFO ][monitor.jvm              ] [Mammomax] [gc][ConcurrentMarkSweep][23397][4441] duration [11s], collections [2]/[11.5s], total [11s]/[1.6h], memory [989.6mb]-&gt;[989.8mb]/[989.8mb], all_pools {[Code Cache] [5.6mb]-&gt;[5.6mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[273mb]/[273mb]}{[Par Survivor Space] [33.9mb]-&gt;[34.1mb]/[34.1mb]}{[CMS Old Gen] [682.6mb]-&gt;[682.6mb]/[682.6mb]}{[CMS Perm Gen] [29.8mb]-&gt;[29.8mb]/[166mb]}
[2013-02-09 14:29:13,540][INFO ][monitor.jvm              ] [Mammomax] [gc][ConcurrentMarkSweep][23398][4444] duration [16.8s], collections [3]/[16.8s], total [16.8s]/[1.6h], memory [989.8mb]-&gt;[989.7mb]/[989.8mb], all_pools {[Code Cache] [5.6mb]-&gt;[5.6mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[273mb]/[273mb]}{[Par Survivor Space] [34.1mb]-&gt;[34mb]/[34.1mb]}{[CMS Old Gen] [682.6mb]-&gt;[682.6mb]/[682.6mb]}{[CMS Perm Gen] [29.8mb]-&gt;[29.8mb]/[166mb]}
[2013-02-09 14:30:30,642][INFO ][monitor.jvm              ] [Mammomax] [gc][ConcurrentMarkSweep][23399][4447] duration [16.5s], collections [3]/[16.5s], total [16.5s]/[1.6h], memory [989.7mb]-&gt;[989.8mb]/[989.8mb], all_pools {[Code Cache] [5.6mb]-&gt;[5.6mb]/[48mb]}{[Par Eden Space] [273mb]-&gt;[273mb]/[273mb]}{[Par Survivor Space] [34mb]-&gt;[34.1mb]/[34.1mb]}{[CMS Old Gen] [682.6mb]-&gt;[682.6mb]/[682.6mb]}{[CMS Perm Gen] [29.8mb]-&gt;[29.8mb]/[166mb]}
[2013-02-09 14:34:33,183][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.
java.lang.OutOfMemoryError: Java heap space
[2013-02-09 16:15:10,910][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: stopping ...
[2013-02-09 16:15:14,695][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: stopped
[2013-02-09 16:15:14,695][INFO ][node                     ] [Mammomax] {0.20.4}[16114]: closing ...
[2013-02-09 16:15:32,688][INFO ][node                     ] [Sin] {0.20.4}[22427]: initializing ...
[2013-02-09 16:15:32,691][INFO ][plugins                  ] [Sin] loaded [], sites []
[2013-02-09 16:15:34,479][INFO ][node                     ] [Sin] {0.20.4}[22427]: initialized
[2013-02-09 16:15:34,479][INFO ][node                     ] [Sin] {0.20.4}[22427]: starting ...
[2013-02-09 16:15:34,579][INFO ][transport                ] [Sin] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/144.76.16.174:9300]}
[2013-02-09 16:15:37,598][INFO ][cluster.service          ] [Sin] new_master [Sin][VPjABUm-REmy24NQ_AkXDQ][inet[/144.76.16.174:9300]], reason: zen-disco-join (elected_as_master)
[2013-02-09 16:15:37,625][INFO ][discovery                ] [Sin] elasticsearch/VPjABUm-REmy24NQ_AkXDQ
[2013-02-09 16:15:37,647][INFO ][http                     ] [Sin] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/144.76.16.174:9200]}
[2013-02-09 16:15:37,648][INFO ][node                     ] [Sin] {0.20.4}[22427]: started
[2013-02-09 16:15:38,530][INFO ][gateway                  ] [Sin] recovered [1] indices into cluster_state
</description><key id="10810224">2636</key><summary>Constantly falls elasticsearch</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nekulin</reporter><labels /><created>2013-02-09T15:24:07Z</created><updated>2016-04-26T02:04:07Z</updated><resolved>2013-02-09T16:07:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-02-09T15:47:36Z" id="13332877">You are not giving elasticsearch enough memory for what you are doing. 
</comment><comment author="nekulin" created="2013-02-09T15:59:26Z" id="13333073">How to understand how much he needed memory and how to increase it in the settings?
</comment><comment author="nekulin" created="2013-02-09T16:00:14Z" id="13333087">is there some sort of monitoring ?
</comment><comment author="imotov" created="2013-02-09T16:07:41Z" id="13333214">Yes, you need to monitor how much heap is used by calling [/_nodes/stats](http://www.elasticsearch.org/guide/reference/api/admin-cluster-nodes-stats.html). You should monitor the nodes as you increase the load and make sure that you always have enough memory. The [installation guide](http://www.elasticsearch.org/guide/reference/setup/installation.html) describes how to increase heap size. We also have a guide that describes [how you can get help with elasticsearch problems](http://www.elasticsearch.org/help/). Since this is not really a bug, I am going to close this issue. If you need more help, let's continue this discussion on [elasticsearch mailing list](https://groups.google.com/forum/?fromgroups=#!forum/elasticsearch). 
</comment><comment author="nekulin" created="2013-02-09T17:54:20Z" id="13335160">Ok https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/mrnnOc0TEoE
</comment><comment author="enilu" created="2016-04-26T02:04:07Z" id="214585163">thanks 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw correct exception when looking for discovery module</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2635</link><project id="" key="" /><description>Currently throws the ClassNotFoundException from the original module loading attempt (something unhelpful like "ec2") instead of the fully qualified class ES looks for in the end (i.e. "org.elasticsearch.discovery.ec2.Ec2DiscoveryModule")

Helped me debug classloader issues with OSGI.
</description><key id="10800291">2635</key><summary>Throw correct exception when looking for discovery module</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jjongsma</reporter><labels /><created>2013-02-08T23:01:28Z</created><updated>2014-07-10T17:25:49Z</updated><resolved>2013-03-01T20:59:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-09T08:15:37Z" id="13327987">Maybe we should put both classnames we try to load into the error message to prevent any confusion here? I agree we should include the last exception but I think it makes sense to have the short name in there as well? This is usually thrown on startup so we should be super verbose, no?
</comment><comment author="jjongsma" created="2013-02-12T23:08:01Z" id="13465087">The shortname (load attempt #1) is already included in the log message:

..."with value [" + get(setting) + "]...

There are three ClassNotFoundExceptions thrown and caught in here, are you referring to #2?
</comment><comment author="s1monw" created="2013-03-01T20:56:14Z" id="14310879">ah yeah I guess you are right... I will merge this soon.
</comment><comment author="s1monw" created="2013-03-01T20:59:46Z" id="14311053">pushed to master - thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow sorting by multi-valued fields </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2634</link><project id="" key="" /><description>Add sorting support for fields that have multiple values per document (For example: string array or double array).
By default when sorting on a multi-valued field the lowest or highest value will be picked from the field values depending on the sort order.

It is possible to define what value should be picked when a document has multiple values in a field via the `sort_mode` option. The following options can be specified:
- `min` - Pick the lowest value.
- `max` - Pick the highest value.
- `sum` - Use the sum of all values as sort value.
- `avg` - Use the average of all values as sort value.
## Example usage

```
curl -XPOST 'localhost:9200/_search' -d '{
   "query" : {
    ...
   },
   "sort" : [
      {"price" : {"order" : "asc", "sort_mode" : "avg"}}
   ]
}'
```

In the above example the field price has multiple prices per document. In this case the result hits will be sort by price ascending based on the average price per document.
</description><key id="10777595">2634</key><summary>Allow sorting by multi-valued fields </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-02-08T12:29:06Z</created><updated>2013-03-12T21:42:41Z</updated><resolved>2013-02-08T12:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-02-08T12:42:18Z" id="13288739">Any chance of making the min or max value configurable? eg:

```
sort: [ { foo: { order: "asc", value: "max"} ]
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>NullPointerException failed to dynamically refresh the mapping in cluster_state from shard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2633</link><project id="" key="" /><description>For a maintenance operation we disabled allocation and restart one node.

```
$ curl -XPUT localhost:9023/_cluster/settings -d '{"transient":{"cluster.routing.allocation.disable_allocation":"true"}}
# stop the node tpsmdt03s
# start the node tpsmdt03s
$ curl -XPUT localhost:9023/_cluster/settings -d '{"transient": "cluster.routing.allocation.disable_allocation":"false"}}
```

Unfortunately, something went wrong with ES: 

```
[2012-11-28 11:02:02,324][INFO ][cluster.routing.allocation.decider] [tpsmdt01s] updating [cluster.routing.allocation.disable_allocation] from [false] to [true]
[2012-11-28 11:08:13,170][INFO ][discovery.zen            ] [tpsmdt01s] master_left [[tpsmdt03s][6Sd72NOiTOayO8nPtlH9dg][inet[/10.26.165.12:9027]]], reason [transport disconnected (with verified connect)]
[2012-11-28 11:08:13,177][INFO ][cluster.service          ] [tpsmdt01s] master {new [tpsmdt01s][JIjtYHpkQBa8_GnvcLM4Nw][inet[/10.26.165.10:9027]], previous [tpsmdt03s][6Sd72NOiTOayO8nPtlH9dg][inet[/10.26.165.12:9027]]}, removed {[tpsmdt03s][6Sd72NOiTOayO8nPtlH9dg][inet[/10.26.165.12:9027]],}, reason: zen-disco-master_failed ([tpsmdt03s][6Sd72NOiTOayO8nPtlH9dg][inet[/10.26.165.12:9027]])
...
[2012-11-28 11:10:02,449][INFO ][cluster.metadata         ] [tpsmdt01s] [4f072ea3a623374256000001] update_mapping [50b5e2f946b88e97370000e5] (dynamic)
[2012-11-28 11:17:12,867][INFO ][cluster.service          ] [tpsmdt01s] added {[tpsmdt03s][m8ckMTZlQ1Kuvd8P7RJtfg][inet[/10.26.165.12:9027]],}, reason: zen-disco-receive(join from node[[tpsmdt03s][m8ckMTZlQ1Kuvd8P7RJtfg][inet[/10.26.165.12:9027]]])
[2012-11-28 11:18:23,379][INFO ][cluster.routing.allocation.decider] [tpsmdt01s] updating [cluster.routing.allocation.disable_allocation] from [true] to [false]
...
2012-11-28 14:40:02,615][INFO ][cluster.metadata         ] [tpsmdt01s] [4f072ea3a623374256000001] update_mapping [50b6143246b88e9737000172] (dynamic)
[2012-11-28 14:40:04,885][INFO ][cluster.metadata         ] [tpsmdt01s] [[4f072ea3a623374256000001]] remove_mapping [50b6143246b88e9737000172]
[2012-11-28 14:45:02,056][INFO ][cluster.metadata         ] [tpsmdt01s] 
...
[4f072ea3a623374256000001] update_mapping [50b6155d46b88e9737000185] (dynamic)
[2012-11-28 14:45:04,359][INFO ][cluster.metadata         ] [tpsmdt01s] [[4f072ea3a623374256000001]] remove_mapping [50b6155d46b88e9737000185]
[2012-11-28 14:45:04,482][WARN ][cluster.metadata         ] [tpsmdt01s] failed to dynamically refresh the mapping in cluster_state from shard
java.lang.NullPointerException
        at org.elasticsearch.cluster.metadata.MetaDataMappingService$1.execute(MetaDataMappingService.java:130)
        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:208)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
```

And now, I have a lot of WARN

```
[2013-02-08 10:45:04,324][INFO ][cluster.metadata         ] [tpsmdt01s] [[4f072ea3a623374256000001]] remove_mapping [5114c91de5c0e0950f0000d2]
[2013-02-08 10:45:04,459][INFO ][cluster.metadata         ] [tpsmdt01s] [[4f072ea3a623374256000001]] remove_mapping [5114c91d511a3418550000d4]
[2013-02-08 10:45:06,599][WARN ][cluster.metadata         ] [tpsmdt01s] failed to dynamically refresh the mapping in cluster_state from shard
java.lang.NullPointerException
[2013-02-08 10:45:06,964][WARN ][cluster.metadata         ] [tpsmdt01s] failed to dynamically refresh the mapping in cluster_state from shard
java.lang.NullPointerException
```

When I say a lot, I mean it ;)

```
$ grep 'failed to dynamically refresh the mapping in cluster_state from shard' search-0.19* | wc -l
58051
```
</description><key id="10774280">2633</key><summary>NullPointerException failed to dynamically refresh the mapping in cluster_state from shard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Filirom1</reporter><labels /><created>2013-02-08T10:29:30Z</created><updated>2013-11-08T07:05:47Z</updated><resolved>2013-08-09T12:26:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="Filirom1" created="2013-02-08T11:02:20Z" id="13285746">Deleting the index is not the solution:

```
$ curl -X DELETE http://tpsmdt03s:9023/4f072ea3a623374256000001
[2013-02-08 11:55:29,855][INFO ][cluster.metadata         ] [tpsmdt01s] [4f072ea3a623374256000001] deleting index
{"ok":true,"acknowledged":false}
```

```
$ curl -XPOST http://localhost:9023/4f072ea3a623374256000001/user2/ -d '{
    "name2" : "Shay Banon"
}'
[2013-02-08 12:01:24,479][INFO ][cluster.metadata         ] [tpsmdt01s] [4f072ea3a623374256000001] creating index, cause [auto(index api)], shards [6]/[1], mappings []
[2013-02-08 12:01:25,057][WARN ][cluster.metadata         ] [tpsmdt01s] failed to dynamically refresh the mapping in cluster_state from shard
java.lang.NullPointerException
```
</comment><comment author="Filirom1" created="2013-02-08T14:19:08Z" id="13292001">What I understand is that the cluster state is in a poor state.
It can not be updated because a NullPointerException is thrown at [MetaDataMappingService.java#L130](https://github.com/elasticsearch/elasticsearch/blob/v0.19.11/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L130)

Finally the Exception is catched at [MetaDataMappingService.java#L145](https://github.com/elasticsearch/elasticsearch/blob/v0.19.11/src/main/java/org/elasticsearch/cluster/metadata/MetaDataMappingService.java#L145) and the state is rolled back to the previsous one (the bad one).
It will be impossible for us to go away from this Error, because the cluster state is saved on disk and will be restored after a cold cluster restart.

Index creation and deletion are very slow:

```
$ time curl -XPOST http://localhost:9023/test2/user3/ -d '{
    "name" : "Shay Banon"
}'
{"ok":true,"_index":"test2","_type":"user3","_id":"OHCEGwWsRgKfQi-DMalzPA","_version":1}
real    0m43.143s
user    0m0.003s
sys 0m0.002s

$ time curl -XDELETE http://localhost:9023/test2/
{"ok":true,"acknowledged":false}
real    0m10.027s
user    0m0.001s
sys 0m0.004s

```

But once created, indexation are fast:

```
$ time curl -XPOST http://localhost:9023/test2/user3/ -d '{
    "name" : "Shay Banon"
}'
{"ok":true,"_index":"test2","_type":"user3","_id":"qGgGgHxNR3297PkdNA94Lg","_version":1}
real    0m0.010s
user    0m0.002s
sys 0m0.003s
```

And querying works:

```
$ time curl http://localhost:9023/test2/_search?q=name:Shay
{"took":2,"timed_out":false,"_shards":{"total":6,"successful":6,"failed":0},"hits":{"total":1,"max_score":0.19178301,"hits":[{"_index":"test2","_type":"user3","_id":"0jNFPBz-RieGCQ-vctYOoA","_score":0.19178301, "_source" : {"name":"Shay Banon"}}]}}
real    0m0.008s
user    0m0.002s
sys 0m0.002s
```
</comment><comment author="Filirom1" created="2013-02-13T08:52:05Z" id="13479585">Ok now, I have a better understanding of what happens on the cluster.

The long index creation time is not related to the NullPointerException : https://groups.google.com/d/msg/elasticsearch/hHniXBx8su8/VfEArmcpqQgJ

I am now able to reproduce the NullPointerException:
Two indexations in parallel, and one deletion on the same index/type.

```
curl -XPOST http://tpsmdt11s:9023/twitter/user -d '{
    "name" : "Shay Banon"
}' &amp;
curl -XPOST http://tpsmdt11s:9023/twitter/user -d '{
    "name" : "Hello You"
}' &amp;
curl -XDELETE http://tpsmdt11s:9023/twitter/user &amp;
```

```
[2013-02-13 09:48:21,369][INFO ][cluster.metadata         ] [tpsmdt11s] [twitter] update_mapping [user] (dynamic)
[2013-02-13 09:48:21,386][INFO ][cluster.metadata         ] [tpsmdt11s] [twitter] update_mapping [user] (dynamic)
[2013-02-13 09:48:21,401][INFO ][cluster.metadata         ] [tpsmdt11s] [[twitter]] remove_mapping [user]
[2013-02-13 09:48:21,479][WARN ][cluster.metadata         ] [tpsmdt11s] failed to dynamically refresh the mapping in cluster_state from shard
java.lang.NullPointerException
```

The same logs in DEBUG : https://gist.github.com/Filirom1/4943162.

I use a 4 nodes cluster in 0.19.11
</comment><comment author="imotov" created="2013-02-14T21:27:12Z" id="13579547">Can you reproduce the NullPointerException issue with 0.20.5 or master? 
</comment><comment author="spinscale" created="2013-08-09T12:26:34Z" id="22390962">Closing this as it is quite stale. Happy to reopen, if this still happens with a current 0.90.3 release. Please tell us!
</comment><comment author="lindstromhenrik" created="2013-11-07T14:32:43Z" id="27969248">I can't reproduce this locally but I can confirm that we see this error in 0.90.5:
[2013-11-07 15:26:39,574][WARN ][cluster.action.index     ] [NODE] failed to dynamically update the mapping in cluster_state from shard
java.lang.NullPointerException
</comment><comment author="spinscale" created="2013-11-07T21:32:14Z" id="28008493">do you have a stack trace as you could paste here?
</comment><comment author="javanna" created="2013-11-07T22:23:46Z" id="28012777">I think the NPE here might be related to `ClusterState#toString` method, probably solved with this commit: https://github.com/elasticsearch/elasticsearch/commit/123bc98d81ebd9777f9f739a51c6db00bc2bad5d
</comment><comment author="lindstromhenrik" created="2013-11-08T07:05:47Z" id="28040860">I'm working on getting the stack but I haven't been able to reproduce this in an environment that allows me to do this. 

Regarding the "toString()" fix we don't use templates so I don't think that is the problem.

My working hypothesis at the moment after looking at the mappings for indices that fails is that they all contain mappings where we have the same property name at different levels and that this also makes the merging of mappings to fail, causing a refresh-mapping giving the same NPE.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Malformed content, must start with an object</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2632</link><project id="" key="" /><description>Failed to execute [index {[tovar][prices][5114af927bffb0760a00012c], source[[

{"suma":"855","suma_retail":2100,"region_id":"1","city_id":"1","tovar_id":20253,"point_sales_id":1,"point_sales_user_id":"17","user_id":16,"categories_id":"1289"}

]]}]
Source
     "_id" : "5114af927bffb0760a00012c",
      "_score" : 1.0, "_source" : {"suma":"858","suma_retail":2145,"region_id":"1","city_id":"1","tovar_id":20253,"point_sales_id":1,"point_sales_user_id":"17","user_id":16,"categories_id":"1289"}

All the types match, why am I getting this error?
</description><key id="10771109">2632</key><summary>Malformed content, must start with an object</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nekulin</reporter><labels /><created>2013-02-08T08:20:43Z</created><updated>2013-02-08T10:50:25Z</updated><resolved>2013-02-08T10:21:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-02-08T10:21:06Z" id="13284317">your document must be an object, not an array of objects. so wrap it in an outer object, eg:

   { data: [{suma:....}]}
</comment><comment author="nekulin" created="2013-02-08T10:22:45Z" id="13284363">I have all we are going to have all the data change? and drive in the data
</comment><comment author="nekulin" created="2013-02-08T10:50:25Z" id="13285223">This problem is solved by not updating the document and append with the same _id
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort and dedupe names</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2631</link><project id="" key="" /><description>The diff looks messy due to sorting, but the dupes removed are:

Anelle
Animus
Arnim Zola
Atlas
Brass
Bulldozer
Cap 'N Hawk
Charles Xavier
Copperhead
Crusader
Destroyer
Destroyer, The
Drake, Frank
Druid
El Aguila
Eliminator
Frog-Man
Gauntlet
Ghost Girl
Ghost Rider
Ghoul
Hannibal King
Hellion
Inferno
Jennifer Walters
Justice
Lila Cheney
Living Pharaoh
Lorna Dane
Magus
Marvel Boy
Mastermind
Mentor
Meteor Man
Mimic
Mister Sinister
Misty Knight
Molly Hayes
Nocturne
Piper
Robert Kelly
Rusty Collins
Shola Inkosi
Solo
Stevie Hunter
Sunset Bain
Super Rabbit
Tempest
Trevor Fitzroy
Veil
Venom
Zebediah Killgrave
</description><key id="10745788">2631</key><summary>Sort and dedupe names</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jsummerfield</reporter><labels /><created>2013-02-07T17:10:13Z</created><updated>2014-06-23T14:38:44Z</updated><resolved>2014-02-06T12:33:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="javanna" created="2014-02-06T12:33:16Z" id="34318708">We recently did some cleanup work there, including resorting the node names and removing duplicates. I think we can close this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>During index creation, mapping can't reference an analyzer</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2630</link><project id="" key="" /><description>The create index API allows to provide mappings:

```
curl -XPOST localhost:9200/test1 -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "analysis" : {
        "analyzer": {
            "an_analyzer": { "type" : "keyword" }
        }
    },
    "mappings" : {
        "a_type" : {
            "properties" : {
                "a_field" : { "type" : "string" }
            }
        }
    }
}'
```

&lt;br/&gt;
But I can't use an analyzer in the mapping at this time, like in the following example:

```
curl -XPOST localhost:9200/test2 -d '{
    "settings" : {
        "number_of_shards" : 1
    },
    "analysis" : {
        "analyzer": {
            "an_analyzer": { "type" : "keyword" }
        }
    },
    "mappings" : {
        "a_type" : {
            "properties" : {
                "a_field" : { "type" : "string", "analyzer" : "an_analyzer" }
            }
        }
    }
}'
```

```
{"error":"MapperParsingException[mapping [a_type]]; nested: MapperParsingException[Analyzer [an_analyzer] not found for field [a_field]]; ","status":400}
```

Creating the index then putting the mapping works, but it is less convenient.
</description><key id="10732454">2630</key><summary>During index creation, mapping can't reference an analyzer</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Volune</reporter><labels /><created>2013-02-07T11:08:58Z</created><updated>2013-02-07T11:15:14Z</updated><resolved>2013-02-07T11:15:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-02-07T11:15:13Z" id="13231436">`analysis` should be passed in the `settings` section:

```
curl -XPUT 'http://127.0.0.1:9200/test2/?pretty=1'  -d '
{
   "mappings" : {
      "a_type" : {
         "properties" : {
            "a_field" : {
               "type" : "string",
               "analyzer" : "an_analyzer"
            }
         }
      }
   },
   "settings" : {
      "number_of_shards" : 1,
      "analysis" : {
         "analyzer" : {
            "an_analyzer" : {
               "type" : "keyword"
            }
         }
      }
   }
}
'
```
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Sort on multi-value numeric fields succeeds incorrectly</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2629</link><project id="" key="" /><description>Instead of throwing an error when trying to sort on a numeric multi-value field, the sort succeeds by just using the first value indexed:

```
curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "foo" : [
      5,
      10
   ]
}
'

curl -XGET 'http://127.0.0.1:9200/_all/_search?pretty=1'  -d '
{
   "sort" : [
      {
         "foo" : "desc"
      }
   ]
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_source" : {
#                "foo" : [
#                   5,
#                   10
#                ]
#             },
#             "sort" : [
#                5
#             ],
#             "_score" : null,
#             "_index" : "test",
#             "_id" : "SxS2qn1MQqyBARTje4ENcA",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : null,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 5
# }
```
</description><key id="10728908">2629</key><summary>Sort on multi-value numeric fields succeeds incorrectly</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2013-02-07T09:01:50Z</created><updated>2013-02-08T12:33:04Z</updated><resolved>2013-02-08T12:33:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-08T12:33:04Z" id="13288481">Fixed in #2634
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Updating non dynamic index level settings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2628</link><project id="" key="" /><description>When trying to update the index settings via UpdateSettings API, we are running into an issue when updating non dynamic settings, gives an impression that the settings has been applied but actually its not the case.

**Scenario 1:**
**updating dynamic settings**

``` bash
curl -XPUT localhost:9200/testindex/_settings -d '{"index":{"number_of_replicas":"3"}}'
```

the request succeeds with 

``` json
{"ok":true}
```

**Scenario 2:**
**updating non dynamic settings**

``` bash
curl -XPUT localhost:9200/testindex/_settings -d '{"index":{"cache.filter.max_size":"-1"}}'
```

the request succeeds with 

``` json
{"ok":true}
```

and has a log entry

```
[White Pilgrim] [testindex] ignoring non dynamic index level settings for open indices: [index.cache.filter.max_size]
```

Shouldn't the Scenario 2 be failing with the same error message instead of the log entry?
</description><key id="10724452">2628</key><summary>Updating non dynamic index level settings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hari-tw</reporter><labels /><created>2013-02-07T04:50:36Z</created><updated>2014-07-04T12:38:14Z</updated><resolved>2014-07-04T12:38:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-08T18:01:30Z" id="13302916">Currently the response either an empty response (ok: true) or an error message in the case an internal error occurs or a non changeable setting is updated (for example `index.number_of_shards`). Makes sense to have a response that tells what settings couldn't be updated or that tells the settings that have been updated.

Not sure what version you're using but the `index.cache.field.max_size` has been removed with issue #1590. 
Btw you can update not dynamic settings if you close the index first.
</comment><comment author="martijnvg" created="2013-02-13T12:13:51Z" id="13488164">See issue #2647 about the updated behaviour for the update index settings api.
</comment><comment author="clintongormley" created="2014-07-04T12:38:14Z" id="48039301">Closed in favour of #6732 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>fixes 2626 : GeoShape intersects filter no longer omits matching docs. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2627</link><project id="" key="" /><description>fixes 2626
</description><key id="10716269">2627</key><summary>fixes 2626 : GeoShape intersects filter no longer omits matching docs. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">IamJeffG</reporter><labels /><created>2013-02-06T23:09:55Z</created><updated>2014-07-08T01:07:16Z</updated><resolved>2013-02-18T17:47:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-18T17:47:54Z" id="13733887">merged
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoShape intersects filter omits matching docs.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2626</link><project id="" key="" /><description>There are some cases where a geo_shape "intersects" (and presumably "within", though I did not check) filter fails to find a matching document.

To reproduce, use a simple, standard mapping:

``` javascript
    {
        "location": {
            "type": "geo_shape",
            "tree": "quadtree",
            "distance_error_pct": 0.0
        }
    }
```

Index a document:

```
curl -XPUT 'localhost:9200/test/type1/b' -d '
{"id": "b",
 "location": {
    "type": "Polygon",
    "coordinates": [[[-122.83, 48.57],
                     [-122.77, 48.56],
                     [-122.79, 48.53],
                     [-122.83, 48.57]]]
 }
}
```

The following query intersects the document and therefore should match it:

```
curl -XGET 'localhost:9200/test/type1/_search' -d '
{
  "query": {
    "constant_score": {
      "boost": 1,
      "filter": {
        "geo_shape": {
          "location": {
            "relation": "intersects",
            "shape": {
              "type": "Polygon",
              "coordinates": [[[-122.88, 48.62],
                               [-122.88, 48.54],
                               [-122.82 ,48.54],
                               [-122.82, 48.62],
                               [-122.88, 48.62]]]
            }
          }
        }
      }
    }
  }
}'
```

However, the query returns 0 matches.

I have tracked the cause of this down to an attempted optimization in `org.elasticsearch.common.lucene.spatial.prefix.tree.SpatialPrefixTree#recursiveGetNodes`.  Specifically, the optimization prevents recursion into the deepest tree level if a parent node in the penultimate level covers all its children.

The bug is that this optimization can be invoked at both indexing and query time:  If the optimization is invoked for the indexed doc's shape, then a query shape that doesn't invoke the optimization will not match.  The converse is also true: if the optimization is invoked for a query shape, it will not match intersecting documents for whose shape the optimization was not invoked.  (I can provide illustrations if this remains unclear.)

One possible bugfix to ensure the indexing and query paths do not _both_ invoke the optimization is to simply disable the optimization at indexing time, at the one-time cost of more nodes being generated and stored at indexing time.  Or, the optimization could be disabled at query time, but at a cost of more nodes being generated at query time.
</description><key id="10715249">2626</key><summary>GeoShape intersects filter omits matching docs.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">IamJeffG</reporter><labels><label>bug</label><label>v0.90.0.Beta1</label></labels><created>2013-02-06T22:41:36Z</created><updated>2013-03-11T11:58:05Z</updated><resolved>2013-02-18T17:47:43Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="IamJeffG" created="2013-02-06T23:10:55Z" id="13211229">I've posted a pull request that fixes this bug: PR https://github.com/elasticsearch/elasticsearch/pull/2627
</comment><comment author="s1monw" created="2013-02-11T16:13:18Z" id="13387365">We are looking into this and try to fix it in Lucene first. I will keep you posted once we have any news on this or resolve this. Thanks for the PR - I will likely merge it in soon.
</comment><comment author="s1monw" created="2013-02-11T17:21:01Z" id="13391140">FYI there is a lucene issue open for this now: https://issues.apache.org/jira/browse/LUCENE-4770
</comment><comment author="IamJeffG" created="2013-02-11T23:30:58Z" id="13409972">Much appreciated, Simon.  Looks like David found (on the Lucene issue thread) that the PR is not quite sufficient: it also needs to check node.getLevel() != 0.
</comment><comment author="s1monw" created="2013-02-12T08:11:39Z" id="13422055">@IamJeffG I will wait until this lucene issue is resolved and then port the final fix. I will still pull your patch and the test and modify it if needed. thanks!
</comment><comment author="s1monw" created="2013-02-18T17:47:43Z" id="13733883">hey @IamJeffG I pulled this and pushed to master. Yet, since we removed the forked code from lucene today we will need to wait unitl lucene 4.2 is out to get this fixed. I hope this is not too much of a problem for you but I feel more comfortable with not having a forked version of this code but wait a little longer for the fix.
</comment><comment author="s1monw" created="2013-03-11T11:58:05Z" id="14709362">FYI - I enabled this test on master and it passes now after upgrading to Lucene 4.2

thanks again!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove XTermsFilter and UidFilter in favour of Lucene 4.1 TermsFilter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2625</link><project id="" key="" /><description>Removed XTermsFilter and UidFilters. Optimized creation of Uid types to prevent extra BytesRef allocation during UTF16 to UTF8 conversion. 
</description><key id="10702117">2625</key><summary>Remove XTermsFilter and UidFilter in favour of Lucene 4.1 TermsFilter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">chilling</reporter><labels /><created>2013-02-06T17:28:34Z</created><updated>2014-07-16T21:53:58Z</updated><resolved>2013-02-06T17:54:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-06T17:31:31Z" id="13194275">good stuff, thanks! I will review soon.
</comment><comment author="s1monw" created="2013-02-06T17:54:06Z" id="13195426">pushed, thanks
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Analyze API returns in YAML format if analyzed string begins with ---</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2624</link><project id="" key="" /><description>```
$ curl -XGET 'http://127.0.0.1:9200/_analyze?analyzer=standard' -d 'this is a test'
{"tokens":[{"token":"test","start_offset":10,"end_offset":14,"type":"&lt;ALPHANUM&gt;","position":4}]}

$ curl -XGET 'http://127.0.0.1:9200/_analyze?analyzer=standard' -d '---this is a test'

---
tokens:
- token: "test"
  start_offset: 13
  end_offset: 17
  type: "&lt;ALPHANUM&gt;"
  position: 4
```

Version:

```
$ curl -XGET 'http://127.0.0.1:9200/'
{
  "ok" : true,
  "status" : 200,
  "name" : "Multiple Man",
  "version" : {
    "number" : "0.20.4",
    "snapshot_build" : false
  },
  "tagline" : "You Know, for Search"
}
```
</description><key id="10698197">2624</key><summary>Analyze API returns in YAML format if analyzed string begins with ---</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">lennartkoopmann</reporter><labels><label>bug</label><label>v0.90.0.RC1</label></labels><created>2013-02-06T16:20:21Z</created><updated>2013-03-02T15:12:32Z</updated><resolved>2013-03-01T21:17:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-03-01T21:02:29Z" id="14311184">what is the rational behind this? I think you should be free to pass anything to this api. I can see people passing lucene query syntax to it and expect json like "-foo:bar +bar:foo"
</comment><comment author="kimchy" created="2013-03-01T21:05:33Z" id="14311339">we autodetect the content type to return based on the format of the request body, `---` is a representation of yaml, so we send it back in yaml. We should disable this behavior for the analyze APi, will fix it shortly.
</comment><comment author="lennartkoopmann" created="2013-03-02T15:12:32Z" id="14329516">Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Timeout with error: UnavailableShardsException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2623</link><project id="" key="" /><description>Recreation script: https://gist.github.com/lukas-vlcek/4722341
</description><key id="10690833">2623</key><summary>Timeout with error: UnavailableShardsException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">lukas-vlcek</reporter><labels /><created>2013-02-06T12:54:40Z</created><updated>2013-02-06T12:59:41Z</updated><resolved>2013-02-06T12:59:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="lukas-vlcek" created="2013-02-06T12:59:41Z" id="13180696">Oops, probably not an issue. It is caused by not having a chance to allocate enough replicas.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Should be used in shared env</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2622</link><project id="" key="" /><description>Hi guys,

Just a quick question. I am planing to use ES in a shared env where I create a Node then access it from multiple threads. Every time I need indexing I will call `node.client()`. Just want to ask if it is safe &amp; efficient to do so, any limit in the number of clients I can acquire.
</description><key id="10681937">2622</key><summary>Should be used in shared env</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">phungleson</reporter><labels /><created>2013-02-06T07:00:46Z</created><updated>2013-02-06T09:05:42Z</updated><resolved>2013-02-06T09:05:42Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-02-06T07:13:50Z" id="13169904">I recommend to use the same Client. It&#180;s threadsafe.
BTW, ask your questions on the mailing list, please. See http://www.elasticsearch.org/help/
</comment><comment author="phungleson" created="2013-02-06T09:05:42Z" id="13172608">Thanks! will do next time

Sorry a bit rush.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Throw a more meaningful message when no document is specified for indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2621</link><project id="" key="" /><description>Currently the a user forgets to send a document when indexing, they get a cryptic error that doesn't point to the actual cause:

```
&#8756; curl -s -XPUT localhost:9200/test/doc/1 | python -mjson.tool
{
    "error": "ElasticSearchParseException[Failed to derive xcontent from (offset=0, length=0): []]",
    "status": 400
}
```

This change makes the error message a bit more readable for someone who runs into this issue:

```
&#8756; curl -s -XPUT localhost:9200/test/doc/1 | python -mjson.tool
{
    "error": "MapperParsingException[failed to parse, document is empty]",
    "status": 400
}
```

The Exception originates from https://github.com/elasticsearch/elasticsearch/blob/ed09ba0a18ebe720db32b78e5146d4383525d014/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java#L456

I chose to make this a MapperParsingException to match the Exception that would normally get thrown in the case that a document didn't map correctly: https://github.com/elasticsearch/elasticsearch/blob/ed09ba0a18ebe720db32b78e5146d4383525d014/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java#L540
</description><key id="10677929">2621</key><summary>Throw a more meaningful message when no document is specified for indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">dakrone</reporter><labels /><created>2013-02-06T02:39:40Z</created><updated>2014-12-12T16:27:35Z</updated><resolved>2013-02-06T22:02:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-02-06T22:02:25Z" id="13207978">@dakrone thanks for raising this and adding a testcase - much appreciated. I just pushed this to master. 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Downloads page defaults to http and has no checksums</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2620</link><project id="" key="" /><description>G'day guys,

The downloads page on www.elasticsearch.org should probably default to https, also are there checksums or signatures available for your packages (specifically .debs for my usecase, but in general) available somewhere?
</description><key id="10674741">2620</key><summary>Downloads page defaults to http and has no checksums</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">richo</reporter><labels /><created>2013-02-06T00:17:27Z</created><updated>2013-03-02T06:41:06Z</updated><resolved>2013-03-01T22:01:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="drewr" created="2013-02-06T00:57:11Z" id="13161455">Hi! You can append `.sha1.txt` to any url to get a hash.  You're probably right that we should default to `https`.  We'll consider switching to that and also linking to the sha file from the website.
</comment><comment author="s1monw" created="2013-03-01T21:03:21Z" id="14311250">@drewr can we close this?
</comment><comment author="drewr" created="2013-03-01T21:04:42Z" id="14311308">I haven't changed the links yet. Lemme do that first.
</comment><comment author="drewr" created="2013-03-01T22:01:48Z" id="14313974">OK, https and sha1 links deployed!
</comment><comment author="s1monw" created="2013-03-02T06:41:06Z" id="14324093">thanks @drewr 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>HIghlight by wildcard fails when there are numeric fields matching highlight wildcard</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2619</link><project id="" key="" /><description>award.type has text and numeric properties inside. when using wildcard highlight for award.type\* searching on text value query fails with NumberFormatException

searching on numeric value instead of DCA works fine so it looks like it is trying to highlight numeric fields by converting search token to number which fails

Adding "require_field_match": true either within highlights or award.type.\* does not make any difference 

``` javascript
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "award.type.\\*:DCA"
        }
      }
    }
  },
  "highlight": {
    "fields": {
      "award.type.*": {}
    }
  }
}
```

error: SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[RLjd3ZLYRlqsoD5d3iOYBQ][award][0]: SearchParseException[[award][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"filtered":{"query":{"query_string":{"query":"award.type.\_:DCA"}}}},"highlight":{"fields":{"award.type._":{}}}}]]]; nested: NumberFormatException[For input string: "DCA"]; }]
status: 500
</description><key id="10670420">2619</key><summary>HIghlight by wildcard fails when there are numeric fields matching highlight wildcard</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-02-05T22:14:00Z</created><updated>2013-02-06T10:37:53Z</updated><resolved>2013-02-06T10:37:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="roytmana" created="2013-02-05T22:19:07Z" id="13155327">Correction it is not highligter (i think I did manage to make it fail with similar exception) it is actually the query itself that fails. If it is the expected behavior, it makes wildcard based multifield queries sort of useless

``` javascript
{
  "query": {
    "filtered": {
      "query": {
        "query_string": {
          "query": "award.type.\\*:DCA"
        }
      }
    }
  }
}
```
</comment><comment author="clintongormley" created="2013-02-06T10:37:43Z" id="13175801">On Tue, 2013-02-05 at 14:19 -0800, roytmana wrote:

&gt; Correction it is not highligter (i think I did manage to make it fail
&gt; with similar exception) it is actually the query itself that fails. If
&gt; it is the expected behavior, it makes wildcard based multifield
&gt; queries sort of useless

Try using the 'lenient' flag
http://www.elasticsearch.org/guide/reference/query-dsl/query-string-query.html
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support _search via form-encoded POST parameters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2618</link><project id="" key="" /><description>Many Ajax UI frameworks do not support passing parameters to ES as un-encode POST body. They do it via parameters. Currently ES GET supports that mode by recognizing source query string parameter as complete query JSON  and applying other url parameters (such as size, start, sort) to the body if any of the parameters are specified. I

The only problem is that in practice accepted  uri size is limited by browser (and potential ES transport implementations) and with larger ES queries (including facets, filters etc) can easily go over the limit

It would be great if the same functionality was supported via POST - if POST comes application/x-www-form-urlencoded encoded extract query from parameters otherwise from body as it is done now
</description><key id="10659556">2618</key><summary>Support _search via form-encoded POST parameters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-02-05T17:34:28Z</created><updated>2014-11-29T19:53:29Z</updated><resolved>2014-11-29T14:07:56Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-29T14:07:56Z" id="64952904">Hi @roytmana 

This feels like the wrong solution here.  Extending an Ajax framework to support JSON is a simple job.  And if not there, then you can do the transformation in an application layer between the browser and Elasticsearch. Elasticsearch shouldn't be exposed directly to users anyway.

Nobody else has show interest in this feature since this ticket was opened, so I'm going to close it.
</comment><comment author="roytmana" created="2014-11-29T19:53:29Z" id="64963166">That's ok to close it but itbis not about supporting json of course it is supported it is aboit supporting post not only with query json as post body but as form encoded parameters similar to currently supported get. It should be trivial just by checking for content type of the post to decide whether it is body payload or encoded parameters - really a low hanging fruit making integration easier in many cases
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support aggregations for "missing" and "other" in tems_stats facet</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2617</link><project id="" key="" /><description>Currently terms_stats facet will not calculate aggregations (I am particularly interested in total) for document missing the term or where term was not included in result due to facet size limit. When doing stats, users expect to see consistent breakdown of their data set  by term. without knowing aggregates for missing and other it is impossible to present user a consistent picture - think a pie-chart with segments for terms. To be complete and consistent it must have Other and Missing segments! 

In case by case scenario it may be possible to calculate multiple facets (statistical, terms and terms_stats) but even that would not allow for differentiation of aggregates for other and missing (except counts since they are coming from terms facet). 

Ideally it should follow terms facet in providing aggregations for missing and other terms.
</description><key id="10659133">2617</key><summary>Support aggregations for "missing" and "other" in tems_stats facet</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-02-05T17:25:01Z</created><updated>2014-08-08T10:07:20Z</updated><resolved>2014-08-08T10:07:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-08-08T10:07:20Z" id="51584122">As facets are deprecated, closing this in favour of #5324
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Better IndexFieldData stats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2616</link><project id="" key="" /><description>Resolves #2615

Adding a feature to have a more granular FieldData stats in the NodeIndicesStats endpoint

Missing is a proper REST response to contain the added data
</description><key id="10652083">2616</key><summary>Better IndexFieldData stats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-02-05T14:38:10Z</created><updated>2014-07-16T21:54:00Z</updated><resolved>2013-06-27T08:43:19Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Better FieldDataStats</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2615</link><project id="" key="" /><description>FieldDataStats was added in commit e8c1180 , but there's no public getter for it in NodeIndicesStats

Also, the only thing that is exposed through the FDS class is the actual memory used by it. It will be extremely useful to have that broken down by field names, so we can actually tell which FieldData is a bottleneck culprit.

Are there any plans to support that, or would you accept a PR enabling that?
</description><key id="10643823">2615</key><summary>Better FieldDataStats</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-02-05T10:03:17Z</created><updated>2013-06-26T16:32:49Z</updated><resolved>2013-06-26T16:32:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-06-26T16:24:06Z" id="20060868">Hey Itamar,

I just peeked at your PR, looks to me as if we expose the stats in the current master (implementation is a bit different due to preferring trove maps instead of normal hashmaps). Correct me, if I overlooked something.
</comment><comment author="synhershko" created="2013-06-26T16:32:48Z" id="20061503">Yeah, already noticed this but forgot to close...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>20.4 returns no results for empty filter in filtered query but worked in 19</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2614</link><project id="" key="" /><description>Easy to work around, but a breaking change for me from 19 to 20.  (My filter is generated and is sometimes empty)

curl http://localhost:9200/foo/bar/a -d '{test:1}'

19.11
curl http://localhost:9200/foo/bar/_search -d '{"query":{"filtered":{"query":{"match_all":{}},"filter":{}}}}'

{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"foo","_type":"bar","_id":"a","_score":1.0, "_source" : {test:1}}]}}

20.4
curl http://localhost:9200/foo/bar/_search -d '{"query":{"filtered":{"query":{"match_all":{}},"filter":{}}}}'

{"took":0,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}
</description><key id="10623479">2614</key><summary>20.4 returns no results for empty filter in filtered query but worked in 19</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">awick</reporter><labels /><created>2013-02-04T20:10:00Z</created><updated>2013-06-13T14:11:18Z</updated><resolved>2013-06-13T14:11:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="bensie" created="2013-04-04T20:11:15Z" id="15920755">I just encountered this same issue.

Like @awick said, not a hard thing to work around, but it's an undocumented change between 0.19.x and 0.20.x.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>When termsFilter is empty, use missingFilter instead</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2613</link><project id="" key="" /><description>As discussed in the mailing list: https://groups.google.com/d/topic/elasticsearch/4Foz1VD0vt8/discussion

Here is a pull request that perform a `missing` filter when terms array is empty. 
</description><key id="10599895">2613</key><summary>When termsFilter is empty, use missingFilter instead</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-02-04T09:15:04Z</created><updated>2014-06-15T02:57:04Z</updated><resolved>2014-03-07T23:47:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-01T07:44:39Z" id="33865907">I think we should close this PR now. @clintongormley WDYT?
</comment><comment author="dadoonet" created="2014-03-07T23:47:51Z" id="37080415">Closing as it will not be merge.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Explain API ignores search_type</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2612</link><project id="" key="" /><description>It appears that the Explain API ignores search_type.  I'm unsure if this is by design, or because of an underlying limitation, but thought it deserved an issue report.

Here is an example query showing scoring disparity due to low doc count (5 docs) spread across 5 shards.  Perfectly normal for a `query_then_fetch` search_type:

```
curl -XGET localhost:9200/startswith/test/_search?pretty -d '{                                                          "query": {
        "match_phrase_prefix": {
           "title": {
             "query": "d",
             "max_expansions": 5
           }
         }
       }
     }' | grep title

      "_score" : 1.0, "_source" : {"title":"drunk"}
      "_score" : 0.30685282, "_source" : {"title":"dzone"}
      "_score" : 0.30685282, "_source" : {"title":"data"}
      "_score" : 0.30685282, "_source" : {"title":"drive"}
```

Same query with search_type set to `dfs_query_then_fetch`, showing appropriate scoring:

```
 curl -XGET 'localhost:9200/startswith/test/_search?pretty=true&amp;search_type=dfs_query_then_fetch' -d '{
        "query": {
        "match_phrase_prefix": {
           "title": {
             "query": "d",
             "max_expansions": 5
           }
         }
       }
     }' | grep title

      "_score" : 1.9162908, "_source" : {"title":"dzone"}
      "_score" : 1.9162908, "_source" : {"title":"data"}
      "_score" : 1.9162908, "_source" : {"title":"drunk"}
      "_score" : 1.9162908, "_source" : {"title":"drive"}
```

Ok, with the verification out of the way, here are the corresponding Explain API examples for the "dzone" document, which in the above example has differential scoring depending on the search_type.  It appears that the Explain API completely ignores the search_type, since the score remains unaffected and maxDocs does not change.

```
curl -XGET 'localhost:9200/startswith/test/gvgXfR5YRSaXu7PfFc4FkA/_explain?pretty=true&amp;search_type=query_then_fetch' -d '{
        "query": {
        "match_phrase_prefix": {
           "title": {
             "query": "d",
             "max_expansions": 5
           }
         }
       }
     }'
{
  "ok" : true,
  "_index" : "startswith",
  "_type" : "test",
  "_id" : "gvgXfR5YRSaXu7PfFc4FkA",
  "matched" : true,
  "explanation" : {
    "value" : 0.30685282,
    "description" : "fieldWeight(title:dzone in 0), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "tf(termFreq(title:dzone)=1)"
    }, {
      "value" : 0.30685282,
      "description" : "idf(docFreq=1, maxDocs=1)"
    }, {
      "value" : 1.0,
      "description" : "fieldNorm(field=title, doc=0)"
    } ]
  }
```

And now dfs_query_then_fetch:

```
#curl -XGET 'localhost:9200/startswith/test/gvgXfR5YRSaXu7PfFc4FkA/_explain?pretty=true&amp;search_type=dfs_query_then_fetch' -d '{
        "query": {
        "match_phrase_prefix": {
           "title": {
             "query": "d",
             "max_expansions": 5
           }
         }
       }
     }'
{
  "ok" : true,
  "_index" : "startswith",
  "_type" : "test",
  "_id" : "gvgXfR5YRSaXu7PfFc4FkA",
  "matched" : true,
  "explanation" : {
    "value" : 0.30685282,
    "description" : "fieldWeight(title:dzone in 0), product of:",
    "details" : [ {
      "value" : 1.0,
      "description" : "tf(termFreq(title:dzone)=1)"
    }, {
      "value" : 0.30685282,
      "description" : "idf(docFreq=1, maxDocs=1)"
    }, {
      "value" : 1.0,
      "description" : "fieldNorm(field=title, doc=0)"
    } ]
  }
```
</description><key id="10594662">2612</key><summary>Explain API ignores search_type</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">polyfractal</reporter><labels><label>:Search</label><label>adoptme</label><label>bug</label><label>low hanging fruit</label></labels><created>2013-02-04T03:14:48Z</created><updated>2016-05-06T13:19:51Z</updated><resolved>2016-05-06T13:19:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-04T08:13:04Z" id="13065918">Hey, the explain api doesn't support the `search_type` parameter. I think adding support for this, makes sense to have distributed idf. 

The explain api behaves the same distribute wise as the get api. Maybe it makes better sense to add just support for global document frequency via a new `dfs` boolean parameter. The `search_type` defines how to execute a distributed search. For the explain api the `search_type` value `count` wouldn't make sense for the explain api.. 
</comment><comment author="beyang" created="2013-09-29T02:35:06Z" id="25313096">+1 on this.  It would be nice to do an apples-to-apples comparison for those of us using dfs_query_then_fetch when trying to figure out why a particular document is not showing up in search results.  Currently, we can only get the score explanation for the given document with the default query_then_fetch search strategy.
</comment><comment author="vaniaravinda" created="2015-08-03T10:47:41Z" id="127190302">Hi,
Can someone let me know how to use "search_type": "DFS_query_then_fetch" in Sense plugin query.

Thanks,
Vani Aravinda
</comment><comment author="clintongormley" created="2016-05-06T13:19:46Z" id="217437288">Duplicate of #15369
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding support for percolating on rolling indices</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2611</link><project id="" key="" /><description>By having an index called _global with mapping that match the mapping of the rolling indices in your system, and registering all percolator queries against this index, you can percolate and index with percolation and get results for all registered queries without worrying about registering all query on all indices in your system.

See discussion here: http://elasticsearch-users.115913.n3.nabble.com/Percolation-while-indexing-to-rolling-indexes-logs-td4028858.html
</description><key id="10584084">2611</key><summary>Adding support for percolating on rolling indices</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-02-03T12:56:30Z</created><updated>2014-06-13T02:38:36Z</updated><resolved>2013-08-13T21:49:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="synhershko" created="2013-02-03T14:57:26Z" id="13047994">One thing is missing - the ability to actually _have_ an index starting with an underscore. I'll leave it as is now - not sure about the naming anyway. This could be resolved rather easily by changing it to be percolator_global.
</comment><comment author="martijnvg" created="2013-08-13T13:28:12Z" id="22564410">In the new percolator in master, any index can contain percolator queries. These queries need to be indexed into the `_percolator` type. With this way of percolating this use case seems to be supported as well, right?
</comment><comment author="synhershko" created="2013-08-13T14:09:21Z" id="22567256">Technically yes, but that will require you to register all queries with all indexes. We also are quite heavy with percolation operations, and discovered its much easier to separate that from the cluster. We still wish it could be made better, but I'll post those complaints in another thread...

We solved this in a different manner, feel free to close this if you don't find it important
</comment><comment author="martijnvg" created="2013-08-13T19:57:36Z" id="22592714">Why would you need to register all queries with all indices? With the new percolator you can also register all your queries to one index.
</comment><comment author="synhershko" created="2013-08-13T21:16:56Z" id="22598275">I'll have to look at it more closely than, but its solved anyway for us. Feel free to close.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Master spanmultiterm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2610</link><project id="" key="" /><description>Hi there - for a project here at Meltwater, we want to make use of multi-term span queries in combination with wildcards, as supported by Lucene via the SpanMultiTermQueryWrapper.

We found ElasticSearch issue #2400 which mentions this problem, and with help from some consulting from Findwise we have implemented the functionality in the fork associated with this pull request.

Would be great to contribute this code back to mainline - let myself or @antonha know if there are any things about the code we could clean up or improve before inclusion.  We do have some test cases included with the changeset.
</description><key id="10565055">2610</key><summary>Master spanmultiterm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">jayaddison</reporter><labels /><created>2013-02-02T00:37:02Z</created><updated>2014-06-13T14:23:24Z</updated><resolved>2013-05-03T14:02:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-05-03T21:31:54Z" id="17419401">I am just curious, can we maybe use `span_multi` rather than `span_multi_term` here?
</comment><comment author="clintongormley" created="2013-05-06T06:04:54Z" id="17467481">Neither `span_multi` nor `span_multi_term` really described to me what this query does - I assumed it was some variation of and/or.  The "wrapper" part of the name seems important. That said `span_multi_term_query_wrapper` is way too verbose.

Perhaps it doesn't matter - people who are interested in span queries only have 5 variations to look at - it'll be easy to figure out how which query does what.  In which case I'd go for the more concise `span_multi`
</comment><comment author="speedplane" created="2013-05-21T12:01:02Z" id="18204247">Thanks for adding this. Any idea which release this will be first available in?
</comment><comment author="spinscale" created="2013-05-21T12:49:47Z" id="18206385">It is included in the [0.90 branch](https://github.com/elasticsearch/elasticsearch/tree/0.90) and will be released with the next 0.90 release
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filter cache stats may report incorrect values when index is recreated</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2609</link><project id="" key="" /><description>To reproduce, start a single node and as soon as node starts run the following script:

```
# Create and delete index a few times
for i in {1..4}
do
    curl -XDELETE localhost:9200/test
    curl -XPUT localhost:9200/test -d '{
        "settings": {
            "index.number_of_shards": 1,
            "index.number_of_replicas": 0
        }
    }'
    curl -XPUT localhost:9200/test/doc/1 -d '{"foo": "bar"}'
    curl -XPUT localhost:9200/test/doc/2 -d '{"foo": "bar"}'
    curl -XPUT localhost:9200/test/doc/3 -d '{"foo": "bar"}'
    curl -XPOST localhost:9200/test/_refresh
    # Do search to fill filter cache
    curl "localhost:9200/test/doc/_search"
done
# Sleep for indices.cache.filter.clean_interval
# Reduce it to make this repro run faster
echo
echo "Waiting 1 min for cache to be cleaned"
sleep 60
curl -s -XGET 'http://localhost:9200/_nodes/stats?pretty=true'
```

The result of this script is

```
...
        "cache" : {
          "filter_count" : -2,
          "filter_evictions" : 3,
          "filter_size" : "-48b",
          "filter_size_in_bytes" : -48,
          "id_cache_size" : "0b",
          "id_cache_size_in_bytes" : 0
        }
...
```

Analysis:
- [IndicesFilterCache](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/indices/cache/filter/IndicesFilterCache.java#L165) is using index name to dispatch removal notifications. 
- When index closes, it doesn't clean the cache immediately. So, it's possible that the cache for the old index will be cleaned when a new index with the same name is already created.
- As a result, it's possible for WeightedFilterCache to receive notifications targeted for the previous incarnations of the index with the same name. 
- WeightedFilterCache decrements stats counters without checking if removal notification was sent for this index or for one of its previous incarnations. 
</description><key id="10560235">2609</key><summary>Filter cache stats may report incorrect values when index is recreated</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">imotov</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-02-01T21:40:17Z</created><updated>2013-04-16T17:45:06Z</updated><resolved>2013-02-11T16:14:25Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>NullPointerException</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2608</link><project id="" key="" /><description>I'm using version 0.20.2. When I perform the following query (obviously flawed syntactically):

``` console
$ curl -XGET localhost:9200/foo/bar/_search -d '{"query": {"terms": {"query": "foo"}}}'
```

I get the following null pointer exception:

```
[2013-01-31 21:52:32,071][DEBUG][action.search.type       ] [Ent] [idents][0], node[mU366vK5T42xL01gFekyvw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1f2f0ce9]
org.elasticsearch.search.SearchParseException: [idents][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query": {"terms": {"query": "foo"}}}]]
  at org.elasticsearch.search.SearchService.parseSource(SearchService.java:566)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:481)
    at org.elasticsearch.search.SearchService.createContext(SearchService.java:466)
    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:236)
    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)
    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)
    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NullPointerException
    at org.elasticsearch.index.mapper.MapperService.smartName(MapperService.java:697)
    at org.elasticsearch.index.query.QueryParseContext.smartFieldMappers(QueryParseContext.java:264)
    at org.elasticsearch.index.query.TermsQueryParser.parse(TermsQueryParser.java:102)
    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:188)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:268)
    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:246)
    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)
    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:554)
    ... 11 more
```
</description><key id="10517027">2608</key><summary>NullPointerException</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/imotov/following{/other_user}', u'events_url': u'https://api.github.com/users/imotov/events{/privacy}', u'organizations_url': u'https://api.github.com/users/imotov/orgs', u'url': u'https://api.github.com/users/imotov', u'gists_url': u'https://api.github.com/users/imotov/gists{/gist_id}', u'html_url': u'https://github.com/imotov', u'subscriptions_url': u'https://api.github.com/users/imotov/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/655851?v=4', u'repos_url': u'https://api.github.com/users/imotov/repos', u'received_events_url': u'https://api.github.com/users/imotov/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/imotov/starred{/owner}{/repo}', u'site_admin': False, u'login': u'imotov', u'type': u'User', u'id': 655851, u'followers_url': u'https://api.github.com/users/imotov/followers'}</assignee><reporter username="">nvie</reporter><labels><label>bug</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-01-31T20:57:00Z</created><updated>2013-02-03T15:02:02Z</updated><resolved>2013-02-03T15:02:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Fix YAML content type detection for CharSequence</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2607</link><project id="" key="" /><description>Fixup on dbe2f53a0082bd5c90e8b4d1b8e6eb3cae91e5e8: Support YAML as content type
</description><key id="10510641">2607</key><summary>Fix YAML content type detection for CharSequence</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ofavre</reporter><labels /><created>2013-01-31T18:12:07Z</created><updated>2014-07-16T21:54:01Z</updated><resolved>2013-02-01T11:47:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-01T11:47:31Z" id="12990995">Pushed, thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove scope support in query and facet dsl.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2606</link><project id="" key="" /><description>Remove support for the `scope` field in facets and `_scope` field in the nested and parent/child queries. The scope support for nested queries will be replaced by the `nested` facet option and a facet filter with a nested filter. The nested filters will now support the a `join` option. Which controls whether to perform the block join. By default this enabled, but when disabled it returns the nested documents as hits instead of the joined root document.

Search request with the current scope support.

```
curl -s -XPOST 'localhost:9200/products/_search' -d '{
    "query" : {
        "nested" : {
            "path" : "offers",
            "query" : {
                "match" : {
                    "offers.color" : "blue"     
                }
            },
            "_scope" : "my_scope"
        }
    },
    "facets" : {
        "size" : {
            "terms" : {
                "field" : "offers.size"
            },
            "scope" : "my_scope"
        }
    }
}'
```

The following will be functional equivalent of using the scope support: 

```
curl -s -XPOST 'localhost:9200/products/_search?search_type=count' -d '{
    "query" : {
        "nested" : {
            "path" : "offers",
            "query" : {
                "match" : {
                    "offers.color" : "blue"     
                }
            }
        }
    },
    "facets" : {
        "size" : {
            "terms" : {
                "field" : "offers.size"
            },
            "facet_filter" : {
                "nested" : {
                    "path" : "offers",
                    "query" : {
                        "match" : {
                            "offers.color" : "blue"     
                        }
                    },
                    "join" : false
                }   
            },
            "nested" : "offers"
        }
    }
}'
```

The scope support for parent/child queries will be replaced by running the child query as filter in a global facet.

Search request with the current scope support:

```
curl -s -XPOST 'localhost:9200/products/_search' -d '{
    "query" : {
        "has_child" : {
            "type" : "offer",
            "query" : {
                "match" : {
                    "color" : "blue"        
                }
            },
            "_scope" : "my_scope"
        }
    },
    "facets" : {
        "size" : {
            "terms" : {
                "field" : "size"
            },
            "scope" : "my_scope"
        }
    }
}'
```

The following is the functional equivalent of using the scope support with parent/child queries:

```
curl -s -XPOST 'localhost:9200/products/_search' -d '{
    "query" : {
        "has_child" : {
            "type" : "offer",
            "query" : {
                "match" : {
                    "color" : "blue"        
                }
            }
        }
    },
    "facets" : {
        "size" : {
            "terms" : {
                "field" : "size"
            },
            "global" : true,
            "facet_filter" : {
                "term" : {
                    "color" : "blue"    
                }
            }
        }
    }
}'
```
</description><key id="10501240">2606</key><summary>Remove scope support in query and facet dsl.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>breaking</label><label>v0.90.0.Beta1</label></labels><created>2013-01-31T14:08:01Z</created><updated>2013-12-26T14:42:47Z</updated><resolved>2013-01-31T14:10:04Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="btiernay" created="2013-09-19T00:14:57Z" id="24709348">`join` should be added to the documentation. I totally missed this functionality.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Id Cache: Allow to configure if ids should be reused (memory wise) or not, default to false</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2605</link><project id="" key="" /><description>The process of checking if ids should be reused or not is expensive, causing loading the field cache to be more time consuming and higher load. Defaults to not try and reuse it (across all the data we have, but still across what is new now).
</description><key id="10458634">2605</key><summary>Id Cache: Allow to configure if ids should be reused (memory wise) or not, default to false</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>enhancement</label><label>v0.20.5</label><label>v0.90.0.Beta1</label></labels><created>2013-01-30T13:38:58Z</created><updated>2013-12-17T10:10:09Z</updated><resolved>2013-01-30T13:42:16Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="serj-p" created="2013-02-27T14:32:11Z" id="14176330">Is this somehow related to  https://github.com/elasticsearch/elasticsearch/issues/2343 ?
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow `search_analyzer` to be updated at runtime</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2604</link><project id="" key="" /><description>Allow `search_analyzer` to be updated via the put mapping api.
</description><key id="10454058">2604</key><summary>Allow `search_analyzer` to be updated at runtime</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-01-30T10:47:11Z</created><updated>2013-01-30T10:49:28Z</updated><resolved>2013-01-30T10:49:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Term frequency settings for different types in the same index seem to get squashed if fields have the same name</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2603</link><project id="" key="" /><description>Seems to happen when adding a lot of data quickly. Here's the PHP code that replicates the issue:

``` PHP
&lt;?php

exec('curl -XDELETE http://localhost:9200/test_index/');
exec('curl -XPUT http://localhost:9200/test_index/');
exec('curl -XPUT http://localhost:9200/test_index/type1/_mapping -d \''.
  json_encode(array('type1' =&gt; array('properties' =&gt;
    array('body' =&gt; array('type' =&gt; 'string'))))).'\'');
exec('curl -XPUT http://localhost:9200/test_index/type2/_mapping -d \''.
  json_encode(array('type2' =&gt; array('properties' =&gt;
    array('body' =&gt; array('type' =&gt; 'string',
      'omit_term_freq_and_positions' =&gt; true))))).'\'');

for ($j = 0; $j &lt; 1000; $j++) {

  $docs = array();
  $type = ($j == 0) ? 'type1' : 'type'.rand(1, 2);

  for ($i = 0; $i &lt; 1000; $i++) {
    $docs[] = array('index' =&gt; array('_id' =&gt; ($j * 1000 + $i + 1)));
    $docs[] = array('body' =&gt; 'test string');
  }

  exec("curl -XPUT http://localhost:9200/test_index/{$type}/_bulk -d '".
    implode("\n", array_map('json_encode', $docs)).'\'');

  // This sleep isn't even necessary, it's just there to prove that
  // the phrase match works at least once. Without it, this loop exits
  // after the first match on my comp.
  sleep(1);

  $line = system('curl -XGET http://localhost:9200/test_index/type1/1/_explain -d \''.
      json_encode(array('query' =&gt; array('match_phrase' =&gt; array(
        'body' =&gt; 'test string')))).'\'');

  // Term frequency for the phrase will go to zero when
  // the bug hits (I think during segment mergeing).
  if (strstr($line, 'phraseFreq=0.0') !== false) {
    exit;
  }

}
```

If you change the names of the fields ("body"), the issue does not happen. I think the problem stems from corruption during segment merging. I didn't dig in too deeply, but disabling segment merging in the Lucene library kept the issue from happening. This seems to affect 0.19 and 0.20. Might be losing other metadata as well, but I was only concerned with phrase matching.
</description><key id="10441640">2603</key><summary>Term frequency settings for different types in the same index seem to get squashed if fields have the same name</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pvulgaris</reporter><labels /><created>2013-01-30T00:55:52Z</created><updated>2014-07-08T19:36:51Z</updated><resolved>2014-07-08T19:36:51Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-02-01T11:42:53Z" id="12990881">Same field name under different types should have the same mappings, otherwise there will be problems. Internally, in order to conserve space, we map them to the same field name. If you really feel you need to separate between them, use different indices. We should give better failure message when we detect conflicts in mappings.
</comment><comment author="pvulgaris" created="2013-02-01T15:51:26Z" id="12999774">Thanks for the explanation. I think it would be helpful to have some kind of debug output logged when mappings conflict like that.
</comment><comment author="FrancisVarga" created="2013-08-27T07:31:57Z" id="23317746">hi, guys this helps me a lot to find my error, pls. provide a proper error message.
</comment><comment author="benjismith" created="2014-02-13T16:14:44Z" id="34994413">Will this issue be resolved by fixing this other (closely related) issue:?

https://github.com/elasticsearch/elasticsearch/issues/4081

This is the one remaining caveat with the otherwise excellent 1.0 release, and I'd be happy to trade a tiny bit of space-inefficiency in the index for unambiguous resolution of field names across all types (including nested types) for an entire index.
</comment><comment author="clintongormley" created="2014-07-08T19:36:51Z" id="48389400">Closing in favour of #4081
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Documment "boost" in "terms" query</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2602</link><project id="" key="" /><description>The "terms" query (http://www.elasticsearch.org/guide/reference/query-dsl/terms-query.html) supports the "boost" parameter (as mentioned by Radu Gheorghe in https://groups.google.com/d/topic/elasticsearch/MtKEKJ0b1Lo/discussion). I've tested it and it works. But it isn't mentioned in the docummentation.

For compatison, the "term" query (http://www.elasticsearch.org/guide/reference/query-dsl/term-query.html) does mention the "boost" parameter.

Please, mention the support of the "boost" parameter in the "terms" query docummentation!
</description><key id="10438061">2602</key><summary>Documment "boost" in "terms" query</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">ArtemGr</reporter><labels /><created>2013-01-29T22:50:44Z</created><updated>2017-01-30T19:52:11Z</updated><resolved>2014-07-08T19:09:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="mattweber" created="2013-01-29T23:16:27Z" id="12864539">All queries support a "boost".  The Elastic.js documentation has documented all options for queries, filters, and facets.

http://docs.fullscale.co/elasticjs/ejs.html 
</comment><comment author="ArtemGr" created="2013-01-29T23:31:12Z" id="12865159">Thanks for the pointer, Mark!

As a newby user the first place I was looking at was the "official" documentation at http://www.elasticsearch.org/.
It hasn't occured to me that to see the full list of options you have to look elsewhere.
</comment><comment author="mattweber" created="2013-01-29T23:36:53Z" id="12865411">The official documentation covers the more common and important options.  While developing Elastic.js, I made a point to document all options I found while going though the ElasticSearch source code and figure it is a good reference for people using the REST api as well as Elastic.js.
</comment><comment author="ArtemGr" created="2013-01-29T23:39:34Z" id="12865508">I see. Cool.
</comment><comment author="HeberLZ" created="2017-01-30T19:52:11Z" id="276170897">Hi everyone, i was also checking the official docs and couldn't find information on boost for terms, i ask because the way to build the config changes between type and type (term expects a value attribute while match expects a query one), and both links provided on this thread don't exist anymore :S</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add ability to fail-fast during bulk indexing</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2601</link><project id="" key="" /><description>It would be nice to have the ability to have Elasticsearch stop processing and return the currently list of bulk responses when the first operation failure occurs.
</description><key id="10435211">2601</key><summary>Add ability to fail-fast during bulk indexing</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dakrone</reporter><labels /><created>2013-01-29T21:36:57Z</created><updated>2014-07-04T10:20:39Z</updated><resolved>2014-07-04T10:20:39Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2014-07-04T10:20:39Z" id="48028648">This doesn't make sense as bulk requests are split and distributed to different shards.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Nested Query Facets with scope conflicts with versioning</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2600</link><project id="" key="" /><description>Suppose you have a document containing 10 nested documents (all at the same path).

Doing a normal terms facet gives correct counts (when summed over the different terms).

Doing the same, but with a scope associated with it, returns a lot more counts. After further investigating this, we discovered that the issue occurs when reindexing the same document multiple times:
-  document create: facet counts are both 10
- add same document:
  - nested terms facet: 10
  - terms facet with cope: **20 results**
- add same document again:
  - nested terms facet: 10
  - terms facet with cope: **30 results**
- ...

Seems like the old versions of the nested documents are still in the index and are being used to calculate the facet counts, **but only** when a **_scope** is set.

_We're using ElasticSearch 0.19.8_
</description><key id="10402461">2600</key><summary>Nested Query Facets with scope conflicts with versioning</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">folke</reporter><labels /><created>2013-01-29T02:34:23Z</created><updated>2013-01-29T12:11:01Z</updated><resolved>2013-01-29T12:11:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="folke" created="2013-01-29T12:11:01Z" id="12831847">This turned out to be a no issue on our end. For the nested facets, we specified both a **_scope** and a **nested**. This apparently yields bizarre results.

An error should probably returned in this case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Deb package wrongly includes additional lucene libraries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2599</link><project id="" key="" /><description>0.20.3 includes new version of Lucene as well..., causes startup failures.
</description><key id="10380116">2599</key><summary>Deb package wrongly includes additional lucene libraries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.4</label></labels><created>2013-01-28T15:56:55Z</created><updated>2013-01-28T15:57:09Z</updated><resolved>2013-01-28T15:57:09Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-28T15:57:09Z" id="12788372">Will be fixed when we do a proper clean build of 0.20.4....
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Tight loop logging startup error creating huge log file</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2598</link><project id="" key="" /><description>I have a small dev setup using an out-of-the-box 0.20.2 config with a single logical index and the default 5 shards on Win XP 64.
Either through the simultaneous read-only use of Luke on a shard or a power-outage the ES index looks to be corrupted (that is not the subject of this issue although this ideally shouldn't have happened anyway).

The problem is that on restarting ES in this state it enters a tight loop logging the same error with no delays between logging messages. I left it for a while and it accumulated a 130MB logfile - my guess it would continue indefinitely in this manner presenting a possible issue for other services on the same box needing CPU or diskspace?
A subset of the log entries is shown below:
------------- Begin log ----------
[2013-01-28 14:40:38,113][WARN ][cluster.action.shard     ] [Shaw, Shinobi] sending failed shard for [myindex][0], node[fsahp9leRqSgSf_pb-cZ9w], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[myindex][0] failed recovery]; nested: EngineCreationFailureException[[myindex][0] failed to create engine]; nested: EOFException[read past EOF: MMapIndexInput(path="C:\javalibs\elasticsearch\elasticsearch-0.20.2\data\MHelasticsearch\nodes\0\indices\myindex\0\index_0.fnm")]; ]]
[2013-01-28 14:40:38,113][WARN ][cluster.action.shard     ] [Shaw, Shinobi] received shard failed for [myindex][0], node[fsahp9leRqSgSf_pb-cZ9w], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[myindex][0] failed recovery]; nested: EngineCreationFailureException[[myindex][0] failed to create engine]; nested: EOFException[read past EOF: MMapIndexInput(path="C:\javalibs\elasticsearch\elasticsearch-0.20.2\data\MHelasticsearch\nodes\0\indices\myindex\0\index_0.fnm")]; ]]
[2013-01-28 14:40:38,129][WARN ][indices.cluster          ] [Shaw, Shinobi] [myindex][1] failed to start shard
org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [myindex][1] failed recovery
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:228)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [myindex][1] failed to create engine
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:252)
    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:547)
    at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:190)
    at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)
    ... 3 more
Caused by: java.io.EOFException: read past EOF: MMapIndexInput(path="C:\javalibs\elasticsearch\elasticsearch-0.20.2\data\MHelasticsearch\nodes\0\indices\myindex\1\index_0.fnm")
    at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readByte(MMapDirectory.java:284)
    at org.apache.lucene.store.DataInput.readVInt(DataInput.java:107)
    at org.apache.lucene.index.FieldInfos.read(FieldInfos.java:314)
    at org.apache.lucene.index.FieldInfos.&lt;init&gt;(FieldInfos.java:74)
    at org.apache.lucene.index.IndexWriter.getFieldInfos(IndexWriter.java:1212)
    at org.apache.lucene.index.IndexWriter.getCurrentFieldInfos(IndexWriter.java:1228)
    at org.apache.lucene.index.IndexWriter.&lt;init&gt;(IndexWriter.java:1161)
    at org.apache.lucene.index.XIndexWriter.&lt;init&gt;(XIndexWriter.java:17)
    at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1365)
    at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:250)
    ... 6 more

..repeated indefinitely.....
</description><key id="10378603">2598</key><summary>Tight loop logging startup error creating huge log file</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">markharwood</reporter><labels /><created>2013-01-28T15:17:48Z</created><updated>2014-09-02T10:46:26Z</updated><resolved>2014-07-08T19:30:30Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-28T15:25:23Z" id="12786717">hey mark,

this looks bad, can you somehow reproduce this? Do you know what the core failure is here? 
</comment><comment author="markharwood" created="2013-01-28T15:33:19Z" id="12787114">Hi Simon,

Sequence events was as follows:
- Started indexing into a blank ES (from a Storm bolt connected to a Stack
  Overflow atom feed getting ~30 docs every 10 seconds)
- Fired up Luke 3.5 to have a nose around the content in one of the shards
- an error was reported by Luke, presumably because of empty data
- Initiated a flush via one of the web-based admin consoles
- Tried Luke again (unsuccessful)
- Managed to pull a power lead out by accident

After reboot I entered this loop which looks like a corrupted index. It's
small enough to email if of interest.
I recognise there are some dodgy aspects to what I was doing here (no
replicas, power outage and playing with Luke on an empty then flushed
index) but the tight loop on the ES error reporting is probably something
that warrants attention.

Cheers,
Mark

On Mon, Jan 28, 2013 at 3:25 PM, Simon Willnauer
notifications@github.comwrote:

&gt; hey mark,
&gt; 
&gt; this looks bad, can you somehow reproduce this? Do you know what the core
&gt; failure is here?
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2598#issuecomment-12786717.
</comment><comment author="s1monw" created="2013-01-28T15:57:44Z" id="12788406">Hey Mark, I would love to look at the index though. I am still puzzled how this can happen though. Did the flush succeed before you pulled the power lead?
</comment><comment author="markharwood" created="2013-01-28T16:03:01Z" id="12788735">I expect the flush worked as it is a tiny amount of data.
A thought on the possible cause of corruption - if ES flushes but does not commit then when Luke tries to open the index it may take it upon itself to clear away what it sees as orphaned files if they are not referenced in a committed segments file? However, under Windows I would have thought that Luke's attempt to delete files held open by ES would fail. If Luke did succeed in deleting the flushed files then presumably a subsequent ES commit would store a reference to now missing files?
I'll try and zip up the index and config dirs and make them available.
</comment><comment author="s1monw" created="2013-01-28T16:06:26Z" id="12788915">I don't see why luke should wipe data here at all. I don't think it opens an indexwriter afaik? THe indexReader should not wipe it though. I would still like to look at the index. 
</comment><comment author="markharwood" created="2013-01-28T16:12:16Z" id="12789228">Didn't readers used to do a spot of tidying if they thought it necessary?
</comment><comment author="s1monw" created="2013-01-28T16:13:28Z" id="12789296">@markharwood not in 3.x afaik
</comment><comment author="s1monw" created="2013-01-28T16:17:44Z" id="12789500">so I took a quick look and the indices for the shards 3 and 4 look good. for 0 and 1 I see lots of files with 0 bytes and all of them have the same timestamp. (11:14 vs. 10:59) do you recall the times roughly when you did what ?

:)
</comment><comment author="markharwood" created="2013-01-28T16:45:13Z" id="12790940">Indexes zero and one are the only ones I tried looking at with Luke so that sounds significant. Some times:

10:48 deleted an earlier run of the index via the admin console
10:49 creating index , cause [auto(index api)], shards [5]/[1], mappings []
???? Some meddling with Luke (no deliberate deletes/optimizes or other destructive activity) 
11:38 power outage
11:42 Machine rebooted
14:36 Restart ES and enters error logging loop reported earlier
14:51 Killed ES process - log file is 130MB

Couldn't see a log entry for the flush - is that logged?
</comment><comment author="markharwood" created="2013-01-30T16:48:40Z" id="12899259">Minimal steps to reproduce the file corruption:
- Start ES with a clean install
- Start a client inserting content (mine adds a small doc ~ every 2 seconds)
- Open a shard in Luke 3.5  in read-only form e.g. [es dir]\data\mycluster\nodes\0\indices\myindex\3\index 
  Luke should display 0 docs
- in the elasticsearch-head-master admin console select the "flush" action on the index
- In Luke hit the "reopen current index" button
  Luke errors with  read past EOF: MMapIndexInput(path="[es dir]\data\mycluster\nodes\0\indices\myindex\3\index_2.fnm")
- Kill Luke, the indexing client and ES processes.
- Restart ES and it logs error: 
  Caused by: java.io.EOFException: read past EOF: MMapIndexInput(path=""[es dir]\data\mycluster\nodes\0\indices\myindex\3\index_0.fnm")
</comment><comment author="s1monw" created="2013-01-30T19:32:45Z" id="12907824">cool stuff! I will try to figure out what is going on tomorrow!
</comment><comment author="clintongormley" created="2014-07-08T19:05:39Z" id="48385498">@s1monw @markharwood can this be closed?
</comment><comment author="s1monw" created="2014-07-08T19:30:30Z" id="48388612">closing for now...
</comment><comment author="Woffendm" created="2014-08-26T20:28:54Z" id="53485234">It might be worth noting that I encountered a similar problem just today. My entire cluster had full disk because of massive 9GB log files generated when repeatedly failing to recover indices. It looks like it just keeps trying forever.
</comment><comment author="clintongormley" created="2014-09-02T10:46:26Z" id="54135192">@Woffendm please could you open a new issue with the relevant data. I agree that it would be good to not try to recover the same index endlessly, but the discussion should happen on a new issue.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Elasticsearch and requirements.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2597</link><project id="" key="" /><description>Elasticsearch requires java framework correct? Darn shame cause I'll never install that platform again and there are quite a few people that feel the same way. Good luck with that and hope you consider another platform again soon if java is required. 
</description><key id="10377683">2597</key><summary>Elasticsearch and requirements.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gfxguru</reporter><labels /><created>2013-01-28T14:49:54Z</created><updated>2013-01-28T23:06:02Z</updated><resolved>2013-01-28T15:05:47Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-28T15:05:46Z" id="12785678">Hi, yes, elasticsearch requires java. Its a shame that you feel that way about java, we work very hard to work around the typical reasons why java got some bad vibe (understandably so) in having programs being hard to use, complex, with configuration hell, and those type of things. But, at the end of the day, Java (the JVM) is a very powerful platform to build performant, concurrent, server side projects, and it has the most advance Java information retrieval library out there in Lucene. 

Also it is, first and foremost, it is a server project, not a framework. 

I will go ahead and close this..., as it is going to remain in java.
</comment><comment author="gfxguru" created="2013-01-28T15:44:46Z" id="12787729">That's a shame.. Would have loved to try elasticsearch. 
</comment><comment author="gfxguru" created="2013-01-28T15:45:43Z" id="12787777">You might want to checkout
http://www.cio-today.com/news/Java-a-Security-Threat--Officials-Warn/story.xhtml?story_id=0310012HU8GJ&amp;full_skip=1
</comment><comment author="hbs" created="2013-01-28T15:48:12Z" id="12787898">We're lucky on this one, ES does not run in the browser! Hurray.
</comment><comment author="gfxguru" created="2013-01-28T15:54:21Z" id="12788217">True but it requires the platform running on the server... and I'm not ready to compromise our clients security. 
</comment><comment author="s1monw" created="2013-01-28T15:54:40Z" id="12788229">@gfxguru I think you are mixing things up here. ElasticSearch is written in Java but it is a server application and the client doesn't require Java or the Java platform at all. The Security threads you are referring to are targeted for Java Applications / Applet that are executed in the clients Browser which is entirely unrelated. Given that a ton of server side applications are running on Java with great success, stable and pretty safe I don't think you need to be afraid.
</comment><comment author="jprante" created="2013-01-28T16:58:18Z" id="12791625">@gfxguru Your concerns are about security, it seems. Note, securing the access to any application is not dependent on a specific language or platform. It is not a question of software alone. It is a question of soft- and hardware architecture, infrastructure, and trust (see "social engineering").

You can set up Elasticsearch on server-side and hide it away behind your firewall or your private networks. As being said Elasticsearch does not rely on Java-specific client access being exposed like in browsers (by applets or sandboxing). 

You can access Elasticsearch with Perl, PHP, Python, Ruby, Javascript Node.js, Scala, Clojure , .NET ... there are dozens of client implementations out there. The interface is HTTP REST, and there are standard authentication and authorization functions available which can be reused for Elasticsearch access without any restrictions.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support MultiPhrasePrefix (MatchQuery.Type.PHRASE_PREFIX) in Highlighters</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2596</link><project id="" key="" /><description>this is related to this PR (https://github.com/elasticsearch/elasticsearch/pull/1065) closed 2 years ago. It seems this never really worked. But given the popularity of this query we should really fix that
</description><key id="10376615">2596</key><summary>Support MultiPhrasePrefix (MatchQuery.Type.PHRASE_PREFIX) in Highlighters</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>bug</label><label>v0.20.4</label><label>v0.90.0.Beta1</label></labels><created>2013-01-28T14:19:32Z</created><updated>2013-01-28T14:21:28Z</updated><resolved>2013-01-28T14:21:28Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Update, Initialization Failed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2595</link><project id="" key="" /><description>I'm using 0.20.3 and ElasticSearch doesn't start.
It was ok with 0.20.2.

I get this in log when I start the service:

```
[2013-01-28 12:18:31,759][INFO ][node                     ] [pierre-N56VM] {0.20.3}[4488]: initializing ...
[2013-01-28 12:18:31,764][INFO ][plugins                  ] [pierre-N56VM] loaded [], sites [head]
```

If I start manually ES:

```
/usr/share/elasticsearch/bin/elasticsearch
&#10140;  ~  log4j:WARN No appenders could be found for logger (node).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
{0.20.3}: Initialization Failed ...
- ComputationException[java.lang.VerifyError: class org.apache.lucene.analysis.ReusableAnalyzerBase overrides final method tokenStream.(Ljava/lang/String;Ljava/io/Reader;)Lorg/apache/lucene/analysis/TokenStream;]
    VerifyError[class org.apache.lucene.analysis.ReusableAnalyzerBase overrides final method tokenStream.(Ljava/lang/String;Ljava/io/Reader;)Lorg/apache/lucene/analysis/TokenStream;]
```

Java version:

```
java -version
java version "1.7.0_09"
OpenJDK Runtime Environment (IcedTea7 2.3.4) (7u9-2.3.4-0ubuntu1.12.10.1)
OpenJDK 64-Bit Server VM (build 23.2-b09, mixed mode)
```
</description><key id="10372898">2595</key><summary>Update, Initialization Failed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">pierrre</reporter><labels /><created>2013-01-28T11:59:57Z</created><updated>2013-04-09T00:41:43Z</updated><resolved>2013-01-29T09:28:49Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-29T09:28:49Z" id="12826648">This is a problem wiht the deb package of 0.20.3, just released 0.20.4 that fixes it.
</comment><comment author="pierrre" created="2013-01-29T10:14:49Z" id="12828168">Thank you! :)
</comment><comment author="jeredding" created="2013-04-09T00:41:43Z" id="16087965">What _was_ the actual issue?  
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Indexes created from index request might not replica initial doc to replica</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2594</link><project id="" key="" /><description>When an index ends up created by an index request, the initial doc might not end up in the replica itself due to race condition during the recovery process the replica does from the primary shard. It is very rare, yet still might happen...
</description><key id="10370483">2594</key><summary>Indexes created from index request might not replica initial doc to replica</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.4</label><label>v0.90.0.Beta1</label></labels><created>2013-01-28T10:29:02Z</created><updated>2013-01-28T10:29:40Z</updated><resolved>2013-01-28T10:29:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Initialization Failed - NoSuchMethodError org.apache.lucene.analysis.miscellaneous.PatternAnalyzer.&lt;init&gt; </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2593</link><project id="" key="" /><description>- Installed fresh copy of Debian 6 Squeeze
- Installed openjdk-6-jre-headless from aptitude
- Installed elasticsearch-0.20.3.deb (Downloaded as of 2013-01-26 04:06 PST)
- `/etc/init.d/elasticsearch start` did not give any error messages, but when checked with status, it says server is not running and there is pid.
- Manually edited /etc/init.d/elasticsearch to show the command it used to start daemon, it was: 
  `/usr/share/elasticsearch/bin/elasticsearch -p /var/run/elasticsearch.pid 
  -Des.default.config=/etc/elasticsearch/elasticsearch.yml 
  -Des.default.path.home=/usr/share/elasticsearch 
  -Des.default.path.logs=/var/log/elasticsearch 
  -Des.default.path.data=/var/lib/elasticsearch 
  -Des.default.path.work=/tmp/elasticsearch 
  -Des.default.path.conf=/etc/elasticsearch`
- Manually ran it, and got this error:
  
  `{0.20.3}: Initialization Failed ...`
  `1) NoSuchMethodError[org.apache.lucene.analysis.miscellaneous.PatternAnalyzer.&lt;init&gt;(Lorg/apache/lucene/util/Version;Ljava/util/regex/Pattern;ZLjava/util/Set;)V]`

Is there anything I can do to correct this error?
</description><key id="10337674">2593</key><summary>Initialization Failed - NoSuchMethodError org.apache.lucene.analysis.miscellaneous.PatternAnalyzer.&lt;init&gt; </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">huanga</reporter><labels /><created>2013-01-26T12:49:09Z</created><updated>2013-01-28T21:02:57Z</updated><resolved>2013-01-28T21:02:57Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="huanga" created="2013-01-26T14:37:39Z" id="12736101">Memo: Installed elasticsearch-0.19.12.deb and it works fine.
</comment><comment author="dadoonet" created="2013-01-28T15:58:35Z" id="12788452">See issue #2599 
You can close this one.
</comment><comment author="huanga" created="2013-01-28T21:02:57Z" id="12804531">Awesome! Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Primary shard failure with initializing replica shards can cause the replica shard to cause allocation failures</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2592</link><project id="" key="" /><description /><key id="10318146">2592</key><summary>Primary shard failure with initializing replica shards can cause the replica shard to cause allocation failures</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>bug</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2013-01-25T16:58:46Z</created><updated>2013-01-25T16:59:05Z</updated><resolved>2013-01-25T16:59:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Expose Common Terms functionality in Highlighter and MatchQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2591</link><project id="" key="" /><description /><key id="10317646">2591</key><summary>Expose Common Terms functionality in Highlighter and MatchQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-25T16:45:43Z</created><updated>2013-01-28T10:57:24Z</updated><resolved>2013-01-28T10:57:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Index creation with wrong settings leaves index in inconsistent state</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2590</link><project id="" key="" /><description>When we try to create an index with one of the settings having a wrong value, the server responds with the right validation. However if the index creation is tried after fixing the setting it responds with IndexAlreadyExists message where the index was never created.

**Steps to recreate:**

**Create index with wrong setting**

``` bash
curl -XPOST localhost:9200/myindex1 -d '{"settings":{"number_of_shards":"a","number_of_replicas":"3"}}'
```

This request fails with the following error message which is expected

``` json
{"error":"SettingsException[Failed to parse int setting [index.number_of_shards] with value [a]]; nested: NumberFormatException[For input string: \"a\"]; ","status":500}
```

**Retry creating the index with the right setting**

``` bash
curl -XPOST localhost:9200/myindex1 -d '{"settings":{"number_of_shards":"2","number_of_replicas":"3"}}'
```

We expect the index be created this time, but it fails with the following error message

``` json
{"error":"IndexAlreadyExistsException[[myindex1] Already exists]","status":400}
```

Isn't this a weird behaviour ?
</description><key id="10302510">2590</key><summary>Index creation with wrong settings leaves index in inconsistent state</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">hari-tw</reporter><labels /><created>2013-01-25T07:30:32Z</created><updated>2013-01-30T15:55:59Z</updated><resolved>2013-01-30T15:55:59Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="katta" created="2013-01-25T07:31:40Z" id="12690803">This is happening on v0.20.2 !
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>ArithmeticException during `stats?all`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2589</link><project id="" key="" /><description>Saw this while helping a user tonight.

```
    # curl -s $(hostname):9200/_nodes/stats\?all=1
    {"error":"ArithmeticException[Value cannot fit in an int: -2562047788015]","status":500}
```
</description><key id="10300498">2589</key><summary>ArithmeticException during `stats?all`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/spinscale/following{/other_user}', u'events_url': u'https://api.github.com/users/spinscale/events{/privacy}', u'organizations_url': u'https://api.github.com/users/spinscale/orgs', u'url': u'https://api.github.com/users/spinscale', u'gists_url': u'https://api.github.com/users/spinscale/gists{/gist_id}', u'html_url': u'https://github.com/spinscale', u'subscriptions_url': u'https://api.github.com/users/spinscale/subscriptions', u'avatar_url': u'https://avatars2.githubusercontent.com/u/667544?v=4', u'repos_url': u'https://api.github.com/users/spinscale/repos', u'received_events_url': u'https://api.github.com/users/spinscale/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/spinscale/starred{/owner}{/repo}', u'site_admin': False, u'login': u'spinscale', u'type': u'User', u'id': 667544, u'followers_url': u'https://api.github.com/users/spinscale/followers'}</assignee><reporter username="">drewr</reporter><labels /><created>2013-01-25T05:04:13Z</created><updated>2013-10-08T10:39:58Z</updated><resolved>2013-10-08T10:39:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-29T09:33:16Z" id="12826771">can we get the log for it that is associated with it?
</comment><comment author="drewr" created="2013-01-29T14:40:14Z" id="12837320">Unfortunately probably not. It was in the midst of the github outage where it wasn't really a priority and I didn't have ready access to it. The cluster state was pretty unstable so it's perhaps not worth tracking down.
</comment><comment author="mahdeto" created="2013-01-31T18:53:33Z" id="12959085">Same issue, here is the exception:

org.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.stats.NodeStats]
Caused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.stats.NodeStats]
        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:150)
        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:127)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:786)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:458)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:439)
        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:558)
        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:553)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:84)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:471)
        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:332)
        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:102)
        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1846295853
        at java.lang.String.&lt;init&gt;(String.java:207)
        at org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:208)
        at org.elasticsearch.common.io.stream.StreamInput.readUTF(StreamInput.java:216)
        at org.elasticsearch.common.io.stream.HandlesStreamInput.readUTF(HandlesStreamInput.java:50)
        at org.elasticsearch.monitor.jvm.JvmStats$MemoryPool.readFrom(JvmStats.java:795)
        at org.elasticsearch.monitor.jvm.JvmStats$MemoryPool.readMemoryPool(JvmStats.java:749)
        at org.elasticsearch.monitor.jvm.JvmStats$Mem.readFrom(JvmStats.java:844)
        at org.elasticsearch.monitor.jvm.JvmStats$Mem.readMem(JvmStats.java:826)
        at org.elasticsearch.monitor.jvm.JvmStats.readFrom(JvmStats.java:417)
        at org.elasticsearch.monitor.jvm.JvmStats.readJvmStats(JvmStats.java:408)
        at org.elasticsearch.action.admin.cluster.node.stats.NodeStats.readFrom(NodeStats.java:269)
</comment><comment author="imotov" created="2013-02-01T21:51:59Z" id="13016189">This error might be caused by [writeVLong](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/io/stream/StreamOutput.java#L160) serializing a negative number. In most cases, the value that is serialized using writeVLong is checked for negativity by the caller. However, sometimes (see #2609 for example), the value can still be negative.

To reproduce: start 2 nodes and follow the steps outlined in #2609 then try retrieving stats from the node where index wasn't allocated.

Would it make sense to add a check for non-negative argument to writeVLong just to make sure that this situation cannot happen?
</comment><comment author="nkvoll" created="2013-02-14T14:45:20Z" id="13552295">Getting this from a cluster right now:

```
{
status: 500,
ok: false,
error: "ArithmeticException[Value cannot fit in an int: 204963823041]"
}
```

Could it stem from the same issue?
</comment><comment author="imotov" created="2013-02-14T14:53:07Z" id="13552673">It's easy to check. Run this `curl -s -XGET 'http://localhost:9200/_nodes/_local/stats?pretty=true'` on every node and check if values in the `cache` section are negative. 
</comment><comment author="kimchy" created="2013-02-14T14:57:34Z" id="13552903">Also, if they are, would love to get a gist of it, so we can check why it happens. Also, version of ES used would be great.
</comment><comment author="nkvoll" created="2013-02-14T15:00:11Z" id="13553021">imotov: indeed, at least one of the instances has a negative cache value.

kimchy: what exactly do you want me to gist? Im about to replace the instance, but can probably get you some important information before I do
</comment><comment author="kimchy" created="2013-02-14T15:02:12Z" id="13553127">@nkvoll the response of the cache stats from the node with the negative value, and the ES version you use.
</comment><comment author="nkvoll" created="2013-02-14T15:05:15Z" id="13553396">@kimchy here you go: https://gist.github.com/nkvoll/6e6ebfbb0365c2431403
</comment><comment author="kimchy" created="2013-02-14T23:40:46Z" id="13585326">@nkvoll it seems like its in the filter cache stats, which we fixed a similar bug there in 0.20.5 (that was just released). It mainly revolved around creating and deleting indices and not properly handling the counts there.
</comment><comment author="nkvoll" created="2013-02-14T23:55:59Z" id="13585865">The cluster in my case was a cluster with three instances.

The cluster was not being written to, but it was recovering from S3, and one of the instances (which was the leader at the time) were experiencing intermittent network issues, which caused the other instances in the cluster quite a head-ache, as some of the pings were left unanswered and some requests took over 90 seconds to receive a response from that one instance. As the faulty instance recovered around five shards, the other instances seemingly lost connectivity to the faulty instance (ping timeouts) and they started leader election again. As they continued recovery, the faulty instance was still not able to respond to all requests within the timeouts, and left/rejoined the cluster a few times because of this. The leaving/rejoining caused the recovery process to take a lot longer time than expected because the faulty instance would be assigned shards for recovery, which seemed to block the two other, non-faulty, instances from recovering as fast as they would otherwise.

We caught on on this recovery fairly quickly, but decided to let it run for a while in order to see if we could at least get some stack traces and maybe some other relevant information to aid in a post-mortem. All in all, ElasticSearch coped rather well with the faulty host, except for this ArithmeticException and that it kept reassigning shards to the faulty instance, temporarily halting recovery to the working instances, but that might be expected.

As we replaced the EC2 instance, the issues went away and the cluster is now in a healthy state.
</comment><comment author="kimchy" created="2013-02-14T23:58:06Z" id="13585942">was the problematic arithmetic failure happened on the faulty node (when you executed it using the local flag)?
</comment><comment author="nkvoll" created="2013-02-15T00:04:12Z" id="13586147">No, the arithmetic failure only shows when asking from node stats for all the instances. When I used the _local-variant, none of the nodes raised an exception, but the faulty node returned invalid values for the filter caches.
</comment><comment author="s1monw" created="2013-03-02T15:49:16Z" id="14330059">@drewr can we resolve this or is this still open?
</comment><comment author="mal" created="2013-03-22T22:48:25Z" id="15325291">Seeing this issue on elasticsearch `0.20.5` using openjdk7 (`7u15-2.3.7-0ubuntu1~12.04.1`) with a 3 node cluster.

I've made a [gist](https://gist.github.com/mal/caf812791d8df19ed651) of various endpoint output (the problem one being `cluster/nodes/stats?all=true`) and the log generated from the problem request in question. It's not all the time (but pretty frequent), using `os=false` seems to abait the situation (or at least make it far less common, I've not observed the error while that param is present).

All requests in the gist were made against node `elastic-a`.

---

As an aside the [docs](http://www.elasticsearch.org/guide/reference/api/admin-cluster-nodes-stats.html) say:

```
curl -XGET 'http://localhost:9200/_cluster/nodes/stats'
...
The first command retrieves stats of all the nodes in the cluster.
```

I have not encountered this (see gist), rather it just returns stats for the local node. Is this a bug I should file, or an error in documentation?
</comment><comment author="mal" created="2013-03-26T14:02:45Z" id="15459753">@drewr @kimchy I managed to reproduce this issue on 0.20.5, hopfully the [log](https://gist.github.com/mal/caf812791d8df19ed651) will prove useful.
</comment><comment author="imotov" created="2013-03-26T14:13:12Z" id="15460333">@mal Could you run these three commands (they basically execute stats locally on each node) and post output here:

```
curl "10.0.0.1:9200/_cluster/nodes/_local/stats?os=true"
curl "10.0.0.2:9200/_cluster/nodes/_local/stats?os=true"
curl "10.0.0.3:9200/_cluster/nodes/_local/stats?os=true"
```
</comment><comment author="mal" created="2013-03-26T15:03:37Z" id="15463492">@imotov I've added the output to the [gist](https://gist.github.com/mal/caf812791d8df19ed651), the results look fine on that endpoint.

I also ran stats all on each node in turn:

```
$ curl "10.0.0.1:9200/_cluster/nodes/stats?all=true"
{"error":"ArithmeticException[Value cannot fit in an int: -2562047788015]","status":500}
$ curl "10.0.0.2:9200/_cluster/nodes/stats?all=true"
{"error":"ArithmeticException[Value cannot fit in an int: -2562047788015]","status":500}
$ curl "10.0.0.3:9200/_cluster/nodes/stats?all=true"
{"error":"ArithmeticException[Value cannot fit in an int: 20015998343]","status":500}
```

I don't always get this error, but when I do I seem to get it on all nodes at the same time.
</comment><comment author="imotov" created="2013-03-26T15:05:04Z" id="15463581">It looks like you missed `/_local/` part in url. Could you rerun the queries with it?
</comment><comment author="mal" created="2013-03-26T15:08:32Z" id="15463806">The `/_local/` output you requested is in the [gist](https://gist.github.com/mal/caf812791d8df19ed651). The output in the comment was more me confirming I was still seeing the issue.
</comment><comment author="imotov" created="2013-03-26T15:20:18Z" id="15464630">@mal thanks! This is really interesting. It looks like you are hitting #2609, except it was supposed to be fixed in 0.20.5. 
</comment><comment author="imotov" created="2013-04-02T11:31:28Z" id="15769881">@mal Are you using percolators on this cluster?
</comment><comment author="mal" created="2013-04-02T14:06:02Z" id="15776868">@imotov Yup, 139 of them.
</comment><comment author="imotov" created="2013-04-02T14:11:51Z" id="15777218">@mal it should be fixed now by  https://github.com/elasticsearch/elasticsearch/commit/10a76ad5d8085a6d951a4bdfb806c713af989c12
</comment><comment author="spinscale" created="2013-05-28T07:24:51Z" id="18534941">@mal can you confirm that the problem is also solved for you as well (in 0.90.0)?
</comment><comment author="mal" created="2013-05-29T13:32:21Z" id="18616232">@spinscale bit swamped at the moment, but as soon as we upgrade I'll report back
</comment><comment author="mal" created="2013-10-08T10:36:11Z" id="25879531">@spinscale Just upgraded to `0.90.2` and tested again, can confirm I'm no longer seeing the issue, cheers!
</comment><comment author="s1monw" created="2013-10-08T10:39:58Z" id="25879722">seems like we can close - thanks for the ping 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Support filter inside has_parent and has_child filter</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2588</link><project id="" key="" /><description>Add filter support in the `has_child` and `has_parent` filters. Example:

```
curl -XPOST 'localhost:9200/_search' -d '{
  "query": {
    "filtered_query": {
      "query": {
        "match": {
          "title": "distributed systems"
        }
      },
      "filter": {
        "has_child": {
          "type": "tag",
          "filter": {
            "term": {
              "name": "book"
            }
          }
        }
      }
    }
  }
}'
```
</description><key id="10287742">2588</key><summary>Support filter inside has_parent and has_child filter</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">martijnvg</reporter><labels><label>enhancement</label><label>v0.20.3</label><label>v0.90.0.Beta1</label></labels><created>2013-01-24T20:28:00Z</created><updated>2013-06-12T13:58:54Z</updated><resolved>2013-01-24T20:42:20Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Strange exception "failed to delete index on stop"</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2587</link><project id="" key="" /><description>So I've got good ole too many open files exception on one of cluster nodes, which I fixed right away. But I found some stange exception while restarting ES node 

[2013-01-24 17:53:22,736][WARN ][index.merge.scheduler    ] [xxxx04] [xxx_index][0] failed to merge
java.io.FileNotFoundException: /xxx/_71hat.fnm (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.&lt;init&gt;(RandomAccessFile.java:233)
        at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.&lt;init&gt;(SimpleFSDirectory.java:71)
        at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.&lt;init&gt;(SimpleFSDirectory.java:98)
        at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.&lt;init&gt;(NIOFSDirectory.java:92)
        at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:79)
        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:345)
        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:521)
        at org.apache.lucene.index.FieldInfos.&lt;init&gt;(FieldInfos.java:71)
        at org.apache.lucene.index.SegmentCoreReaders.&lt;init&gt;(SegmentCoreReaders.java:80)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:116)
        at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:696)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4231)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3901)
        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:388)
        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:456)
[2013-01-24 17:53:22,748][INFO ][node                     ] [xxx04] {0.19.8}[5718]: stopping ...
[2013-01-24 17:53:23,720][WARN ][indices                  ] [xxx04] failed to delete index on stop [xxx_index]
org.apache.lucene.store.AlreadyClosedException: this IndexReader is closed
        at org.apache.lucene.index.IndexReader.ensureOpen(IndexReader.java:245)
        at org.apache.lucene.index.IndexReader.decRef(IndexReader.java:216)
        at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:104)
        at org.apache.lucene.search.SearcherManager.decRef(SearcherManager.java:57)
        at org.apache.lucene.search.ReferenceManager.release(ReferenceManager.java:174)
        at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearchResult.release(RobinEngine.java:1390)
        at org.elasticsearch.search.internal.SearchContext.release(SearchContext.java:204)
        at org.elasticsearch.search.SearchService.freeContext(SearchService.java:512)
        at org.elasticsearch.search.SearchService.releaseContextsForIndex(SearchService.java:146)
        at org.elasticsearch.search.SearchService$CleanContextOnIndicesLifecycleListener.beforeIndexClosed(SearchService.java:620)
        at org.elasticsearch.indices.InternalIndicesLifecycle.beforeIndexClosed(InternalIndicesLifecycle.java:92)
        at org.elasticsearch.indices.InternalIndicesService.deleteIndex(InternalIndicesService.java:335)
        at org.elasticsearch.indices.InternalIndicesService.access$000(InternalIndicesService.java:93)
        at org.elasticsearch.indices.InternalIndicesService$1.run(InternalIndicesService.java:147)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
[2013-01-24 17:53:47,278][INFO ][node                     ] [xxx04] {0.19.8}[5718]: stopped
[2013-01-24 17:53:47,279][INFO ][node                     ] [xxx04] {0.19.8}[5718]: closing ...
[2013-01-24 17:53:47,439][INFO ][node                     ] [xxx04] {0.19.8}[5718]: closed
[2013-01-24 18:03:22,229][INFO ][node                     ] [xxx04] {0.19.8}[4543]: initializing ...
[2013-01-24 18:03:22,240][INFO ][plugins                  ] [xxx04] loaded [river-couchdb], sites [head]
[2013-01-24 18:03:23,769][INFO ][node                     ] [xxx04] {0.19.8}[4543]: initialized
[2013-01-24 18:03:23,769][INFO ][node                     ] [xxx04] {0.19.8}[4543]: starting ...

So my question is, why does it want to delete index on stop??
</description><key id="10280493">2587</key><summary>Strange exception "failed to delete index on stop"</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">Rzulf</reporter><labels /><created>2013-01-24T17:11:55Z</created><updated>2013-02-08T17:31:58Z</updated><resolved>2013-02-08T17:31:58Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="martijnvg" created="2013-02-08T17:31:58Z" id="13301456">Maybe some other process was executing a delete during shutdown? Please ask questions in the mailing list, Github issues is for development / bug reports only.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Highlighting percolator (0.21)</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2586</link><project id="" key="" /><description>I don't give up :)
</description><key id="10279828">2586</key><summary>Highlighting percolator (0.21)</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">synhershko</reporter><labels /><created>2013-01-24T16:52:42Z</created><updated>2014-06-26T08:58:16Z</updated><resolved>2013-07-05T08:56:17Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="maxcom" created="2013-07-04T12:01:01Z" id="20473537">I would like to see this feature in ElasticSearch. Currently we have to request highlighting in separate query after percolate found some matches. This is very inefficient, especially when do not need that data in index.
</comment><comment author="synhershko" created="2013-07-05T08:56:17Z" id="20507586">Well, this works. However, ElasticSearch's API has changed since I submitted this and in the latest 0.90 or 1 beta this would have to be done a bit differently.

I'm closing this now, but as a I said - it works for us on our fork.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add suggest api</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2585</link><project id="" key="" /><description># Suggest feature

The suggest feature suggests similar looking terms based on a provided text by using a suggester. At the moment there the only supported suggester is `fuzzy`. The suggest feature is available from version `0.21.0`.
# Fuzzy suggester

The `fuzzy` suggester suggests terms based on edit distance. The provided suggest text is analyzed before terms are suggested. The suggested terms are provided per analyzed suggest text token. The `fuzzy` suggester doesn't take the query into account that is part of request. 
# Suggest API

The suggest request part is defined along side the query part as top field in the json request.

```
curl -s -XPOST 'localhost:9200/_search' -d '{
  "query" : {
    ...
  },
  "suggest" : {
    ...
  }
}' 
```

Several suggestions can be specified per request. Each suggestion is identified with an arbitary name. In the example below two suggestions are requested. Both `my-suggest-1` and `my-suggest-2` suggestions use the `fuzzy` suggester, but have a different `text`. 

```
"suggest" : {
  "my-suggest-1" : {
    "text" : "the amsterdma meetpu",
    "fuzzy" : {
      "field" : "body"
    }
  },
  "my-suggest-2" : {
    "text" : "the rottredam meetpu",
    "fuzzy" : {
      "field" : "title",
    }
  }
}
```

The below suggest response example includes the suggestion response for `my-suggest-1` and `my-suggest-2`. Each suggestion part contains entries. Each entry is effectively a token from the suggest text and contains the suggestion entry text, the original start offset and length in the suggest text and if found an arbitary number of options.

```
{
  ...
  "suggest": {
    "my-suggest-1": [
      {
        "text" : "amsterdma",
        "offset": 4,
        "length": 9,
        "options": [
           ...
        ]
      },     
      ...       
    ],
    "my-suggest-2" : [
      ... 
    ]
  }
  ...
}
```

Each options array contains a option object that includes the suggested text, its document frequency and score compared to the suggest entry text. The meaning of the score depends on the used suggester. The fuzzy suggester's score is based on the edit distance.

```
"options": [
  {
    "text": "amsterdam",
    "freq": 77,
    "score": 0.8888889
  },
  ...
]  
```
# Global suggest text

To avoid repitition of the suggest text, it is possible to define a global text. In the example below the suggest text is defined globally and applies to the `my-suggest-1` and `my-suggest-2` suggestions.

```
"suggest" : {
  "text" : "the amsterdma meetpu"
  "my-suggest-1" : {
    "fuzzy" : {
      "field" : "title"
    }
  },
  "my-suggest-2" : {
    "fuzzy" : {
      "field" : "body"
    }
  }
}
```

The suggest text can in the above example also be specied as suggestion specific option. The suggest text specified on suggestion level override the suggest text on the global level.
# Other suggest example.

In the below example we request suggestions for the following suggest text: `devloping distibutd saerch engies` on the `title` field with a maximum of 3 suggestions per term inside the suggest text. Note that in this example we use the `count` search type. This isn't required, but a nice optimalization. The suggestions are gather in the `query` phase and in the case that we only care about suggestions (so no hits) we don't need to execute the `fetch` phase.

```
curl -s -XPOST 'localhost:9200/_search?search_type=count' -d '{
  "suggest" : {
    "my-title-suggestions-1" : {
      "text" : "devloping distibutd saerch engies",
      "fuzzy" : {
        "size" : 3,
        "field" : "title"  
      }
    }
  }
}'
```

The above request could yield the response as stated in the code example below. As you can see if we take the first suggested options of each suggestion entry we get `developing distributed search engines` as result.

```
{
  ...
  "suggest": {
    "my-title-suggestions-1": [
      {
        "text": "devloping",
        "offset": 0,
        "length": 9,
        "options": [
          {
            "text": "developing",
            "freq": 77,
            "score": 0.8888889
          },
          {
            "text": "deloping",
            "freq": 1,
            "score": 0.875
          },
          {
            "text": "deploying",
            "freq": 2,
            "score": 0.7777778
          }
        ]
      },
      {
        "text": "distibutd",
        "offset": 10,
        "length": 9,
        "options": [
          {
            "text": "distributed",
            "freq": 217,
            "score": 0.7777778
          },
          {
            "text": "disributed",
            "freq": 1,
            "score": 0.7777778
          },
          {
            "text": "distribute",
            "freq": 1,
            "score": 0.7777778
          }
        ]
      },
      {
        "text": "saerch",
        "offset": 20,
        "length": 6,
        "options": [
          {
            "text": "search",
            "freq": 1038,
            "score": 0.8333333
          },
          {
            "text": "smerch",
            "freq": 3,
            "score": 0.8333333
          },
          {
            "text": "serch",
            "freq": 2,
            "score": 0.8
          }
        ]
      },
      {
        "text": "engies",
        "offset": 27,
        "length": 6,
        "options": [
          {
            "text": "engines",
            "freq": 568,
            "score": 0.8333333
          },
          {
            "text": "engles",
            "freq": 3,
            "score": 0.8333333
          },
          {
            "text": "eggies",
            "freq": 1,
            "score": 0.8333333
          }
        ]
      }
    ]
  }
  ...
}
```
# Common suggest options:
- `text` - The suggest text. The suggest text is a required option that needs to be set globally or per suggestion.
# Common fuzzy suggest options
- `field` - The field to fetch the candidate suggestions from. This is an required option that either needs to be set globally or per suggestion.
- `analyzer` - The analyzer to analyse the suggest text with. Defaults to the search analyzer of the suggest field.
- `size` - The maximum corrections to be returned per suggest text token.
- `sort` - Defines how suggestions should be sorted per suggest text term. Two possible value:
  *\* `score` - Sort by sore first, then document frequency and then the term itself.
  *\* `frequency` - Sort by document frequency first, then simlarity score and then the term itself.
- `suggest_mode` - The suggest mode controls what suggestions are included or controls for what suggest text terms, suggestions should be suggested. Three possible values can be specified:
  *\* `missing` - Only suggest terms in the suggest text that aren't in the index. This is the default.
  *\* `popular` - Only suggest suggestions that occur in more docs then the original suggest text term.
  *\* `always` - Suggest any matching suggestions based on terms in the suggest text.
# Other fuzzy suggest options:
- `lowercase_terms` - Lower cases the suggest text terms after text analyzation.
- `max_edits` - The maximum edit distance candidate suggestions can have in order to be considered as a suggestion. Can only be a value between 1 and 2. Any other value result in an bad request error being thrown. Defaults to 2.
- `min_prefix` - The number of minimal prefix characters that must match in order be a candidate suggestions. Defaults to 1. Increasing this number improves spellcheck performance. Usually misspellings don't occur in the beginning of terms.
- `min_query_length` -  The minimum length a suggest text term must have in order to be included. Defaults to 4.
- `shard_size` - Sets the maximum number of suggestions to be retrieved from each individual shard. During the reduce phase only the top N suggestions are returned based on the `size` option. Defaults to the `size` option. Setting this to a value higher than the `size` can be useful in order to get a more accurate document frequency for spelling corrections at the cost of performance. Due to the fact that terms are partitioned amongst shards, the shard level document frequencies of spelling corrections may not be precise. Increasing this will make these document frequencies more precise.
- `max_inspections` - A factor that is used to multiply with the `shards_size` in order to inspect more candidate spell corrections on the shard level. Can improve accuracy at the cost of performance. Defaults to 5.
- `threshold_frequency` - The minimal threshold in number of documents a suggestion should appear in. This can be specified as an absolute number or as a relative percentage of number of documents. This can improve quality by only suggesting high frequency terms. Defaults to 0f and is not enabled. If a value higher than 1 is specified then the number cannot be fractional. The shard level document frequencies are used for this option.
- `max_query_frequency` - The maximum threshold in number of documents a sugges text token can exist in order to be included. Can be a relative percentage number (e.g 0.4) or an absolute number to represent document frequencies. If an value higher than 1 is specified then fractional can not be specified. Defaults to 0.01f. This can be used to exclude high frequency terms from being spellchecked. High frequency terms are usually spelled correctly on top of this this also improves the spellcheck performance.  The shard level document frequencies are used for this option.
</description><key id="10274425">2585</key><summary>Add suggest api</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">martijnvg</reporter><labels><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-01-24T14:35:31Z</created><updated>2013-03-01T20:39:11Z</updated><resolved>2013-01-24T14:41:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-24T14:41:45Z" id="12653881">cool shit! +1
</comment><comment author="mattweber" created="2013-01-24T15:51:51Z" id="12657410">Thank you!  This is one of the major features I needed!
</comment><comment author="Paikan" created="2013-01-24T16:12:44Z" id="12658533">@martijnvg thanks this was really needed!

IMO it is a great example of the ES philosophy the feature come with sensible default values which make it easy to use out of the box, it is highly customisable for power users but with protection to avoid too costly computations
</comment><comment author="otisg" created="2013-01-24T17:49:21Z" id="12663637">Very nice.
One Q/comment.  The term "suggestion" seems to be used in both directions, so to speak.  The inputs into suggester is being called "suggestions", as are the outputs (the actual suggestions).  The former is a bit confusing, because what is being input is not a suggestion really.  It's some sort of input for which you would like to get suggestions.
</comment><comment author="dadoonet" created="2013-01-24T18:07:58Z" id="12664560">Yes! Smart.
And +1 for @otisg comment. Perhaps `queries` or `phrases` could be easier to understand?
</comment><comment author="jprante" created="2013-01-24T20:42:46Z" id="12672241">+1 for @otisg I would call the input `candidates` or just `terms`.

Would be cool to see an auto-resubmit feature for queries, with corrected terms based on best/first suggestion, so it could work totally behind the scenes.
</comment><comment author="martijnvg" created="2013-01-28T14:24:34Z" id="12783708">Updated the api and documentation. The actual suggestions are now called `options` and some field names have been changed (`term` -&gt; `text`, `start_offset` -&gt; `offset` ...). Also the request format has changed a bit. The suggester is now specified as a json object (each suggester will have its own set of options) and top level field suggestions has been removed.
</comment><comment author="jtreher" created="2013-02-04T15:21:01Z" id="13081399">Thank you so much!!!!
</comment><comment author="mkleen" created="2013-02-15T12:47:27Z" id="13605054">+1 love it, thanks a lot! 
</comment><comment author="jtreher" created="2013-03-01T20:39:11Z" id="14310128">Note for others, min_prefix is now prefix_length.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Cleanup MatchQuery</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2584</link><project id="" key="" /><description>I cleaned up MatchQuery a bit and simply moved the IOException up one level. It won't happen unless there is something horribly wrong and then we should throw anyway.
</description><key id="10271686">2584</key><summary>Cleanup MatchQuery</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-24T13:01:25Z</created><updated>2014-07-16T21:54:02Z</updated><resolved>2013-01-28T14:49:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-28T14:49:01Z" id="12784833">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose CommonTermsQuery in ES</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2583</link><project id="" key="" /><description>Lucene 4.1 has a handy CommonTermsQuery. We should expose it
</description><key id="10270654">2583</key><summary>Expose CommonTermsQuery in ES</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>feature</label></labels><created>2013-01-24T12:24:54Z</created><updated>2013-01-24T14:02:18Z</updated><resolved>2013-01-24T14:02:18Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Reuse MemoryIndex instances across Percolator requests.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2582</link><project id="" key="" /><description>- added configurable MemoryIndexPool that pools MemoryIndex instance across Threads
- Pool can be configured based on the number of pooled instances as well as the maximum number of bytes that is reused across the pooled instances

Closes #2581
</description><key id="10268391">2582</key><summary>Reuse MemoryIndex instances across Percolator requests.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-24T10:54:24Z</created><updated>2014-07-16T21:54:03Z</updated><resolved>2013-01-24T10:56:12Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-24T10:56:12Z" id="12646266">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Reuse MemoryIndex across requests</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2581</link><project id="" key="" /><description>With Lucene 4.1 memory index can reuse its internal memory structures. We should take advantage of this and reuse MemoryIndex in Percolator requests
</description><key id="10267545">2581</key><summary>Reuse MemoryIndex across requests</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label></labels><created>2013-01-24T10:24:07Z</created><updated>2013-01-24T10:55:34Z</updated><resolved>2013-01-24T10:55:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Question of search</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2580</link><project id="" key="" /><description>I want to use it on the project and want to understand whether it will approach for us. The opportunity should be the union queries on the sql data type. There are 2 tables with data, can you bring in one thread a result of the 2 tables? thus creating aliases and structure of the tables is different
Example:

The first table(tovar)
id
name
categories_id
barcode

The second table(prices)
price_id
categories_id
tovar_id
price

1) Need to do a search on tovar and the name field
SELECT id FROM table1 WHERE MATCH('search')
2) Attach the recording with prices and specify conditions for tovar_id
SELECT price_id, tovar_id, barcode FROM table1, prices WHERE MATCH('search') and categories_id IN (2,3,4) GRIUP BY tovar_id ORDER BY price

This is an example of the request as if it worked, I need to bring up those records which are in table prices on tovar_id and sort them, if no entries in the prices_id down, this is the problem I solved by checking the existence of the field

SELECT price_id, tovar_id, barcode, EXIST('price', 99999999.99) as suma_sort FROM table1, prices WHERE MATCH('search') and categories_id IN (2,3,4) GRIUP BY tovar_id ORDER BY ASC suma_sort

If there are no fields price the value of suma_sort will be 99999999.99 and then sort. And also the opportunity to sample the offset limit

SELECT price_id, tovar_id, barcode, EXIST('price', 99999999.99) as suma_sort FROM table1, prices WHERE MATCH('search') and categories_id IN (2,3,4) GRIUP BY tovar_id ORDER BY suma_sort ASC LIMIT 0,20 for the implementation of the paging output.

I would like to know if I can do it in elasticsearch. Thank you very much!
</description><key id="10262480">2580</key><summary>Question of search</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">nekulin</reporter><labels /><created>2013-01-24T05:53:42Z</created><updated>2013-01-24T16:43:06Z</updated><resolved>2013-01-24T16:43:06Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-01-24T16:43:05Z" id="12660209">Hiya

The issues list is for problems with Elasticsearch itself, not for support.

Please email the mailing list with your question
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>A forgotten Lucene 4.1 version reference</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2579</link><project id="" key="" /><description /><key id="10233571">2579</key><summary>A forgotten Lucene 4.1 version reference</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">synhershko</reporter><labels /><created>2013-01-23T14:20:16Z</created><updated>2014-07-16T21:54:03Z</updated><resolved>2013-01-23T14:36:01Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-23T14:27:02Z" id="12597459">thanks for catching this! running tests again and then I will push it
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>remove flush check IW#commit always adds a commit point now even if no docs have changed</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2578</link><project id="" key="" /><description /><key id="10231259">2578</key><summary>remove flush check IW#commit always adds a commit point now even if no docs have changed</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-23T12:59:55Z</created><updated>2014-07-16T21:54:04Z</updated><resolved>2013-01-23T13:07:33Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-23T13:07:33Z" id="12594633">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove settings option for index store compression, compression is always enabled</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2577</link><project id="" key="" /><description>Compression on stored fields is always enabled, and the `index.store.compress.xxx` flags are no longer relevant, so they are removed. The compression now uses the Lucene codec based compression.
</description><key id="10229974">2577</key><summary>Remove settings option for index store compression, compression is always enabled</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>breaking</label><label>enhancement</label><label>v0.90.0.Beta1</label></labels><created>2013-01-23T12:11:30Z</created><updated>2013-01-23T12:11:53Z</updated><resolved>2013-01-23T12:11:53Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Upgrade to Lucene 4.1</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2576</link><project id="" key="" /><description>Lucene 4.1 has been release... we should upgrade
</description><key id="10229093">2576</key><summary>Upgrade to Lucene 4.1</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label></labels><created>2013-01-23T11:36:22Z</created><updated>2013-01-23T11:37:32Z</updated><resolved>2013-01-23T11:37:32Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-23T11:37:32Z" id="12591854">upgrade committed in 2880cd01720455bcd8fffea23034ec6e8b220bfd
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>0.20</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2575</link><project id="" key="" /><description /><key id="10224986">2575</key><summary>0.20</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">noQ</reporter><labels /><created>2013-01-23T09:03:52Z</created><updated>2014-07-03T02:17:39Z</updated><resolved>2013-01-26T02:58:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="imotov" created="2013-01-26T02:58:23Z" id="12729978">I assume that it was opened by accident. Closing.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Adding java-6-openjdk-amd64 to the list of available java's</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2574</link><project id="" key="" /><description /><key id="10223884">2574</key><summary>Adding java-6-openjdk-amd64 to the list of available java's</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">DanielMuller</reporter><labels /><created>2013-01-23T08:06:17Z</created><updated>2014-07-16T21:54:05Z</updated><resolved>2013-01-23T17:37:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-23T17:37:02Z" id="12611708">pushed thanks!
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for externalValues in GeoMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2573</link><project id="" key="" /><description>Here is the support of externalValue(Point) for GeoPoint Field Mapper.
Fix #2553
</description><key id="10192841">2573</key><summary>Add support for externalValues in GeoMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-01-22T14:03:45Z</created><updated>2014-06-13T09:46:49Z</updated><resolved>2014-02-03T07:40:23Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2014-02-03T07:40:23Z" id="33929760">Closed in favor of #4986.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unicast discovery fails in client mode if #unicast.hosts &lt; mimimum_master_nodes</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2572</link><project id="" key="" /><description>When using unicast discovery where the ping hosts are a subset of the nodes in the cluster (4 'seeds', 8 nodes total), joining a data node (`node.data: true`) to the cluster works fine, but joining a client node (`node.client: true`) fails as it feels it's not found the master. Pinging the seeding hosts works fine from a network point of view. 

The join _is_ successful if either enough nodes are available in the `discovery.zen.ping.unicast.hosts` setting or `discovery.zen.minimum_master_nodes` is set to a value equal or less than the length of the hosts array.
</description><key id="10187779">2572</key><summary>Unicast discovery fails in client mode if #unicast.hosts &lt; mimimum_master_nodes</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/martijnvg/following{/other_user}', u'events_url': u'https://api.github.com/users/martijnvg/events{/privacy}', u'organizations_url': u'https://api.github.com/users/martijnvg/orgs', u'url': u'https://api.github.com/users/martijnvg', u'gists_url': u'https://api.github.com/users/martijnvg/gists{/gist_id}', u'html_url': u'https://github.com/martijnvg', u'subscriptions_url': u'https://api.github.com/users/martijnvg/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/580421?v=4', u'repos_url': u'https://api.github.com/users/martijnvg/repos', u'received_events_url': u'https://api.github.com/users/martijnvg/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/martijnvg/starred{/owner}{/repo}', u'site_admin': False, u'login': u'martijnvg', u'type': u'User', u'id': 580421, u'followers_url': u'https://api.github.com/users/martijnvg/followers'}</assignee><reporter username="">akaIDIOT</reporter><labels><label>bug</label></labels><created>2013-01-22T10:51:09Z</created><updated>2014-11-29T14:00:10Z</updated><resolved>2014-11-29T14:00:10Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-11-29T14:00:10Z" id="64952700">This appears to be fixed in 1.4.

Please reopen if you find this is not the case.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Site plugins are not invoked when running ES behind an HTTP proxy</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2571</link><project id="" key="" /><description>I want to run ES behind apache server as a proxy so it appears to my web application to be on the same server and I do not have to deal with cross domain scripting issues plus to abstract out ES physical server/port

I tested mapping /search onto ES http://localhost:9200/
(ProxyPass /search http://localhost:9200/)

It works fine for all ES core calls but plugins does not get invoked

for example

http://localhost/search/_plugin/head/

returns following error

No handler found for uri [//_plugin/head/] and method [GET]
</description><key id="10162774">2571</key><summary>Site plugins are not invoked when running ES behind an HTTP proxy</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">roytmana</reporter><labels /><created>2013-01-21T16:31:31Z</created><updated>2014-11-02T11:25:25Z</updated><resolved>2013-01-21T18:02:52Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-01-21T17:19:51Z" id="12507658">Did you try: `http://localhost:9200/_plugin/head/`?
</comment><comment author="roytmana" created="2013-01-21T17:23:44Z" id="12507835">yes accessing it directly it works fine
</comment><comment author="dadoonet" created="2013-01-21T17:29:40Z" id="12508121">So your issue seems to be an apache configuration problem, isn't it?
I'm wondering if the double slash (`//`) at the start could not be your issue.

`No handler found for uri [//_plugin/head/] and method [GET]`
</comment><comment author="roytmana" created="2013-01-21T17:34:56Z" id="12508412">David,

I do not think it is apache config issue. Please note that all core ES urls
work fine via proxy. I can do "_search" fine and tried few admin URLs they
work fine. It is just site plugins seem to be the problem. I suspect that
"_plugin" handler which delegates to registered plugins can't resolve
plugin from the URL because it is not expecting /search/_plugin or more
likely resolving it but constructing wrong URL (without /search/ part) for
accessinf the plugin

Alex

On Mon, Jan 21, 2013 at 12:29 PM, David Pilato notifications@github.comwrote:

&gt; So your issue seems to be an apache configuration problem, isn't it?
&gt; I'm wondering if the double slash (//) at the start could not be your
&gt; issue.
&gt; 
&gt; No handler found for uri [//_plugin/head/] and method [GET]
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2571#issuecomment-12508121.
</comment><comment author="dadoonet" created="2013-01-21T17:40:55Z" id="12508713">Just try:

``` sh
curl localhost:9200//_plugin/head/
```

You will get:

```
No handler found for uri [//_plugin/head/] and method [GET]
```

But this works:

``` sh
curl localhost:9200/_plugin/head/
```

IMHO, it's a misconfiguration on your side.
</comment><comment author="roytmana" created="2013-01-21T17:48:20Z" id="12509063">Yes David you are correct I jumped to the conclusion. I do have an extra /
at the end of my redirect statement
Sorry for the false alarm

On Mon, Jan 21, 2013 at 12:41 PM, David Pilato notifications@github.comwrote:

&gt; Just try:
&gt; 
&gt; curl localhost:9200//_plugin/head/
&gt; 
&gt; You will get:
&gt; 
&gt; No handler found for uri [//_plugin/head/] and method [GET]
&gt; 
&gt; But this works:
&gt; 
&gt; curl localhost:9200/_plugin/head/
&gt; 
&gt; IMHO, it's a misconfiguration on your side.
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHubhttps://github.com/elasticsearch/elasticsearch/issues/2571#issuecomment-12508713.
</comment><comment author="dadoonet" created="2013-01-21T18:01:07Z" id="12509614">Could you please close the issue?
</comment><comment author="roytmana" created="2013-01-21T18:02:52Z" id="12509692">Miss-configuration on my side. proxy was adding an extra /
</comment><comment author="aliostad" created="2014-11-02T11:25:25Z" id="61402893">+1 Thanks guys, pointed out my problem.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Uncertain field types when extracting fields from org.elasticsearch.search.SearchHit.getSource()</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2570</link><project id="" key="" /><description>I'm trying to extract fields from `org.elasticsearch.search.SearchHit.getSource()` but I'm having difficulties of determining types of fields. For example I have a field named `rank` defined as `long` with `_mapping` and when I extract it from `Map&lt;String, Object&gt;` I couldn't be able to cast it `Long` because it returns as `java.lang.Double` or `java.lang.Integer`. I encountered same problem for short type and it returns as `java.lang.Integer`
</description><key id="10120846">2570</key><summary>Uncertain field types when extracting fields from org.elasticsearch.search.SearchHit.getSource()</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">buremba</reporter><labels /><created>2013-01-19T03:18:38Z</created><updated>2013-01-20T19:35:24Z</updated><resolved>2013-01-20T19:35:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow facet "queries" to run against unmapped fields</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2569</link><project id="" key="" /><description>An optional parameter on the facet, like sort's "ignore_unmapped" parameter would be great.

Context : We use "dynamic_templates" in our index mapping and allow clients to search, sort, and "facet" on any field that is valid by the rules of the mapping.

The problem arises when facets are run on fields that do not exist in the data, and thus are not "actually" in the mapping.

Example : With a "dynamic_template" for "rating-*" fields, we can search, filter, and sort
   (with "ignore_unmapped" = true), against a "rating-Tuna" field, even though there are no
   documents in the index with that field.

See Gist https://gist.github.com/4568689 for a script that will quickly demonstrate the problem.
</description><key id="10114240">2569</key><summary>Allow facet "queries" to run against unmapped fields</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/uboness/following{/other_user}', u'events_url': u'https://api.github.com/users/uboness/events{/privacy}', u'organizations_url': u'https://api.github.com/users/uboness/orgs', u'url': u'https://api.github.com/users/uboness', u'gists_url': u'https://api.github.com/users/uboness/gists{/gist_id}', u'html_url': u'https://github.com/uboness', u'subscriptions_url': u'https://api.github.com/users/uboness/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/211019?v=4', u'repos_url': u'https://api.github.com/users/uboness/repos', u'received_events_url': u'https://api.github.com/users/uboness/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/uboness/starred{/owner}{/repo}', u'site_admin': False, u'login': u'uboness', u'type': u'User', u'id': 211019, u'followers_url': u'https://api.github.com/users/uboness/followers'}</assignee><reporter username="">milosimpson</reporter><labels /><created>2013-01-18T21:36:11Z</created><updated>2014-01-22T11:46:02Z</updated><resolved>2014-01-22T11:46:02Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="rphadake" created="2013-05-03T05:57:25Z" id="17379614"> +1 

TermsStatsFacetParser.java has added a check for this unmapped field explicitly, adding "ignore_unmapped" option will be good.

We are heavily using unmapped fields as we don't know whether some fields will be present in index entry or not.  
</comment><comment author="uboness" created="2013-05-14T12:14:56Z" id="17872098">added support to terms facet on unmapped fields to be compatible with 0.20.x (it will also be available on 0.90.1 bug fix release)

it's not supported by other facets (yet) to keep backward compatibility

commit: d06a15ec3e
</comment><comment author="eborden" created="2013-10-21T16:35:09Z" id="26733305">I'm curious if there has been any progress on applying this setting to other facet types?

Also there is this duplicate issue that might need to be merged: https://github.com/elasticsearch/elasticsearch/issues/3086
</comment><comment author="javanna" created="2013-10-22T07:58:46Z" id="26783779">@eborden thanks for pointing that out, I just closed that other issue as duplicate of this one.

Other facets haven't been updated to keep backward compatibility. Also, for 1.0 we are focusing on aggregations, I'd suggest to have a look at issue #3300 to know more.
</comment><comment author="rookie7799" created="2013-11-20T23:29:28Z" id="28943203">hello @uboness ,
you said "it's not supported by other facets (yet) to keep backward compatibility" but will it ever be ?
</comment><comment author="rauanmayemir" created="2013-11-22T10:34:38Z" id="29063352">hi @uboness ,
is this included in elasticsearch 0.90.7? i'm getting 'nested: ElasticSearchParseException[unknown parameter [ignore_unmapped] while parsing terms facet' exception.
</comment><comment author="spinscale" created="2013-11-23T06:57:25Z" id="29127353">@rauanmaemirov the commit done was not adding the `ignore_unmapped` option, but simply adding unmapped fields to the `missing` fields in counting, if I judge it correctly.
</comment><comment author="jpountz" created="2014-01-22T11:46:02Z" id="33014605">The new aggregation framework is supposed to work fine on unmapped fields. Closing this issue as we are not going to work on facets anymore in order to focus on aggregations but please open an issue if you find problems when running aggregations against unmapped fields.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Unify maximum edit distance across fuzzy and match query to `max_edits`</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2568</link><project id="" key="" /><description>Closes #2565
</description><key id="10111875">2568</key><summary>Unify maximum edit distance across fuzzy and match query to `max_edits`</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-18T20:23:25Z</created><updated>2014-07-16T21:54:07Z</updated><resolved>2014-01-29T11:36:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2013-01-18T21:13:30Z" id="12441278">Heya @s1monw 

We should probably continue to support the old parameters, even if we remove them from the documentation
</comment><comment author="s1monw" created="2013-01-19T11:22:15Z" id="12453542">hmm, I kept all old parameters around and deprecated the API getters and setters. This should be fully bw compatible?
</comment><comment author="clintongormley" created="2013-01-19T11:23:49Z" id="12453566">Apologies - I didn't read the change carefully enough
</comment><comment author="clintongormley" created="2013-01-19T12:40:29Z" id="12454374">This change should also be applied to the query_string https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/QueryStringQueryParser.java#L164 and field https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/FieldQueryParser.java#L117 queries.

Also, what should the default be? Currently the min_similarity is 0.5.  Change to max_edits: 2?
</comment><comment author="clintongormley" created="2013-01-19T12:44:09Z" id="12454406">Similarly, change `flt` default for `max_edits` to 2?
</comment><comment author="clintongormley" created="2013-01-19T12:46:41Z" id="12454431">How does `max_edits` affect fuzzy queries when used on numeric or date fields?  Does anybody actually do this? A range would surely be more useful.
</comment><comment author="s1monw" created="2014-01-29T11:36:34Z" id="33577060">invalid has been fixed in a different issue
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Respect lowercase_expanded_terms in MappingQueryParser</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2567</link><project id="" key="" /><description>Fixes #2566
</description><key id="10110228">2567</key><summary>Respect lowercase_expanded_terms in MappingQueryParser</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-18T19:32:16Z</created><updated>2014-06-24T08:45:13Z</updated><resolved>2013-01-19T13:01:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Make lowercase_expanded_terms apply to fuzzy words in query_string</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2566</link><project id="" key="" /><description>In the query_string query, `lowercase_expanded_terms` applies to wildcards, but not to fuzzy terms:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&amp;explain=true'  -d '
{
   "field" : {
      "t" : {
         "query" : "full text Saerch~2 Wild*",
         "default_operator" : "AND"
      }
   }
}
'

# {
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 1,
#       "total" : 1
#    },
#    "explanations" : [
#       {
#          "index" : "test",
#          "explanation" : "+t:full +t:text +t:Saerch~2 +t:wild*",
#          "valid" : true
#       }
#    ],
#    "valid" : true
# }
```

Note: it isn't just validate that shows this - it is borne out in tests
</description><key id="10109000">2566</key><summary>Make lowercase_expanded_terms apply to fuzzy words in query_string</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>bug</label></labels><created>2013-01-18T18:52:49Z</created><updated>2013-01-19T13:01:31Z</updated><resolved>2013-01-19T13:01:31Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Rename min_similarity to fuzziness</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2565</link><project id="" key="" /><description>Now that fuzziness is really an edit_distance of 1 or 2, we should rename `min_similarity` in the fuzzy, flt and flt_field queries, and `fuzzy_min_sim` in the query_string and field queries to `fuzziness`, which is consistent with the `match` query.
</description><key id="10103485">2565</key><summary>Rename min_similarity to fuzziness</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">clintongormley</reporter><labels><label>enhancement</label></labels><created>2013-01-18T16:12:17Z</created><updated>2014-01-28T21:17:05Z</updated><resolved>2014-01-28T21:17:05Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-18T16:54:36Z" id="12430816">why don't we just call it `edit_distance`? that is really what it is?
</comment><comment author="clintongormley" created="2013-01-18T16:59:28Z" id="12431027">Sure - just for consistency. the use of `fuzziness` in the `match` query is probably more self-explanatory than `edit_distance`
</comment><comment author="s1monw" created="2013-01-18T17:02:09Z" id="12431144">@clintongormley I really doubt this, fuzziness is a really bad terms since it relates to different things for different people. Like AI / ML people might confuse it with fuzzy logic. We somehow understand it as `not_exact` but at the end of the day its a edit_distance on a string with a very specific algorithm.
</comment><comment author="clintongormley" created="2013-01-18T17:06:53Z" id="12431348">OK agreed. But I'd make it `max_edits` rather than `edit_distance` (shorter and more accurate). And I'd change `fuzzy_prefix_length` in query_string and field queries to just `prefix_length`
</comment><comment author="s1monw" created="2013-01-18T17:08:20Z" id="12431401">sounds good to me
</comment><comment author="uboness" created="2013-01-18T19:36:07Z" id="12437572">@clintongormley ++1 on max_edits, I'd also change it on the `match` query as well
</comment><comment author="ankane" created="2013-08-10T20:25:58Z" id="22446709">`max_edits` would be great since as far as I can tell, there is currently no way to specify an absolute value for the edit distance with `fuzziness`.
</comment><comment author="s1monw" created="2013-08-11T17:22:18Z" id="22461412">@ankane since 0.90 you can specify the absolute edits ie. `1` or `2` as well as a backwards compatible float `[0..1)`
</comment><comment author="ankane" created="2013-08-11T17:30:17Z" id="22461533">@s1monw good to know. Thanks!
</comment><comment author="s1monw" created="2014-01-28T21:17:05Z" id="33526402">this has actually been fixed here #4082 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Provide more information if a null DocumentMapper is returned</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2564</link><project id="" key="" /><description>if a null DocumentMapper is returned from the mapping service in TransportMoreLikeThisAction we almost certainly run into a NPE. We should throw a more reasonable exception here.
</description><key id="10101390">2564</key><summary>Provide more information if a null DocumentMapper is returned</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-18T15:13:49Z</created><updated>2014-07-16T21:54:07Z</updated><resolved>2013-01-18T15:48:13Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>return date histogram interval in respone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2563</link><project id="" key="" /><description>Users can provide various intervals for the date histogram but application code doesn't know which interval was chosen at query time. Knowing this is particularly useful when building client-side visualizations where an axis is required or labels need to be generated.
</description><key id="10100504">2563</key><summary>return date histogram interval in respone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">egaumer</reporter><labels /><created>2013-01-18T14:46:00Z</created><updated>2014-07-08T19:03:07Z</updated><resolved>2014-07-08T19:03:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="defeated" created="2013-07-31T15:46:03Z" id="21872745">this can be closed. duplicate of #2559 
</comment><comment author="spinscale" created="2013-08-01T07:01:53Z" id="21918557">no, this is the accompanying issue to the pull request #2559. A pull request is not a real issue at github. It cannot have labels for example (which we use to generate changelogs on releases).
</comment><comment author="dadoonet" created="2013-08-01T07:14:55Z" id="21919077">@spinscale it can but it's more complicated than for issues. You can do it when listing issues. In the list, you can select a pull request (checkbox) and on top on the list select labels and milestones. ;-)
</comment><comment author="defeated" created="2013-08-01T14:22:55Z" id="21939670">the github.com ui doesn't make it as easy to apply labels, but pull requests are exactly like issues (plus extra stuff.)

I originally commented because #2559 has all the salient discussion under it already, this is redundant. (also, not even a link to this issue.)
</comment><comment author="clintongormley" created="2014-07-08T19:03:07Z" id="48385147">Given that facets are going away, and aggregations are getting a new `meta` element which can contain any data that the user passes in, I think this issue is no longer relevant.  Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Filters wrongly compute size for filter cache size based eviction</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2562</link><project id="" key="" /><description /><key id="10095814">2562</key><summary>Filters wrongly compute size for filter cache size based eviction</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">kimchy</reporter><labels><label>non-issue</label></labels><created>2013-01-18T11:50:15Z</created><updated>2013-01-18T11:50:48Z</updated><resolved>2013-01-18T11:50:48Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="kimchy" created="2013-01-18T11:50:48Z" id="12418765">Not relevant..., wrong...
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Expose regexp_flags in the query_string and fields queries</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2561</link><project id="" key="" /><description>Now that `query_string` and `fields` queries support regexp syntax, we should support the same `flags` and `flags_value` parameters that the `regexp` query has.

Probably best named `regexp_flags` and `regexp_flags_value`
</description><key id="10095213">2561</key><summary>Expose regexp_flags in the query_string and fields queries</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-01-18T11:22:33Z</created><updated>2014-07-03T19:31:26Z</updated><resolved>2014-07-03T19:31:26Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-03T19:31:26Z" id="47974416">In retrospect, I'd rather keep regexes in the query_string query simple - closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Remove deprecated StreamInput/Output#read/writeUTF</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2560</link><project id="" key="" /><description>StreamInput / Output # read/writeUTF has been deprecated quite a while now. We should get rid of this deprecated api.
</description><key id="10076680">2560</key><summary>Remove deprecated StreamInput/Output#read/writeUTF</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-17T21:43:31Z</created><updated>2014-07-16T21:54:08Z</updated><resolved>2013-01-18T07:03:46Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>return date histogram interval in respone</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2559</link><project id="" key="" /><description>Users can provide various intervals for the date histogram but
application code doesn't know which interval was chosen at query time.
Knowing this is particularly useful when building client-side
visualizations where an axis is required or labels need to be generated.

This patch returns the interval inside the date histogram facet
response.
</description><key id="10071863">2559</key><summary>return date histogram interval in respone</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">egaumer</reporter><labels /><created>2013-01-17T19:41:03Z</created><updated>2014-07-08T19:03:41Z</updated><resolved>2014-07-08T19:03:41Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-18T13:43:49Z" id="12422172">hey Eric, 

I guess returning the interval makes sense though. Can you open a separate issue for this so we can track it an connect the PR with the issue. I think I pushed something this morning that broke your PR since readUTF is gone now in favour of readString, can you update that please?
</comment><comment author="kimchy" created="2013-01-18T13:44:57Z" id="12422223">heya, I would love to suspend this pull request, we are possibly going to push soon a refactoring of field data and facet execution, would prefer to get it in afterwards...
</comment><comment author="synhershko" created="2013-01-18T14:10:08Z" id="12423146">@kimchy what sort of refactoring? any chance you'd support sampling out of the box?
</comment><comment author="egaumer" created="2013-01-18T14:24:26Z" id="12423759">@kimchy not a problem. I'll hold off until after the refactoring.
</comment><comment author="kimchy" created="2013-01-18T14:30:00Z" id="12423964">@synhershko the idea of the refactoring is to allow for different potential implementations on different levels, the goal would be to first get the refactoring in, and then start to have different impls (for example, the idea is that field data would work as an abstraction on top of DocValues, so you can facet on it as well easily).
</comment><comment author="synhershko" created="2013-01-18T14:34:17Z" id="12424133">@kimchy looking forward to it
</comment><comment author="lukas-vlcek" created="2013-03-29T13:49:31Z" id="15641314">Has this been already implemented? It is very useful feature, we would welcome it.
</comment><comment author="lukas-vlcek" created="2014-04-11T11:38:19Z" id="40194645">Ping, any update on this? Is this on the road map? I would like to drop our custom code that does this.
</comment><comment author="clintongormley" created="2014-07-08T19:03:41Z" id="48385221">Given that facets are going away, and aggregations are getting a new `meta` element which can contain any data that the user passes in, I think this issue is no longer relevant. Closing
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Allow ShardsAllocator to be configured via node level settings.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2558</link><project id="" key="" /><description>- Default ShardsAllocator is set to BalancedShardsAllocator
- Core ShardsAllocator implementations can be defined via 'cluster.routing.allocation.type'
- Core ShardsAllocator implementations are exposed via short keys 'balanced' (BalancedShardsAllocator) and 'even_shards' (EvenShardsCountAllocator)
- Third party allocators can be loaded via fully-qualified class names.

Closes #2557
</description><key id="10062142">2558</key><summary>Allow ShardsAllocator to be configured via node level settings.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-17T15:28:03Z</created><updated>2014-07-07T14:59:29Z</updated><resolved>2013-01-17T21:06:24Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Allow ShardAllocators to be configured via node level settings. </title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2557</link><project id="" key="" /><description>since #2555 we have multiple options for ShardAllocators. We should give users the ability to select their implementation as well as one of the defaults impls.
</description><key id="10061736">2557</key><summary>Allow ShardAllocators to be configured via node level settings. </summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label></labels><created>2013-01-17T15:17:03Z</created><updated>2013-01-17T21:06:14Z</updated><resolved>2013-01-17T21:06:14Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dakrone" created="2013-01-17T16:26:07Z" id="12375477">Are there any plans to allow additional weights to be added to shard distribution in the future?

For example, we created https://github.com/sonian/elasticsearch-equilibrium/ to work around ES' shard allocation because it was not taking shard sizes into account, causing some nodes in the cluster to run out of disk while others had plenty of disk. It'd be great if adding shard-size as a configurable weight was an option.
</comment><comment author="s1monw" created="2013-01-17T17:02:06Z" id="12377422">hey @dakrone, further developments will likely add additional weights related to what you are mentioning. I personally would want to have this alg. to bake in a bit before we make it more complicated.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>#2555 Added BalancedShardsAllocator that balances shards based on a weight function.</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2556</link><project id="" key="" /><description>- Weights are calculated per index and incorporate index level, global and primary related parameters
- Balance operations are executed based on a win maximation strategy that tries to relocate shards first that offer the biggest gain towards the weight functions optimum
- The WeightFunction allows settings to prefer index based balance over global balance and vice versa
- Balance operations can be throttled by raising a threshold resulting in less agressive balance operations
- WeightFunction shipps with defaults to achive evenly distributed indexes while maintaining a global balance

This closes issue #2555
</description><key id="10052977">2556</key><summary>#2555 Added BalancedShardsAllocator that balances shards based on a weight function.</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">s1monw</reporter><labels /><created>2013-01-17T10:47:54Z</created><updated>2014-06-22T09:41:57Z</updated><resolved>2013-01-17T11:04:15Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="s1monw" created="2013-01-17T11:04:15Z" id="12363346">pushed
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add new ShardsAllocator that takes indices into account when allocating and balancing shards</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2555</link><project id="" key="" /><description>The current EvenShardCountAllocator balances shards in a global fashion across all eligible nodes in the cluster. Under certain circumstances this algorithm can overload certain nodes with multiple shards for a single index while still gaining global balance. 
The shard allocation algorithm should be more flexible to take into account more information like indexes or primaries and eventually things like the size of a shard or request patterns of an index. 
</description><key id="10052926">2555</key><summary>Add new ShardsAllocator that takes indices into account when allocating and balancing shards</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/s1monw/following{/other_user}', u'events_url': u'https://api.github.com/users/s1monw/events{/privacy}', u'organizations_url': u'https://api.github.com/users/s1monw/orgs', u'url': u'https://api.github.com/users/s1monw', u'gists_url': u'https://api.github.com/users/s1monw/gists{/gist_id}', u'html_url': u'https://github.com/s1monw', u'subscriptions_url': u'https://api.github.com/users/s1monw/subscriptions', u'avatar_url': u'https://avatars0.githubusercontent.com/u/973334?v=4', u'repos_url': u'https://api.github.com/users/s1monw/repos', u'received_events_url': u'https://api.github.com/users/s1monw/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/s1monw/starred{/owner}{/repo}', u'site_admin': False, u'login': u's1monw', u'type': u'User', u'id': 973334, u'followers_url': u'https://api.github.com/users/s1monw/followers'}</assignee><reporter username="">s1monw</reporter><labels><label>enhancement</label><label>feature</label><label>v0.90.0.Beta1</label></labels><created>2013-01-17T10:46:01Z</created><updated>2013-01-17T11:03:38Z</updated><resolved>2013-01-17T11:03:38Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments /><attachments /><subtasks /><customfields /></item><item><title>Terms facet silently ignores 'a'</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2554</link><project id="" key="" /><description>I've tried the following:

```
 curl -XPOST 'http://localhost:9200/foo/test' -d '{"d":"a", "p":2}'
 curl -XPOST 'http://localhost:9200/foo/test' -d '{"d":"b", "p":2}'
 curl -XPOST 'http://localhost:9200/foo/test' -d '{"d":"c", "p":2}'
```

Then a facetted query on d.

```
 curl -XGET 'http://localhost:9200/foo/_search?pretty=true' -d @foo-query 
```

where `foo-query` contains

```
{
"fields": ["d" , "p"],
"query": {
    "match_all": {}
},
"facets": {
    "f1": {
        "terms": {
            "field": "d"
        }
    }
}
}
```

The result excludes the count for d=='a': 

```
"facets" : {
  "f1" : {
    "_type" : "terms",
    "missing" : 1,
    "total" : 2,
    "other" : 0,
    "terms" : [ {
      "term" : "c",
      "count" : 1
    }, {
      "term" : "b",
      "count" : 1
    } ]
  }
}
```

But that doesn't seem to relate to anything I have done. If I try with different values (not 'a') it works fine.
</description><key id="10047920">2554</key><summary>Terms facet silently ignores 'a'</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">iwein</reporter><labels /><created>2013-01-17T06:35:43Z</created><updated>2013-01-18T07:01:54Z</updated><resolved>2013-01-18T07:01:54Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-01-17T07:06:07Z" id="12356841">You are using a default analyzer.
"a" is an english stopword. So it&#180;s not indexed.
</comment><comment author="iwein" created="2013-01-18T07:01:53Z" id="12410022">That makes sense.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Add support for externalValues in GeoMapper</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2553</link><project id="" key="" /><description>As we support it for the `StringFieldMapper`, we should be able to support external values with the GeoMapper.
</description><key id="10026139">2553</key><summary>Add support for externalValues in GeoMapper</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">dadoonet</reporter><labels /><created>2013-01-16T17:05:15Z</created><updated>2013-01-22T14:04:44Z</updated><resolved>2013-01-22T14:04:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="dadoonet" created="2013-01-22T14:04:44Z" id="12545601">Closing the issue as pull request #2573 exists now.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>GeoShape Within Query/Filter improvement to algorithm</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2552</link><project id="" key="" /><description>The algorithm used for the within filters &amp; queries for geo_shapes (TermQueryPrefixStrategy) return inconsistent results for geometries that are equal to or slightly smaller than the filter-geometry. In most cases such geometries would not be returned by a within filter. The reason is that the hashes in the must-not part of the term-filter will likely overlap with the hashes from the intersect-filter (See TermQueryPrefixStrategy around line 126 and 145).

A simple fix would be to drop all terms from the must-not filter that are in the should-filter, and therefore represent the edges of the filter geometry.

That would result in a more consistent filter behavior: within will always return geometries that are equal to the filter geometry.
</description><key id="10022577">2552</key><summary>GeoShape Within Query/Filter improvement to algorithm</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">gjdev</reporter><labels><label>discuss</label></labels><created>2013-01-16T15:44:30Z</created><updated>2014-07-25T08:42:34Z</updated><resolved>2014-07-25T08:42:34Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="clintongormley" created="2014-07-08T18:59:39Z" id="48384670">Related to #2361? 
</comment><comment author="clintongormley" created="2014-07-25T08:42:34Z" id="50122974">Closing in favour of #2361 
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Boolean stored fields returned as strings</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2551</link><project id="" key="" /><description>A stored boolean field is being returned as a string, not as a boolean:

```
curl -XPUT 'http://127.0.0.1:9200/test/?pretty=1'  -d '
{
   "mappings" : {
      "test" : {
         "properties" : {
            "foo" : {
               "store" : "yes",
               "type" : "boolean"
            }
         }
      }
   }
}
'

curl -XPOST 'http://127.0.0.1:9200/test/test?pretty=1'  -d '
{
   "foo" : false
}
'
```

Compare `foo` with `_source.foo`:

```
curl -XGET 'http://127.0.0.1:9200/test/test/_search?pretty=1'  -d '
{
   "fields" : [
      "foo",
      "_source.foo"
   ]
}
'

# {
#    "hits" : {
#       "hits" : [
#          {
#             "_score" : 1,
#             "fields" : {
#                "foo" : "false",
#                "_source.foo" : false
#             },
#             "_index" : "test",
#             "_id" : "KKX8lZ1fRO-_MPCKsbzfnQ",
#             "_type" : "test"
#          }
#       ],
#       "max_score" : 1,
#       "total" : 1
#    },
#    "timed_out" : false,
#    "_shards" : {
#       "failed" : 0,
#       "successful" : 5,
#       "total" : 5
#    },
#    "took" : 3
# }
```
</description><key id="10002357">2551</key><summary>Boolean stored fields returned as strings</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">clintongormley</reporter><labels /><created>2013-01-16T00:15:15Z</created><updated>2015-04-22T20:02:59Z</updated><resolved>2014-07-03T19:30:07Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="monken" created="2013-01-16T00:16:00Z" id="12297413">+1
</comment><comment author="rwstauner" created="2013-07-21T22:08:26Z" id="21318014">This appears to have been fixed in `0.90.0`
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Handles URLs with basic authentication for plugin downloader</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2550</link><project id="" key="" /><description>The plugin downloader which is invoked by the plugin command line does not support downloading the plugin from a URL which is secured using basic auth.

**Use Case**

In the continuous integration process if we want to install a plugin where the plugin artifact is available on the CI server which is restricted with basic auth this is not possible.

**Workaround**

Download using wget passing auth details and use the elasticsearch plugin command line passing the _file_ url instead of _http_ url

**Good to have**

Plugin command line to have options to take in basic auth credentials and pass it on to the downloader.
</description><key id="9978416">2550</key><summary>Handles URLs with basic authentication for plugin downloader</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">{u'following_url': u'https://api.github.com/users/dadoonet/following{/other_user}', u'events_url': u'https://api.github.com/users/dadoonet/events{/privacy}', u'organizations_url': u'https://api.github.com/users/dadoonet/orgs', u'url': u'https://api.github.com/users/dadoonet', u'gists_url': u'https://api.github.com/users/dadoonet/gists{/gist_id}', u'html_url': u'https://github.com/dadoonet', u'subscriptions_url': u'https://api.github.com/users/dadoonet/subscriptions', u'avatar_url': u'https://avatars3.githubusercontent.com/u/274222?v=4', u'repos_url': u'https://api.github.com/users/dadoonet/repos', u'received_events_url': u'https://api.github.com/users/dadoonet/received_events', u'gravatar_id': u'', u'starred_url': u'https://api.github.com/users/dadoonet/starred{/owner}{/repo}', u'site_admin': False, u'login': u'dadoonet', u'type': u'User', u'id': 274222, u'followers_url': u'https://api.github.com/users/dadoonet/followers'}</assignee><reporter username="">katta</reporter><labels /><created>2013-01-15T13:43:08Z</created><updated>2014-07-09T17:57:44Z</updated><resolved>2014-07-09T17:57:44Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="katta" created="2013-01-15T13:43:55Z" id="12267127">This pull request fixes https://github.com/elasticsearch/elasticsearch/issues/2549
</comment><comment author="katta" created="2013-01-15T13:55:53Z" id="12267654">The plugin downloader which is invoked by the plugin command line does not support downloading the plugin from a URL which is secured using basic auth.

**Use Case**

In the continuous integration process if we want to install a plugin where the plugin artifact is available on the CI server which is restricted with basic auth this is not possible.

**Workaround**

Download using wget passing auth details and use the elasticsearch plugin command line passing the _file_ url instead of _http_ url

**Good to have**

Plugin command line to have options to take in basic auth credentials and pass it on to the downloader.
</comment><comment author="clintongormley" created="2014-07-08T18:57:57Z" id="48384432">@dadoonet have you seen this?
</comment><comment author="dadoonet" created="2014-07-09T09:10:28Z" id="48446564">Thanks @clintongormley! Will look at it.

@katta Could sign the CLA so I can merge it? http://www.elasticsearch.org/contributor-agreement/

Thanks!
</comment><comment author="katta" created="2014-07-09T10:54:20Z" id="48455679">&#8203;Hi David,

I have signed the agreement. Please go ahead with the pull request.

Thanks,
 Katta&#8203;

On Wed, Jul 9, 2014 at 2:41 PM, David Pilato notifications@github.com
wrote:

&gt; Thanks @clintongormley https://github.com/clintongormley! Will look at
&gt; it.
&gt; 
&gt; @katta https://github.com/katta Could sign the CLA so I can merge it?
&gt; http://www.elasticsearch.org/contributor-agreement/
&gt; 
&gt; Thanks!
&gt; 
&gt; &#8212;
&gt; Reply to this email directly or view it on GitHub
&gt; https://github.com/elasticsearch/elasticsearch/pull/2550#issuecomment-48446564
&gt; .
</comment><comment author="dadoonet" created="2014-07-09T17:57:43Z" id="48510591">Thanks! Due to lot of work that was done in the meantime, I applied your work in #6803.

Closing this one.
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Plugin Installation from URL with basic auth</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2549</link><project id="" key="" /><description>The plugin downloader which is invoked by the plugin command line does not support downloading the plugin from a URL which is secured using basic auth.

**Use Case**

In the continuous integration process if we want to install a plugin where the plugin artifact is available on the CI server which is restricted with basic auth this is not possible.

**Workaround**

Download using wget passing auth details and use the elasticsearch plugin command line passing the _file_ url instead of _http_ url

**Good to have**

Plugin command line to have options to take in basic auth credentials and pass it on to the downloader.
</description><key id="9976963">2549</key><summary>Plugin Installation from URL with basic auth</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">katta</reporter><labels /><created>2013-01-15T12:46:49Z</created><updated>2013-01-15T13:55:40Z</updated><resolved>2013-01-15T13:55:40Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="katta" created="2013-01-15T13:55:40Z" id="12267643">Closing this and updating the description in pull request
</comment></comments><attachments /><subtasks /><customfields /></item><item><title>Bulk update via partial document</title><link>https://api.github.com/repos/elastic/elasticsearch/issues/2548</link><project id="" key="" /><description>I'd like to be able to send potentially thousands (or more) of partial updates (new in 0.20) to elastic using the Bulk API (which worked out well for initial index creation). Is it currently possible?

If not, please treat this as a feature request. Note that I don't care whether it supports the script syntax or not for my current use case.

Chris
</description><key id="9957560">2548</key><summary>Bulk update via partial document</summary><type iconUrl="" id="" /><priority iconUrl="" id="" /><status description="" iconUrl="" id="">closed</status><statusCategory colorName="" id="" key="" /><resolution colorName="" id="" /><assignee username="">None</assignee><reporter username="">scriby</reporter><labels /><created>2013-01-14T21:05:47Z</created><updated>2013-05-28T07:14:11Z</updated><resolved>2013-05-28T07:14:11Z</resolved><version /><fixVersion /><component /><due /><votes /><watches /><comments><comment author="spinscale" created="2013-05-28T07:14:11Z" id="18534622">Closed via 9ddd675a02431a72daf48da991ce7eda726f1c79

Please create a new issue, if you are missing something specific. Thanks!
</comment></comments><attachments /><subtasks /><customfields /></item></channel></rss>